FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Ben-Porat, O
   Tennenholtz, M
AF Ben-Porat, Omer
   Tennenholtz, Moshe
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Game-Theoretic Approach to Recommendation Systems with Strategic
   Content Providers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator fulfills the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.
C1 [Ben-Porat, Omer; Tennenholtz, Moshe] Technion Israel Inst Technol, IL-32000 Haifa, Israel.
RP Ben-Porat, O (reprint author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.
EM omerbp@campus.technion.ac.il; moshe@ie.technion.ac.il
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation programme [740435]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant agreement no 740435).
CR Adamopoulos P., 2014, P 8 ACM C REC SYST R, P153, DOI DOI 10.1145/2645710.2645752
   Ben Basat  R., 2015, P ICTIR, P51
   Ben Porat  O., 2017, ADV NEURAL INFORM PR, P1498
   Ben-Porat  O., 2018, ACCEPTED MATH OPERAT
   Ben-Porat  O., 2018, ARXIV180600955
   Bozdag E, 2013, ETHICS INF TECHNOL, V15, P209, DOI 10.1007/s10676-013-9321-6
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Burke  R., 2017, ARXIV170700093
   Burke R. D., 2016, UMAP
   Cary Matthew, 2014, ACM Transactions on Economics and Computation, V2, DOI 10.1145/2632226
   Chierichetti  F., 2017, ADV NEURAL INFORM PR, P5036
   Cohen J., 2017, P 31 INT C NEUR INF
   DENG XT, 1994, MATH OPER RES, V19, P257, DOI 10.1287/moor.19.2.257
   Dubey P., 1975, International Journal of Game Theory, V4, P131, DOI 10.1007/BF01780630
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Garg  V., 2016, ADV NEURAL INFORM PR, P1552
   Ieong S., 2005, P 6 ACM C EL COMM AC, P193, DOI DOI 10.1145/1064009.1064030
   Kamiran Faisal, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P869, DOI 10.1109/ICDM.2010.50
   Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3
   Kamishima T., 2012, DECISIONS RECSYS, P8
   Kamishima  T., 2014, RECSYS POSTERS
   Kamishima  T., 2015, ICML2015 WORKSH FAIR, P51
   Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404
   Luce R. D, 2005, INDIVIDUAL CHOICE BE
   Modani Natwar, 2017, Advances in Knowledge Discovery and Data Mining. 21st Pacific-Asia Conference, PAKDD 2017. Proceedings: LNAI 10235, P144, DOI 10.1007/978-3-319-57529-2_12
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Myerson R. B., 1977, Mathematics of Operations Research, V2, P225, DOI 10.1287/moor.2.3.225
   Pariser E., 2011, FILTER BUBBLE WHAT I
   Pedreschi D., 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959
   Pizzato L., 2010, P 4 ACM C REC SYST, P207, DOI DOI 10.1145/1864708.1864747
   Pleiss Geoff, 2017, ADV NEURAL INFORM PR, P5684
   Raifer N, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P465, DOI 10.1145/3077136.3080785
   Roughgarden T, 2009, ACM S THEORY COMPUT, P513
   Shapley L. S., 1952, TECHNICAL REPORT
   Wooldridge M, 2011, LECT NOTES ARTIF INT, V6682, P1, DOI 10.1007/978-3-642-22000-5_1
   Young H. P., 1985, International Journal of Game Theory, V14, P65, DOI 10.1007/BF01769885
   Yu H., 2011, J INFORM COMPUTATION, V8, P4061
   Zemel R., 2013, JMLR P, P325
   Zheng  Y., 2017, ARXIV170708913
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301013
DA 2019-06-15
ER

PT S
AU Benaim, S
   Wolf, L
AF Benaim, Sagie
   Wolf, Lior
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI One-Shot Unsupervised Cross Domain Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Given a single image x from domain A and a set of images from domain B, our task is to generate the analogous of x in B. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain B is trained. Then, given the new sample x, we create a variational autoencoder for domain A by adapting the layers that are close to the image in order to directly fit x, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample x, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain A. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation.
C1 [Benaim, Sagie; Wolf, Lior] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
   [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel.
RP Benaim, S (reprint author), Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
FU European Research Council (ERC) under the European Union [ERC CoG
   725974]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant ERC CoG 725974). The contribution of Sagie Benaim is
   part of Ph.D. thesis research conducted at Tel Aviv University.
CR Benaim S, 2017, ADV NEUR IN, V30
   Choi Y., 2018, IEEE C COMP VIS PATT
   Conneau A, 2017, INT C LEARN REPR ICL
   Cordts M., 2016, IEEE C COMP VIS PATT
   Fauconnier G, 2003, WAY WE THINK CONCEPT
   Fung P., 1998, P 17 INT C COMP LING, V1, P414
   Galanti T, 2018, INT C LEARN REPR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hoshen Y, 2018, C EMP METH NAT LANG
   Hoshen Y, 2018, INT C LEARN REPR ICL
   Isola P., 2017, IEEE C COMP VIS PATT
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim T, 2017, INT C MACH LEARN ICM
   Koehn P., 2002, P ACL WORKSH UNS LEX, P9
   Lample  G., 2018, INT C LEARN REPR
   LeCun Y, 2010, MNIST HANDWRITTEN DI
   Liu M.-Y., 2016, ADV NEURAL INFORM PR, P469
   Liu MY, 2017, ADV NEUR IN, V30
   LIU ZW, 2017, PROCEEDINGS OF THE 5, V1, P1959, DOI DOI 10.1109/TPAMI.2017.2737535
   Mao X, 2016, MULTI CLASS GENERATI
   Mikolov T., 2013, ARXIV13013781
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Radford  A., 2015, ARXIV151106434
   Rapp R, 1999, P 37 ANN M ASS COMP
   Schafer C, 2002, P CONLL 2002, V20, P1
   Taigman Y, 2017, INT C LEARN REPR ICL
   Tylecek R, 2013, GERM C PATT REC
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zhang M, 2017, EMNLP, P1934
   Zhu J-Y, 2017, IEEE ICC
NR 31
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302014
DA 2019-06-15
ER

PT S
AU Benson, AR
   Kleinberg, J
AF Benson, Austin R.
   Kleinberg, Jon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Found Graph Data and Planted Vertex Covers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CORE-PERIPHERY STRUCTURE; BIG DATA; CENTRALITY; DYNAMICS; MODELS
AB A typical way in which network data is recorded is to measure all interactions involving a specified set of core nodes, which produces a graph containing this core together with a potentially larger set of fringe nodes that link to the core. Interactions between nodes in the fringe, however, are not present in the resulting graph data. For example, a phone service provider may only record calls in which at least one of the participants is a customer; this can include calls between a customer and a non-customer, but not between pairs of non-customers. Knowledge of which nodes belong to the core is crucial for interpreting the dataset, but this metadata is unavailable in many cases, either because it has been lost due to difficulties in data provenance, or because the network consists of "found data" obtained in settings such as counter-surveillance. This leads to an algorithmic problem of recovering the core set. Since the core is a vertex cover, we essentially have a planted vertex cover problem, but with an arbitrary underlying graph. We develop a framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core. Our algorithms are fast, simple to implement, and out-perform several baselines based on core-periphery structure on various real-world datasets.
C1 [Benson, Austin R.; Kleinberg, Jon] Cornell Univ, Ithaca, NY 14853 USA.
RP Benson, AR (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM arb@cs.cornell.edu; kleinber@cs.cornell.edu
FU Simons Investigator Award; NSF TRIPODS Award [1740822]; NSF Award
   [DMS-1830274]
FX We thank Jure Leskovec for providing access to the email-Eu data; Mason
   Porter and Sang Hoon Lee for providing the Path-Core code; and Travis
   Martin and Thomas Zhang for providing the belief propagation code. This
   research was supported in part by a Simons Investigator Award, NSF
   TRIPODS Award #1740822, and NSF Award DMS-1830274.
CR Abbe E., 2015, ADV NEURAL INFORM PR, P676
   Abbe E., 2016, ADV NEURAL INFORM PR, V29, P1334
   Abbe E, 2018, J MACH LEARN RES, V18
   Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.3.CO;2-K
   Bader DA, 2007, LECT NOTES COMPUT SC, V4863, P124
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Borgatti SP, 1999, SOC NETWORKS, V21, P375
   Brandes U, 2001, J MATH SOCIOL, V25, P163, DOI 10.1080/0022250X.2001.9990249
   Brandes U, 2008, SOC NETWORKS, V30, P136, DOI 10.1016/j.socnet.2007.11.001
   Buneman P, 2001, LECT NOTES COMPUT SC, V1973, P316
   BUSS JF, 1993, SIAM J COMPUT, V22, P560, DOI 10.1137/0222038
   COMREY AL, 1962, PSYCHOL REP, V11, P15, DOI 10.2466/pr0.1962.11.1.15
   Craswell N., 2005, TREC, V5, P199
   Csermely P, 2013, J COMPLEX NETW, V1, P93, DOI 10.1093/comnet/cnt016
   Cucuringu M, 2016, EUR J APPL MATH, V27, P846, DOI 10.1017/S095679251600022X
   Damaschke P, 2006, THEOR COMPUT SCI, V351, P337, DOI 10.1016/j.tcs.2005.10.004
   Damaschke P, 2009, J DISCRET ALGORITHMS, V7, P391, DOI 10.1016/j.jda.2009.01.003
   Decelle  A., 2011, PHYS REV E, V84
   Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y
   Downey R. G., 2012, PARAMETERIZED COMPLE
   Eagle N, 2006, PERS UBIQUIT COMPUT, V10, P255, DOI 10.1007/s00779-005-0046-3
   Easley D., 2010, NETWORKS CROWDS MARK
   Eppstein D, 2011, LECT NOTES COMPUT SC, V6630, P364
   FEIGE U., 2010, ASS DISCRETE MATH TH, P189
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Geisberger R, 2008, SIAM PROC S, P90
   Gile KJ, 2010, SOCIOL METHODOL, V40, P285, DOI 10.1111/j.1467-9531.2010.01223.x
   Hier SP, 2009, SURVEILLANCE: POWER, PROBLEMS, AND POLITICS, P1
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Holme P, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.046111
   Jagadish HV, 2014, COMMUN ACM, V57, P86, DOI 10.1145/2611567
   Khabbazian M, 2017, ELECTRON J STAT, V11, P4769, DOI 10.1214/17-EJS1358
   Kim M., 2011, SIAM, V11, P47
   Klimt B., 2004, CEAS
   Kossinets G, 2006, SOC NETWORKS, V28, P247, DOI 10.1016/j.socnet.2005.07.002
   Koutra D., 2013, P SIAM INT C DAT MIN, P162
   Kuny  T., 1997, 63 IFLA COUNC GEN C
   Laumann E. O., 1989, RES METHODS SOC NETW, V61, P87
   Lee SH, 2014, PHYS REV E, V89, DOI 10.1103/PhysRevE.89.032810
   Leskovec J., 2005, P 11 ACM SIGKDD INT
   Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727
   Leskovec Jure, 2010, P 19 INT C WORLD WID, DOI 10.1145/1772690.1772756
   Lynch C, 2008, NATURE, V455, P28, DOI 10.1038/455028a
   Meka  R., 2015, P 47 ANN ACM S THEOR
   Monahan T, 2010, SURVEILL SOC, V8, P106
   MOSSEL E., 2014, P 27 C LEARN THEOR, V35, P356
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Oard  D., 2006, TECHNICAL REPORT
   Panzarasa P, 2009, J AM SOC INF SCI TEC, V60, P911, DOI 10.1002/asi.21015
   Peel L, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1602548
   Ripeanu M, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/4236.978369
   Rombach P, 2017, SIAM REV, V59, P619, DOI 10.1137/17M1130046
   Romero  D.M., 2016, P 25 INT C WORLD WID
   Seshadhri C., 2013, SDM, V4, P5
   Simmhan YL, 2005, SIGMOD RECORD, V34, P31, DOI 10.1145/1084805.1084812
   Spielman  D.A., 2010, GRAPHS NETWORKS LECT
   Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039
   Tan W.-C., 2004, IEEE DATA ENG B, V27, p[45, 1, 14]
   Tsiatas  A., 2013, P 22 INT C WORLD WID
   Wu  Y., 2006, CEAS
   Yin  H., 2017, P 23 ACM SIGKDD INT
   Zhang  X., 2015, PHYS REV E, V91
NR 63
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301035
DA 2019-06-15
ER

PT S
AU Bernacchia, A
   Lengyel, M
   Hennequin, G
AF Bernacchia, Alberto
   Lengyel, Mate
   Hennequin, Guillaume
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Exact natural gradient in deep linear networks and application to the
   nonlinear case
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHMS
AB Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity.
C1 [Bernacchia, Alberto; Lengyel, Mate; Hennequin, Guillaume] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.
   [Lengyel, Mate] Cent European Univ, Dept Cognit Sci, H-1051 Budapest, Hungary.
RP Bernacchia, A (reprint author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.
EM ab2347@cam.ac.uk; m.lengyel@eng.cam.ac.uk; g.hennequin@eng.cam.ac.uk
FU Wellcome Trust [202111/Z/16/Z]; Wellcome Trust Investigator Award
   [095621/Z/11/Z]
FX We thank Richard Turner and James Martens for discussions. This work was
   supported by Wellcome Trust Seed Award 202111/Z/16/Z (G.H.) and Wellcome
   Trust Investigator Award 095621/Z/11/Z (A.B., M.L.).
CR Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Ba J., 2016, P 5 INT C LEARN REPR
   Bishop C M, 2016, PATTERN RECOGNITION
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Desjardins G., 2015, ADV NEURAL INFORM PR, V28, P2071
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fujimoto Yuki, 2018, Artificial Intelligence and Soft Computing. 17th International Conference, ICAISC 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10841), P47, DOI 10.1007/978-3-319-91253-0_5
   Grosse R., 2016, ICML, P573
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Luo H., 2017, ARXIV170207097
   Mandt S, 2017, J MACH LEARN RES, V18
   Martens J, 2014, ARXIV14121193
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Martens J., 2015, INT C MACH LEARN, P2408
   Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006
   Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4
   Pascanu R., 2013, P 2 INT C LEARN REPR
   Povey Daniel, 2014, ARXIV14107455
   Roux N. L., 2008, ADV NEURAL INFORM PR, P849
   Saxe A. M., 2013, P 2 INT C LEARN REPR
   Vinyals  O., 2012, INT C ART INT STAT L, P1261
   Yang HH, 1998, NEURAL COMPUT, V10, P2137, DOI 10.1162/089976698300017007
   Zeiler M.D., 2012, ARXIV12125701
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000044
DA 2019-06-15
ER

PT S
AU Bernstein, G
   Sheldon, D
AF Bernstein, Garrett
   Sheldon, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private Bayesian Inference for Exponential Families
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.
C1 [Bernstein, Garrett; Sheldon, Daniel] Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
RP Bernstein, G (reprint author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
EM gbernstein@cs.umass.edu; sheldon@cs.umass.edu
FU National Science Foundation [1522054, 1617533]
FX This material is based upon work supported by the National Science
   Foundation under Grant Nos. 1522054 and 1617533.
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Bernstein Garrett, 2017, INT C MACH LEARN, P478
   Bickel P. J., 2015, MATH STAT BASIC IDEA, VI
   Bickel Peter J., 2015, MATH STAT BASIC IDEA, V117
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Cook SR, 2006, J COMPUT GRAPH STAT, V15, P675, DOI 10.1198/106186006X136976
   DIACONIS P, 1979, ANN STAT, V7, P269, DOI 10.1214/aos/1176344611
   Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork Cynthia, 2014, FDN TRENDS THEORETIC
   Fisher R. A., 1922, PHILOS T R SOC A, V222, P309, DOI DOI 10.1098/RSTA.1922.0009
   Foulds J, 2016, P UAI 2016, P192
   Geumlek Joseph, 2017, ADV NEURAL INFORM PR, P5295
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Jain P., 2013, P 30 INT C MACH LEAR, P118
   Karwa V, 2016, ANN STAT, V44, P87, DOI 10.1214/15-AOS1358
   Karwa V, 2014, LECT NOTES COMPUT SC, V8744, P143, DOI 10.1007/978-3-319-11257-2_12
   Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090
   Kifer D, 2011, P 2011 ACM SIGMOD IN, P193, DOI DOI 10.1145/1989323.1989345
   Kifer D., 2012, J MACHINE LEARNING R, V1, P3
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Park T, 2008, J AM STAT ASSOC, V103, P681, DOI 10.1198/016214508000000337
   Petersen K. B., 2008, TU DENMARK, V7, P510
   ROBBINS H, 1948, B AM MATH SOC, V54, P1151, DOI 10.1090/S0002-9904-1948-09142-X
   Rubinstein B. I., 2009, ARXIV09115708
   Schein Aaron, 2018, NIPS 2017 WORKSH ADV
   Smith A, 2011, ACM S THEORY COMPUT, P813
   Wang Y.-X., 2015, P 32 INT C MACH LEAR, P2493
   Williams Oliver, 2010, ADV NEURAL INFORM PR, P2451
   Zhang Zuhe, 2016, 30 AAAI C ART INT
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302090
DA 2019-06-15
ER

PT S
AU Bertsimas, D
   McCord, C
AF Bertsimas, Dimitris
   McCord, Christopher
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Optimization over Continuous and Multi-dimensional Decisions with
   Observational Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets.
C1 [Bertsimas, Dimitris] MIT, Sloan Sch Management, Cambridge, MA 02142 USA.
   [McCord, Christopher] MIT, Operat Res Ctr, Cambridge, MA 02142 USA.
RP Bertsimas, D (reprint author), MIT, Sloan Sch Management, Cambridge, MA 02142 USA.
EM dbertsim@mit.edu; mccord@mit.edu
CR Athey Susan, 2017, ARXIV PREPRINT ARXIV
   Bertsimas D., 2014, ARXIV14025481
   Bertsimas D, 2018, MATH PROGRAM, V167, P235, DOI 10.1007/s10107-017-1125-8
   Bertsimas Dimitris, 2017, POWER LIMITS PREDICT
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   Dunn Jack, 2018, OPTIMAL PRESCRIPTIVE
   Elmachtoub Adam N, 2017, ARXIV171008005
   Flores Carlos A, 2005, ESTIMATION DOSE RESP
   Hirano Keisuke, 2004, APPL BAYESIAN MODELI, P73, DOI [10.1002/0470090456.ch7, DOI 10.1002/0470090456.CH7]
   Kallus  N., 2017, P INT C MACH LEARN, P1789
   Kallus Nathan, 2017, ARXIV170507384
   Klein TE, 2009, NEW ENGL J MED, V360, P753, DOI 10.1056/NEJMoa0809329
   Maurer A., 2009, ARXIV09073740
   Misic Velibor V, 2017, ARXIV170510883
   Rosenbaum PR, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-1213-8
   Roy Benjamin V, 2009, ADV NEURAL INFORM PR, P889
   Swaminathan A, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P939, DOI 10.1145/2740908.2742564
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wager  S., 2017, J AM STAT ASS
   Zhou Angela, 2018, ARXIV180206037
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302094
DA 2019-06-15
ER

PT S
AU Bhatia, K
   Pacchiano, A
   Flammarion, N
   Bartlett, PL
   Jordan, MI
AF Bhatia, Kush
   Pacchiano, Aldo
   Flammarion, Nicolas
   Bartlett, Peter L.
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gen-Oja: A Simple and Efficient Algorithm for Streaming Generalized
   Eigenvector Computation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID STOCHASTIC-APPROXIMATION
AB In this paper, we study the problems of principal Generalized Eigenvector computation and Canonical Correlation Analysis in the stochastic setting. We propose a simple and efficient algorithm, Gen-Oja, for these problems. We prove the global convergence of our algorithm, borrowing ideas from the theory of fast-mixing Markov chains and two-time-scale stochastic approximation, showing that it achieves the optimal rate of convergence. In the process, we develop tools for understanding stochastic processes with Markovian noise which might be of independent interest.
C1 [Bhatia, Kush; Pacchiano, Aldo; Flammarion, Nicolas; Bartlett, Peter L.; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Bhatia, K (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM kushbhatia@berkeley.edu; pacchiano@berkeley.edu;
   flammarion@berkeley.edu; peter@berkeley.edu; jordan@cs.berkeley.edu
FU NSF [IIS-1619362]; BAIR-Huawei PhD Fellowship; Mathematical Data Science
   program of the Office of Naval Research [N00014-181-2764]; AFOSR
   [FA9550-17-1-0308]
FX We gratefully acknowledge the support of the NSF through grant
   IIS-1619362. AP acknowledges Huawei's support through a BAIR-Huawei PhD
   Fellowship. This work was supported in part by the Mathematical Data
   Science program of the Office of Naval Research under grant number
   N00014-181-2764. This work was partially supported by AFOSR through
   grant FA9550-17-1-0308.
CR Allen-Zhu Z., 2017, INT C MACH LEARN
   Allen-Zhu Z, 2017, ANN IEEE SYMP FOUND, P487, DOI 10.1109/FOCS.2017.51
   Andrieu C, 2005, SIAM J CONTROL OPTIM, V44, P283, DOI 10.1137/S0363012902417267
   Arora R., 2017, ADV NEURAL INFORM PR
   Bach F., 2013, ADV NEURAL INFORM PR
   Benveniste A, 1990, ADAPTIVE ALGORITHMS
   Borkar V. S., 2009, STOCHASTIC APPROXIMA, V48
   Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129, DOI DOI 10.1145/1553374.1553391
   Diaconis P, 1999, SIAM REV, V41, P45, DOI 10.1137/S0036144598338446
   Dieuleveut A., 2017, ARXIV170706386
   Gao C., 2017, ARXIV170206533
   Garber D., 2016, INT C MACH LEARN
   Ge R., 2016, INT C INT C MACH
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Jain P., 2016, C LEARN THEOR
   Kakade SM, 2007, LECT NOTES COMPUT SC, V4539, P82, DOI 10.1007/978-3-540-72927-3_8
   Karampatziakis N., 2013, ARXIV13101934
   Lu Y., 2014, ADV NEURAL INFORM PR
   Ma Z., 2015, INT C INT C MACH LEA
   Meyn S., 2009, MARKOV CHAINS STOCHA
   Musco C, 2015, ADV NEURAL INFORM PR
   Nemirovsky A. S., 1983, WILEY INTERSCIENCE S
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shamir O., 2016, INT C MACH LEARN
   Steinsaltz D, 1999, ANN PROBAB, V27, P1952, DOI 10.1214/aop/1022677556
   Tripuraneni N., 2018, C LEARN THEOR
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang W., 2016, ADV NEURAL INFORM PR
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001055
DA 2019-06-15
ER

PT S
AU Bhattacharjya, D
   Subramanian, D
   Gao, T
AF Bhattacharjya, Debarun
   Subramanian, Dharmashankar
   Gao, Tian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Proximal Graphical Event Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Event datasets involve irregular occurrences of events over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEMs) as a representation of such datasets. PGEMs belong to a broader family of graphical models that characterize relationships between various types of events; in a PGEM, the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over state-of-the-art models is that learning is entirely data driven and without the need for additional inputs from the user, which can require knowledge of the domain such as choice of basis functions and hyper-parameters. We theoretically justify our learning of parental sets and their optimal windows, proposing sound and complete algorithms in terms of parent structure learning. We present efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.
C1 [Bhattacharjya, Debarun; Subramanian, Dharmashankar; Gao, Tian] IBM Res, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Bhattacharjya, D (reprint author), IBM Res, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM debarunb@us.ibm.com; dharmash@us.ibm.com; tgao@us.ibm.com
CR Dahlhaus R, 2000, METRIKA, V51, P157, DOI 10.1007/s001840000055
   de Campos CP, 2011, J MACH LEARN RES, V12, P663
   Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x
   Didelez V, 2008, J ROY STAT SOC B, V70, P245, DOI 10.1111/j.1467-9868.2007.00634.x
   Eichler M., 1999, THESIS
   Fournier-Viger P, 2014, J MACH LEARN RES, V15, P3389
   Gao T., 2018, P INT C MACH LEARN I, P1671
   Gerner D. J., 2002, INT STUD ASS ISA ANN
   Goulding J, 2016, IEEE DATA MINING, P161, DOI [10.1109/ICDM.2016.0027, 10.1109/ICDM.2016.150]
   Gunawardana A., 2016, P 19 INT C ART INT S, P556
   Gunawardana A., 2011, ADV NEURAL INFORM PR, P1962
   Meek C., 2014, P UAI WORKSH CAUS IN, P43
   Murphy K. P., 2002, THESIS
   Nodelman  U., 2002, P 18 C UNC ART INT, P378
   O'Brien SP, 2010, INT STUD REV, V12, P87, DOI 10.1111/j.1468-2486.2009.00914.x
   Parikh A. P., 2012, P UNC ART INT WORKSH
   Rajaram S., 2005, P 10 INT WORKSH ART, P277
   Simma A., 2010, P 26 C UNC ART INT U, P546
   Simma A., 2008, P 24 C UAI AUAI, P484
   Teyssier M., 2005, UAI, P584
   Weiss Jeremy C., 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P547, DOI 10.1007/978-3-642-40994-3_35
   Zhou Ke, 2013, P 30 INT C MACH LEAR, V28, P1301
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002066
DA 2019-06-15
ER

PT S
AU Birdal, T
   Simsekli, U
   Eken, MO
   Ilic, S
AF Birdal, Tolga
   Simsekli, Umut
   Eken, M. Onur
   Ilic, Slobodan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bayesian Pose Graph Optimization via Bingham Distributions and Tempered
   Geodesic MCMC
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CAMERA MOTION
AB We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of solutions. We devise theoretical convergence guarantees and extensively evaluate our method on synthetic and real benchmarks. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.
C1 [Birdal, Tolga; Eken, M. Onur; Ilic, Slobodan] Tech Univ Munich, CAMP Chair, D-85748 Munich, Germany.
   [Birdal, Tolga; Eken, M. Onur; Ilic, Slobodan] Siemens AG, D-81739 Munich, Germany.
   [Simsekli, Umut] Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.
RP Birdal, T (reprint author), Tech Univ Munich, CAMP Chair, D-85748 Munich, Germany.; Birdal, T (reprint author), Siemens AG, D-81739 Munich, Germany.
FU French National Research Agency (ANR) as a part of the FBIMATRIX project
   [ANR-16-CE23-0014]; industrial chair Machine Learning for Big Data from
   Telecom ParisTech
FX We would like to thank Robert M. Gower and Francois Portier for fruitful
   discussions and Hans Peschke for his feedback and efforts in verifying
   the correctness of our descriptions. We thank Antonio Vargas of the
   Mathematics-StackExchange for providing the reference on inequalities
   for generalized hypergeometric functions. This work is partly supported
   by the French National Research Agency (ANR) as a part of the FBIMATRIX
   project (ANR-16-CE23-0014) and by the industrial chair Machine Learning
   for Big Data from Telecom ParisTech.
CR Arrigoni F., 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P491, DOI 10.1109/3DV.2014.48
   Arrigoni F, 2016, INT CONF 3D VISION, P546, DOI 10.1109/3DV.2016.64
   Arrigoni Federica, 2015, ARXIV150608765
   BINGHAM C, 1974, ANN STAT, V2, P1201, DOI 10.1214/aos/1176342874
   Birdal T, 2016, IEEE WINT CONF APPL
   Birdal Tolga, 2016, APPL COMP VIS WACV 2, P1
   Birdal Tolga, 2017, IEEE INT C COMP VIS
   Briales Jesus, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5134, DOI 10.1109/ICRA.2017.7989600
   Briales J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4630, DOI 10.1109/IROS.2016.7759681
   Busam B, 2017, IEEE INT CONF COMP V, P2436, DOI 10.1109/ICCVW.2017.287
   Byrne S, 2013, SCAND J STAT, V40, P825, DOI 10.1111/sjos.12036
   Carlone L, 2018, IEEE ROBOT AUTOM LET, V3, P1160, DOI 10.1109/LRA.2018.2793352
   Carlone L, 2015, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ICRA.2015.7139836
   Chatterjee A, 2018, IEEE T PATTERN ANAL, V40, P958, DOI 10.1109/TPAMI.2017.2693984
   Chatterjee A, 2013, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2013.70
   Chen C., 2016, AISTATS
   Chen C., 2015, NIPS, P2269
   Dalalyan A. S., 2017, ARXIV170404752
   Diaconis P., 2013, ADV MODERN STAT THEO, P102
   Durmus Alain, 2016, ADV NEURAL INFORM PR, P2047
   Fredriksson J., 2012, P AS C COMP VIS, P245
   Gao X., 2018, ARXIV180904618
   GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055
   Glover J, 2014, IEEE INT CONF ROBOT, P4133, DOI 10.1109/ICRA.2014.6907460
   Glover J, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P97
   Govindu V. M., 2001, Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, pII, DOI 10.1109/CVPR.2001.990963
   Govindu Venu Madhav, 2004, COMP VIS PATT REC 20, V1, pI
   Haarbach A, 2018, INT CONF 3D VISION, P381, DOI 10.1109/3DV.2018.00051
   Hartley R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3041, DOI 10.1109/CVPR.2011.5995745
   Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0
   Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X
   Hwang Chii-Ruey, 1980, ANN PROBAB, P1177
   Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599
   Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607
   Kurz G, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1487
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Lian X., 2015, ADV NEURAL INFORM PR, P2737
   Liu Chang, 2016, ADV NEURAL INFORM PR, P3009
   Liu Y., 2017, ADV NEURAL INFORM PR, P4875
   Ma Y. A., 2015, ADV NEURAL INFORM PR, P2899
   Matthews Charles, 2015, MOL DYNAMICS DETERMI, V39
   Neal R., 2011, HDB MARKOV CHAIN MON, P2, DOI DOI 10.1201/B10905-6
   Ozyesil O, 2015, SIAM J IMAGING SCI, V8, P1220, DOI 10.1137/140977576
   Pettersson J, 2018, IEEE INT CONF COMP
   Raginsky Maxim, 2017, C LEARN THEOR, P1674
   Rosen D. M., 2017, MITCSAILTR2017002
   Simsekli Umut, 2017, INT C MACH LEARN
   Simsekli  Umut, 2016, INT C MACH LEARN, P642
   Simsekli Umut, 2018, ICML 2018
   Steenrod NE, 1951, TOPOLOGY FIBRE BUNDL, V14
   Strecha C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587706
   Torsello A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2441, DOI 10.1109/CVPR.2011.5995565
   Triggs B., 1999, P INT WORKSH VIS ALG, P298, DOI DOI 10.1007/3-540-44480-7_21
   Tron R, 2014, LECT NOTES COMPUT SC, V8693, P804, DOI 10.1007/978-3-319-10602-1_52
   Tron Roberto, 2016, P IEEE C COMP VIS PA, P77
   Tzen Belinda, 2018, C LEARN THEOR
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Wilson K, 2016, LECT NOTES COMPUT SC, V9911, P255, DOI 10.1007/978-3-319-46478-7_16
   Wilson Kyle, 2014, P EUR C COMP VIS ECC
   Wu C., 2011, VISUALSFM VISUAL STR
   Ye N., 2018, INT JOINT C ART INT, V7, P3019
   Zhang  Hongyi, 2016, COLT, P1617
   Zhang  Y., 2017, COLT, P1980
NR 63
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300029
DA 2019-06-15
ER

PT S
AU Bistritz, I
   Leshem, A
AF Bistritz, Ilai
   Leshem, Amir
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Distributed Multi-Player Bandits - a Game of Thrones Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MULTIARMED BANDIT; MEDIUM ACCESS
AB We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O (log(2) T). This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.
C1 [Bistritz, Ilai] Stanford Univ, Stanford, CA 94305 USA.
   [Leshem, Amir] Bar Ilan Univ, Ramat Gan, Israel.
RP Bistritz, I (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM bistritz@stanford.edu; Amir.Leshem@biu.ac.il
CR Anandkumar A, 2011, IEEE J SEL AREA COMM, V29, P731, DOI 10.1109/JSAC.2011.110406
   Avner Orly, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P66, DOI 10.1007/978-3-662-44848-9_5
   Avner O., 2016, P 35 ANN IEEE INT C, P1
   Bertsekas D. P., 1988, Annals of Operations Research, V14, P105, DOI 10.1007/BF02186476
   Besson L., 2018, ALGORITHMIC LEARNING, P56
   Cesa- Bianchi N., 2016, C LEARN THEOR, P605
   Chung KM, 2012, LEIBNIZ INT PR INFOR, V14, P124, DOI 10.4230/LIPIcs.STACS.2012.124
   Cohen J., 2017, P 31 INT C NEUR INF
   Evirgen N., 2017, ARXIV171101628
   Hillel E., 2013, ADV NEURAL INFORM PR, P854
   Kalathil D, 2014, IEEE T INFORM THEORY, V60, P2331, DOI 10.1109/TIT.2014.2302471
   Korda N., 2016, J MACH LEARN RES, P1301
   Lai LF, 2008, CONF REC ASILOMAR C, P98, DOI 10.1109/ACSSC.2008.5074370
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Landgren P, 2016, IEEE DECIS CONTR P, P167, DOI 10.1109/CDC.2016.7798264
   Liu HY, 2013, IEEE T INFORM THEORY, V59, P1902, DOI 10.1109/TIT.2012.2230215
   Liu KQ, 2010, IEEE T SIGNAL PROCES, V58, P5667, DOI 10.1109/TSP.2010.2062509
   Marden JR, 2014, SIAM J CONTROL OPTIM, V52, P2753, DOI 10.1137/110850694
   Menon A., 2013, AM CONTR C ACC
   Nayyar N., 2016, IEEE T CONTROL NETW, VPP, P1
   Papadimitriou C. H., 1998, COMBINATORIAL OPTIMI
   Pradelski BSR, 2012, GAME ECON BEHAV, V75, P882, DOI 10.1016/j.geb.2012.02.017
   Rosenski J., 2016, INT C MACH LEARN, P155
   Shahrampour S, 2017, INT CONF ACOUST SPEE, P2786, DOI 10.1109/ICASSP.2017.7952664
   Szorenyi B., 2013, P INT C MACH LEARN, P19
   Vakili S, 2013, IEEE J-STSP, V7, P759, DOI 10.1109/JSTSP.2013.2263494
   Zavlanos MM, 2008, IEEE DECIS CONTR P, P1212, DOI 10.1109/CDC.2008.4739098
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001074
DA 2019-06-15
ER

PT S
AU Bjorck, J
   Gomes, C
   Selman, B
   Weinberger, KQ
AF Bjorck, Johan
   Gomes, Carla
   Selman, Bart
   Weinberger, Kilian Q.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Understanding Batch Normalization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.
C1 [Bjorck, Johan; Gomes, Carla; Selman, Bart; Weinberger, Kilian Q.] Cornell Univ, Ithaca, NY 14853 USA.
RP Bjorck, J (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM njb225@cornell.edu; gomes@cornell.edu; selman@cornell.edu;
   kqw4@cornell.edu
FU NSF [CCF-1522054]; AFOSR [FA9550-18-1-0136, FA9550-17-1-0292]; National
   Science Foundation [III-1618134, III-1526012, IIS-1149882, IIS-1724282,
   TRIPODS-1740822]; Bill and Melinda Gates Foundation; Office of Naval
   Research; SAP America Inc.
FX We would like to thank Yexiang Xue, Guillaume Perez, Rich Bernstein,
   Zdzislaw Burda, Liam McAllister, Yang Yuan, Vilja Jarvi, Marlene Berke
   and Damek Davis for help and inspiration. This research is supported by
   NSF Expedition CCF-1522054 and Awards FA9550-18-1-0136 and
   FA9550-17-1-0292 from AFOSR. KQW was supported in part by the
   III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822
   grants from the National Science Foundation, and generous support from
   the Bill and Melinda Gates Foundation, the Office of Naval Research, and
   SAP America Inc.
CR Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Ba J. L., 2016, ARXIV160706450
   Bertsekas D. P., 2015, CONVEX OPTIMIZATION
   Bjorck Johan, 2018, ARXIV180602375
   Chaudhari P, 2017, ARXIV171011029
   Chaudhari Pratik, 2016, ARXIV161101838
   Chen Y, 2017, ADV NEURAL INFORM PR, P4470
   Choromanska A., 2015, ARTIF INTELL, P192
   Cooijmans T., 2016, ARXIV160309025
   de Sa Chris, 2017, ADV MACHINE LEARNING
   EDELMAN A, 1988, SIAM J MATRIX ANAL A, V9, P543, DOI 10.1137/0609045
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goyal Priya, 2017, ARXIV170602677
   Hardt M., 2016, ARXIV161104231
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Hochreiter Sepp, 1991, UNTERSUCHUNGEN DYNAM, V91
   Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Ioffe Sergey, 2017, ADV NEURAL INFORM PR, P1942
   Jastrzkebski Stanislaw, 2017, ARXIV171104623
   Keskar N. S., 2016, ARXIV160904836
   Klambauer G., 2017, ADV NEURAL INFORM PR, P972
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Liu DZ, 2016, ANN I H POINCARE-PR, V52, P1734, DOI 10.1214/15-AIHP696
   Louart Cosme, 2017, ARXIV170205419
   Lu Haihao, 2017, ARXIV170208580
   Masters D., 2018, ARXIV180407612
   Mishkin Dmytro, 2015, ARXIV151106422
   Molina Carles Roger Riera, 2017, ARXIV171202609
   Neyshabur B., 2017, ADV NEURAL INFORM PR, P5949
   Pennington  J., 2017, INT C MACH LEARN, V70, P2798
   Pennington Jeffrey, NONLINEAR RANDOM MAT
   RENEGAR J, 1995, SIAM J OPTIMIZ, V5, P506, DOI 10.1137/0805026
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SAARINEN S, 1993, SIAM J SCI COMPUT, V14, P693, DOI 10.1137/0914044
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Schilling F., 2016, EFFECT BATCH NORMALI
   Schoenholz Samuel S, 2016, ARXIV161101232
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Smith Leslie N, 2017, ARXIV170807120
   Smith Leslie N, 2018, ARXIV180309820
   Smith S. L., 2017, ARXIV171100489
   Smith Samuel L, 2018, BAYESIAN PERSPECTIVE
   Ulyanov D., 2016, ARXIV160708022
   van der Smagt P, 1998, LECT NOTES COMPUT SC, V1524, P193
   Wu Shuang, 2018, ARXIV180209769
   Wu Y.X., 2018, ARXIV180308494
   Xiao Lechao, 2018, ARXIV180605393
   Yun Chulhee, 2017, ARXIV170702444
   Zhang C, 2016, ARXIV161103530
   Zolezzi T, 2003, SIAM J OPTIMIZ, V14, P507, DOI 10.1037/S1052623402411885
NR 58
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002026
DA 2019-06-15
ER

PT S
AU Blier, L
   Ollivier, Y
AF Blier, Leonard
   Ollivier, Yann
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Description Length of Deep Learning Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Solomonoff's general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Grunwald, 2007; Rissanen, 2007) formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded.
   We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks (Hinton and Van Camp, 1993; Schmidhuber, 1997). Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.
C1 [Blier, Leonard] Ecole Normale Super, Paris, France.
   [Ollivier, Yann] Facebook Artificial Intelligence Res, Paris, France.
RP Blier, L (reprint author), Ecole Normale Super, Paris, France.
EM leonard.blier@normalesup.org; yol@fb.com
CR Achille  A., 2017, ARXIV170601350
   Arora Sanjeev, 2018, ARXIV180205296
   Ba J, 2014, ADV NEURAL INFORM PR, V1, P2654
   Blum A, 2003, LECT NOTES ARTIF INT, V2777, P344, DOI 10.1007/978-3-540-45167-9_26
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Chaitin G. J., 2007, THINKING GODEL TURIN
   DAWID AP, 1984, J ROY STAT SOC A STA, V147, P278, DOI 10.2307/2981683
   Dziugaite G. K., 2017, P 33 C UNC ART INT S
   FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766
   Gao T., 2016, ARXIV160309260
   Graves A., 2011, NEURAL INFORM PROCES
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR
   Hinton G. E., 1993, P 6 ANN C COMP LEARN
   Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762
   Hutter M., 2007, THEORETICAL COMPUTER, V384
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kingma D.P., 2013, ARXIV13126114
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li  Chunyuan, 2018, ARXIV180408838
   Li M., 2008, INTRO KOLMOGOROV COM
   Louizos  C., 2017, P 30 INT C NEUR INF, P3290
   MacKay D. J, 2003, INFORM THEORY INFERE
   Ollivier Y., 2014, ARXIV14037752
   RISSANEN J, 1992, IEEE T INFORM THEORY, V38, P315, DOI 10.1109/18.119689
   Rissanen J., 2007, INFORM COMPLEXITY ST
   Romero A., 2015, P INT C LEARN REPR
   Schmidhuber J, 1997, NEURAL NETWORKS, V10, P857, DOI 10.1016/S0893-6080(96)00127-X
   See A., 2016, ARXIV160609274
   Shannon C. E., 1948, BELL SYSTEM TECHNICA, V27
   Shwartz-Ziv R., 2017, ARXIV170300810
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Solomonoff R., 1964, INFORM CONTROL
   Tallec C., 2018, PYVARINF VARIATIONAL
   Ting-Bing Xu, 2017, Image and Graphics. 9th International Conference, ICIG 2017. Revised Selected Papers: LNCS 10666, P590, DOI 10.1007/978-3-319-71607-7_52
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Ullrich K., 2017, ARXIV170204008
   van Erven T, 2012, J R STAT SOC B, V74, P361, DOI 10.1111/j.1467-9868.2011.01025.x
   Yang YH, 1999, ANN STAT, V27, P1564
   Zagoruyko S., 2015, 92 45 CIFAR 10 TORCH
   Zeng HQ, 2017, PROC INT CONF RECON
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302024
DA 2019-06-15
ER

PT S
AU Blum, A
   Gunasekar, S
   Lykouris, T
   Srebro, N
AF Blum, Avrim
   Gunasekar, Suriya
   Lykouris, Thodoris
   Srebro, Nathan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On preserving non-discrimination when combining expert advice
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of equalized odds that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, equalized error rates, we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination.
C1 [Blum, Avrim; Gunasekar, Suriya; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA.
   [Lykouris, Thodoris] Cornell Univ, Ithaca, NY 14853 USA.
RP Blum, A (reprint author), TTI Chicago, Chicago, IL 60637 USA.
EM avrim@ttic.edu; suriya@ttic.edu; teddlyk@cs.cornell.edu; nati@ttic.edu
FU NSF [CCF-1800317, CCF-1563714]; Google Ph.D. Fellowship
FX The authors would like to thank Manish Raghavan for useful discussions
   that improved the presentation of the paper. This work was supported by
   the NSF grants CCF-1800317 and CCF-1563714, as well as a Google Ph.D.
   Fellowship.
CR Angwin J, 2016, PROPUBLICA
   Angwin Julia, 2016, PROPUBLICA BLOG, V28
   Balcan Maria-Florina, 2018, ENVY FREE CLASSIFICA
   Barocas Solon, 2016, CALIFORNIA LAW REV
   Bird S., 2016, WORKSH FAIRN ACC TRA
   Blum Avrim, 2005, P 18 ANN C LEARN THE
   Buolamwini Joy, 2018, C FAIRN ACC TRANSP
   Calders Toon, 2009, IEEE INT C DAT MIN I
   Celis L. Elisa, 2017, WORKSH FAIRN ACC TRA
   Chouldechova A, 2017, BIG DATA-US, V5, P153, DOI 10.1089/big.2016.0047
   Corbett-Davies Sam, 2018, ARXIV180800023
   Datta Amit, 2015, P PRIV ENH TECHN
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S
   Even-Dar Eyal, 2008, J MACHINE LEARNING J
   Feldman Michael, 2015, P 21 ACM SIGKDD INT
   Feller Avi, 2016, WASHINGTON POST
   Freund Yoav, 1997, J COMPUT SYST SCI
   Gillen Stephen, 2018, ADV NEURAL INFORM PR
   Goh Gabriel, 2016, ADV NEURAL INFORM PR
   Hardt Moritz, 2016, ADV NEURAL INFORM PR
   Herbster Mark, 2001, J MACHINE LEARNING R
   Joseph Matthew, ADV NEURAL INFORM PR
   Kannan Sampath, 2018, ADV NEURAL INFORM PR
   Kannan Sampath, 2017, P 2017 ACM C EC COMP
   Kay Matthew, 2015, P 33 ANN ACM C HUM F
   Kearns Michael, 2018, P 35 INT C MACH LEAR
   Kilbertus Niki, 2017, ADV NEURAL INFORM PR
   Kleinberg Jon M., 2017, INNOVATIONS THEORETI
   Kusner Matt J, 2017, ADV NEURAL INFORM PR
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Liu Katherine A, 2016, PHARM PRACTICE
   Liu Lydia T., 2018, 35 INT C MACH LEARN
   Liu Yang, 2017, WORKSH FAIRN ACC TRA
   Luo Haipeng, 2015, P 28 C LEARN THEOR C
   Lykouris Thodoris, 2016, P 27 ANN ACM SIAM S
   Pedreshi Dino, 2008, P 14 ACM SIGKDD INT
   Raghavan Manish, 2018, P 31 C LEARN THEOR C
   Sweeney L, 2013, COMMUN ACM, V56, P44, DOI 10.1145/2447976.2447990
   Woodworth Blake, 2017, C LEARN THEOR COLT
   Zafar Muhammad Bilal, 2017, P 30 NEUR INF PROC S
   Zemel Rich, 2013, INT C MACH LEARN ICM
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002088
DA 2019-06-15
ER

PT S
AU Bogunovic, I
   Scarlett, J
   Jegelka, S
   Cevher, V
AF Bogunovic, Ilija
   Scarlett, Jonathan
   Jegelka, Stefanie
   Cevher, Volkan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarially Robust Optimization with Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point. We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm STABLEOPT for this purpose. We rigorously establish the required number of samples for STABLEOPT to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound. We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that STABLEOPT consistently succeeds in finding a stable maximizer where several baseline methods fail.
C1 [Bogunovic, Ilija; Cevher, Volkan] Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland.
   [Scarlett, Jonathan] Natl Univ Singapore, Singapore, Singapore.
   [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Bogunovic, I (reprint author), Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland.
EM ilija.bogunovic@epfl.ch; scarlett@comp.nus.edu.sg; stefje@mit.edu;
   volkan.cevher@epfl.ch
FU Swiss National Science Foundation (SNSF) [407540_167319]; European
   Research Council (ERC) under the European Union [725594]; DARPA DSO's
   Lagrange program [FA86501827838]; NUS startup grant
FX This work was partially supported by the Swiss National Science
   Foundation (SNSF) under grant number 407540_167319, by the European
   Research Council (ERC) under the European Union's Horizon 2020 research
   and innovation programme (grant agreement no725594 - time-data), by
   DARPA DSO's Lagrange program under grant FA86501827838, and by an NUS
   startup grant.
CR Auer Peter, 1998, TECHNICAL REPORT
   Beland Justin J., 2017, NIPS BAYESOPT 2017 W
   Bertsimas D, 2010, INFORMS J COMPUT, V22, P44, DOI 10.1287/ijoc.1090.0319
   Bertsimas D, 2010, OPER RES, V58, P161, DOI 10.1287/opre.1090.0715
   Bogunovic I., 2016, P INT C ART INT STAT, P314
   Bogunovic Ilija, 2018, INT C ART INT STAT A, P890
   Bogunovic Ilija, 2017, INT C MACH LEARN ICM, P508
   Bogunovic Ilija, 2016, ADV NEURAL INFORM PR, V29, P1507
   Chen Robert S, 2017, P 29 NIPS, P4708
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Dinh Laurent, 2017, INT C MACH LEARN ICM
   Gonzalez  J., 2016, P 19 INT C ART INT S, P648
   Gopalan Aditya, 2017, ICML, P844
   Gotovos A., 2013, P 23 INT JOINT C ART, P1344
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Kandasamy K., 2015, INT C MACH LEARN, V37, P295
   Krause A, 2008, J MACH LEARN RES, V9, P2761
   Krause Andreas, 2011, ADV NEURAL INFORM PR, P2447
   Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944
   Martinez-Cantin Ruben, 2018, INT C ART INT STAT A
   Mezghani F, 2016, IEEE ICC
   Rolland Paul, 2018, INT C ART INT STAT A, P298
   Ru Binxin, 2017, ARXIV171100673
   Scarlett Jonathan, 2017, C LEARN THEOR COLT
   Shekhar Shubhanshu, 2017, ARXIV171201447
   Sinha Aman, 2018, INT C LEARN REPR ICL
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Staib Matthew, 2018, ARXIV180205249
   Sui Y, 2015, P 32 INT C MACH LEAR, P997
   Vanchinathan H. P, 2014, P 8 ACM C REC SYST, P225
   Wang Zi, 2017, INT C MACH LEARN ICM, P3656
   Wang Zi, 2017, P 34 INT C MACH LEAR, V70, P3627
   Wilder Bryan, 2017, C ART INT AAAI
   Williams C. K., 2006, GAUSSIAN PROCESSES M, V1
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000028
DA 2019-06-15
ER

PT S
AU Branzei, S
   Mehta, R
   Nisan, N
AF Branzei, Simina
   Mehta, Ruta
   Nisan, Noam
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Universal Growth in Production Economies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
C1 [Branzei, Simina] Purdue Univ, W Lafayette, IN 47907 USA.
   [Mehta, Ruta] Univ Illinois, Urbana, IL 61801 USA.
   [Nisan, Noam] Hebrew Univ Jerusalem, Jerusalem, Israel.
   [Nisan, Noam] Microsoft Res, Redmond, WA USA.
RP Branzei, S (reprint author), Purdue Univ, W Lafayette, IN 47907 USA.
EM simina@purdue.edu; rutamehta@illinois.edu; noam@cs.huji.ac.il
FU European Research Council (ERC) under the European Union [740282]; ISF
   [1435/14]; Israel-USA Bi-national Science Foundation (BSF) grant
   [2014389]; NSF [CCF 1750436]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant agreement No 740282), from the ISF grant 1435/14
   administered by the Israeli Academy of Sciences and Israel-USA
   Bi-national Science Foundation (BSF) grant 2014389, and from the NSF
   grant CCF 1750436.
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 1
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302001
DA 2019-06-15
ER

PT S
AU Bravo, M
   Leslie, D
   Mertikopoulos, P
AF Bravo, Mario
   Leslie, David
   Mertikopoulos, Panayotis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bandit Learning in Concave N-Person Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OPTIMIZATION; DYNAMICS
AB This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents' most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that no-regret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.
C1 [Bravo, Mario] Univ Santiago Chile, Dept Matemat & Ciencia Computac, Santiago, Chile.
   [Leslie, David] Univ Lancaster, Lancaster, England.
   [Leslie, David] PROWLER Io, Cambridge, England.
   [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LIG, F-38000 Grenoble, France.
RP Bravo, M (reprint author), Univ Santiago Chile, Dept Matemat & Ciencia Computac, Santiago, Chile.
EM mario.bravo.g@usach.cl; d.leslie@lancaster.ac.uk;
   panayotis.mertikopoulos@imag.fr
FU FONDECYT [11151003]; Huawei HIRP flagship grant ULTRON; French National
   Research Agency (ANR) grant ORACLESS [ANR-16-CE33-0004-01]; ECOS project
   [C15E03]
FX M. Bravo gratefully acknowledges the support provided by FONDECYT grant
   11151003. P. Mertikopoulos was partially supported by the Huawei HIRP
   flagship grant ULTRON, and the French National Research Agency (ANR)
   grant ORACLESS (ANR-16-CE33-0004-01). Part of this work was carried out
   with financial support by the ECOS project C15E03.
CR Agarwal Alekh, 2010, COLT 10 P 23 ANN C L
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Auer Peter, 1995, P 36 ANN S FDN COMP
   Bauschke H.H., 2017, CONVEX ANAL MONOTONE
   Benaim M, 1999, LECT NOTES MATH, V1709, P1
   Bervoets Sebastian, 2018, LEARNING MINIMAL INF
   Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026
   Cohen Johanne, 2017, NIPS 17 P 31 INT C N
   D'Oro S, 2015, IEEE T WIREL COMMUN, V14, P6536, DOI 10.1109/TWC.2015.2456063
   DEBREU G, 1952, P NATL ACAD SCI USA, V38, P886, DOI 10.1073/pnas.38.10.886
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Foster Dylan J., 2016, ADV NEURAL INFORM PR, P4727
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Kleinberg Robert D., 2004, NIPS 04 P 18 ANN C N
   Mertikopoulos P, 2017, IEEE T SIGNAL PROCES, V65, P2277, DOI 10.1109/TSP.2017.2656847
   Mertikopoulos P, 2016, IEEE J SEL AREA COMM, V34, P743, DOI 10.1109/JSAC.2016.2544600
   Mertikopoulos Panayotis, 2018, MATH PROGRAMMING
   Mertikopoulos Panayotis, 2018, OPTIMISTIC MIRROR DE
   Mertikopoulos Panayotis, 2018, SODA 18 P 29 ANN ACM
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Orda Ariel, 1993, IEEE ACM T NETWORK, V1, P614
   Palaiopanos Gerasimos, 2017, NIPS 17 P 31 INT C N
   Perkins S, 2014, J ECON THEORY, V152, P179, DOI 10.1016/j.jet.2014.04.008
   Perkins S, 2017, IEEE T AUTOMAT CONTR, V62, P379, DOI 10.1109/TAC.2015.2511930
   ROSEN JB, 1965, ECONOMETRICA, V33, P520, DOI 10.2307/1911749
   Scutari G, 2010, IEEE SIGNAL PROC MAG, V27, P35, DOI 10.1109/MSP.2010.936021
   Shamir Ohad, 2013, COLT 13 P 26 ANN C L
   Sorin S, 2016, J DYN GAMES, V3, P101, DOI 10.3934/jdg.2016005
   Spall JC, 1997, AUTOMATICA, V33, P109, DOI 10.1016/S0005-1098(96)00149-5
   Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989
   Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000019
DA 2019-06-15
ER

PT S
AU Bresler, G
   Park, SM
   Persu, M
AF Bresler, Guy
   Park, Sung Min
   Persu, Madalina
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sparse PCA from Sparse Linear Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PRINCIPAL COMPONENTS; OPTIMAL RATES; POWER METHOD; EIGENVALUE;
   AGGREGATION; SELECTION; RECOVERY
AB Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomial-time algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA.
C1 [Bresler, Guy; Park, Sung Min; Persu, Madalina] MIT, Cambridge, MA 02139 USA.
   [Persu, Madalina] Two Sigma, New York, NY USA.
RP Bresler, G (reprint author), MIT, Cambridge, MA 02139 USA.
EM guy@mit.edu; sp765@mit.edu; mpersu@mit.edu
CR Amini AA, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-6, P2454, DOI 10.1109/ISIT.2008.4595432
   Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414
   Berthet Q., 2013, C LEARN THEOR, V30, P1046
   Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Bunea F, 2007, LECT NOTES COMPUT SC, V4539, P530, DOI 10.1007/978-3-540-72927-3_38
   Bunea F, 2007, ANN STAT, V35, P1674, DOI 10.1214/009053606000001587
   Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   d'Aspremont A, 2014, MATH PROGRAM, V148, P89, DOI 10.1007/s10107-014-0751-7
   Dai D, 2014, ELECTRON J STAT, V8, P302, DOI 10.1214/14-EJS886
   David  Gamarnik, 2017, P MACHINE LEARNING R, V65, P948
   Deshpande Y., 2014, ADV NEURAL INFORM PR, P334
   Do TT, 2008, CONF REC ASILOMAR C, P581, DOI 10.1109/ACSSC.2008.5074472
   Fletcher AK, 2009, IEEE T INFORM THEORY, V55, P5758, DOI 10.1109/TIT.2009.2032726
   Gamarnik  David, 2017, ARXIV171104952
   Gataric  Milana, 2017, ARXIV171205630
   Johnstone Iain M, 2009, J AM STAT ASS
   Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544
   Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148
   Journee M, 2010, J MACH LEARN RES, V11, P517
   Khanna  Rajiv, 2015, AISTATS
   Koyejo Oluwasanmi O, 2014, ADV NEURAL INFORM PR, P676
   Krauthgamer  Robert, 2013, TECHNICAL REPORT
   Laurent B, 2000, ANN STAT, V28, P1302
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854
   Rudelson  Mark, 2012, C LEARN THEOR, P10
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   van de Geer Sara, 2007, SEM STAT EIDG TECHN
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Vu V.Q., 2013, ADV NEURAL INFORM PR, P2670
   Wainwright M, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P961, DOI 10.1109/ISIT.2007.4557348
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zhang  T., 2009, ADV NEURAL INFORM PR, P1921
   Zhang  Yuchen, 2015, ARXIV150303188
   Zhang  Yuchen, 2014, ARXIV14021918
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005052
DA 2019-06-15
ER

PT S
AU Brosse, N
   Moulines, E
   Durmus, A
AF Brosse, Nicolas
   Moulines, Eric
   Durmus, Alain
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The promises and pitfalls of Stochastic Gradient Langevin Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.
C1 [Brosse, Nicolas; Moulines, Eric] Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France.
   [Durmus, Alain] Ecole Normale Super CMLA, 61 Av President Wilson, F-94235 Cachan, France.
RP Brosse, N (reprint author), Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France.
EM nicolas.brosse@polytechnique.edu; eric.moulines@polytechnique.edu;
   alain.durmus@cmla.ens-cachan.fr
CR Ahn S., 2014, P 31 INT C MACH LEAR, V32, P1044
   Ahn S., 2012, P 29 INT C MACH LEAR
   Baker J., 2017, 170605439 ARXIV
   Bardenet R, 2017, J MACH LEARN RES, V18, P1
   Chatterji N. S., 2018, 180205431 ARXIV
   Chen C., 2017, 170901180 ARXIV
   Chen C., 2015, ADV NEURAL INFORM PR, P2278
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Dalalyan A., 2017, P 2017 C LEARN THEOR, V65, P678
   Dalalyan A. S., 2017, 171000095 ARXIV
   Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154
   Durmus A., 2016, 160501559 ARXIV
   Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238
   GRENANDER U, 1994, J R STAT SOC B, V56, P549
   Grenander U., 1983, TUTORIAL PATTERN THE
   Hasenclever L, 2017, J MACH LEARN RES, V18
   Jones E., 2001, SCIPY OPEN SOURCE SC
   Karatzas  I., 1991, GRADUATE TEXTS MATH
   Korattikara A., 2014, P 31 INT C INT C MAC, V32
   LI C, 2016, P 30 AAAI C ART INT, P1788
   Ma Y.-A., 2015, ADV NEURAL INFORM PR, V2, P2917
   Nagapetyan T., 2017, 170602692 ARXIV
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Sato I., 2014, P 31 INT C MACH LEAR, V32, P982
   SungjinAhn M. W, 2016, P 19 INT C ART INT S, P723
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Teh Y. W., 2016, J MACHINE LEARNING R, V17, P193
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002078
DA 2019-06-15
ER

PT S
AU Brown, N
   Sandholm, T
   Amos, B
AF Brown, Noam
   Sandholm, Tuomas
   Amos, Brandon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Depth-Limited Solving for Imperfect-Information Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GO
AB A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold' em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.
C1 [Brown, Noam; Sandholm, Tuomas; Amos, Brandon] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Brown, N (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM noamb@cs.cmu.edu; sandholm@cs.cmu.edu; bamos@cs.cmu.edu
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082, as well as XSEDE computing
   resources provided by the Pittsburgh Supercomputing Center. We thank
   Thore Graepel, Marc Lanctot, David Silver, Ariel Procaccia, Fei Fang,
   and our anonymous reviewers for helpful inspiration, feedback,
   suggestions, and support.
CR Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown N., 2017, P ANN C NEUR INF PRO, P689
   Brown N., 2017, SCIENCE
   Brown N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P7
   Brown Noam, 2015, P INT JOINT C ARTIFI
   Brown Noam, 2016, P 25 INT JOINT C ART, P4238
   Burch Neil, 2014, AAAI C ART INT AAAI, P602
   Burch Neil, 2016, AIVAT NEW VARIANCE R
   Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1
   Cermak Jiri, 2018, ARXIV180305392
   Ganzfried S., 2014, AAAI C ART INT AAAI
   Ganzfried S, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P37
   Ganzfried Sam, 2013, P 23 INT JOINT C ART, P120
   Gilpin A., 2008, P 7 INT C AUT AG MUL, P911
   Hart Peter E, 1972, SIGART NEWSL, P28, DOI DOI 10.1145/1056777.1056779
   Heinrich Johannes, 2016, ARXIV160301121
   Jackson Eric, 2017, AAAI WORKSH COMP POK
   Jackson Eric, 2014, AAAI WORKSH COMP POK
   Johanson M., 2013, P 2013 INT C AUT AG, P271
   Johanson Michael, 2012, AAAI, P1371
   Kingma D. P., 2014, ARXIV14126980
   Lanctot  M., 2017, ADV NEURAL INFORM PR, V2017, P4193
   Lanctot Marc, 2009, ADV NEURAL INFORM PR, P1078
   LIN S, 1965, AT&T TECH J, V44, P2245, DOI 10.1002/j.1538-7305.1965.tb04146.x
   Lisy Viliam, 2016, ARXIV161207547
   McMahan H. B., 2003, P 20 INT C MACH LEAR, P536
   Moravcik Matej, 2017, SCIENCE
   Moravcik Matej, 2016, AAAI C ART INT AAAI
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Newell Allen, 1965, P IFIP C, V65, P17
   Nilsson NJ, 1971, PROBLEM SOLVING METH
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   SAMUEL AL, 1959, IBM J RES DEV, V3, P211, DOI 10.1147/rd.33.0210
   Schnizlein D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P278
   Shannon C.E., 1950, Philosophical Magazine, V41, P256
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645
   Tesauro G, 2002, ARTIF INTELL, V134, P181, DOI 10.1016/S0004-3702(01)00110-2
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002023
DA 2019-06-15
ER

PT S
AU Brunel, VE
AF Brunel, Victor-Emmanuel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Signed Determinantal Point Processes through the Principal
   Minor Assignment Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CYCLE BASIS
AB Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning, where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work, we consider a new class of DPP's, which we call signed DPP's, where we break the symmetry and allow attractive behaviors. We set the ground for learning signed DPP's through a method of moments, by solving the so called principal assignment problem for a class of matrices K that satisfy K-i,K-j = +/- K-j,K-i, i not equal j, in polynomial time.
C1 [Brunel, Victor-Emmanuel] MIT, Dept Math, Cambridge, MA 02139 USA.
RP Brunel, VE (reprint author), MIT, Dept Math, Cambridge, MA 02139 USA.
EM vebrunel@mit.edu
CR Affandi R. H., 2014, P 31 INT C MACH LEAR, P1224
   Amaldi E, 2010, LECT NOTES COMPUT SC, V6080, P397, DOI 10.1007/978-3-642-13036-6_30
   Anari N., 2016, C LEARN THEOR, P103
   Bardenet Remi, 2015, ADV NEURAL INFORM PR, P3393
   Batmanghelich Nematollah Kayhan, 2014, CORR
   Borcea J, 2009, J AM MATH SOC, V22, P521
   Brunel Victor-Emmanuel, 2017, C LEARN THEOR
   Dupuy Christophe, 2016, ARXIV161005925
   Gartrell Mike, 2016, P 10 ACM C REC SYST, P349
   Gartrell Mike, 2016, ARXIV160205436
   Gillenwater J. A., 2014, P ADV NEUR INF PROC, P3149
   HORTON JD, 1987, SIAM J COMPUT, V16, P358, DOI 10.1137/0216026
   Johnson C.R., 1995, LINEAR MULTILINEAR A, V38, P233
   Kulesza A., 2011, P INT C MACH LEARN, P1193
   Kulesza Alex, 2012, DETERMINANTAL POINT
   Lee D, 2016, LECT NOTES COMPUT SC, V9910, P330, DOI 10.1007/978-3-319-46466-4_20
   Lin H., 2012, P 28 C UNC ART INT, P479
   MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855
   Mariet Zelda, 2015, P ICML 2015, P2389
   Mariet Zelda E., 2016, ADV NEURAL INFORM PR, P2694
   Oeding L, 2011, ALGEBR NUMBER THEORY, V5, P75, DOI 10.2140/ant.2011.5.75
   Rising J, 2015, LINEAR ALGEBRA APPL, V473, P126, DOI 10.1016/j.laa.2014.04.019
   Snoek J., 2013, ADV NEURAL INFORM PR
   Urschel John, 2017, ICML
   Xu HT, 2016, IEEE-ACM T AUDIO SPE, V24, P978, DOI 10.1109/TASLP.2016.2537203
   Yao J.-g., 2016, P AAAI 2016, P3080
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001088
DA 2019-06-15
ER

PT S
AU Buckman, J
   Hafner, D
   Tucker, G
   Brevdo, E
   Lee, H
AF Buckman, Jacob
   Hafner, Danijar
   Tucker, George
   Brevdo, Eugene
   Lee, Honglak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value
   Expansion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, SIEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.
C1 [Buckman, Jacob; Hafner, Danijar; Tucker, George; Brevdo, Eugene; Lee, Honglak] Google Brain, Mountain View, CA 94043 USA.
   [Buckman, Jacob] Google AI Residency Program, San Francisco, CA 94103 USA.
RP Buckman, J (reprint author), Google Brain, Mountain View, CA 94043 USA.; Buckman, J (reprint author), Google AI Residency Program, San Francisco, CA 94103 USA.
EM jacobbuckman@gmail.com; mail@danijar.com; gjt@google.com;
   ebrevdo@google.com; honglak@google.com
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Barth- Maron G., 2018, INT C LEARN REPR
   Brockman G, 2016, ARXIV160601540
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Depeweg S., 2016, LEARNING POLICY SEAR
   Espeholt L., 2018, P INT C MACH LEARN
   Feinberg V., 2018, ARXIV180300101
   Fleiss J L, 1993, Stat Methods Med Res, V2, P121, DOI 10.1177/096228029300200202
   Fujimoto S., 2018, P MACHINE LEARNING R, V80, P1587
   Gal Y., 2016, INT C MACH LEARN, P1050
   Gal Y., IMPROVING PILCO BAYE
   Gu  S., 2016, INT C MACH LEARN, P2829
   Gu S., 2017, INT C LEARN REPR
   Haarnoja T., 2018, SOFT ACTOR CRITIC OF
   Heess N., 2015, ADV NEURAL INFORM PR, P2944
   Horgan D., 2018, INT C LEARN REPR
   Kalweit G., 2017, C ROB LEARN, P195
   Kingma D. P., 2015, INT C LEARN REPR
   Klimov O., ROBOSCHOOL
   Kurutach T., 2018, INT C LEARN REPR
   Lillicrap Timothy P., 2016, INT C LEARN REPR
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Munos Remi, 2016, ADV NEURAL INFORM PR, P1054
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Schulman  J., 2017, ARXIV170706347
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Thomas P. S., 2015, ADV NEURAL INFORM PR, P334
   Weber T., 2017, 31 C NEUR INF PROC S
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002074
DA 2019-06-15
ER

PT S
AU Bunel, R
   Turkaslan, I
   Torr, PHS
   Kohli, P
   Kumar, MP
AF Bunel, Rudy
   Turkaslan, Ilker
   Torr, Philip H. S.
   Kohli, Pushmeet
   Kumar, M. Pawan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Unified View of Piecewise Linear Neural Network Verification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. These methods are however still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we make two key contributions. First, we present a unified framework that encompasses previous methods. This analysis results in the identification of new methods that combine the strengths of multiple existing approaches, accomplishing a speedup of two orders of magnitude compared to the previous state of the art. Second, we propose a new data set of benchmarks which includes a collection of previously released testcases. We use the benchmark to provide the first experimental comparison of existing algorithms and identify the factors impacting the hardness of verification problems.
C1 [Bunel, Rudy; Turkaslan, Ilker; Torr, Philip H. S.] Univ Oxford, Oxford, England.
   [Kohli, Pushmeet] Deepmind, London, England.
   [Kumar, M. Pawan] Univ Oxford, Alan Turing Inst, Oxford, England.
RP Bunel, R (reprint author), Univ Oxford, Oxford, England.
EM rudy@robots.ox.ac.uk; ilker.turkaslan@lmh.ox.ac.uk;
   philip.torr@eng.ox.ac.uk; pushmeet@google.com; pawan@robots.ox.ac.uk
FU ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [EP/M013774/1]; EPSRC/MURI
   [EP/N019474/1]
FX This work was supported by ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC
   grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would
   also like to acknowledge the Royal Academy of Engineering and FiveAI.
CR Barrett Clark, 2006, INT C LOG PROGR ART
   Bastani Osbert, 2016, NIPS
   Buxton John N, 1970, SOFTWARE ENG TECHNIQ
   Cheng Chih-Hong, 2017, ARXIV171003107
   Dvijotham K., 2018, UAI
   Ehlers Ruediger, 2017, PLANET
   Ehlers Ruediger, 2017, AUTOMATED TECHNOLOGY
   Hein Matthias, 2017, NIPS
   Hickey Timothy, 2001, J ACM JACM
   Huang Xiaowei, 2017, INT C COMP AID VER
   Katz Guy, 2017, RELUPLEX
   Katz Guy, 2017, CAV
   Kolter Zico, 2017, ARXIV171100851
   Marques-Silva Joao P, 1999, IEEE T COMPUTERS
   Narodytska Nina, 2017, ARXIV170906662
   Pulina Luca, 2010, CAV
   Sherali Hanif D, 1994, DISCRETE APPL MATH
   Tjeng V., 2017, ARXIV171107356
   Xiang Weiming, 2017, ARXIV170803322
   Zakrzewski Radosiaw R., 2001, IJCNN
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304077
DA 2019-06-15
ER

PT S
AU Burkov, E
   Lempitsky, V
AF Burkov, Egor
   Lempitsky, Victor
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Neural Networks with Box Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Box filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as a part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of network units. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.
C1 [Burkov, Egor; Lempitsky, Victor] Samsung AI Ctr, Moscow, Russia.
   [Burkov, Egor; Lempitsky, Victor] Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.
RP Burkov, E (reprint author), Samsung AI Ctr, Moscow, Russia.; Burkov, E (reprint author), Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.
FU Skolkovo Institute of Science and Technology; Ministry of Science of
   Russian Federation [14.756.31.0001]
FX Most of the work was done when both authors were full-time with Skolkovo
   Institute of Science and Technology. The work was supported by the
   Ministry of Science of Russian Federation grant 14.756.31.0001.
CR CHANDRA S, 2016, P ECCV, V9911, P402, DOI DOI 10.1007/978-3-319-46478-7_25
   Chen L.-C., 2015, ICML, P1785
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1501/0000000035, 10.1561/0600000035]
   Dollar  P., 2009, P BMVC
   Ghodrati A, 2017, INT J COMPUT VISION, V124, P115, DOI 10.1007/s11263-017-1006-x
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Holschneider M., 1990, WAVELETS, P286, DOI DOI 10.1007/978-3-642-75988-8_28
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kingma D. P., 2014, ABS14126980 CORR
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lewis J.P., 1995, VISION INTERFACE 198, V95, P15
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Paszke  A., 2016, ABS160602147 CORR
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Shotton J., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587503
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Simonyan K, 2014, ABS14091556 CORR
   Song S., 2015, CVPR, V5, P6
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2015, ARXIV150500387
   Szegedy C., 2017, AAAI, V4, P12
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Viola P., 2001, P CVPR
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Yu  F., 2015, P ICLR
   Zhao H., 2017, IEEE C COMP VIS PATT, P2881
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000069
DA 2019-06-15
ER

PT S
AU Cai, DN
   Mitzenmacher, M
   Adams, RP
AF Cai, Diana
   Mitzenmacher, Michael
   Adams, Ryan P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Bayesian Nonparametric View on Count-Min Sketch
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The count-min sketch is a time-and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream. The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n - grams. We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation. In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens. Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees. Using simulated data and text data, we investigate the properties of these estimators. Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.
C1 [Cai, Diana; Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA.
   [Mitzenmacher, Michael] Harvard Univ, Cambridge, MA 02138 USA.
RP Cai, DN (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM dcai@cs.princeton.edu; michaelm@eecs.harvard.edu; rpa@princeton.edu
FU NSF [IIS-1421780]; Alfred P. Sloan Foundation
FX Michael Mitzenmacher was supported in part by NSF grants CCF-1563710,
   CCF-1535795, CCF-1320231, and CNS-1228598. Ryan Adams was supported in
   part by NSF IIS-1421780 and the Alfred P. Sloan Foundation.
CR Aggarwal C. C., 2010, SDM, P802
   Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   Broderick T, 2012, BAYESIAN ANAL, V7, P439, DOI 10.1214/12-BA715
   Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693
   Chung K.-M., 2013, THEORY COMPUT, V9, P897
   Cohen S., 2003, P 2003 ACM SIGMOD IN, P241, DOI DOI 10.1145/872757.872787
   Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001
   Cormode G, 2005, SIAM PROC S, P44
   Cormode G, 2011, FOUND TRENDS DATABAS, V4, P1, DOI 10.1561/1900000004
   Dwork C., 2010, P 1 S INN COMP SCI I
   Estan C, 2003, ACM T COMPUT SYST, V21, P270, DOI 10.1145/859716.859719
   EWENS WJ, 1972, THEOR POPUL BIOL, V3, P87, DOI 10.1016/0040-5809(72)90035-4
   Fan L, 2000, IEEE ACM T NETWORK, V8, P281, DOI 10.1109/90.851975
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Goyal A., 2009, P HUM LANG TECHN 200, P512
   Griffiths TL, 2011, J MACH LEARN RES, V12, P1185
   Kingman J. F. C., 1992, POISSON PROCESSES
   Minka T. P, 2000, TECHNICAL REPORT
   MISRA J, 1982, SCI COMPUT PROGRAM, V2, P143, DOI 10.1016/0167-6423(82)90012-0
   Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002
   Pitman J., 2006, LECT NOTES MATH
   Teh Y. W., 2009, ADV NEURAL INFORM PR, V22, P1838
   Watterson CA., 1974, ADV APPL PROBAB, V6, P463
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003033
DA 2019-06-15
ER

PT S
AU Cai, RC
   Qiao, J
   Zhang, K
   Zhang, ZJ
   Hao, ZF
AF Cai, Ruichu
   Qiao, Jie
   Zhang, Kun
   Zhang, Zhenjie
   Hao, Zhifeng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Causal Discovery from Discrete Data using Hidden Compact Representation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Causal discovery from a set of observations is one of the fundamental problems across several disciplines. For continuous variables, recently a number of causal discovery methods have demonstrated their effectiveness in distinguishing the cause from effect by exploring certain properties of the conditional distribution, but causal discovery on categorical data still remains to be a challenging problem, because it is generally not easy to find a compact description of the causal mechanism for the true causal direction. In this paper we make an attempt to find a way to solve this problem by assuming a two-stage causal process: the first stage maps the cause to a hidden variable of a lower cardinality, and the second stage generates the effect from the hidden representation. In this way, the causal mechanism admits a simple yet compact representation. We show that under this model, the causal direction is identifiable under some weak conditions on the true causal mechanism. We also provide an effective solution to recover the above hidden compact representation within the likelihood framework. Empirical studies verify the effectiveness of the proposed approach on both synthetic and real-world data.
C1 [Cai, Ruichu; Qiao, Jie; Hao, Zhifeng] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA.
   [Zhang, Zhenjie] Yitu Technol Ltd, Singapore R&D, Shanghai, Peoples R China.
   [Hao, Zhifeng] Foshan Univ, Sch Math & Big Data, Foshan, Peoples R China.
RP Cai, RC (reprint author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China.
EM cairuichu@gdut.edu.cn; qiaojie.chn@gmail.com; kunz1@andrew.cmu.edu;
   zhenjie.zhang@yitu-inc.com; zfhao@gdut.edu.cn
FU NSFC-Guangdong Joint Found [U1501254]; Natural Science Foundation of
   China [61876043, 61472089]; NSF of Guangdong [2014A030306004,
   2014A030308008]; Science and Technology Planning Project of Guangdong
   [2015B010108006, 2015B010131015]; Guangdong High-level Personnel of
   Special Support Program [2015TQ01X140]; Pearl River S&T Nova Program of
   Guangzhou [201610010101]; United States Air Force [FA8650-17-C-7715];
   National Science Foundation under EAGER [IIS-1829681]; National
   Institutes of Health [NIH-1R01EB022858-01, FAINR01EB022858,
   NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; Department of
   Defense [FA8702-15-D-0002]; Carnegie Mellon University for the operation
   of the Software Engineering Institute
FX This research was supported in part by NSFC-Guangdong Joint Found
   (U1501254), Natural Science Foundation of China (61876043, 61472089),
   NSF of Guangdong (2014A030306004, 2014A030308008), Science and
   Technology Planning Project of Guangdong (2015B010108006,
   2015B010131015), Guangdong High-level Personnel of Special Support
   Program (2015TQ01X140), Pearl River S&T Nova Program of Guangzhou
   (201610010101). This material is partially based upon work supported by
   United States Air Force under Contract No. FA8650-17-C-7715, by National
   Science Foundation under EAGER Grant No. IIS-1829681, and National
   Institutes of Health under Contract No. NIH-1R01EB022858-01,
   FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and
   FAIN-U54HG008540, and work funded and supported by the Department of
   Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon
   University for the operation of the Software Engineering Institute, a
   federally funded research and development center. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and do not necessarily reflect the views of the
   United States Air Force or the National Institutes of Health or the
   National Science Foundation. We appreciate the comments from anonymous
   reviewers, which greatly helped to improve the paper.
CR Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351
   Budhathoki K, 2017, IEEE DATA MINING, P751, DOI 10.1109/ICDM.2017.87
   Cai Ruichu, 2018, AAAI
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu FR, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700477
   Liu Furui, 2016, NEURAL COMPUTATION
   Pearl J, 2009, CAUSALITY MODELS REA
   Peters J., 2010, INT C ART INT STAT, P597
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Shimizu S, 2011, J MACH LEARN RES, V12, P1225
   Spirtes P., 2000, CAUSATION PREDICTION
   Verma T.S., 1995, STUDIES LOGIC FDN MA, V134, P789, DOI DOI 10.1016/S0049-237X(06)80074-1
   Zhang K., 2009, P 25 C UNC ART INT, P647
   Zhang  K., 2006, P 13 INT C NEUR INF
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302066
DA 2019-06-15
ER

PT S
AU Calandriello, D
   Rosasco, L
AF Calandriello, Daniele
   Rosasco, Lorenzo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Statistical and Computational Trade-Offs in Kernel K-Means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID QUANTIZATION
AB We investigate the efficiency of k-means in terms of both statistical and computational requirements. More precisely, we study a Nystrom approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves the same accuracy of exact kernel k-means with only a fraction of computations. Indeed, we prove under basic assumptions that sampling \root pn Nystrom landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result of this kind for unsupervised learning.
C1 [Calandriello, Daniele] IIT, LCSL, Genoa, Italy.
   [Calandriello, Daniele; Rosasco, Lorenzo] MIT, Genoa, Italy.
   [Rosasco, Lorenzo] Univ Genoa, IIT, LCSL, Genoa, Italy.
RP Calandriello, D (reprint author), IIT, LCSL, Genoa, Italy.; Calandriello, D (reprint author), MIT, Genoa, Italy.
FU Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216];
   Italian Institute of Technology; NVIDIA Corporation; AFOSR (European
   Office of Aerospace Research and Development) [FA9550-17-1-0390,
   BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project [NoMADS -
   DLV-777826]; European Research Council [SEQUOIA 724063]
FX This material is based upon work supported by the Center for Brains,
   Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the
   Italian Institute of Technology. We gratefully acknowledge the support
   of NVIDIA Corporation for the donation of the Titan Xp GPUs and the
   Tesla k40 GPU used for this research. L. R. acknowledges the support of
   the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007
   (European Office of Aerospace Research and Development), and the EU
   H2020-MSCA-RISE project NoMADS - DLV-777826. A. R. acknowledges the
   support of the European Research Council (grant SEQUOIA 724063).
CR Ailon N., 2009, NIPS, P10
   Aizerman M, 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678
   Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Avron Haim, 2017, P 34 INT C MACH LEAR
   Bach Francis, 2013, C LEARN THEOR
   Backurs Arturs, 2017, ADV NEURAL INFORM PR
   Biau G, 2008, IEEE T INFORM THEORY, V54, P781, DOI 10.1109/TIT.2007.913516
   Calandriello Daniele, 2017, INT C MACH LEARN
   Calandriello Daniele, 2017, AISTATS
   Calandriello Daniele, 2017, THESIS
   Calandriello Daniele, 2017, ADV NEURAL INFORM PR, P6140
   Canas Guillermo, 2012, ADV NEURAL INFORM PR, P2465
   Chitta R., 2011, P 17 ACM SIGKDD INT, P895
   Dhillon Inderjit S., 2004, TR0425 UTCS U TEX AU
   Francis FB, 2005, P 22 INT C MACH LEAR, P33
   Graf S, 2000, LECT NOTES MATH
   Johnson W. B., 1984, CONT MATH, V26
   Levrard C, 2015, ANN STAT, V43, P592, DOI 10.1214/14-AOS1293
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Mahoney Michael W., 2015, NEURAL INFORM PROCES
   Maurer A, 2010, IEEE T INFORM THEORY, V56, P5839, DOI 10.1109/TIT.2010.2069250
   Musco Cameron, 2017, ADV NEURAL INFORM PR, V30
   Musco Cameron, 2017, NIPS
   Oglic Dino, 2017, J MACHINE LEARNING R
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rahimi A, 2007, NEURAL INFORM PROCES
   Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657
   Rudi Alessandro, 2017, ADV NEURAL INFORM PR, P3218
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Tropp Joel A, 2017, ADV NEURAL INFORM PR, P1225
   Yann LeCun, 2010, MNIST HANDWRITTEN DI
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003087
DA 2019-06-15
ER

PT S
AU Camburu, OM
   Rocktaschel, T
   Lukasiewicz, T
   Blunsom, P
AF Camburu, Oana-Maria
   Rocktaschel, Tim
   Lukasiewicz, Thomas
   Blunsom, Phil
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI e-SNLI: Natural Language Inference with Natural Language Explanations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset(1) thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.
C1 [Camburu, Oana-Maria; Lukasiewicz, Thomas; Blunsom, Phil] Univ Oxford, Dept Comp Sci, Oxford, England.
   [Rocktaschel, Tim] UCL, Dept Comp Sci, London, England.
   [Lukasiewicz, Thomas] Alan Turing Inst, London, England.
   [Blunsom, Phil] DeepMind, London, England.
RP Camburu, OM (reprint author), Univ Oxford, Dept Comp Sci, Oxford, England.
EM oana-maria.camburu@cs.ox.ac.uk; t.rocktaschel@ucl.ac.uk;
   thomas.lukasiewicz@cs.ox.ac.uk; phil.blunsom@cs.ox.ac.uk
FU Alan Turing Institute under the EPSRC [EP/N510129/1]
FX This work was supported by the Alan Turing Institute under the EPSRC
   grant EP/N510129/1. We would also like to thank Jakob Foerster for the
   valuable discussions.
CR Alvarez-Melis D., 2017, ABS170701943 CORR
   Bahdanau  Dzmitry, 2014, ABS14090473 CORR
   Bowman S. R., 2015, ABS150805326 CORR
   Chan W., 2015, ABS150801211 CORR
   Chen Q., 2016, ABS160906038 CORR
   Chen Q., 2017, ABS171104289 CORR
   Conneau A., 2017, ABS170502364 CORR
   Das A., 2016, ABS160603556 CORR
   Dasgupta I., 2018, EVALUATING COMPOSITI
   Deng J., 2009, P CVPR
   Glockner M., 2018, P ACL
   Gong Y., 2017, ABS170904348 CORR
   Gururangan S., 2018, P NAACL
   Hill F., 2016, ABS160203483 CORR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jansen P. A., 2018, ABS180203052 CORR
   Kiros R., 2015, ABS150606726 CORR
   Ling W., 2017, ABS170504146 CORR
   Liu P., 2016, ABS160505573 CORR
   Marelli M., 2014, SICK CURE EVALUATION
   Nie Y., 2017, P 2 WORKSH EV VECT S, P41
   Parikh A. P., 2016, ABS160601933 CORR
   Park D. H., 2018, ABS180208129 CORR
   Ribeiro M. T., 2016, ABS160204938 CORR
   Rocktaschel T., 2015, ABS150906664 CORR
   Williams A., 2017, ABS170405426 CORR
   Xu K., 2015, ABS150203044 CORR
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004013
DA 2019-06-15
ER

PT S
AU Canonne, CL
   Diakonikolas, I
   Stewart, A
AF Canonne, Clement L.
   Diakonikolas, Ilias
   Stewart, Alistair
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Testing for Families of Distributions via the Fourier Transform
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID LOG-CONCAVE; EQUILIBRIA
AB We study the general problem of testing whether an unknown discrete distribution belongs to a specified family of distributions. More specifically, given a distribution family P and sample access to an unknown discrete distribution P, we want to distinguish (with high probability) between the case that P is an element of P and the case that P is epsilon-far, in total variation distance, from every distribution in P. This is the prototypical hypothesis testing problem that has received significant attention in statistics and, more recently, in computer science. The main contribution of this work is a simple and general testing technique that is applicable to all distribution families whose Fourier spectrum satisfies a certain approximate sparsity property. We apply our Fourier-based framework to obtain near sample-optimal and computationally efficient testers for the following fundamental distribution families: Sums of Independent Integer Random Variables (SIIRVs), Poisson Multinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For the first two, ours are the first non-trivial testers in the literature, vastly generalizing previous work on testing Poisson Binomial Distributions. For the third, our tester improves on prior work in both sample and time complexity.
C1 [Canonne, Clement L.] Stanford Univ, Stanford, CA 94305 USA.
   [Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Los Angeles, CA USA.
RP Canonne, CL (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ccanonne@stanford.edu; diakonik@usc.edu; stewart.al@gmail.com
CR Acharya J., 2015, P NIPS 15
   Acharya Jayadev, 2015, P 26 ANN ACM SIAM S, P1829
   An M. Y., 1995, LOG CONCAVE PROBABIL
   Barbour A. D., 1988, J APPL PROBAB A, V25, P175
   BARBOUR A. D., 1992, POISSON APPROXIMATIO
   Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113
   Bentkus V, 2003, J STAT PLAN INFER, V113, P385, DOI 10.1016/S0378-3758(02)00094-0
   Bhaskara Aditya, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P423, DOI 10.1007/978-3-642-32512-0_36
   Borgs C, 2008, ACM S THEORY COMPUT, P365
   Canonne C.L., 2015, ELECT C COMPUTATIONA, V22, P63
   Canonne C. L., 2017, ABS170605738 CORR
   Canonne C. L., 2017, THEORY COMPUTING SYS
   Canonne CL, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P735, DOI 10.1145/3188745.3188756
   Chan Siu-On, 2014, P 25 ANN ACM SIAM S, P1193, DOI DOI 10.1137/1.9781611973402.88
   Chen LHY, 2011, PROBAB APPL SER, P1, DOI 10.1007/978-3-642-15007-4
   Chen SX, 1997, STAT SINICA, V7, P875
   Cheng Y, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P616
   Chenz L. H. Y., 2010, ZERO BIAS DISCRETIZE
   CHERNOFF H, 1952, ANN MATH STAT, V23, P493, DOI 10.1214/aoms/1177729330
   Daskalakis C., 2012, STOC, P709
   Daskalakis C., 2014, J EC THEORY
   Daskalakis C., 2016, P STOC 16
   Daskalakis C, 2007, ANN IEEE SYMP FOUND, P83, DOI 10.1109/FOCS.2007.24
   Daskalakis C, 2013, ANN IEEE SYMP FOUND, P217, DOI 10.1109/FOCS.2013.31
   Daskalakis C, 2009, ACM S THEORY COMPUT, P75
   Daskalakis C, 2008, ANN IEEE SYMP FOUND, P25, DOI 10.1109/FOCS.2008.84
   De A., 2015, FOCS
   Diakonikolas I., 2016, P 29 C LEARN THEOR C, P850
   Diakonikolas I., 2015, P SODA 15
   Diakonikolas I., 2016, P STOC 16
   Diakonikolas I., 2016, P COLT 2016, P831
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Goldberg PW, 2017, J COMPUT SYST SCI, V90, P80, DOI 10.1016/j.jcss.2017.07.002
   Gopalan P., 2015, FOCS
   Gopalan P, 2011, ACM S THEORY COMPUT, P253
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kruopis J., 1986, LITH MATH J, V26, P37
   Lehmann EL, 2005, SPRINGER TEXTS STAT
   Loh W.-L., 1992, ANN APPL PROBAB, V2, P536
   Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135
   Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009
   Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987
   Poisson S. D., 1837, RECHERCHES PROBABILI
   PRESMAN EL, 1984, THEOR PROBAB APPL+, V28, P393
   Roos B, 1999, J MULTIVARIATE ANAL, V69, P120, DOI 10.1006/jmva.1998.1789
   Roos B, 2010, BERNOULLI, V16, P23, DOI 10.3150/08-BEJ171
   Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052
   Saumard A, 2014, STAT SURV, V8, P45, DOI 10.1214/14-SS107
   STANLEY RP, 1989, ANN NY ACAD SCI, V576, P500, DOI 10.1111/j.1749-6632.1989.tb16434.x
   Valiant G., 2014, FOCS
   Valiant G., 2010, ELECT C COMPUTATIONA, V17
   Valiant G, 2017, SIAM J COMPUT, V46, P429, DOI 10.1137/151002526
   Valiant G, 2011, ACM S THEORY COMPUT, P685
   Valiant P, 2008, ACM S THEORY COMPUT, P383
   Walther G, 2009, STAT SCI, V24, P319, DOI 10.1214/09-STS303
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004060
DA 2019-06-15
ER

PT S
AU Cao, J
   Hu, YB
   Zhang, HW
   He, R
   Sun, ZA
AF Cao, Jie
   Hu, Yibo
   Zhang, Hongwen
   He, Ran
   Sun, Zhenan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning a High Fidelity Pose Invariant Model for High-resolution Face
   Frontalization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile. Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful results and preserve texture details in a high-resolution. This paper proposes a High Fidelity Pose Invariant Model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose Adversarial Residual Dictionary Learning (ARDL) to supervise facial texture map recovering with only monocular images. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only boosts the performance of pose-invariant face recognition but also dramatically improves high-resolution frontalization appearances.
C1 [He, Ran] Univ Chinese Acad Sci, CASIA, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.
   Univ Chinese Acad Sci, CASIA, Ctr Res Intelligent Percept & Comp, Beijing 100049, Peoples R China.
   Univ Chinese Acad Sci, CASIA, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100049, Peoples R China.
RP He, R (reprint author), Univ Chinese Acad Sci, CASIA, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.
EM jie.cao@cripac.ia.ac.cn; yibo.hu@cripac.ia.ac.cn;
   hongwen.zhang@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn
FU National Key Research and Development Program of China [2017YFC0821602,
   2016YFB1001000]; National Natural Science Foundation of China [61427811,
   61573360]
FX This work is funded by the National Key Research and Development Program
   of China (Grant No. 2017YFC0821602, 2016YFB1001000) and the National
   Natural Science Foundation of China (Grant No. 61427811, 61573360).
CR Arjovsky Martin, 2017, ICML
   Blanz Volker, 1999, SIGGRAPH
   Booth James, 2014, ICIP
   Chen X., 2016, NEURIPS
   Cole F, 2017, CVPR
   Dana Hang, 2017, CVPR
   Deng Jiankang, 2018, CVPR
   Dovgard Roman, 2004, ECCV
   Ferrari Claudio, 2016, ICPR
   Goodfellow I J, 2014, NEURIPS
   Gross Ralph, 2010, IVC
   Guler Riza Alp, 2018, CVPR
   Guler Riza Alp, 2017, CVPR
   Hassner Tal, 2015, CVPR
   Hassner Tal, 2013, ICCV
   He R., 2017, AAAI
   Hu Yibo, 2018, CVPR
   Huang G.B., 2007, TECHNICAL REPORT
   Huang Huaibo, 2017, ICCV
   Huang R., 2017, ICCV
   Jian  Zhao, 2018, CVPR
   Jian  Zhao, 2018, IJCAI
   Johnson J., 2016, ECCV
   Karras Tero, 2018, ICLR
   Klare B. F., 2015, PUSHING FRONTIERS UN
   Liu  Z., 2015, ICCV
   Mao Xudong, 2017, ICCV
   Parkhi Omkar M, 2015, BMVC
   Paszke Adam, 2017, NEURIPS W
   Paysan Pascal, 2009, AVSS
   Radford A., 2016, ICLR
   Shrivastava Ashish, 2017, CVPR
   Tian Yu, 2018, IJCAI
   Tran Luan, 2017, CVPR
   Tran Luan, 2018, CVPR
   Van Gemert Jan C, 2008, ECCV
   Yin Xi, 2017, ICCV
   Zhao Jian, 2017, NEURIPS
   Zhu J.-Y., 2017, ICCV
   Zhu Xiangyu, 2015, CVPR
   Zhu Z., 2016, CVPR
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302085
DA 2019-06-15
ER

PT S
AU Cao, W
   Wang, D
   Li, J
   Zhou, H
   Li, YT
   Li, L
AF Cao, Wei
   Wang, Dong
   Li, Jian
   Zhou, Hao
   Li, Yitan
   Li, Lei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI BRITS: Bidirectional Recurrent Imputation for Time Series
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Time series are ubiquitous in many classification/regression applications. However, the time series data in real applications may contain many missing values. Hence, given multiple (possibly correlated) time series data, it is important to fill in missing values and at the same time to predict their class labels. Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose a novel method, called BRITS, based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a healthcare dataset, and a localization dataset for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression.
C1 [Cao, Wei; Li, Jian] Tsinghua Univ, Beijing, Peoples R China.
   [Cao, Wei; Zhou, Hao; Li, Yitan; Li, Lei] Bytedance AI Lab, Beijing, Peoples R China.
   [Wang, Dong] Duke Univ, Durham, NC 27706 USA.
RP Cao, W (reprint author), Tsinghua Univ, Beijing, Peoples R China.; Cao, W (reprint author), Bytedance AI Lab, Beijing, Peoples R China.
EM cao-13@tsinghua.org.cn; dong.wang363@duke.edu;
   lijian83@mail.tsinghua.edu.cn; haozhou0806@gmail.com;
   liyitan@bytedance.com; lileilab@bytedance.com
FU National Basic Research Program of China [2015CB358700]; National
   Natural Science Foundation of China [61822203, 61772297, 61632016,
   61761146003]; Microsoft Research Asia
FX Wei Cao and Jian Li are supported in part by the National Basic Research
   Program of China Grant 2015CB358700, the National Natural Science
   Foundation of China Grant 61822203, 61772297, 61632016, 61761146003, and
   a grant from Microsoft Research Asia.
CR Ansley C. F., 1984, TIME SERIES ANAL IRR, P9
   Azur MJ, 2011, INT J METH PSYCH RES, V20, P40, DOI 10.1002/mpr.329
   Basharat A, 2009, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2009.5459429
   Batres-Estrada B., 2015, DEEP LEARNING MULTIV
   Bauer S., 2016, INT C MACH LEARN, P2043
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Berglund M., 2015, ADV NEURAL INFORM PR, p856 
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Brakel P, 2013, J MACH LEARN RES, V14, P2771
   Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9
   Cho K, 2014, ARXIV14061078
   Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301
   Friedman J., 2001, SPRINGER SERIES STAT, V1
   Fung D. S, 2006, METHODS ESTIMATION M
   Harvey AC, 1990, FORECASTING STRUCTUR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ian G, 2016, DEEP LEARNING
   Kaluza B, 2010, LECT NOTES COMPUT SC, V6439, P177, DOI 10.1007/978-3-642-16917-5_18
   Kreindler D. M., 2012, NONLINEAR DYNAMICAL, P135
   Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507
   Lipton Z. C., 2016, MACH LEARN HEALTHC C, P253
   Liu Zitao, 2016, Proc SIAM Int Conf Data Min, V2016, P810
   Moritz S, 2017, R J, V9, P207
   Ozaki T., 1985, HDB STATISTICS, V5, P25
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Rani S., 2012, INT J COMPUTER APPL, V52
   Shi X., 2015, ADV NEURAL INFORM PR, V9199, P802
   Silva I, 2012, COMPUT CARDIOL, V39, P245
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Wang D, 2017, PROC INT CONF DATA, P243, DOI 10.1109/ICDE.2017.83
   Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P501, DOI 10.1145/1148170.1148257
   Yi X., 2016, ST MVL FILLING MISSI
   Yoon J., 2017, MULTIDIRECTIONAL REC
   Yu H.-F., 2016, ADV NEURAL INFORM PR, P847
   Zhang J., 2017, AAAI
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001033
DA 2019-06-15
ER

PT S
AU Carmon, Y
   Duchi, JC
AF Carmon, Yair
   Duchi, John C.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Analysis of Krylov Subspace Solutions of Regularized Nonconvex Quadratic
   Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and have long been used in practice. We prove error bounds of the form 1/t(2) and e(-4t/root kappa), where kappa is a condition number for the problem, and t is the Krylov subspace order (number of Lanczos iterations). We also provide lower bounds showing that our analysis is sharp.
C1 [Carmon, Yair] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Duchi, John C.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Duchi, John C.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Carmon, Y (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM yairc@stanford.edu; jduchi@stanford.edu
FU NSF-CAREER Award [1553086]; Sloan Foundation; Stanford Graduate
   Fellowship
FX We thank the anonymous reviewers for several helpful questions and
   suggestions. Both authors were supported by NSF-CAREER Award 1553086 and
   the Sloan Foundation. YC was partially supported by the Stanford
   Graduate Fellowship.
CR Agarwal N., 2017, P 49 ANN ACM S THEOR
   Allen-Zhu Z., 2017, P 8 INN THEOR COMP S
   Blanchet J., 2016, ARXIV160907428MATHOC
   Carmon Y., 2017, ARXIV171100841MATHOC
   Carmon Y., 2017, P 34 INT C MACH LEAR
   Carmon Y., 2016, ARXIV161200547MATHOC
   Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296
   Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5
   Coakley ES, 2013, APPL COMPUT HARMON A, V34, P379, DOI 10.1016/j.acha.2012.06.003
   Conn A. R., 2000, TRUST REGION METHODS
   Cullum J., 1974, 1974 P IEEE C DEC CO, V13, P505
   Druskin V., 1991, U S S R COMPUTATIONA, V31, P970
   Golub G. H., 1977, MATH SOFTWARE, V3, P361
   Golub G. H., 1989, MATRIX COMPUTATIONS
   Gould NIM, 1999, SIAM J OPTIMIZ, V9, P504, DOI 10.1137/S1052623497322735
   Gould NIM, 2003, ACM T MATH SOFTWARE, V29, P373, DOI 10.1145/962437.962439
   Griewank A., 1981, NA12
   Hazan E, 2016, MATH PROGRAM, V158, P363, DOI 10.1007/s10107-015-0933-y
   Hestenes M. R., 1952, J RES NBS, V49
   Ho-Nguyen N., 2016, ARXIV160303366MATHOC
   Jin C., 2017, ARXIV171110456CSLG
   Kohler J. M., 2017, P 34 INT C MACH LEAR
   KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066
   Lenders F, 2018, OPTIM METHOD SOFTW, V33, P420, DOI 10.1080/10556788.2018.1449842
   Musco Christopher, 2017, ARXIV170807788CSDS
   Nemirovski A., 1994, EFFICIENT METHODS CO
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Regier J, 2017, ADV NEUR IN, V30
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Simchowitz M., 2018, ARXIV180709386CSLG
   TREFETHEN L. N., 1997, NUMERICAL LINEAR ALG
   Tripuraneni N., 2017, ARXIV171102838CSLG
   Tseng P, 2008, ACCELERATED PROXIMAL
   Yao Z., 2018, ARXIV180206925MATHOC
   Zhang LH, 2017, SIAM J OPTIMIZ, V27, P2110, DOI 10.1137/16M1095056
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005030
DA 2019-06-15
ER

PT S
AU Carratino, L
   Rudi, A
   Rosasco, L
AF Carratino, Luigi
   Rudi, Alessandro
   Rosasco, Lorenzo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning with SGD and Random Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID APPROXIMATION
AB Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments.
C1 [Carratino, Luigi; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy.
   [Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.
   [Rosasco, Lorenzo] IIT, LCSL, Genoa, Italy.
   [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA.
RP Carratino, L (reprint author), Univ Genoa, Genoa, Italy.
EM luigi.carratino@dibris.unige.it
FU Center for Brains, Minds and Machines (CBMM); NSF STC [CCF-1231216];
   Italian Institute of Technology; AFOSR [FA9550-17-1-0390,
   BAA-AFRL-AFOSR-2016-0007]; EU [MADS -DLV-777826]; European Research
   Council [SEQUOIA 724063]
FX This material is based upon work supported by the Center for Brains,
   Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the
   Italian Institute of Technology. We gratefully acknowledge the support
   of NVIDIA Corporation for the donation of the Titan Xp GPUs and the
   Tesla k40 GPU used for this research. L. R. acknowledges the support of
   the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007
   (European Office of Aerospace Research and Development), and the EU
   H2020-MSCA-RISE project NoMADS -DLV-777826. A. R. acknowledges the
   support of the European Research Council (grant SEQUOIA 724063).
CR Agarwal A., 2012, ADV NEURAL INFORM PR, P1538
   Alaoui Ahmed, 2015, ADV NEURAL INFORM PR, V28, P775
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Avron H., 2013, NIPS, V26, P2994
   BACH F, 2017, J MACHINE LEARNING R, V18
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Camoriano Raffaello, 2016, P 19 INT C ART INT S, P1403
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Ciliberto Carlo, 2018, ARXIV180602402
   Ciliberto Carlo, 2017, ADV NEURAL INFORM PR, P1986
   Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412
   Cucker F, 2002, B AM MATH SOC, V39, P1
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   De Vito E, 2005, J MACH LEARN RES, V6, P883
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391
   Dieuleveut Aymeric, 2017, J MACHINE LEARNING R, V18, P3520
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Hamid Raffay, 2014, P 31 INT C MACH LEAR, P19
   Le Quoc, 2013, P INT C MACH LEARN, V85
   LIN JH, 2017, J MACHINE LEARNING R, V18
   Lin Junhong, 2017, ARXIV171007797
   Lin Junhong, 2017, ARXIV170700577
   Lin Junhong, 2018, APPL COMPUTATIONAL H
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Osokin Anton, 2017, ADV NEURAL INFORM PR, P302
   Pillaud-Vivien Loucas, 2018, P 31 C LEARN THEOR, V75, P250
   Pillaud-Vivien Loucas, 2018, ADV NEURAL INFORM PR, V31, P8125
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rosasco Lorenzo, 2015, ADV NEURAL INFORM PR, P1630
   Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657
   Rudi  A., 2013, ADV NEURAL INFORM PR, P2067
   Rudi Alessandro, 2018, ADV NEURAL INFORM PR, V31, P5677
   Rudi Alessandro, 2017, ADV NEURAL INFORM PR, P3891
   Rudi Alessandro, 2017, ADV NEURAL INFORM PR, V30, P3215
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089
   Smola A. J., 2000, SPARSE GREEDY MATRIX
   Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart Ingo, 2009, COLT
   Tu Stephen, 2016, ARXIV160205310
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yang Yun, 2015, ARXIV150106195
   Yu Felix X, 2016, ADV NEURAL INFORM PR, P1975
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004072
DA 2019-06-15
ER

PT S
AU Carreira-Perpinan, MA
   Tavallali, P
AF Carreira-Perpinan, Miguel A.
   Tavallali, Pooya
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Alternating Optimization of Decision Trees, with Application to Learning
   Sparse Oblique Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHMS
AB Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (its structure and the parameter values at its nodes), produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.
C1 [Carreira-Perpinan, Miguel A.; Tavallali, Pooya] Univ Calif Merced, Dept EECS, Merced, CA 95343 USA.
RP Carreira-Perpinan, MA (reprint author), Univ Calif Merced, Dept EECS, Merced, CA 95343 USA.
EM mcarreira-perpinan@ucmerced.edu; ptavallali@ucmerced.edu
FU NSF [IIS-1423515]
FX Work funded in part by NSF award IIS-1423515.
CR Auer P., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P21
   Bennett  K., 1994, COMPUTING SCI STAT, P156
   Bennett K. P., 1992, P 4 MIDW ART INT COG, P97
   Bertsimas D, 2017, MACH LEARN, V106, P1039, DOI 10.1007/s10994-017-5633-9
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Carreira-Perpinan M. A., 2017, ARXIV170704319
   Carreira-Perpinan M. A., 2014, ARTIF INTELL, P10
   Carreira-Perpinan  M.A., 2017, ARXIV170701209
   Carreira-Perpinan MA, 2018, PROC CVPR IEEE, P8532, DOI 10.1109/CVPR.2018.00890
   Clment K., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857
   Criminisi A, 2013, ADV COMPUTER VISION
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209
   Guruswami V, 2009, SIAM J COMPUT, V39, P742, DOI 10.1137/070685798
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Hastie  T., 2015, MONOGRAPHS STAT APPL
   Hastie T., 2009, SPRINGER SERIES STAT
   HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011
   Hyafil  L., 1975, INFORM PROCESSING LE, V5, P15
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Kumar Ashish, 2017, 34 INT C MACH LEARN, P1935
   Mathy C., 2015, AAAI C ART INT, P2864
   MEGIDDO N, 1988, DISCRETE COMPUT GEOM, V3, P325, DOI 10.1007/BF02187916
   Murthy S.K., 1994, J ARTIFICIAL INTELLI, V2, P1, DOI DOI 10.1613/JAIR.63
   Norouzi  M., 2015, ADV NEURAL INFORM PR, V28, P1720
   Norouzi  M., 2015, ARXIV150606155
   Quinlan J. R, 1993, C4 5 PROGRAMS MACHIN
   Rokach L., 2007, SERIES MACHINE PERCE, V69
   Schapire RE, 2012, ADAPT COMPUT MACH LE, P1
   Tjortjis C, 2002, LECT NOTES COMPUT SC, V2412, P50
   Tzirakis P, 2017, ADV DATA ANAL CLASSI, V11, P353, DOI 10.1007/s11634-016-0246-x
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301022
DA 2019-06-15
ER

PT S
AU Casale, FP
   Dalca, AV
   Saglietti, L
   Listgarten, J
   Fusi, N
AF Casale, Francesco Paolo
   Dalca, Adrian V.
   Saglietti, Luca
   Listgarten, Jennifer
   Fusi, Nicolo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gaussian Process Prior Variational Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.
C1 [Casale, Francesco Paolo; Saglietti, Luca; Fusi, Nicolo] Microsoft Res New England, Cambridge, MA 02142 USA.
   [Dalca, Adrian V.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Dalca, Adrian V.] HMS, MGH, Martinos Ctr Biomed Imaging, Boston, MA USA.
   [Saglietti, Luca] Italian Inst Genom Med, Turin, Italy.
   [Listgarten, Jennifer] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
RP Casale, FP (reprint author), Microsoft Res New England, Cambridge, MA 02142 USA.
EM frcasale@microsoft.com
FU NSF [0339122]
FX Stimulus images courtesy of Michael J. Tarr, Center for the Neural Basis
   of Cognition and Department of Psychology, Carnegie Mellon University,
   http://www.tarrlab.org.Funding provided by NSF award 0339122.
CR Bauer M, 2016, ADV NEURAL INFORM PR, P1533
   Bonilla E. V., 2007, P 11 INT C ART INT S, P43
   Casale FP, 2017, PLOS GENET, V13, DOI 10.1371/journal.pgen.1006693
   Casale FP, 2015, NAT METHODS, V12, P755, DOI [10.1038/nmeth.3439, 10.1038/NMETH.3439]
   Chen X., 2016, ARXIV161102731
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Dalca AV, 2015, LECT NOTES COMPUT SC, V9351, P519, DOI 10.1007/978-3-319-24574-4_62
   Durrande Nicolas, 2011, ARXIV11116233
   Gal Y., 2014, ADV NEURAL INFORM PR, P3257
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Harville D. A, 1997, MATRIX ALGEBRA STAT, V1
   HENDERSON HV, 1981, SIAM REV, V23, P53, DOI 10.1137/1023004
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   Hoffman Matthew D, 2016, WORKSH ADV APPR BAY
   Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131
   Jiang Z., 2016, ARXIV161105148
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2
   Lonsdale J, 2013, NAT GENET, V45, P580, DOI 10.1038/ng.2653
   Maaloe Lars, 2016, ARXIV160205473
   Nalisnick Eric, 2016, NIPS WORKSH BAY DEEP, V2
   Pandey G, 2017, IEEE IJCNN, P308, DOI 10.1109/IJCNN.2017.7965870
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rakitsch B, 2013, ADV NEURAL INFORM PR, P1466
   Ranganath Rajesh, 2016, INT C MACH LEARN, P324
   Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63
   Rezende D. J, 2014, ARXIV14014082
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Righi G, 2012, VIS COGN, V20, P143, DOI 10.1080/13506285.2012.654624
   Shu Rui, 2016, ECCV WORKSH ACT ANT, V2
   Siddharth N, 2017, ARXIV E PRINTS
   Siddharth N, 2016, ARXIV161107492
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Sohn  K., 2015, ADV NEURAL INFORM PR, P3483
   Stegle Oliver, 2011, ADV NEURAL INFORM PR, P630
   Suzuki Masahiro, 2016, ARXIV161101891
   Titsias M, 2009, ARTIF INTELL, P567
   Tomczak J. M., 2017, ARXIV170507120
   Tran Dustin, 2015, ARXIV151106499
   Vedantam Ramakrishna, 2017, ARXIV1705010762
   Wang W., 2016, ARXIV161003454
   Wilson A., 2013, P 30 INT C MACH LEAR, P1067
   Wilson A. G, 2016, P 19 INT C ART INT S, P370
   Wu Mike, 2018, ARXIV180205335
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004088
DA 2019-06-15
ER

PT S
AU Caterini, AL
   Doucet, A
   Sejdinovic, D
AF Caterini, Anthony L.
   Doucet, Arnaud
   Sejdinovic, Dino
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hamiltonian Variational Auto-Encoder
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Variational Auto-Encoders (VAEs) have become very popular techniques to perform inference and learning in latent variable models: they allow us to leverage the rich representational power of neural networks to obtain flexible approximations of the posterior of latent variables as well as tight evidence lower bounds (ELBOs). Combined with stochastic variational inference, this provides a methodology scaling to large datasets. However, for this methodology to be practically efficient, it is necessary to obtain low-variance unbiased estimators of the ELBO and its gradients with respect to the parameters of interest. While the use of Markov chain Monte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously suggested to achieve this [25, 28], the proposed methods require specifying reverse kernels which have a large impact on performance. Additionally, the resulting unbiased estimator of the ELBO for most MCMC kernels is typically not amenable to the reparameterization trick. We show here how to optimally select reverse kernels in this setting and, by building upon Hamiltonian Importance Sampling (HIS) [19], we obtain a scheme that provides low-variance unbiased estimators of the ELBO and its gradients using the reparameterization trick. This allows us to develop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be re-interpreted as a target-informed normalizing flow [22] which, within our context, only requires a few evaluations of the gradient of the sampled likelihood and trivial Jacobian calculations at each iteration.
C1 [Caterini, Anthony L.; Doucet, Arnaud; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England.
   [Doucet, Arnaud; Sejdinovic, Dino] Alan Turing Inst Data Sci, London, England.
RP Caterini, AL (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM anthony.caterini@stats.ox.ac.uk; doucet@stats.ox.ac.uk;
   dino.sejdinovic@stats.ox.ac.uk
FU UK government
FX Anthony L. Caterini is a Commonwealth Scholar, funded by the UK
   government.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Burda Yuri, 2016, 4 INT C LEARN REPR I
   Crooks GE, 1998, J STAT PHYS, V90, P1481, DOI 10.1023/A:1023208217925
   Cuendet MA, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.120602
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Glasserman Paul, 1991, GRADIENT ESTIMATION, V116
   Heng Jeremy, 2017, ARXIV170808396
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Hoffman Matthew D, 2017, INT C MACH LEARN, P1510
   Jarzynski C, 1997, PHYS REV LETT, V78, P2690, DOI 10.1103/PhysRevLett.78.2690
   Jarzynski C, 2000, J STAT PHYS, V98, P77, DOI 10.1023/A:1018670721277
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2014, 2 INT C LEARN REPR I
   Maddison Chris J, 2017, ADV NEURAL INFORM PR, P6576
   Neal Radford M, 2005, BANFF INT RES STAT B
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Neal RM, 2011, HDB MARKOV CHAIN MON, V2
   Procacci P, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2360273
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   Scholl-Paschinger E, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2227025
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   van den Berg Rianne, 2018, ARXIV180305649
   Wolf Christopher, 2016, ARXIV160908203
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002069
DA 2019-06-15
ER

PT S
AU Chakraborty, R
   Yang, CH
   Zhen, XJ
   Banerjee, M
   Archer, D
   Vaillancourt, D
   Singh, V
   Vemuri, BC
AF Chakraborty, Rudrasis
   Yang, Chun-Hao
   Zhen, Xingjian
   Banerjee, Monami
   Archer, Derek
   Vaillancourt, David
   Singh, Vikas
   Vemuri, Baba C.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Statistical Recurrent Model on the Manifold of Symmetric Positive
   Definite Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID TENSOR-BASED MORPHOMETRY; DTI SEGMENTATION; TRACTOGRAPHY
AB In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed are non-Euclidean in nature. Geometric deep learning corresponds to techniques that generalize deep neural network models to such non-Euclidean spaces. Several recent papers have shown how convolutional neural networks (CNNs) can be extended to learn with graph-based data. In this work, we study the setting where the data (or measurements) are ordered, longitudinal or temporal in nature and live on a Riemannian manifold - this setting is common in a variety of problems in statistical machine learning, vision and medical imaging. We show how recurrent statistical network models can be defined in such spaces. Then, we present an efficient algorithm and conduct a rigorous analysis of its statistical properties. We perform numerical experiments demonstrating competitive performance with state of the art methods but with significantly fewer parameters. We also show applications to a statistical analysis task in brain imaging, a regime where deep neural network models have only been utilized in limited ways.
C1 [Chakraborty, Rudrasis; Yang, Chun-Hao; Banerjee, Monami; Archer, Derek; Vaillancourt, David; Vemuri, Baba C.] Univ Florida, Gainesville, FL 32611 USA.
   [Zhen, Xingjian; Singh, Vikas] Univ Wisconsin, Madison, WI USA.
RP Chakraborty, R (reprint author), Univ Florida, Gainesville, FL 32611 USA.
FU NSF [IIS-1525431, IIS-1724174, R01 NS052318]; NSF CAREER award [1252725,
   R01 EB022883]; UW CPCP [U54 AI117924]
FX This research was funded in part by the NSF grant IIS-1525431 and
   IIS-1724174 to BCV, R01 NS052318 to DV and NSF CAREER award 1252725 and
   R01 EB022883 to VS. XZ and VS were also supported by UW CPCP (U54
   AI117924).
CR Afsari B, 2013, IEEE DECIS CONTR P, P1162, DOI 10.1109/CDC.2013.6760039
   Afsari B, 2012, PROC CVPR IEEE, P2208, DOI 10.1109/CVPR.2012.6247929
   Afsari B, 2011, P AM MATH SOC, V139, P655, DOI 10.1090/S0002-9939-2010-10541-5
   Aganj I, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P1398, DOI 10.1109/ISBI.2009.5193327
   Archer Derek B, 2017, CEREB CORTEX, P1
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1
   Behrens TEJ, 2007, NEUROIMAGE, V34, P144, DOI 10.1016/j.neuroimage.2006.09.018
   Bissacco Alessandro, 2001, COMP VIS PATT REC 20, V2, pII
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chakraborty Rudrasis, 2018, ARXIV180505487
   Cheng G, 2015, IEEE T MED IMAGING, V34, P298, DOI 10.1109/TMI.2014.2355138
   Cheng G, 2012, LECT NOTES COMPUT SC, V7578, P390, DOI 10.1007/978-3-642-33786-4_29
   Cherian A, 2011, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2011.6126523
   Cho K., 2014, ARXIV14091259
   Cohen T. S., 2018, ARXIV180110130
   Cohen Taco S, 2016, ARXIV161208498
   Dominici F, 2002, AM J EPIDEMIOL, V156, P193, DOI 10.1093/aje/kwf062
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Frechet M, 1948, ANN I H POINCARE, V10, P215
   Hall Brian, 2015, LIE GROUPS LIE ALGEB, V222
   Helgason Sigurdur, 1962, DIFFERENTIAL GEOMETR, V12
   Henaff Mikael, 2016, ARXIV160206662
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hua X, 2008, NEUROIMAGE, V43, P458, DOI 10.1016/j.neuroimage.2008.07.013
   Huang Z., 2016, ARXIV161105742
   Huang Zhiwu, 2017, ASS ADV ARTIF INTELL, V2, P6
   Jian B, 2007, NEUROIMAGE, V37, P164, DOI 10.1016/j.neuroimage.2007.03.074
   Jing  L., 2017, ARXIV170602761
   KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81
   Kim HJ, 2014, PROC CVPR IEEE, P2705, DOI 10.1109/CVPR.2014.352
   Kim Hyunwoo J, 2017, P IEEE C COMP VIS PA
   Kondor R., 2018, ARXIV180203690
   Koutnik J., 2014, ARXIV14023511
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebanon Guy, 2015, RIEMANNIAN GEOMETRY
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lenglet C, 2006, IEEE T MED IMAGING, V25, P685, DOI 10.1109/TMI.2006.873299
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Mammasis K, 2010, EURASIP J WIREL COMM, DOI 10.1155/2010/307265
   Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oliva Junier B, 2017, ARXIV170300381
   PARK FC, 1995, J MECH DESIGN, V117, P36, DOI 10.1115/1.2826114
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Pujol S, 2015, J NEUROIMAGING, V25, P875, DOI 10.1111/jon.12283
   Quirk Chris, 2005, P 43 ANN M ASS COMP, P271, DOI DOI 10.3115/1219840.1219874
   Salehian H, 2013, IEEE I CONF COMP VIS, P1793, DOI 10.1109/ICCV.2013.225
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Sharma S., 2015, ARXIV151104119
   Sra Suvrit, 2011, TECHNICAL REPORT
   Srivastava A., 2007, IEEE C COMP VIS PATT, P1
   Srivastava N., 2015, INT C MACH LEARN, P843
   Straub J., 2015, P 18 INT C ART INT S, P930
   Triacca U, 2016, ECONOMETRICS, V4, DOI 10.3390/econometrics4030032
   Tsay R. S., 2005, ANAL FINANCIAL TIME, V543
   Turaga Pavan, 2008, P IEEE C COMP VIS PA, P1
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Wang ZZ, 2005, IEEE T MED IMAGING, V24, P1267, DOI 10.1109/TMI.2005.854516
   Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419
   Yang Yinchong, 2017, ARXIV170701786
   Yu K., 2017, ARXIV170306817
   Zacur E, 2014, J MATH IMAGING VIS, V50, P18, DOI 10.1007/s10851-013-0479-7
NR 68
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003044
DA 2019-06-15
ER

PT S
AU Chan, J
   Perrone, V
   Spence, JP
   Jenkins, PA
   Mathieson, S
   Song, YS
AF Chan, Jeffrey
   Perrone, Valerio
   Spence, Jeffrey P.
   Jenkins, Paul A.
   Mathieson, Sara
   Song, Yun S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Likelihood-Free Inference Framework for Population Genetic Data using
   Exchangeable Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID APPROXIMATE BAYESIAN COMPUTATION; STATISTICS
AB An explosion of high-throughput DNA sequencing in the past decade has led to a surge of interest in population-scale inference with whole-genome data. Recent work in population genetics has centered on designing inference methods for relatively simple model classes, and few scalable general-purpose inference techniques exist for more realistic, complex models. To achieve this, two inferential challenges need to be addressed: (1) population data are exchangeable, calling for methods that efficiently exploit the symmetries of the data, and (2) computing likelihoods is intractable as it requires integrating over a set of correlated, extremely high-dimensional latent variables. These challenges are traditionally tackled by likelihood-free methods that use scientific simulators to generate datasets and reduce them to hand-designed, permutation-invariant summary statistics, often leading to inaccurate inference. In this work, we develop an exchangeable neural network that performs summary statistic-free, likelihood-free inference. Our framework can be applied in a black-box fashion across a variety of simulation-based tasks, both within and outside biology. We demonstrate the power of our approach on the recombination hotspot testing problem, outperforming the state-of-the-art.
C1 [Chan, Jeffrey; Spence, Jeffrey P.; Song, Yun S.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Perrone, Valerio; Jenkins, Paul A.] Univ Warwick, Warwick, England.
   [Mathieson, Sara] Swarthmore Coll, Swarthmore, PA 19081 USA.
RP Chan, J (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM chanjed@berkeley.edu; v.perrone@warwick.ac.uk;
   spence.jeffrey@berkeley.edu; p.jenkins@warwick.ac.uk;
   smathie1@swarthmore.edu; yss@berkeley.edu
FU NSF Graduate Research Fellowship; EPSRC [EP/L016710/1, EP/L018497/1];
   NIH [R01-GM094402]; Packard Fellowship for Science and Engineering;
   Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231];
   NVIDIA Corporation
FX We thank Ben Graham for helpful discussions and Yuval Simons for his
   suggestion to use the decile. This research is supported in part by an
   NSF Graduate Research Fellowship (JC); EPSRC grants EP/L016710/1 (VP)
   and EP/L018497/1 (PJ); an NIH grant R01-GM094402 (JC, JPS, SM, and YSS);
   and a Packard Fellowship for Science and Engineering (YSS). YSS is a
   Chan Zuckerberg Biohub investigator. We gratefully acknowledge the
   support of NVIDIA Corporation with the donation of the Titan X Pascal
   GPU used for this research. This research also used resources of the
   National Energy Research Scientific Computing Center, a DOE Office of
   Science User Facility supported by the Office of Science of the U.S.
   Department of Energy under Contract No. DE-AC02-05CH11231.
CR Auton A., 2014, ARXIV14034264
   Beaumont MA, 2002, GENETICS, V162, P2025
   Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0
   Boitard S, 2016, PLOS GENET, V12, DOI 10.1371/journal.pgen.1005877
   Brutzkus Alon, 2017, ARXIV170207966
   Fearnhead P, 2006, BIOINFORMATICS, V22, P3061, DOI 10.1093/bioinformatics/btl540
   Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x
   Flagel Lex, 2018, BIORXIV
   Gibbs RA, 2003, NATURE, V426, P789, DOI 10.1038/nature02168
   Guo C, 2017, ARXIV170604599
   Guttenberg N., 2016, ARXIV161204530
   Hey J, 2004, PLOS BIOL, V2, P730, DOI 10.1371/journal.pbio.0020190
   Jiang B., 2015, ARXIV151002175
   Jin Chi, 2018, ARXIV180309357
   Kamm JA, 2016, GENETICS, V203, P1381, DOI 10.1534/genetics.115.184820
   Kelleher J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004842
   Kingma D. P., 2014, ARXIV14126980
   Kingman JFC, 1982, STOCHASTIC PROCESS A, V13, P235, DOI DOI 10.1016/0304-4149(82)90011-4
   Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6402
   Li J, 2006, AM J HUM GENET, V79, P628, DOI 10.1086/508066
   Papamakarios G., 2016, ARXIV160506376
   Pavlidis P, 2010, GENETICS, V185, P907, DOI 10.1534/genetics.110.116459
   Petes TD, 2001, NAT REV GENET, V2, P360, DOI 10.1038/35072078
   Pritchard JK, 1999, MOL BIOL EVOL, V16, P1791, DOI 10.1093/oxfordjournals.molbev.a026091
   Ravanbakhsh  S., 2016, ARXIV161104500
   Schrider DR, 2015, GENOME BIOL EVOL, V7, P3511, DOI 10.1093/gbe/evv228
   Sheehan S, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004845
   Shivaswamy PK, 2006, P 23 INT C MACH LEAR, P817
   Sousa VC, 2009, GENETICS, V181, P1507, DOI 10.1534/genetics.108.098129
   Wall J. D., 2016, G3 GENES GENOMES GEN
   Wang Y, 2009, P NATL ACAD SCI USA, V106, P6215, DOI 10.1073/pnas.0900418106
   Wegmann D, 2009, GENETICS, V182, P1207, DOI 10.1534/genetics.109.102509
   Zaheer M., 2017, NEURAL INFORM PROCES
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003018
DA 2019-06-15
ER

PT S
AU Chang, JL
   Gu, J
   Wang, LF
   Meng, GF
   Xiang, SM
   Pan, CH
AF Chang, Jianlong
   Gu, Jie
   Wang, Lingfeng
   Meng, Gaofeng
   Xiang, Shiming
   Pan, Chunhong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Structure-Aware Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Convolutional neural networks (CNNs) are inherently subject to invariable filters that can only aggregate local inputs with the same topological structures. It causes that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g., images), not ones with non-Euclidean or graph structures (e.g., traffic networks). To broaden the reach of CNNs, we develop structure-aware convolution to eliminate the invariance, yielding a unified mechanism of dealing with both Euclidean and non-Euclidean structured data. Technically, filters in the structure-aware convolution are generalized to univariate functions, which are capable of aggregating local inputs with diverse topological structures. Since infinite parameters are required to determine a univariate function, we parameterize these filters with numbered learnable parameters in the context of the function approximation theory. By replacing the classical convolution in CNNs with the structure-aware convolution, Structure-Aware Convolutional Neural Networks (SACNNs) are readily established. Extensive experiments on eleven datasets strongly evidence that SACNNs outperform current models on various machine learning tasks, including image classification and clustering, text categorization, skeleton-based action recognition, molecular activity detection, and taxi flow prediction.
C1 [Chang, Jianlong; Gu, Jie; Wang, Lingfeng; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, NLPR, Inst Automat, Beijing, Peoples R China.
   [Chang, Jianlong; Gu, Jie; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
RP Chang, JL (reprint author), Chinese Acad Sci, NLPR, Inst Automat, Beijing, Peoples R China.; Chang, JL (reprint author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM jianlong.chang@nlpr.ia.ac.cn; jie.gu@nlpr.ia.ac.cn;
   lfwang@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn;
   chpan@nlpr.ia.ac.cn
FU National Natural Science Foundation of China [91646207, 61773377,
   61573352]; Beijing Natural Science Foundation [L172053]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 91646207, 61773377 and 61573352, and the Beijing
   Natural Science Foundation under Grants L172053. We would like to thank
   Lele Yu, Bin Fan, Cheng Da, Tingzhao Yu, Xue Ye, Hongfei Xiao, and Qi
   Zhang for their invaluable contributions in shaping the early stage of
   this work.
CR Atwood J., 2016, ADV NEURAL INFORM PR, V29, P1993
   Boscaini D., 2016, ADV NEURAL INFORM PR, P3189
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J., 2013, ABS13126203 CORR
   Chang JL, 2018, PATTERN RECOGN, V77, P438, DOI 10.1016/j.patcog.2017.10.022
   Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Coates A, 2011, P 14 INT C ART INT S, P215
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   DEFFERRARD M., 2016, ADV NEURAL INFORM PR, P3837
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Gilmer J, 2017, P 34 INT C MACH LEAR, V34, P1263
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Hammond D. K., 2009, APPL COMPUTATIONAL H, V30, P129
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henaff  M., 2015, ABS150605163 CORR
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200
   Kaggle, 2012, MERCK MOL ACT CHALL
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kipf T. N., 2016, ARXIV160902907
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lang K., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P331
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li  Ruoyu, 2018, ABS180103226 CORR
   Li  Y., 2015, ABS151105493 CORR
   Liao  Renjie, 2018, ABS180306272 CORR
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruderman  Avraham, 2018, ABS180404438 CORR
   Schlichtkrull M. S., 2017, ABS170306103 CORR
   Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Velickovic  Petar, 2017, ABS171010903 CORR
   Verma  Nitika, 2017, ABS170605206 CORR
   Yu F., 2015, ABS151107122 CORR
   Zhang Q, 2018, INT C PATT RECOG, P1018, DOI 10.1109/ICPR.2018.8545106
NR 42
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300002
DA 2019-06-15
ER

PT S
AU Chang, S
   Yang, J
   Choi, J
   Kwak, N
AF Chang, Simyung
   Yang, John
   Choi, Jaeseok
   Kwak, Nojun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Genetic-Gated Networks for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance.
C1 [Chang, Simyung] Seoul Natl Univ, Samsung Elect, Seoul, South Korea.
   [Yang, John; Choi, Jaeseok; Kwak, Nojun] Seoul Natl Univ, Seoul, South Korea.
RP Chang, S (reprint author), Seoul Natl Univ, Samsung Elect, Seoul, South Korea.
EM timelighter@snu.ac.kr; yjohn@snu.ac.kr; jaeseok.choi@snu.ac.kr;
   nojunk@snu.ac.kr
FU Next-Generation Information Computing Development Program through the
   National Research Foundation of Korea (NRF) [2017M3C4A7077582]
FX This work was supported by Next-Generation Information Computing
   Development Program through the National Research Foundation of Korea
   (NRF) (2017M3C4A7077582).
CR Brockman G, 2016, ARXIV160601540
   Fortunato M., 2017, ARXIV170610295
   Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713
   HOLLAND JH, 1992, SCI AM, V267, P66, DOI 10.1038/scientificamerican0792-66
   Lillicrap T P, 2015, ARXIV150902971
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Osband I., 2017, ARXIV170307608
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Plappert M., 2017, ARXIV170601905
   Ruckstiess T, 2008, LECT NOTES ARTIF INT, V5212, P234, DOI 10.1007/978-3-540-87481-2_16
   Ruffio E, 2011, TUTORIAL 2 ZERO ORDE
   Salimans T., 2017, ARXIV170303864
   Schulman  J., 2017, ARXIV170706347
   Silver  D., 2014, ICML, P387
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Such F. P., 2017, ARXIV171206567
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Wang Z., 2015, ARXIV151106581
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu, 2017, ADV NEURAL INFORM PR, P5285
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301071
DA 2019-06-15
ER

PT S
AU Chen, BH
   Deng, WH
   Shen, HF
AF Chen, Binghui
   Deng, Weihong
   Shen, Haifeng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Virtual Class Enhanced Discriminative Embedding Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.
C1 [Chen, Binghui; Deng, Weihong] Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
   [Shen, Haifeng] Didi Chuxing, AI Labs, Beijing 100193, Peoples R China.
RP Chen, BH (reprint author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.
EM chenbinghui@bupt.edu.cn; whdeng@bupt.edu.cn; shenhaifeng@didiglobal.com
FU National Natural Science Foundation of China [61573068, 61871052];
   Beijing Nova Program [Z161100004916088]; DiDi GAIA Research
   Collaboration Initiative
FX This work was partially supported by the National Natural Science
   Foundation of China under Grant Nos. 61573068 and 61871052, Beijing Nova
   Program under Grant No. Z161100004916088, and sponsored by DiDi GAIA
   Research Collaboration Initiative.
CR Branson S., 2014, ARXIV14062952
   Chen B., 2018, ARXIV180600974
   Chen B., 2017, IEEE C COMP VIS PATT
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Chrabaszcz Patryk, 2017, ARXIV170708819
   Deng WH, 2017, PATTERN RECOGN, V66, P63, DOI 10.1016/j.patcog.2016.11.023
   Goodfellow I.J., 2013, ARXIV13024389
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu J., 2017, ARXIV170901507
   Huang G, 2016, ARXIV160806993
   Huang G.B., 2007, TECHNICAL REPORT
   Jeon Y., 2017, ARXIV170309076
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lee C. Y., 2015, ARTIF INTELL, P562
   Lee C.-Y., 2016, INT C ART INT STAT
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu W., 2017, ARXIV170408063
   Liu W., 2016, P 33 INT C MACH LEAR, P507
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Parkhi O M, 2015, P BR MACH VIS, P6
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sohn K., 2016, ADV NEURAL INFORM PR, P1849
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang F., 2017, ARXIV170406369
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Welinder P., 2010, CALTECH UCSD BIRDS 2
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu X., 2015, ARXIV151102683
   Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514
   Zagoruyko S, 2016, ARXIV160507146
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301089
DA 2019-06-15
ER

PT S
AU Chen, IY
   Johansson, FD
   Sontag, D
AF Chen, Irene Y.
   Johansson, Fredrik D.
   Sontag, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Why Is My Classifier Discriminatory?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.
C1 [Chen, Irene Y.; Johansson, Fredrik D.; Sontag, David] MIT, Cambridge, MA 02139 USA.
RP Chen, IY (reprint author), MIT, Cambridge, MA 02139 USA.
EM iychen@mit.edu; fredrikj@mit.edu; dsontag@csail.mit.edu
FU Office of Naval Research Award [N00014-17-1-2791]; NSF CAREER award
   [1350965]
FX The authors would like to thank Yoni Halpern and Hunter Lang for helpful
   comments, and Zeshan Hussain for clinical guidance. This work was
   partially supported by Office of Naval Research Award No.
   N00014-17-1-2791 and NSF CAREER award #1350965.
CR AMARI SI, 1993, NEURAL NETWORKS, V6, P161, DOI 10.1016/0893-6080(93)90013-M
   Angwin J, 2016, PROPUBLICA
   [Anonymous], ACTES RECHERCHE SCI
   Antos A, 1999, IEEE T PATTERN ANAL, V21, P643, DOI 10.1109/34.777375
   Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31
   Bechavod Yahav, 2017, ARXIV170700044
   Bhattacharyya A., 1943, Bulletin of the Calcutta Mathematical Society, V35, P99
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Brier GW., 1950, MONTHLY WEATHER REVI, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
   Buolamwini J, 2018, C FAIRN ACC TRANSP, V81, P77
   Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x
   Calmon Flavio, 2017, ADV NEURAL INFORM PR, P3995
   Chouldechova Alexandra, 2017, ARXIV170300056
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Devijver P.A., 1982, PATTERN RECOGNITION
   Domhan T, 2015, 24 INT JOINT C ART I
   Domingos P, 2000, P 17 INT C MACH LEAR, P231
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Efron B., 1992, BREAKTHROUGHS STAT, P569, DOI DOI 10.1007/978-1-4612-4380-9_41
   Ensign Danielle, 2017, CORR
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Fish B, 2016, P 2016 SIAM INT C DA, P144
   Fisher R, 1925, STAT METHODS RES WOR
   Friedler Sorelle A, 2018, ARXIV180204422
   FUKUNAGA K, 1987, IEEE T PATTERN ANAL, V9, P634, DOI 10.1109/TPAMI.1987.4767958
   Ghassemi Marzyeh, 2014, KDD, V2014, P75
   Gnanesh, 2017, GOODREADS BOOK REV
   Hajian S, 2013, IEEE T KNOWL DATA EN, V25, P1445, DOI 10.1109/TKDE.2012.72
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Hebert-Johnson Ursula, 2017, ARXIV171108513
   Hestness J., 2017, ARXIV171200409
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Kamiran Faisal, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P869, DOI 10.1109/ICDM.2010.50
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Kleinberg Jon, 2016, ARXIV160905807
   Koepke Hoyt, 2012, ARXIV12064680
   Kusner M. J., 2017, ADV NEURAL INFORM PR, P4069
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu Lydia T, 2018, ARXIV180304383
   Mahalanobis P. C., 1936, GEN DISTANCE STAT
   Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389, DOI DOI 10.1145/2110363.2110408
   Mukherjee S, 2003, J COMPUT BIOL, V10, P119, DOI 10.1089/106652703321825928
   Pleiss Geoff, 2017, ADV NEURAL INFORM PR, P5684
   Quionero-Candela Joaquin, 2009, DATASET SHIFT MACHIN
   Ruggieri S, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754432
   TUKEY JW, 1949, BIOMETRICS, V5, P99, DOI 10.2307/3001913
   Tumer K., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P695, DOI 10.1109/ICPR.1996.546912
   Woodworth Blake, 2017, C LEARN THEOR
   Zafar Muhammad Bilal, 2017, ARXIV150705259
   Zemel R., 2013, JMLR P, P325
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303053
DA 2019-06-15
ER

PT S
AU Chen, JF
   Zhu, J
   Teh, YW
   Zhang, T
AF Chen, Jianfei
   Zhu, Jun
   Teh, Yee Whye
   Zhang, Tong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Stochastic Expectation Maximization with Variance Reduction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EM ALGORITHM
AB Expectation-Maximization (EM) is a popular tool for learning latent variable models, but the vanilla batch EM does not scale to large data sets because the whole data set is needed at every E-step. Stochastic Expectation Maximization (sEM) reduces the cost of E-step by stochastic approximation. However, sEM has a slower asymptotic convergence rate than batch EM, and requires a decreasing sequence of step sizes, which is difficult to tune. In this paper, we propose a variance reduced stochastic EM (sEM-vr) algorithm inspired by variance reduced stochastic gradient descent algorithms. We show that sEM-vr has the same exponential asymptotic convergence rate as batch EM. Moreover, sEM-vr only requires a constant step size to achieve this rate, which alleviates the burden of parameter tuning. We compare sEM-vr with batch EM, sEM and other algorithms on Gaussian mixture models and probabilistic latent semantic analysis, and sEM-vr converges significantly faster than these baselines.
C1 [Chen, Jianfei; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, BNRist Ctr,Inst AI,THBI Lab, Beijing 100084, Peoples R China.
   [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England.
   [Zhang, Tong] Tencent AI Lab, Nanjing, Jiangsu, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, BNRist Ctr,Inst AI,THBI Lab, Beijing 100084, Peoples R China.
EM chenjian14@mails.tsinghua.edu.cn; dcszj@.tsinghua.edu.cn;
   y.w.teh@stats.ox.ac.uk; tongzhang@tongzhang-ml.org
FU National Key Research and Development Program of China [2017YFA0700904];
   NSFC [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp.
   Stan [2016ZXFB00001]; Tsinghua Tiangong Institute for Intelligent
   Computing; NVIDIA NVAIL Program; Siemens; European Research Council
   under the European Union [617071]; Tencent AI Lab through the
   Oxford-Tencent Collaboration on Large Scale Machine Learning
FX We thank Chris Maddison, Adam Foster, and Jin Xu for proofreading. J.C.
   and J.Z. were supported by the National Key Research and Development
   Program of China (No. 2017YFA0700904), NSFC projects (Nos. 61620106010,
   61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No.
   2016ZXFB00001), Tsinghua Tiangong Institute for Intelligent Computing,
   the NVIDIA NVAIL Program and a Project from Siemens. YWT was supported
   by funding from the European Research Council under the European Union's
   Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no.
   617071, and from Tencent AI Lab through the Oxford-Tencent Collaboration
   on Large Scale Machine Learning.
CR Asuncion A., 2007, UCI MACHINE LEARNING
   Asuncion Arthur, 2009, P 25 C UNC ART INT, P27
   Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cappe O, 2011, J COMPUT GRAPH STAT, V20, P728, DOI 10.1198/jcgs.2011.09109
   Cappe O, 2009, J ROY STAT SOC B, V71, P593, DOI 10.1111/j.1467-9868.2009.00698.x
   Chatterji N. S., 2018, ARXIV180205431
   Chen Changyou, 2017, ARXIV170901180
   Chen JF, 2016, PROC VLDB ENDOW, V9, P744, DOI 10.14778/2977797.2977801
   Chen Jianshu, 2015, ADV NEURAL INFORM PR, V28, P1765
   Defazio Aaron, 2014, ADV NEURAL INFORM PR, P1646
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154
   Dupuy Christophe, 2017, J MACHINE LEARNING R, V18, P4581
   Durbin R., 1998, BIOL SEQUENCE ANAL P
   Foulds J. R., 2013, P 19 ACM SIGKDD INT, P446, DOI DOI 10.1145/2487575.2487697
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Ishiguro K, 2017, J MACH LEARN RES, V18
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Lei Lihua, 2017, ARTIF INTELL, P148
   Liang P., 2009, P HUM LANG TECHN 200, P611, DOI DOI 10.3115/1620754.1620843
   Mandt S., 2014, ADV NEURAL INFORM PR, P2438
   McLachlan G., 2004, FINITE MIXTURE MODEL
   Mimno David, 2012, ARXIV12066425
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Sato Issei, 2012, ICML
   Spearman C, 1950, HUMAN ABILITY
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   TITTERINGTON DM, 1984, J ROY STAT SOC B MET, V46, P257
   Wallach Hanna M., 2009, P 26 ANN INT C MACH, P1105, DOI DOI 10.1145/1553374.1553515
   Wang Z., 2014, ARXIV14128729
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Zaheer Manzil, 2016, J MACH LEARN RES, P966
   Zhang A., 2013, P 22 INT C WORLD WID, P1489
   Zhu Rongda, 2017, INT C MACH LEARN, P4180
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002051
DA 2019-06-15
ER

PT S
AU Chen, JC
   Zhang, Q
   Zhou, Y
AF Chen, Jiecao
   Zhang, Qin
   Zhou, Yuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Tight Bounds for Collaborative PAC Learning via Multiplicative Weights
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the collaborative PAC learning problem recently proposed in Blum et al. [3], in which we have k players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead). We obtain a collaborative learning algorithm with overhead O(ln k), improving the one with overhead O(ln(2) k) in [3]. We also show that an Omega(ln k) overhead is inevitable when k is polynomial bounded by the VC dimension of the hypothesis class. Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum et al. [3] on real-world datasets.
C1 [Chen, Jiecao; Zhang, Qin; Zhou, Yuan] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
   [Zhou, Yuan] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL USA.
RP Chen, JC (reprint author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
EM jiecchen@iu.edu; qzhangcs@indiana.edu; yuanz@illinois.edu
FU NSF [CCF-1525024, CCF-1844234, IIS-1633215]
FX Jiecao Chen and Qin Zhang are supported in part by NSF CCF-1525024,
   CCF-1844234 and IIS-1633215. Part of the work was done when Yuan Zhou
   was visiting the Shanghai University of Finance and Economics.
CR Balcan M. - F., 2013, NIPS, P1995
   Balcan M. -F., 2012, COLT
   Balcan M. -F., 2014, ADV NEURAL INFORM PR, P3113
   Blum Avrim, 2017, NIPS, P2389
   Bock R., 2003, INTERNAL NOTE CERN
   Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016
   Daume Hal  III, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P154, DOI 10.1007/978-3-642-34106-9_15
   EHRENFEUCHT A, 1989, INFORM COMPUT, V82, P247, DOI 10.1016/0890-5401(89)90002-3
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   FREY PW, 1991, MACH LEARN, V6, P161, DOI 10.1023/A:1022606404104
   Guha S, 2017, PROCEEDINGS OF THE 29TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'17), P143, DOI 10.1145/3087556.3087568
   Hanneke S., 2016, J MACHINE LEARNING R, V17, P1319
   Mansour Y, 2008, P 22 ANN C NEUR INF, P1041
   Nguyen H. L., 2018, ARXIV180508356
   Phillips J. M., 2012, AISTATS, P282
   Wang J., 2016, AISTATS, P751
NR 16
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303058
DA 2019-06-15
ER

PT S
AU Chen, JC
   Azer, ES
   Zhang, Q
AF Chen, Jiecao
   Azer, Erfan Sadeqi
   Zhang, Qin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Practical Algorithm for Distributed Clustering and Outlier Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID K-MEANS
AB We study the classic k - means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers. We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively. To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.
C1 [Chen, Jiecao; Azer, Erfan Sadeqi; Zhang, Qin] Indiana Univ, Bloomington, IN 47405 USA.
RP Chen, JC (reprint author), Indiana Univ, Bloomington, IN 47405 USA.
EM jiecchen@indiana.edu; esadeqia@indiana.edu; qzhangcs@indiana.edu
FU NSF [CCF-1525024, CCF-1844234, IIS-1633215]
FX Jiecao Chen, Erfan Sadeqi Azer and Qin Zhang are supported in part by
   NSF CCF-1525024, NSF CCF-1844234 and IIS-1633215.
CR Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915
   Balcan M. - F., 2013, NIPS, P1995
   Balcan M. -F., 2014, ADV NEURAL INFORM PR, P3113
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Charikar M, 2001, SIAM PROC S, P642
   Chawla Sanjay, 2013, P 13 SIAM INT C DAT, P189, DOI DOI 10.1137/1.9781611972832.21
   Chen Jiecao, 2016, 29 ADV NEURAL INFORM, P3720
   Chen K, 2009, SIAM J COMPUT, V39, P923, DOI 10.1137/070699007
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Diakonikolas I., 2017, ADV NEURAL INFORM PR, P6394
   Ene A., 2011, P 17 ACM SIGKDD INT, P681, DOI DOI 10.1145/2020408.2020515
   Feldman D., 2012, P 23 ANN ACM SIAM S, P1343
   Guha S, 2003, IEEE T KNOWL DATA EN, V15, P515, DOI 10.1109/TKDE.2003.1198387
   GUHA S, 2017, SPAA, P143, DOI DOI 10.1145/3087556.3087568
   Gupta S, 2017, PROC VLDB ENDOW, V10, P757, DOI 10.14778/3067421.3067425
   Li S., 2018, ARXIV181007852
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mettu R. R., 2002, P 18 C UNC ART INT, P344
   Sanderson C, 2010, ARMADILLO OPEN SOURC
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302027
DA 2019-06-15
ER

PT S
AU Chen, LM
   Zhang, GX
   Zhou, HN
AF Chen, Laming
   Zhang, Guoxin
   Zhou, Hanning
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast Greedy MAP Inference for Determinantal Point Process to Improve
   Recommendation Diversity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The determinantal point process (DPP) is an elegant probabilistic model of repulsion with applications in various machine learning tasks including summarization and search. However, the maximum a posteriori (MAP) inference for DPP which plays an important role in many applications is NP-hard, and even the popular greedy algorithm can still be too computationally expensive to be used in large-scale real-time scenarios. To overcome the computational challenge, in this paper, we propose a novel algorithm to greatly accelerate the greedy MAP inference for DPP. In addition, our algorithm also adapts to scenarios where the repulsion is only required among nearby few items in the result sequence. We apply the proposed algorithm to generate relevant and diverse recommendations. Experimental results show that our proposed algorithm is significantly faster than state-of-the-art competitors, and provides a better relevance-diversity trade-off on several public datasets, which is also confirmed in an online A/B test.
C1 [Chen, Laming; Zhou, Hanning] Hulu LLC, Beijing, Peoples R China.
   [Zhang, Guoxin] Kwai Inc, Beijing, Peoples R China.
RP Chen, LM (reprint author), Hulu LLC, Beijing, Peoples R China.
EM laming.chen@hulu.com; zhangguoxin@kuaishou.com; ericzhouh@gmail.com
CR Adomavicius G., 2011, P WORKSH NOV DIV REC, P3
   Agrawal R., 2009, P 2 ACM INT C WEB SE, P5, DOI DOI 10.1145/1498759.1498766
   Ahmed A., 2012, WSDM, P333
   Ashkan A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1742
   Aytekin T, 2014, J INTELL INF SYST, V42, P1, DOI 10.1007/s10844-013-0252-9
   Bertin-Mahieux T., 2011, P INT SOC MUS INF RE, V2, P10
   Borodin A., 2012, P 31 S PRINC DAT SYS, P155, DOI DOI 10.1145/2213556.2213580
   Bradley K., 2001, P 12 IR C ART INT CO, P85
   Buchbinder N, 2015, SIAM J COMPUT, V44, P1384, DOI 10.1137/130929205
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Chandar P., 2013, P 36 INT ACM SIGIR C, P413
   Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018
   Gartrell M., 2017, P 31 AAAI C ART INT, P1912
   Gartrell Mike, 2016, P 10 ACM C REC SYST, P349
   Gillenwater J., 2014, APPROXIMATE INFERENC
   Gillenwater J. A., 2014, P ADV NEUR INF PROC, P3149
   Gillenwater Jennifer, 2012, P ADV NEUR INF PROC, P2735
   Gong B., 2014, P ADV NEUR INF PROC, P2069
   Han I., 2017, P ICML 2017, P1384
   He J., 2012, P ADV NEUR INF PROC, P1142
   Hurley N, 2011, ACM T INTERNET TECHN, V10, DOI 10.1145/1944339.1944341
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Karypis G., 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P247, DOI 10.1145/502585.502627
   KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kulesza A., 2011, P INT C MACH LEARN, P1193
   Kulesza A., 2010, P ADV NEUR INF PROC, P1171
   Kulesza A., 2011, P 27 C UNC ART INT, P419
   Kulesza A, 2012, FOUND TRENDS MACH LE, V5, P123, DOI 10.1561/2200000044
   Lee S. C., 2017, P INT C WORLD WID WE, P809
   Li C., 2016, ICML, P1766
   MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855
   Mariet Zelda, 2015, P ICML 2015, P2389
   MEHTA ML, 1960, NUCL PHYS, V18, P420, DOI 10.1016/0029-5582(60)90414-4
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Niemann K, 2013, P 19 ACM SIGKDD INT, P955, DOI DOI 10.1145/2487575.2487656
   Parambath S. A. P., 2016, P 10 ACM C REC SYST, P15, DOI DOI 10.1145/2959100.2959149
   Qin L., 2013, IJCAI, P2698
   Rubi Boim, 2011, P 20 ACM INT C INF K, P739, DOI DOI 10.1145/2063576.2063684
   Schott J. R., 1999, J AM STAT ASSOC, V94, P1388
   Steck H., 2011, P 5 ACM C REC SYST, P125, DOI DOI 10.1145/2043932.2043957
   Su R., 2013, P 7 ACM C REC SYST, P415
   Teo C. H, 2016, P 10 ACM C REC SYST, P35
   Vargas  S., 2014, P 8 ACM C REC SYST R, P209, DOI DOI 10.1145/2645710.2645743
   Voorhees E., 1999, P 8 TEXT RETR C TREC, P77
   Wu L, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700496
   Xia L, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P535, DOI 10.1145/3077136.3080775
   Yao J.-g., 2016, P AAAI 2016, P3080
   Yu C., 2009, EDBT, P368
   Zhang M, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P123
   Ziegler C.-N., 2005, P 14 INT C WORLD WID, P22, DOI DOI 10.1145/1060745.1060754
NR 52
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000015
DA 2019-06-15
ER

PT S
AU Chen, LC
   Collins, MD
   Zhu, YK
   Papandreou, G
   Zoph, B
   Schroff, F
   Adam, H
   Shlens, J
AF Chen, Liang-Chieh
   Collins, Maxwell D.
   Zhu, Yukun
   Papandreou, George
   Zoph, Barret
   Schroff, Florian
   Adam, Hartwig
   Shlens, Jonathon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Searching for Efficient Multi-Scale Architectures for Dense Image
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.
C1 [Chen, Liang-Chieh; Collins, Maxwell D.; Zhu, Yukun; Papandreou, George; Zoph, Barret; Schroff, Florian; Adam, Hartwig; Shlens, Jonathon] Google Inc, Mountain View, CA 94043 USA.
RP Chen, LC (reprint author), Google Inc, Mountain View, CA 94043 USA.
CR Andriluka M., 2014, CVPR
   Badrinarayanan Vijay, 2015, ARXIV151100561
   Baker B., 2017, ICLR
   Bell S., 2015, CVPR
   Bergstra  J., 2012, JMLR
   Bulo  S.R., 2018, CVPR
   Cai  H., 2018, AAAI
   Chan W., 2016, ICASSP
   Chen L.-C., 2018, ECCV
   Chen  L.-C., 2018, CVPR
   Chen  L.-C., 2017, TPAMI
   Chen L. C., 2015, ICLR
   Chen L.-C., 2016, CVPR
   Chen L.C., 2017, ARXIV170605587
   Chen  T., 2016, ICLR
   Chen X., 2014, CVPR
   Chollet F., 2017, CVPR
   Cordts M., 2016, CVPR
   Dai J., 2015, CVPR
   Dai J., 2016, CVPR
   Dai J., 2016, NIPS
   Dai J., 2017, ICCV
   Eigen D., 2015, ICCV
   Everingham  M., 2014, IJCV
   Fang  H.-S., 2018, CVPR
   Farabet  C., 2013, PAMI
   Fourure  D., 2017, BMVC
   Fu  J., 2017, ARXIV170804943
   Giusti  A., 2013, ICIP
   Golovin  D., 2017, SIGKDD
   Gould S., 2009, ICCV
   Hariharan B., 2011, ICCV
   He K., 2016, CVPR
   He Kaiming, 2017, ICCV
   He  X., 2004, CVPR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Holschneider M., 1989, WAVELETS TIME FREQUE, P289
   Howard Andrew G., 2017, ARXIV170404861
   Huang G, 2017, CVPR
   Ioffe S., 2015, ICML
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Krahenbuhl  P., 2011, NIPS
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   Ladicky L., 2009, ICCV
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liang  X., 2017, CVPR
   Lin  D., 2018, ECCV
   Lin  Guosheng, 2017, CVPR
   Lin Guosheng, 2016, CVPR
   Lin T.-Y., 2014, ECCV
   Liu  C., 2018, ECCV
   Liu  H., 2018, ICLR
   Liu  H., 2018, ARXIV180609055
   Liu W., 2016, ECCV
   Liu  W., 2015, ARXIV150604579
   Liu  Z., 2015, ICCV
   Long  J., 2015, CVPR
   Miikkulainen R, 2017, ARXIV170300548
   Mostajabi  M., 2015, CVPR
   Negrinho R., 2017, ARXIV170408792
   Papandreou  G., 2018, ECCV
   Papandreou  G., 2015, CVPR
   Peng  C., 2017, CVPR
   Pham  H., 2018, ICML
   Pinheiro P. H., 2014, ICML
   Qi  H., 2017, ICCV COCO CHALL WORK
   Real  E., 2017, ICML
   Real E., 2018, ARXIV180201548
   Redmon J., 2016, CVPR
   Ren S., 2015, NIPS
   Ronneberger O., 2015, MICCAI
   Russakovsky Olga, 2015, IJCV
   Sandler M., 2018, CVPR
   Saxena  S., 2016, NIPS
   Schwing A. G., 2015, ARXIV150302351
   Sermanet  P., 2014, ICLR
   Shotton  J., 2009, IJCV
   Sifre L., 2014, THESIS
   Silberman  N., 2012, ECCV
   Stanley K. O., 2002, EVOLUTIONARY COMPUTA
   Sutskever  I., 2014, NIPS
   Szegedy C., 2016, CVPR
   Szegedy C., 2015, CVPR
   Vanhoucke  V., 2014, ICLR INVITED TALK
   Wang  M., 2016, ARXIV160804337
   Wang P., 2017, ARXIV170208502
   Wu Y., 2016, ARXIV160908144
   Xia  F., 2017, CVPR
   Xie  L., 2017, ICCV
   Yang  M., 2018, CVPR
   Yao J., 2012, CVPR
   Yu  C., 2018, CVPR
   Yu F., 2016, ICLR
   Zhang  H., 2018, CVPR
   Zhang  Z., 2018, ECCV
   Zhao H., 2017, CVPR
   Zheng S., 2015, ICCV
   Zhong  Z., 2018, AAAI
   Zoph B., 2017, ICLR
   Zoph Barret, 2018, CVPR
NR 101
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003027
DA 2019-06-15
ER

PT S
AU Chen, LJ
   Wang, HY
   Zhao, JM
   Koutris, P
   Papailiopoulos, D
AF Chen, Lingjiao
   Wang, Hongyi
   Zhao, Jinman
   Koutris, Paraschos
   Papailiopoulos, Dimitris
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Effect of Network Width on the Performance of Large-batch Training
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that-for a fixed number of parameters-wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.
C1 [Chen, Lingjiao; Wang, Hongyi; Zhao, Jinman; Koutris, Paraschos] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
   [Papailiopoulos, Dimitris] Univ Wisconsin, Dept Elect & Comp Engn, 1415 Johnson Dr, Madison, WI 53706 USA.
RP Chen, LJ (reprint author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003082
DA 2019-06-15
ER

PT S
AU Chen, LQ
   Dai, SY
   Tao, CY
   Shen, DH
   Gan, Z
   Zhang, HC
   Zhang, YZ
   Carin, L
AF Chen, Liqun
   Dai, Shuyang
   Tao, Chenyang
   Shen, Dinghan
   Gan, Zhe
   Zhang, Haichao
   Zhang, Yizhe
   Carin, Lawrence
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial Text Generation via Feature-Mover's Distance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the featuremover's distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness.
C1 [Chen, Liqun; Dai, Shuyang; Tao, Chenyang; Shen, Dinghan; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA.
   [Gan, Zhe] Microsoft Dynam 365 AI Res, Newark, DE USA.
   [Zhang, Yizhe] Microsoft Res, Redmond, WA USA.
   [Zhang, Haichao] Baidu Res, Sunnyvale, CA USA.
RP Chen, LQ (reprint author), Duke Univ, Durham, NC 27706 USA.
EM liqun.chen@duke.edu
FU DARPA; ONR; NSF; DOE; NIH
FX This research was supported in part by DARPA, DOE, NIH, ONR and NSF.
CR Afriat S., 1971, SIAM J APPL MATH
   Arjovsky  M., 2017, ICML
   Arjovsky Martin, 2017, ICLR
   Artetxe M., 2018, ICLR
   Bandanau D., 2015, ICLR
   Bengio S., 2015, NIPS
   Bruen A. A., 2011, CRYPTOGRAPHY INFORM, V68
   Che T., 2017, ARXIV170207983
   Chen L., 2018, AISTATS
   Cho  K., 2014, EMNLP
   Collobert R., 2011, JMLR
   CONNEAU A., 2017, ARXIV171004087
   Cuturi M., 2013, NIPS
   Fang H, 2015, CVPR
   Fedus W., 2018, ICLR
   Francis W. N., 1979, BROWN CORPUS MANUAL
   Gan Z., 2017, NIPS
   Gan Z., 2017, EMNLP
   Genevay A., 2018, AISTATS
   Gomez A. N., 2018, ARXIV180104883
   Goodfellow I., 2014, NIPS
   Gretton A., 2012, JMLR
   Gulrajani  Ishaan, 2017, NIPS
   Guo J., 2018, AAAI
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hu Z., 2017, ICML
   Huszar Ferenc, 2015, ARXIV151105101
   Jang Eric, 2017, ICLR
   Kim Yoon, 2014, EMNLP
   Kucera H., 1979, STANDARD CORPUS PRES
   Kusner M. J., 2016, ARXIV161104051
   Lamb A., 2016, ARXIV160203220
   Lample G., 2018, ARXIV180407755
   LI  Chongxuan, 2017, NIPS
   Li J., 2018, NAACL
   Li J., 2017, EMNLP
   Lin K., 2017, NIPS
   Lin T.-Y., 2014, ECCV
   Liu Y., 2017, ARXIV170207817
   Maddison C. J., 2017, ICLR
   Mikolov T., 2010, ISCA
   Nowozin Sebastian, 2016, NIPS
   Papineni K., 2002, ACL
   Prabhumoye S., 2018, ACL
   Pu Y., 2018, ICML
   Pu Y., 2017, NIPS
   Ranzato Marc Aurelio, 2016, ICLR
   Rubner Y., 1998, ICCV
   Salimans T., 2018, ICLR
   Salimans T., 2016, NIPS
   Shen D., 2018, AAAI
   Shen D., 2018, ACL
   Shen S., 2015, ACL
   Shen T., 2017, NIPS
   Tao C., 2018, ICML
   Vinyals O, 2015, CVPR
   Welinder P., 2010, CNSTR2010001 CAL I T
   Wiseman S., 2016, EMNLP
   Xie Y., 2018, ARXIV180204307
   Yu Lantao, 2017, AAAI
   Zhang Y., 2017, ICML
   Zhu J.-Y., 2017, ICCV
   Zhu Y., 2018, SIGIR
NR 63
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304066
DA 2019-06-15
ER

PT S
AU Chen, LX
   Xu, J
   Lu, Z
AF Chen, Lixing
   Xu, Jie
   Lu, Zhuo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Contextual Combinatorial Multi-armed Bandits with Volatile Arms and
   Submodular Reward
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By "volatile arms", we mean that the available arms to select from in each round may change; and by "submodular rewards", we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed algorithm is proved to achieve O(cT(2 alpha+D/3 alpha+D) log(T)) regret after a span of T rounds. The performance of CC-MAB is evaluated by experiments conducted on a realworld crowdsourcing dataset, and the result shows that our algorithm outperforms the prior art.
C1 [Chen, Lixing; Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
   [Lu, Zhuo] Univ S Florida, Dept Elect Engn, Tampa, FL 33620 USA.
RP Chen, LX (reprint author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
EM lx.chen@miami.edu; jiexu@miami.edu; zhuolu@usf.edu
FU Army Research Office [W911NF-18-1-0343]
FX L. Chen and J. Xu's work is supported in part by the Army Research
   Office under Grant W911NF-18-1-0343. The views and conclusions contained
   in this document are those of the authors and should not be interpreted
   as representing the official policies, either expressed or implied, of
   the Army Research Office or the U.S. Government. The U.S. Government is
   authorized to reproduce and distribute reprints for Government purposes
   notwithstanding any copyright notation herein.
CR Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P., 2002, J MACHINE LEARNING R, V3, p397 , DOI DOI 10.1162/153244303321897663
   Bnaya Z., 2013, AAAI LATE BREAKING D
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chen L., 2017, ADV NEURAL INFORM PR, P141
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Hazan E, 2012, J MACH LEARN RES, V13, P2903
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7
   Kleinberg R, 2008, ACM S THEORY COMPUT, P681
   Kleinberg Robert D, 2005, ADV NEURAL INFORM PR, P697
   Krause A., 2008, ICML TUTORIALS
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Qin L, 2014, P 2014 SIAM INT C DA, P461
   Radanovic G., 2018, P 32 AAAI C ART INT
   Radlinski F., 2008, P 25 INT C MACH LEAR, P784, DOI DOI 10.1145/1390156.1390255
   Yang P, 2017, IEEE INTERNET THINGS, V4, P1193, DOI 10.1109/JIOT.2017.2726820
   Yue Y., 2011, NIPS, V23, P2483
   Zhang Y, 2012, IEEE J SEL AREA COMM, V30, P2136, DOI 10.1109/JSAC.2012.121206
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303026
DA 2019-06-15
ER

PT S
AU Chen, MS
   Yang, LF
   Wang, MD
   Zhao, T
AF Chen, Minshuo
   Yang, Lin F.
   Wang, Mengdi
   Zhao, Tuo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dimensionality Reduction for Stationary Time Series via Stochastic
   Nonconvex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Stochastic optimization naturally arises in machine learning. Efficient algorithms with provable guarantees, however, are still largely missing, when the objective function is nonconvex and the data points are dependent. This paper studies this fundamental challenge through a streaming PCA problem for stationary time series data. Specifically, our goal is to estimate the principle component of time series data with respect to the covariance matrix of the stationary distribution. Computationally, we propose a variant of Oja's algorithm combined with downsampling to control the bias of the stochastic gradient caused by the data dependency. Theoretically, we quantify the uncertainty of our proposed stochastic algorithm based on diffusion approximations. This allows us to prove the asymptotic rate of convergence and further implies near optimal asymptotic sample complexity. Numerical experiments are provided to support our analysis.
C1 [Chen, Minshuo; Zhao, Tuo] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Yang, Lin F.; Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA.
RP Chen, MS (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM mchen393@gatech.edu; lin.yang@princeton.edu; mengdiw@princeton.edu;
   tourzhao@gatech.edu
CR Allen-Zhu Zeyuan, 2016, ARXIV160707837
   Allen-Zhu  Zeyuan, 2017, ARXIV170808694
   Bottou L, 1998, NEURAL NETWORKS, V17, P142
   Bradley RC, 2005, PROBAB SURV, V2, P107, DOI 10.1214/154957805100000104
   CHEN Z., 2017, INT C MACH LEARN
   CHUNG K. L., 2004, CHANCE CHOICE MEMORA, P79
   Dang CD, 2015, SIAM J OPTIMIZ, V25, P856, DOI 10.1137/130936361
   De Vito S, 2008, SENSOR ACTUAT B-CHEM, V129, P750, DOI 10.1016/j.snb.2007.09.060
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P1549, DOI 10.1137/110836043
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659
   Durrett R., 2010, PROBABILITY THEORY E
   Ethier S. N., 2009, MARKOV PROCESSES CHA, V282
   Freidlin M. I., 1998, GRUND MATH WISS, V260
   Ge R, 2016, ADV NEURAL INFORM PR
   GE R., 2015, C LEARN THEOR
   GLYNN P. W., 1990, HDB OR MS, V2, P145
   Griffiths D. F., 2010, NUMERICAL METHODS OR
   HALL E. C., 2016, ARXIV160502693
   Homem-De-Mello T, 2008, SIAM J OPTIMIZ, V19, P524, DOI 10.1137/060657418
   HSU D. J., 2015, ADV NEURAL INFORM PR
   Jain P., 2016, C LEARN THEOR
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35
   LEE J. D, 2017, ARXIV171007406
   LI X, 2016, ARXIV161209296
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   REDDI S. J., 2016, ADV NEURAL INFORM PR
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   SACKS J, 1958, ANN MATH STAT, V29, P373, DOI 10.1214/aoms/1177706619
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   SHAMIR O., 2013, INT C MACH LEARN
   SREBRO N., 2004, ADV NEURAL INFORM PR
   SUN J., 2016, INF THEOR ISIT 2016
   SUN J., 2015, SAMPL THEOR APPL SAM
   TJOSTHEIM D, 1990, ADV APPL PROBAB, V22, P587, DOI 10.2307/1427459
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303049
DA 2019-06-15
ER

PT S
AU Chen, PH
   Si, S
   Li, Y
   Chelba, C
   Hsieh, CJ
AF Chen, Patrick H.
   Si, Si
   Li, Yang
   Chelba, Ciprian
   Hsieh, Cho-Jui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model
   Shrinking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. As a case study, a neural language model usually consists of one or more recurrent layers sandwiched between an embedding layer used for representing input tokens and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves great performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6 times compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26 times compression rate, which translates to a factor of 12.8 times compression for the entire model with very little degradation in perplexity.
C1 [Chen, Patrick H.; Hsieh, Cho-Jui] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
   [Si, Si; Li, Yang; Chelba, Ciprian] Google Res, Mountain View, CA USA.
   [Chen, Patrick H.] Google, Mountain View, CA 94043 USA.
RP Chen, PH (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.; Chen, PH (reprint author), Google, Mountain View, CA 94043 USA.
EM patrickchen@g.ucla.edu; sisidaisy@google.com; liyang@google.com;
   ciprianchelba@google.com; chohsieh@cs.ucla.edu
FU NSF [IIS-1719097]; Intel faculty award; Google Cloud; Nvidia
FX This research is mainly done during Patrick Chen's internship at Google
   Research. We also acknowledge the support by NSF via IIS-1719097, Intel
   faculty award, Google Cloud and Nvidia.
CR Cettolo  Mauro, 2014, P INT WORKSH SPOK LA
   Chelba C., 2013, ARXIV13123005
   Chen Y., 2016, ARXIV161003950
   Choi  Yoojin, 2018, ABS180202271 CORR
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Han Song, 2015, ABS151000149 CORR
   Han  Song, 2015, ABS150602626 CORR
   Howard Andrew G., 2017, ARXIV170404861
   Hubara I., 2016, ARXIV160907061
   Jaderberg M., 2014, ARXIV14053866
   Jozefowicz R., 2016, ARXIV160202410
   Kim Y. - D., 2016, ICLR
   Klein G., 2017, ARXIV170102810
   Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598
   Lin D. D., 2016, INT C MACH LEARN, P2849
   Lobacheva E., 2017, ARXIV170800077
   Lu  Zhiyun, 2016, ABS160402594 CORR
   Maximilian Lam, 2018, ARXIV180305651
   Narang  Sharan, ICLR
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Shu  Raphael, 2018, ICLR
   Si  Si, J MACH LEARN RES, V32, P701
   Srebro N., 2003, ICML, V20, P720
   Tjandra A, 2017, IEEE IJCNN, P4451, DOI 10.1109/IJCNN.2017.7966420
   Wu  Jiaxiang, 2015, ABS151206473 CORR
   Xu  Yuhui, 2018, ABS180303289 CORR
   Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005056
DA 2019-06-15
ER

PT S
AU Chen, RTQ
   Rubanova, Y
   Bettencourt, J
   Duvenaud, D
AF Chen, Ricky T. Q.
   Rubanova, Yulia
   Bettencourt, Jesse
   Duvenaud, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Ordinary Differential Equations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ADJOINT
AB We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.
C1 [Chen, Ricky T. Q.; Rubanova, Yulia; Bettencourt, Jesse; Duvenaud, David] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP Chen, RTQ; Rubanova, Y; Bettencourt, J (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM rtqichen@cs.toronto.edu; rubanova@cs.toronto.edu;
   jessebett@cs.toronto.edu; duvenaud@cs.toronto.edu
CR Alvarez MA, 2011, J MACH LEARN RES, V12, P1459
   Amos Brandon, 2017, INT C MACH LEARN, P136
   Andersson J., 2013, THESIS
   Baydin AG, 2018, J MACH LEARN RES, V18
   Bo Chang, 2017, ARXIV170903698
   Bo Chang, 2018, INT C LEARN REPR
   Carpenter B, 2015, ARXIV150907164
   Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9
   Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301
   Coddington E. A., 1955, THEORY ORDINARY DIFF
   Dinh L., 2014, ARXIV14108516
   Du N., 2016, P 22 ACM SIGKDD INT, P1555, DOI DOI 10.1145/2939672.2939875
   Farrell Patrick, 2013, SIAM J SCI COMPUTING
   Figurnov Michael, 2017, SPATIALLY ADAPTIVE C
   Futoma J., 2017, ARXIV E PRINTS
   Gomez A. N., 2017, ADV NEURAL INFORM PR, P2211
   Graves A., 2016, ARXIV160308983
   Ha D., 2016, ARXIV160909106
   Hairer E., 1987, SOLVING ORDINARY DIF
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, NEURAL NETWORKS MACH
   Jernite Yacine, 2016, ARXIV161106188
   Joel A E, 2018, MATH PROGRAMMING COM
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2014, ARXIV14126980
   Kingma Diederik, 2014, INT C LEARN REPR
   Kutta W., 1901, Z MATH PHYS, V46, P435
   Le Cun Y, 1988, P 1988 CONN MOD SUMM, V1, P21
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lipton Z. C., 2016, MACH LEARN HEALTHC C, P253
   Long Z., 2017, ARXIV E PRINTS
   Lu Yiping, 2017, ARXIV171010121
   Maclaurin Dougal, 2015, ICML WORKSH AUT MACH
   Mei Hongyuan, 2017, ADV NEURAL INFORM PR, P6757
   Melicher V, 2017, COMPUTATION STAT, V32, P1621, DOI 10.1007/s00180-017-0765-8
   Palm Conny, 1943, ERICSSON TECHNICS
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   PEARLMUTTER BA, 1995, IEEE T NEURAL NETWOR, V6, P1212, DOI 10.1109/72.410363
   Pontryagin L, 1962, MATH THEORY OPTIMAL
   RAISSI  M., 2018, ARXIV180101236
   Raissi M, 2018, SIAM J SCI COMPUT, V40, pA172, DOI 10.1137/17M1120762
   Raissi M, 2018, J COMPUT PHYS, V357, P125, DOI 10.1016/j.jcp.2017.11.039
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   RUNGE C., 1896, MATH ANN, V46, P167, DOI [DOI 10.1007/BF01446807, 10.1007/BF01446807]
   Ruthotto Lars, 2018, ARXIV180404272
   Ryder T., 2018, ARXIV E PRINTS
   Schober M, 2014, ADV NEUR IN, V27
   Schonlieb CB, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa7429
   Schulam  P., 2017, ARXIV170310651
   Soleimani H, 2017, IEEE ICC
   Soleimani Hossein, 2017, ARXIV170402038
   Stam J, 1999, COMP GRAPH, P121
   Stapor Paul, 2018, BIORXIV
   Tomczak Jakub M., 2016, ARXIV161109630
   van den Berg Rianne, 2018, ARXIV180305649
   Wiewel S., 2018, ARXIV180210123
   Yang Li, 2017, ARXIV170800065
   Zhang H, 2014, SIAM J SCI COMPUT, V36, pC504, DOI 10.1137/130912335
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001014
DA 2019-06-15
ER

PT S
AU Chen, RTQ
   Li, XC
   Grosse, R
   Duvenaud, D
AF Chen, Ricky T. Q.
   Li, Xuechen
   Grosse, Roger
   Duvenaud, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Isolating Sources of Disentanglement in VAEs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INDEPENDENT COMPONENT ANALYSIS
AB We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the beta-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the beta-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.
C1 [Chen, Ricky T. Q.; Li, Xuechen; Grosse, Roger; Duvenaud, David] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP Chen, RTQ (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM rtqichen@cs.toronto.edu; lxuchen@cs.toronto.edu; rgrosse@cs.toronto.edu;
   duvenaud@cs.toronto.edu
CR Achille A, 2018, IEEE T PATTERN ANAL
   Achille  A., 2017, ARXIV170601350
   Alemi  A., 2017, INT C LEARN REPR
   Aubry M., 2014, CVPR
   Belghazi I., 2018, ARXIV180104062
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Burgess Christopher, 2017, LEARN DIS REPR PERC
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Cheung B., 2014, ARXIV14126583
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Desjardins G., 2012, ARXIV12105474
   Gao Shuyang, 2018, ARXIV180205822
   Glorot X, 2011, P 28 INT C MACH LEAR, P513
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grathwohl Will, 2016, ARXIV161204440
   Higgins I., 2017, INT C LEARN REPR
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hoffman M. D., 2016, WORKSH ADV APPR BAYE
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Jutten Christian, 2003, P 4 INT S IND COMP A
   Karaletsos Theofanis, 2016, INT C LEARN REPR
   Kim H., 2018, ARXIV180205983
   Kingma D.P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Kumar Abhishek, 2017, ARXIV171100848
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Makhzani Alireza, 2016, ICLR 2016 WORKSH INT
   Matthey Loic, 2017, DSPRITES DISENTANGLE
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Radford A., 2015, ARXIV151106434
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Rezende D. J, 2014, ARXIV14014082
   Ridgeway Karl, 2016, ARXIV161205299
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   SOnderby C. K., 2017, INT C LEARN REPR
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Tang Y., 2013, P 30 INT C MACH LEAR, P163
   Vedantam Ramakrishna, 2018, INT C LEARN REPR
   Ver Steeg G, 2015, P 18 INT C ART INT S, P1004
   WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066
   Williams Christopher K. I., 2018, INT C LEARN REPR
   Xu Jin, 2018, ARXIV180605953
   Zemel R. S., 1994, ADV NEURAL INFORMATI, P3
   Zhao S., 2017, ARXIV170602262
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302061
DA 2019-06-15
ER

PT S
AU Chen, S
   Banerjee, A
AF Chen, Sheng
   Banerjee, Arindam
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI An Improved Analysis of Alternating Minimization for Structured
   Multi-Response Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them, which has an unknown covariance structure. To find the coefficient vector, estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance, which can be generally solved by alternating-minimization-type procedures. Due to the non-convex nature of such joint estimators, the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures, which requires access to fresh samples in each iteration. In this work, we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular, we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter, and the statistical error of the parameter can be expressed by the complexity measure, Gaussian width, which is related to the assumed structure. More importantly, to the best of our knowledge, our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments.
C1 [Chen, Sheng] Voleon Grp, Berkeley, CA 94704 USA.
   [Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
   [Chen, Sheng] Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.
RP Chen, S (reprint author), Voleon Grp, Berkeley, CA 94704 USA.; Chen, S (reprint author), Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.
EM chen2832@umn.edu; banerjee@cs.umn.edu
FU NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986,
   CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX The research was supported by NSF grants IIS-1563950, IIS-1447566,
   IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and
   Yahoo.
CR Anderson T. W, 2003, INTRO MULTIVARIATE S
   Banerjee A., 2014, ADV NEURAL INFORM PR
   Barber R., 2017, ARXIV170307755
   BHOJANAPALLI S, 2016, ADV NEURAL INFORM PR, P3873
   Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee S., 2014, ADV NEURAL INFORM PR
   Chen S., 2015, NIPS, P2908
   Chen S., 2017, ADV NEURAL INFORM PR, P2835
   Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Ge Rong, 2017, ARXIV170400708
   Goncalves A R, 2014, CIKM, P451, DOI DOI 10.1145/2661829.2662091
   Goncalves Andre R, 2016, J MACHINE LEARNING R, V17, P1205
   GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761
   Greene William H., 2011, ECONOMETRIC ANAL
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jain P., 2015, ADV NEURAL INFORM PR, P1126
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013
   Li Q., 2016, ARXIV161103060
   Ma Cong, 2017, ARXIV171110467
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Netrapalli P., 2013, NIPS
   Oberhofer W., 1974, ECONOMETRICA     MAY, P579
   Oymak S., 2015, ARXIV150704793
   Plan Y., 2016, INFORM INFERENCE
   Rai P., 2012, NIPS, P3185
   Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188
   Shen  Jie, 2017, INT C MACH LEARN, P3115
   Sun J., 2016, ARXIV160206664
   Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162
   Sun R., 2015, FOCS
   Talagrand  M., 2014, UPPER LOWER BOUNDS S
   TALAGRAND M, 2005, SPRINGER MG MATH, P1
   Tu S., 2015, ARXIV150703566
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Vershynin R., 2015, ESTIMATION HIGH DIME, P3
   Waldspurger I, 2018, IEEE T INFORM THEORY, V64, P3301, DOI 10.1109/TIT.2018.2800663
   Yi X., 2014, P 31 INT C MACH LEAR, P613
   Zheng Q., 2015, ADV NEURAL INFORM PR, P109
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001018
DA 2019-06-15
ER

PT S
AU Chen, T
   Murali, A
   Gupta, A
AF Chen, Tao
   Murali, Adithyavairavan
   Gupta, Abhinav
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hardware Conditioned Policies for Multi-Robot Transfer Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.
C1 [Chen, Tao; Murali, Adithyavairavan; Gupta, Abhinav] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
RP Chen, T (reprint author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
EM taoc1@cs.cmu.edu; amurali@cs.cmu.edu; abhinavg@cs.cmu.edu
FU ONR MURI [N000141612007]; Army Research Office [W911NF-18-1-0019]; Sloan
   Research Fellowship; Uber Fellowship
FX This research is partly sponsored by ONR MURI N000141612007 and the Army
   Research Office and was accomplished under Grant Number
   W911NF-18-1-0019. Abhinav was supported in part by Sloan Research
   Fellowship and Adithya was partly supported by a Uber Fellowship. The
   views and conclusions contained in this document are those of the
   authors and should not be interpreted as representing the official
   policies, either expressed or implied, of the Army Research Office or
   the U.S. Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Government purposes notwithstanding any
   copyright notation herein. The authors would also like to thank Senthil
   Purushwalkam and Deepak Pathak for feedback on the early draft and
   Lerrel Pinto and Roberto Shu for insightful discussions.
CR Abbeel P., 2006, P 23 INT C MACH LEAR, V3, P1
   Al-Shedivat Maruan, 2017, ARXIV171003641
   Andrychowicz Marcin, 2017, CORR
   Aravind Rajeswaran, 2017, ICLR
   ARTSTEIN Z, 1980, SIAM REV, V22, P172, DOI 10.1137/1022026
   Barrett Samuel, 2010, 9 INT C AUT AG MULT
   Bocsi B., 2013, 2013 INT JOINT C NEU, P1
   Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250
   Erez T, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P73
   Fu J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4019, DOI 10.1109/IROS.2016.7759592
   Gupta A, 2017, ARXIV170302949
   Helwa Mohamed K., 2017, MULTIROBOT TRANSFER
   Kahn Gregory, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3342, DOI 10.1109/ICRA.2017.7989379
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2015, CORR
   Mahler Jeffrey, 2017, RSS
   Mandlekar Ajay, 2017, IROS
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mordatch I, 2015, IEEE INT C INT ROBOT, P5307, DOI 10.1109/IROS.2015.7354126
   Murali Adithyavairavan, 2018, ICRA
   Nagarajan U., 2009, P IEEE INT C ROB AUT, P998
   Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216
   Park Hae-Won, 2010, DYN WALK C DW MIT JU
   Peng X. B., 2017, ARXIV171006537
   Pinto Lerrel, 2017, ICRA
   Rusu A. A., 2016, PROGR NEURAL NETWORK
   Sanchez-Gonzalez Alvaro, 2018, ARXIV180601242
   Schaul T., 2015, P 32 INT C MACH LEAR, P1312
   Schulman John, 2017, CORR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   SLOTINE JJE, 1988, IEEE T AUTOMAT CONTR, V33, P995, DOI 10.1109/9.14411
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Tobin J, 2017, DOMAIN RANDOMIZATION
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Wang Tingwu, 2018, INT C LEARN REPR
   Xijian Huo, 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P277, DOI 10.1109/ROBIO.2012.6490979
   Yu W., 2017, ARXIV170202453
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003085
DA 2019-06-15
ER

PT S
AU Chen, TQ
   Zheng, LM
   Yan, E
   Jiang, ZH
   Moreau, T
   Ceze, L
   Guestrin, C
   Krishnamurthy, A
AF Chen, Tianqi
   Zheng, Lianmin
   Yan, Eddie
   Jiang, Ziheng
   Moreau, Thierry
   Ceze, Luis
   Guestrin, Carlos
   Krishnamurthy, Arvind
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Optimize Tensor Programs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PARALLELISM
AB We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, current systems rely on manually optimized libraries, e.g., cuDNN, that support only a narrow range of server class GPUs. Such reliance limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search using effective model transfer across workloads. Experimental results show that our framework delivers performance that is competitive with state-of-the-art hand-tuned libraries for low-power CPUs, mobile GPUs, and server-class GPUs.
C1 [Chen, Tianqi; Yan, Eddie; Jiang, Ziheng; Moreau, Thierry; Ceze, Luis; Guestrin, Carlos; Krishnamurthy, Arvind] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.
   [Zheng, Lianmin] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
RP Chen, TQ (reprint author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.
FU Google PhD Fellowship; ONR [N00014-16-1-2795]; NSF [CCF-1518703,
   CNS-1614717, CCF-1723352]
FX We would like to thank members of Sampa, SAMPL and Systems groups at the
   Allen School for their feedback on the work and manuscript. This work
   was supported in part by a Google PhD Fellowship for Tianqi Chen, ONR
   award #N00014-16-1-2795, NSF under grants CCF-1518703, CNS-1614717, and
   CCF-1723352, and gifts from Intel (under the CAPA program), Oracle,
   Huawei and anonymous sources.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Agarwal A., 2014, MSRTR2014112
   Allamanis Miltiadis, 2018, INT C LEARN REPR
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bondhugula U, 2008, PLDI'08: PROCEEDINGS OF THE 2008 SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN & IMPLEMENTATION, P101, DOI 10.1145/1375581.1375595
   Borges C., 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   Chen Tianqi, 2015, NEURAL INFORM PROCES
   Chen Tianqi, 2018, 13 USENIX S OP SYST
   Chen Xinyun, 2018, CORR
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Frigo M, 1998, INT CONF ACOUST SPEE, P1381, DOI 10.1109/ICASSP.1998.681704
   Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043
   He K., 2016, ARXIV160305027
   Henriksen T, 2017, ACM SIGPLAN NOTICES, V52, P556, DOI [10.1145/3140587.3062354, 10.1145/3062341.3062354]
   Howard A. G., 2017, CORR
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Hutter F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4197
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Kjolstad Fredrik, 2017, P ACM PROGR LANG, V1
   Kraska Tim, 2017, CORR
   Krause A, 2014, TRACTABILITY, P71
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435
   Li Lisha, 2016, CORR
   Mirhoseini A., 2017, P 34 INT C MACH LEAR, P2430
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mullapudi RT, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925952
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Palkar Shoumik, 2017, CORR
   Radford A., 2015, ARXIV151106434
   Ragan-Kelley J, 2013, ACM SIGPLAN NOTICES, V48, P519, DOI 10.1145/2499370.2462176
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2015, P 32 INT C MACH LEAR, P2171
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Steuwer M, 2017, INT SYM CODE GENER, P74, DOI 10.1109/CGO.2017.7863730
   Sujeeth A., 2011, ICML, P609
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tai K. S., 2015, ARXIV150300075
   Vasilache  N., 2018, CORR
   Vasilache Nicolas, COMMUNICATION
   VERDOOLAEGE S, 2013, ACM T ARCHIT CODE OP, V9, DOI DOI 10.1145/2400682.2400713
   Whaley R. C., 1998, P 1998 ACM IEEE C SU, P1, DOI DOI 10.1109/SC.1998.10004
   Zaremba W, 2014, ARXIV14092329
NR 44
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303039
DA 2019-06-15
ER

PT S
AU Chen, TY
   Giannakis, GB
   Sun, T
   Yin, WT
AF Chen, Tianyi
   Giannakis, Georgios B.
   Sun, Tao
   Yin, Wotao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper presents a new class of gradient methods for distributed machine learning that adaptively skip the gradient calculations to learn with reduced communication and computation. Simple rules are designed to detect slowly-varying gradients and, therefore, trigger the reuse of outdated gradients. The resultant gradient-based algorithms are termed Lazily Aggregated Gradient - justifying our acronym LAG used henceforth. Theoretically, the merits of this contribution are: i) the convergence rate is the same as batch gradient descent in strongly-convex, convex, and nonconvex cases; and, ii) if the distributed datasets are heterogeneous (quantified by certain measurable constants), the communication rounds needed to achieve a targeted accuracy are reduced thanks to the adaptive reuse of lagged gradients. Numerical experiments on both synthetic and real data corroborate a significant communication reduction compared to alternatives.
C1 [Chen, Tianyi; Giannakis, Georgios B.] Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.
   [Sun, Tao] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
   [Sun, Tao; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
RP Chen, TY (reprint author), Univ Minnesota Twin Cities, Minneapolis, MN 55455 USA.
EM chen3827@umn.edu; georgios@umn.edu; nudtsuntao@163.com;
   wotaoyin@math.ucla.edu
FU NSF [1500713, 1711471, DMS-1720237]; University of Minnesota; China
   Scholarship Council; ONR [N0001417121]; NIH [1R01GM104975-01]
FX The work by T. Chen and G. Giannakis is supported in part by NSF 1500713
   and 1711471, and NIH 1R01GM104975-01. The work by T. Chen is also
   supported by the Doctoral Dissertation Fellowship from the University of
   Minnesota. The work by T. Sun is supported in part by China Scholarship
   Council. The work by W. Yin is supported in part by NSF DMS-1720237 and
   ONR N0001417121.
CR Aji A. F., 2017, P 2017 C EMP METH NA, P440
   Alistarh D., 2017, ADV NEURAL INF PROCE, P1709
   Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961
   Bottou L., 2016, 160604838 ARXIV
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cannelli  L., 2016, 160704818 ARXIV
   Chen  T., 2018, P IEEE
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Giannakis GB, 2016, SCI COMPUT, P461, DOI 10.1007/978-3-319-41589-5_14
   Gurbuzbalaban M, 2017, SIAM J OPTIMIZ, V27, P1035, DOI 10.1137/15M1049695
   Jaggi M., 2014, ADV NEURAL INFORM PR, P3068
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Jordan M. I., 2018, J AM STAT ASS
   Lan  G., 2017, 170103961 ARXIV
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Mu, 2014, ADV NEURAL INFORM PR, P19
   Lian X., 2015, ADV NEURAL INFORM PR, P2737
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Ma C, 2017, OPTIM METHOD SOFTW, V32, P813, DOI 10.1080/10556788.2016.1278445
   McMahan  B., 2017, GOOGLE RES BLOG  APR
   McMahan H. B., 2017, ARTIF INTELL, P1273
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950
   Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6
   Shamir O., 2014, INT C MACH LEARN, P1000
   Smith  V., 2017, P 32 ANN C NEUR INF, P4427
   Song L., 2007, P 24 INT C MACH LEAR, P823, DOI DOI 10.1145/1273496.1273600
   Stoica  I., 2017, 171205855 ARXIV
   Sun Tao, 2017, P ADV NEUR INF PROC, P6183
   Suresh A. T., 2017, INT C MACH LEARN, V70, P3329
   Wen  W., 2017, P NIPS, P1509
   Yaohua Liu, 2017, 2017 IEEE 56th Annual Conference on Decision and Control (CDC), P6696, DOI 10.1109/CDC.2017.8264668
   Zhang  Y., 2013, J MACHINE LEARNING R, V14
   Zhang Y., 2015, P 32 INT C MACH LEAR, P362
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305009
DA 2019-06-15
ER

PT S
AU Chen, XH
   Liu, JL
   Wang, ZY
   Yin, WT
AF Chen, Xiaohan
   Liu, Jialin
   Wang, Zhangyang
   Yin, Wotao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Theoretical Linear Convergence of Unfolded ISTA and its Practical
   Weights and Thresholds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available.(2).
C1 [Chen, Xiaohan; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
   [Liu, Jialin; Yin, Wotao] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90095 USA.
RP Chen, XH (reprint author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
EM chernxh@tamu.edu; liujl11@math.ucla.edu; atlaswang@tamu.edu;
   wotaoyin@math.ucla.edu
FU NSF [RI-1755701, DMS-1720237]; ONR [N0001417121]
FX The work by X. Chen and Z. Wang is supported in part by NSF RI-1755701.
   The work by J. Liu and W. Yin is supported in part by NSF DMS-1720237
   and ONR N0001417121. We would also like to thank all anonymous reviewers
   for their tremendously useful comments to help improve our work.
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsimas D, 1997, INTRO LINEAR OPTIMIZ
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Blumensath T, 2008, J FOURIER ANAL APPL, V14, P629, DOI 10.1007/s00041-008-9035-z
   Borgerding Mark, 2017, IEEE T SIGNAL PROCES
   Borgerding Mark, 2016 IEEE GLOB C SIG
   Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1
   Di Kai, 2018, AAAI C ART INT
   Giryes Raja, 2018, IEEE T SIGNAL PROCES
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Hale E., 2008, SIAM J OPTIMIZ, V19, P1
   Jian Zhang, 2018, IEEE CVPR
   Kulkarni Kuldeep, 2016, P IEEE C COMP VIS PA
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler  C., 2017, P ADV NEUR INF PROC, P1770
   Moreau Thomas, 2017, ICLR
   Osher Stanley, 2010, COMMUNICATIONS MATH
   Sprechmann Pablo, 2015, IEEE T PATTERN ANAL
   Tao SZ, 2016, SIAM J OPTIMIZ, V26, P313, DOI 10.1137/151004549
   Wang Z., 2016, P 30 AAAI C ART INT, P2194
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wang Zhangyang, 2016, LEARNING DEEP ENCODE, P2174
   Wang Zhaowen, SPARSE CODING ITS AP
   Wang Z, 2016, INT CONF SOFTW ENG, P369, DOI 10.1109/ICSESS.2016.7883088
   Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997
   Xin B., 2016, P ADV NEUR INF PROC, P4340
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Zhang LF, 2017, OPTIMIZATION, V66, P1177, DOI 10.1080/02331934.2017.1318133
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003060
DA 2019-06-15
ER

PT S
AU Chen, XW
   Huang, WR
   Chen, W
   Lui, JCS
AF Chen, Xiaowei
   Huang, Weiran
   Chen, Wei
   Lui, John C. S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Community Exploration: From Offline Optimization to Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FINITE-TIME ANALYSIS; SUMS
AB We introduce the community exploration problem that has many real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an "upper confidence" like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound.
C1 [Chen, Xiaowei; Lui, John C. S.] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Huang, Weiran] Huawei Noahs Ark Lab, Hong Kong, Peoples R China.
   [Chen, Wei] Microsoft Res, San Diego, CA USA.
RP Chen, XW (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM xwchen@cse.cuhk.edu.hk; huang.inbox@outlook.com; weic@microsoft.com;
   cslui@cse.cuhk.edu.hk
FU National Natural Science Foundation of China [61433014]; GRF [14208816]
FX We thank Jing Yu from School of Mathematical Sciences at Fudan
   University for her insightful discussion on the offline problems,
   especially, we thank Jing Yu for her method to find a good initial
   allocation, which leads to a faster greedy method. Wei Chen is partially
   supported by the National Natural Science Foundation of China (Grant No.
   61433014). The work of John C.S. Lui is supported in part by the GRF
   Grant 14208816.
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Berry Donald A, 1985, BANDIT PROBLEMS SEQU, V5, P71
   Bressan Marco, 2015, ARXIV151207901
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck S, 2013, J MACH LEARN RES, V14, P601
   Chen Wei, 2016, NIPS, P1659
   Chen Weigen, 2013, T CHINA ELECTROTECHN, V01, P50, DOI DOI 10.1145/2463209.2488881
   CHRISTMAN MC, 1994, STAT SINICA, V4, P335
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1
   Finkelstein M, 1998, STAT PROBABIL LETT, V37, P423, DOI 10.1016/S0167-7152(97)00146-6
   Gabillon Victor, 2013, P ADV NEUR INF PROC, P2697
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008
   Katzir Liran, 2011, WWW
   Kveton B, 2015, P 18 INT C ART INT S, P535
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Wang Q., 2017, NIPS, P1161
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000002
DA 2019-06-15
ER

PT S
AU Chen, XY
   Liu, C
   Song, D
AF Chen, Xinyun
   Liu, Chang
   Song, Dawn
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Tree-to-tree Neural Networks for Program Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.
C1 [Chen, Xinyun; Liu, Chang; Song, Dawn] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Chen, XY (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM xinyun.chen@berkeley.edu; liuchang2005acm@gmail.com;
   dawnsong@cs.berkeley.edu
FU National Science Foundation [TWC-1409915]; DARPA D3M [FA8750-17-2-0091]
FX We thank the anonymous reviewers for their valuable comments. This
   material is in part based upon work supported by the National Science
   Foundation under Grant No. TWC-1409915, Berkeley DeepDrive, and DARPA
   D3M under Grant No. FA8750-17-2-0091. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the National
   Science Foundation.
CR Aharoni  R., 2017, ACL
   Allamanis  M., 2017, ARXIV170906182
   Alvarez-Melis  D., 2017, ICLR
   Nguyen AT, 2015, IEEE INT CONF AUTOM, P585, DOI 10.1109/ASE.2015.74
   Balog M., 2017, ICLR
   Bandanau D., 2015, ICLR
   Bradbury  J., 2017, ARXIV170901915
   Chen  X., 2018, ICLR
   Cho S. J. K., 2015, ACL
   Devlin  J., 2017, ARXIV170307469
   Dong L., 2016, ACL
   Dyer  C., 2016, NAACL
   Eriguchi  A., 2016, ACL
   Karaivanov S., 2014, P 2014 ACM INT S NEW, P173, DOI DOI 10.1145/2661136.2661148
   Karpathy A., 2015, ARXIV150602078
   Kusner M. J, 2017, ARXIV170301925
   Ling  W., 2016, ACL
   Liu  C., 2016, ADV NEURAL INFORM PR, P4574
   Luong T., 2015, P 2015 C EMP METH NA, P1412, DOI DOI 10.18653/V1/D15-1166
   Nguyen A. T., 2013, P 2013 9 JOINT M FDN, P651
   Oda Y, 2015, IEEE INT CONF AUTOM, P574, DOI 10.1109/ASE.2015.36
   Parisotto  E., 2017, ICLR
   Rabinovich Maxim, 2017, P 55 ANN M ASS COMP, V1, P1139, DOI DOI 10.18653/V1/P17-1105
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Tai KS, 2015, P ANN M ASS COMP LIN
   Nguyen TD, 2016, 2016 IEEE/ACM 38TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING COMPANION (ICSE-C), P756, DOI 10.1145/2889160.2892661
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Vinyals O., 2015, NIPS
   Xia  Y., 2016, ADV NEURAL INFORM PR, P820
   Yin  P., 2017, ACL
   Zhang XY, 2016, AER ADV ENG RES, V63, P310
   Zhu X., 2015, P 32 INT C MACH LEAR, P1604
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302055
DA 2019-06-15
ER

PT S
AU Chen, Y
   Yang, ZR
   Xie, YC
   Wang, ZR
AF Chen, Yi
   Yang, Zhuoran
   Xie, Yuchen
   Wang, Zhaoran
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Contrastive Learning from Pairwise Measurements
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Learning from pairwise measurements naturally arises from many applications, such as rank aggregation, ordinal embedding, and crowdsourcing. However, most existing models and algorithms are susceptible to potential model misspecification. In this paper, we study a semiparametric model where the pairwise measurements follow a natural exponential family distribution with an unknown base measure. Such a semiparametric model includes various popular parametric models, such as the Bradley-Terry-Luce model and the paired cardinal model, as special cases. To estimate this semiparametric model without specifying the base measure, we propose a data augmentation technique to create virtual examples, which enables us to define a contrastive estimator. In particular, we prove that such a contrastive estimator is invariant to model misspecification within the natural exponential family, and moreover, attains the optimal statistical rate of convergence up to a logarithmic factor. We provide numerical experiments to corroborate our theory.
C1 [Chen, Yi; Xie, Yuchen; Wang, Zhaoran] Northwestern Univ, Evanston, IL 60208 USA.
   [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA.
RP Chen, Y (reprint author), Northwestern Univ, Evanston, IL 60208 USA.
EM yichen2016@u.northwestern.edu; zy6@princeton.edu;
   ycxie@u.northwestern.edu; zhaoran.wang@northwestern.edu
CR Boucheron S., 2013, CONCENTRATION INEQUA
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   CAO Y., 2015, INF THEOR ISIT 2015
   Chan KCG, 2013, BIOMETRIKA, V100, P269, DOI 10.1093/biomet/ass056
   Diao GQ, 2012, INT J BIOSTAT, V8, DOI 10.1515/1557-4679.1372
   GEER S. A, 2000, EMPIRICAL PROCESSES, V6
   GOPALAN P. K, 2014, ADV NEURAL INFORM PR
   GUIVER J., 2009, P 26 ANN INT C MACH
   GUNASEKAR S, 2014, INT C MACH LEARN
   JAIN L, 2016, ADV NEURAL INFORM PR
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   KHETAN A, 2016, ADV NEURAL INFORM PR
   Klopp O, 2017, PROBAB THEORY REL, V169, P523, DOI 10.1007/s00440-016-0736-y
   LAFOND J, 2015, C LEARN THEOR
   LEDOUX M., 2005, CONCENTRATION MEASUR, V89
   Liang KY, 2000, J ROY STAT SOC B, V62, P773, DOI 10.1111/1467-9868.00263
   LU Y, 2015, COMM CONTR COMP ALL
   Luce R. D, 2005, INDIVIDUAL CHOICE BE
   NEGAHBAN S, 2017, ARXIV170407228
   NEGAHBAN S, 2012, ADV NEURAL INFORM PR
   Negahban S, 2017, OPER RES, V65, P266, DOI 10.1287/opre.2016.1534
   Ning Y, 2017, ANN STAT, V45, P2299, DOI 10.1214/16-AOS1483
   Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567
   RAJKUMAR A, 2014, INT C MACH LEARN
   SAMBASIVAN A. V, 2018, IEEE T INFORM THEORY
   SHAH N, 2015, ARTIFICIAL INTELLIGE
   SOUFIANI H. A, 2013, ADV NEURAL INFORM PR
   Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048
   YANG Z, 2014, ARXIV14128697
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005049
DA 2019-06-15
ER

PT S
AU Chen, YB
   Paiton, DM
   Olshausen, BA
AF Chen, Yubei
   Paiton, Dylan M.
   Olshausen, Bruno A.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Sparse Manifold Transform
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INDEPENDENT COMPONENT ANALYSIS; NATURAL IMAGES; REPRESENTATIONS;
   STATISTICS; EMERGENCE; FRAMEWORK; MODEL; FIELD
AB We present a signal representation framework called the sparse manifold transform that combines key ideas from sparse coding, manifold learning, and slow feature analysis. It turns non-linear transformations in the primary sensory signal space into linear interpolations in a representational embedding space while maintaining approximate invertibility. The sparse manifold transform is an unsupervised and generative framework that explicitly and simultaneously models the sparse discreteness and low-dimensional manifold structure found in natural scenes. When stacked, it also models hierarchical composition. We provide a theoretical description of the transform and demonstrate properties of the learned representation on both synthetic data and natural videos.
C1 [Chen, Yubei; Paiton, Dylan M.; Olshausen, Bruno A.] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.
   [Chen, Yubei] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Paiton, Dylan M.; Olshausen, Bruno A.] Univ Calif Berkeley, Vis Sci Grad Grp, Berkeley, CA 94720 USA.
   [Olshausen, Bruno A.] Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA.
   [Olshausen, Bruno A.] Univ Calif Berkeley, Sch Optometry, Berkeley, CA 94720 USA.
RP Chen, YB (reprint author), Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.; Chen, YB (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM yubeic@eecs.berkeley.edu
FU NIH/NEI [T32 EY007043];  [NSF-IIS-1718991];  [NSF-DGE-1106400]
FX We thank Joan Bruna, Fritz Sommer, Ryan Zarcone, Alex Anderson, Brian
   Cheung and Charles Frye for many fruitful discussions; Karl Zipser for
   sharing computing resources; Eero Simoncelli and Chris Rozell for
   pointing us to some valuable references. This work is supported by
   NSF-IIS-1718991, NSF-DGE-1106400, and NIH/NEI T32 EY007043.
CR ATICK JJ, 1992, NEURAL COMPUT, V4, P196, DOI 10.1162/neco.1992.4.2.196
   Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308
   Balle  Johannes, 2015, ARXIV151106281
   Belkin M, 2002, ADV NEUR IN, V14, P585
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Berkes P, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000495
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna  Joan, 2013, ARXIV13114025
   Cadieu CF, 2012, NEURAL COMPUT, V24, P827, DOI 10.1162/NECO_a_00247
   Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x
   De Silva V., 2004, P S POINT BAS GRAPH, P157, DOI DOI 10.2312/SPBG/SPBG04/157-166
   De Silva  Vin, 2004, TECHNICAL REPORT
   Denton E. L., 2017, ADV NEURAL INFORM PR, P4417
   DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Elhamifar E., 2011, ADV NEURAL INFORM PR, V24, P55
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Gilbert A. C., 2017, ARXIV170508664
   Goroshin R, 2015, ADV NEURAL INFORM PR, P1234
   Henaff  OJ, 2018, COMPUTATIONAL SYSTEM
   Hosoya  Haruo, 2016, NEURAL COMPUTATION
   Huo  Xiaoming, 2007, RECENT ADV DATA MINI, P691, DOI DOI 10.1142/9789812779861_0015
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   Hyvarinen A, 2003, J OPT SOC AM A, V20, P1237, DOI 10.1364/JOSAA.20.001237
   Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Karhunen J, 1997, IEEE T NEURAL NETWOR, V8, P486, DOI 10.1109/72.572090
   Koster U, 2010, NEURAL COMPUT, V22, P2308, DOI 10.1162/NECO_a_00010
   Le Quoc V, 2012, P 29 INT C INT C MAC, P507
   Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078
   Lee J., 2012, INTRO SMOOTH MANIFOL
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Lyu Siwei, 2008, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2008, P1
   Malo J, 2006, NETWORK-COMP NEURAL, V17, P85, DOI 10.1080/09548980500439602
   Mumford D, 2010, PATTERN THEORY STOCH
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 2003, IEEE IMAGE PROC, P41
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Olshausen BA, 2013, PROC SPIE, V8651, DOI 10.1117/12.2013504
   Osindero S, 2006, NEURAL COMPUT, V18, P381, DOI 10.1162/089976606775093936
   Paiton Dylan M, 2016, P 9 EAI INT C BIOINS, P535
   PERONA P, 1995, IEEE T PATTERN ANAL, V17, P488, DOI 10.1109/34.391394
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   Sabour S., 2017, ADV NEURAL INFORM PR, P3859
   Shan  Honghao, 2013, ARXIV13126077
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Silva J., 2006, ADV NEURAL INF PROCE, V18, P1241
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577
   Vladymyrov Max, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P256, DOI 10.1007/978-3-642-40994-3_17
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
NR 62
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005012
DA 2019-06-15
ER

PT S
AU Chen, YP
   Kalantidis, Y
   Li, JS
   Yan, SC
   Feng, JS
AF Chen, Yunpeng
   Kalantidis, Yannis
   Li, Jianshu
   Yan, Shuicheng
   Feng, Jiashi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A(2)-Nets: Double Attention Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the "double attention block", a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.
C1 [Chen, Yunpeng; Li, Jianshu; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore.
   [Kalantidis, Yannis] Facebook Res, Singapore, Singapore.
   [Yan, Shuicheng] Natl Univ Singapore, Qihoo 360 AI Inst, Singapore, Singapore.
RP Chen, YP (reprint author), Natl Univ Singapore, Singapore, Singapore.
EM chenyunpeng@u.nus.edu; yannisk@fb.com; jianshu@u.nus.edu;
   eleyans@nus.edu.sg; elefjia@nus.edu.sg
CR Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen L.-C., 2016, ARXIV160600915
   Chen Tianqi, 2015, ARXIV151201274
   Chen Y, 2017, ADV NEURAL INFORM PR, P4470
   Chen Y., 2018, EUR C COMP VIS ECCV
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Girdhar R., 2017, ADV NEURAL INFORM PR, P33
   Girshick R., 2015, ARXIV150408083
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu J., 2018, IEEE C COMP VIS PATT
   Kay W., 2017, ARXIV170506950
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li  Peihua, 2017, ARXIV170308050
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Ma N., 2018, ARXIV180711164
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Paszke Adam, 2017, PYTORCH
   Ray Jamie, 2017, ARXIV170805038
   Sandler M., 2018, ARXIV180104381
   Soomro K., 2012, ARXIV12120402
   Tran  Du, 2018, 2018 IEEE C COMP VIS
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Wang  Xiaolong, 2018, COMPUTER VISION PATT
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300033
DA 2019-06-15
ER

PT S
AU Chen, YX
   Singla, A
   Mac Aodha, O
   Perona, P
   Yue, YS
AF Chen, Yuxin
   Singla, Adish
   Mac Aodha, Oisin
   Perona, Pietro
   Yue, Yisong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Understanding the Role of Adaptivity in Machine Teaching: The Case of
   Version Space Learners
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SAMPLE
AB In real-world applications of education, an effective teacher adaptively chooses the next example to teach based on the learner's current state. However, most existing work in algorithmic machine teaching focuses on the batch setting, where adaptivity plays no role. In this paper, we study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner's new state. We highlight that adaptivity does not speed up the teaching process when considering existing models of version space learners, such as the "worst-case" model (the learner picks the next hypothesis randomly from the version space) and the "preference-based" model (the learner picks hypothesis according to some global preference). Inspired by human teaching, we propose a new model where the learner picks hypotheses according to some local preference defined by the current hypothesis. We show that our model exhibits several desirable properties, e.g., adaptivity plays a key role, and the learner's transitions over hypotheses are smooth/interpretable. We develop adaptive teaching algorithms, and demonstrate our results via simulation and user studies.
C1 [Chen, Yuxin; Mac Aodha, Oisin; Perona, Pietro; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA.
   [Singla, Adish] MPI SWS, Saarbrucken, Germany.
RP Chen, YX (reprint author), CALTECH, Pasadena, CA 91125 USA.
EM chenyux@caltech.edu; adishs@mpi-sws.org; macaodha@caltech.edu;
   perona@caltech.edu; yyue@caltech.edu
FU Northrop Grumman; Bloomberg; AWS Research Credits; Google as part of the
   Visipedia project; Swiss NSF Early Mobility Postdoctoral Fellowship
FX This work was supported in part by Northrop Grumman, Bloomberg, AWS
   Research Credits, Google as part of the Visipedia project, and a Swiss
   NSF Early Mobility Postdoctoral Fellowship.
CR Balbach FJ, 2008, THEOR COMPUT SCI, V397, P94, DOI 10.1016/j.tcs.2008.02.025
   Balbach FJ, 2011, INFORM COMPUT, V209, P296, DOI 10.1016/j.ic.2010.11.005
   BALBACH RJ, 2005, ALT, V3734, P474
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bonawitz E, 2014, COGNITIVE PSYCHOL, V74, P35, DOI 10.1016/j.cogpsych.2014.06.003
   Cakmak M., 2012, AAAI
   Chen Y., 2018, CORR
   Chen Yuxin, 2018, CORR
   Chen Yuxin, 2018, AISTATS
   Doliwa T, 2014, J MACH LEARN RES, V15, P3107
   E Mark Gold, 1967, INFORM CONTROL, V10
   Gao ZY, 2017, J MACH LEARN RES, V18, P1
   Gao Ziyuan, 2017, ALT, P185
   GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003
   Haug Luis, 2018, ADV NEURAL INFORM PR
   Hellerstein L, 2015, LECT NOTES COMPUT SC, V9079, P235, DOI 10.1007/978-3-319-18173-8_17
   Hengst Bernhard, 2010, HIERARCHICAL REINFOR, P495
   Jha S, 2017, ACTA INFORM, V54, P693, DOI 10.1007/s00236-017-0294-5
   Johns E, 2015, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2015.7298877
   Lange S, 1996, J COMPUT SYST SCI, V53, P88, DOI 10.1006/jcss.1996.0051
   LEVINE M, 1975, COGNITIVE THEORY LEA
   [刘景元 Liu Jingyuan], 2016, [空军工程大学学报. 自然科学版, Journal of Air Force Engineering University. Natural Science Edition], V17, P1
   Liu W., 2017, ICML, P2149
   Mei S., 2015, AAAI, P2871
   Meng DY, 2017, INFORM SCIENCES, V414, P319, DOI 10.1016/j.ins.2017.05.043
   Rafferty AN, 2016, COGNITIVE SCI, V40, P1290, DOI 10.1111/cogs.12290
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shalev-Shwartz Shai, 2012, FDN TRENDS MACH LEAR, V4, P2
   Singla A., 2013, NIPS WORKSH DAT DRIV
   Singla A., 2014, P 31 INT C MACH LEAR, P154
   Tekin C, 2015, INT CONF ACOUST SPEE, P5545, DOI 10.1109/ICASSP.2015.7179032
   Vygotsky L, 1987, MIND SOC DEV HIGHER, V5291, P157
   Weld Daniel S, 2012, HCOMP
   Zhu X, 2015, AAAI, P4083
   Zhu X., 2018, CORR
   Zhu Xiaojin, 2013, P ADV NEUR INF PROC, P1905
   Zilles S, 2011, J MACH LEARN RES, V12, P349
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301046
DA 2019-06-15
ER

PT S
AU Cheng, R
   Wang, ZY
   Fragkiadaki, K
AF Cheng, Ricson
   Wang, Ziyan
   Fragkiadaki, Katerina
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Geometry-Aware Recurrent Neural Networks for Active Visual Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present recurrent geometry-aware neural networks that integrate visual information across multiple views of a scene into 3D latent feature tensors, while maintaining an one-to-one mapping between 3D physical locations in the world scene and latent feature locations. Object detection, object segmentation, and 3D reconstruction is then carried out directly using the constructed 3D feature memory, as opposed to any of the input 2D images. The proposed models are equipped with differentiable egomotion-aware feature warping and (learned) depth-aware unprojection operations to achieve geometrically consistent mapping between the features in the input frame and the constructed latent model of the scene. We empirically show the proposed model generalizes much better than geometry unaware LSTM/GRU networks, especially under the presence of multiple objects and cross-object occlusions. Combined with active view selection policies, our model learns to select informative viewpoints to integrate information from by "undoing" cross-object occlusions, seamlessly combining geometry with learning from experience.
C1 [Cheng, Ricson; Wang, Ziyan; Fragkiadaki, Katerina] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Cheng, R (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ricsonc@andrew.cmu.edu; ziyanw1@andrew.cmu.edu; katef@cs.cmu.edu
CR Aloimonos J., 1988, IJCV
   Ammirato P., 2017, ICRA
   Bajcsy R., 1988, P IEEE
   Caicedo J., 2015, ICCV
   Chang A. X., 2015, ARXIV151203012CSGR S
   Cho K., 2014, CORR
   Denzler J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P400
   Godard C., 2016, CORR
   Gonzalez-Garcia A, 2015, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2015.7298921
   Gordon D., 2017, CORR
   Gupta S., 2017, P IEEE C COMP VIS PA, P2616
   Hadsell Raia, 2006, CVPR
   Harley A. W., 2015, CORR
   Held R., 1963, J COMP PHYSL PSYCHOL
   Henriques J. F., 2018, P IEEE C COMP VIS PA
   Hoffman J., 2017, CORR
   JAYARAMAN D, 2016, EUR C COMP VIS, V9909, P489, DOI DOI 10.1007/978-3-319-46454-1_30
   Jayaraman D., 2017, CORR
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kar A., 2017, CORR
   Kerl C., 2013, IROS
   Malmir M., 2015, BMVC
   Mathe S., 2016, CVPR
   Mirowski P., 2016, ARXIV161103673
   Mnih V., 2014, CORR
   Parisotto E., 2017, CORR
   Qi C. R., 2016, CORR
   Ranzato M, 2014, ARXIV14055488
   Ren S., 2015, CORR
   Rivlin E., 2000, IJCV
   Schops T., 2014, ISMAR
   Soatto S., 2009, ICCV
   Song Shuran, 2017, IEEE C COMP VIS PATT
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tatler BW, 2011, J VISION, V11, DOI 10.1167/11.5.5
   Tulsiani S., 2017, CORR
   Tung H. F., 2017, ICCV
   Vijayanarasimhan Sudheendra, 2017, ARXIV170407804
   Wilkes D., 1992, CVPR
   Wong L. L., 2013, ROB SCI SYST RSS WOR
   Wu J., 2017, CORR
   Wu J., 2018, INT J COMPUTER VISIO
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Zhou  T., 2017, UNSUPERVISED LEARNIN
   Zhu Y., 2017, ICRA
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305012
DA 2019-06-15
ER

PT S
AU Cheng, Y
   Diakonikolas, I
   Kane, DM
   Stewart, A
AF Cheng, Yu
   Diakonikolas, Ilias
   Kane, Daniel M.
   Stewart, Alistair
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Robust Learning of Fixed-Structure Bayesian Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We investigate the problem of learning Bayesian networks in a robust model where an epsilon-fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.
C1 [Cheng, Yu] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.
   [Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Kane, Daniel M.] Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.
RP Cheng, Y (reprint author), Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.
EM yucheng@cs.duke.edu; ilias.diakonikolas@gmail.com; dakane@ucsd.edu;
   stewart.al@gmail.com
FU NSF [CCF-1527084, CCF-1535972, CCF-1637397, CCF-1704656, IIS-1447554];
   NSF CAREER Award [CCF-1750140, CCF-1652862, CCF-1553288]; Sloan Research
   Fellowship
FX We are grateful to Daniel Hsu for suggesting the model of Bayes nets,
   and for pointing us to [Das97]. Yu Cheng is supported in part by NSF
   CCF-1527084, CCF-1535972, CCF-1637397, CCF-1704656, IIS-1447554, and NSF
   CAREER Award CCF-1750140. Ilias Diakonikolas is supported by NSF CAREER
   Award CCF-1652862 and a Sloan Research Fellowship. Daniel Kane is
   supported by NSF CAREER Award CCF-1553288 and a Sloan Research
   Fellowship.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Acharya J., 2016, P 33 INT C MACH LEAR, V48, P2878
   Acharya J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1278
   Anandkumar A., 2012, P 27 ANN C NEUR INF, P1061
   BALAKRISHNAN S., 2017, P 30 C LEARN THEOR C, P169
   Beinlich IA, 1989, ALARM MONITORING SYS
   BERNHOLT T., 2006, TECHNICAL REPORT
   Bresler G., 2015, P 47 ANN ACM S THEOR, P771
   Bresler G., 2014, NIPS, P2852
   Bresler G, 2013, SIAM J COMPUT, V42, P563, DOI 10.1137/100796029
   Canonne C. L., 2017, P 30 C LEARN THEOR C, P370
   CHAN S., 2014, NIPS, P1844
   Chan S., 2014, STOC, P604, DOI DOI 10.1145/2591796.2591848
   Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380
   Chen M., 2015, CORR
   Chen MJ, 2016, ELECTRON J STAT, V10, P3752, DOI 10.1214/16-EJS1216
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Daly R, 2011, KNOWL ENG REV, V26, P99, DOI 10.1017/S0269888910000251
   Dasgupta S, 1997, MACH LEARN, V29, P165, DOI 10.1023/A:1007417612269
   Daskalakis C., 2014, THEORY COMPUTING, V10, P535
   Diakonikolas I., 2018, C LEARN THEOR COLT 2, P819
   Diakonikolas I., 2018, CORR
   Diakonikolas I., 2017, P 34 INT C MACH LEAR, P999
   Diakonikolas I., 2018, P 29 ACM SIAM S DISC
   Diakonikolas I., 2018, ARXIV180302815
   Diakonikolas I., 2016, P 57 IEEE S FDN COMP
   Diakonikolas I, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1061, DOI 10.1145/3188745.3188754
   Diakonikolas I, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1047, DOI 10.1145/3188745.3188758
   Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16
   Hampel F. R., 1986, ROBUST STAT APPROACH
   Hardt M., 2013, C LEARN THEOR, V30, P354
   Hopkins SB, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1021, DOI 10.1145/3188745.3188748
   Huber P. J., 2009, ROBUST STAT
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Jensen F.V., 2007, BAYESIAN NETWORKS DE
   Johnson D. S., 1978, Theoretical Computer Science, V6, P93, DOI 10.1016/0304-3975(78)90006-3
   Klivans A., 2018, P 31 ANN C LEARN THE, P1420
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kothari PK, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1035, DOI 10.1145/3188745.3188970
   Lai K. A., 2016, P 57 IEEE S FDN COMP
   Liu L., 2018, CORR
   Loh P.-L., 2012, NIPS, P2096
   Neapolitan R. E., 2003, LEARNING BAYESIAN NE
   Prasad A., 2018, ARXIV180206485
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Tukey JW, 1975, P INT C MATH, P523
   Wainwright M. J., 2006, P 12 ADV NEUR INF PR, P1465
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004080
DA 2019-06-15
ER

PT S
AU Cheron, G
   Alayrac, JB
   Laptev, I
   Schmid, C
AF Cheron, Guilhem
   Alayrac, Jean-Baptiste
   Laptev, Ivan
   Schmid, Cordelia
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A flexible model for training action localization with varying levels of
   supervision
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less-demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.
C1 [Cheron, Guilhem; Alayrac, Jean-Baptiste; Laptev, Ivan] PSL Res Univ, CNRS, Ecole Normale Super, INRIA, F-75005 Paris, France.
   [Cheron, Guilhem; Schmid, Cordelia] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
RP Cheron, G (reprint author), PSL Res Univ, CNRS, Ecole Normale Super, INRIA, F-75005 Paris, France.; Cheron, G (reprint author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
FU ERC; MSR-Inria joint lab; Louis Vuitton ENS Chair on Artificial
   Intelligence; Amazon academic research award; Intel gift; DGA project
   DRAAF
FX We thank Remi Leblond for his thoughtful comments and help with the
   paper. This work was supported in part by ERC grants ACTIVIA and
   ALLEGRO, the MSR-Inria joint lab, the Louis Vuitton ENS Chair on
   Artificial Intelligence, an Amazon academic research award, the Intel
   gift and the DGA project DRAAF.
CR Alayrac Jean-Baptiste, 2016, CVPR
   Arbelaez Pablo, 2014, CVPR
   Bach F., 2007, NIPS
   Bojanowski P., 2014, ECCV
   Bojanowski Piotr, 2013, ICCV
   Carreira J., 2017, CVPR
   Chen Wei, 2015, ICCV
   Chunhui Gu, 2018, CVPR
   Duchenne Olivier, 2009, CVPR
   Frank Marguerite, 1956, NAVAL RES LOGISTICS
   Gkioxari G., 2015, CVPR
   Gorban  A., 2015, THUMOS CHALLENGE ACT
   Huang De-An, 2016, ECCV
   Jiong Yang, 2017, ICCV
   Joulin A., 2010, CVPR
   Kaiming He, 2016, CVPR
   Kalogeiton Vicky, 2017, ICCV
   Kay Will, 2017, CORR
   Lacoste-Julien S., 2013, ICML
   Laptev I., 2007, ICCV
   Limin Wang, 2017, CVPR
   Lin T.-Y., 2014, ECCV
   Linli Xu, 2004, NIPS
   Lucas B. D., 1981, IJCAI
   Mettes P., 2016, ECCV
   Mettes Pascal, 2017, BMVC
   Miech A., 2017, ICCV
   Oneata Dan, 2014, ECCV
   Osokin Anton, 2016, ICML
   Peng Xiaojiang, 2016, ECCV
   Richard Alexander, 2017, CVPR
   Rui Hou, 2017, ICCV
   Russakovsky Olga, 2015, IJCV
   Saha Suman, 2017, ICCV
   Saha Suman, 2016, BMVC
   Shaoqing Ren, 2015, NIPS
   Singh Gurkirt, 2017, ICCV
   Singh Krishna Kumar, 2017, ICCV
   Siva Parthipan, 2011, BMVC
   Soomro K., 2012, CORR
   Soomro Khurram, 2017, ICCV
   Uijlings J., 2013, IJCV
   Van Gemert JC, 2015, BMVC
   Weinzaepfel P., 2015, ICCV
   Weinzaepfel Philippe, 2016, CORR
   Yan Ke, 2005, ICCV
   Zolfaghari Mohammadreza, 2017, ICCV
NR 47
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300087
DA 2019-06-15
ER

PT S
AU Cheung, YK
AF Cheung, Yun Kuen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multiplicative Weights Updates with Constant Step-Size in Graphical
   Constant-Sum Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REPLICATOR; COMPLEXITY
AB Since Multiplicative Weights (MW) updates are the discrete analogue of the continuous Replicator Dynamics (RD), some researchers had expected their qualitative behaviours would be similar. We show that this is false in the context of graphical constant-sum games, which include two-person zero-sum games as special cases. In such games which have a fully-mixed Nash Equilibrium (NE), it was known that RD satisfy the permanence and Poincare recurrence properties, but we show that MW updates with any constant step-size epsilon > 0 converge to the boundary of the state space, and thus do not satisfy the two properties. Using this result, we show that MW updates have a regret lower bound of Omega(1/(epsilon T)), while it was known that the regret of RD is upper bounded by O(1/T).
   Interestingly, the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game, if it has a unique NE which is fully mixed, then we show, via regret, that for any sufficiently small epsilon, there exist at least two probability densities and a constant Z > 0, such that for any arbitrarily small z > 0, each of the two densities fluctuates above Z and below z infinitely often.
C1 [Cheung, Yun Kuen] Singapore Univ Technol & Design, Singapore, Singapore.
RP Cheung, YK (reprint author), Singapore Univ Technol & Design, Singapore, Singapore.
EM yunkuen_cheung@sutd.edu.sg
FU Singapore NRF 2018 Fellowship [NRF-NRFF2018-07]; MOE AcRF
   [2016-T2-1-170]
FX The author would like to acknowledge Singapore NRF 2018 Fellowship
   NRF-NRFF2018-07 and MOE AcRF Tier 2 Grant 2016-T2-1-170. The author
   thanks the anonymous reviewers for their helpful suggestions and
   comments, and for pointing out the prior work about continuous
   replicator/FTRL dynamics and the interplay between them and their
   discrete counterparts.
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Bailey James P., 2018, P 2018 ACM C EC COMP, P321
   Benaim M, 1999, LECT NOTES MATH, V1709, P1
   Cai Y, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P217
   Chastain E., 2013, ITCS, P57
   Chen XH, 2009, J MECH PHYS SOLIDS, V57, P1, DOI 10.1016/j.jmps.2008.10.008
   Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003
   Daskalakis C, 2009, LECT NOTES COMPUT SC, V5556, P423, DOI 10.1007/978-3-642-02930-1_35
   Daskalakis C, 2009, SIAM J COMPUT, V39, P195, DOI 10.1137/070699652
   Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163
   GAUNERSDORFER A, 1995, GAME ECON BEHAV, V11, P279, DOI 10.1006/game.1995.1052
   GAUNERSDORFER A, 1992, SIAM J APPL MATH, V52, P1476, DOI 10.1137/0152085
   Hart S, 2010, GAME ECON BEHAV, V69, P107, DOI 10.1016/j.geb.2007.12.002
   Hofbauer J, 1987, DYNAMICAL SYSTEMS, V287, P70
   Hofbauer J, 2009, MATH OPER RES, V34, P263, DOI 10.1287/moor.1080.0359
   Kwon J, 2017, J DYN GAMES, V4, P125, DOI 10.3934/jdg.2017008
   Mertikopoulos Panayotis, 2018, P 29 ANN ACM SIAM S, P2703
   Nagarajan SG, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P685
   NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529
   Palaiopanos Gerasimos, 2017, NIPS, V30, P5874
   Piliouras Georgios, 2014, P 25 ANN ACM SIAM S, P861
   Piliouras Georgios, 2018, ITCS
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Sato Y, 2002, P NATL ACAD SCI USA, V99, P4748, DOI 10.1073/pnas.032086299
   SCHUSTER P, 1981, BIOL CYBERN, V40, P1, DOI 10.1007/BF00326675
   Sigmund K, 1998, EVOLUTIONARY GAMES P
   Sorin S, 2009, MATH PROGRAM, V116, P513, DOI 10.1007/s10107-007-0111-y
   Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989
   Zeeman E. C., 1980, LECT NOTES MATH, V819, P472
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303052
DA 2019-06-15
ER

PT S
AU Chien, I
   Pan, C
   Milenkovic, O
AF Chien, I. (Eli)
   Pan, Chao
   Milenkovic, Olgica
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Query K-means Clustering and the Double Dixie Cup Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHM
AB We consider the problem of approximate K-means clustering with outliers and side information provided by same-cluster queries and possibly noisy answers. Our solution shows that, under some mild assumptions on the smallest cluster size, one can obtain an (1 + epsilon)-approximation for the optimal potential with probability at least 1 - delta, where epsilon > 0 and (delta is an element of (0, 1), using an expected number of O(K-3/epsilon delta) noiseless same-cluster queries and comparison-based clustering of complexity O(ndK + K-3/epsilon delta); here, n denotes the number of points and d the dimension of space. Compared to a handful of other known approaches that perform importance sampling to account for small cluster sizes, the proposed query technique reduces the number of queries by a factor of roughly O(K-6/epsilon(3)), at the cost of possibly missing very small clusters. We extend this settings to the case where some queries to the oracle produce erroneous information, and where certain points, termed outliers, do not belong to any clusters. Our proof techniques differ from previous methods used for K-means clustering analysis, as they rely on estimating the sizes of the clusters and the number of points needed for accurate centroid estimation and subsequent nontrivial generalizations of the double Dixie cup problem. We illustrate the performance of the proposed algorithm both on synthetic and real datasets, including MNIST and CIFAR 10.
C1 [Chien, I. (Eli); Pan, Chao; Milenkovic, Olgica] UIUC, Dept ECE, Urbana, IL 61801 USA.
RP Chien, I (reprint author), UIUC, Dept ECE, Urbana, IL 61801 USA.
EM ichien3@illinois.edu; chaopan2@illinois.edu; milenkov@illinois.edu
FU grants 239 SBC Purdue STC Center for Science of Information
   [4101-38050]; NSF [CCF 15-27636]
FX This work was supported in part by the grants 239 SBC Purdue 4101-38050
   STC Center for Science of Information and NSF CCF 15-27636.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15
   Ailon N., 2017, ARXIV170401862
   Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216
   Awasthi P., 2015, ARXIV150203316
   Bradley P., 2000, MICROSOFT RES REDMON, P1
   Dasarathy G., 2015, P C LEARN THEOR, P503
   Doumas AV, 2016, ESAIM-PROBAB STAT, V20, P367, DOI 10.1051/ps/2016016
   Gamlath B., 2018, ARXIV180300926
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Hui S, 2014, COMMUN STAT-THEOR M, V43, P4103, DOI 10.1080/03610926.2012.705941
   Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kim T., 2017, ARXIV170903202
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lee E, 2017, INFORM PROCESS LETT, V120, P40, DOI 10.1016/j.ipl.2016.11.009
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274
   Mazumdar A, 2016, ANN ALLERTON CONF, P738
   Newman D. J., 1960, AM MATH MONTHLY, V67, P58, DOI 10.2307/2308930
   Ray S, 1999, P 4 INT C ADV PATT R, P137
   Shank N. B., 2013, ELECTRON J COMB, V20, P33
   Skala M., 2013, ARXIV13115939
   Szpankowski W., 2017, ADV NEURAL INFORM PR, P5790
   Szpankowski W., 2001, AVERAGE CASE ANAL AL, P442
   Wikipedia contributors, 2018, CHERN BOUND WIK FREE
   Woo KG, 2004, INFORM SOFTWARE TECH, V46, P255, DOI 10.1016/j.infsof.2003.07.003
   Yun S. Y., 2014, COLT, P138
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001021
DA 2019-06-15
ER

PT S
AU Chierichetti, F
   Dasgupta, A
   Haddadan, S
   Kumar, R
   Lattanzi, S
AF Chierichetti, Flavio
   Dasgupta, Anirban
   Haddadan, Shahrzad
   Kumar, Ravi
   Lattanzi, Silvio
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Mallows Models for Top-k Lists
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MIXING TIMES
AB The classic Mallows model is a widely-used tool to realize distributions on permutations. Motivated by common practical situations, in this paper, we generalize Mallows to model distributions on top-k lists by using a suitable distance measure between top-k lists. Unlike many earlier works, our model is both analytically tractable and computationally efficient. We demonstrate this by studying two basic problems in this model, namely, sampling and reconstruction, from both algorithmic and experimental points of view.
C1 [Chierichetti, Flavio; Haddadan, Shahrzad] Sapienza Univ, Rome, Italy.
   [Dasgupta, Anirban] IIT, Gandhinagar, India.
   [Kumar, Ravi] Google, Mountain View, CA USA.
   [Lattanzi, Silvio] Google, Zurich, Switzerland.
RP Chierichetti, F (reprint author), Sapienza Univ, Rome, Italy.
EM flavio@di.uniroma1.it; anirban.dasgupta@gmail.com;
   shahrzad.haddadan@uniroma1.it; ravi.k53@gmail.com; silviolat@gmail.com
FU ERC [DMAP 680153]; SIR Grant [RBSI14Q743]; Google Focused Award;
   "Dipartimenti di Eccellenza 2018-2022" grant
FX Flavio Chierichetti and Shahrzad Haddadan were supported by the ERC
   Starting Grant DMAP 680153, by the SIR Grant RBSI14Q743, by a Google
   Focused Award, and by the "Dipartimenti di Eccellenza 2018-2022" grant
   awarded to the Dipartimento di Informatica at Sapienza.
CR Awasthi P., 2014, ADV NEURAL INFORM PR, P2609
   Benjamini I, 2005, T AM MATH SOC, V357, P3013, DOI 10.1090/S0002-9947-05-03610-X
   Black D, 1987, THEORY COMMITTEES EL
   Braverman M., 2009, 09101191 ARXIV
   Chierichetti F., 2014, P APPR RAND COMB OPT, P604
   Chierichetti F., 2015, P C INN THEOR COMP S, P85
   Critchlow DE, 1985, LECT NOTES STAT, V34
   De Stefani L., 2016, AAAI, P1526
   DeConde R, 2006, STAT APPL GENET MOL, V5
   Doignon JP, 2004, PSYCHOMETRIKA, V69, P33, DOI 10.1007/BF02295838
   Fagin R, 2003, SIAM J DISCRETE MATH, V17, P134, DOI 10.1137/S0895480102412856
   Fahandar M. A., 2017, ICML, P1078
   FLIGNER MA, 1986, J R STAT SOC B, V48, P359
   Hsu DF, 2005, INFORM RETRIEVAL, V8, P449, DOI 10.1007/s10791-005-6994-4
   Ilyas IF, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1391729.1391730
   Klementiev A., 2008, P 25 INT C MACH LEAR, P472
   Kwak Haewoon, 2010, WWW, P591, DOI DOI 10.1145/1772690.1772751
   Lebanon G, 2008, J MACH LEARN RES, V9, P2401
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Lu T, 2014, J MACH LEARN RES, V15, P3783
   Marden JI, 1996, ANAL MODELING RANK D
   Martin R, 2006, COMB PROBAB COMPUT, V15, P411, DOI 10.1017/S0963548305007352
   Meila M, 2010, J MACH LEARN RES, V11, P3481
   Niu S., 2012, CIKM, P2519
   Wilson DB, 2004, ANN APPL PROBAB, V14, P274, DOI 10.1214/aoap/1075828054
   Xia F., 2009, ADV NEURAL INFORM PR, P2098
   Zhang J., 2007, P 16 INT C WORLD WID, P221, DOI DOI 10.1145/1242572.1242603
   Ziegler C.-N., 2005, P 14 INT C WORLD WID, P22, DOI DOI 10.1145/1060745.1060754
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304040
DA 2019-06-15
ER

PT S
AU Chizat, L
   Bach, F
AF Chizat, Lenaic
   Bach, Francis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On the Global Convergence of Gradient Descent for Over-parameterized
   Models using Optimal Transport
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.
C1 [Chizat, Lenaic; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France.
RP Chizat, L (reprint author), PSL Res Univ, ENS, INRIA, Paris, France.
EM lenaic.chizat@inria.fr; francis.bach@inria.fr
FU Region Ile-de-France; European Research Council [SEQUOIA 724063]
FX We acknowledge supports from grants from Region Ile-de-France and the
   European Research Council (grant SEQUOIA 724063).
CR Abraham Ralph, 1967, TRANSVERSAL MAPPINGS
   Absil PA, 2005, SIAM J OPTIMIZ, V16, P531, DOI 10.1137/040605266
   Ambrosio L., 2008, LECT MATH
   BACH F, 2017, J MACH LEARN RES, V18
   Blanchet A, 2018, J FUNCT ANAL, V275, P1650, DOI 10.1016/j.jfa.2018.06.014
   Boyd N, 2017, SIAM J OPTIMIZ, V27, P616, DOI 10.1137/15M1035793
   Bredies K, 2013, ESAIM CONTR OPTIM CA, V19, P190, DOI 10.1051/cocv/2011205
   BROWDER FE, 1983, P SYMP PURE MATH, V39, P49
   Catala P, 2017, J PHYS CONF SER, V904, DOI 10.1088/1742-6596/904/1/012015
   Chu Wang, 2015, ARXIV151002558
   Cohn Donald L., 1980, MEASURE THEORY, V165
   Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10
   de Castro Y, 2012, J MATH ANAL APPL, V395, P336, DOI 10.1016/j.jmaa.2012.05.011
   Duval V, 2015, FOUND COMPUT MATH, V15, P1315, DOI 10.1007/s10208-014-9228-6
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gunasekar S, 2017, ADV NEUR IN, V30
   Haeffele Benjamin D., 2017, P IEEE C COMP VIS PA, P7331
   Hauer Daniel, 2017, ARXIV170703129
   Haykin S, 1994, NEURAL NETWORKS COMP
   Jaggi M, 2013, P INT C MACH LEARN I
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35
   Lasserre Jean-Bernard, 2010, MOMENTS POSITIVE POL, V1
   Li Yuanzhi, 2017, ADV NEURAL INFORM PR, P597
   Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115
   Nitanda Atsushi, 2017, ARXIV171205438
   Poon Clarice, 2018, ARXIV180208464
   Rockafellar RT, 1997, CONVEX ANAL
   Rotskoff Grant M, 2018, ARXIV180500915
   Santambrogio F., 2015, OPTIMAL TRANSPORT AP
   Santambrogio F, 2017, B MATH SCI, V7, P87, DOI 10.1007/s13373-017-0101-1
   SCIEUR D., 2017, ADV NEURAL INFORM PR, V30, P1109
   Sirignano Justin, 2018, ARXIV180501053
   Soltanolkotabi Mahdi, 2018, IEEE T INFORM THEORY
   Soudry  D., 2017, ARXIV170205777
   Venturi Luca, 2018, ARXIV180206384
   Whitney H., 1935, DUKE MATH J, V1, P514
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303007
DA 2019-06-15
ER

PT S
AU Choi, E
   Xiao, C
   Stewart, WF
   Sun, JM
AF Choi, Edward
   Xiao, Cao
   Stewart, Walter F.
   Sun, Jimeng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI MiME: Multilevel Medical Embedding of Electronic Health Records for
   Predictive Healthcare
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems. External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology. To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes. We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings. In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.
C1 [Choi, Edward] Google Brain, Mountain View, CA USA.
   [Xiao, Cao] IBM Res, Cambridge, MA USA.
   [Stewart, Walter F.] HINT Consultants, Orinda, CA USA.
   [Choi, Edward; Sun, Jimeng] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Stewart, Walter F.] Sutter Hlth, Sacramento, CA USA.
RP Choi, E (reprint author), Google Brain, Mountain View, CA USA.
EM edwardchoi@google.com; cxiao@us.ibm.com; wfs502000@yahoo.com;
   jsun@cc.gatech.edu
FU National Science Foundation, award IIS [1418511]; National Science
   Foundation, award CCF [1533768]; National Institute of Health
   [1R01MD011682-01, R56HL138415]; Samsung Scholarship
FX This work was supported by the National Science Foundation, award
   IIS-#1418511 and CCF-#1533768, the National Institute of Health award
   1R01MD011682-01 and R56HL138415, and Samsung Scholarship. We would also
   like to thank Sherry Yan for her helpful comments on the original
   manuscript.
CR Bajor JM, 2017, ICLR
   Bandanau D., 2015, ICLR
   Baytas Inci M, 2017, SIGKDD
   Benton Adrian, 2017, ARXIV171203538
   Caruana R, 1996, ADV NEUR IN, V8, P959
   Che Chao, 2017, SIAM ON DATA MINING
   Che Z., 2015, P 21 ACM SIGKDD INT, P507
   Che Zhengping, 2015, SIGKDD
   Cho  K., 2014, EMNLP
   Choi E., 2016, J AM MED INFORM ASS
   Choi Edward, 2016, NIPS
   Choi Edward, 2016, MLHC
   Choi Edward, 2016, SIGKDD
   Choi Edward, 2017, SIGKDD
   Choi Youngduck, 2016, AMIA SUMM TRANSL SCI
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   Esteban Cristobal, 2016, ICHI
   Farhan Wael, 2016, JMIR MED INFORM
   Fukui A., 2016, EMNLP
   Futoma Joseph, 2015, JBI
   Gao Y, 2016, CVPR
   Harutyunyan H., 2017, ARXIV170307771
   He K., 2016, CVPR
   Kim J. H, 2017, ICLR
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lipton Zachary C, 2016, ICLR
   Ma Fenglong, 2017, SIGKDD
   Miotto Riccardo, 2016, SCI REPORTS
   Ngufor C, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI 2015), P76, DOI 10.1109/ICHI.2015.16
   Nguyen Phuoc, 2018, ARXIV180200948
   Nori N., 2015, P 21 ACM SIGKDD INT, P855
   Pham T., 2017, J BIOMEDICAL INFORM
   Roger Veronique L, 2004, JAMA
   Saito T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118432
   Suresh Harini, 2017, MLHC
   Tenenbaum JB, 2000, NEURAL COMPUTATION
   Tensorflow Team, 2016, OSDI
   Tran Truyen, 2015, J BIOMEDICAL INFORM
   Vijayakrishnan Rajakrishnan, 2014, J CARDIAC FAILURE
NR 39
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304055
DA 2019-06-15
ER

PT S
AU Chow, Y
   Nachum, O
   Duenez-Guzman, E
   Ghavamzadeh, M
AF Chow, Yinlam
   Nachum, Ofir
   Duenez-Guzman, Edgar
   Ghavamzadeh, Mohammad
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Lyapunov-based Approach to Safe Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.
C1 [Chow, Yinlam; Duenez-Guzman, Edgar] DeepMind, London, England.
   [Nachum, Ofir] Google Brain, Mountain View, CA USA.
   [Ghavamzadeh, Mohammad] Facebook AI Res, Menlo Pk, CA USA.
RP Chow, Y (reprint author), DeepMind, London, England.
EM yinlamchow@google.com; ofirnachum@google.com; duenez@google.com;
   mgh@fb.com
CR Abe N., 2010, P 16 ACM SIGKDD C KD, P75, DOI [10.1145/1835804.1835817, DOI 10.1145/1835804.1835817]
   Achiam J., 2017, INT C MACH LEARN
   Altman E, 1998, MATH METHOD OPER RES, V48, P387, DOI 10.1007/s001860050035
   Altman E., 1999, CONSTRAINED MARKOV D, V7
   Amodei Dario, 2016, ARXIV160606565
   Berkenkamp F, 2017, ADV NEURAL INFORM PR, V30, P908
   Bertsekas D.P., 1995, DYNAMIC PROGRAMMING
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chow Y., 2015, J DYNAMIC SYSTEMS ME, V137
   Chow Y., 2015, ARXIV151201629
   Dalal G., 2018, ARXIV180108757
   ELCHAMIE M, 2016, AM CONTR C, P6290
   Fruit R., 2017, AISTATS
   Gabor Z., 1998, INT C MACH LEARN
   Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666
   Glynn P. W., 2008, MARKOV PROCESS RELAT, P195, DOI DOI 10.1214/074921708000000381
   Gu  S., 2016, INT C MACH LEARN, P2829
   Hasselt H. V., 2010, ADV NEURAL INFORM PR, P2613
   Junges S, 2016, LECT NOTES COMPUT SC, V9636, P130, DOI 10.1007/978-3-662-49674-9_8
   Kakade  S., 2002, ICML, P267
   Khalil HK, 1996, NONINEAR SYSTEMS, V2, P5
   LEE J. D, 2017, ARXIV171007406
   Leike Jan, 2017, ARXIV171109883
   Luenberger D.G, 1984, LINEAR NONLINEAR PRO, V2
   Mastronarde N, 2011, IEEE T SIGNAL PROCES, V59, P6262, DOI 10.1109/TSP.2011.2165211
   Mnih V., 2013, ARXIV13125602
   Moldovan T., 2012, ARXIV12054810
   Mossalam Hossam, 2016, ARXIV161002707
   Neely M. J., 2010, SYNTHESIS LECT COMMU, V3, P1, DOI DOI 10.2200/S00271ED1V01Y201006CNT007
   Neu G., 2017, ARXIV170507798
   Ono M, 2015, AUTON ROBOT, V39, P555, DOI 10.1007/s10514-015-9467-7
   Perkins T., 2002, J MACHINE LEARNING R, V3, P803
   Pirotta M., 2013, P INT C MACH LEARN I, P307
   Pirotta M., 2013, ADV NEURAL INFORM PR, P1394
   Regan Kevin, 2009, UAI, P444
   Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987
   Rusu AA, 2015, ARXIV151106295
   Schaul T, 2015, ARXIV151105952
   Scherrer B, 2013, J MACH LEARN RES, V14, P1181
   Schmitt M, 2006, J MACH LEARN RES, V7, P55
   Shani G, 2005, J MACH LEARN RES, V6, P1265
   Tamar A., 2012, INT C MACH LEARN
NR 42
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002062
DA 2019-06-15
ER

PT S
AU Chua, K
   Calandra, R
   McAllister, R
   Levine, S
AF Chua, Kurtland
   Calandra, Roberto
   McAllister, Rowan
   Levine, Sergey
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Reinforcement Learning in a Handful of Trials using Probabilistic
   Dynamics Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PREDICTIVE CONTROL
AB Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).
C1 [Chua, Kurtland; Calandra, Roberto; McAllister, Rowan; Levine, Sergey] Univ Calif Berkeley, Berkeley Artificial Intelligence Res, Berkeley, CA 94720 USA.
RP Chua, K (reprint author), Univ Calif Berkeley, Berkeley Artificial Intelligence Res, Berkeley, CA 94720 USA.
EM kchua@berkeley.edu; roberto.calandra@berkeley.edu;
   rmcallister@berkeley.edu; svlevine@berkeley.edu
CR Abbeel P., 2006, P 23 INT C MACH LEAR, V3, P1
   Agrawal Pulkit, 2016, ADV NEURAL INFORM PR, P5074
   Atkeson C. G., 1997, INT C ROB AUT ICRA
   Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008
   BELLMAN R, 1957, J MATH MECH, V6, P679
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Botev ZI, 2013, HANDB STAT, V31, P35, DOI 10.1016/B978-0-444-53859-8.00003-5
   BROOKS SH, 1958, OPER RES, V6, P244, DOI 10.1287/opre.6.2.244
   Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626
   Camacho E. F., 2013, MODEL PREDICTIVE CON
   Candela JQ, 2003, INT CONF ACOUST SPEE, P701
   Chebotar Y., 2017, INT C MACH LEARN ICM
   Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218
   Depeweg S., 2018, INT C MACH LEARN ICM, P1192
   Depeweg S., 2016, ARXIV E PRINTS
   Dhariwal P., 2017, OPENAI BASELINES
   DRAEGER A, 1995, IEEE CONTR SYST MAG, V15, P61, DOI 10.1109/37.466261
   Efron B., 1994, INTRO BOOTSTRAP
   Finn C., 2016, INT C ROB AUT ICRA
   Fu J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4019, DOI 10.1109/IROS.2016.7759592
   Gal Y., 2017, ADV NEURAL INFORM PR, P3584
   Gal Y., 2016, ICML WORKSH DAT EFF
   Girard A, 2002, ADV NEURAL INFORM PR, V15, P529
   Grancharova A, 2008, AUTOMATICA, V44, P1621, DOI 10.1016/j.automatica.2008.04.002
   Gu  S., 2016, INT C MACH LEARN, P2829
   Haarnoja T., 2018, INT C MACH LEARN ICM, V80, P1856
   Hernandez E., 1990, Proceedings of the 1990 American Control Conference (IEEE Cat. No.90CH2896-9), P2454
   Hernandez-Lobato J. M., 2015, INT C MACH LEARN ICM, P1861
   Hernandez-Lobato J. M., 2016, INT C MACH LEARN ICM, V48, P1511
   Higuera J. C. G., 2018, ARXIV180302291
   Kamthe S., 2018, INT C ART INT STAT A
   Ko J, 2007, IEEE INT CONF ROBOT, P742, DOI 10.1109/ROBOT.2007.363075
   Kocijan J, 2004, P AMER CONTR CONF, P2214
   Kupcsik A. G., 2013, P 27 AAAI C ART INT, P1401
   Kurutach T., 2018, ARXIV180210592
   Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405
   Lenz  I., 2015, ROBOTICS SCI SYSTEMS
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2016, INT C LEARN REPR ICL
   Lin L. - J., 1992, THESIS
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   McAllister R., 2017, ADV NEURAL INFORM PR, P2037
   MILLER WT, 1990, IEEE T ROBOTIC AUTOM, V6, P1, DOI 10.1109/70.88112
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mordatch I, 2016, IEEE INT CONF ROBOT, P242, DOI 10.1109/ICRA.2016.7487140
   Nagabandi A., 2017, ARXIV E PRINTS
   Neal R M, 1995, THESIS
   Nguyen-Tuong D., 2008, ADV NEURAL INFORM PR, V21, P1193
   Osband I., 2016, NIPS WORKSH BAYES DE
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Parmas P., 2018, INT C MACH LEARN ICM, V80, P4062
   Peters J., 2009, ADV NEURAL INFORM PR, V21, P849
   Punjani A, 2015, IEEE INT CONF ROBOT, P3223, DOI 10.1109/ICRA.2015.7139643
   Ramachandran P., 2017, ARXIV171005941
   Rasmussen C. E., 2003, ADV NEURAL INFORM PR, V4, P1
   Schulman  J., 2017, ARXIV170706347
   Thrun S. B., 1992, CMUCS92102
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Wang JB, 2017, INT C ELECTR MACH SY
   Williams G., 2017, INT C ROB AUT ICRA
NR 60
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304074
DA 2019-06-15
ER

PT S
AU Chung, YA
   Weng, WH
   Tong, S
   Glass, J
AF Chung, Yu-An
   Weng, Wei-Hung
   Tong, Schrasing
   Glass, James
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISCOVERY
AB Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the experimental results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.
C1 [Chung, Yu-An; Weng, Wei-Hung; Tong, Schrasing; Glass, James] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Chung, YA (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM andyyuan@mit.edu; ckbjimmy@mit.edu; st9@mit.edu; glass@mit.edu
CR Amodei  D., 2016, ICML
   Artetxe M., 2016, EMNLP
   Artetxe M., 2017, ACL
   Artetxe M., 2018, ICLR
   Bengio S, 2014, INTERSPEECH, P1053
   Bojanowski P, 2016, TACL, V5, P135, DOI [DOI 10.1162/TACL_, DOI 10.1162/tacl_a_00051]
   Cao H., 2016, COLING
   Chan W., 2016, ICASSP
   Chen Y.-C., 2018, ARXIV180310952
   Cho  K., 2014, EMNLP
   Chorowski J. K., 2015, NIPS
   Chung Y.-A., 2018, INTERSPEECH
   Chung  Y.-A., 2017, NIPS ML4AUDIO WORKSH
   Chung YA, 2016, INTERSPEECH, P765, DOI 10.21437/Interspeech.2016-82
   Conneau A., 2018, ICLR
   Dinu G., 2015, ICLR WORKSH TRACK
   Duong L., 2016, EMNLP
   Faruqui M., 2014, EACL
   Goodfellow I., 2014, NIPS
   Graves A., 2013, ICASSP
   Graves A., 2014, ICML
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   He  W., 2017, ICLR
   Jansen A., 2011, ASRU
   Kamper  H., 2016, ICASSP
   Kamper  H., 2017, ASRU
   Kamper H, 2017, COMPUT SPEECH LANG, V46, P154, DOI 10.1016/j.csl.2017.04.008
   Kamper H, 2016, IEEE-ACM T AUDIO SPE, V24, P669, DOI 10.1109/TASLP.2016.2517567
   Kocabiyikoglu A., 2018, LREC
   Kohn A., 2016, LREC
   Lample G., 2018, ICLR
   Lee C.-y., 2015, T ACL, V3, P389
   Levin  K., 2013, ASRU
   Mikolov T., 2013, NIPS
   MIKOLOV Tomas, 2013, ARXIV13094168
   Panayotov V., 2015, ICASSP
   Park AS, 2008, IEEE T AUDIO SPEECH, V16, P186, DOI 10.1109/TASL.2007.909282
   Pennington Jeffrey, 2014, EMNLP
   Rasanen  Okko, 2015, INTERSPEECH
   Settle  S., 2016, SLT
   Smith S. L., 2016, ICLR
   Sun M, 2013, COMPUT SPEECH LANG, V27, P969, DOI 10.1016/j.csl.2012.09.006
   Sutskever  I., 2014, NIPS
   Waibel A, 2008, IEEE SIGNAL PROC MAG, V25, P70, DOI 10.1109/MSP.2008.918415
   Walter O., 2013, ASRU
   Xing C., 2015, NAACL HLT
   Zhang M., 2017, EMNLP
   Zhang M., 2017, ACL
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001087
DA 2019-06-15
ER

PT S
AU Ciccone, M
   Gallieri, M
   Masci, J
   Osendorfer, C
   Gomez, F
AF Ciccone, Marco
   Gallieri, Marco
   Masci, Jonathan
   Osendorfer, Christian
   Gomez, Faustino
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI NAIS-NET: Stable Deep Networks from Non-Autonomous Differential
   Equations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper introduces Non-Autonomous Input-Output Stable Network (NAIS-Net), a very deep architecture where each stacked processing block is derived from a time-invariant non-autonomous dynamical system. Non-autonomy is implemented by skip connections from the block input to each of the unrolled processing stages and allows stability to be enforced so that blocks can be unrolled adaptively to a pattern-dependent processing depth. NAIS-Net induces non-trivial, Lipschitz input-output maps, even for an infinite unroll length. We prove that the network is globally asymptotically stable so that for every initial condition there is exactly one input-dependent equilibrium assuming tank units, and multiple stable equilibria for ReL units. An efficient implementation that enforces the stability under derived conditions for both fully-connected and convolutional layers is also presented. Experimental results show how NAIS-Net exhibits stability in practice, yielding a significant reduction in generalization gap compared to ResNets.
C1 [Ciccone, Marco] Politecn Milan, NNAISENSE SA, Milan, Italy.
   [Gallieri, Marco; Masci, Jonathan; Osendorfer, Christian; Gomez, Faustino] NNAISENSE SA, Lugano, Switzerland.
RP Ciccone, M (reprint author), Politecn Milan, NNAISENSE SA, Milan, Italy.
EM marco.ciccone@polimi.it; marco@nnaisense.com; jonathan@nnaisense.com;
   christian@nnaisense.com; tino@nnaisense.com
CR Ascher U. M., 1998, COMPUTER METHODS ORD, V61
   Baldi P, 1996, ADV NEUR IN, V8, P451
   Battenberg E., 2017, CORR
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chang Bo, 2017, ARXIV171010348
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Cisse  M., 2017, INT C MACH LEARN, P854
   Deng Jia, 2009, C COMP VIS PATT REC
   DOYA K, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P2777, DOI 10.1109/ISCAS.1992.230622
   Figurnov M., 2017, CORR
   Gomez A., 2017, NIPS
   Graves A., 2016, CORR
   Greff K., 2016, ARXIV161207771
   Gregor K., 2010, INT C MACH LEARN ICM
   Haber E., 2017, ARXIV170503341
   Haschke R, 2005, NEUROCOMPUTING, V64, P25, DOI 10.1016/j.neucom.2004.11.030
   He K., 2015, ARXIV150201852
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1991, THESIS
   Horn R. A., 2012, MATRIX ANAL
   Huang GL, 2017, IEEE ICC
   Jastrzebski Stanislaw, 2017, ARXIV171004773
   Kanai  S., 2017, P NEUR INF PROC SYST, P435
   Khalil Hassan K, 2014, NONLINEAR SYSTEMS
   Knight J. N., 2008, STABILITY ANAL RECUR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lang J. K., 1988, P 1988 CONN MOD SUMM, P52
   Larsson G., 2016, ARXIV160507648
   Laurent T., 2016, ARXIV161206212
   Liao Qianli, 2016, ARXIV160403640
   Lu Y., 2018, FINITE LAYER NEURAL
   Miyato T., 2018, INT C LEARN REPR
   Monti F., 2017, CVPR2017
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Singh J, 2016, NEURAL NETWORKS, V74, P58, DOI 10.1016/j.neunet.2015.10.013
   Sontag E.D, 1998, MATH CONTROL THEORY
   Srivastava R. K., 2015, ARXIV150500387
   Steil Jochen J, 1999, INPUT OUTPUT STABILI
   Strogatz S. H., 2015, NONLINEAR DYNAMICS C
   Sutskever I., 2014, CORR
   Szegedy C, 2013, ARXIV13126199
   Tallec C., 2018, INT C LEARN REPR
   Veit A., 2017, CORR
   Vorontsov Eugene, 2017, ARXIV170200071
   Weinan E, 2017, COMMUN MATH STAT, V5, P1, DOI 10.1007/s40304-017-0103-z
   Yann LeCun, 1998, MNIST DATABASE HANDW
   Yoshida Yuichi, 2017, ARXIV170510941
   Zhang XC, 2017, PROC CVPR IEEE, P3900, DOI 10.1109/CVPR.2017.415
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zilly J. G., 2017, P 34 INT C MACH LEAR, P4189
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303006
DA 2019-06-15
ER

PT S
AU Cohen-Addad, V
   Kanade, V
   Mallmann-Trenn, F
AF Cohen-Addad, Vincent
   Kanade, Varun
   Mallmann-Trenn, Frederik
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Clustering Redemption-Beyond the Impossibility of Kleinberg's Axioms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Kleinberg [20] stated three axioms that any clustering procedure should satisfy and showed there is no clustering procedure that simultaneously satisfies all three. One of these, called the consistency axiom, requires that when the data is modified in a helpful way, i.e. if points in the same cluster are made more similar and those in different ones made less similar, the algorithm should output the same clustering. To circumvent this impossibility result, research has focused on considering clustering procedures that have a clustering quality measure (or a cost) and showing that a modification of Kleinberg's axioms that takes cost into account lead to feasible clustering procedures. In this work, we take a different approach, based on the observation that the consistency axiom fails to be satisfied when the "correct" number of clusters changes. We modify this axiom by making use of cost functions to determine the correct number of clusters, and require that consistency holds only if the number of clusters remains unchanged. We show that single linkage satisfies the modified axioms, and if the input is well-clusterable, some popular procedures such as k-means also satisfy the axioms, taking a step towards explaining the success of these objective functions for guiding the design of algorithms.
C1 [Cohen-Addad, Vincent] UPMC Univ Paris 06, Sorbonne Univ, CNRS, LIP6, Paris, France.
   [Kanade, Varun] Univ Oxford, Oxford, England.
   [Mallmann-Trenn, Frederik] MIT, Cambridge, MA 02139 USA.
RP Cohen-Addad, V (reprint author), UPMC Univ Paris 06, Sorbonne Univ, CNRS, LIP6, Paris, France.
EM vincent.cohen-addad@lip6.fr; varunk@cs.ox.ac.uk; mallmann@mit.edu
FU Alan Turing Institute through the EPSRC [EP/N510129/1]; NSF
   [CCF-1461559, CCF-0939370, CCF-1810758]
FX This work was supported in part by the Alan Turing Institute through the
   EPSRC grant EP/N510129/1.; This work was supported in part by NSF Award
   Numbers CCF-1461559, CCF-0939370, and CCF-1810758.
CR Ackerman  M., 2013, JMLR WORKSH C P, P66
   Ackerman  M., 2010, COLT, P270
   Ackerman  M., 2012, AAAI
   Ackerman  M., 2010, ADV NEURAL INFORM PR, P10
   Ackerman  M., 2012, THEORETICAL FDN CLUS
   Alon N, 1997, SIAM J COMPUT, V26, P1733, DOI 10.1137/S0097539794270248
   Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.3.CO;2-K
   Angelidakis H, 2017, ACM S THEORY COMPUT, P438, DOI 10.1145/3055399.3055487
   Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4
   Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006
   Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575
   Ben-David  S., 2015, ARXIV150100437
   Ben-David S., 2009, P ADV NEUR INF PROC, P121
   Ben-David S., 2014, P 31 INT C MACH JMLR, P280
   Bilu Y, 2012, COMB PROBAB COMPUT, V21, P643, DOI 10.1017/S0963548312000193
   Daniely  A., 2012, ARXIV12054891
   Dudoit S, 2002, GENOME BIOL, V3
   Dutta  A., 2017, ARXIV171201241
   Kleinberg J. M., 2002, P NIPS, P463
   Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Meila  M., 2005, P 22 INT C MACH LEAR, P577, DOI DOI 10.1145/1102351.1102424
   Ostrovsky R, 2006, ANN IEEE SYMP FOUND, P165
   Puzicha J, 2000, PATTERN RECOGN, V33, P617, DOI 10.1016/S0031-3203(99)00076-X
   THORNDIKE RL, 1953, PSYCHOMETRIKA, V18, P267
   Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293
   van Laarhoven T, 2014, J MACH LEARN RES, V15, P193
   Weisstein E. W., TREE MATHWORLD WOLFR
   Williams  A., 2015, IS CLUSTERING MATH I
   Zadeh R. B., 2009, P 25 C UNC ART INT, P639
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003011
DA 2019-06-15
ER

PT S
AU Conti, E
   Madhavan, V
   Such, FP
   Lehman, J
   Stanley, KO
   Clune, J
AF Conti, Edoardo
   Madhavan, Vashisht
   Such, Felipe Petroski
   Lehman, Joel
   Stanley, Kenneth O.
   Clune, Jeff
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Improving Exploration in Evolution Strategies for Deep Reinforcement
   Learning via a Population of Novelty-Seeking Agents
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.
C1 [Conti, Edoardo; Madhavan, Vashisht; Such, Felipe Petroski; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff] Uber AI Labs, San Francisco, CA 94107 USA.
RP Conti, E; Madhavan, V (reprint author), Uber AI Labs, San Francisco, CA 94107 USA.
EM edoardo.conti@gmail.com; vashisht@uber.com
CR Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Brockman G., 2016, OPENAI GYM
   Cuccu G, 2011, LECT NOTES COMPUT SC, V6624, P234, DOI 10.1007/978-3-642-20525-5_24
   Cuccu G, 2011, IEEE C EVOL COMPUTAT, P158
   Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422
   Cully A, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P175
   Dauphin Yann, 2014, ABS14062572 ARXIV
   Fortunato M., 2017, ARXIV170610295
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Gomes Jorge, 2014, ARXIV14070577
   Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970
   Huizinga J, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P125, DOI 10.1145/2908812.2908836
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg M., 2017, ARXIV171109846
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kirkpatrick James, 2017, P NATL ACAD SCI USA
   Lange S., 2010, 2010 INT JOINT C NEU, P1
   Lehman J, 2011, GENET EVOL COMPUT, P37
   Lehman J, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P211
   Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025
   Liepins Gunar E., 1990, CONF90071751 ORNL TE
   Liu Y., 2017, ARXIV170402399
   Meyerson Elliot, 2016, P GEN EV COMP C GECC
   Miikkulainen R, 2017, ARXIV170300548
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mouret Jean-Baptiste, 2015, ARXIV150404909
   Naddaf Yavar, 2010, GAME INDEPENDENT AGE
   Ostrovski Georg, 2017, ARXIV170301310
   Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007
   Paquette Phillip, 2016, OPENAI GYM
   Pathak Deepak, 2017, ARXIV170505363
   Plappert M., 2017, ARXIV170601905
   Pugh JK, 2015, GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P967, DOI 10.1145/2739480.2754664
   Pugh Justin K, 2016, QUALITY DIVERSITY NE, V3
   Rechenberg I, 1978, SIMULATIONSMETHODEN, P83
   Rusu AA, 2015, ARXIV151106295
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Salimans T., 2017, ARXIV170303864
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004
   Stadie BC, 2015, ARXIV150700814
   Stanton Christopher, 2016, PLOS ONE
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tang Haoran, 2017, NIPS, P2750
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Velez R, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P737, DOI 10.1145/2576768.2598225
   Velez Roby, 2017, ARXIV170507241
   Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305007
DA 2019-06-15
ER

PT S
AU Cortes, C
   Kuznetsov, V
   Mohri, M
   Storcheus, D
   Yang, S
AF Cortes, Corinna
   Kuznetsov, Vitaly
   Mohri, Mehryar
   Storcheus, Dmitry
   Yang, Scott
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient Gradient Computation for Structured Output Learning with
   Rational and Tropical Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or n-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a naive implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the n-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.
C1 [Cortes, Corinna; Kuznetsov, Vitaly] Google Res, New York, NY 10011 USA.
   [Mohri, Mehryar; Storcheus, Dmitry] Courant Inst, New York, NY 10012 USA.
   [Mohri, Mehryar; Storcheus, Dmitry] Google Res, New York, NY 10012 USA.
   [Yang, Scott] DE Shaw & Co, New York, NY 10036 USA.
   [Yang, Scott] Courant Inst Math Sci, New York, NY USA.
RP Cortes, C (reprint author), Google Res, New York, NY 10011 USA.
EM corinna@google.com; vitalyk@google.com; mohri@cims.nyu.edu;
   dstorcheus@google.com; yangs@cims.nyu.edu
FU NSF [IIS-1618662]
FX This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.
CR Abadi M., 2016, P USENIX
   Allauzen C., 2007, P CIAA
   Amodei D., 2016, P ICML
   Chang K., 2015, P ICML
   Chen Tianqi, 2015, CORR
   Cortes C, 2004, J MACH LEARN RES, V5, P1035
   Cortes C., 2015, P COLT
   Cortes C., 2007, PREDICTING STRUCTURE
   Cortes C., 2016, P NIPS
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Doppa JR, 2014, J MACH LEARN RES, V15, P1317
   Eban E., 2017, P 20 INT C ART INT S, P832
   Gimpel K., 2010, P ACL
   Graves A., 2014, P ICML
   Joachims T., 2005, P ICML
   Joachims T., 2006, NEW ALGORITHMS MACRO, V49, P57
   Jurafsky D., 2009, SPEECH LANGUAGE PROC
   Lafferty J., 2001, P ICML
   Lam M., 2015, CVPR
   Lucchi A., 2013, P CVPR
   Manning C.D., 1999, FDN STAT NATURAL LAN
   McAllester D. A., 2010, P NIPS
   Mohri M., 2003, International Journal of Foundations of Computer Science, V14, P957, DOI 10.1142/S0129054103002114
   Mohri M, 1997, COMPUT LINGUIST, V23, P269
   Mohri M., 2002, Journal of Automata, Languages and Combinatorics, V7, P321
   Mohri M, 2009, MONOGR THEOR COMPUT, P213, DOI 10.1007/978-3-642-01492-5_6
   Nadeau D, 2007, LINGUIST INVESTIG, V30, P3
   Norouzi M., 2016, P NIPS
   Och F. J., 2003, P ACL, V1
   Paszke A., 2017, P NIPS
   Poon H., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P689, DOI 10.1109/ICCVW.2011.6130310
   Prabhavalkar Rohit, 2017, ARXIV171201818
   Ranjbar M, 2013, IEEE T PATTERN ANAL, V35, P911, DOI 10.1109/TPAMI.2012.168
   Ranzato M., 2015, ARXIV151106732
   Ross Stephane, 2011, P AISTATS
   Seide F., 2016, P KDD
   Shen S., 2016, P ACL, V1
   Sutskever I., 2014, P NIPS
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Vert J. P., 2004, KERNEL METHODS COMPU
   Vinyals O., 2015, P NIPS
   Vinyals O., 2015, P CVPR
   Wu Y., 2016, CORR
   Zhang D., 2008, P IJCNLP
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001036
DA 2019-06-15
ER

PT S
AU Crowley, EJ
   Gray, G
   Storkey, A
AF Crowley, Elliot J.
   Gray, Gavin
   Storkey, Amos
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Moonshine: Distilling with Cheap Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.
C1 [Crowley, Elliot J.; Gray, Gavin; Storkey, Amos] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Crowley, EJ (reprint author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
EM elliot.j.crowley@ed.ac.uk; g.d.b.gray@ed.ac.uk; a.storkey@ed.ac.uk
FU European Union [732204]; Swiss State Secretariat for Education, Research
   and Innovation (SERI) [16.0159]
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No. 732204
   (Bonseyes). This work is supported by the Swiss State Secretariat for
   Education, Research and Innovation (SERI) under contract number 16.0159.
   The opinions expressed and arguments employed herein do not necessarily
   reflect the official views of these funding bodies. The authors are
   grateful to Sam Albanie, Luke Darlow, Jack Turner, and the anonymous
   reviewers for their helpful suggestions.
CR Ba L. J., 2014, ADV NEURAL INFORM PR
   Bucilua C, 2006, ACM SIGKDD INT C KNO
   Chollet F, 2016, ARXIV161002357
   Cordts M., 2016, P IEEE C COMP VIS PA
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Denil M., 2013, ADV NEURAL INFORM PR
   Garipov T., 2016, ARXIV161103214
   Han  S., 2016, INT C LEARN REPR
   He K., 2016, P IEEE C COMP VIS PA
   He K., 2016, EUR C COMP VIS
   Hinton G., 2015, ARXIV150302531
   Howard Andrew G., 2017, ARXIV170404861
   Iandola Forrest N., 2016, ARXIV160207360
   Ioannou Y., 2017, P IEEE C COMP VIS PA
   Ioffe S, 2015, INT C MACH LEARN
   Jaderberg M., 2014, BRIT MACH VIS C
   Jin J., 2015, INT C LEARN REPR
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Z., 2017, ARXIV170603912
   Moczulski M., 2016, INT C LEARN REPR
   Romera E., 2017, IEEE INT VEH S GOLD
   Romera E., 2017, IEEE T INTELLIGENT T
   Romero A., 2015, INT C LEARN REPR
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Sainath T. N., 2013, IEEE INT C AC SPEECH
   Sifre L., 2014, THESIS
   Urban G., 2017, INT C LEARN REPR
   Wang  M., 2016, ARXIV160804337
   Xie S., 2017, P IEEE C COMP VIS PA
   Yang Z, 2015, INT CONF ASIC
   Yu F., 2016, INT C LEARN REPR
   Zagoruyko S., 2017, INT C LEARN REPR
   Zagoruyko S, 2016, BRIT MACH VIS C
   Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958
   Zoph Barret, 2018, P IEEE C COMP VIS PA
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302087
DA 2019-06-15
ER

PT S
AU Cui, H
   Marinescu, R
   Khardon, R
AF Cui, Hao
   Marinescu, Radu
   Khardon, Roni
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI From Stochastic Planning to Marginal MAP
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB It is well known that the problems of stochastic planning and probabilistic inference are closely related. This paper makes two contributions in this context. The first is to provide an analysis of the recently developed SOGBOFA heuristic planning algorithm that was shown to be effective for problems with large factored state and action spaces. It is shown that SOGBOFA can be seen as a specialized inference algorithm that computes its solutions through a combination of a symbolic variant of belief propagation and gradient ascent. The second contribution is a new solver for Marginal MAP (MMAP) inference. We introduce a new reduction from MMAP to maximum expected utility problems which are suitable for the symbolic computation in SOGBOFA. This yields a novel algebraic gradient-based solver (AGS) for MMAP. An experimental evaluation illustrates the potential of AGS in solving difficult MMAP problems.
C1 [Cui, Hao] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
   [Marinescu, Radu] IBM Res, Dublin, Ireland.
   [Khardon, Roni] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
RP Cui, H (reprint author), Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
EM hao.cui@tufts.edu; radu.marinescu@ie.ibm.com; rkhardon@iu.edu
FU NSF [IIS-1616280]; Tufts Technology Services
FX This work was partly supported by NSF under grant IIS-1616280. Some of
   the experiments in this paper were performed on the Tufts Linux Research
   Cluster supported by Tufts Technology Services.
CR Boutilier C., 1995, New Directions in AI Planning, P157
   Cui Hao, 2016, P INT JOINT C ART IN
   Cui Hao, 2015, P AAAI C ART INT
   Domshlak C., 2006, P 16 INT C AUT PLANN, P243
   Furmston Thomas, 2010, P AISTATS, P241
   Griewank A, 2008, EVALUATING DERIVATIV
   Keller Thomas, 2013, P INT C AUT PLANN SC
   Kiselev Igor, 2014, P AAAI C ART INT, P3112
   Kolobov Andrey, 2012, P INT C AUT PLANN SC
   Lee Junkyu, 2016, P AAAI C ART INT, P3255
   Lee Junkyu, 2016, P INT S ART S ART IN
   Liu Q., 2012, UNCERTAINTY ARTIFICI, P523
   Liu Q, 2013, J MACH LEARN RES, V14, P3165
   Marinescu Radu, 2017, P 31 AAAI C ART INT, P3775
   Maua DD, 2016, INT J APPROX REASON, V68, P211, DOI 10.1016/j.ijar.2015.03.007
   Mladenov Martin, 2017, P 31 AAAI C ART INT, P1199
   Neumann G., 2011, P 28 INT C MACH LEAR, P817
   Nguyen Duc Thien, 2017, AAAI C ART INT, P3036
   Nitti D, 2015, LECT NOTES ARTIF INT, V9285, P327, DOI 10.1007/978-3-319-23525-7_20
   Park JD, 2004, J ARTIF INTELL RES, V21, P101
   Pearl Judea, 1989, MORGAN KAUFMANN SERI
   Puterman M. L., 1994, MARKOV DECISION PROC
   Sabbadin R, 2012, INT J APPROX REASON, V53, P66, DOI 10.1016/j.ijar.2011.09.007
   Sanner Scott, 2010, RELATIONAL DYN UNPUB
   Toussaint Marc, 2006, P INT C MACH LEARN I
   van de Meent Jan-Willem, 2016, AISTATS, P1195
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303011
DA 2019-06-15
ER

PT S
AU Cui, XD
   Zhang, W
   Tuske, Z
   Picheny, M
AF Cui, Xiaodong
   Zhang, Wei
   Tuske, Zoltan
   Picheny, Michael
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step, it guarantees that the best fitness in the population will never degrade. In addition, individuals in the population optimized with various SGD-based optimizers using distinct hyperparameters in the SGD step are considered as competing species in a coevolution setting such that the complementarity of the optimizers is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition, image recognition and language modeling, using networks with a variety of deep architectures.
C1 [Cui, Xiaodong; Zhang, Wei; Tuske, Zoltan; Picheny, Michael] IBM Res AI, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Cui, XD (reprint author), IBM Res AI, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM cuix@us.ibm.com; weiz@us.ibm.com; Zoltan.Tuske@ibm.com;
   picheny@us.ibm.com
CR Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI 10.1109/4235.985690
   Bottou  L., 2016, ARXIV160604838
   Chrabaszcz P., 2018, ARXIV180208842
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Ebrahimi S., 2017, C ROB LEARN CORL
   Gal Y., 2016, P 30 INT C NEUR INF, P1027
   Garcia-Pedrajas N, 2005, IEEE T EVOLUT COMPUT, V9, P271, DOI 10.1109/TEVC.2005.844158
   Garcia-Pedrajas N, 2003, IEEE T NEURAL NETWOR, V14, P575, DOI 10.1109/TNN.2003.810618
   Goldberg DE, 1989, GENETIC ALGORITHM SE
   Gomez F, 2008, J MACH LEARN RES, V9, P937
   Hansen Nikolaus, 2016, ARXIV160400772
   He K., 2015, C COMP VIS PATT REC
   HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Igel C, 2003, IEEE C EVOL COMPUTAT, P2588
   Jaderberg M., 2017, ARXIV171109846
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Krizhevsky A., 2009, TECHNICAL REPORT
   Laine S., 2017, INT C LEARN REPR ICL
   Lehman J., 2017, ARXIV171206568
   Liang Jason, 2018, ARXIV180303745
   Liu C., 2017, ARXIV171200559
   Loshchilov I., 2016, INT C LEARN REPR ICL
   Loshchilov I, 2017, EVOL COMPUT, V25, P143, DOI 10.1162/EVCO_a_00168
   Loshchilov Ilya, 2017, INT C LEARN REPR ICL
   Ma X., 2017, INT C LEARN REPR ICL
   Merity S., 2018, INT C LEARN REPR ICL
   Miikkulainen R, 2017, ARXIV170300548
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Morse G, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P477, DOI 10.1145/2908812.2908916
   Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543
   Real E., 2017, P 34 INT C MACH LEAR, P2902
   Real E., 2018, ARXIV180201548
   Salimans T., 2017, ARXIV170303864
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Such F. P., 2017, ARXIV171206567
   Suganuma M., 2017, P GEN EV COMP C GECC, P497, DOI DOI 10.1145/3071178.3071229
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Yang ZY, 2008, INFORM SCIENCES, V178, P2985, DOI 10.1016/j.ins.2008.02.017
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
   Zhang X., 2017, ARXIV171206564
   Zolna K., 2018, INT C LEARN REPR ICL
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000054
DA 2019-06-15
ER

PT S
AU Cullina, D
   Bhagoji, AN
   Mittal, P
AF Cullina, Daniel
   Bhagoji, Arjun Nitin
   Mittal, Prateek
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI PAC-learning in the presence of evasion adversaries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of an evasion adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the adversarial VC-dimension. We then show that sample complexity upper bounds from the Fundamental Theorem of Statistical learning can be extended to the case of evasion adversaries, where the sample complexity is controlled by the adversarial VC-dimension. We then explicitly derive the adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimension. Finally, we prove that the adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right.
C1 [Cullina, Daniel; Bhagoji, Arjun Nitin; Mittal, Prateek] Princeton Univ, Princeton, NJ 08544 USA.
RP Cullina, D (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM dcullina@princeton.edu; abhagoji@princeton.edu; pmittal@princeton.edu
FU National Science Foundation [CNS-1553437, CIF-1617286, CNS-1409415];
   Intel; Office of Naval Research through the Young Investigator Program
   (YIP) Award
FX This work was supported by the National Science Foundation under grants
   CNS-1553437, CIF-1617286 and CNS-1409415, by Intel through the Intel
   Faculty Research Award and by the Office of Naval Research through the
   Young Investigator Program (YIP) Award.
CR Abbasi Mahdieh, 2017, ARXIV170206856
   Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Arnab Anurag, 2018, CVPR
   Bagnall Alexander, 2017, ARXIV171204006
   Bhagoji A. N., 2017, ARXIV170402654
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   Biggio B., 2012, P 29 INT C MACH LEAR, P1807
   Biggio B., 2017, ARXIV171203141
   Brendel W., 2018, ICLR
   Brown N., 2017, SCIENCE
   Carlini N., 2016, ARXIV160704311
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini Nicholas, 2018, DLS IEEE SP
   Carlini Nicholas, 2017, ARXIV171108478
   Carlini Nicholas, 2017, AISEC
   Carlini Nicholas, 2016, CORR
   Chen P. - Y., 2017, P 10 ACM WORKSH ART, P15
   Chen  P.-Y., 2018, AAAI
   Chen S. T., 2018, ARXIV180405810
   Cisse Moustapha, 2017, NIPS
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Das Nilaksh, 2018, ARXIV180206816
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Dudley R. M., 1967, J FUNCT ANAL, V1, P290
   Dziugaite G. K., 2016, ARXIV160800853
   Evtimov Ivan, 2018, CVPR
   Fawzi A, 2018, MACH LEARN, V107, P481, DOI 10.1007/s10994-017-5663-3
   Fawzi Alhussein, 2016, NIPS
   Feinman R., 2017, ARXIV170300410
   Fischer Volker, 2017, ICLR WORKSH
   Gilmer Justin, 2018, ICLR
   Gong Z., 2017, ARXIV170404960
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Grosse Kathrin, 2017, Computer Security - ESORICS 2017. 22nd European Symposium on Research in Computer Security. Proceedings: LNCS10493, P62, DOI 10.1007/978-3-319-66399-9_4
   Grosse K., 2017, ARXIV170206280
   HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D
   He Warren, 2018, ICLR WORKSH
   Hein Matthias, 2017, ADV NEURAL INFORM PR, P2263
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang Sandy, 2017, ICLR
   Jagielski Matthew, 2018, IEEE SECURITY PRIVAC
   Julian K. D., 2016, P 35 DIG AV SYST C D, P1, DOI DOI 10.1109/DASC.2016.7778091
   Kantchelian Alex, 2016, P 33 INT C MACH LEAR
   KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052
   Kolter J. Z., 2018, ICML
   Kos J., 2017, ARXIV170206832
   Kos Jernej, 2017, ICLR WORKSH
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kurakin A., 2016, ARXIV160702533
   Lin CY, 2017, INT CONF IMAG VIS
   Liu Q, 2018, IEEE ACCESS, V6, P12103, DOI 10.1109/ACCESS.2018.2805680
   Liu Y., 2017, ICLR
   Lu Jiajun, 2017, ARXIV171202494
   Madry Aleksander, 2018, ICLR
   Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057
   Moosavi-Dezfooli S. - M., 2016, CVPR
   Moosavi-Dezfooli S. M., 2017, CVPR
   Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960
   Mozaffari-Kermani M, 2015, IEEE J BIOMED HEALTH, V19, P1893, DOI 10.1109/JBHI.2014.2344095
   Papernot N., 2016, ARXIV161103814
   Papernot  N., 2016, ARXIV160507277
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Papernot  Nicolas, 2017, P 2017 ACM AS C COMP
   Raghunathan A., 2018, ICLR
   Rubinstein Benjamin I P, 2009, Performance Evaluation Review, V37, P73
   Schmidt Ludwig, 2018, ARXIV180411285
   Shaham Uri, 2018, ARXIV180310840
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sinha A., 2018, ICLR
   Sitawarin Chawin, 2018, DLS IEEE SP
   Smutz Charles, 2016, NDSS
   Szegedy C, 2013, ARXIV13126199
   Tramer  Florian, 2018, ICLR
   Wang QL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1145, DOI 10.1145/3097983.3098158
   Wang Y, 2018, ICML
   Weng T.-W., 2018, ICLR
   Xu W., 2016, P 2016 NETW DISTR SY
   Xu Weilin, 2018, NDSS
   Yuan Xuejing, 2018, USENIX SECURITY
NR 83
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300022
DA 2019-06-15
ER

PT S
AU Cummings, R
   Krehbiel, S
   Mei, YJ
   Tuo, R
   Zhang, WR
AF Cummings, Rachel
   Krehbiel, Sara
   Mei, Yajun
   Tuo, Rui
   Zhang, Wanrong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private Change-Point Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The change-point detection problem seeks to identify distributional changes at an unknown change-point k. in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point detection through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and provide empirical validation of our results.
C1 [Cummings, Rachel; Mei, Yajun; Zhang, Wanrong] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Krehbiel, Sara] Univ Richmond, Richmond, VA 23173 USA.
   [Tuo, Rui] Texas A&M Univ, College Stn, TX 77843 USA.
RP Zhang, WR (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM rachelc@gatech.edu; krehbiel@richmond.edu; ymei@gatech.edu;
   ruituo@tamu.edu; wanrongz@gatech.edu
FU Mozilla Research Grant; NSF [DMS-156443, CMMI-1362876]
FX R.C. and S.K. were supported in part by a Mozilla Research Grant. Y.M.
   and W.Z. were supported in part by NSF grant CMMI-1362876. R.T. was
   supported in part by NSF grant DMS-156443. R.T.'s contribution was
   completed while the author was visiting the Georgia Institute of
   Technology.
CR Bai J, 2003, J APPL ECONOMET, V18, P1, DOI 10.1002/jae.659
   CARLSTEIN E, 1988, ANN STAT, V16, P188, DOI 10.1214/aos/1176350699
   Chan HP, 2017, ANN STAT, V45, P2736, DOI 10.1214/17-AOS1546
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2009, ACM S THEORY COMPUT, P381
   HINKLEY DV, 1970, BIOMETRIKA, V57, P1
   Kulldorff M, 2001, J ROY STAT SOC A STA, V164, P61, DOI 10.1111/1467-985X.00186
   LAI TL, 1995, J R STAT SOC B, V57, P613
   Lai TL, 2001, STAT SINICA, V11, P303
   LORDEN G, 1971, ANN MATH STAT, V42, P1897, DOI 10.1214/aoms/1177693055
   Lund R, 2002, J CLIMATE, V15, P2547, DOI 10.1175/1520-0442(2002)015<2547:DOUCAR>2.0.CO;2
   Mei Y, 2010, BIOMETRIKA, V97, P419, DOI 10.1093/biomet/asq010
   Mei Y., 2008, SEQUENTIAL ANAL, V27, P354, DOI DOI 10.1080/07474940802445790
   Mei YJ, 2006, ANN STAT, V34, P92, DOI 10.1214/009053605000000859
   MOUSTAKIDES GV, 1986, ANN STAT, V14, P1379, DOI 10.1214/aos/1176350164
   PAGE ES, 1954, BIOMETRIKA, V41, P100, DOI 10.2307/2333009
   POLLAK M, 1985, ANN STAT, V13, P206, DOI 10.1214/aos/1176346587
   POLLAK M, 1987, ANN STAT, V15, P749, DOI 10.1214/aos/1176350373
   ROBERTS SW, 1966, TECHNOMETRICS, V8, P411, DOI 10.2307/1266688
   Shewhart W. A, 1980, EC CONTROL QUALITY M
   Shiryaev A. N., 1963, THEORY PROBA ITS APP, V8, P22, DOI [DOI 10.1137/1108002, 10.1137/1108002]
   WELLNER J. A., 1996, WEAK CONVERGENCE
   Zhang NR, 2012, STAT SINICA, V22, P1507, DOI 10.5705/ss.2010.257
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005041
DA 2019-06-15
ER

PT S
AU Cummings, R
   Krehbiel, S
   Lai, KA
   Tantipongpipat, U
AF Cummings, Rachel
   Krehbiel, Sara
   Lai, Kevin A.
   Tantipongpipat, Uthaipon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differential Privacy for Growing Databases
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows [DNPR10, CSS11]. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm of [HR10], which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded.
C1 [Cummings, Rachel; Lai, Kevin A.; Tantipongpipat, Uthaipon] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Krehbiel, Sara] Univ Richmond, Richmond, VA 23173 USA.
RP Cummings, R (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM rachelc@gatech.edu; krehbiel@richmond.edu; kevinlai@gatech.edu;
   tao@gatech.edu
FU Mozilla Research Grant; NSF [CCF-24067E5, CCF-1740776, IIS-1453304];
   Georgia Institute of Technology ARC fellowship
FX R.C. and S.K. supported in part by a Mozilla Research Grant. K.L.
   supported in part by NSF grant IIS-1453304. U.T. supported in part by
   NSF grants CCF-24067E5 and CCF-1740776, and by a Georgia Institute of
   Technology ARC fellowship.
CR Agarwal  Naman, 2017, INT C MACH LEARN ICM
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Bassily  Raef, 2016, P 48 ANN ACM S THEOR
   Blum A, 2008, ACM S THEORY COMPUT, P609
   Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24
   Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626
   Cummings R, 2016, C LEARN THEOR JUN, P772
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2015, SCIENCE, V349, P636, DOI 10.1126/science.aaa9375
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2009, ACM S THEORY COMPUT, P381
   Dwork  Cynthia, 2010, P 42 ACM S THEOR COM
   Guha Thakurta A., 2013, NIPS, V26, P2733
   Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85
   Jain  Prateek, 2012, P IEEE INT C POW EL, P1
   Jameson GJO, 2016, MATH GAZ, V100, P298, DOI 10.1017/mag.2016.67
   Ji  Zhanglong, 2014, 14127584 ARXIV
   Kifer D., 2012, J MACH LEARN RES, V23, P1
   Shalev-Shwartz  Shai, 2009, P 22 ANN C LEARN THE
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003042
DA 2019-06-15
ER

PT S
AU Cutkosky, A
   Busa-Fekete, R
AF Cutkosky, Ashok
   Busa-Fekete, Robert
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Distributed Stochastic Optimization via Adaptive SGD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques. Our analysis yields a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial online learning algorithm, streamlining prior analysis and allowing us to leverage the significant progress that has been made in designing adaptive algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.
C1 [Cutkosky, Ashok] Stanford Univ, Stanford, CA 94305 USA.
   [Busa-Fekete, Robert] Yahoo Res, New York, NY USA.
   [Cutkosky, Ashok] Google, Mountain View, CA 94043 USA.
RP Cutkosky, A (reprint author), Stanford Univ, Stanford, CA 94305 USA.; Cutkosky, A (reprint author), Google, Mountain View, CA 94043 USA.
EM cutkosky@google.com; busafekete@oath.com
CR Agarwal A, 2014, J MACH LEARN RES, V15, P1111
   Babanezhad  R., 2015, ADV NEURAL INFORM PR, P2251
   Beygelzimer Alina, 2018, ALGORITHMIC NOTIFICA
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Chen-Guang He, 2010, Proceedings 2010 IEEE Youth Conference on Information, Computing and Telecommunications (YC-ICT 2010), P351, DOI 10.1109/YCICT.2010.5713117
   Cotter A., 2011, ADV NEURAL INFORM PR, P1647
   Cutkosky Ashok, 2017, P 30 C LEARN THEOR C, V65, P643
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Frostig R., 2015, P C LEARN THEOR PAR, P728
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Lei Lihua, 2016, ARXIV160903261
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Orabona Francesco, 2016, ARXIV160101974
   Reddi Sashank J., 2016, ARXIV160806879
   Ross Stephane, 2013, ARXIV13056646
   Shah V., 2016, ARXIV160306861
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shamir Ohad, 2013, CORR
   Wang Jialei, 2017, CORR
   Zhang Yi, 2015, CORR
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301086
DA 2019-06-15
ER

PT S
AU Dai, B
   Dai, HJ
   He, NA
   Liu, WY
   Liu, Z
   Chen, JS
   Xiao, L
   Song, L
AF Dai, Bo
   Dai, Hanjun
   He, Niao
   Liu, Weiyang
   Liu, Zhen
   Chen, Jianshu
   Xiao, Lin
   Song, Le
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Coupled Variational Bayes via Optimization Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Variational inference plays a vital role in learning graphical models, especially on large-scale datasets. Much of its success depends on a proper choice of auxiliary distribution class for posterior approximation. However, how to pursue an auxiliary distribution class that achieves both good approximation ability and computation efficiency remains a core challenge. In this paper, we proposed coupled variational Bayes which exploits the primal-dual view of the ELBO with the variational distribution class generated by an optimization procedure, which is termed optimization embedding. This flexible function class couples the variational distribution with the original parameters in the graphical models, allowing end-to-end learning of the graphical models by back-propagation through the variational distribution. Theoretically, we establish an interesting connection to gradient flow and demonstrate the extreme flexibility of this implicit distribution family in the limit sense. Empirically, we demonstrate the effectiveness of the proposed method on multiple graphical models with either continuous or discrete latent variables comparing to state-of-the-art methods.
C1 [Dai, Bo; Dai, Hanjun; Liu, Weiyang; Liu, Zhen; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Dai, Bo] Google Brain, Mountain View, CA 94043 USA.
   [He, Niao] Univ Illinois, Champaign, IL USA.
   [Chen, Jianshu] Tencent AI, Shenzhen, Peoples R China.
   [Xiao, Lin] Microsoft Res, Montreal, PQ, Canada.
   [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China.
RP Dai, B (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.; Dai, B (reprint author), Google Brain, Mountain View, CA 94043 USA.
FU NSF [CCF-1755829, CMMI-1761699, IIS-1218749, CAREER IIS-1350983,
   IIS-1639792 EAGER, IIS-1841351 EAGER, CCF-1836822, CNS-1704701]; NIH
   BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA;
   Amazon AWS; Google Cloud; Siemens
FX Part of this work was done when BD was with Georgia Tech. NH is
   supported in part by NSF CCF-1755829 and NSF CMMI-1761699. LS is
   supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF
   CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF IIS-1841351 EAGER, NSF
   CCF-1836822, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA,
   Amazon AWS, Google Cloud and Siemens.
CR Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Belanger David, 2017, ARXIV170305667
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Chen Jianshu, 2015, ADV NEURAL INFORM PR, V28, P1765
   Chien JT, 2018, IEEE T PATTERN ANAL, V40, P318, DOI 10.1109/TPAMI.2017.2677439
   Dai Bo, 2016, ARTIF INTELL, P985
   Dai Bo, 2016, ABS160704579 CORR
   Dinh L., 2016, ARXIV160508803
   Domke J., 2012, J MACHINE LEARNING R, P318
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Gershman SJ, 2012, P 29 INT C MACH LEAR, P663
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Greff  K., 2017, ADV NEURAL INFORM PR, P6694
   Hershey J. R., 2014, ARXIV14092574
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jaakkola Tommi S., 1999, IMPROVING MEAN FIELD, P163
   Jang Eric, 2016, ARXIV161101144
   Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105
   Kim Yoon, 2018, ARXIV180202550
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2013, ARXIV13126114
   Liu Qiang, 2017, ADV NEURAL INFORM PR, P3118
   Maddison Chris J, 2016, ARXIV161100712
   Marino Joseph, 2018, INT C MACH LEARN, P3400
   Mescheder Lars, 2018, ARXIV170104722
   Minka T. P., 2001, THESIS
   Mnih Andriy, 2014, ARXIV14020030
   Neal R. M., 1993, CRGTR931 U TOR DEP C
   Neal RM, 2011, HDB MARKOV CHAIN MON, V2
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Otto Felix, 2001, GEOMETRY DISSIPATIVE
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Rockafellar R. T., 1998, VARIATIONAL ANAL
   Salimans Tim, 2008, INT C MACH LEARN, P1218
   Shapiro Alexander, 2014, LECT STOCHASTIC PROG, V16
   Stoyanov Veselin, 2011, JMLR P, P725
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Tomczak Jakub M., 2016, ARXIV161109630
   Tran Dustin, 2015, ARXIV151106499
   Turner R. E., 2011, BAYESIAN TIME SERIES, V1, P3
   Wainwright M. J., 2003, 649 UC BERK DEP STAT
   Zellner Arnold, 1988, AM STAT, V42
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004027
DA 2019-06-15
ER

PT S
AU Dai, B
   Fidler, S
   Lin, DH
AF Dai, Bo
   Fidler, Sanja
   Lin, Dahua
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Neural Compositional Paradigm for Image Captioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.
C1 [Dai, Bo; Lin, Dahua] Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.
   [Fidler, Sanja] Univ Toronto, Toronto, ON, Canada.
   [Fidler, Sanja] Vector Inst, Hyderabad, India.
   [Fidler, Sanja] NVIDIA, Santa Clara, CA USA.
RP Dai, B (reprint author), Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.
EM bdai@ie.cuhk.edu.hk; fidler@cs.toronto.edu; dhlin@ie.cuhk.edu.hk
FU Big Data Collaboration Research grant from SenseTime Group (CUHK)
   [TS1610626]; General Research Fund (GRF) of Hong Kong [14236516]
FX This work is partially supported by the Big Data Collaboration Research
   grant from SenseTime Group (CUHK Agreement No. TS1610626), the General
   Research Fund (GRF) of Hong Kong (No. 14236516).
CR Anderson P, 2017, ARXIV170707998
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   CARNIE ANDREW, 2013, SYNTAX GENERATIVE IN
   CHEN D, 2014, P 2014 C EMP METH NA, V2014, P740, DOI DOI 10.3115/V1/D14-1082
   Dai B., 2017, ADV NEURAL INFORM PR, V30, P898
   Dai Bo, 2018, P EUR C COMP VIS ECC, P282
   Dai Bo, 2017, P IEEE INT C COMP VI
   Denkowski M., 2014, P 9 WORKSH STAT MACH, P376
   Devlin J., 2015, ARXIV150504467
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   He  K., 2015, ARXIV151203385
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Klein  D., 2003, P 41 ANN M ASS COMP
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova P., 2014, J T ASS COMPUT LINGU, V2, P351
   Li S., 2011, P 15 C COMP NAT LANG, P220
   Lin C. Y., 2004, TEXT SUMMARIZATION B, V8
   Lin Dahua, 2015, BMVC
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   Ling Huan, 2017, NIPS
   Liu P., 2016, ARXIV160505101
   Lu J., 2016, ARXIV161201887
   Manning C.D., 2014, P 52 ANN M ASS COMP, P55, DOI DOI 10.3115/V1/P14-5010
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Petrov S., 2007, HUMAN LANGUAGE TECHN, p404 
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Socher Richard, 2013, P 51 ANN M ASS COMP, P455
   Tan Y. H., 2016, P AS C COMP VIS, P101
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang Yufei, 2017, P IEEE C COMP VIS PA, P7272
   Xu K., 2015, ICML, V14, P77
   Yao Ting, 2016, ARXIV161101646
   Young P., 2014, P TACL, V2, P67
   Yu LC, 2015, IEEE I CONF COMP VIS, P2461, DOI 10.1109/ICCV.2015.283
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300061
DA 2019-06-15
ER

PT S
AU Dai, LQ
   Tang, L
   Xie, Y
   Tang, JH
AF Dai, Longquan
   Tang, Liang
   Xie, Yuan
   Tang, Jinhui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Designing by Training: Acceleration Neural Network for Fast
   High-Dimensional Convolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BILATERAL FILTER; FAST APPROXIMATION
AB The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are continuously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to gCP layers to build AccNet. After training, the activation function g together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results.
C1 [Dai, Longquan; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
   [Tang, Liang] CASA Environm Technol Co Ltd, CASA EM&EW IOT Res Ctr, Wuxi, Peoples R China.
   [Xie, Yuan] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
RP Tang, JH (reprint author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
EM dailongquan@njust.edu.cn; tangl@casaet.com; yuan.xie@ia.ac.cn;
   jinhuitang@njust.edu.cn
FU 973 Program [2014CB347600]; National Natural Science Foundation of China
   [61701235, 61732007, 61522203, 61772275, 61873293, 61772524];
   Fundamental Research Funds for the Central Universities [30917011323];
   Beijing Municipal Natural Science Foundation [4182067]
FX This work was supported by the 973 Program (Project No. 2014CB347600),
   the National Natural Science Foundation of China (Grant No. 61701235,
   61732007, 61522203, 61772275, 61873293 and 61772524), the Fundamental
   Research Funds for the Central Universities (Grant No. 30917011323) and
   the Beijing Municipal Natural Science Foundation (Grant No. 4182067).
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009
   Baek J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866191
   Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38
   Barron Jonathan T., 2015, IEEE C COMP VIS PATT
   Chaudhury KN, 2016, IEEE T IMAGE PROCESS, V25, P2519, DOI 10.1109/TIP.2016.2548363
   Chen J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239554
   Cohen  N., 2016, P 33 INT C MACH LEAR, P955
   Dai LQ, 2016, IEEE T IMAGE PROCESS, V25, P2657, DOI 10.1109/TIP.2016.2549701
   Durand F, 2002, ACM T GRAPHIC, V21, P257
   Elboer Elhanan, 2013, IEEE C COMP VIS PATT
   GADDE R, 2016, EUR C COMP VIS ECCV, V9905, P597, DOI DOI 10.1007/978-3-319-46448-0_36
   GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004
   Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9
   Jampani V., 2017, IEEE C COMP VIS PATT
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, V24, P109
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Porikli Fatih, 2008, IEEE C COMP VIS PATT
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524
   Szeliski R., 2011, COMPUTER VISION ALGO, P812, DOI DOI 10.1007/978-1-84882-935-0.
   Tomasi  C., 1998, IEEE INT C COMP VIS
   Vineet V, 2014, INT J COMPUT VISION, V110, P290, DOI 10.1007/s11263-014-0708-6
   Yang QX, 2015, INT J COMPUT VISION, V112, P307, DOI 10.1007/s11263-014-0764-y
   Yang Qingxiong, 2009, IEEE C COMP VIS PATT
   Zbontar J., 2015, IEEE C COMP VIS PATT
   Zhang B, 2008, IEEE T IMAGE PROCESS, V17, P664, DOI 10.1109/TIP.2008.919949
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301045
DA 2019-06-15
ER

PT S
AU Dan, C
   Liu, LQ
   Aragam, B
   Ravikumar, P
   Xing, EP
AF Dan, Chen
   Liu Leqi
   Aragam, Bryon
   Ravikumar, Pradeep
   Xing, Eric P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Sample Complexity of Semi-Supervised Learning with Nonparametric
   Mixture Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID IDENTIFIABILITY; FINITE; CONVERGENCE
AB We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an Omega(K logK) labeled sample complexity bound without imposing parametric assumptions, where K is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification (K > 2), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator.
C1 [Dan, Chen; Liu Leqi; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA.
RP Dan, C (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM cdan@cs.cmu.edu; leqil@cs.cmu.edu; naragam@cs.cmu.edu;
   pradeepr@cs.cmu.edu; epxing@cs.cmu.edu
FU NSF [IIS-1149803, IIS-1664720, DMS-1264033]; ONR [N000141812861]; NIH
   [R01GM114311, P30DA035778]
FX P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720,
   DMS-1264033, and ONR via N000141812861. E.X. acknowledges the support of
   NIH R01GM114311, P30DA035778.
CR Aragam  Bryon, 2018, ARXIV180204397
   Aragam Bryon, 2016, ARXIV151108963
   Azizyan M, 2013, ANN STAT, V41, P751, DOI 10.1214/13-AOS1092
   BARNDORF.O, 1965, J MATH ANAL APPL, V12, P115, DOI 10.1016/0022-247X(65)90059-4
   Ben-David  Shai, 2008, P 21 ANN C LEARN THE, P33
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   CASTELLI V, 1995, PATTERN RECOGN LETT, V16, P105, DOI 10.1016/0167-8655(94)00074-D
   Castelli V, 1996, IEEE T INFORM THEORY, V42, P2102, DOI 10.1109/18.556600
   CHEN JH, 1995, ANN STAT, V23, P221, DOI 10.1214/aos/1176324464
   Collier O, 2016, J MACH LEARN RES, V17
   Cozman FG, 2003, P 20 INT C MACH LEAR, P99
   Dai Z., 2017, ADV NEURAL INFORM PR, P6513
   Darnstadt  Malte, 2013, UNLABELED DATA DOES
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   FLAJOLET P, 1992, DISCRETE APPL MATH, V39, P207, DOI 10.1016/0166-218X(92)90177-C
   Flammarion N., 2016, ARXIV160702435
   Fogel  F., 2013, ADV NEURAL INFORM PR, P1016
   Globerson  Amir, 2017, C LEARN THEOR, P978
   Hall P, 2003, ANN STAT, V31, P201
   Heinrich  Philippe, 2015, ARXIV150403506
   Ho N., 2016, ARXIV160902655
   Kaariainen M, 2005, LECT NOTES COMPUT SC, V3559, P127, DOI 10.1007/11503415_9
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109
   Lim C. H., 2014, ADV NEURAL INFORM PR, V27, P2168
   Newman D. J., 1960, AM MATH MONTHLY, V67, P58, DOI 10.2307/2308930
   Ho N, 2016, ELECTRON J STAT, V10, P271, DOI 10.1214/16-EJS1105
   Niyogi P, 2013, J MACH LEARN RES, V14, P1229
   Pananjady A, 2016, ANN ALLERTON CONF, P417, DOI 10.1109/ALLERTON.2016.7852261
   Rigollet P, 2007, J MACH LEARN RES, V8, P1369
   Seeger  Matthias, 2000, TECHNICAL REPORT
   Singh A., 2009, P ADV NEUR INF PROC, V21, P1513
   TEICHER H, 1961, ANN MATH STAT, V32, P244, DOI 10.1214/aoms/1177705155
   TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862
   TEICHER H, 1967, ANN MATH STAT, V38, P1300, DOI 10.1214/aoms/1177698805
   Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085
   Wasserman  L., 2008, ADV NEURAL INFORM PR, P801
   Nguyen X, 2013, ANN STAT, V41, P370, DOI 10.1214/12-AOS1065
   YAKOWITZ SJ, 1968, ANN MATH STAT, V39, P209, DOI 10.1214/aoms/1177698520
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003084
DA 2019-06-15
ER

PT S
AU Dann, C
   Jiang, N
   Krishnamurthy, A
   Agarwal, A
   Langford, J
   Schapire, RE
AF Dann, Christoph
   Jiang, Nan
   Krishnamurthy, Akshay
   Agarwal, Alekh
   Langford, John
   Schapire, Robert E.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On Oracle-Efficient PAC RL with Rich Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation-accessing policy and value function classes exclusively through standard optimization primitives-and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE [1], cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.
C1 [Dann, Christoph] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Jiang, Nan] UIUC, Urbana, IL USA.
   [Krishnamurthy, Akshay; Langford, John; Schapire, Robert E.] Microsoft Res, New York, NY USA.
   [Agarwal, Alekh] Microsoft Res, Redmond, WA USA.
   [Jiang, Nan] MSR NYC, New York, NY USA.
RP Dann, C (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM cdann@cdann.net; nanjiang@illinois.edu; akshay@cs.umass.edu;
   alekha@microsoft.com; jcl@microsoft.com; schapire@microsoft.com
CR Agarwal Alekh, 2014, INT C MACH LEARN
   Allwein E., 2000, J MACHINE LEARNING R
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Antos Andras, 2008, MACHINE LEARNING
   Arora Sanjeev, 2012, THEORY COMPUTING
   Auer Peter, 2009, ADV NEURAL INFORM PR
   Azar Mohammad Gheshlaghi, 2017, INT C MACH LEARN
   Azizzadenesheli Kamyar, 2016, ARXIV161103907
   Azizzadenesheli Kamyar, 2016, C LEARN THEOR
   Bagnell J Andrew, 2004, ADV NEURAL INFORM PR
   Beygelzimer A., 2009, ALGORITHMIC LEARNING
   Bo Dai, 2018, P 35 INT C MACH LEAR, P1133
   Brafman R., 2003, J MACHINE LEARNING R
   Chang Kai-Wei, 2015, INT C MACH LEARN
   Dann Christoph, 2017, ADV NEURAL INFORM PR
   Dann Christoph, 2015, ADV NEURAL INFORM PR
   Ernst Damien, 2005, J MACHINE LEARNING R
   Farahmand Amir-Massoud, 2010, ADV NEURAL INFORM PR
   Gordon Geoffrey J, 1995, INT C MACH LEARN
   Grande R., 2014, INT C MACH LEARN
   Gretton Arthur, 2012, J MACHINE LEARNING R
   Grotschel Martin, 1981, COMBINATORICA
   Guo Zhaohan Daniel, 2016, ARTIFICIAL INTELLIGE
   Hsu Daniel, 2010, THESIS
   Jiang Nan, 2017, INT C MACH LEARN
   Johnson Matthew, 2016, INT JOINT C ART INT
   Kakade S., 2003, INT C MACH LEARN
   Kearns M. J., 2002, MACHINE LEARNING
   Kearns Michael, 1999, INT JOINT C ART INT
   Khachiyan Leonid G, 1980, USSR COMPUTATIONAL M
   Krishnamurthy Akshay, 2016, ADV NEURAL INFORM PR
   Langford John, 2005, INT C COMP LEARN THE
   Langford  John, 2008, ADV NEURAL INFORM PR
   Li Lihong, 2006, INT S ART INT MATH
   Mnih V., 2015, NATURE
   Munos Remi, 2008, J MACHINE LEARNING R
   Osband Ian, 2014, ADV NEURAL INFORM PR
   Pazis J., 2013, AAAI C ART INT
   Pazis Jason, 2016, AAAI C ART INT
   Ross Stephane, 2013, THESIS
   Ross Stephane, 2014, ARXIV14065979
   Russo Dan, 2013, ADV NEURAL INFORM PR
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Strehl A. L., 2006, INT C MACH LEARN
   Strehl Alexander L., 2005, INT C MACH LEARN
   Wen Zheng, 2013, ADV NEURAL INFORM PR
   Wen Zheng, 2017, MATH OPERATIONS RES
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301041
DA 2019-06-15
ER

PT S
AU Dasgupta, S
   Dey, A
   Roberts, N
   Sabato, S
AF Dasgupta, Sanjoy
   Dey, Akansha
   Roberts, Nicholas
   Sabato, Sivan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning from discriminative feature feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of learning a multi-class classifier from labels as well as simple explanations that we call discriminative features. We show that such explanations can be provided whenever the target concept is a decision tree, or can be expressed as a particular type of multi-class DNF formula. We present an efficient online algorithm for learning from such feedback and we give tight bounds on the number of mistakes made during the learning process. These bounds depend only on the representation size of the target concept and not on the overall number of available features, which could be infinite. We also demonstrate the learning procedure experimentally.
C1 [Dasgupta, Sanjoy; Dey, Akansha; Roberts, Nicholas] Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA.
   [Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.
RP Dasgupta, S (reprint author), Univ Calif San Diego, Dept Comp Sci & Engn, San Diego, CA 92103 USA.
EM dasgupta@eng.ucsd.edu; n3robert@ucsd.edu; a1dey@ucsd.edu;
   sabatos@cs.bgu.ac.il
FU National Science Foundation [CCF-1813160]; United-States-Israel
   Binational Science Foundation (BSF) [2017641]
FX This research was supported by National Science Foundation grant
   CCF-1813160, and by a United-States-Israel Binational Science Foundation
   (BSF) grant no. 2017641. Part of the work was done while SD and SS were
   at the "Foundations of Machine Learning" program at the Simons Institute
   for the Theory of Computing, Berkeley.
CR Aodha O. Mac, 2018, IEEE C COMP VIS PATT
   Balcan M.-F., 2006, P 23 INT C MACH LEAR
   BLUM A, 1992, MACH LEARN, V9, P373, DOI 10.1023/A:1022653502461
   Branson S., 2010, EUR C COMP VIS
   Croft W. B., 1990, P 13 INT C RES DEV I, P349
   Daniely A., 2016, P 29 C LEARN THEOR C, P815
   Druck G., 2008, P ACM SPEC INT GROUP
   Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843
   Parikh D., 2011, P INT C COMP VIS
   Perona P, 2010, P IEEE, V98, P1526, DOI 10.1109/JPROC.2010.2049621
   PITT L, 1988, J ACM, V35, P965, DOI 10.1145/48014.63140
   Poulis S., 2017, 20 INT C ART INT STA
   Raghavan H, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P841
   Settles B., 2011, EMPIRICAL METHODS NA
   Yang L., 2013, INNOVATIONS THEORETI
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303091
DA 2019-06-15
ER

PT S
AU Dash, S
   Gunluk, O
   Wei, D
AF Dash, Sanjeeb
   Gunluk, Oktay
   Wei, Dennis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Boolean Decision Rules via Column Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID LOGICAL ANALYSIS; FRAMEWORK
AB This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity tradeoff in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.
C1 [Dash, Sanjeeb; Gunluk, Oktay; Wei, Dennis] IBM Res, Yorktown Hts, NY 10598 USA.
RP Dash, S (reprint author), IBM Res, Yorktown Hts, NY 10598 USA.
EM sanjeebd@us.ibm.com; gunluk@us.ibm.com; dwei@us.ibm.com
CR Agrawal Rakesh, 1994, P 20 INT C VER LARG, V12, P487, DOI DOI 10.1055/S-2007-996789
   Angelino E, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P35, DOI 10.1145/3097983.3098047
   Barnhart C, 1998, OPER RES, V46, P316, DOI 10.1287/opre.46.3.316
   Bazaraa MS, 2010, LINEAR PROGRAMMING N
   Bertsimas D, 2017, MACH LEARN, V106, P1039, DOI 10.1007/s10994-017-5633-9
   Bi Jinbo, 2004, P 10 ACM SIGKDD INT, P521
   Bing Liu, 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P80
   Borgelt C., 2005, P 1 INT WORKSH OP SO, P1, DOI DOI 10.1145/1133905.1133907
   Boros E, 2000, IEEE T KNOWL DATA EN, V12, P292, DOI 10.1109/69.842268
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Chen GQ, 2006, DECIS SUPPORT SYST, V42, P674, DOI 10.1016/j.dss.2005.03.005
   Cheng H, 2007, PROC INT CONF DATA, P691
   Clark P., 1989, Machine Learning, V3, P261, DOI 10.1007/BF00116835
   Cohen W. W., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P115
   Cohen WW, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P335
   Conforti M, 2014, GRAD TEXTS MATH, V271, P1, DOI 10.1007/978-3-319-11008-0
   Dash Sanjeeb, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3360, DOI 10.1109/ICASSP.2014.6854223
   Dembczynski K, 2010, DATA MIN KNOWL DISC, V21, P52, DOI 10.1007/s10618-010-0177-7
   Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092
   Domingos P, 1996, MACH LEARN, V24, P141
   Dua D., 2017, UCI MACHINE LEARNING
   Feldman Vitaly, 2012, P C LEARN THEOR COLT
   Frank E., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P144
   Frank E., 2016, DATA MINING PRACTICA
   FREITAS A. A, 2014, ACM SIGKDD EXPLORATI, V15, P1, DOI DOI 10.1145/2594473.2594475
   Friedman JH, 1999, STAT COMPUT, V9, P123, DOI 10.1023/A:1008894516817
   Frieman JH, 2008, ANN APPL STAT, V2, P916, DOI 10.1214/07-AOAS148
   Furnkranz J., 2014, FDN RULE LEARNING
   Hammer PL, 2006, ANN OPER RES, V148, P203, DOI 10.1007/s10479-006-0075-y
   Klivans AR, 2004, J COMPUT SYST SCI, V68, P303, DOI 10.1016/j.jcss.2003.07.007
   Lakkaraju Himabindu, 2016, KDD, V2016, P1675
   Lakkaraju Himabindu, 2017, P MACHINE LEARNING R, V54, P166
   Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848
   Li WM, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P369, DOI 10.1109/ICDM.2001.989541
   Li Xi, 2013, P 30 INT C MACH LEAR
   Malioutov  D., 2013, P INT C MACH LEARN, P765
   Marchand M, 2003, J MACH LEARN RES, V3, P723, DOI 10.1162/jmlr.2003.3.4-5.723
   Muselli M, 2002, IEEE T KNOWL DATA EN, V14, P1258, DOI 10.1109/TKDE.2002.1047766
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peter Clark, 1991, P 5 EUR WORK SESS LE, P151, DOI DOI 10.1007/BFB0017011
   Quinlan J. R, 1993, C4 5 PROGRAMS MACHIN
   Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1023/A:1022607331053
   SALZBERG S, 1991, MACH LEARN, V6, P251, DOI 10.1023/A:1022661727670
   Su Guolong, 2016, P IEEE INT WORKSH MA, P1
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vanderbeck F, 1996, OPER RES LETT, V19, P151, DOI 10.1016/0167-6377(96)00033-8
   Wang F., 2015, JMLR WORKSHOP C P, P1013
   Wang JY, 2005, SIAM PROC S, P205
   Wang T, 2017, J MACH LEARN RES, V18, P1
   Wang Tong, 2015, ARXIV151102210
   Yang Hongyu, 2017, P INT C MACH LEARN I, P1013
   Yin XX, 2003, SIAM PROC S, P331
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304065
DA 2019-06-15
ER

PT S
AU Daskalakis, C
   Panageas, I
AF Daskalakis, Constantinos
   Panageas, Ioannis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Limit Points of (Optimistic) Gradient Descent in Min-Max
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods inmin-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of OGDA-stable critical points is a superset of GDA-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.
C1 [Daskalakis, Constantinos] MIT, CSAIL, Cambridge, MA 02138 USA.
   [Panageas, Ioannis] SUTD, ISTD, Singapore 487371, Singapore.
RP Daskalakis, C (reprint author), MIT, CSAIL, Cambridge, MA 02138 USA.
EM costis@csail.mit.edu; ioannis@sutd.edu.sg
FU NSF [CCF-1617730, IIS-1741137]; Simons Investigator Award; Google
   Faculty Research Award; MIT-IBM Watson AI Lab research grant; SRG ISTD
   [2018 136]
FX Constantinos Daskalakis was supported by NSF awards CCF-1617730 and
   IIS-1741137, a Simons Investigator Award, a Google Faculty Research
   Award, and an MIT-IBM Watson AI Lab research grant. Ioannis Panageas was
   supported by SRG ISTD 2018 136. This work was done when Ioannis was a
   postdoctoral fellow at MIT.
CR Adler I, 2013, INT J GAME THEORY, V42, P165, DOI 10.1007/s00182-012-0328-8
   Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Blackwell D., 1956, PAC J MATH, V6, P1, DOI DOI 10.2140/PJM.1956.6.1
   Brown G. W., 1951, ACTIVITY ANAL PRODUC
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cherukuri A, 2017, SIAM J CONTROL OPTIM, V55, P486, DOI 10.1137/15M1026924
   Daskalakis C, 2018, ICLR
   Galor O, 2007, DISCRETE DYNAMICAL S
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Jin C, 2016, ADV NEURAL INFORM PR, P4116
   Lee Jason D., 2017, CORR
   Lessard L., 2016, SIAM J OPTIMIZATION
   Mai Tung, 2018, EC COMPUTATION EC
   Mehta Ruta, 2015, INNOVATIONS THEORETI
   Mertikopoulos Panayotis, 2018, P 29 ANN ACM SIAM S, P2703
   Nagarajan V., 2017, NIPS
   Palaiopanos Gerasimos, 2017, NIPS, V30, P5874
   Rakhlin Alexander, 2013, COLT, P993
   Ratliff Lillian J., 2013, ALLERTON
   ROBINSON J, 1951, ANN MATH, V54, P296, DOI 10.2307/1969530
   von Neumann J, 1928, MATH ANN, V100, P295
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003076
DA 2019-06-15
ER

PT S
AU Daskalakis, C
   Dikkala, N
   Jayanti, S
AF Daskalakis, Constantinos
   Dikkala, Nishanth
   Jayanti, Siddhartha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI HOGWILD!-Gibbs Can Be PanAccurate
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition [DSOR16]. We investigate whether it can be used to accurately estimate expectations of functions of all the variables of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by O(tau log n), where n is the number of variables in the graphical model, and tau is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in n. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results [DDK17, GLP17, GSS18], we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multiprocessor machine to empirically illustrate our theoretical findings.
C1 [Daskalakis, Constantinos; Dikkala, Nishanth; Jayanti, Siddhartha] MIT, EECS, Cambridge, MA 02139 USA.
   [Daskalakis, Constantinos; Dikkala, Nishanth; Jayanti, Siddhartha] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Daskalakis, C (reprint author), MIT, EECS, Cambridge, MA 02139 USA.; Daskalakis, C (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM costis@csail.mit.edu; nishanthd@csail.mit.edu; jayanti@mit.edu
CR Daskalakis C, 2017, ADV NEUR IN, V30
   Daskalakis C, 2011, PROBAB THEORY REL, V149, P149, DOI 10.1007/s00440-009-0246-2
   Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S
   De C. M. Sa, 2015, ADV NEURAL INFORM PR, P2674
   De Sa Christopher, 2016, JMLR Workshop Conf Proc, V48, P1567
   Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493
   Felsenstein J., 2004, INFERRING PHYLOGENIE
   Geman S., 1986, P INT C MATH, P1496
   Gheissari Reza, 2017, ARXIV170600121
   Gotze Friedrich, 2018, ARXIV180106348
   Johnson M. J., 2013, ADV NEURAL INFORM PR, P2715
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Mania Horia, 2015, ARXIV150706970
   Mitliagkas I, 2015, PROC VLDB ENDOW, V8, P874, DOI 10.14778/2757807.2757812
   Montanari A, 2010, P NATL ACAD SCI USA, V107, P20196, DOI 10.1073/pnas.1004098107
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Noel Cyprien, 2014, NIPS WORKSH DISTR MA
   Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931
   Terenin A., 2015, ARXIV150908999
   Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168
   Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300004
DA 2019-06-15
ER

PT S
AU Davidson, I
   Gourru, A
   Ravi, SS
AF Davidson, Ian
   Gourru, Antoine
   Ravi, S. S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Cluster Description Problem - Complexity Results, Formulations and
   Approximations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Consider the situation where you are given an existing k - way clustering pi. A challenge for explainable AI is to find a compact and distinct explanation of each cluster which in this paper is assumed to use instance-level descriptors/tags from a common dictionary. Since the descriptors/tags were not given to the clustering method, this is not a semi-supervised learning situation. We show that the feasibility problem of testing whether any distinct description (not necessarily the most compact) exists is generally intractable for just two clusters. This means that unless P = NP, there cannot exist an efficient algorithm for the cluster description problem. Hence, we explore ILP formulations for smaller problems and a relaxed but restricted setting that leads to a polynomial time algorithm for larger problems. We explore several extensions to the basic setting such as the ability to ignore some instances and composition constraints on the descriptions of the clusters. We show our formulation's usefulness on Twitter data where the communities were found using social connectivity (i.e. follower relation) but the explanation of the communities is based on behavioral properties of the nodes (i.e. hashtag usage) not available to the clustering method.
C1 [Davidson, Ian] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
   [Gourru, Antoine] Univ Lyon, ERIC, Lyon 2, Lyon, France.
   [Ravi, S. S.] Univ Virginia, Biocomplex Inst, Charlottesville, VA 22903 USA.
   [Davidson, Ian] Coll Lyon, Lyon, France.
   [Ravi, S. S.] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
RP Davidson, I (reprint author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.; Davidson, I (reprint author), Coll Lyon, Lyon, France.
EM davidson@cs.ucdavis.edu; antoine.gourru@univ-lyon2.fr; ssravi0@gmail.com
FU Deep Graph Models of Functional Networks Grant [ONR-N000141812485];
   Functional Network Discovery NSF Grant [IIS-1422218]; NSF DIBBS Grant
   [ACI-1443054]; NSF BIG DATA Grant [IIS-1633028]; NSF EAGER Grant
   [CMMI-1745207]
FX Ian Davidson was an Institute of Advanced Studies Fellow at the
   Collegium de Lyon at the time of writing and was also supported by Deep
   Graph Models of Functional Networks Grant ONR-N000141812485 and
   Functional Network Discovery NSF Grant IIS-1422218. S. S. Ravi was
   supported in part by NSF DIBBS Grant ACI-1443054, NSF BIG DATA Grant
   IIS-1633028 and NSF EAGER Grant CMMI-1745207. The twitter data was
   provided by the ERIC lab at the University of Lyon 2 and was prepared by
   one of the authors (Antoine Gourru). Thanks to Yue Wu (UC Davis) for
   writing the MATLAB code available at www.cs.ucdavis.edu/<SUP>similar
   to</SUP>davidson/description-clustering/nips18_code.
CR Basu S, 2009, CH CRC DATA MIN KNOW, P1
   Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008
   Bodlaender H. L., 1993, Acta Cybernetica, V11, P1
   Chabert Maxime, 2017, Principles and Practice of Constraint Programming. 23rd International Conference, CP 2017. Proceedings: LNCS 10416, P460, DOI 10.1007/978-3-319-66158-2_30
   Chattopadhyay R., 2013, P 30 INT C MACH LEAR, P253
   Davidson I, 2007, DATA MIN KNOWL DISC, V14, P25, DOI 10.1007/s10618-006-0053-7
   Davidson I, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1034
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Fisher D. H., 1987, Machine Learning, V2, P139, DOI 10.1007/BF00114265
   Garey M. R, 1979, COMPUTERS INTRACTABI
   GENNARI JH, 1989, ARTIF INTELL, V40, P11, DOI 10.1016/0004-3702(89)90046-5
   Gilpin S., 2013, P 19 ACM SIGKDD INT, P113, DOI DOI 10.1145/2487575.2487620
   Guns T, 2013, IEEE T KNOWL DATA EN, V25, P402, DOI 10.1109/TKDE.2011.204
   Jha S, 2005, LECT NOTES COMPUT SC, V3679, P397
   Kotthoff  L., 2015, P 14 INT WORKSH CONS, P1
   Langley  Pat, 1996, ELEMENTS MACHINE LEA
   Metivier Jean-Philippe, 2012, Advances in Intelligent Data Analysis XI. Proceedings 11th International Symposium, IDA 2012, P207, DOI 10.1007/978-3-642-34156-4_20
   Mueller M, 2010, LECT NOTES ARTIF INT, V6332, P159, DOI 10.1007/978-3-642-16184-1_12
   Ouali  A., 2016, P 25 INT JOINT C ART, P647
   Qian B, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P569
   Walker PB, 2017, IEEE ACM T COMPUT BI, V14, P534, DOI 10.1109/TCBB.2016.2591549
   Wang XS, 2013, PROC EUR S-STATE DEV, P234, DOI 10.1109/ESSDERC.2013.6818862
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000067
DA 2019-06-15
ER

PT S
AU Dean, S
   Mania, H
   Matni, N
   Recht, B
   Tu, S
AF Dean, Sarah
   Mania, Horia
   Matni, Nikolai
   Recht, Benjamin
   Tu, Stephen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Regret Bounds for Robust Adaptive Control of the Linear Quadratic
   Regulator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider adaptive control of the Linear Quadratic Regulator (LQR), where an unknown linear system is controlled subject to quadratic costs. Leveraging recent developments in the estimation of linear systems and in robust controller synthesis, we present the first provably polynomial time algorithm that provides high probability guarantees of sub-linear regret on this problem. We further study the interplay between regret minimization and parameter estimation by proving a lower bound on the expected regret in terms of the exploration schedule used by any algorithm. Finally, we conduct a numerical study comparing our robust adaptive algorithm to other methods from the adaptive LQR literature, and demonstrate the flexibility of our proposed method by extending it to a demand forecasting problem subject to state constraints.
C1 [Dean, Sarah; Mania, Horia; Matni, Nikolai; Recht, Benjamin; Tu, Stephen] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Dean, S (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
FU NSF Graduate Research Fellowship [DGE 1752814]; NSF CISE Expeditions
   Award [CCF-1730628]; DHS Award [HSHQDC-16-3-00083]; NSF [CCF-1359814];
   ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-17-1-2502]; DARPA
   Fundamental Limits of Learning (Fun LoL) Program; DARPA Lagrange
   Program; Amazon AWS AI Research Award
FX We thank the anonymous reviewers for their feedback, which improved the
   clarity of our presentation. SD is supported by an NSF Graduate Research
   Fellowship under Grant No. DGE 1752814. As part of the RISE lab, HM is
   generally supported in part by NSF CISE Expeditions Award CCF-1730628,
   DHS Award HSHQDC-16-3-00083, and gifts from Alibaba, Amazon Web
   Services, Ant Financial, CapitalOne, Ericsson, GE, Google, Huawei,
   Intel, IBM, Microsoft, Scotiabank, Splunk and VMware. BR is generously
   supported in part by NSF award CCF-1359814, ONR awards N00014-17-1-2191,
   N00014-17-1-2401, and N00014-17-1-2502, the DARPA Fundamental Limits of
   Learning (Fun LoL) and Lagrange Programs, and an Amazon AWS AI Research
   Award.
CR Abbasi-Yadkori Yasin, 2015, C UNC ART INT
   Abbasi-Yadkori Yasin, 2011, C LEARN THEOR
   Abbasi-Yadkori Yasin, 2018, ARXIV180406021
   Abeille Marc, 2017, AISTATS
   Bittanti S., 2006, COMMUNICATIONS INFOR, V6
   Dean S, 2017, ARXIV171001688
   Dean Sarah, 2018, ARXIV180509388
   Faradonbeh Mohamad Kazem Shirani, 2017, ARXIV171107230
   Fazel Maryam, 2018, INT C MACH LEARN
   Fiechter Claude-Nicolas, 1997, C LEARN THEOR
   Ibrahimi Morteza, 2012, NEURAL INFORM PROCES
   Ioannou P.A., 1996, ROBUST ADAPTIVE CONT, V1
   Krstic M., 1995, NONLINEAR ADAPTIVE C
   Matni Nikolai, 2017, IEEE C DEC CONTR
   Osband Ian, 2016, ARXIV160802731
   Simchowitz Max, 2018, C LEARN THEOR
   Tu Stephen, 2018, INT C MACH LEARN
   Wang Y.-S., 2016, ARXIV161004815
   Yi Ouyang, 2017, ARXIV170904047
   Zhou K., 1995, ROBUST OPTIMAL CONTR
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304022
DA 2019-06-15
ER

PT S
AU Defossez, A
   Zeghidour, N
   Usunier, N
   Bottou, L
   Bach, F
AF Defossez, Alexandre
   Zeghidour, Neil
   Usunier, Nicolas
   Bottou, Leon
   Bach, Francis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI SING: Symbol-to-Instrument Neural Generator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recent progress in deep learning for audio synthesis opens the way to models that directly produce the waveform, shifting away from the traditional paradigm of relying on vocoders or MIDI synthesizers for speech or music generation. Despite their successes, current state-of-the-art neural audio synthesizers such as WaveNet and SampleRNN [24, 17] suffer from prohibitive training and inference times because they are based on autoregressive models that generate audio samples one at a time at a rate of 16kHz. In this work, we study the more computationally efficient alternative of generating the waveform frame-by-frame with large strides. We present SING, a lightweight neural audio synthesizer for the original task of generating musical notes given desired instrument, pitch and velocity. Our model is trained end-to-end to generate notes from nearly 1000 instruments with a single decoder, thanks to a new loss function that minimizes the distances between the log spectrograms of the generated and target waveforms. On the generalization task of synthesizing notes for pairs of pitch and instrument not seen during training, SING produces audio with significantly improved perceptual quality compared to a state-of-the-art autoencoder based on WaveNet [4] as measured by a Mean Opinion Score (MOS), and is about 32 times faster for training and 2; 500 times faster for inference.
C1 [Defossez, Alexandre] PSL Res Univ, INRIA, ENS, Facebook AI Res, Paris, France.
   [Zeghidour, Neil] PSL Res Univ, CNRS, INRIA, LSCP,ENS,EHESS,Facebook AI Res, Paris, France.
   [Usunier, Nicolas] Facebook AI Res, Paris, France.
   [Bottou, Leon] Facebook AI Res, New York, NY USA.
   [Bach, Francis] PSL Res Univ, Ecole Normale Super, INRIA, Paris, France.
RP Defossez, A (reprint author), PSL Res Univ, INRIA, ENS, Facebook AI Res, Paris, France.
EM defossez@fb.com; neilz@fb.com; usunier@fb.com; leonb@fb.com;
   francis.bach@ens.fr
CR Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Caracalla  Hugo, 2017, DAFX 2017
   EBCIOGLU K, 1988, COMPUT MUSIC J, V12, P43, DOI 10.2307/3680335
   Engel  Jesse, 2017, 17040129 ARXIV
   Gehring J., 2017, ARXIV170503122
   GOLDSTEIN JL, 1967, J ACOUST SOC AM, V41, P676, DOI 10.1121/1.1910396
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Hadjeres  Gaetan, 2016, 161201010 ARXIV
   Haque  Albert, 2018, 180400047 ARXIV
   Herremans  Dorien, 2016, MORPHEUS AUTOMATIC M
   Hinton  Geoffrey, 2015, 150302531 ARXIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Itakura  Fumitada, 1968, P 6 INT C AC, P280
   Kalchbrenner  Nal, 2018, 180208435 ARXIV
   Kingma D. P., 2015, INT C LEARN REPR
   Macmillan N. A., 2004, DETECTION THEORY USE
   Mehri  Soroush, 2016, 61207837 ARXIV
   Ping W., 2018, P 6 INT C LEARN REPR
   Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416
   Sotelo J., 2017, CHAR2WAV END TO END
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Taigman  Yaniv, 2017, 170706588 ARXIV
   Van Den Oord A., 2016, 160903499 ARXIV
   van den Oord  Aaron, 2017, 171110433 ARXIV
   Wang  Yuxuan, 2017, 170310135 ARXIV
   Williams R. J., 1995, BACKPROPAGATION THEO, V1, P433, DOI DOI 10.1080/02673039508720837
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003058
DA 2019-06-15
ER

PT S
AU Delyon, B
   Portier, F
AF Delyon, Bernard
   Portier, Francois
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Asymptotic optimality of adaptive importance sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Adaptive importance sampling (AIS) uses past samples to update the sampling policy q(t). Each stage t is formed with two steps : (i) to explore the space with n(t) points according to q(t) and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the allocation policy n(t), the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some "oracle" strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.
C1 [Delyon, Bernard] Univ Rennes 1, IRMAR, Rennes, France.
   [Portier, Francois] Univ Paris Saclay, Telecom ParisTech, St Aubin, France.
RP Delyon, B (reprint author), Univ Rennes 1, IRMAR, Rennes, France.
EM bernard.delyon@univ-rennes1.fr; francois.portier@gmail.com
CR Bardenet Remi, 2016, ARXIV160500361
   Cappe O, 2004, J COMPUT GRAPH STAT, V13, P907, DOI 10.1198/106186004X12803
   Cappe O, 2008, STAT COMPUT, V18, P447, DOI 10.1007/s11222-008-9059-x
   Chopin N, 2004, ANN STAT, V32, P2385, DOI 10.1214/009053604000000698
   Cornuet JM, 2012, SCAND J STAT, V39, P798, DOI 10.1111/j.1467-9469.2011.00756.x
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Delyon B, 2016, BERNOULLI, V22, P2177, DOI 10.3150/15-BEJ725
   Douc R, 2007, ANN STAT, V35, P420, DOI 10.1214/009053606000001154
   Douc R., 2007, ESAIM-PROBAB STAT, V11, P427
   Douc R, 2008, ANN STAT, V36, P2344, DOI 10.1214/07-AOS514
   Elvira V., 2015, ARXIV151103095
   Erraqabi Akram, 2016, INT C MACH LEARN, P2121
   Evans Michael, 2000, OXFORD STAT SCI SERI
   GEWEKE J, 1989, ECONOMETRICA, V57, P1317, DOI 10.2307/1913710
   Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737
   Hammersley John Michael, 1964, MONTE CARLO METHODS, P50
   HANSEN LP, 1982, ECONOMETRICA, V50, P1029, DOI 10.2307/1912775
   Hashimoto Tatsunori B, 2018, ARXIV180403761
   Jie Tang, 2010, ADV NEURAL INFORM PR, P1000
   KLOEK T, 1978, ECONOMETRICA, V46, P1, DOI 10.2307/1913641
   Lou Qi, 2017, ADV NEURAL INFORM PR, P3199
   Marin Jean-Michel, 2012, ARXIV12112548
   Neddermeyer JC, 2009, J AM STAT ASSOC, V104, P788, DOI 10.1198/jasa.2009.0122
   Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185
   Oh M.-S., 1992, J STAT COMPUT SIM, V41, P143
   Owen A, 2000, J AM STAT ASSOC, V95, P135, DOI 10.2307/2669533
   Peters J, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P1607
   Portier Francois, 2018, ARXIV180101797
   Richard JF, 2007, J ECONOMETRICS, V141, P1385, DOI 10.1016/j.jeconom.2007.02.007
   Schulman  J., 2015, INT C MACH LEARN, P1889
   van der Vaart A., 1998, CAMBRIDGE SERIES STA, V3
   Veach E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P419
   Zhang P, 1996, J AM STAT ASSOC, V91, P1245, DOI 10.2307/2291743
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303016
DA 2019-06-15
ER

PT S
AU Denevi, G
   Ciliberto, C
   Stamos, D
   Pontil, M
AF Denevi, Giulia
   Ciliberto, Carlo
   Stamos, Dimitris
   Pontil, Massimiliano
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning To Learn Around A Common Mean
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. The goal addressed in LTL is to select an algorithm that works well on tasks sampled from a meta-distribution. In this work, we consider the family of algorithms given by a variant of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We show that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a novel meta-algorithm to efficiently solve it. At each iteration the meta-algorithm processes only one dataset. Specifically, it firstly estimates the stochastic LS objective function, by splitting this dataset into two subsets used to train and test the inner algorithm, respectively. Secondly, it performs a stochastic gradient step with the estimated value. Under specific assumptions, we present a bound for the generalization error of our meta-algorithm, which suggests the right splitting parameter to choose. When the hyper-parameters of the problem are fixed, this bound is consistent as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments confirm our theoretical findings, highlighting the advantage of our approach, with respect to independent task learning.
C1 [Denevi, Giulia; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy.
   [Denevi, Giulia] Univ Genoa, Genoa, Italy.
   [Ciliberto, Carlo] Imperial Coll London, London, England.
   [Ciliberto, Carlo; Stamos, Dimitris; Pontil, Massimiliano] UCL, London, England.
RP Denevi, G (reprint author), Ist Italiano Tecnol, Genoa, Italy.; Denevi, G (reprint author), Univ Genoa, Genoa, Italy.
FU UK Defence Science and Technology Laboratory (Dstl); Engineering and
   Physical Research Council (EPSRC) [EP/P009069/1]
FX This work was supported in part by the UK Defence Science and Technology
   Laboratory (Dstl) and Engineering and Physical Research Council (EPSRC)
   under grant EP/P009069/1. This is part of the collaboration between US
   DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research
   Initiative.
CR Alquier P., 2017, P MACHINE LEARNING R, V54, P261
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Balcan M. -F., 2015, C LEARN THEOR, P191
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Bhatia R., 1997, MATRIX ANAL
   Camoriano R., 2017, INT C ROB AUT
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Cavallanti G, 2010, J MACH LEARN RES, V11, P2901
   Ciliberto C, 2015, PROC CVPR IEEE, P131, DOI 10.1109/CVPR.2015.7298608
   Ciliberto Carlo, 2017, ADV NEURAL INFORM PR, P1986
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Denevi G., 2018, P 34 C UNC ART INT U
   Dieuleveut Aymeric, 2017, J MACHINE LEARNING R, V18, P3520
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Finn  C., 2017, P 34 INT C MACH LEAR, P1126
   Franceschi L., 2018, INT C MACH LEARN, V80, P568
   Herbster M., 2016, ADV NEURAL INFORM PR, P3954
   Jacob L., 2009, ADV NEURAL INFORM PR, V21, P745
   Maurer A, 2005, J MACH LEARN RES, V6, P967
   Maurer A., 2013, INT C MACH LEARN
   Maurer A, 2016, J MACH LEARN RES, V17
   Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7
   McDonald A. M., 2016, J MACHINE LEARNING R, V17, P1
   Pentina A., 2016, ADV NEURAL INFORM PR, P3612
   Pentina A., 2014, P INT C MACH LEARN, P991
   Ravi S., 2017, I5 INT C LEARN REPR
   Rebuffi S. -A., 2017, P CVPR
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Thrun S., 1998, LEARNING LEARN
   Wu Y., 2014, ROBOTICS AUTONOMOUS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004070
DA 2019-06-15
ER

PT S
AU Deng, YT
   Kim, Y
   Chiu, J
   Guo, DM
   Rush, AM
AF Deng, Yuntian
   Kim, Yoon
   Chiu, Justin
   Guo, Demi
   Rush, Alexander M.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Latent Alignment and Variational Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.
C1 [Deng, Yuntian; Kim, Yoon; Chiu, Justin; Guo, Demi; Rush, Alexander M.] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
RP Deng, YT (reprint author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM dengyuntian@seas.harvard.edu; yoonkim@seas.harvard.edu;
   justinchiu@g.harvard.edu; dguo@college.harvard.edu;
   srush@seas.harvard.edu
FU Facebook Research Award (Low Resource NMT); Google AI PhD Fellowship;
   Bloomberg Research Award; NSF [CCF-1704834]; Amazon AWS Research award
FX We are grateful to Sam Wiseman and Rachit Singh for insightful comments
   and discussion, as well as Christian Puhrsch for help with translations.
   This project was supported by a Facebook Research Award (Low Resource
   NMT). YK is supported by a Google AI PhD Fellowship. YD is supported by
   a Bloomberg Research Award. AMR gratefully acknowledges the support of
   NSF CCF-1704834 and an Amazon AWS Research award.
CR Alvarez-Melis David, 2017, P EMNLP
   Anderson Peter, 2018, P CVPR
   Ba J., 2015, P ICLR
   Ba Jimmy, 2015, P NIPS
   Bahdanau D., 2015, P ICLR
   Bahdanau Dzmitry, 2017, P ICLR
   Bahuleyan Hareesh, 2017, ARXIV171208207
   Biao Zhang, 2016, P EMNLP
   Bojar Ondrej, 2017, P 2 C MACH TRANSL AS
   Bornschein Jorg, 2017, P NIPS
   Brown P. F., 1993, Computational Linguistics, V19, P263
   Burda Yuri, 2015, P ICLR
   Cettolo Mauro, 2014, P IWSLT
   Chan W., 2015, ARXIV150801211
   Chen Zhu, 2017, P ICCV
   Cho Kyunghyun, 2015, IEEE T MULTIMEDIA
   Chorowski J. K., 2015, P NIPS
   Chung Junyoung, 2015, P NIPS
   Cohn Trevor, 2016, P NAACL
   Deng Yuntian, 2017, P ICML
   Dyer Chris, 2013, P NAACL
   Edunov Sergey, 2018, P NAACL
   Fraccaro Marco, 2016, P NIPS
   Goyal  A., 2017, P NIPS
   Grathwohl Will, 2018, P ICLR
   Gu Jiatao, 2016, INCORPORATING COPYIN
   Gulcehre C., 2016, ARXIV160700036
   He K., 2016, P CVPR
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Huang Po-Sen, 2018, P ICLR
   Jang Eric, 2017, P ICLR
   Jang Eric, 2016, ARXIV161101144
   Jankowiak Martin, 2018, P ICML
   Kim Y., 2017, P ICLR
   Kim Yoon, 2018, ARXIV180202550
   Kingma  D., 2014, P ICLR
   Kingma D. P., 2015, P ICLR
   Koehn P., 2007, P 45 ANN M ACL INT P, P177, DOI DOI 10.3115/1557769.1557821
   Koehn Philipp, 2017, ARXIV170603872
   Krishnan Rahul G., 2017, P AAAI
   Krishnan Rahul G., 2018, P AISTATS
   Lawson Dieterich, 2018, P ICASSP
   Lee Jason, 2018, ARXIV180206901
   Lei Tao, 2016, P EMNLP
   Lei Yu, 2017, P ICLR
   Lei Yu, 2016, P EMNLP
   Liu Yang, 2017, P TACL
   Luong Minh-Thang, 2015, P EMNLP
   Ma Xuezhe, 2017, P ICLR
   Maddison Chris J., 2017, P ICLR
   Martins Andre F. T., 2016, P ICML
   Mensch Arthur, 2018, P ICML
   Mnih Andriy, 2014, P ICML
   Mnih Andriy, 2016, P ICML
   Mnih Andriy, 2016, ARXIV160206725
   Mnih Volodymyr, 2015, P NIPS
   Niculae Vlad, 2018, P ICML
   Niculae Vlad, 2017, P NIPS
   Novak Roman, 2016, ARXIV161006602
   Pennington J., 2014, P EMNLP
   Raffel Colin, 2017, P ICML
   Ranganath Rajesh, 2014, P AISTATS
   Ren S., 2015, P NIPS
   Rezende Danilo Jimenez, 2014, P ICML
   Rocktaschel Tim, 2016, P ICLR
   Rush Alexander M., 2015, P EMNLP
   Schulz Philip, 2018, P ACL
   Sennrich  R., 2016, P ACL
   Serban Iulian Vlad, 2017, P AAAI
   Shankar Shiv, 2018, P EMNLP
   Shin Bonggun, 2017, P IJCNN
   Srivastava Akash, 2017, P ICLR
   Su Jinsong, 2018, P AAAI
   Sukhbaatar Sainbayar, 2015, P NIPS
   Tu Zhaopeng, 2016, P ACL
   Tucker George, 2017, P NIPS
   Vaswani A., 2017, P NIPS
   Vogel Stephan, 1996, P COLING
   Williams Ronald J., 1992, MACHINE LEARNING, V8
   Wiseman Sam, 2016, P EMNLP
   Wu Shijie, 2018, P EMNLP
   Wu Y., 2016, ARXIV160908144
   Xu Kelvin, 2015, P ICML
   Yang Zichao, 2016, P CVPR
NR 84
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004029
DA 2019-06-15
ER

PT S
AU Deng, ZW
   Chen, JC
   Fu, YF
   Mori, G
AF Deng, Zhiwei
   Chen, Jiacheng
   Fu, Yifang
   Mori, Greg
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Probabilistic Neural Programmed Networks for Scene Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PRODUCTS
AB In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain varied visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations. We propose PNP-Net, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.
C1 [Deng, Zhiwei; Chen, Jiacheng; Fu, Yifang; Mori, Greg] Simon Fraser Univ, Burnaby, BC, Canada.
RP Deng, ZW (reprint author), Simon Fraser Univ, Burnaby, BC, Canada.
EM zhiweid@sfu.ca; jca348@sfu.ca; yifangf@sfu.ca; mori@cs.sfu.ca
CR Andreas J., 2016, CVPR
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chen X., 2016, ARXIV161102731
   Deng Z., 2017, COMPUTER VISION PATT
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Ganin Yaroslav, 2018, CORR
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gregor K., 2016, ADV NEURAL INFORM PR, P3549
   Gulrajani Ishaan, 2017, ICLR
   Hihn J, 2016, AEROSP CONF PROC
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hoffman Matthew D, 2017, INT C MACH LEARN, P1510
   Isola Phillip, 2017, C COMP VIS PATT REC
   Johnson J., 2017, CVPR
   Johnson Justin, 2018, CVPR
   Karacan Levent, 2016, CORR
   Karpathy Andrej, 2014, NIPS
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Le Tuan Anh, 2016, CORR
   Oord  A.v.d., 2016, ARXIV160106759
   Parisotto Emilio, 2016, ARXIV
   Radford A., 2015, ARXIV151106434
   Reed S., 2016, ARXIV160505396
   Reed Scott, 2016, ICLR
   Reed Scott E, 2015, ADV NEURAL INFORM PR
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Salimans Tim, 2017, ARXIV170105517
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Vedantam  Ramakrishna, 2017, ARXIV170510762
   Villegas Ruben, 2017, INT C MACH LEARN ICM
   Walker Jacob, 2017, INT C COMP VIS ICCV
   Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439
   Wu Jiajun, 2017, ADV NEURAL INFORM PR
   Wu  Jiajun, 2017, CVPR
   Zhu JY, 2017, INT C COMP VIS ICCV
   Zhu Shizhan, 2017, CORR
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304007
DA 2019-06-15
ER

PT S
AU Dennis, DK
   Pabbaraju, C
   Simhadri, HV
   Jain, P
AF Dennis, Don Kurian
   Pabbaraju, Chirag
   Simhadri, Harsha Vardhan
   Jain, Prateek
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multiple Instance Learning for Efficient Sequential Data Classification
   on Resource-constrained Devices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of fast and efficient classification of sequential data (such as time-series) on tiny devices, which is critical for various IoT related applications like audio keyword detection or gesture detection. Such tasks are cast as a standard classification task by sliding windows over the data stream to construct data points. Deploying such classification modules on tiny devices is challenging as predictions over sliding windows of data need to be invoked continuously at a high frequency. Each such predictor instance in itself is expensive as it evaluates large models over long windows of data. In this paper, we address this challenge by exploiting the following two observations about classification tasks arising in typical IoT related applications: (a) the "signature" of a particular class (e.g. an audio keyword) typically occupies a small fraction of the overall data, and (b) class signatures tend to be discernible early on in the data. We propose a method, EMI-RNN, that exploits these observations by using a multiple instance learning formulation along with an early prediction technique to learn a model that achieves better accuracy compared to baseline models, while simultaneously reducing computation by a large fraction. For instance, on a gesture detection benchmark [26], EMI-RNN requires 72x less computation than standard LSTM while improving accuracy by 1%. This enables us to deploy such models for continuous real-time prediction on devices as small as a Raspberry Pi0 and Arduino variants, a task that the baseline LSTM could not achieve. Finally, we also provide an analysis of our multiple instance learning algorithm in a simple setting and show that the proposed algorithm converges to the global optima at a linear rate, one of the first such result in this domain. The code for EMI-RNN is available at [14].
C1 [Dennis, Don Kurian; Pabbaraju, Chirag; Simhadri, Harsha Vardhan; Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India.
RP Dennis, DK (reprint author), Microsoft Res, Bengaluru, Karnataka, India.
EM t-dodenn@microsoft.com; t-chpab@microsoft.com; harshasi@microsoft.com;
   prajain@microsoft.com
CR Amores J, 2013, ARTIF INTELL, V201, P81, DOI 10.1016/j.artint.2013.06.003
   Anguita  D., 2013, ESANN
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Auer Peter, 1997, P 29 ANN ACM S THEOR, P314
   Bello J. P., 2018, COMPUTATIONAL ANAL S, P373
   Bhatia Kush, 2015, P 29 ANN C NEUR INF
   Chang Shih-Fu, 2018, INT C LEAR REPR
   Cho K., 2014, ARXIV14091259
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J, 2014, ARXIV14123555
   Collins J., 2016, ARXIV161109913
   D'Ausilio A, 2012, BEHAV RES METHODS, V44, P305, DOI 10.3758/s13428-011-0163-z
   Dachraoui A, 2015, LECT NOTES ARTIF INT, V9284, P433, DOI 10.1007/978-3-319-23528-8_27
   Dennis Don Kurian, 2018, EDGEML LIB CODE EMI
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P655, DOI 10.1109/FOCS.2016.85
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Ghalwash MF, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-195
   Gupta C., 2017, INT C MACH LEARN, V70, P1331
   Halfacree G., 2012, RASPBERRY PI USER GU
   Hammerla N. Y., 2016, ARXIV160408880
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2014, ARXIV14126980
   Kumar Ashish, 2017, 34 INT C MACH LEARN, P1935
   MA SG, 2016, PROC CVPR IEEE, P1942, DOI DOI 10.1109/CVPR.2016.214
   Mori U., 2017, IEEE T NEURAL NETWOR, P1
   Patil Shishir, 2018, TECHNICAL REPORT
   Sabato S, 2012, J MACH LEARN RES, V13, P2999
   Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
   Sainath TN, 2015, 16 ANN C INT SPEECH
   Sak H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1468
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Tang R., 2017, ARXIV171100333
   Tavenard R., 2016, JOINT EUR C MACH LEA, P632
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Warden P., 2017, SPEECH COMMANDS PUBL
   Xing ZZ, 2012, KNOWL INF SYST, V31, P105, DOI 10.1007/s10115-011-0400-x
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005053
DA 2019-06-15
ER

PT S
AU Derezinski, M
   Warmuth, MK
   Hsu, D
AF Derezinski, Michal
   Warmuth, Manfred K.
   Hsu, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Leveraged volume sampling for linear regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID APPROXIMATION; MATRICES
AB Suppose an n x d design matrix in a linear regression problem is given, but the response for each point is hidden unless explicitly requested. The goal is to sample only a small number k << n of the responses, and then produce a weight vector whose sum of squares loss over all points is at most 1 + epsilon times the minimum. When k is very small (e.g., k = d), jointly sampling diverse subsets of points is crucial. One such method called volume sampling has a unique and desirable property that the weight vector it produces is an unbiased estimate of the optimum. It is therefore natural to ask if this method offers the optimal unbiased estimate in terms of the number of responses k needed to achieve a 1 + epsilon loss approximation. Surprisingly we show that volume sampling can have poor behavior when we require a very accurate approximation - indeed worse than some i.i.d. sampling techniques whose estimates are biased, such as leverage score sampling. We then develop a new rescaled variant of volume sampling that produces an unbiased estimate which avoids this bad behavior and has at least as good a tail bound as leverage score sampling: sample size k = O(d log d + d/epsilon) suffices to guarantee total loss at most 1 + epsilon. times the minimum with high probability. Thus we improve on the best previously known sample size for an unbiased estimator, k = O(d(2)/epsilon). Our rescaling procedure leads to a new efficient algorithm for volume sampling which is based on a determinantal rejection sampling technique with potentially broader applications to determinantal point processes. Other contributions include introducing the combinatorics needed for rescaled volume sampling and developing tail bounds for sums of dependent random matrices which arise in the process.
C1 [Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
   [Hsu, Daniel] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
RP Derezinski, M (reprint author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
EM mderezin@berkeley.edu; manfred@ucsc.edu; djhsu@cs.columbia.edu
FU NSF [CCF-1740833, IIS-1619271]
FX Michal Derezinski and Manfred K. Warmuth were supported by NSF grant
   IIS-1619271. Daniel Hsu was supported by NSF grant CCF-1740833.
CR Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096
   Allen-Zhu Z., 2017, P MACHINE LEARNING R, V70, P126
   Ando T., 1987, LINEAR MULTILINEAR A, V21, P345
   Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287
   Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873
   Celis L. E., 2018, ARXIV180204023
   Celis L Elisa, 2016, ARXIV161007183
   CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen  Xue, 2017, ABS171110051 CORR
   Derezinski M, 2018, J MACH LEARN RES, V19, P1
   Derezinski  Michal, 2018, P 21 INT C ART INT S
   Derezinski  Michal, 2017, ADV NEURAL INFORM PR, V30, P3087
   Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681
   Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38
   Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Fedorov Valerii V., 1972, PROBABILITY MATH STA
   Gartrell Mike, 2016, P 10 ACM C REC SYST, P349
   Gross D., 2010, ARXIV10012738
   Harvey N. J., 2014, P 25 ANN ACM SIAM S, P926
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kulesza A., 2011, P INT C MACH LEARN, P1193
   Kulesza Alex, 2012, DETERMINANTAL POINT
   Lee YT, 2015, ANN IEEE SYMP FOUND, P250, DOI 10.1109/FOCS.2015.24
   Li Chengtao, 2017, ADV NEURAL INFORM PR, V30, P5045
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Mariet Z., 2017, ADV NEURAL INFORM PR, V30, P2136
   Nikolov  Aleksandar, 2018, ARXIV180208318
   Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Woodruff D. P., 2014, THEORETICAL COMPUTER, V10, P1
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302051
DA 2019-06-15
ER

PT S
AU Desai, N
   Critch, A
   Russell, S
AF Desai, Nishant
   Critch, Andrew
   Russell, Stuart
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Negotiable Reinforcement Learning for Pareto Optimal Sequential
   Decision-Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB It is commonly believed that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a Pareto optimal policy, i.e. a policy that cannot be improved upon for one principal without making sacrifices for another. Harsanyi's theorem shows that when the principals have a common prior on the outcome distributions of all policies, a Pareto optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. In this paper, we derive a more precise generalization for the sequential decision setting in the case of principals with different priors on the dynamics of the environment. We refer to this generalization as the Negotiable Reinforcement Learning (NRL) framework. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. To gain insight into the dynamics of this new framework, we implement a simple NRL agent and empirically examine its behavior in a simple environment.
C1 [Desai, Nishant] Univ Calif Berkeley, Ctr Human Compatible AI, Berkeley, CA 94720 USA.
   [Critch, Andrew] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
   [Russell, Stuart] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
RP Desai, N (reprint author), Univ Calif Berkeley, Ctr Human Compatible AI, Berkeley, CA 94720 USA.
EM nishantdesai@berkeley.edu; critch@berkeley.edu; russell@cs.berkeley.edu
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Armstrong S, 2016, AI SOC, V31, P201, DOI 10.1007/s00146-015-0590-y
   Baum Seth D, 2016, AQUAT SCI, P1
   BELLMAN R, 1957, DYNAMIC PROGRAMMING
   Bostrom N., 2014, SUPERINTELLIGENCE PA
   Gabor Z., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P197
   Ghavamzadeh Mohammad, 2016, ARXIV E PRINTS
   Hadfield-Menell D., 2016, COOPERATIVE INVERSE
   Harsanyi John C, 1980, ESSAYS ETHICS SOCIAL, P6
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Pineua J., 2003, INT JOINT C ART INT, V3, P1025
   Roijers DM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1666
   Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964
   SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071
   Soh H, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P713
   Tzeng GH, 2011, MULTIPLE ATTRIBUTE DECISION MAKING: METHODS AND APPLICATIONS, P1
   Wang W., 2014, THESIS
   Zhang Chongjie, 2014, ADV NEURAL INFORM PR, P2636
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304070
DA 2019-06-15
ER

PT S
AU Deshpande, Y
   Montanari, A
   Mossel, E
   Sen, S
AF Deshpande, Yash
   Montanari, Andrea
   Mossel, Elchanan
   Sen, Subhabrata
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Contextual Stochastic Block Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COMMUNITY DETECTION; MUTUAL INFORMATION; PHASE-TRANSITION; NETWORKS
AB We provide the first information theoretic tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in the detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretical necessity of combining the different sources of information. Our analysis holds for networks of large degrees as well as for a Gaussian version of the model.
C1 [Deshpande, Yash; Mossel, Elchanan; Sen, Subhabrata] MIT, Dept Math, Cambridge, MA 02139 USA.
   [Montanari, Andrea] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Montanari, Andrea] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Deshpande, Y (reprint author), MIT, Dept Math, Cambridge, MA 02139 USA.
FU [NSF DMS-1613091];  [NSF CCF-1714305];  [NSF IIS-1741162];  [NSF
   DMS-1737944];  [ONR N00014-17-1-2598]
FX A.M. was partially supported by grants NSF DMS-1613091, NSF CCF-1714305
   and NSF IIS-1741162. E.M was partially supported by grants NSF
   DMS-1737944 and ONR N00014-17-1-2598. Y.D would like to acknowledge
   Nilesh Tripuraneni for discussions about this paper.
CR Abbe E, 2014, IEEE TRANS NETW SCI, V1, P10, DOI 10.1109/TNSE.2014.2368716
   Abbe Emmanuel, 2017, ARXIV170310146
   Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Aicher C, 2015, J COMPLEX NETW, V3, P221, DOI 10.1093/comnet/cnu026
   Baik J, 2005, ANN PROBAB, V33, P1643, DOI 10.1214/009117905000000233
   Balasubramanyan R., 2011, P SIAM INT C DAT MIN, V11, P450, DOI DOI 10.1137/1.9781611972818.39
   Benaych-Georges F, 2011, ADV MATH, V227, P494, DOI 10.1016/j.aim.2011.02.007
   Binkiewicz N, 2017, BIOMETRIKA, V104, P361, DOI 10.1093/biomet/asx008
   Bothorel C, 2015, NETW SCI, V3, P408, DOI 10.1017/nws.2015.9
   Capitaine M, 2009, ANN PROBAB, V37, P1, DOI 10.1214/08-AOP394
   Chang J, 2010, ANN APPL STAT, V4, P124, DOI 10.1214/09-AOAS309
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Cheng H, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1921632.1921638
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   Cucuringu M, 2015, J COMPLEX NETW, V3, P469, DOI 10.1093/comnet/cnu050
   Dang T, 2012, INT C DIG SOC ICDS, P7
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Deshpande Y, 2017, INF INFERENCE, V6, P125, DOI 10.1093/imaiai/iaw017
   Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y
   Eaton E., 2012, P 26 AAAI C ART INT, P900
   Gibert J, 2012, PATTERN RECOGN, V45, P3072, DOI 10.1016/j.patcog.2012.01.009
   Gunnemann S, 2013, IEEE DATA MINING, P231, DOI 10.1109/ICDM.2013.110
   Guo DN, 2005, IEEE T INFORM THEORY, V51, P1261, DOI 10.1109/TIT.2005.844072
   Hoff Peter D, 2003, RANDOM EFFECTS MODEL
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Johnstone Iain M, 2004, SPARSE PRINCIP UNPUB
   Kanade V, 2016, IEEE T INFORM THEORY, V62, P5906, DOI 10.1109/TIT.2016.2516564
   Kim  Myunghwan, 2012, ARXIV12054546
   Knowles A, 2013, COMMUN PUR APPL MATH, V66, P1663, DOI 10.1002/cpa.21450
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Kumar V, 2012, MAGN RESON IMAGING, V30, P1234, DOI 10.1016/j.mri.2012.06.010
   Lelarge M, 2015, IEEE TRANS NETW SCI, V2, P152, DOI 10.1109/TNSE.2015.2490580
   Leskovec J., 2012, ADV NEURAL INFORM PR, P539
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Montanari  A., 2012, COMPRESSED SENSING T
   Montanari  Andrea, 2015, ADV NEURAL INFORM PR, P217
   Mossel E, 2013, COMBINATORICA, V38, P1
   Mossel E, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P71, DOI 10.1145/2840728.2840749
   Mossel E, 2015, PROBAB THEORY REL, V162, P431, DOI 10.1007/s00440-014-0576-6
   Neville J, 2003, P TEXT MIN LINK AN W, P9
   Newman MEJ, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11863
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Onatski A, 2013, ANN STAT, V41, P1204, DOI 10.1214/13-AOS1100
   Paul D, 2007, STAT SINICA, V17, P1617
   Peche S, 2006, PROBAB THEORY REL, V134, P127, DOI 10.1007/s00440-005-0466-z
   Peel L., 2012, ARXIV12095561
   Silva A, 2012, PROC VLDB ENDOW, V5, P466, DOI 10.14778/2140436.2140443
   Smith LM, 2016, ACM T KNOWL DISCOV D, V11, DOI 10.1145/2968451
   Talagrand M, 2010, MEAN FIELD MODELS SP, VI
   Tramel Eric W, 2014, ARXIV14095557
   Hoang TA, 2014, LECT NOTES COMPUT SC, V8851, P1, DOI 10.1007/978-3-319-13734-6_1
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Xu Z., 2012, P 2012 ACM SIGMOD IN, P505, DOI DOI 10.1145/2213836.2213894
   Yang J, 2013, IEEE DATA MINING, P1151, DOI 10.1109/ICDM.2013.167
   Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927
   Zanghi H, 2010, PATTERN RECOGN LETT, V31, P830, DOI 10.1016/j.patrec.2010.01.026
   Zhang P, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.052802
   Zhang Y, 2016, ELECTRON J STAT, V10, P3153, DOI 10.1214/16-EJS1206
   Zhou Y., 2009, PVLDB, V2, P718, DOI DOI 10.14778/1687627.1687709
NR 61
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003017
DA 2019-06-15
ER

PT S
AU Detommaso, G
   Cui, TG
   Spantini, A
   Marzouk, Y
   Scheichl, R
AF Detommaso, Gianluca
   Cui, Tiangang
   Spantini, Alessio
   Marzouk, Youssef
   Scheichl, Robert
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Stein variational Newton method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INVERSE PROBLEMS
AB Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]: it minimizes the Kullback-Leibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases.
C1 [Detommaso, Gianluca] Univ Bath, Bath, Avon, England.
   [Detommaso, Gianluca] Alan Turing Inst, London, England.
   [Cui, Tiangang] Monash Univ, Clayton, Vic, Australia.
   [Spantini, Alessio; Marzouk, Youssef] MIT, Cambridge, MA 02139 USA.
   [Scheichl, Robert] Heidelberg Univ, Heidelberg, Germany.
RP Detommaso, G (reprint author), Univ Bath, Bath, Avon, England.; Detommaso, G (reprint author), Alan Turing Inst, London, England.
EM gd391@bath.ac.uk; Tiangang.Cui@monash.edu; spantini@mit.edu;
   ymarz@mit.edu; r.scheichl@uni-heidelberg.de
FU EPSRC Centre for Doctoral Training in Statistical Applied Mathematics at
   Bath [EP/L015684/1]; Alan Turing Institute; AFOSR Computational
   Mathematics Program
FX G. Detommaso is supported by the EPSRC Centre for Doctoral Training in
   Statistical Applied Mathematics at Bath (EP/L015684/1) and by a
   scholarship from the Alan Turing Institute. T. Cui, G. Detommaso, A.
   Spantini, and Y. Marzouk acknowledge support from the MATRIX Program on
   "Computational Inverse Problems" held at the MATRIX Institute,
   Australia, where this joint collaboration was initiated. A. Spantini and
   Y. Marzouk also acknowledge support from the AFOSR Computational
   Mathematics Program.
CR Anderes E., 2012, ARXIV12055314
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Chen W. Y., 2018, INT C MACH LEARN
   Cui T, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/11/114015
   Cui TG, 2016, J COMPUT PHYS, V304, P109, DOI 10.1016/j.jcp.2015.10.008
   Francois D., 2005, P INT S APPL STOCH M, P238
   Gershman S., 2012, ARXIV12064665
   Gilks WR, 1995, MARKOV CHAIN MONTE C
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Han J., 2017, ARXIV170405201
   Khan M. E., 2017, ARXIV171105560
   Khan M. E., 2017, ARXIV171201038
   Liu C., 2017, ARXIV171111216
   Liu Qiang, 2017, ADV NEURAL INFORM PR, P3118
   Liu Qiang, 2016, ADV NEURAL INFORM PR, V29, P2378
   Liu Y., 2017, ARXIV170402399
   Luenberger D.G., 1997, OPTIMIZATION VECTOR
   Martin J, 2012, SIAM J SCI COMPUT, V34, pA1460, DOI 10.1137/110845598
   MARZOUK Y., 2016, HDB UNCERTAINTY QUAN, P1, DOI DOI 10.1007/978-3-319-11259-623-1
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Pu Y., 2017, ARXIV170405155
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Spantini A., 2018, J MACHINE LEARNING R
   Stuart AM, 2010, ACTA NUMER, V19, P451, DOI 10.1017/S0962492910000061
   Tabak E. G., 2013, COMMUNICATIONS PURE, P145
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang D., 2017, ARXIV171107168
   Wright S, 1999, NUMERICAL OPTIMIZATI
   Zhuo J., 2017, ARXIV171104425
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003070
DA 2019-06-15
ER

PT S
AU Deudon, M
AF Deudon, Michel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning semantic similarity in a continuous space
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover's Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to "travel" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves competitive results on Quora duplicate questions dataset. Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.
C1 [Deudon, Michel] Ecole Polytech, Palaiseau, France.
RP Deudon, M (reprint author), Ecole Polytech, Palaiseau, France.
EM michel.deudon@polytechnique.edu
FU Ecole Polytechnique
FX We would like to thank Ecole Polytechnique for financial support and
   Telecom Paris-Tech for GPU resources. We are grateful to Professor Chloe
   Clavel, Professor Gabriel Peyre, Constance Noziere and Paul Bertin for
   their critical reading of the paper. Special thanks go to Magdalena
   Fuentes for helping running the code on Telecom's clusters. We also
   thank Professor Francis Bach and Professor Guillaume Obozinski for their
   insightful course on probabilistic graphical models at ENS Cachan, as
   well as Professor Michalis Vazirgiannis for his course on Text Mining
   and NLP at Ecole Polytechnique. We also thank the reviewers for their
   valuable comments and feedback.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Arora Sanjeev, 2017, INT C LEARN REPR ICL
   Bahdanau  D., 2015, INT C LEARN REPR ICL
   Beaulieu M. M., 1997, Fifth Text REtrieval Conference (TREC-5) (NIST SP 500-238), P143
   Bird S., 2009, NATURAL LANGUAGE PRO
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bowman S.R., 2016, P 20 SIGNLL C COMP N, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Bowman S. R., 2015, P 2015 C EMP METH NA, P632
   Chopra S, 2005, PROC CVPR IEEE, P539
   Conneau A, 2017, P 2017 C EMP METH NA, P670
   Dadashov Elkhan, CS224N
   Dhingra Bhuwan, 2018, P NAACL HLT, P59
   Ganitkevitch J., 2013, P 2013 C N AM CHAPT, P758
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Gong Yichen, 2018, INT C LEARN REPR ICL
   Hill F., 2016, P 2016 C N AM CHAPT, P1367
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Homma Yushi, CS224N
   Kantorovich L. V., 1942, DOKL AKAD NAUK SSSR, V37, P227
   Kim Y, 2014, P 2014 C EMP METH NA, P1746, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kiros R., 2015, ADV NEURAL INFORM PR, P3294
   Kusner M., 2015, P 32 INT C MACH LEAR, P957
   Li J., 2015, LONG PAPERS, V1, P1106
   Lin Z., 2017, INT C LEARN REPR ICL
   Ma Mingbo, 2015, P 53 ANN M ASS COMP, P1106
   Marelli Marco, 2014, LREC, P216
   Miao Y., 2016, P INT C MACH LEARN, P1727
   Mikolov Tomas, 2013, INT C LEARN REPR ICL
   Monge Gaspard, 1781, HIST ACAD ROYALE SCI
   Mu Jiaqi, 2018, INT C LEARN REPR ICL
   Mueller J., 2017, INT C MACH LEARN, P2536
   Nickel Maximillian, 2017, ADV NEURAL INFORM PR, V30, P6341
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Rehurek Radim, 2010, P LREC 2010 WORKSH N, P45, DOI DOI 10.13140/2.1.2393.1847
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Sanborn Adrian, 2015, CS224D
   Shen Dinghan, 2018, 32 AAAI C ART INT AA
   Shen Dinghan, 2018, P 56 ANN M ASS COMP
   SPARCKJONES K, 1972, J DOC, V28, P11, DOI 10.1108/eb026526
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tomar Gaurav Singh, 2017, P 1 WORKSH SUBW CHAR, P142
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Verges- Llahi Jaume, 2005, PATTERN RECOGN, P13
   Villani C., 2003, GRADUATE STUDIES MAT, V58
   Vilnis Luke, 2015, INT C LEARN REPR ILC
   Wang Z, 2017, IEEE IJCNN, P1411, DOI 10.1109/IJCNN.2017.7966018
   Wieting John, 2016, INT C LEARN REPR ICL
   Wieting John, 2018, P 56 ANN M ASS COMP, V1, P451
   Williams Adina, 2018, P NAACL HLT
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301002
DA 2019-06-15
ER

PT S
AU Dezfouli, A
   Morris, R
   Ramos, F
   Dayan, P
   Balleine, BW
AF Dezfouli, Amir
   Morris, Richard
   Ramos, Fabio
   Dayan, Peter
   Balleine, Bernard W.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Integrated accounts of behavioral and neuroimaging data using flexible
   recurrent neural network models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DECISION-MAKING; REWARD; SIGNALS; FMRI
AB Neuroscience studies of human decision-making abilities commonly involve subjects completing a decision-making task while BOLD signals are recorded using fMRI. Hypotheses are tested about which brain regions mediate the effect of past experience, such as rewards, on future actions. One standard approach to this is model-based fMRI data analysis, in which a model is fitted to the behavioral data, i.e., a subject's choices, and then the neural data are parsed to find brain regions whose BOLD signals are related to the model's internal signals. However, the internal mechanics of such purely behavioral models are not constrained by the neural data, and therefore might miss or mischaracterize aspects of the brain. To address this limitation, we introduce a new method using recurrent neural network models that are flexible enough to be jointly fitted to the behavioral and neural data. We trained a model so that its internal states were suitably related to neural activity during the task, while at the same time its output predicted the next action a subject would execute. We then used the fitted model to create a novel visualization of the relationship between the activity in brain regions at different times following a reward and the choices the subject subsequently made. Finally, we validated our method using a previously published dataset. We found that the model was able to recover the underlying neural substrates that were discovered by explicit model engineering in the previous work, and also derived new results regarding the temporal pattern of brain activity.
C1 [Dezfouli, Amir; Balleine, Bernard W.] UNSW Sydney, Sydney, NSW, Australia.
   [Dezfouli, Amir] CSIRO, Data61, Canberra, ACT, Australia.
   [Morris, Richard; Ramos, Fabio] Univ Sydney, Sydney, NSW, Australia.
   [Dayan, Peter] UCL, Gatsby Unit, London, England.
RP Dezfouli, A (reprint author), UNSW Sydney, Sydney, NSW, Australia.; Dezfouli, A (reprint author), CSIRO, Data61, Canberra, ACT, Australia.
EM akdezfuli@gmail.com; richardumorris@gmail.com; p.dayan@ucl.ac.uk;
   fabio.ramos@sydney.edu.au; bernard.balleine@unsw.edu.au
FU UNSW Sydney; National Health and Medical Research Council of Australia
   [GNT1079561]; Gatsby Charitable Foundation
FX AD and BWB were supported by funding from UNSW Sydney and the National
   Health and Medical Research Council of Australia GNT1079561. PD was
   funded by the Gatsby Charitable Foundation. Part of this work was
   conducted whilst PD was at Uber Technologies. Neither body played a part
   in its design, execution or communication. PD is affiliated with Max
   Planck Institute for Biological Cybernetics, Tubingen,
   Germany(peter.dayan@tuebingen.mpg.de).
CR Abadi M., 2016, ARXIV160304467
   Balleine BW, 2010, NEUROPSYCHOPHARMACOL, V35, P48, DOI 10.1038/npp.2009.131
   Breakspear M, 2017, NAT NEUROSCI, V20, P340, DOI 10.1038/nn.4497
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Cohen JD, 2017, NAT NEUROSCI, V20, P304, DOI 10.1038/nn.4499
   Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766
   Dayan P, 2002, NEURON, V36, P285, DOI 10.1016/S0896-6273(02)00963-7
   Doya K, 2008, NAT NEUROSCI, V11, P410, DOI 10.1038/nn2077
   Gold Joshua I, 2007, ANN REV NEUROSCIENCE, V30
   Halpern David, 2018, P 11 INT C ED DAT MI
   Hare TA, 2011, P NATL ACAD SCI USA, V108, P18120, DOI 10.1073/pnas.1109322108
   Henson R, 2007, STATISTICAL PARAMETRIC MAPPING: THE ANALYSIS OF FUNCTIONAL BRAIN IMAGES, P178, DOI 10.1016/B978-012372560-8/50014-0
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   O'Doherty J, 2004, SCIENCE, V304, P452, DOI 10.1126/science.1094285
   O'Doherty JP, 2007, ANN NY ACAD SCI, V1104, P35, DOI 10.1196/annals.1390.022
   Rangel A, 2010, CURR OPIN NEUROBIOL, V20, P262, DOI 10.1016/j.conb.2010.03.001
   Seo H, 2007, J NEUROSCI, V27, P8366, DOI 10.1523/JNEUROSCI.2369-07.2007
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Simonyan K., 2013, 13126034 ARXIV
   Song HF, 2017, ELIFE, V6, DOI [10.7554/elife.21492, 10.7554/eLife.21492]
   Sunnaker M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002803
   Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26
   Turner BM, 2013, NEUROIMAGE, V72, P193, DOI 10.1016/j.neuroimage.2013.01.048
   Walton ME, 2004, NAT NEUROSCI, V7, P1259, DOI 10.1038/nn1339
   Wunderlich K, 2009, P NATL ACAD SCI USA, V106, P17199, DOI 10.1073/pnas.0901077106
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304026
DA 2019-06-15
ER

PT S
AU Dhamija, AR
   Gunther, M
   Boult, TE
AF Dhamija, Akshay Raj
   Guenther, Manuel
   Boult, Terrance E.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reducing Network Agnostophobia
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from a networks training set. In an uncontrolled environment, networks face many instances that are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction cannot return none of the known classes, or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We also introduce a new evaluation metric that focuses on comparing the performance of multiple approaches in scenarios where such unseen classes or unknowns are encountered. Our major contributions are simple yet effective Entropic Open-Set and Objectosphere losses that train networks using negative samples from some classes. These novel losses are designed to maximize entropy for unknown inputs while increasing separation in deep feature space by modifying magnitudes of known and unknown samples. Experiments on networks trained to classify classes from MNIST and CIFAR-10 show that our novel loss functions are significantly better at dealing with unknown inputs from datasets such as Devanagari, NotMNIST, CIFAR-100, and SVHN.
C1 [Dhamija, Akshay Raj; Guenther, Manuel; Boult, Terrance E.] Univ Colorado, Vis & Secur Technol Lab, Colorado Springs, CO 80907 USA.
RP Dhamija, AR (reprint author), Univ Colorado, Vis & Secur Technol Lab, Colorado Springs, CO 80907 USA.
EM adhamija@vast.uccs.edu; mgunther@vast.uccs.edu; tboult@vast.uccs.edu
FU NSF [IIS-1320956]; Office of the Director of National Intelligence
   (ODNI), Intelligence Advanced Research Projects Activity (IARPA)
   [2014-14071600012]
FX This research is based upon work funded in part by NSF IIS-1320956 and
   in part by the Office of the Director of National Intelligence (ODNI),
   Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D
   Contract No. 2014-14071600012. The views and conclusions contained
   herein are those of the authors and should not be interpreted as
   necessarily representing the official policies or endorsements, either
   expressed or implied, of the ODNI, IARPA, or the U.S. Government. The
   U.S. Government is authorized to reproduce and distribute reprints for
   Governmental purposes notwithstanding any copyright annotation thereon.
CR Bendale  Abhijit, 2016, C COMP VIS PATT REC
   Bridle John S., 1989, NEUROCOMPUTING ALGOR
   Cardoso DO, 2017, MACH LEARN, V106, P1547, DOI 10.1007/s10994-017-5646-4
   Chang Eric I., 1994, ADV NEURAL INFORM PR
   Chow C.K., 1957, Institute of Radio Engineers Transactions on Electronic Computers, VEC-6, P247
   De Stefano C, 2000, IEEE T SYST MAN CY C, V30, P84, DOI 10.1109/5326.827457
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68
   Gal  Yarin, 2016, INT C MACH LEARN ICM
   Geifman  Yonatan, 2017, ADV NEURAL INFORM PR
   Girshick  Ross, 2014, C COMP VIS PATT REC
   Girshick  Ross, 2015, INT C COMP VIS CVPR
   Graves A., 2011, ADV NEURAL INFORM PR
   Grother  Patrick, 2016, TECHNICAL REPORT
   Gunther  Manuel, 2017, INT JOINT C BIOM IJB
   Hu  Peiyun, 2017, C COMP VIS PATT REC
   Jonathon Phillips  P., 2011, HDB FACE RECOGNITION
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lakshminarayanan  Balaji, 2017, ADV NEURAL INFORM PR
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lin Tsung-Yi, 2014, EUR C COMP VIS ECCV
   Matan  Ofer, 1990, USPS ADV TECHN C
   Mor  Noam, 2018, WINT C APPL COMP VIS
   Netzer  Yuval, 2011, ADV NEUR INF PROC SY
   Oksuz  Kemal, 2018, EUR C COMP VIS ECCV
   Panareda  P., 2017, INT C COMP VIS ICCV
   Pant AK, 2012, ASIAN HIMAL INT CONF
   Plummer Bryan A., 2015, INT C COMP VIS ICCV
   Redmon  Joseph, 2016, C COMP VIS PATT REC
   Ren S., 2015, ADV NEURAL INFORM PR
   Russakovsky O., 2013, INT C COMP VIS ICCV
   Sermanet  P., 2013, OVERFEAT INTEGRATED
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Springenberg J. T., 2016, ADV NEURAL INFORM PR
   Tommasi T, 2017, ADV COMPUT VIS PATT, P37, DOI 10.1007/978-3-319-58347-1_2
   Vyas  Apoorv, 2018, EUR C COMP VIS ECCV
   Wen Y., 2016, EUR C COMP VIS ECCV
   Xu  Li, 2014, ADV NEURAL INFORM PR
   Yang  Bin, 2016, C COMP VIS PATT REC
   Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460
   Zhou K, 2016, DESTECH TRANS COMP
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003069
DA 2019-06-15
ER

PT S
AU Dhouib, S
   Redko, I
AF Dhouib, Sofien
   Redko, Ievgen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Revisiting (epsilon, gamma, tau)-similarity learning for domain
   adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a (epsilon, gamma, tau)-good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an (epsilon, gamma)-good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source distribution dominates the target one, then principally new domain adaptation learning bounds can be proved.
C1 [Dhouib, Sofien] Univ Claude Bernard Lyon 1, Univ Lyon, UJM St Etienne, INSA Lyon,CNRS,Inserm,CREATIS,UMR 5220,U1206, F-69100 Lyon, France.
   [Redko, Ievgen] Univ Lyon, UJM St Etienne, Inst Opt, CNRS,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France.
   [Redko, Ievgen] CREATIS, Lyon, France.
RP Dhouib, S (reprint author), Univ Claude Bernard Lyon 1, Univ Lyon, UJM St Etienne, INSA Lyon,CNRS,Inserm,CREATIS,UMR 5220,U1206, F-69100 Lyon, France.
EM sofiane.dhouib@creatis.insa-lyon.fr; ieygen.redko@uniy-st-etienne.fr
FU CNRS from the Defi Imag'In
FX This work benefited from the support provided by the CNRS funding from
   the Defi Imag'In.
CR Balcan MF, 2008, MACH LEARN, V72, P89, DOI 10.1007/s10994-008-5059-5
   Balcan Maria- Florina, 2008, P 21 ANN C LEARN THE, P287
   Bellet A., 2013, ARXIV13066709
   Bellet Aurelien, 2012, ICML
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cao Bin, 2011, P 22 INT JOINT C ART, P1204
   Cortes Corinna, 2011, ALT
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Germain Pascal, 2016, ICML, V48, P859
   Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556
   Irina Nicolae, 2015, ICONIP, P10
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Mansour Y., 2009, UAI, P367
   Mansour  Yishay, 2009, COLT
   Margolis Anna, 2011, LIT REV DOMAIN ADAPT
   Morvant E, 2012, KNOWL INF SYST, V33, P309, DOI 10.1007/s10115-012-0516-7
   Nicolae MI, 2015, LECT NOTES ARTIF INT, V9284, P594, DOI 10.1007/978-3-319-23528-8_37
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Perrot Michael, 2015, P 32 INT C MACH LEAR, P1708
   Weiss Karl, 2016, J BIG DATA, V3
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001091
DA 2019-06-15
ER

PT S
AU Dhurandhar, A
   Shanmugam, K
   Luss, R
   Olsen, P
AF Dhurandhar, Amit
   Shanmugam, Karthikeyan
   Luss, Ronny
   Olsen, Peder
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Improving Simple Models with Confidence Profiles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly improves (3-4%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly 13%.
C1 [Dhurandhar, Amit; Shanmugam, Karthikeyan; Luss, Ronny; Olsen, Peder] IBM Res, Yorktown Hts, NY 10598 USA.
RP Dhurandhar, A (reprint author), IBM Res, Yorktown Hts, NY 10598 USA.
EM adhuran@us.ibm.com; karthikeyan.shanmugam2@ibm.com; rluss@us.ibm.com;
   pederao@us.ibm.com
CR Agarwal D., 2011, INT C ART INT STAT, P93
   Alain  G., 2016, ARXIV161001644
   Ba L. J., 2013, CORR
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Bastani O., 2017, ARXIV170508504
   Bengio Y., 2009, P 26 ANN INT C MACH
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Bucilua C., 2006, P 12 ACM SIGKDD INT
   Carrier P.-L., 2013, ICML
   Dhurandhar A., 2017, ARXIV170602952
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   He K., 2015, INT C COMP VIS PATT
   Hinton J. D. Geoffrey, 2015, DISTILLING KNOWLEDGE
   Krizhevsky A., 2009, TECH REPORT
   Lindstrom M., 2016, SMALL DATA TINY CLUE
   Lipton Z. C., 2015, KDNUGGETS
   Liu F, 2016, 2016 IEEE 7TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS MOBILE COMMUNICATION CONFERENCE (UEMCON)
   Lopez-Paz D., 2016, INT C LEARN REPR ICL
   Lundberg S.-I. L. Scott, 2017, ADV NEURAL INF P SYS
   Montavon G., 2017, DIGITAL SIGNAL PROCE
   Reagen B., 2016, IEEE EXPLORE
   Ribeiro M., 2016, ACM SIGKDD INT C KNO
   Romero A, 2015, ARXIV14126550
   Selvaraju RR, 2016, GRAD CAM VISUAL EXPL
   Simonyan Karen, 2013, CORR
   Subramanya R. B. Akshayvarun, 2017, ARXIV170707013
   Tan S., 2017, CORR
   Wang JB, 2017, INT C ELECTR MACH SY
   Weiss S., 2013, ACM SIGKDD C KNOWL D
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004081
DA 2019-06-15
ER

PT S
AU Dhurandhar, A
   Chen, PY
   Luss, R
   Tu, CC
   Ting, PS
   Shanmugam, K
   Das, P
AF Dhurandhar, Amit
   Chen, Pin-Yu
   Luss, Ronny
   Tu, Chun-Chen
   Ting, Paishun
   Shanmugam, Karthikeyan
   Das, Payel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Explanations based on the Missing: Towards Contrastive Explanations with
   Pertinent Negatives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily absent (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically absent is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.
C1 [Dhurandhar, Amit; Chen, Pin-Yu; Luss, Ronny; Shanmugam, Karthikeyan; Das, Payel] IBM Res, Yorktown Hts, NY 10598 USA.
   [Tu, Chun-Chen; Ting, Paishun] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Dhurandhar, A (reprint author), IBM Res, Yorktown Hts, NY 10598 USA.
EM adhuran@us.ibm.com; pin-yu.chen@ibm.com; rluss@us.ibm.com;
   timtu@umich.edu; paishun@umich.edu; karthikeyan.shanmugam2@ibm.com;
   daspa@us.ibm.com
CR A. M. System, 2013, EXCL PART LIST
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Caruana R, 2015, P 21 ACM SIGKDD INT, P1721, DOI [DOI 10.1145/2783258.2788613, 10.1145/2783258.2788613]
   Chen  P.-Y., 2018, AAAI
   Craddock RC, 2012, HUM BRAIN MAPP, V33, P1914, DOI 10.1002/hbm.21333
   Dhurandhar A., 2017, ARXIV170602952
   Dhurandhar  A., 2015, ACM SIGKDD C KNOWL D
   Di Martino A, 2014, MOL PSYCHIATR, V19, P659, DOI 10.1038/mp.2013.78
   Doshi-Velez Finale, 2017, ARXIV171101134
   Gurumoorthy  K., 2017, ARXIV170701212
   Heinsfeld AS, 2018, NEUROIMAGE-CLIN, V17, P16, DOI 10.1016/j.nicl.2017.08.017
   Herman  A., 2016, MED DAILY
   Hull JV, 2017, FRONT PSYCHIATRY, V7, DOI 10.3389/fpsyt.2016.00205
   Ide T, 2017, KNOWL INF SYST, V51, P235, DOI 10.1007/s10115-016-0976-2
   Kim B, 2016, ADV NEUR IN, V29
   Kindermans  P.-J., 2018, INT C LEARN REPR ICL
   Lapuschkin S, 2016, J MACH LEARN RES, V17
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei Tao, 2016, ARXIV160604155
   Liska  A., 2017, ISMRM HON
   Montavon G., 2017, DIGITAL SIGNAL PROCE
   Mousavi  A., 2017, ARXIV170703386
   Nguyen A., 2016, ADV NEURAL INFORM PR, P3387
   Nguyen A., 2016, ARXIV160203616
   Nielsen JA, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00599
   Oramas  J., 2017, ARXIV171206302
   Ribeiro M., 2016, ACM SIGKDD INT C KNO
   Ribeiro M. T., 2018, AAAI C ART INT AAAI
   Samek W., 2017, IEEE T NEURAL NETWOR
   Scott Lundberg  S.-I.L., 2017, ADV NEURAL INF PROC
   Selvaraju RR, 2016, GRAD CAM VISUAL EXPL
   Simonyan K., 2013, 13126034 ARXIV
   Su  G., 2016, INTERPRETABLE 2 LEVE
   Tejwani  R., 2017, NIPS WORKSH BIGNEURO
   Wang  F., 2015, IN AISTATS
   Yannella P. N., 2018, ANAL ARTICLE 29 WORK
   Yeo BTT, 2011, J NEUROPHYSIOL, V106, P1125, DOI 10.1152/jn.00338.2011
   Zhang  X., 2018, INTERPRETING NEURAL
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300055
DA 2019-06-15
ER

PT S
AU Diakonikolas, I
   Kane, DM
   Stewart, A
AF Diakonikolas, Ilias
   Kane, Daniel M.
   Stewart, Alistair
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sharp Bounds for Generalized Uniformity Testing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of generalized uniformity testing of a discrete probability distribution: Given samples from a probability distribution p over an unknown size discrete domain Omega, we want to distinguish, with probability at least 2/3, between the case that p is uniform on some subset of Omega versus epsilon-far, in total variation distance, from any such uniform distribution. We establish tight bounds on the sample complexity of generalized uniformity testing. In more detail, we present a computationally efficient tester whose sample complexity is optimal, within constant factors, and a matching worst-case information-theoretic lower bound. Specifically, we show that the sample complexity of generalized uniformity testing is Theta (1/(epsilon(4/3)parallel to p parallel to(3)) + 1/(epsilon(2)parallel to p parallel to(2))).
C1 [Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Kane, Daniel M.] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Diakonikolas, I (reprint author), Univ Southern Calif, Los Angeles, CA 90089 USA.
EM diakonik@usc.edu; dakane@ucsd.edu; stewart.al@gmail.com
CR Acharya J., 2015, ADV NEURAL INFORM PR, P3591
   Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435
   Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113
   Batu T., 2017, CORR
   Batu T., 2004, P 36 ANN ACM S THEOR, P381
   Boucheron S., 2013, CONCENTRATION INEQUA
   Canonne C.L., 2015, ELECT C COMPUTATIONA, V22, P63
   Canonne C. L., 2017, CORR
   Canonne C. L., 2017, P 30 C LEARN THEOR C, P370
   Canonne C. L., 2016, 33 S THEOR ASP COMP
   Canonne CL, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P735, DOI 10.1145/3188745.3188756
   Chan Siu-On, 2014, P 25 ANN ACM SIAM S, P1193, DOI DOI 10.1137/1.9781611973402.88
   Daskalakis C., 2017, P 30 C LEARN THEOR C, P697
   Daskalakis C., 2016, CORR
   Daskalakis C, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1833
   Diakonikolas I., 2017, CORR
   Diakonikolas I., 2018, 45 INT C AUT LANG PR
   Diakonikolas I., 2016, ELECT C COMPUTATIONA, V23, P178
   Diakonikolas I., 2017, 44 INT C AUT LANG PR
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78
   Diakonikolas I, 2015, ANN IEEE SYMP FOUND, P1183, DOI 10.1109/FOCS.2015.76
   Diakonikolas Ilias, 2015, P 26 ANN ACM SIAM S, P1841
   Goldreich O., 2000, TR00020 EL C COMP CO
   Goldreich O., 2016, ECCC, V23
   Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987
   Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052
   Valiant G., 2014, FOCS
   Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000068
DA 2019-06-15
ER

PT S
AU Dieleman, S
   van den Oord, A
   Simonyan, K
AF Dieleman, Sander
   van den Oord, Aaron
   Simonyan, Karen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The challenge of realistic music generation: modelling raw audio at
   scale
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.
C1 [Dieleman, Sander; van den Oord, Aaron; Simonyan, Karen] DeepMind, London, England.
RP Dieleman, S (reprint author), DeepMind, London, England.
EM sedielem@google.com; avdnoord@google.com; simonyan@google.com
CR Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Bachman Philip, 2016, ADV NEURAL INFORM PR, P4826
   Belghazi Mohamed Ishmael, 2018, ARXIV180201071
   Bengio Y., 2013, ARXIV13083432
   Boulanger-Lewandowski Nicolas, 2012, ICML
   Bowman S. R., 2015, ARXIV151106349
   Briot J. -P., 2017, ARXIV170901620
   Chang S., 2017, ADV NEURAL INFORM PR, P76
   Chen X., 2016, ARXIV161102731
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Cope D, 1996, EXPT MUSICAL INTELLI, V12
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Donahue C., 2018, ARXIV180204208
   ElHihi S, 1996, ADV NEUR IN, V8, P493
   Engel J., 2017, NEURAL AUDIO SYNTHES
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves Alex, 2018, ARXIV180402476
   Gulrajani Ishaan, 2016, ABS161105013 ARXIV
   Henaff Mikael, 2016, ARXIV160206662
   Herremans D, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108242
   Heusel M., 2017, ADV NEURAL INFORM PR, P6629
   Jaderberg M., 2017, ARXIV171109846
   Kalchbrenner Nal, 2018, CORR
   Kingma D. P., 2014, CORR
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2013, ARXIV13126114
   Kolesnikov A., 2017, ICML, V70, P1905
   Koutnik J., 2014, ARXIV14023511
   Liu Peter J, 2018, ARXIV180110198
   Liu Shikun, 2017, ARXIV170505994
   Mehri Soroush, 2017, ICLR
   Mikolov T., 2014, ARXIV14127753
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Rezende D. J, 2014, ARXIV14014082
   Roberts Adam, 2018, ARXIV180305428
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Salimans Tim, 2017, ARXIV170105517
   Serban Iulian V., 2017, AAAI, P3295
   Simon  I., 2017, PERFORMANCE RNN GENE
   Trinh Trieu H, 2018, ARXIV180300144
   Van Den Oord A., 2016, ARXIV160903499
   van den Oord A., 2016, ICML
   van den Oord  A., 2017, ADV NEURAL INFORM PR, P6309
   van den Oord Aaron, 2017, CORR
   Yu F., 2016, ICLR
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002053
DA 2019-06-15
ER

PT S
AU Dimakopoulou, M
   Osband, I
   Van Roy, B
AF Dimakopoulou, Maria
   Osband, Ian
   Van Roy, Benjamin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Scalable Coordinated Exploration in Concurrent Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling [1] and randomized value function learning [11]. We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods [1]. With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.
C1 [Dimakopoulou, Maria; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA.
   [Osband, Ian] Google DeepMind, London, England.
RP Dimakopoulou, M (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM madima@stanford.edu; iosband@google.com; bvr@stanford.edu
CR Dimakopoulou Maria, 2018, ICML
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Gu Shixiang, 2016, DEEP REINFORCEMENT L
   Guo Z., 2015, AAAI C ART INT, P2624
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   Kim Michael Jong, 2017, IEEE T AUTOMATIC CON
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Osband I., 2013, ADV NEURAL INFORM PR, P3003
   Osband Ian, 2018, ARXIV180603335
   Osband Ian, 2017, MULT C REINF LEARN D
   Osband Ian, 2017, ICML
   Osband Ian, 2016, ARXIV160802731
   Osband  Ian, 2016, P 33 INT C MACH LEAR, P2377
   Pazis Jason, 2013, AAAI
   Pazis Jason, 2016, AAAI
   Russo  Daniel, 2017, ARXIV170702038
   Silver David, 2013, P 30 INT C MACH LEAR, P924
   STRENS M, 2000, P 17 INT C MACH LEAR, P943
   Sutton R. S., 2017, REINFORCEMENT LEARNI
   Szepesvari C., 2010, SYNTHESIS LECT ARTIF
   Tassa Y., 2018, ARXIV180100690
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304025
DA 2019-06-15
ER

PT S
AU Ding, YX
   Zhou, ZH
AF Ding, Yao-Xiang
   Zhou, Zhi-Hua
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Preference Based Adaptation for Learning Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In many real-world learning tasks, it is hard to directly optimize the true performance measures, meanwhile choosing the right surrogate objectives is also difficult. Under this situation, it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work, we discuss the task of objective adaptation, in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized, this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm (DLM)-M-2 is proposed to learn the optimal parameter, which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update, a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypotheses to the current objective. We apply the overall approach to multi-label learning, and show that the proposed approach achieves significant performance under various multi-label performance measures.
C1 [Ding, Yao-Xiang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
RP Ding, YX (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM dingyx@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn
FU National Key R&D Program of China [2018YFB1004300]; NSFC [61751306];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization; Outstanding PhD Candidate Program of Nanjing
   University
FX This research is supported by National Key R&D Program of China
   (2018YFB1004300), NSFC (61751306) and Collaborative Innovation Center of
   Novel Software Technology and Industrialization. Yao-Xiang Ding is
   supported by the Outstanding PhD Candidate Program of Nanjing
   University. The Authors would like to thank the anonymous reviewers for
   constructive suggestions, as well as Lijun Zhang, Ming Pang, Xi-Zhu Wu
   and Yichi Xiao for helpful discussions.
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Abeille Marc, 2017, AISTATS, P176
   Agarwal Alekh, 2014, COLT, P726
   Chappelle O., 2010, P 16 ACM SIGKDD INT, P1189, DOI DOI 10.1145/1835804.1835953
   Deb K., 2014, SEARCH METHODOLOGIES, P403, DOI DOI 10.1007/978-1-4614-6940-7_15
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Kumagai W., 2017, NIPS, P1488
   Li N, 2013, IEEE T PATTERN ANAL, V35, P1370, DOI 10.1109/TPAMI.2012.172
   Rosset S, 2004, J MACH LEARN RES, V5, P941
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Wu X.-Z., 2017, P 34 INT C MACH LEAR, P3780
   Yue Y., 2009, P 26 ANN INT C MACH, P1201, DOI DOI 10.1145/1553374.1553527
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Zhang  L., 2016, P INT C MACH LEARN, P392
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002038
DA 2019-06-15
ER

PT S
AU Diochnos, DI
   Mahloujifar, S
   Mahmoody, M
AF Diochnos, Dimitrios I.
   Mahloujifar, Saeed
   Mahmoody, Mohammad
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial Risk and Robustness: General Definitions and Implications
   for the Uniform Distribution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DNF
AB We study adversarial perturbations when the instances are uniformly distributed over {0, 1}(n). We study both "inherent" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes.
   As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition.
   Using the error-region definition of adversarial perturbations, we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0,1}(n). Using the isoperimetric inequality for the Boolean hypercube, we show that for initial error 0.01, there always exists an adversarial perturbation that changes O(root n) bits of the instances to increase the risk to 0.5, making classifier's decisions meaningless. Furthermore, by also using the central limit theorem we show that when n -> infinity , at most c. root n bits of perturbations, for a universal constant c < 1.17 suffice for increasing the risk to 0.5, and the same c root n bits of perturbations on average suffice to increase the risk to 1, hence bounding the robustness by c . root n.
C1 [Diochnos, Dimitrios I.; Mahloujifar, Saeed; Mahmoody, Mohammad] Univ Virginia, Charlottesville, VA 22903 USA.
RP Diochnos, DI (reprint author), Univ Virginia, Charlottesville, VA 22903 USA.
EM diochnos@virginia.edu; saeed@virginia.edu; mohammad@virginia.edu
FU NSF [CAREER CCF-1350939]; University of Virginia SEAS Research
   Innovation Award
FX `Supported by NSF CAREER CCF-1350939 and University of Virginia SEAS
   Research Innovation Award.
CR Attias Idan, 2018, ARXIV181002180
   Bastani Osbert, 2016, ADV NEURAL INFORM PR, P2613
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57
   Blum A., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P253, DOI 10.1145/195058.195147
   Bruner Jerome S., 1957, STUDY THINKING
   Carlini N., 2017, P 10 ACM WORKSH ART, P3
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   DIOCHNOS DI, 2009, SAGA, V5792, P74
   Diochnos DI, 2016, LECT NOTES ARTIF INT, V9925, P98, DOI 10.1007/978-3-319-46379-7_7
   Fawzi Alhussein, 2018, ARXIV180208686
   Feige Uriel, 2015, P 28 C LEARN THEOR C, P637
   Feige Uriel, 2018, ALGORITHMIC LEARNING, P368
   Gilmer Justin, 2018, ARXIV180102774
   Goodfellow I., 2015, ICLR
   HARPER K. H., 1966, J COMBINATORIAL THEO, V1, P385, DOI DOI 10.1016/S0021-9800(66)80059-5
   Huang L., 2011, P 4 ACM WORKSH SEC A, P43, DOI DOI 10.1145/2046684.2046692
   Jackson Jeffrey, 2006, THEORY COMPUTING, V2, P147
   Lowd Daniel, 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950
   Madry A., 2018, INT C LEARN REPR ICL
   Mitchell T. M., 1997, MACHINE LEARNING
   MOOSAVIDEZFOOLI SM, 2016, CVPR, P2574, DOI DOI 10.1109/CVPR.2016.282
   Nelson B, 2012, J MACH LEARN RES, V13, P1293
   Nelson Blaine, 2010, PSDM, P92
   Nigmatullin R. G., 1967, DISKRETNYL ANAL, V9, P47
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Sakai Y, 2000, THEOR COMPUT SYST, V33, P17, DOI 10.1007/s002249910002
   Schmidt Ludwig, 2018, ARXIV180411285
   Sellie L, 2009, ACM S THEORY COMPUT, P45
   Suggala Arun Sai, 2018, ARXIV180602924
   Szegedy C., 2014, ICLR
   Tsipras D., 2018, ARXIV180512152
   Valiant LG, 2009, J ACM, V56, DOI 10.1145/1462153.1462156
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Xu W., 2018, NETW DISTR SYST SEC
   YishayMansour Aviad Rubinstein, 2015, P 26 ANN ACM SIAM S, P449
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004087
DA 2019-06-15
ER

PT S
AU Djolonga, J
   Jegelka, S
   Krause, A
AF Djolonga, Josip
   Jegelka, Stefanie
   Krause, Andreas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Provable Variational Inference for Constrained Log-Submodular Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID THEOREMS; TREES
AB Submodular maximization problems appear in several areas of machine learning and data science, as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions, as well as the decisions made with the computed solutions, are subject to statistical noise and randomness, it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end, we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables, but also satisfy hard combinatorial constraints. Namely, the variables are assumed to take on one of - possibly exponentially many - set of states, which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms, which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations, our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover, for the case of facility location and weighted coverage functions, we prove the first constant factor guarantee in this setting-an efficiently certifiable e/(e - 1) approximation of the log-partition function. Finally, we empirically demonstrate the effectiveness of our approach on several instances.
C1 [Djolonga, Josip; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Djolonga, J (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM josipd@inf.ethz.ch; stefje@csail.mit.edu; krausea@ethz.ch
FU ERC [StG 307036]; Google European PhD Fellowship; NSF CAREER award
   [1553284]
FX The research was partially supported by ERC StG 307036, Google European
   PhD Fellowship, and NSF CAREER award 1553284.
CR Agrawal S, 2010, PROC APPL MATH, V135, P1087
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Borcea J, 2009, J AM MATH SOC, V22, P521
   Bouchard-Cote A., 2010, NEURAL INFORM PROCES
   BURTON R, 1993, ANN PROBAB, V21, P1329, DOI 10.1214/aop/1176989121
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Celis L. E., 2018, ARXIV180204023
   Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60
   Djolonga J., 2014, NEURAL INFORM PROCES
   DJOLONGA J., 2015, INT C MACH LEARN ICM
   Djolonga J., 2016, NEURAL INFORM PROCES
   DRESS AWM, 1995, APPL MATH LETT, V8, P77, DOI 10.1016/0893-9659(95)00070-7
   Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [DOI 10.1007/BF01584082, 10.1007/BF01584082]
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Fujishige S, 2003, MATH OPER RES, V28, P463, DOI 10.1287/moor.28.3.463.16393
   Gomes Ryan, 2010, P 27 INT C MACH LEAR, P391
   Gomez-Rodriguez M., 2012, ACM T KNOWLEDGE DISC
   Gotovos A., 2015, NEURAL INFORM PROCES
   Hazan T., 2012, ARXIV12066410
   Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014
   Jerrum M., 1998, MATH FDN MARKOV CHAI, P116
   Karimi Mohammad, 2017, NEURAL INFORM PROCES, P6856
   Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108
   Koo  T., 2007, P 2007 JOINT C EMP M
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulesza A., 2012, FDN TRENDS MACHINE L, V5, P2
   Leme RP, 2017, GAME ECON BEHAV, V106, P294, DOI 10.1016/j.geb.2017.10.016
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Li Chengtao, 2016, ADV NEURAL INFORM PR, P4188
   Lyons R, 2003, PUBL MATH IHES, P167
   MAURER SB, 1976, SIAM J APPL MATH, V30, P143, DOI 10.1137/0130017
   Minka T., 2005, TECH REP
   Murota K, 2003, DISCRETE CONVEX ANAL
   Murota K., 2016, J MECH INSTIT DES, V1, P151
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Oxley J. G., 2006, MATROID THEORY, V3
   Papandreou G, 2011, IEEE I CONF COMP VIS, P193, DOI 10.1109/ICCV.2011.6126242
   Rebeschini P., 2015, 28 C LEARN THEOR COL
   RENYI A., 1961, P 4 BERK S MATH STAT, V1, P547, DOI DOI 10.1021/JP106846B
   Risteski A., 2016, C LEARN THEOR
   Shioura A, 2009, DISCRET MATH ALGORIT, V1, P1, DOI 10.1142/S1793830909000063
   Smith D. A., 2008, P C EMP METH NAT LAN
   Swersky K., 2012, ADV NEURAL INFORM PR, P3293
   Tarlow D., 2012, ARXIV12104899
   Tschiatschek S., 2016, INT C ART INT STAT A
   Van Erven T., 2012, ARXIV12062459
   Vondrak J., 2007, THESIS
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2006, IEEE T SIGNAL PROCES, V54, P2099, DOI 10.1109/TSP.2006.874409
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
   Xia SY, 2015, INT CONF IMAG VIS
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302069
DA 2019-06-15
ER

PT S
AU Domke, J
   Sheldon, D
AF Domke, Justin
   Sheldon, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Importance Weighting and Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.
C1 [Domke, Justin; Sheldon, Daniel] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA.
RP Domke, J (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
FU National Science Foundation [1617533]
FX We thank Tom Rainforth for insightful comments regarding asymptotics and
   Theorem 3 and Linda Siew Li Tan for comments regarding Lemma 7. This
   material is based upon work supported by the National Science Foundation
   under Grant No. 1617533.
CR Agakov FV, 2004, LECT NOTES COMPUT SC, V3316, P561
   Bachman Philip, 2015, NIPS WORKSH ADV APPR
   Bamler Robert, 2017, NIPS
   Bickel P. J., 2015, MATH STAT BASIC IDEA, VI
   Burda Yuri, 2015, IMPORTANCE WEIGHTED
   Cremer Chris, 2017, REINTERPRETING IMPOR
   Dieng Adji Bousso, 2017, NIPS, P2729
   Fang Kaitai, 1990, MONOGRAPHS STAT APPL, V36
   GILKS WR, 1994, J R STAT SOC D-STAT, V43, P169, DOI DOI 10.2307/2348941
   Kingma Diederik P., ICLR
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Le Tuan Anh, 2018, ICLR
   Maddison Chris J, 2017, ADV NEURAL INFORM PR, P6576
   Marcinkiewicz Jozef, 1937, FUND MATH, V28, P60
   Minka T., 2005, DIVERGENCE MEASURES
   Minka T. P., 2001, UAI
   Naesseth Christian A., 2018, AISTATS
   Owen AB, 2013, MONTE CARLO THEORY M
   Rainforth Tom, TIGHTER VARIATIONAL
   Ranganath Rajesh, 2014, AISTATS
   Romon Gabriel, 2018, BOUNDS MOMENTS SAMPL
   Ruiz Francisco J. R., 2016, UAI
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Stan Development Team, 2017, MOD LANG US GUID REF
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304048
DA 2019-06-15
ER

PT S
AU Dong, CS
   Chen, YR
   Zeng, B
AF Dong, Chaosheng
   Chen, Yiran
   Zeng, Bo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Generalized Inverse Optimization through Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specifically in batch setting, where all the data is available in advance. As a consequence, there has been rare use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically, we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of O(1/root T) and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a dramatic improvement in computational efficacy over the batch learning approach.
C1 [Dong, Chaosheng; Zeng, Bo] Univ Pittsburgh, Dept Ind Engn, Pittsburgh, PA 15260 USA.
   [Chen, Yiran] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RP Dong, CS (reprint author), Univ Pittsburgh, Dept Ind Engn, Pittsburgh, PA 15260 USA.
EM chaosheng@pitt.edu; yiran.chen@duke.edu; bzeng@pitt.edu
FU National Science Foundation [CMMI-1642514]; NSF at the Pittsburgh
   Supercomputing Center (PSC) [ACI-1445606]
FX This work was partially supported by CMMI-1642514 from the National
   Science Foundation. This work used the Bridges system, which is
   supported by NSF award number ACI-1445606, at the Pittsburgh
   Supercomputing Center (PSC).
CR Aggarwal C. C., 2016, RECOMMENDER SYSTEMS
   Ahuja RK, 2001, OPER RES, V49, P771, DOI 10.1287/opre.49.5.771.10607
   Aswani  Anil, 2018, OPERATIONS RES
   Barmann  Andreas, 2017, ICML
   Bertsimas D, 2015, MATH PROGRAM, V153, P595, DOI 10.1007/s10107-014-0819-4
   Bertsimas D, 2012, OPER RES, V60, P1389, DOI 10.1287/opre.1120.1115
   Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671
   Bonnans JF, 1998, SIAM REV, V40, P228, DOI 10.1137/S0036144596302644
   Chan TCY, 2014, OPER RES, V62, P680, DOI 10.1287/opre.2014.1267
   Chan Timothy CY, 2018, MANAGEMENT SCI
   Cheng  Li, 2007, NIPS
   Dempe  Stephan, 2006, RECENT ADV OPTIMIZAT, P19
   Dong  Chaosheng, 2018, ARXIV180800935
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Esfahani Peyman Mohajerin, 2017, MATH PROGRAM, P1
   Frederic Bonnans  J, 2013, PERTURBATION ANAL OP
   Iyengar G, 2005, OPER RES LETT, V33, P319, DOI 10.1016/j.orl.2004.04.007
   Keshavarz A, 2011, 2011 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT CONTROL (ISIC), P613, DOI 10.1109/ISIC.2011.6045410
   Kulis  Brian, 2010, ICML
   Mas-Collell  Andreu, 1995, MICROECONOMIC THEORY, P10
   Nystrom N. A., 2015, P XSEDE C
   Schaefer AJ, 2009, OPTIM LETT, V3, P483, DOI 10.1007/s11590-009-0131-z
   Wang LZ, 2009, OPER RES LETT, V37, P114, DOI 10.1016/j.orl.2008.12.001
   Wang T, 2017, J MACH LEARN RES, V18, P1
   YANG H, 1992, TRANSPORT RES B-METH, V26, P417, DOI 10.1016/0191-2615(92)90008-K
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300009
DA 2019-06-15
ER

PT S
AU Dong, HY
   Liang, XD
   Gong, K
   Lai, HJ
   Zhu, J
   Yin, J
AF Dong, Haoye
   Liang, Xiaodan
   Gong, Ke
   Lai, Hanjiang
   Zhu, Jia
   Yin, Jian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is lightweight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our WarpingGAN that significantly outperforms all existing methods on two large datasets.
C1 [Dong, Haoye; Gong, Ke; Lai, Hanjiang; Yin, Jian] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Dong, Haoye; Gong, Ke; Yin, Jian] Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China.
   [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
   [Zhu, Jia] South China Normal Univ, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China.
RP Liang, XD (reprint author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
EM donghy7@mail2.sysu.edu.cn; xdliang328@gmail.com; kegong936@gmail.com;
   laihanj3@mail.sysu.edu.cn; jzhu@m.scun.edu.cn; issjyin@mail.sysu.edu.cn
FU National Natural Science Foundation of China [61472453, U1401256,
   U1501252, U1611264, U1711261, U1711262, 61602530]; National Natural
   Science Foundation of China (NSFC) [61836012]
FX This work is supported by the National Natural Science Foundation of
   China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262,
   61602530), and National Natural Science Foundation of China (NSFC) under
   Grant No. 61836012.
CR Balakrishnan G, 2018, CVPR
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544
   Cao  Z., 2017, CVPR
   Choi Yunjey, 2018, CVPR
   Deng  Zhijie, 2017, ADV NEURAL INFORM PR, P3899
   Dong P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P489, DOI 10.1109/ICIP.2002.1039014
   Esser  Patrick, 2018, CVPR
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Han  Xintong, 2017, ARXIV171108447
   Hu  Zhiting, 2018, NIPS
   Isola  Phillip, 2017, CVPR
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim T, 2017, ARXIV170305192
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang X, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417724179
   Liang  Xiaodan, 2018, P EUR C COMP VIS ECC, P558
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma L., 2018, CVPR
   Ma  Liqian, 2017, NIPS
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Pumarola A., 2018, CVPR
   Radford A., 2016, ICLR
   Rocco I., 2017, CVPR
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Salimans T., 2016, NIPS
   Siarohin  Aliaksandr, 2018, CVPR
   Wang  Bochao, 2018, ECCV
   Wang T.-C., 2017, ARXIV171111585
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan  Yichao, 2017, ACM MM
   Yi Zili, 2017, DUALGAN UNSUPERVISED
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhu J.-Y., 2017, ICCV
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300044
DA 2019-06-15
ER

PT S
AU Dong, S
   Van Roy, B
AF Dong, Shi
   Van Roy, Benjamin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI An Information-Theoretic Analysis for Thompson Sampling with Many
   Actions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Information-theoretic Bayesian regret bounds of Russo and Van Roy [8] capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases. We establish new bounds that depend instead on a notion of rate-distortion. Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit. We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.
C1 [Dong, Shi; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA.
RP Dong, S (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM sdong15@stanford.edu; bvr@stanford.edu
FU Boeing Corporation; Jane Dwight Stanford Graduate Fellowship
FX This work was supported by a grant from the Boeing Corporation and the
   Herb and Jane Dwight Stanford Graduate Fellowship. We would also like to
   thank Daniel Russo, David Tse and Xiuyuan Lu for useful conversations.
CR Shipra,, 2017, J ACM, V64, DOI 10.1145/3088510
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dani V., 2008, COLT, P355
   Li  Lihong, 2017, INT C MACH LEARN, P2071
   Russo D., 2014, ADV NEURAL INFORM PR, P1583
   Russo D, 2016, J MACH LEARN RES, V17
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Russo  Daniel, 2018, ARXIV180302855
   Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304019
DA 2019-06-15
ER

PT S
AU Donini, M
   Oneto, L
   Ben-David, S
   Shawe-Taylor, J
   Pontil, M
AF Donini, Michele
   Oneto, Luca
   Ben-David, Shai
   Shawe-Taylor, John
   Pontil, Massimiliano
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Empirical Risk Minimization Under Fairness Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.
C1 [Donini, Michele; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy.
   [Oneto, Luca] Univ Genoa, Genoa, Italy.
   [Ben-David, Shai] Univ Waterloo, Waterloo, ON, Canada.
   [Shawe-Taylor, John; Pontil, Massimiliano] UCL, London, England.
RP Donini, M (reprint author), Ist Italiano Tecnol, Genoa, Italy.
FU SAP SE; EPSRC
FX We wish to thank Amon Elders, Theodoros Evgeniou and Andreas Maurer for
   useful comments. This work was supported in part by SAP SE and the
   EPSRC.
CR Adebayo J., 2016, C FAIRN ACC TRANSP M
   Agarwal A., 2017, C FAIRN ACC TRANSP M
   Agarwal A., 2018, ARXIV180302453
   Alabi D., 2018, ARXIV180404503
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bechavod Y., 2018, ARXIV170700044V3
   Berk R., 2017, ARXIV170602409
   Beutel A., 2017, C FAIRN ACC TRANSP M
   Calders T., 2009, IEEE INT C DAT MIN
   Calmon F., 2017, ADV NEURAL INFORM PR
   Chierichetti F., 2017, ADV NEURAL INFORM PR
   Donini M., 2017, NIPS WORKSH PRIOR ON
   Dwork Cynthia, 2018, C FAIRN ACC TRANSP
   Feldman M., 2015, INT C KNOWL DISC DAT
   Hardt Moritz, 2016, ADV NEURAL INFORM PR
   Jabbari S., 2016, C FAIRN ACC TRANSP M
   Joseph M., 2016, ADV NEURAL INFORM PR
   Kamiran F., 2009, INT C COMP CONTR COM
   Kamiran F., 2010, MACH LEARN C
   Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8
   Kamishima T., 2011, INT C DAT MIN WORKSH
   Kearns M., 2017, ARXIV171105144
   Kilbertus Niki, 2017, ADV NEURAL INFORM PR
   Kusner Matt J, 2017, ADV NEURAL INFORM PR
   Lum K., 2016, ARXIV161008077
   Maurer A., 2004, CS0411099 ARXIV
   Menon A. K., 2018, C FAIRN ACC TRANSP
   Oneto L., 2019, AAAI ACM C AI ETH SO
   Perez-Suay A., 2017, MACHINE LEARNING KNO
   Pleiss G., 2017, ADV NEURAL INFORM PR
   Quadrianto N., 2017, ADV NEURAL INFORM PR
   Rockafellar R T, 1970, CONVEX ANAL
   Scholkopf B, 2001, COMPUTATIONAL LEARNI
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Smola A. J., 2001, LEARNING KERNELS
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Woodworth B., 2017, COMPUTATIONAL LEARNI
   Yao S., 2017, ADV NEURAL INFORM PR
   Zafar M. B., 2017, ADV NEURAL INFORM PR
   Zafar M. B., 2017, INT C ART INT STAT
   Zafar M. B., 2017, INT C WORLD WID WEB
   Zemel R., 2013, INT C MACH LEARN
   Zliobaite I., 2015, ARXIV150505723
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302078
DA 2019-06-15
ER

PT S
AU Draief, M
   Kutzkov, K
   Scaman, K
   Vojnovic, M
AF Draief, Moez
   Kutzkov, Konstantin
   Scaman, Kevin
   Vojnovic, Milan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI KONG: Kernels for ordered-neighborhood graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets. In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e., graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.
C1 [Draief, Moez; Scaman, Kevin] Huawei Noahs Ark Lab, Hong Kong, Peoples R China.
   [Kutzkov, Konstantin; Vojnovic, Milan] London Sch Econ, London, England.
RP Kutzkov, K (reprint author), London Sch Econ, London, England.
EM moez.draief@huawei.com; kutzkov@gmail.com; kevin.scaman@huawei.com;
   m.vojnovic@lse.ac.uk
FU Huawei Technologies
FX The work has been supported by a research collaboration grant funded by
   Huawei Technologies.
CR Borgwardt K.M., 2005, P IEEE INT C DAT MIN, P74, DOI DOI 10.1109/1CDM.2005.132
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6
   Da San Martino  Giovanni, 2012, P 12 SIAM INT C DAT, P975
   Da San Martino  Giovanni, 2013, IJCAI 2013, P1294
   DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feigenbaum J, 2005, THEOR COMPUT SCI, V348, P207, DOI 10.1016/j.tcs.2005.09.013
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Grover Aditya, 2016, KDD, V2016, P855
   Haussler D., 1999, CONVOLUTION KERNELS
   Helma C, 2001, BIOINFORMATICS, V17, P107, DOI 10.1093/bioinformatics/17.1.107
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Kersting K., 2016, BENCHMARK DATA SETS
   Kriege N, 2014, IEEE DATA MINING, P881, DOI 10.1109/ICDM.2014.129
   Kriege Nils M., 2017, ABS170300676 CORR
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Leslie CS, 2002, P PSB, V7, P566
   Manzoor E. A., 2016, KDD, P1035
   Neumann M, 2016, MACH LEARN, V102, P209, DOI 10.1007/s10994-015-5517-9
   Niepert M., 2016, INT C MACH LEARN, P2014
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Pham N., 2013, P 19 ACM SIGKDD INT, P239
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rual JF, 2005, NATURE, V437, P1173, DOI 10.1038/nature04209
   Schomburg I, 2004, NUCLEIC ACIDS RES, V32, pD431, DOI 10.1093/nar/gkh081
   Shervashidze N., 2009, P 12 INT C ART INT S, V2009, P488
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Wale N, 2006, IEEE DATA MINING, P678
   Yanardag P., 2015, P 21 ACM SIGKDD INT, P1365, DOI DOI 10.1145/2783258.2783417
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304009
DA 2019-06-15
ER

PT S
AU Drumond, M
   Lin, T
   Jaggi, M
   Falsafi, B
AF Drumond, Mario
   Lin, Tao
   Jaggi, Martin
   Falsafi, Babak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Training DNNs with Hybrid Block Floating Point
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The wide adoption of DNNs has given birth to unrelenting computing requirements, forcing datacenter operators to adopt domain-specific accelerators to train them. These accelerators typically employ densely packed full-precision floating-point arithmetic to maximize performance per area. Ongoing research efforts seek to further increase that performance density by replacing floating-point with fixedpoint arithmetic. However, a significant roadblock for these attempts has been fixed point's narrow dynamic range, which is insufficient for DNN training convergence. We identify block floating point (BFP) as a promising alternative representation since it exhibits wide dynamic range and enables the majority of DNN operations to be performed with fixed-point logic. Unfortunately, BFP alone introduces several limitations that preclude its direct applicability. In this work, we introduce HBFP, a hybrid BFP-FP approach, which performs all dot products in BFP and other operations in floating point. HBFP delivers the best of both worlds: the high accuracy of floating point at the superior hardware density of fixed point. For a wide variety of models, we show that HBFP matches floating point's accuracy while enabling hardware implementations that deliver up to 8.5 x higher throughput.
C1 [Drumond, Mario; Lin, Tao; Jaggi, Martin; Falsafi, Babak] Ecole Polytech Fed Lausanne, Ecocloud, Lausanne, Switzerland.
RP Drumond, M (reprint author), Ecole Polytech Fed Lausanne, Ecocloud, Lausanne, Switzerland.
EM mario.drumond@epfl.ch; tao.lin@epfl.ch; martin.jaggi@epfl.ch;
   babak.falsafi@epfl.ch
FU ColTraIn project of the Microsoft-EPFL Joint Research Center; SNSF
   [200021_175796]
FX The authors thank the anonymous reviewers, Mark Sutherland, Siddharth
   Gupta, and Alexandros Daglis for their precious comments and feedback.
   We also thank Ryota Tomioka and Eric Chung for many inspiring
   conversations on low-precision DNN processing. This work has been
   partially funded by the ColTraIn project of the Microsoft-EPFL Joint
   Research Center and by SNSF grant 200021_175796.
CR Alistarh D., 2017, P 31 C NEUR INF PROC
   Behera RK, 2015, PROCEEDINGS 2015 INTERNATIONAL CONFERENCE ON MAN AND MACHINE INTERFACING (MAMI)
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Courbariaux M., 2015, P 29 C NEUR INF PROC
   Dally W., 2015, HIGH PERFORMANCE HAR
   Emer J. S., 2016, P 43 INT S COMP ARCH
   He K., 2016, P C COMP VIS PATT RE
   Huang G., 2017, P C COMP VIS PATT RE
   Hubara I., 2016, P 30 C NEUR INF PROC
   Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353
   Jouppi N. P., 2017, P 44 INT S COMP ARCH
   Koster U., 2017, P 31 C NEUR INF PROC
   Krizhevsky A., 2009, TECHNICAL REPORT
   Li  F., 2016, CORR
   Mao H., 2017, P 5 INT C LEARN REPR
   Marsaglia G., 2003, J STAT SOFTWARE, V8
   Merity  S., 2018, P 6 INT C LEARN REPR
   Micikevicius P., 2018, P 6 INT C LEARN REPR
   Mikolov T., 2010, P 11 ANN C INT SPEEC
   Netzer Y., 2011, DEEP LEARN UNS FEAT
   Rastegari M., 2016, P 13 EUR C COMP VIS
   Russakovsky O., 2014, CORR
   Sa C. D., 2017, P 44 INT S COMP ARCH
   Song Z., 2018, P 32 AAAI C ART INT
   Zagoruyko S., 2016, P BRIT MACH VIS C BM
   Zhang H., 2017, P 34 INT C MACH LEAR
   Zhou S., 2016, CORR
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300042
DA 2019-06-15
ER

PT S
AU Du, SS
   Hu, W
   Lee, JD
AF Du, Simon S.
   Hu, Wei
   Lee, Jason D.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Algorithmic Regularization in Learning Deep Homogeneous Models: Layers
   are Automatically Balanced
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes eta(t) = O (t(-(1/2+delta))) (0 < delta <= 1/2) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.
C1 [Du, Simon S.] Carnegie Mellon Univ, Machine Learning Dept, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Hu, Wei] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.
   [Lee, Jason D.] Univ Southern Calif, Dept Data Sci & Operat, Marshall Sch Business, Los Angeles, CA 90089 USA.
RP Du, SS (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM ssdu@cs.cmu.edu; huwei@cs.princeton.edu; jasonlee@marshall.usc.edu
FU ARO [W911NF-11-1-0303]
FX We thank Phil Long for his helpful comments on an earlier draft of this
   paper. JDL acknowledges support from ARO W911NF-11-1-0303.
CR Absil PA, 2005, SIAM J OPTIMIZ, V16, P531, DOI 10.1137/040605266
   Arora S., 2018, ARXIV180206509
   Bartlett Peter L, 2018, ARXIV180206093
   Brutzkus Alon, 2017, ARXIV171010174
   Brutzkus Alon, 2017, ARXIV170207966
   Choromanska A., 2015, ARTIF INTELL, P192
   Clarke FH, 2008, NONSMOOTH ANAL CONTR, V178
   Davis  Damek, 2018, ARXIV180407795
   Drusvyatskiy D, 2015, SIAM J CONTROL OPTIM, V53, P114, DOI 10.1137/130920216
   Du Simon S, 2017, ARXIV170906129
   Du Simon S, 2017, ARXIV171200779
   Freeman C Daniel, 2016, ARXIV161101540
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Ge R., 2017, P 34 INT C MACH LEAR, V70, P1233
   Ge Rong, 2017, ARXIV171100501
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   HAEFFELE B. D., 2015, ARXIV150607540
   Hardt M., 2016, ARXIV161104231
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Lee Jason D, 2018, ARXIV180301206
   Li Yuanzhi, 2017, ARXIV170509886
   Ma  Siyuan, 2017, ARXIV171206559
   Neyshabur B., 2015, ADV NEURAL INFORM PR, P2422
   Neyshabur  Behnam, 2015, ARXIV151106747
   Nguyen  Quynh, 2017, ARXIV171010928
   Nguyen  Quynh, 2017, ARXIV170408045
   Panageas  I., 2016, ARXIV160500405
   Safran I., 2016, INT C MACH LEARN, P774
   Safran I., 2017, ARXIV171208968
   Shamir O., 2018, ARXIV180406739
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   Tian Yuandong, 2017, ARXIV170300560
   Tu S., 2015, ARXIV150703566
   Vidal Rene, 2017, ARXIV171204741
   Zhang  Jingzhao, 2018, ARXIV180500521
   Zhong Kai, 2017, ARXIV170603175
   Zhou  Pan, 2017, ARXIV170507038
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300036
DA 2019-06-15
ER

PT S
AU Du, SS
   Wang, YN
   Zhai, XY
   Balakrishnan, S
   Salakhutdinov, R
   Singh, A
AF Du, Simon S.
   Wang, Yining
   Zhai, Xiyu
   Balakrishnan, Sivaraman
   Salakhutdinov, Ruslan
   Singh, Aarti
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI How Many Samples are Needed to Estimate a Convolutional Neural Network?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fully-connected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an m-dimensional convolutional filter with linear activation acting on a d-dimensional input, the sample complexity of achieving population prediction error of epsilon is (O) over tilde (m/epsilon(2))(2), whereas the sample-complexity for its FNN counterpart is lower bounded by Omega (d/epsilon(2)) samples. Since, in typical settings m << d, this result demonstrates the advantage of using a CNN. We further consider the sample complexity of estimating a onehidden-layer CNN with linear activation where both the m-dimensional convolutional filter and the r-dimensional output weights are unknown. For this model, we show that the sample complexity is (O) over tilde((m + r)/epsilon(2)) when the ratio between the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs.
C1 [Du, Simon S.; Wang, Yining; Balakrishnan, Sivaraman; Salakhutdinov, Ruslan; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Zhai, Xiyu] MIT, Cambridge, MA 02139 USA.
RP Du, SS (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
FU AFRL grant [FA8750-17-2-0212]; DARPA [D17AP00001]
FX This research was partly funded by AFRL grant FA8750-17-2-0212 and DARPA
   D17AP00001.
CR Anandkumar A., 2015, FEATURE EXTRACTION M, P116
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Arora Sanjeev, 2018, ARXIV180205296
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6241
   Bartlett P. L., 2017, ARXIV1703
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Brutzkus Alon, 2017, ARXIV170207966
   Choromanska A., 2015, ARTIF INTELL, P192
   Du S. S, 2018, ARXIV180301206
   Du S. S., 2017, ARXIV170906129
   Du Simon S., 2017, ARXIV171200779
   Dudley R. M., 1967, J FUNCT ANAL, V1, P290
   Feng J., 2017, ARXIV170507038
   Freeman C Daniel, 2016, ARXIV161101540
   Ge R., 2017, P 34 INT C MACH LEAR, V70, P1233
   Ge Rong, 2017, ARXIV171100501
   Goel S., 2017, ARXIV170906010
   Goel S., 2016, ARXIV161110258
   Goel S., 2018, ARXIV180202547
   GRAHAM RL, 1980, IEEE T INFORM THEORY, V26, P37, DOI 10.1109/TIT.1980.1056141
   HAEFFELE B. D., 2015, ARXIV150607540
   Hardt M., 2016, ARXIV161104231
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Klivans A., 2017, ARXIV170803708
   Konstantinos P., 2017, ARXIV180100171
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2
   Li Yuanzhi, 2017, ARXIV170509886
   Neyshabur Behnam, 2017, ARXIV170709564
   Nguyen Q., 2017, ARXIV171010928
   Nguyen Quynh, 2017, ARXIV170408045
   Qu Q., 2017, ARXIV171200716
   Safran I., 2016, INT C MACH LEARN, P774
   Safran I., 2017, ARXIV171208968
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Singh A, 2016, AAAI
   Singh S., 2018, INT C ART INT STAT, P1327
   Song L., 2017, ADV NEURAL INFORM PR, P5520
   Tian Yuandong, 2017, ARXIV170300560
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   van de Geer S. A, 2000, EMPIRICAL PROCESSES, V6
   van der Vaart A.W, 1998, ASYMPTOTIC STAT, V3
   Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z
   Wasserman  L., 2013, ALL STAT CONCISE COU
   Yu A. W., 2018, ARXIV180409541
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   Zhang Y, 2015, ARXIV151107948
   Zhang  Y., 2017, P IEEE C COMP VIS PA, P4894
   Zhong K., 2017, ARXIV171103440
   Zhong Kai, 2017, ARXIV170603175
NR 51
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300035
DA 2019-06-15
ER

PT S
AU Du, YL
   Liu, ZJ
   Basevi, H
   Leonardis, A
   Freeman, WT
   Tenenbaum, JB
   Wu, JJ
AF Du, Yilun
   Liu, Zhijian
   Basevi, Hector
   Leonardis, Ales
   Freeman, William T.
   Tenenbaum, Joshua B.
   Wu, Jiajun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Exploit Stability for 3D Scene Parsing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Human scene understanding uses a variety of visual and non-visual cues to perform inference on object types, poses, and relations. Physics is a rich and universal cue that we exploit to enhance scene understanding. In this paper, we integrate the physical cue of stability into the learning process by looping in a physics engine into bottom-up recognition models, and apply it to the problem of 3D scene parsing. We first show that applying physics supervision to an existing scene understanding model increases performance, produces more stable predictions, and allows training to an equivalent performance level with fewer annotated training examples. We then present a novel architecture for 3D scene parsing named Prim R-CNN, learning to predict bounding boxes as well as their 3D size, translation, and rotation. With physics supervision, Prim R-CNN outperforms existing scene understanding approaches on this problem. Finally, we show that finetuning with physics supervision on unlabeled real images improves real domain transfer of models training on synthetic data.
C1 [Du, Yilun; Liu, Zhijian; Freeman, William T.; Tenenbaum, Joshua B.; Wu, Jiajun] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Basevi, Hector; Leonardis, Ales] Univ Birmingham, Birmingham, W Midlands, England.
RP Wu, JJ (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM jiajunwu@mit.edu
FU ONR MURI [N00014-16-1-2007]; Center for Brain, Minds, and Machines
   (CBMM); NSF [1447476]; Toyota Research Institute; Facebook; MoD/Dstl;
   EPSRC [EP/N019415/1]
FX This work is in part supported by ONR MURI N00014-16-1-2007, the Center
   for Brain, Minds, and Machines (CBMM), Toyota Research Institute, NSF
   #1447476, and Facebook. We acknowledge MoD/Dstl and EPSRC for providing
   the grant to support the UK academics' involvement in a Department of
   Defense funded MURI project through EPSRC grant EP/N019415/1.
CR Ba  Jimmy, 2015, ICLR
   Bansal Aayush, 2016, CVPR
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Battaglia Peter W., 2016, NIPS
   Brachmann Eric, 2016, CVPR
   Chang Michael B, 2017, ICLR
   Coumans Erwin, 2010, B PHYS ENGINE
   Dai JH, 2017, IEEE T CYBERNETICS, V47, P2460, DOI 10.1109/TCYB.2016.2636339
   Ehrhardt S., 2017, ARXIV170300247
   Eigen D., 2015, ICCV
   Finn Chelsea, 2016, NIPS
   Firman Michael, 2016, CVPR
   Fisher M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366154
   Fisher M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964929
   Fragkiadaki Katerina, 2016, ICLR
   Gupta A., 2010, ECCV
   He  K., 2015, CVPR
   He Kaiming, 2017, ICCV
   Hinton G. E., 2016, NIPS
   Hoiem  Derek, 2005, ICCV
   Huang Jonathan, 2015, ICLR WORKSH
   Jia Zhaoyin, 2013, CVPR
   Lerer Adam, 2016, ICML
   Li Wenbin, 2017, ICRA
   Lin T.-Y, 2017, CVPR
   Liu Zhijian, 2018, ECCV
   McCormac John, 2017, ICCV
   Mottaghi Roozbeh, 2016, ECCV
   Ren S., 2015, NIPS
   Roberts L.G., 1963, THESIS
   Shao Tianjia, 2014, ACM TOG, V33
   Silberman  N., 2012, ECCV
   Song Shuran, 2017, CVPR
   Song Shuran, 2015, CVPR
   Stewart Russell, 2017, AAAI
   Tulsiani S., 2018, CVPR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu J., 2016, THESIS
   Wu JHC, 2017, INT CONF SOFTW ENG, P1, DOI [10.1080/09553002.2017.1364801, 10.1109/ICSESS.2017.8342851]
   Wu Jiajun, 2016, BMVC
   Wu  Jiajun, 2017, CVPR
   Wu JP, 2015, INT CONF SOFTW ENG, P1, DOI 10.1109/ICSESS.2015.7338994
   Zhang Renqiao, 2016, COGSCI
   Zhang Yinda, 2017, CVPR
   Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4
   Zheng Bo, 2013, CVPR
   Zitnick C. L., 2014, ECCV
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301069
DA 2019-06-15
ER

PT S
AU Duan, X
   Huang, WB
   Gan, C
   Wang, JD
   Zhu, WW
   Huang, JZ
AF Duan, Xuguang
   Huang, Wenbing
   Gan, Chuang
   Wang, Jingdong
   Zhu, Wenwu
   Huang, Junzhou
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Weakly Supervised Dense Event Captioning in Videos
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and most real-world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to demonstrate the ability of our model on both dense event captioning and sentence localization in videos.
C1 [Duan, Xuguang; Zhu, Wenwu] Tsinghua Univ, Beijing, Peoples R China.
   [Huang, Wenbing; Huang, Junzhou] Tencent AI Lab, Beijing, Peoples R China.
   [Gan, Chuang] MIT IBM Watson Lab, Beijing, Peoples R China.
   [Wang, Jingdong] Microsoft Res Asia, Beijing, Peoples R China.
RP Duan, X (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM duan_xg@outlook.com; hwenbing@126.com; ganchuang1990@gmail.com;
   jingdw@microsoft.com; wwzhu@tsinghua.edu.cn; joehhuang@tencent.com
FU National Program on Key Basic Research Project [2015CB352300]; National
   Natural Science Foundation of China Major Project [U1611461]
FX This work was supported in part by National Program on Key Basic
   Research Project (No. 2015CB352300), and National Natural Science
   Foundation of China Major Project (No. U1611461).
CR Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   CHIDUME CE, 1987, P AM MATH SOC, V99, P283, DOI 10.2307/2046626
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gao J., 2017, ARXIV170306189
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Karpathy A., 2014, CVPR
   Kay W., 2017, ARXIV170506950
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li Yehao, 2018, ARXIV180408274
   Lin C.-Y., 2004, P 42 ANN M ASS COMP, P605, DOI DOI 10.3115/1218955.1219032
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Naim Iftekhar, 2014, P 28 AAAI C ART INT, P1558
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Shen Zhiqiang, 2017, P IEEE C COMP VIS PA, V2, P10
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Song Young Chol, 2016, P INT JOINT C ART IN, P2025
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2014, ARXIV14124729
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Wang Jingwen, 2018, ARXIV180400100
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Xu Huijuan, 2015, ARXIV150505914
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yao Ting, MSR ASIA MSM ACTIVIT
   Yao Ting, 2016, OPEN REV, V2, P8
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yuan Yitian, 2018, ARXIV180407014
   Zhou L., 2018, ARXIV180400819
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303009
DA 2019-06-15
ER

PT S
AU Duarte, K
   Rawat, YS
   Shah, M
AF Duarte, Kevin
   Rawat, Yogesh S.
   Shah, Mubarak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI VideoCapsuleNet: A Simplified Network for Action Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue and make the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves state-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive similar to 20% improvement on UCF-101 and similar to 15% improvement on J-HMDB in terms of v-mAP scores.
C1 [Duarte, Kevin; Rawat, Yogesh S.; Shah, Mubarak] Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA.
RP Duarte, K (reprint author), Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA.
EM kevin_duarte@knights.ucf.edu; yogesh@crcv.ucf.edu; shah@crcv.ucf.edu
FU Office of the Director of National Intelligence (ODNI), Intelligence
   Advanced Research Projects Activity (IARPA) [D17PC00345]
FX This research is based upon work supported by the Office of the Director
   of National Intelligence (ODNI), Intelligence Advanced Research Projects
   Activity (IARPA), via IARPA R&D Contract No. D17PC00345. The views and
   conclusions contained herein are those of the authors and should not be
   interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of the ODNI, IARPA, or the
   U.S. Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gu Chunhui, 2018, CVPR
   He Jiawei, 2018, WACV
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hinton Geoffrey E, 2018, INT C LEARN REPR
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Kalogeiton Vicky, 2017, ICCV IEEE INT C COMP
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kingma D. P., 2014, ARXIV14126980
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001
   Sabour S., 2017, ADV NEURAL INFORM PR, P3859
   Saha Suman, 2016, BMVC
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Singh Gurkirt, 2017, ONLINE REAL TIME MUL
   Soomro K., 2012, ARXIV12120402
   Yang Zhenheng, 2017, BMVC
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002018
DA 2019-06-15
ER

PT S
AU Dubey, A
   Gupta, O
   Raskar, R
   Naik, N
AF Dubey, Abhimanyu
   Gupta, Otkrist
   Raskar, Ramesh
   Naik, Nikhil
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Maximum Entropy Fine-Grained Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve stateof-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.
C1 [Dubey, Abhimanyu; Gupta, Otkrist; Raskar, Ramesh; Naik, Nikhil] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Dubey, A (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM dubeya@mit.edu; otkrist@mit.edu; raskar@mit.edu; naik@mit.edu
CR Abe S., 2001, NONEXTENSIVE STAT ME, V560
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6240
   Branson S., 2014, ARXIV14062952
   Chen YH, 2009, J MACH LEARN RES, V10, P747
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cui  Yin, 2017, IEEE C COMP VIS PATT
   Deng J., 2009, CVPR09
   Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Golub GH, 1999, SIAM J MATRIX ANAL A, V21, P185, DOI 10.1137/S0895479897326432
   Grandvalet  Yves, ENTROPY REGULARIZATI
   Gull S. F., 1988, MAXIMUM ENTROPY BAYE, P53
   Guo  Chuan, 2017, ARXIV170604599
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   JONSSON D, 1982, J MULTIVARIATE ANAL, V12, P1, DOI 10.1016/0047-259X(82)90080-X
   Khosla  Aditya, NOVEL DATASET FINE G
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2014, THE CIFAR 10 DATASET
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lin  Tsung-Yu, 2017, ARXIV170706772
   Liu ML, 2016, LECT NOTES COMPUT SC, V10040, P337, DOI 10.1007/978-3-319-48674-1_30
   Luo  Yuping, 2016, ARXIV160801281
   Maji S., 2013, ARXIV13065151
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Moghimi Mohammad, 2016, BRIT MACH VIS C BMVC
   Neyshabur Behnam, 2017, ARXIV170709564
   Paskze  Adam, TENSORS DYNAMIC NEUR
   Pereyra G., 2017, ARXIV170106548
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788
   Shawe-Taylor  John, 2009, ARTIF INTELL, P480
   Simon  Marcel, 2017, ARXIV170500487
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Su YH, 2016, IEEE CONF COMPUT
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Szummer M, 2002, ADV NEUR IN, V14, P945
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Zhang N, 2012, PROC CVPR IEEE, P3665, DOI 10.1109/CVPR.2012.6248364
   ZHANG XP, 2016, PROC CVPR IEEE, P1134, DOI DOI 10.1109/CVPR.2016.128
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zhu J, 2009, J MACH LEARN RES, V10, P2531
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300059
DA 2019-06-15
ER

PT S
AU Nguyen, DT
   Kumar, A
   Lau, HC
AF Duc Thien Nguyen
   Kumar, Akshat
   Lau, Hoong Chuin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Credit Assignment For Collective Multiagent RL With Global Rewards
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Scaling decision theoretic planning to large multiagent systems is challenging due to uncertainty and partial observability in the environment. We focus on a multiagent planning model subclass, relevant to urban settings, where agent interactions are dependent on their "collective influence" on each other, rather than their identities. Unlike previous work, we address a general setting where system reward is not decomposable among agents. We develop collective actor-critic RL approaches for this setting, and address the problem of multiagent credit assignment, and computing low variance policy gradient estimates that result in faster convergence to high quality solutions. We also develop difference rewards based credit assignment methods for the collective setting. Empirically our new approaches provide significantly better solutions than previous methods in the presence of global rewards on two real world problems modeling taxi fleet optimization and multiagent patrolling, and a synthetic grid navigation domain.
C1 [Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin] Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.
RP Nguyen, DT (reprint author), Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.
EM dtnguyen.2014@smu.edu.sg; akshatkumar@smu.edu.sg; hclau@smu.edu.sg
FU National Research Foundation Singapore under its Corp Lab @ University
   scheme; Fujitsu Limited; A*STAR graduate scholarship
FX This research project is supported by National Research Foundation
   Singapore under its Corp Lab @ University scheme and Fujitsu Limited.
   First author is also supported by A*STAR graduate scholarship.
CR Agogino A.K., 2004, P 3 INT JOINT C AUT, V2, P980
   Agussurja Lucas, 2018, AAAI C ART INT, P6086
   Amato C, 2015, IEEE INT CONF ROBOT, P1241, DOI 10.1109/ICRA.2015.7139350
   Amato C, 2010, AUTON AGENT MULTI-AG, V21, P293, DOI 10.1007/s10458-009-9103-z
   Asadi K., 2017, ARXIV170900503
   Becker R, 2004, J ARTIF INTELL RES, V22, P423, DOI 10.1613/jair.1497
   Becker R., 2004, P INT JOINT C AUT AG, P302
   Bernstein DS, 2002, MATH OPER RES, V27, P819, DOI 10.1287/moor.27.4.819.297
   Chang YH, 2004, ADV NEUR IN, V16, P807
   Chase J., 2017, IEEE S SER COMP INT, P1
   Ciosek Kamil, 2018, AAAI C ART INT
   Devlin S, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P165
   Diaconis P, 1980, STUDIES INDUCTIVE LO, V2, P233
   Dibangoye Jilles Steeve, 2018, INT C MACH LEARN, P1241
   Durfee Ed, 2013, MULTIAGENT SYSTEMS, P485
   Foerster Jakob, 2018, AAAI C ART INT
   Gomes DA, 2010, J MATH PURE APPL, V93, P308, DOI 10.1016/j.matpur.2009.10.010
   Guestrin C, 2002, ADV NEUR IN, V14, P1523
   Gupta Tarun, 2018, AAAI C ART INT, P6186
   Kok JR, 2006, J MACH LEARN RES, V7, P1789
   Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691
   Liu Liping, 2014, INT C MACH LEARN, P1602
   Lowe R., 2017, ADV NEURAL INFORM PR, V30, P6382
   Nair R., 2005, AAAI, P133
   Nguyen D. T., 2017, ADV NEURAL INFORM PR, P4322
   Nguyen Duc Thien, 2017, AAAI C ART INT, P3036
   Niepert M., 2014, AAAI C ART INT, P2467
   Poupart Pascal, 2003, ADV NEURAL INFORM PR, P823
   Rashid Tabish, 2018, INT C MACH LEARN, P4292
   Robbel Philipp, 2016, AAAI C ART INT, P2537
   Sheldon Daniel R, 2011, ADV NEURAL INFORM PR, P1161
   Silver  D., 2014, ICML, P387
   Sonu Ekhlas, 2015, INT C AUT PLANN SCHE, P202
   Spaan M., 2008, P 7 INT C AUT AG MUL, V1, P525
   Sun Tao, 2015, P INT C MACH LEARN, P853
   Sunehag Peter, 2017, ARXIV170605296
   SUTTON R.S., 1999, NIPS, V99, P1057
   Tan Ming, 1993, P 10 INT C MACH LEAR, P330
   Tumer K, 2009, ADV COMPLEX SYST, V12, P475, DOI 10.1142/S0219525909002295
   Tumer Kagan, 2007, AUTON AGENT MULTI-AG
   Turner K., 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P378
   Varakantham P., 2014, AAAI C ART INT, P2505
   Varakantham Pradeep Reddy, 2012, UNCERTAINTY ARTIFICI, P1471
   Witwicki S. J., 2010, INT C AUT PLANN SCHE, P185
   Yang Jiachen, 2018, INT C LEARN REPR
   Yang Yaodong, 2018, INT C MACH LEARN, P5567
   Zhang Chongjie, 2011, AAAI C ART INT
   Zhang Y, 2013, 2013 INTERNATIONAL CONFERENCE ON MECHANICAL AND AUTOMATION ENGINEERING (MAEE 2013), P110, DOI 10.1109/MAEE.2013.37
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002063
DA 2019-06-15
ER

PT S
AU Dumitrascu, B
   Feng, KR
   Engelhardt, BE
AF Dumitrascu, Bianca
   Feng, Karen
   Engelhardt, Barbara E.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REGRET
AB We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of PolyaGamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.
C1 [Dumitrascu, Bianca] Princeton Univ, Lewis Sigler Inst Integrat Genom, Princeton, NJ 08540 USA.
   [Feng, Karen; Engelhardt, Barbara E.] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.
RP Dumitrascu, B (reprint author), Princeton Univ, Lewis Sigler Inst Integrat Genom, Princeton, NJ 08540 USA.
EM biancad@princeton.edu; karenfeng@princeton.edu; bee@princeton.edu
CR Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P12
   Abeille M., 2017, AISTATS 2017
   AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934
   Agrawal S., 2013, P 30 INT C MACH LEAR, P127
   Shipra,, 2017, J ACM, V64, DOI 10.1145/3088510
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Barber R. F., 2016, STAT ANAL HIGH DIMEN, P15
   Bay SD, 2000, ACM SIGKDD EXPLORATI, V2, P81, DOI [DOI 10.1145/380995.381030, 10.1145/380995.381030]
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Choi HM, 2013, ELECTRON J STAT, V7, P2054, DOI 10.1214/13-EJS837
   Chu W., 2009, P KDD
   Devroye L, 2009, STAT PROBABIL LETT, V79, P2251, DOI 10.1016/j.spl.2009.07.028
   Duff M. O., 2003, P 20 INT C MACH LEAR, P131
   Filippi S., 2010, ADV NEURAL INFORM PR, V22, P586
   Gentile C, 2014, ICML, P757
   Glynn C., 2015, ARXIV151103947
   Hazan E., 2014, P 27 C LEARN THEOR, P197
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Jun K.-S., 2017, ARXIV170600136
   Li L., 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Linderman S., 2015, ADV NEURAL INFORM PR, P3456
   McMahan H. B., 2012, C LEARN THEOR, P44
   Moi T., 1986, REASONING CRIMINAL R, P1
   Osband  Ian, 2015, ARXIV150700300
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Russo D, 2016, J MACH LEARN RES, V17
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Russo  Daniel, 2017, ARXIV170702038
   Scott J., 2012, ADV NEURAL INFORM PR, P1898
   STRENS M, 2000, P 17 INT C MACH LEAR, P943
   Tewari A., 2017, MOBILE HLTH, P495
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Urteaga I., 2017, ARXIV170903162
   Windle J, 2014, ARXIV14050506
   WOODROOFE M, 1979, J AM STAT ASSOC, V74, P799, DOI 10.2307/2286402
   Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304062
DA 2019-06-15
ER

PT S
AU Duncker, L
   Sahani, M
AF Duncker, Lea
   Sahani, Maneesh
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Temporal alignment and latent Gaussian process factor inference in
   population spike trains
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DYNAMICS
AB We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains, which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly, as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model, achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence, as well as trial-to-trial variation in trajectories. Finally, we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data, and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.
C1 [Duncker, Lea; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.
RP Duncker, L (reprint author), UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.
EM duncker@gatsby.ucl.ac.uk; maneesh@gatsby.ucl.ac.uk
FU Simons Foundation [SCGB 323228, 543039]; Gatsby Charitable Foundation
FX We would like to thank Vincent Adam for early contributions to this
   project, Gergo Bohner for helpful discussions, and the Shenoy laboratory
   at Stanford University for sharing the centre-out reaching dataset. This
   work was funded by the Simons Foundation (SCGB 323228, 543039; MS) and
   the Gatsby Charitable Foundation.
CR Abbott P., 2005, MATH J, V9, P689
   Adam V, 2016, P 26 INT WORKSH MACH
   Afshar A, 2011, NEURON, V71, P555, DOI 10.1016/j.neuron.2011.05.047
   Arribas-Gil A, 2014, COMPUT STAT DATA AN, V69, P255, DOI 10.1016/j.csda.2013.08.011
   Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Cuturi M, 2017, ICML, P894
   Damianou A. C., 2013, P AISTATS, V31, P207
   Gao Y., 2016, ADV NEURAL INFORM PR, V29, P163
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   Hensman J., 2015, P 18 INT C ART INT S
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Kaiser M, 2017, ARXIV171002766
   Kazlauskaite I, 2018, ARXIV180302603
   Keogh EJ, 2001, P 2001 SIAM INT C DA, P1, DOI DOI 10.1137/1.9781611972719.1
   Kollmorgen S, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003508
   Lawlor PN, 2018, J COMPUTATIONAL NEUR
   Lazaro-Gredilla M., 2012, ADV NEURAL INFORM PR, P1619
   Lloyd C, 2015, P 32 INT C MACH LEAR
   Macke JH, 2015, ADV STATE SPACE METH, P137
   Mena G, 2014, NEURAL COMPUT, V26, P2790, DOI 10.1162/NECO_a_00676
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387
   Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9
   Petreska B., 2011, ADV NEURAL INF PROCE, V24, P756
   Poole B, 2017, FRONTIERS NEUROSCIEN
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Salimbeni H., 2017, ADV NEURAL INFORM PR, P4591
   Saul A., 2016, 19 INT C ART INT STA, V51, P1431
   Snoek J., 2014, INT C MACH LEARN, P1674
   Titsias M, 2009, ARTIF INTELL, P567
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   Wu A, 2017, ADV NEURAL INFORM PR, P3499
   Wu W, 2013, J COMPUT NEUROSCI, V34, P391, DOI 10.1007/s10827-012-0427-3
   Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008
   Zemel Y, 2017, ARXIV170106876
   Zhao Y, 2017, NEURAL COMPUT, V29, P1293, DOI 10.1162/NECO_a_00953
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P279, DOI 10.1109/TPAMI.2015.2414429
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005006
DA 2019-06-15
ER

PT S
AU Dunner, C
   Parnell, T
   Sarigiannis, D
   Ioannou, N
   Anghel, A
   Ravi, G
   Kandasamy, M
   Pozidis, H
AF Dunner, Celestine
   Parnell, Thomas
   Sarigiannis, Dimitrios
   Ioannou, Nikolas
   Anghel, Andreea
   Ravi, Gummadi
   Kandasamy, Madhusudanan
   Pozidis, Haralampos
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Snap ML: A Hierarchical Framework for Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results, including those obtained using TensorFlow and scikit-learn.
C1 [Dunner, Celestine; Parnell, Thomas; Sarigiannis, Dimitrios; Ioannou, Nikolas; Anghel, Andreea; Pozidis, Haralampos] IBM Res, Zurich, Switzerland.
   [Ravi, Gummadi; Kandasamy, Madhusudanan] IBM Syst, Bangalore, Karnataka, India.
RP Dunner, C (reprint author), IBM Res, Zurich, Switzerland.
EM cdu@zurich.ibm.com; tpa@zurich.ibm.com; rig@zurich.ibm.com;
   nio@zurich.ibm.com; aan@zurich.ibm.com; ravigumm@in.ibm.com;
   madhusudanan@in.ibm.com; hap@zurich.ibm.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Barthelemy Dagenais, 2018, PY4J BRIDGE PYTHON J
   Chang C-C, 2011, ACM T INTELLIGENT SY
   Criteo Labs, 2018, LEARN CLICK THROUGH
   Criteo Labs, 2015, CRIT REL IND LARG EV
   Dunner  C, 2017, ADV NEURAL INFORM PR, P4261
   Dunner  Celestine, 2017, P IEEE INT C BIG DAT, P99
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Marsaglia G., 2003, J STAT SOFTWARE, V8
   Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565
   Meza Gonzalo Gasca, 2017, SAMPLES GOOGLE CLOUD
   NVIDIA, 2018, THRUST
   Parnell T, 2017, IEEE SYM PARA DISTR, P419, DOI 10.1109/IPDPSW.2017.140
   Parnell  Thomas, 2018, FUTURE GENERATION CO
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rambler Digital Solutions, 2017, CRIT 1TB BENCHM
   Smith Virginia, 2018, JMLR, V18, P1
   Stack Overflow, 2016, US LARG DAT TENS
   Sterbenz  Andreas, 2017, USING GOOGLE CLOUD M
   Yu HF, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086743
   Yu HF, 2011, MACH LEARN, V85, P41, DOI 10.1007/s10994-010-5221-8
   Zhang H, 2016, IEEE DATA MINING, P619, DOI [10.1109/ICDM.2016.171, 10.1109/ICDM.2016.0073]
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300024
DA 2019-06-15
ER

PT S
AU Dupont, E
AF Dupont, Emilien
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Disentangled Joint Continuous and Discrete Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.
C1 [Dupont, Emilien] Schlumberger Software Technol Innovat Ctr, Menlo Pk, CA 94025 USA.
RP Dupont, E (reprint author), Schlumberger Software Technol Innovat Ctr, Menlo Pk, CA 94025 USA.
EM dupont@slb.com
CR Arjovsky M., 2017, ARXIV170107875
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Burgess Christopher, 2017, NIPS 2017 DIS WORKSH
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Gao Shuyang, 2018, ARXIV180205822
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Gumbel Emil Julius, 1954, NAT BUR STANDARDS AP, V33
   Han Xiao, 2017, ARXIV170807747
   Higgins I., 2017, ARXIV170708475
   Higgins  Irina, 2016, ICLR 2017
   Higgins  Irina, 2017, ARXIV170703389
   Jang Eric, 2016, ARXIV161101144
   Kim H., 2018, ARXIV180205983
   Kingma D.P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Lake Brenden M, 2017, BEHAV BRAIN SCI, V40
   Liu  Z., 2015, P INT C COMP VIS ICC
   Maddison Chris J, 2016, ARXIV161100712
   Makhzani Alireza, 2017, ADV NEURAL INFORM PR, P1972
   Reed Scott, 2014, P 31 INT C MACH LEAR, V32, P1431
   Rezende D. J, 2014, ARXIV14014082
   Tian Qi Chen, 2018, ARXIV180204942
   Whitney W. F., 2016, ARXIV160206822
   Yang J., 2015, ADV NEURAL INFORM PR, P1099
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300066
DA 2019-06-15
ER

PT S
AU Dutordoir, V
   Salimbeni, H
   Deisenroth, MP
   Hensman, J
AF Dutordoir, Vincent
   Salimbeni, Hugh
   Deisenroth, Marc Peter
   Hensman, James
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gaussian Process Conditional Density Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GPs) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.
C1 [Dutordoir, Vincent; Salimbeni, Hugh; Deisenroth, Marc Peter; Hensman, James] PROWLER Io, Cambridge, England.
   [Salimbeni, Hugh; Deisenroth, Marc Peter] Imperial Coll London, London, England.
RP Dutordoir, V (reprint author), PROWLER Io, Cambridge, England.
EM vincent@prowler.io; hugh@prowler.io; marc@prowler.io; james@prowler.io
CR Adams Ryan P, 2009, ARXIV09124896
   Alvarez Mauricio A, 2012, FDN TRENDS MACHINE L
   Amari Shun-Ichi, 1998, NEURAL COMPUTATION
   Arbel Michael, 2018, KERNEL CONDITIONAL E
   Bishop C. M., 1994, MIXTURE DENSITY NETW
   Bodin Erik, 2017, ARXIV170705534
   Bui Thang D, 2015, BLACK BOX LEARN INF
   Cremer Chris, 2018, ARXIV180103558
   Dai Zhenwen, 2017, ADV NEURAL INFORM PR
   Dai Zhenwen, 2015, INT C LEARN REPR
   Damianou Andreas, 2015, UNCERTAINTY ARTIFICI
   Depeweg Stefan, 2016, INT C LEARN REPR
   Girard Agathe, 2003, ADV NEURAL INFORM PR
   Glorot Xavier, 2010, ARTIFICIAL INTELLIGE
   Hensman James, 2013, UNCERTAINTY ARTIFICI
   Hoffman Matthew D., 2013, J MACHINE LEARNING R
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P, 2014, ADV NEURAL INFORM PR
   Lake B., 2015, SCIENCE
   Lawrence Neil D, 2006, INT C MACH LEARN
   Matthews Alexander, 2016, ARTIFICIAL INTELLIGE
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rezende D. J., 2014, INT C MACH LEARN
   Sohn Kihyuk, ADV NEURAL INFORM PR
   Sriperumbudur B., 2017, J MACHINE LEARNING R, V18, P1
   Titsias Michalis, 2010, ARTIFICIAL INTELLIGE
   Titsias Michalis, 2013, ADV NEURAL INFORM PR
   Trippe Brian L, 2017, BAYES DEEP LEARN WOR
   Wang Chunyi, 2012, ARXIV12126246
   Wu Yuhuai, 2016, ARXIV161104273
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302040
DA 2019-06-15
ER

PT S
AU Dvurechensky, P
   Dvinskikh, D
   Gasnikov, A
   Uribe, CA
   Nedic, A
AF Dvurechensky, Pavel
   Dvinskikh, Darina
   Gasnikov, Alexander
   Uribe, Cesar A.
   Nedic, Angelia
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISTANCE
AB We study the decentralized distributed computation of discrete approximations for the regularized Wasserstein barycenter of a finite set of continuous probability measures distributedly stored over a network. We assume there is a network of agents/machines/computers, and each agent holds a private continuous probability measure and seeks to compute the barycenter of all the measures in the network by getting samples from its local measure and exchanging information with its neighbors. Motivated by this problem, we develop, and analyze, a novel accelerated primal-dual stochastic gradient method for general stochastic convex optimization problems with linear equality constraints. Then, we apply this method to the decentralized distributed optimization setting to obtain a new algorithm for the distributed semi-discrete regularized Wasserstein barycenter problem. Moreover, we show explicit non-asymptotic complexity for the proposed algorithm. Finally, we show the effectiveness of our method on the distributed computation of the regularized Wasserstein barycenter of univariate Gaussian and von Mises distributions, as well as some applications to image aggregation.(1)
C1 [Dvurechensky, Pavel; Dvinskikh, Darina] RAS, Inst Informat Transmiss Problems, Weierstrass Inst Appl Anal & Stochast, Moscow, Russia.
   [Gasnikov, Alexander] RAS, Inst Informat Transmiss Problems, Moscow Inst Phys & Technol, Moscow, Russia.
   [Uribe, Cesar A.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Nedic, Angelia] Arizona State Univ, Moscow Inst Phys & Technol, Tempe, AZ 85287 USA.
RP Dvurechensky, P (reprint author), RAS, Inst Informat Transmiss Problems, Weierstrass Inst Appl Anal & Stochast, Moscow, Russia.
EM pavel.dvurechensky@wias-berlin.de; darina.dvinskikh@wias-berlin.de;
   gasnikov@yandex.ru; cauribe@mit.edu; angelia.nedich@asu.edu
FU National Science Foundation; CPS [15-44953]; Russian Science Foundation
   [18-71-10108]
FX The work of A. Nedic and C.A. Uribe in Sect. 5 is supported by the
   National Science Foundation under grant no. CPS 15-44953. The research
   by P. Dvurechensky, D. Dvinskikh, and A. Gasnikov in Sect. 3 and Sect. 4
   was funded by the Russian Science Foundation (project 18-71-10108).
CR Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741
   Anikin AS, 2017, COMP MATH MATH PHYS+, V57, P1262, DOI 10.1134/S0965542517080048
   Arjovsky M, 2017, ARXIV170107875
   Beiglbock M, 2013, FINANC STOCH, V17, P477, DOI 10.1007/s00780-013-0205-8
   Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439
   Bigot J, 2017, ANN I H POINCARE-PR, V53, P1, DOI 10.1214/15-AIHP706
   Buttazzo G, 2012, PHYS REV A, V85, DOI 10.1103/PhysRevA.85.062502
   Chernov A, 2016, LECT NOTES COMPUT SC, V9869, P391, DOI 10.1007/978-3-319-44914-2_31
   Claici S., 2018, P MACHINE LEARNING R, V80, P999
   Cuturi M., 2014, INT C MACH LEARN, P685
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600
   Del Barrio E, 1999, ANN PROBAB, V27, P1009, DOI 10.1214/aop/1022677394
   Dvurechensky P., 2018, ARXIV180603915
   Dvurechensky P., 2016, SUPPL P 9 INT C DISC, P584
   Dvurechensky P., 2017, ARXIV170607622
   Dvurechensky P., 2018, P MACHINE LEARNING R, V80, P1367
   Dvurechensky P, 2016, J OPTIMIZ THEORY APP, V171, P121, DOI 10.1007/s10957-016-0999-6
   Ebert J., 2017, ARXIV170303658
   Gasnikov AV, 2016, COMP MATH MATH PHYS+, V56, P514, DOI 10.1134/S0965542516040084
   Genevay Aude, 2016, ADV NEURAL INFORM PR, P3440
   Guigues V, 2017, OPTIM METHOD SOFTW, V32, P1033, DOI 10.1080/10556788.2017.1350177
   Ho N., 2017, INT C MACH LEARN, V70, P1501
   Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199
   Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801
   Kusner M., 2015, P 32 INT C MACH LEAR, P957
   Lan G., 2018, MATH PROGRAM, P1
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Monge G, 1781, MEMOIRE THEORIE DEBL
   Nedic A., 2017, ARXIV170402718
   Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316
   Nedic A, 2017, IEEE T AUTOMAT CONTR, V62, P5538, DOI 10.1109/TAC.2017.2690401
   Nedic A, 2017, P AMER CONTR CONF, P3950, DOI 10.23919/ACC.2017.7963560
   Olfati-Saber R., 2006, BELIEF CONSENSUS DIS, P169
   Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387
   Rogozin A., 2018, ARXIV180506045
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18
   Scaman K., 2017, P 34 INT C MACH LEAR, P3027
   Solomon J., 2014, P 31 INT C MACH LEAR, V32
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Staib M., 2017, ADV NEURAL INFORM PR, P2644
   Uribe C. A., 2018, ARXIV180900710
   Uribe C. A., 2017, ARXIV171200232
   Uribe CA, 2018, IEEE DECIS CONTR P, P6544, DOI 10.1109/CDC.2018.8619160
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005035
DA 2019-06-15
ER

PT S
AU Dwivedi, R
   Ho, N
   Khamaru, K
   Wainwright, MJ
   Jordan, MI
AF Dwivedi, Raaz
   Ho, Nhat
   Khamaru, Koulik
   Wainwright, Martin J.
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Theoretical guarantees for the EM algorithm when applied to
   mis-specified Gaussian mixture models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONVERGENCE PROPERTIES; FINITE; IDENTIFIABILITY
AB Recent years have witnessed substantial progress in understanding the behavior of EM for mixture models that are correctly specified. Given that model mis-specification is common in practice, it is important to understand EM in this more general setting. We provide non-asymptotic guarantees for the population and sample-based EM algorithms when used to estimate parameters of certain mis-specified Gaussian mixture models. Due to mis-specification, the EM iterates no longer converge to the true model and instead converge to the projection of the true model onto the fitted model class. We provide two classes of theoretical guarantees: (a) a characterization of the bias introduced due to the mis-specification; and (b) guarantees of geometric convergence of the population EM to the model projection given a suitable initialization. This geometric convergence rate for population EM implies that the EM algorithm based on n samples converges to an estimate with 1/root n accuracy. We validate our theoretical findings in different cases via several numerical examples.
C1 [Dwivedi, Raaz; Ho, Nhat; Khamaru, Koulik; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Wainwright, Martin J.] Univ Calif Berkeley, Voleon Grp, Berkeley, CA USA.
RP Dwivedi, R (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM raaz.rsk@berkeley.edu; minhnhat@berkeley.edu; koulik@berkeley.edu;
   wainwrig@berkeley.edu; jordan@berkeley.edu
CR Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435
   Cai T. T., ANN STAT
   Daskalakis C., 2017, P 2017 C LEARN THEOR
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Hao B., J MACHINE LEARNING R
   Heinrich P, 2018, ANN STAT, V46, P2844, DOI 10.1214/17-AOS1641
   Jin C., 2016, ADV NEURAL INFORM PR, V29
   Ledoux M., 1991, PROBABILITY BANACH S
   Ma JW, 2000, NEURAL COMPUT, V12, P2881, DOI 10.1162/089976600300014764
   TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862
   van der Vaart A. W., 2000, WEAK CONVERGENCE EMP
   Vershynin R., ARXIV10113027V7
   Villani C., 2008, OPTIMAL TRANSPORT OL
   Wang Z., 2015, ADV NEURAL INFORM PR, V28
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Xu J., 2016, ADV NEURAL INFORM PR, V29
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
   Nguyen X, 2013, ANN STAT, V41, P370, DOI 10.1214/12-AOS1065
   Yan BW, 2017, ADV NEUR IN, V30
   Yi X., 2015, ADV NEURAL INFORM PR, V28
NR 20
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004026
DA 2019-06-15
ER

PT S
AU Dziugaite, GK
   Roy, DM
AF Dziugaite, Gintare Karolina
   Roy, Daniel M.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Data-dependent PAC-Bayes priors via differential privacy
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BOUNDS
AB The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how an e-differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors can also yield generalization bounds. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes bound given control of the 2-Wasserstein distance to an epsilon-differentially private stationary distribution. We study our data-dependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous.
C1 [Dziugaite, Gintare Karolina] Univ Cambridge, Element AI, Cambridge, England.
   [Roy, Daniel M.] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP Dziugaite, GK (reprint author), Univ Cambridge, Element AI, Cambridge, England.
FU EPSRC studentship; NSERC Discovery Grant; Ontario Early Researcher Award
FX The authors would like to thank Olivier Catoni, Pascal Germain, Mufan
   Li, David McAllester, and Alexander Rakhlin, John Shawe-Taylor, for
   helpful discussions. This research was carried out in part while the
   authors were visiting the Simons Institute for the Theory of Computing
   at UC Berkeley. GKD was additionally supported by an EPSRC studentship.
   DMR was additionally supported by an NSERC Discovery Grant and Ontario
   Early Researcher Award.
CR Alquier P, 2018, MACH LEARN, V107, P887, DOI 10.1007/s10994-017-5690-0
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6241
   Bassily R., 2014, ARXIV14057085V2CSLG
   Bassily R, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1046, DOI 10.1145/2897518.2897566
   Begin L., 2016, P 19 INT C ART INT S, P435
   Ben London, 2017, ADV NEURAL INFORM PR, P2931
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Catoni O., 2007, LECT NOTES MONOGRAPH, DOI [10.1214/074921707000000391, DOI 10.1214/074921707000000391]
   Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21
   Dwork C., 2015, P 47 ANN ACM S THEOR, P117
   Dwork C., 2015, ADV NEURAL INFORM PR, P2350
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dziugaite G. K., 2017, P 33 C UNC ART INT U
   Erdogdu M. A., 2018, ARXIV181012361
   Germain P., 2016, ADV NEURAL INFORM PR, P1884
   Grunwald P. D., 2016, ARXIV160500252
   Grunwald P. D., 2017, ARXIV171007732
   Kifer D., 2012, J MACHINE LEARNING R, V1, P41
   Langford J., 2001, CMUCS01102
   Langford J., 2002, THESIS
   Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013
   Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3
   Maurer A., 2004, ARXIVCS0411099CSLG
   McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Minami K., 2016, ADV NEURAL INFORM PR, P956
   Mir D. J., 2013, THESIS
   Neyshabur B., 2017, ADV NEURAL INFORM PR, P5949
   Neyshabur B., 2017, P INT C LEARN REPR I
   Oneto L, 2017, PATTERN RECOGN LETT, V89, P31, DOI 10.1016/j.patrec.2017.02.006
   Parrado-Hernandez E, 2012, J MACH LEARN RES, V13, P3507
   Raginsky M., 2017, P C LEARN THEOR COLT
   Rivasplata O., 2018, ADV NEURAL INFO P SY, V31, P9234
   Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466
   Smith S. L., 2018, P INT C LEARN REPR I
   Thiemann Niklas, 2017, INT C ALG LEARN THEO, V76, P466
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Wang Y.-X., 2015, P 32 INT C MACH LEAR, P2493
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003003
DA 2019-06-15
ER

PT S
AU Efroni, Y
   Dalal, G
   Scherrer, B
   Mannor, S
AF Efroni, Yonathan
   Dalal, Gal
   Scherrer, Bruno
   Mannor, Shie
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multiple-Step Greedy Policies in Online and Approximate Reinforcement
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work [5], multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.
C1 [Efroni, Yonathan; Dalal, Gal; Mannor, Shie] Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.
   [Scherrer, Bruno] INRIA, Villers Les Nancy, France.
RP Efroni, Y (reprint author), Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.
EM jonathan.efroni@gmail.com; gald@campus.technion.ac.il;
   bruno.scherrer@inria.fr; shie@ee.technion.ac.il
FU Israel Science Foundation [1380/16]
FX This work was partially funded by the Israel Science Foundation under
   contract 1380/16.
CR Bagnell JA, 2004, ADV NEUR IN, V16, P831
   Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953
   Bouzy B, 2004, INT FED INFO PROC, V135, P159
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Efroni  Yonathan, 2018, ARXIV180203654
   Ernst D, 2009, IEEE T SYST MAN CY B, V39, P517, DOI 10.1109/TSMCB.2008.2007630
   Farahmand A. M., 2010, ADV NEURAL INFORM PR, V23, P568
   Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181
   Kakade  S., 2002, ICML, P267
   Konda VR, 1999, SIAM J CONTROL OPTIM, V38, P94, DOI 10.1137/S036301299731669X
   Lazaric  Alessandro, 2016, J MACHINE LEARNING R, V17, P583
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384
   Munos  Remi, 2003, P 20 INT C INT C MAC, P560
   Negenborn R. R., 2005, IFAC P VOLUMES, V38, P354
   Perkins  Steven, 2013, STOCHASTIC SYSTEMS, V2, P409
   Petrik Marek, 2009, ADV NEURAL INFORM PR, P1265
   Puterman M. L., 1994, MARKOV DECISION PROC
   Scherrer Bruno, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P35, DOI 10.1007/978-3-662-44845-8_3
   Scherrer B, 2014, P 31 INT C MACH LEAR, V32, P1314
   Scherrer  Bruno, 2016, MARKOV DECISION PROC
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sheppard B, 2002, ARTIF INTELL, V134, P241, DOI 10.1016/S0004-3702(01)00166-7
   Silver D., 2017, ARXIV171201815
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Strehl AL, 2009, J MACH LEARN RES, V10, P2413
   Tamar Aviv, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P336, DOI 10.1109/ICRA.2017.7989043
   Tesauro G, 1997, ADV NEUR IN, V9, P1068
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305027
DA 2019-06-15
ER

PT S
AU Ellis, K
   Ritchie, D
   Solar-Lezama, A
   Tenenbaum, JB
AF Ellis, Kevin
   Ritchie, Daniel
   Solar-Lezama, Armando
   Tenenbaum, Joshua B.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Infer Graphics Programs from Hand-Drawn Images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of (LTEX)-T-A. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.
C1 [Ellis, Kevin; Solar-Lezama, Armando; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA.
   [Ritchie, Daniel] Brown Univ, Providence, RI 02912 USA.
RP Ellis, K (reprint author), MIT, Cambridge, MA 02139 USA.
EM ellisk@mit.edu; daniel_ritchie@brown.edu; asolar@csail.mit.edu;
   jbt@mit.edu
FU NSF GRFP; NSF [1753684]; MUSE program (DARPA) [FA8750-14-2-0242]; AFOSR
   [FA9550-16-1-0012]; Center for Brains, Minds and Machines (CBMM) - NSF
   STC award [CCF-1231216]
FX We are grateful for advice from Will Grathwohl and Jiajun Wu on the
   neural architecture, and for funding from NSF GRFP, NSF Award #1753684,
   the MUSE program (DARPA grant FA8750-14-2-0242), and AFOSR award
   FA9550-16-1-0012. This material is based upon work supported by the
   Center for Brains, Minds and Machines (CBMM), funded by NSF STC award
   CCF-1231216.
CR Balog M., 2016, ARXIV161101989
   Beltramelli  Tony, 2017, ABS170507962 CORR
   Deng  Yuntian, 2017, ICML
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Forbus K, 2011, TOP COGN SCI, V3, P648, DOI 10.1111/j.1756-8765.2011.01149.x
   Ganin  Yaroslav, 2018, ICML
   Gaunt Alexander L, 2016, ARXIV160804428
   Goodfellow I., DEEP LEARNING
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hempel B, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P379, DOI 10.1145/2984511.2984575
   Hinton G. E., 2016, NIPS
   Huang HB, 2017, IEEE ICC
   Jaderberg M., 2015, NIPS
   Kalyan Ashwin, 2018, ICLR
   Levin Leonid A, 1973, PROBL PEREDACHI INF, V9, P115
   Lezama Armando Solar, 2008, THESIS
   Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951
   Paige  Brooks, 2016, INT C MACH LEARN, P3040
   Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2814270.2814310, 10.1145/2858965.2814310]
   Ritchie  Daniel, 2016, NIPS
   Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2
   Solomonoff Raymond J, 1984, OPTIMUM SEQUENTIAL S
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu  Jiajun, 2017, CVPR
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000055
DA 2019-06-15
ER

PT S
AU Ellis, K
   Morales, L
   Sable-Meyer, M
   Solar-Lezama, A
   Tenenbaum, JB
AF Ellis, Kevin
   Morales, Lucas
   Sable-Meyer, Mathias
   Solar-Lezama, Armando
   Tenenbaum, Joshua B.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Library Learning for Neurally-Guided Bayesian Program Induction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Successful approaches to program induction require a hand-engineered domain-specific language (DSL), constraining the space of allowed programs and imparting prior knowledge of the domain. We contribute a program induction algorithm called EC2 that learns a DSL while jointly training a neural network to efficiently search for programs in the learned DSL. We use our model to synthesize functions on lists, edit text, and solve symbolic regression problems, showing how the model learns a domain-specific library of program components for expressing solutions to problems in the domain.
C1 [Ellis, Kevin; Morales, Lucas; Solar-Lezama, Armando; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA.
   [Sable-Meyer, Mathias] ENS Paris Saclay, Cachan, France.
RP Ellis, K (reprint author), MIT, Cambridge, MA 02139 USA.
EM ellisk@mit.edu; lucasem@mit.edu; mathsm@mit.edu; asolar@csail.mit.edu;
   jbt@mit.edu
FU NSF GRFP; AFOSR [FA9550-16-1-0012]; MIT-IBM Watson AI Lab; MUSE program
   (Darpa) [FA8750-14-2-0242]; AWS ML Research Award; Center for Brains,
   Minds and Machines (CBMM) - NSF STC award [CCF-1231216]
FX We are grateful for collaborations with Eyal Dechter, whose EC algorithm
   directly inspired this work, and for funding from the NSF GRFP, AFOSR
   award FA9550-16-1-0012, the MIT-IBM Watson AI Lab, the MUSE program
   (Darpa grant FA8750-14-2-0242), and an AWS ML Research Award. This
   material is based upon work supported by the Center for Brains, Minds
   and Machines (CBMM), funded by NSF STC award CCF-1231216.
CR Allamanis M, 2014, 22ND ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (FSE 2014), P472, DOI 10.1145/2635868.2635901
   Alur Rajeev, 2016, ARXIV161107627
   Balog Matej, 2016, ICLR
   Bishop C. M., 2006, PATTERN RECOGNITION
   Cho K, 2014, ARXIV14061078
   Cohn Trevor, JMLR
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Dechter Eyal, 2013, IJCAI
   Devlin Jacob, 2017, ICML
   Devlin Jacob, 2017, NIPS
   Ellis Kevin, NIPS
   Ellis Kevin, 2018, NIPS
   Ellis Kevin, 2016, ADV NEURAL INFORM PR
   Feser John K, 2015, PLDI
   Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423
   Henderson Robert John, 2013, THESIS
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hwang Irvin, 2011, ARXIV11105667
   Johnson Justin, CVPR
   Kalyan Ashwin, 2018, ICLR
   Katayama S, 2015, LECT NOTES ARTIF INT, V9205, P111, DOI 10.1007/978-3-319-21365-1_12
   Koza J. R., 1993, GENETIC PROGRAMMING
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lau Tessa, 2001, THESIS
   Le Tuan Anh, 2017, AISTATS
   Lezama Armando Solar, 2008, THESIS
   LIANG P., 2010, ICML
   Lin Dianhuan, 2014, ECAI 2014
   Menon A., 2013, P 30 INT C MACH LEAR, P187
   Muggleton SH, 2015, MACH LEARN, V100, P49, DOI 10.1007/s10994-014-5471-y
   O'Donnell TJ, 2015, PRODUCTIVITY REUSE L
   Osera PM, 2015, ACM SIGPLAN NOTICES, V50, P619, DOI 10.1145/2737924.2738007
   Pierce BC, 2002, TYPES PROGRAMMING LA
   Polikarpova N, 2016, ACM SIGPLAN NOTICES, V51, P522, DOI [10.1145/2980983.2908093, 10.1145/2908080.2908093]
   Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2814270.2814310, 10.1145/2858965.2814310]
   Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150
   Schmid U, 2011, COGN SYST RES, V12, P237, DOI 10.1016/j.cogsys.2010.12.002
   Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2
   Shin Richard, 2018, PROGRAM SYNTHESIS LE
   Solomonoff Ray J, 1989, 6 ISR C ART INT COMP
   Stuhlmuller Andreas, 2013, NIPS
NR 41
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002036
DA 2019-06-15
ER

PT S
AU Elsayed, GF
   Shankar, S
   Cheung, B
   Papernot, N
   Kurakin, A
   Goodfellow, I
   Sohl-Dickstein, J
AF Elsayed, Gamaleldin F.
   Shankar, Shreya
   Cheung, Brian
   Papernot, Nicolas
   Kurakin, Alexey
   Goodfellow, Ian
   Sohl-Dickstein, Jascha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial Examples that Fool both Computer Vision and Time-Limited
   Humans
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MODELS
AB Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
C1 [Elsayed, Gamaleldin F.; Kurakin, Alexey; Goodfellow, Ian; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA.
   [Shankar, Shreya] Stanford Univ, Stanford, CA 94305 USA.
   [Cheung, Brian] Univ Calif Berkeley, Berkeley, CA USA.
   [Papernot, Nicolas] Penn State Univ, University Pk, PA 16802 USA.
RP Elsayed, GF (reprint author), Google Brain, Mountain View, CA USA.
EM gamaleldin.elsayed@gmail.com; jaschasd@google.com
CR Athalye Anish, 2017, ROBUST ADVERSARIAL E
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   Brown T. B., 2017, ARXIV171209665
   Buckman Jacob, 2018, INT C LEARN REPR
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Eckstein MP, 2017, CURR BIOL, V27, P2827, DOI 10.1016/j.cub.2017.07.068
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   Gatys Leon A., 2015, ARXIV150806576
   Geirhos Robert, 2017, ARXIV170606969
   Goodfellow I., 2017, ATTACKING MACHINE LE
   Goodfellow IJ, 2014, ARXIV14126572
   Grosse Kathrin, 2017, Computer Security - ESORICS 2017. 22nd European Symposium on Research in Computer Security. Proceedings: LNCS10493, P62, DOI 10.1007/978-3-319-66399-9_4
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   He K., 2016, ARXIV E PRINTS
   Hillis JM, 2002, SCIENCE, V298, P1627, DOI 10.1126/science.1075396
   Ibbotson M, 2011, CURR OPIN NEUROBIOL, V21, P553, DOI 10.1016/j.conb.2011.05.012
   Kolter J. Z., 2017, ARXIV171100851
   KOVACS G, 1995, P NATL ACAD SCI USA, V92, P5587, DOI 10.1073/pnas.92.12.5587
   Kummerer M, 2014, ARXIV14111045
   Kummerer Matthias, 2017, J VISION, V17, P1147
   Kurakin A., 2016, ARXIV E PRINTS
   Kurakin Alexey, 2016, ICLR 2017 WORKSH
   Land MF, 2012, OXF ANIMAL BIOL SER, P1
   Liu Y., 2016, ARXIV161102770
   Madry Aleksander, 2017, ARXIV170606083
   McIntosh Lane T, 2016, Adv Neural Inf Process Syst, V29, P1369
   Olshausen Bruno A, 2013, 20 YEARS COMPUTATION, P243
   Papernot N., 2017, P 2017 ACM AS C COMP, P506, DOI DOI 10.1145/3052973.3053009
   Papernot  N., 2016, ARXIV160507277
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot Nicolas, 2015, ABS151107528 CORR
   Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z
   Rajalingham Rishi, 2018, BIORXIV
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Sutskever I., 2017, ARXIV170707397
   Szegedy C., 2015, ARXIV E PRINTS
   Szegedy C., 2016, ARXIV E PRINTS
   Szegedy C, 2013, ARXIV13126199
   Tramer F., 2017, ARXIV E PRINTS
   VANESSEN DC, 1995, INTRO NEURAL ELECT N, P45
   Xu Weilin, 2017, ARXIV170401155
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
NR 43
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303087
DA 2019-06-15
ER

PT S
AU Elsayed, GF
   Krishnan, D
   Mobahi, H
   Regan, K
   Bengio, S
AF Elsayed, Gamaleldin F.
   Krishnan, Dilip
   Mobahi, Hossein
   Regan, Kevin
   Bengio, Samy
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Large Margin Deep Networks for Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a formulation of deep learning that aims at producing a large margin classifier. The notion of margin, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer. Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any l(p) norm (p > 1) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks: generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques such as weight decay, dropout, and batch norm.(2 )
C1 [Elsayed, Gamaleldin F.; Krishnan, Dilip; Mobahi, Hossein; Regan, Kevin; Bengio, Samy] Google Res, Salt Lake City, UT 84102 USA.
RP Elsayed, GF (reprint author), Google Res, Salt Lake City, UT 84102 USA.
EM gamaleldin@google.com; dilipkay@google.com; hmobahi@google.com;
   kevinregan@google.com; bengio@google.com
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cisse  M., 2017, INT C MACH LEARN, P854
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Drucker H., 1997, NEURAL INFORM PROCES, V9, P155
   Gal  Yarin, 2017, ARXIV170302910
   Goodfellow IJ, 2014, ARXIV14126572
   Guo C., 2017, ARXIV171100117
   Hein M., 2017, ADV NEURAL INFORM PR, P2266
   Hinton  G., NEURAL NETWORKS MACH
   Huang R., 2015, ARXIV151103034
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kurakin A., 2016, ARXIV E PRINTS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu W., 2016, P 33 INT C MACH LEAR, P507
   Madry Aleksander, 2017, ARXIV170606083
   Matyasko A, 2017, IEEE IJCNN, P300, DOI 10.1109/IJCNN.2017.7965869
   Moosavi-Dezfooli Seyed-Mohsen, 2016, ARXIV161008401
   Papernot N., 2017, P 2017 ACM AS C COMP, P506, DOI DOI 10.1145/3052973.3053009
   Papernot Nicolas, 2016, ARXIV160202697
   Poovendran Radha, 2017, ARXIV170405051
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Reed S., 2014, ARXIV14126596
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Sokolic Jure, 2016, CORR
   Soudry Daniel, 2017, ARXIV171010345
   Sukhbaatar S., 2014, ARXIV14062080
   Sun Shizhao, 2015, CORR
   Sutskever I., 2017, ARXIV170707397
   Szegedy C., 2013, CORR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tieleman T., 2012, MACH LEARN, V4, P26
   Vapnik VN, 1995, NATURE STAT LEARNING
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Xuezhi Liang, 2017, Neural Information Processing. 24th International Conference, ICONIP 2017. Proceedings: LNCS 10635, P413, DOI 10.1007/978-3-319-70096-0_43
   Zagoruyko S, 2016, ARXIV160507146
NR 37
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300078
DA 2019-06-15
ER

PT S
AU Erdogdu, MA
   Mackey, L
   Shamir, O
AF Erdogdu, Murat A.
   Mackey, Lester
   Shamir, Ohad
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Global Non-convex Optimization with Discretized Diffusions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONVERGENCE
AB An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems. We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure.
C1 [Erdogdu, Murat A.] Univ Toronto, Toronto, ON, Canada.
   [Erdogdu, Murat A.] Vector Inst, Toronto, ON, Canada.
   [Mackey, Lester] Microsoft Res, Montreal, PQ, Canada.
   [Shamir, Ohad] Weizmann Inst Sci, Rehovot, Israel.
RP Erdogdu, MA (reprint author), Univ Toronto, Toronto, ON, Canada.; Erdogdu, MA (reprint author), Vector Inst, Toronto, ON, Canada.
EM erdogdu@cs.toronto.edu; lmackey@microsoft.com;
   ohad.shamir@weizmann.ac.il
CR Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Cattiaux P, 2014, LECT NOTES MATH, V2123, P231, DOI 10.1007/978-3-319-11970-0_9
   Cerrai S., 2001, 2 ORDER PDES FINITE, V1762
   Chen C., 2015, ADV NEURAL INFORM PR, P2278
   Cheng X., 2018, ARXIV180501648
   Dalalyan AS, 2012, J COMPUT SYST SCI, V78, P1423, DOI 10.1016/j.jcss.2011.12.023
   Dalalyan A. S., 2017, ARXIV170404752
   Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183
   Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238
   Dwivedi R., 2018, ARXIV180102309
   Eberle A, 2016, PROBAB THEORY REL, V166, P851, DOI 10.1007/s00440-015-0673-1
   Erdogdu M. A., 2019, MULTIVARIATE S UNPUB
   Gao X., 2018, ARXIV180904618
   GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055
   Gorham J., 2016, ARXIV161106972
   Gradshteyn I. S., 2014, TABLE INTEGRALS SERI
   Jameson GJO, 2013, AM MATH MON, V120, P936, DOI 10.4169/amer.math.monthly.120.10.936
   Khasminskii R., 2011, STOCHASTIC STABILITY, V66
   Ma Y.-A., 2015, ADV NEURAL INFORM PR, V2, P2917
   Mathai AM, 1992, QUADRATIC FORMS RAND
   Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3
   Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527
   Pardoux E, 2001, ANN PROBAB, V29, P1061
   Raginsky  M., 2017, ARXIV170203849
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Wang F., 2016, ARXIV160804471
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Xu P., 2018, ADV NEURAL INFORM PR, P3126
   Zisserman A., 2004, MULTIPLE VIEW GEOMET
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004025
DA 2019-06-15
ER

PT S
AU Eriksson, D
   Dong, K
   Lee, EH
   Bindel, D
   Wilson, AG
AF Eriksson, David
   Dong, Kun
   Lee, Eric Hans
   Bindel, David
   Wilson, Andrew Gordon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Scaling Gaussian Process Regression with Derivatives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONVOLUTION
AB Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at n points in d dimensions requires linear solves and log determinants with an n(d + 1) x n(d + 1) positive definite matrix - leading to prohibitive O(n(3)d(3)) computations for standard direct methods. We propose iterative solvers using fast O(nd) matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, enables Bayesian optimization with derivatives to scale to high-dimensional problems and large evaluation budgets.
C1 [Eriksson, David; Dong, Kun] Cornell Univ, Ctr Appl Math, Ithaca, NY 14853 USA.
   [Lee, Eric Hans; Bindel, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   [Wilson, Andrew Gordon] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA.
RP Eriksson, D (reprint author), Cornell Univ, Ctr Appl Math, Ithaca, NY 14853 USA.
EM dme65@cornell.edu; kd383@cornell.edu; ehl59@cornell.edu;
   bindel@cornell.edu; andrew@cornell.edu
FU NSF [DMS-1620038, HS-1563887]; Facebook Research
FX We thank NSF DMS-1620038, NSF HS-1563887, and Facebook Research for
   support.
CR Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003
   Ben-Ari Einat Neumann, 2007, Quality Engineering, V19, P327, DOI 10.1080/08982110701580930
   Constantine PG, 2015, ACTIVE SUBSPACES EME
   Cutajar Kurt, 2016, P 33 INT C MACH LEAR, P2529
   Dong Kun, 2017, ADV NEURAL INFORM PR, P6330
   Forrester A., 2008, ENG DESIGN VIA SURRO
   Gardner Jacob R, 2018, ARTIFICIAL INTELLIGE
   Gingras David, 2010, Proceedings of the 2010 Seventh Canadian Conference on Computer and Robot Vision (CRV 2010), P191, DOI 10.1109/CRV.2010.32
   Hadsell R, 2010, INT J ROBOT RES, V29, P981, DOI 10.1177/0278364910369996
   Han I, 2015, ICML, P908
   Harbrecht H, 2012, APPL NUMER MATH, V62, P428, DOI 10.1016/j.apnum.2011.10.001
   Hensman James, 2013, P C UNC ART INT UAI
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Konolige K, 2010, SPR TRA ADV ROBOT, V66, P201
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Macedo I, 2011, COMPUT GRAPH FORUM, V30, P27, DOI 10.1111/j.1467-8659.2010.01785.x
   MacKay D. J, 2003, INFORM THEORY INFERE
   Meijering EHW, 1999, IEEE T IMAGE PROCESS, V8, P192, DOI 10.1109/83.743854
   Puget Sound LiDAR Consortium, 2002, MOUNT SAINT HEL LIDA
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rasmussen CE, 2001, ADV NEUR IN, V13, P294
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Surjanovic S., 2018, VIRTUAL LIB SIMULATI
   Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974
   Wang Z, 2013, P 23 INT JOINT C ART, P1778
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson Andrew G, 2015, ADV NEURAL INFORM PR, V28, P2854
   Wu Jian, 2017, ADV NEURAL INFORM PR, V30, P5273
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001041
DA 2019-06-15
ER

PT S
AU Evans, TW
   Nair, PB
AF Evans, Trefor W.
   Nair, Prasanth B.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Discretely Relaxing Continuous Variables for tractable Variational
   Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed "DIRECT" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 10(2352) log-likelihood evaluations, we train on datasets with over two-million points in just seconds.
C1 [Evans, Trefor W.; Nair, Prasanth B.] Univ Toronto, Toronto, ON, Canada.
RP Evans, TW (reprint author), Univ Toronto, Toronto, ON, Canada.
EM trefor.evans@mail.utoronto.ca; pbn@utias.utoronto.ca
FU NSERC Discovery Grant; Canada Research Chairs program
FX Research funded by an NSERC Discovery Grant and the Canada Research
   Chairs program.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Bishop C. M., 2006, PATTERN RECOGNITION
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Bradshaw J., 2017, TECH REP
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Chen  W., 2015, INT C MACH LEARN, P2285
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Gong Y., 2014, ARXIV14126115
   GOODMAN LA, 1974, BIOMETRIKA, V61, P215, DOI 10.1093/biomet/61.2.215
   Grathwohl W., 2017, INT C LEARN REPR
   Han S., 2015, ARXIV151000149
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Horn R. A., 1994, TOPICS MATRIX ANAL, P208
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Jacob B., 2017, ARXIV171205877
   Jang Eric, 2016, ARXIV161101144
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2013, ARXIV13126114
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   LAZARSFELD P. F., 1968, LATENT STRUCTURE ANA
   Li  F., 2016, ARXIV160504711
   Louizos C., 2017, ADV NEURAL INFORM PR, P3288
   Maddison Chris J, 2016, ARXIV161100712
   Nielsen F., 2016, ARXIV160605850
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Ranganath  R., 2014, AISTATS, P814
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Tran D., 2016, ARXIV161009787
   Tucker G, 2017, ADV NEURAL INFORM PR, P2627
   Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
   Wilson A. G, 2016, P 19 INT C ART INT S, P370
   Zhou A., 2017, ARXIV170203044
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005008
DA 2019-06-15
ER

PT S
AU Evron, I
   Moroshko, E
   Crammer, K
AF Evron, Itay
   Moroshko, Edward
   Crammer, Koby
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient Loss-Based Decoding on Graphs for Extreme Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In extreme classification problems, learning algorithms are required to map instances to labels from an extremely large label set. We build on a recent extreme classification framework with logarithmic time and space [19], and on a general approach for error correcting output coding (ECOC) with loss-based decoding [1], and introduce a flexible and efficient approach accompanied by theoretical bounds. Our framework employs output codes induced by graphs, for which we show how to perform efficient loss-based decoding to potentially improve accuracy. In addition, our framework offers a tradeoff between accuracy, model size and prediction time. We show how to find the sweet spot of this tradeoff using only the training data. Our experimental study demonstrates the validity of our assumptions and claims, and shows that our method is competitive with state-of-the-art algorithms.
C1 [Evron, Itay] Technion, Comp Sci Dept, Haifa, Israel.
   [Moroshko, Edward; Crammer, Koby] Technion, Elect Engn Dept, Haifa, Israel.
RP Evron, I (reprint author), Technion, Comp Sci Dept, Haifa, Israel.
EM evron.itay@gmail.com; edward.moroshko@gmail.com; koby@ee.technion.ac.il
FU Israel Science Foundation [2030/16]
FX We would like to thank Eyal Bairey for the fruitful discussions. This
   research was supported in part by The Israel Science Foundation, grant
   No. 2030/16.
CR Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133
   Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741
   Bengio S., 2010, ADV NEURAL INFORM PR, P163
   Beygelzimer A., 2009, P 25 C UNC ART INT, P51
   Bhatia K., 2015, ADV NEURAL INFORM PR, P730
   Bredensteiner EJ, 1999, COMPUT OPTIM APPL, V12, P53, DOI 10.1023/A:1008663629662
   Choromanska AE, 2015, ADV NEURAL INFORM PR, V28, P55
   Cisse M., 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P506, DOI 10.1007/978-3-642-33460-3_38
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crammer K., 2009, ADV NEURAL INFORM PR, P414
   Crammer K., 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628
   Daume Hal, 2017, P 34 INT C MACH LEAR, V70, p[923, 06]
   Dembczynski Krzysztof, 2016, P 2016 EUR C MACH LE, P511
   Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263
   Escalera S, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P117
   Furnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605
   Jain H., 2016, P 22 ACM SIGKDD INT, P935
   Jasinska Kalina, 2016, ARXIV161101964
   Jernite Yacine, 2017, P MACHINE LEARNING R, P1665
   Kibriya AM, 2007, LECT NOTES ARTIF INT, V4702, P140
   Li SR, 2015, LECT NOTES COMPUT SC, V9371, P259, DOI 10.1007/978-3-319-25087-8_25
   Lin Shu, 2004, ERROR CONTROL CODING
   Liu MX, 2016, IEEE T PATTERN ANAL, V38, P2335, DOI 10.1109/TPAMI.2015.2430325
   Mesterharm Chris, 1999, ADV NEURAL INFORM PR
   Morin F., 2005, P AISTATS, P246
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Prabhu Y., 2014, P 20 ACM SIGKDD INT, P263
   Prabhu Yashoteja, 2018, P 2018 WORLD WID WEB
   Schapire R. E., 2013, EMPIRICAL INFERENCE, P37, DOI DOI 10.1007/978-3-642-41136-6_5
   Tagami Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P455, DOI 10.1145/3097983.3097987
   VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010
   Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219
   Weston Jason, 2011, IJCAI, P2764, DOI [10.5591/978-1-57735-516-8/IJCAI11-460, DOI 10.5591/978-1-57735-516-8/IJCAI11-460]
   Yen IEH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P545, DOI 10.1145/3097983.3098083
   Yen Ian En-Hsu, 2016, P 33 INT C MACH LEAR, V48, P3069
   Yu H., 2014, P 31 INT C MACH LEAR, P593
   Zhao Bin, 2013, INT J COMPUT VISION, V119, P60
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001075
DA 2019-06-15
ER

PT S
AU Fan, XH
   Li, B
   Sisson, SA
AF Fan, Xuhui
   Li, Bin
   Sisson, Scott A.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Rectangular Bounding Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PREDICTION
AB Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model - the Rectangular Bounding Process (RBP) - to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful non-parametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply the RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of the RBP in rich yet parsimonious expressiveness compared to the state-of-the-art methods.
C1 [Fan, Xuhui; Sisson, Scott A.] Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia.
   [Li, Bin] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
RP Fan, XH (reprint author), Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia.
EM xuhui.fan@unsw.edu.au; libin@fudan.edu.cn; scott.sisson@unsw.edu.au
FU Australian Research Council through the Australian Centre of Excellence
   in Mathematical and Statistical Frontiers (ACEMS) [CE140100049]; Fudan
   University Startup Research Grant; Shanghai Municipal Science &
   Technology Commission [16JC1420401];  [DP160102544]
FX Xuhui Fan and Scott A. Sisson are supported by the Australian Research
   Council through the Australian Centre of Excellence in Mathematical and
   Statistical Frontiers (ACEMS, CE140100049), and Scott A. Sisson through
   the Discovery Project Scheme (DP160102544). Bin Li is supported by Fudan
   University Startup Research Grant and Shanghai Municipal Science &
   Technology Commission (16JC1420401).
CR Adams R, 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376
   Airoldi E. M., 2009, ADV NEURAL INFORM PR, P33
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Caldas J, 2008, MACHINE LEARN SIGN P, P291, DOI 10.1109/MLSP.2008.4685495
   Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285
   Chung KL, 2001, COURSE PROBABILITY T
   Coraddu A, 2016, P I MECH ENG M-J ENG, V230, P136, DOI 10.1177/1475090214540874
   Dheeru D., 2017, UCI MACHINE LEARNING
   Fan Xuhui, 2016, AAAI, P1547
   Fan Xuhui, 2018, AISTATS, V84, P1859
   Freund Y., 1999, J JAPANESE SOC ARTIF, V14, P1612
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Givoni Inmar, 2006, UAI, P200
   Ishiguro K., 2010, P ANN C NEUR INF PRO, V23, P919
   Ishiguro Katsuhiko, 2016, AAAI, P1701
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   KEMP C., 2006, P NAT C ART INT, V21, P381, DOI DOI 10.1145/1837026.1837061
   Lakshminarayanan B., 2016, P 19 INT C ART INT S, P1478
   Lakshminarayanan B., 2014, ADV NEURAL INFORM PR, P3140
   Leskovec J., 2010, P 19 INT C WORLD WID, P641, DOI DOI 10.1145/1772690.1772756
   Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454
   Linero Antonio R, 2018, J AM STAT ASSOC, P1
   McAuley J. J., 2012, NIPS, P548
   Miller K.T., 2009, ADV NEURAL INFORM PR, V22, P1276
   Nakano Masahiro, 2014, P ICML, P361
   Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735
   Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pratola Matthew, 2017, ARXIV170907542
   Pratola MT, 2014, J COMPUT GRAPH STAT, V23, P830, DOI 10.1080/10618600.2013.841584
   Roy Daniel M., 2011, THESIS
   Roy Daniel M., 2009, ADV NEURAL INF PROCE, P1377
   Schmidt MN, 2013, IEEE SIGNAL PROC MAG, V30, P110, DOI 10.1109/MSP.2012.2235191
   Tufekci P, 2014, INT J ELEC POWER, V60, P126, DOI 10.1016/j.ijepes.2014.02.027
   Wang P., 2011, P 2011 SIAM INT C DA, P331
   Wang Yi, 2015, ICML, P1339
   Yeh IC, 1998, CEMENT CONCRETE RES, V28, P1797, DOI 10.1016/S0008-8846(98)00165-3
   Zafarani R., 2009, SOCIAL COMPUTING DAT
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002019
DA 2019-06-15
ER

PT S
AU Fang, C
   Li, CJ
   Lin, ZC
   Zhang, T
AF Fang, Cong
   Li, Chris Junchi
   Lin, Zhouchen
   Zhang, Tong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path
   Integrated Differential Estimator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we propose a new technique named Stochastic Path-Integrated Differential EstimatoR (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. We provide a few error-bound results on its convergence rates. Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of O (min(n(1/2)epsilon(-2), epsilon(-3))) to find an epsilon-approximate first-order stationary point. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting. Our SPIDER technique can be further applied to find an (epsilon, O(epsilon(0.5)))-approximate second-order stationary point at a gradient computation cost of (O) over tilde (min(n(1/2)epsilon(-2) + epsilon(-2.5), epsilon(-3))).
C1 [Fang, Cong; Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Intelligence MoE, Beijing, Peoples R China.
   [Li, Chris Junchi; Zhang, Tong] Tencent AI Lab, Bellevue, WA USA.
RP Lin, ZC (reprint author), Peking Univ, Sch EECS, Key Lab Machine Intelligence MoE, Beijing, Peoples R China.
EM fangcong@pku.edu.cn; junchi.li.duke@gmail.com; zlin@pku.edu.cn;
   tongzhang@tongzhang-ml.org
FU National Basic Research Program of China (973 Program) [2015CB352502];
   National Natural Science Foundation (NSF) of China [61625301, 61731018];
   Microsoft Research Asia
FX The authors would like to thank Jeffrey Z. HaoChen for his help on the
   numerical experiments, thank an anonymous reviewer to point out a
   mistake in the original proof of Theorem 1 and thank Zeyuan Allen-Zhu
   and Quanquan Gu for relevant discussions and pointing out references
   Zhou et al. [39, 40], also Jianqiao Wangni for pointing out references
   Nguyen et al. [28, 29], and Zebang Shen, Ruoyu Sun, Haishan Ye, Pan Zhou
   for very helpful discussions and comments. Zhouchen Lin is supported by
   National Basic Research Program of China (973 Program, grant no.
   2015CB352502), National Natural Science Foundation (NSF) of China (grant
   nos. 61625301 and 61731018), and Microsoft Research Asia.
CR Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464
   Allen-Zhu Z., 2018, ADV NEURAL INFORM PR
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Carmon Y., 2016, SIAM J OPTIMIZATION
   Carmon  Y., 2017, INT C MACH LEARN, P654
   Carmon Y., 2017, ARXIV171011606
   Cauchy A, 1847, CR HEBD ACAD SCI, V25, P536
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Durrett R., 2010, PROBABILITY THEORY E
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hazan Elad, 2015, ADV NEURAL INFORM PR, P1594
   Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058
   Jin C., 2017, INT C MACH LEARN, V70, P1724
   Jin  Chi, 2017, ARXIV171110456
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Levy Kfir Y, 2016, ARXIV161104831
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nguyen L. M., 2017, P 34 INT C MACH LEAR, P2613
   Nguyen L. M., 2017, ARXIV170507261
   Paquette C., 2018, P 21 INT C ART INT S, P613
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Reddi Sashank J., 2018, P MACHINE LEARNING R, P1233
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6
   Tripuraneni N., 2018, ADV NEURAL INFORM PR
   Woodworth B., 2016, ADV NEURAL INFORM PR, P3639
   Woodworth  Blake, 2017, ARXIV170903594
   Xu Yi, 2017, ARXIV171101944
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P699
   Zhou D., 2018, ARXIV180608782
   Zhou  Dongruo, 2018, ARXIV180607811
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300064
DA 2019-06-15
ER

PT S
AU Farahmand, AM
AF Farahmand, Amir-massoud
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Iterative Value-Aware Model Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BOUNDS
AB This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment. This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by finding the maximum likelihood estimate, without taking into account the decision problem. Value-Aware Model Learning (VAML) framework argues that this might not be a good idea, especially if the true model of the environment does not belong to the model class from which we are estimating the model.
   The original VAML framework, however, may result in an optimization problem that is difficult to solve. This paper introduces a new MBRL class of algorithms, called Iterative VAML, that benefits from the structure of how the planning is performed (i.e., through approximate value iteration) to devise a simpler optimization problem. The paper theoretically analyzes Iterative VAML and provides finite sample error upper bound guarantee for it.
C1 [Farahmand, Amir-massoud] Vector Inst, Toronto, ON, Canada.
   [Farahmand, Amir-massoud] MERL, Cambridge, MA 02139 USA.
RP Farahmand, AM (reprint author), Vector Inst, Toronto, ON, Canada.; Farahmand, AM (reprint author), MERL, Cambridge, MA 02139 USA.
EM farahmand@vectorinstitute.ai
CR Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2
   Asadi Kavosh, 2018, FAIM WORKSH PRED GEN
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Bertsekas Dimitri P., 2011, Journal of Control Theory and Applications, V9, P310, DOI 10.1007/s11768-011-1005-3
   Doukhan P., 1994, LECT NOTES STAT, V85
   Ernst D, 2005, J MACH LEARN RES, V6, P503
   Farahmand A. M., 2010, ADV NEURAL INFORM PR, V23, P568
   Farahmand AM, 2009, P AMER CONTR CONF, P725, DOI 10.1109/ACC.2009.5160611
   Farahmand AM, 2016, J MACH LEARN RES, V17
   Farahmand AM, 2012, J STAT PLAN INFER, V142, P493, DOI 10.1016/j.jspi.2011.08.007
   Farahmand Amir-massoud, 2012, ADV NEURAL INFORM PR, P1349
   Farahmand Amir-massoud, 2011, THESIS
   Farahmand Amir-massoud, 2016, AAAI C ART INT
   Farahmand Amir-massoud, 2017, AM CONTR C ACC
   Farahmand Amir-massoud, 2017, P INT C ART INT STAT, P1486
   Farahmand Amir-massoud, 2016, 13 EUR WORKSH REINF
   Farquhar Gregory, 2018, INT C LEARN REPR ICL
   Gordon Geoffrey, 1995, INT C MACH LEARN ICM
   Gyorfi L, 2002, DISTRIBUTION FREE TH
   Huang De-An, 2015, AAAI C ART INT
   Joseph J, 2013, IEEE INT CONF ROBOT, P939, DOI 10.1109/ICRA.2013.6630686
   Lagoudakis M., 2003, J MACHINE LEARNING R, V4, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107
   Lazaric A, 2012, J MACH LEARN RES, V13, P3041
   Lee WS, 2008, IEEE T INFORM THEORY, V54, P4395, DOI 10.1109/TIT.2008.928242
   Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577
   Mann TA, 2015, J ARTIF INTELL RES, V53, P375, DOI 10.1613/jair.4676
   Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810
   Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohri M., 2009, ADV NEURAL INFORM PR, P1097
   Mohri M, 2010, J MACH LEARN RES, V11, P789
   Munos R, 2008, J MACH LEARN RES, V9, P815
   Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384
   Nickl R., 2015, MATH FDN INFINITE DI
   Oh Junhyuk, 2017, ADV NEURAL INFORM PR, P6118
   Scherrer Bruno, 2012, P 29 INT C MACH LEAR
   Silver D., 2017, P 34 INT C MACH LEAR, P3191
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart Ingo, 2009, ADV NEURAL INF PROCE, p[1768, 6]
   Sutton Richard S., 1990, P 7 INT C MACH LEARN
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Szepesvari Csaba, 2004, P 21 INT C MACH LEAR
   Tosatto Samuele, 2017, P 34 INT C MACH LEAR, p[3434, 2, 5]
   VAN DE GEER S. A., 2000, EMPIRICAL PROCESSES
   Weber T., 2017, ADV NEURAL INFORM PR, P5690
   Yang YH, 1999, ANN STAT, V27, P1564
   YU B, 1994, ANN PROBAB, V22, P94, DOI 10.1214/aop/1176988849
   Zhou DX, 2003, IEEE T INFORM THEORY, V49, P1743, DOI 10.1109/TIT.2003.813564
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003061
DA 2019-06-15
ER

PT S
AU Farina, G
   Celli, A
   Gatti, N
   Sandholm, T
AF Farina, Gabriele
   Celli, Andrea
   Gatti, Nicola
   Sandholm, Tuomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Ex ante coordination and collusion in zero-sum multi-player
   extensive-form games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EFFICIENT COMPUTATION; IMPERFECT RECALL; EQUILIBRIA
AB Recent milestones in equilibrium computation, such as the success of Libratus, show that it is possible to compute strong solutions to two-player zero-sum games in theory and practice. This is not the case for games with more than two players, which remain one of the main open challenges in computational game theory. This paper focuses on zero-sum games where a team of players faces an opponent, as is the case, for example, in Bridge, collusion in poker, and many non-recreational applications such as war, where the colluders do not have time or means of communicating during battle, collusion in bidding, where communication during the auction is illegal, and coordinated swindling in public. The possibility for the team members to communicate before game play-that is, coordinate their strategies ex ante-makes the use of behavioral strategies unsatisfactory. The reasons for this are closely related to the fact that the team can be represented as a single player with imperfect recall. We propose a new game representation, the realization form, that generalizes the sequence form but can also be applied to imperfect-recall games. Then, we use it to derive an auxiliary game that is equivalent to the original one. It provides a sound way to map the problem of finding an optimal ex-antecoordinated strategy for the team to the well-understood Nash equilibrium-finding problem in a (larger) two-player zero-sum perfect-recall game. By reasoning over the auxiliary game, we devise an anytime algorithm, fictitious team-play, that is guaranteed to converge to an optimal coordinated strategy for the team against an optimal opponent, and that is dramatically faster than the prior state-of-the-art algorithm for this problem.
C1 [Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
   [Celli, Andrea; Gatti, Nicola] Politecn Milan, DEIB, Milan, Italy.
RP Farina, G (reprint author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
EM gfarina@cs.cmu.edu; andrea.celli@polimi.it; nicola.gatti@polimi.it;
   sandholm@cs.cmu.edu
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082.
CR Basilico N., 2017, AAAI C ART INT AAAI, P356
   Brown G. W., 1951, ACTIVITY ANAL PRODUC, P374
   Brown N., 2017, P ANN C NEUR INF PRO, P689
   Brown N., 2017, SCIENCE
   Brown N., 2015, INT C AUT AG MULT SY
   Celli A., 2018, AAAI C ART INT AAAI
   Cermak J., 2017, P INT JOINT C ART IN, P936
   Cermak J, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P902
   Cermak J, 2018, INT J APPROX REASON, V93, P290, DOI 10.1016/j.ijar.2017.11.010
   Ganzfried S., 2014, AAAI C ART INT AAAI
   Heinrich J., 2015, JMLR WORKSH C P, P805
   Koller D, 1996, GAME ECON BEHAV, V14, P247, DOI 10.1006/game.1996.0051
   Kroer C., 2016, P ACM C EC COMP EC
   Lanctot M., 2012, INT C MACH LEARN ICM
   Maschler M., 2013, GAME THEORY
   McMahan H. B., 2003, P 20 INT C MACH LEAR, P536
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Piccione M, 1997, GAME ECON BEHAV, V20, P3, DOI 10.1006/game.1997.0536
   ROBINSON J, 1951, ANN MATH, V54, P296, DOI 10.2307/1969530
   Shoham  Y., 2008, MULTIAGENT SYSTEMS A
   Southey F., 2005, P 21 ANN C UNC ART I
   Tawarmalani M, 2005, MATH PROGRAM, V103, P225, DOI 10.1007/s10107-005-0581-8
   von Stengel B, 2008, MATH OPER RES, V33, P1002, DOI 10.1287/moor.1080.0340
   vonStengel B, 1997, GAME ECON BEHAV, V21, P309, DOI 10.1006/game.1997.0527
   vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050
   Waugh K., 2009, S ABSTR REF APPR SAR
   Wichardt PC, 2008, GAME ECON BEHAV, V63, P366, DOI 10.1016/j.geb.2007.08.007
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004022
DA 2019-06-15
ER

PT S
AU Farina, G
   Gatti, N
   Sandholm, T
AF Farina, Gabriele
   Gatti, Nicola
   Sandholm, Tuomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Practical exact algorithm for trembling-hand equilibrium refinements in
   games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Nash equilibrium strategies have the known weakness that they do not prescribe rational play in situations that are reached with zero probability according to the strategies themselves, for example, if players have made mistakes. Trembling-hand refinements-such as extensive-form perfect equilibria and quasi-perfect equilibria-remedy this problem in sound ways. Despite their appeal, they have not received attention in practice since no known algorithm for computing them scales beyond toy instances. In this paper, we design an exact polynomial-time algorithm for finding trembling-hand equilibria in zero-sum extensive-form games. It is several orders of magnitude faster than the best prior ones, numerically stable, and quickly solves game instances with tens of thousands of nodes in the game tree. It enables, for the first time, the use of trembling-hand refinements in practice.
C1 [Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
   [Gatti, Nicola] Politecn Milan, DEIB, Milan, Italy.
RP Farina, G (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM gfarina@cs.cmu.edu; nicola.gatti@polimi.it; sandholm@cs.cmu.edu
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082.
CR Bertsimas D, 1997, INTRO LINEAR OPTIMIZ
   Brown N., 2017, P ANN C NEUR INF PRO, P689
   Brown Noam, 2017, SCIENCE
   Brown Noam, 2017, INT C MACH LEARN ICM
   Cermak J, 2014, FRONT ARTIF INTEL AP, V263, P201, DOI 10.3233/978-1-61499-419-0-201
   Dantzig GB, 2006, LINEAR PROGRAMMING, V1
   Farina Gabriele, 2017, AAAI C ART INT AAAI
   Farina Gabriele, 2017, INT C MACH LEARN ICM
   Ganzfried Sam, INT C AUT AG MULT SY
   GLPK, 2017, GNU LIN PROGR KIT VE
   Hillas John, 2002, HDB GAME THEORY EC A
   Koller Daphne, 1996, GAMES EC BEHAV, V14
   KREPS DM, 1982, ECONOMETRICA, V50, P863, DOI 10.2307/1912767
   Kroer Christian, 2017, P INT JOINT C ART IN
   Kuhn H. W., 1950, CONTRIBUTIONS THEORY, V1, P97
   Miltersen PB, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P107, DOI 10.1145/1109557.1109570
   Miltersen Peter Bro, 2010, EC THEORY, V42
   Myerson Roger B., 1978, INT J GAME THEORY, V15, P133
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Romanovskii I., 1962, SOVIET MATH, V3
   ROSS SM, 1971, J APPL PROBAB, V8, P621, DOI 10.2307/3212187
   Selten Reinhard, 1975, INT J GAME THEORY
   Shoham  Y., 2008, MULTIAGENT SYSTEMS A
   Southey F., 2005, P 21 ANN C UNC ART I
   van Damme E., 1987, STABILITY PERFECTION
   van Damme Eric, 1984, INT J GAME THEORY
   von Stengel Bernhard, 1996, GAMES EC BEHAV, V14
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305008
DA 2019-06-15
ER

PT S
AU Farnia, F
   Tse, D
AF Farnia, Farzan
   Tse, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Convex Duality Framework for GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence score, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class such as convolutional neural nets. Then, a natural question is how the divergence minimization interpretation will change as we constrain F. In this work, we address this question by developing a convex duality framework for analyzing GAN minimax problems. For a convex set F, this duality framework interprets the original vanilla GAN problem as finding the generative model with the minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in F. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. We further apply the convex duality framework to explain why regularizing the discriminator's Lipschitz constant, e.g. via spectral normalization or gradient penalty, can greatly improve the training performance in a general f-GAN problem including the vanilla GAN formulation. We prove that Lipschitz regularization can be interpreted as convolving the original divergence score with the first-order Wasserstein distance, which results in a continuously-behaving target divergence measure. We numerically explore the power of Lipschitz regularization for improving the continuity behavior and training performance in GAN problems.
C1 [Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Farnia, F (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM farnia@stanford.edu; dntse@stanford.edu
FU Stanford Graduate Fellowship; National Science Foundation [CCF-1563098];
   Center for Science of Information (CSoI), an NSF Science and Technology
   Center [CCF-0939370]
FX We are grateful for support under a Stanford Graduate Fellowship, the
   National Science Foundation grant under CCF-1563098, and the Center for
   Science of Information (CSoI), an NSF Science and Technology Center
   under grant agreement CCF-0939370.
CR Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13
   Arjovsky M., 2017, ARXIV170104862
   Arjovsky Martin, 2017, INT C MACH LEARN
   Arora S., 2017, ARXIV170608224
   Arora S., 2017, ARXIV170300573
   Boyd S., 2004, CONVEX OPTIMIZATION
   Csiszar I., 2004, Foundations and Trends in Communications and Information Theory, V1, P1
   Daskalakis C., 2017, ARXIV171100141
   Dudik M, 2007, J MACH LEARN RES, V8, P1217
   Dziugaite G. K., 2015, ARXIV150503906
   Farnia  F., 2016, P ADV NEUR INF PROC, P4240
   Fathony Rizal, 2017, ADV NEURAL INFORM PR, V30, P563
   Fathony Rizal, 2016, ADV NEURAL INFORM PR, V29, P559
   Feizi S, 2017, ARXIV171010793
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian, 2016, ARXIV170100160
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2014, ARXIV14126980
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li Chun-Liang, 2017, ADV NEURAL INFORM PR, P2200
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Liu Shuang, 2017, ADV NEURAL INFORM PR, P5551
   Liu  Z., 2015, P INT C COMP VIS ICC
   Mescheder L., 2017, ADV NEURAL INFORM PR, P1823
   Miyato T., 2018, INT C LEARN REPR
   Nagarajan Vaishnavh, 2017, ADV NEURAL INFORM PR, P5591
   Nock Richard, 2017, ADV NEURAL INFORM PR, P456
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Radford  A., 2015, ARXIV151106434
   Razaviyayn Meisam, 2015, ADV NEURAL INFORM PR, P3276
   Roth Kevin, 2017, ADV NEURAL INFORM PR, P2015
   Sanjabi Maziar, 2018, ARXIV180208249
   Santurkar Shibani, 2017, ARXIV171100970
   Sinha  A., 2018, INT C LEARN REPR
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Xu Chen, 2018, INT C LEARN REPR
   Yu F., 2015, ARXIV150603365
   Zhang Pengchuan, 2018, INT C LEARN REPR
   Zhao J., 2016, ARXIV160903126
   Zhao Shengjia, 2018, ARXIV180606514
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305028
DA 2019-06-15
ER

PT S
AU Fathony, R
   Rezaei, A
   Bashiri, MA
   Zhang, XH
   Ziebart, BD
AF Fathony, Rizal
   Rezaei, Ashkan
   Bashiri, Mohammad Ali
   Zhang, Xinhua
   Ziebart, Brian D.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Distributionally Robust Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OPTIMIZATION
AB In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods-probabilistic graphical models and large margin methods-have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments.
C1 [Fathony, Rizal; Rezaei, Ashkan; Bashiri, Mohammad Ali; Zhang, Xinhua; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
RP Fathony, R (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM rfatho2@uic.edu; arezae4@uic.edu; mbashi4@uic.edu; zhangx@uic.edu;
   bziebart@uic.edu
FU National Science Foundation [1652530]; Future of Life Institute
   FLI-RFP-AI1 program
FX This work was supported, in part, by the National Science Foundation
   under Grant No. 1652530, and by the Future of Life Institute
   (futureoflife.org) FLI-RFP-AI1 program.
CR Asif Kaiser, 2015, P C UNC ART INT
   Behpour Sima, 2018, AAAI C ART INT
   Boyd S., 2008, NOTES DECOMPOSITION
   Carreras X., 2005, P 9 C COMP NAT LANG, P152
   Chen RD, 2018, J MACH LEARN RES, V19
   Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412
   Cohn Trevor, 2005, P 9 C COMP NAT LANG, P169
   COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D
   Cowell Robert G., 2006, PROBABILISTIC NETWOR
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1
   Fathony Rizal, 2017, ADV NEURAL INFORM PR, V30, P563
   Fathony Rizal, 2016, ADV NEURAL INFORM PR, V29, P559
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   Hashimoto Tatsunori, 2018, P 35 INT C MACH LEAR, V80, P1929
   Hatori Jun, 2008, COLING 2008 COMPANIO, P43
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Kim M, 2010, LECT NOTES COMPUT SC, V6313, P649
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Li Jia, 2016, P 25 INT JOINT C ART, P1690
   Li S. Z., 2009, MARKOV RANDOM FIELD
   Liu Yufeng, 2007, AISTATS, P291
   Livni Roi, 2012, P 15 INT C ART INT S, P722
   Manning C.D., 1999, FDN STAT NATURAL LAN
   McMahan H. B., 2003, P 20 INT C MACH LEAR, P536
   Namkoong Hongseok, 2017, ADV NEURAL INFORM PR, P2971
   Namkoong Hongseok, 2016, ADV NEURAL INFORM PR, P2208
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Osokin A, 2017, ADV NEURAL INFORM PR, P301
   Pearl Judea, 1985, P 7 C COGN SCI SOC 1
   Sadeghian Ali, 2016, P WORKSH LEG TEXT DO, P70
   Shafieezadeh-Abadeh S, 2015, ADV NEURAL INFORM PR, P1576
   Sion M., 1958, PAC J MATH, V8, P171, DOI DOI 10.2140/PJM.1958.8.171
   Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219
   Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013
   Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   TOPSOE F, 1979, KYBERNETIKA, V15, P8
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Vapnik V. N., 1998, STAT LEARNING THEORY, V1
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Von- Neumann J., 1945, B AM MATH SOC, V51, P498
   Wang Hong, 2015, ADV NEURAL INFORM PR
   Xue Nianwen, 2004, P 2004 C EMP METH NA
   Zhang T, 2004, J MACH LEARN RES, V5, P1225
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002085
DA 2019-06-15
ER

PT S
AU Fawzi, A
   Fawzi, H
   Fawzi, O
AF Fawzi, Alhussein
   Fawzi, Hamza
   Fawzi, Omar
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial vulnerability for any classifier
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.
C1 [Fawzi, Alhussein] DeepMind, London, England.
   [Fawzi, Hamza] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge, England.
   [Fawzi, Omar] Univ Lyon, ENS Lyon, CNRS, UCBL,LIP, F-69342 Lyon 07, France.
RP Fawzi, A (reprint author), DeepMind, London, England.
EM afawzi@google.com; h.fawzi@damtp.cam.ac.uk; omar.fawzi@ens-lyon.fr
CR Alemi A. A., 2016, ARXIV161200410
   Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   BORELL C, 1975, INVENT MATH, V30, P207, DOI 10.1007/BF01425510
   Carlini N., 2017, P 10 ACM WORKSH ART, P3
   Chicco D., 2014, P 5 ACM C BIOINF COM, P533, DOI DOI 10.1145/2649387.2649442
   Cisse  M., 2017, INT C MACH LEARN, P854
   Dvijotham K., 2018, ARXIV180306567
   Elsayed G. F., 2018, ARXIV180208195
   Fawzi A., 2015, CORR
   Fawzi Alhussein, 2016, NEURAL INFORM PROCES
   Gilmer Justin, 2018, ARXIV180102774
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   He  K., 2015, ARXIV151203385
   Hein Matthias, 2017, ADV NEURAL INFORM PR, P2263
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Ilyas A., 2017, ARXIV171209196
   Kingma D.P., 2013, ARXIV13126114
   Kos J., 2017, ARXIV170206832
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Liu Y., 2016, ARXIV161102770
   Madry Aleksander, 2017, ARXIV170606083
   Moosavi-Dezfooli S.-M., 2016, IEEE C COMP VIS PATT
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Papernot N., 2015, ARXIV151104508
   Peck Jonathan, 2017, ADV NEURAL INFORM PR, P804
   Radford A., 2015, ARXIV151106434
   Raghunathan Aditi, 2018, ARXIV180109344
   Rauber J., 2017, ROBUST VISION BENCHM
   Samangouei P., 2018, INT C LEARN REPR
   Shaham  U., 2015, ARXIV151105432
   Simonyan K., 2014, INT C LEARN REPR ICL
   Sinha Aman, 2017, ARXIV171010571
   Spencer M, 2015, IEEE ACM T COMPUT BI, V12, P103, DOI 10.1109/TCBB.2014.2343960
   Sudakov V. N., 1978, J SOVIET MATH, V9, P9
   Szegedy  C., 2014, INT C LEARN REPR ICL
   Tanay  T., 2016, ARXIV160807690
   Uesato J., 2018, ARXIV180205666
   Zagoruyko S, 2016, ARXIV160507146
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301019
DA 2019-06-15
ER

PT S
AU Fehr, M
   Buffett, O
   Thomas, V
   Dibangoye, J
AF Fehr, Mathieu
   Buffett, Olivier
   Thomas, Vincent
   Dibangoye, Junes
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a "fully observable" problem-a belief MDP-and exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex Delta). This approach has been extended to solving rho-POMDPs-i.e., for information-oriented criteria-when the reward rho is convex in Delta. General rho-POMDPs can also be turned into "fully observable" problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and rho-POMDPs with lambda rho-Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upper- and lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems.
C1 [Fehr, Mathieu] Ecole Normale Super, Rue Ulm, Paris, France.
   [Buffett, Olivier; Thomas, Vincent] Univ Lorraine, CNRS, INRIA, LORIA, Nancy, France.
   [Dibangoye, Junes] Univ Lyon, INSA Lyon, INRIA, CITI, Lyon, France.
RP Fehr, M (reprint author), Ecole Normale Super, Rue Ulm, Paris, France.
EM mathieu.fehr@ens.fr; olivier.buffet@loria.fr; vincent.thomas@loria.fr;
   jilles.dibangoye@inria.fr
CR Araya- Lopez M., 2010, ADV NEURAL INFORM PR
   ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X
   BELLMAN R, 1957, J MATH MECH, V6, P679
   Dibangoye J., 2013, P 23 INT JOINT C ART
   Dibangoye J., 2016, J ARTIFICIAL INTELLI, V55
   Dufour F, 2012, J MATH ANAL APPL, V388, P1254, DOI 10.1016/j.jmaa.2011.11.015
   Egorov M., 2016, P 30 AAAI C ART INT
   Fonteneau R., 2009, P IEEE S APPR DYN PR
   Fox D, 1998, ROBOT AUTON SYST, V25, P195, DOI 10.1016/S0921-8890(98)00049-9
   Hansen E., 2004, P 19 NAT C ART INT A
   Hinderer K, 2005, MATH METHOD OPER RES, V62, P3, DOI 10.1007/s00186-005-0438-1
   Ieong S., 2007, P NAT C ART INT AAAI
   Kurniawati H., 2008, ROBOTICS SCI SYSTEMS, VIV
   Laraki R., 2004, MATH OPERATIONS RES, V29
   Mihaylova L., 2006, NATO SCI SERIES DATA, V198
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Pineau J., 2006, J ARTIFICIAL INTELLI, V27
   Pineau J., 2003, P 18 INT JOINT C ART
   Platzman L. K., 1977, THESIS
   Poupart P., 2011, P 21 INT C AUT PLANN
   Rachelson E., 2004, P INT S ART INT MATH
   Satsangi Y., 2015, IASUVA1501
   Smallwood R., 1973, OPERATION RES, V21
   Smith T., 2007, THESIS
   Smith T., 2005, P 21 C UNC ART INT U
   Smith T., 2004, P ANN C UNC ART INT
   Sondik E. J., 1971, THESIS
   Spaan M. T., 2015, AUTONOMOUS AGENTS MU, V29
   Zhang NL, 2001, J ARTIF INTELL RES, V14, P29, DOI 10.1613/jair.761
   Zhang Z., 2014, P 31 INT C MACH LEAR
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001047
DA 2019-06-15
ER

PT S
AU Feizi, S
   Javadi, H
   Zhang, J
   Tse, D
AF Feizi, Soheil
   Javadi, Hamid
   Zhang, Jesse
   Tse, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Porcupine Neural Networks: Approximating Neural Network Landscapes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.
C1 [Feizi, Soheil] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Javadi, Hamid] Rice Univ, Dept Elect & Comp Engn, Houston, TX 77251 USA.
   [Zhang, Jesse; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Feizi, S (reprint author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM sfeizi@cs.umd.edu; hrhakim@rice.edu; jessez@stanford.edu;
   dntse@stanford.edu
FU Center for Science of Information (CSoI), an NSF Science and Technology
   Center [CCF-0939370]
FX This work supported by the Center for Science of Information (CSoI), an
   NSF Science and Technology Center, under grant agreement CCF-0939370.
CR Bach F, 2017, J MACH LEARN RES, V18
   Blum  A., 1989, P ADV NEUR INF PROC, P494
   Brutzkus Alon, 2017, ARXIV170207966
   Cheng XY, 2013, RANDOM MATRICES-THEO, V2, DOI 10.1142/S201032631350010X
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Choromanska A., 2015, ARTIF INTELL, P192
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Daniely Amit, 2017, ARXIV170307872
   Daniely Amit, 2016, NIPS, P2253
   Do Y, 2012, ARXIV12063763
   El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648
   Fan Zhou, 2015, ARXIV150705343
   Ge Rong, 2017, ARXIV171100501
   Hazan Elad, 2015, ADV NEURAL INFORM PR, P1594
   Heinemann Uri, 2016, ARTIF INTELL, P1159
   Janzamin Majid, 2015, ARXIV150608473
   Kakade S., 2011, ADV NEURAL INFORM PR, P927
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LI Han, 2017, ARXIV171209913
   Li Yuanzhi, 2017, ARXIV170509886
   Mei Song, 2016, ARXIV160706534
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nguyen Quynh, 2017, ARXIV170408045
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Soltanolkotabi Mahdi, 2017, ARXIV170504591
   Soltanolkotabi Mahdi, 2017, ARXIV170704926
   Soudry D, 2016, ARXIV160508361
   Tian Yuandong, 2017, ARXIV170300560
   Tian Yuandong, 2016, SYMMETRY BREAKING CO
   Vidal Rene, 2017, ARXIV171204741
   Yun Chulhee, 2017, ARXIV170702444
   Zhang Qiuyi, 2017, ARXIV170200458
   Zhong Kai, 2017, ARXIV170603175
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304081
DA 2019-06-15
ER

PT S
AU Feldman, M
   Karbasi, A
   Kazemi, E
AF Feldman, Moran
   Karbasi, Amin
   Kazemi, Ehsan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Do Less, Get More: Streaming Submodular Maximization with Subsampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to 4p + 2 - o(1). To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty-fold while maintaining practically the same utility. We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations.
C1 [Feldman, Moran] Open Univ Israel, Raanana, Israel.
   [Karbasi, Amin; Kazemi, Ehsan] Yale Univ, New Haven, CT 06520 USA.
RP Feldman, M (reprint author), Open Univ Israel, Raanana, Israel.
EM moranfe@openu.ac.il; amin.karbasi@yale.edu; ehsan.kazemi@yale.edu
FU AFOSR Young Investigator Award [FA9550-18-1-0160]
FX The work of Amin Karbasi was supported by AFOSR Young Investigator Award
   (FA9550-18-1-0160).
CR Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Bilmes Jeffrey A., 2017, ABS170108939 CORR
   Buchbinder N, 2014, P 25 ANN ACM SIAM S, V25, P1433
   Buchbinder N., 2015, SODA, P1202
   Buchbinder  Niv, 2016, ABS161103253 CORR
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Chakrabarti A, 2015, MATH PROGRAM, V154, P225, DOI 10.1007/s10107-015-0900-7
   CHEKURI C, 2015, ICALP, V9134, P318, DOI DOI 10.1007/978-3-662-47672-7_26
   Chen  Jiecao, 2016, ABS161100129 CORR
   Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34
   Epasto A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P421, DOI 10.1145/3038912.3052699
   Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46
   Feldman  Moran, 2017, COLT, P758
   Feldman  Moran, 2011, ESA, P784
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098
   Gomes Ryan, 2010, P 27 INT C MACH LEAR, P391
   Gong B., 2014, P ADV NEUR INF PROC, P2069
   GUPTA A, 2010, WINE, V6484, P246
   Kazemi E, 2018, ICML, P2549
   KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684
   Krause A, 2005, P 21 C UNC ART INT U, P324
   Kulesza A, 2012, FDN TRENDS MACHINE L, V5
   Lawrence N., 2003, P ADV NEUR INF PROC, P625
   Lee J, 2010, MATH OPER RES, V35, P795, DOI 10.1287/moor.1100.0463
   Lee J, 2010, SIAM J DISCRETE MATH, V23, P2053, DOI 10.1137/090750020
   Libbrecht Maxwell W., 2018, PROTEINS STRUCTURE F
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855
   Mirzasoleiman B, 2016, J MACH LEARN RES, V17
   Mirzasoleiman Baharan, 2016, ICML, P1358
   Mirzasoleiman  Baharan, 2017, INT C MACH LEARN, P2449
   Mirzasoleiman  Baharan, 2018, AAAI C ART INT
   Mitrovic  Marko, 2018, ICML, P3593
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Salehi Mehraveh, 2017, Medical Image Computing and Computer Assisted Intervention - MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P478, DOI 10.1007/978-3-319-66182-7_55
   Schrijver A., 2003, COMBINATORIAL OPTIMI
   Seeger  Matthias, 2004, TECHNICAL REPORT
   Tschiatschek S., 2014, ADV NEURAL INFORM PR, P1413
   UberDataset, 2014, UBERDATASET UBER PIC
   Varadaraja AB, 2011, LECT NOTES COMPUT SC, V6755, P379, DOI 10.1007/978-3-642-22006-7_32
   Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318
   Wang  Yanhao, 2017, ABS170604764 CORR
   Ward J, 2012, LEIBNIZ INT PR INFOR, V14, P42, DOI 10.4230/LIPIcs.STACS.2012.42
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300068
DA 2019-06-15
ER

PT S
AU Feldman, V
   Vondrak, J
AF Feldman, Vitaly
   Vondrak, Jan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Generalization Bounds for Uniformly Stable Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID STABILITY
AB Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002). Specifically, for a loss function with range bounded in [0, 1], the generalization error of a gamma-uniformly stable learning algorithm on n samples is known to be within O((gamma + 1/n), root n log(1/delta)) of the empirical error with probability at least 1 - delta. Unfortunately, this bound does not lead to meaningful generalization bounds in many common settings where gamma >= 1/root n. At the same time the bound is known to be tight only when gamma = O(1/n).
   We substantially improve generalization bounds for uniformly stable algorithms without making any additional assumptions. First, we show that the bound in this setting is O(root(gamma + 1/n) log(1/delta)) with probability at least 1 - delta. In addition, we prove a tight bound of O(gamma(2) + 1/n) on the second moment of the estimation error. The best previous bound on the second moment is O(gamma + 1/n). Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms.
C1 [Feldman, Vitaly] Google Brain, Mountain View, CA 94043 USA.
   [Vondrak, Jan] Stanford Univ, Stanford, CA 94305 USA.
RP Feldman, V (reprint author), Google Brain, Mountain View, CA 94043 USA.
CR Abou-Moustafa Karim T., 2018, ISAIM
   Bassily R, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1046, DOI 10.1145/2897518.2897566
   Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Devroye L., 1996, PROBABILISTIC THEORY
   DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P202, DOI 10.1109/TIT.1979.1056032
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork Cynthia, 2014, CORR
   Dwork Cynthia, 2018, CORR
   Elisseeff A, 2005, J MACH LEARN RES, V6, P55
   Feldman Vitaly, 2016, CORR
   Hardt M, 2016, P 33 INT C MACH LEAR, P1225
   Kakade S. M., 2008, NIPS, P793
   Kale S, 2011, P 25 INT C SUP 31 MA, P487
   Kumar Ravi, 2013, ICML, P27
   Liu T., 2017, P 34 INT C MACH LEAR, V70, P2159
   London Ben, 2017, NIPS, P2935
   Maurer Andreas, 2017, C LEARN THEOR, P1461
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Nissim Kobbi, 2015, CORR
   Nissim Kobbi, 2017, CORR
   Poggio T, 2004, NATURE, V428, P419, DOI 10.1038/nature02341
   ROGERS WH, 1978, ANN STAT, V6, P506, DOI 10.1214/aos/1176344196
   Rosasco Lorenzo Wibisono Andre, 2009, MITCSAILTR2009060
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Steinke Thomas, 2017, ARXIV170103493
   Wu X, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1307, DOI 10.1145/3035918.3064047
   Zhang T, 2003, NEURAL COMPUT, V15, P1397, DOI 10.1162/089976603321780326
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004032
DA 2019-06-15
ER

PT S
AU Feng, J
   Yu, Y
   Zhou, ZH
AF Feng, Ji
   Yu, Yang
   Zhou, Zhi-Hua
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multi-Layered Gradient Boosting Decision Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability.
C1 [Feng, Ji; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Feng, Ji] Sinovat Ventures AI Inst, Beijing, Peoples R China.
RP Feng, J (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.; Feng, J (reprint author), Sinovat Ventures AI Inst, Beijing, Peoples R China.
EM fengj@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn
FU NSFC [61751306]; National Key R&D Program of China [2018YFB1004300];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization
FX This research was supported by NSFC (61751306), National Key R&D Program
   of China (2018YFB1004300) and Collaborative Innovation Center of Novel
   Software Technology and Industrialization.
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Bengio Y., 2013, ICML, P552
   Bengio Y, 2014, ARXIV14077906
   Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   CLARKE FH, 1976, PAC J MATH, V64, P97, DOI 10.2140/pjm.1976.64.97
   Feng J., 2018, AAAI
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Frosst N., 2017, ARXIV171109784
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   HE X, 2014, ADKDD
   Hinton G. E., 1986, PARALLEL DISTRIBUTED, V1, P77
   Huang Sandy, 2017, ARXIV170202284
   Ke G., 2017, ADV NEURAL INFORM PR, P3149
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172
   Korlakai V. R., 2015, INT C ART INT STAT, P489
   Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31
   Lichman M., 2013, UCI MACHINE LEARNING
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Nokland Arild, 2016, ADV NEURAL INFORM PR, P1037
   Rory M., 2017, PEERJ COMPUTER SCI, V3, P127
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Si S., 2017, ICML, V70, P3182
   Tishby N., 2015, ARXIV150302406
   Tong H., 2015, JMLR W CP, V42, P69
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Werbos PJ, 1974, THESIS
   Zhou Z. - H., 2012, ENSEMBLE METHODS FDN
   Zhou Z.-H., 2018, NATL SCI REV
   Zhou Z. - H., 2017, P 26 INT JOINT C ART, P3553, DOI [10.24963/ijcai.2017/497, DOI 10.24963/IJCAI.2017/497]
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303054
DA 2019-06-15
ER

PT S
AU Feng, ZL
   Wang, XC
   Ke, CL
   Zeng, AX
   Tao, DC
   Song, ML
AF Feng, Zunlei
   Wang, Xinchao
   Ke, Chenglong
   Zeng, Anxiang
   Tao, Dacheng
   Song, Mingli
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dual Swap Disentangling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a "encoding-swap-decoding" process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the "encoding-swap-decoding" process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.
C1 [Feng, Zunlei; Ke, Chenglong; Song, Mingli] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Wang, Xinchao] Stevens Inst Technol, Hoboken, NJ 07030 USA.
   [Zeng, Anxiang] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
   [Tao, Dacheng] Univ Sydney, Sydney, NSW, Australia.
RP Song, ML (reprint author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
EM zunleifeng@zju.edu.cn; xinchao.wang@stevens.edu; chenglongke@zju.edu.cn;
   renzhong@taobao.com; dctao@sydney.edu.au; brooksong@zju.edu.cn
FU Natonal Basic Research Program of China [2015CB352400]; National Natural
   Science Foundation of China [61572428, U1509206]; Fundamental Research
   Funds for the Central Universities [2017FZA5014]; Key Research and
   Development Program of Zhejiang Province [2018C01004]; Australian
   Research Council [FL-170100117, DP-140102164]
FX This work is supported by Natonal Basic Research Program of China under
   Grant No. 2015CB352400, National Natural Science Foundation of China
   (61572428,U1509206), Fundamental Research Funds for the Central
   Universities (2017FZA5014), Key Research and Development Program of
   Zhejiang Province (2018C01004), and Australian Research Council Projects
   (FL-170100117, DP-140102164).
CR Banijamali Ershad, 2017, JADE JOINT AUTOENCOD
   Bouchacourt D., 2017, MULTILEVEL VARIATION
   Burgess Christopher, 2017, NIPS 2017 DIS WORKSH
   Chen T. Q., 2018, ARXIV180204942
   Chen X, 2016, INFOGAN INTERPRETABL
   Dupont Emilien, 2018, ARXIV180400104
   Eastwood Cian, 2018, INT C LEARN REPR
   Feng Zunlei, 2018, INTERPRETABLE PARTIT
   Gao Shuyang, 2018, ARXIV180205822
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gulrajani Ishaan, 2017, IMPROVED TRAINING WA
   Haykin S., 2009, GRADIENTBASED LEARNI, P306
   Higgins I., 2017, ARXIV170708475
   Higgins  I., 2016, BETA VAE LEARNING BA
   Higgins Irina, 2017, SCAN LEARNING ABSTRA
   Kim H., 2018, ARXIV180205983
   Kingma D., 2014, COMPUTER SCI
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kulkarni T.D., 2015, DEEP CONVOLUTIONAL I, V71, P2539
   Lake Brenden M, 2017, BEHAV BRAIN SCI, V40
   Moreno P, 2016, LECT NOTES COMPUT SC, V9915, P170, DOI 10.1007/978-3-319-49409-8_16
   Perarnau G, 2016, INVERTIBLE CONDITION
   Shen XY, 2016, COMPUT GRAPH FORUM, V35, P93, DOI 10.1111/cgf.12814
   Siddharth N, 2017, LEARNING DISENTANGLE
   Wang C. Y., 2017, P 26 INT JOINT C ART, P2901
   Xia Yingce, 2016, DUAL LEARNING MACHIN
   Xiao Taihong, 2017, DNA GAN LEARNING DIS
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000040
DA 2019-06-15
ER

PT S
AU Fichtenberger, H
   Rohde, D
AF Fichtenberger, Hendrik
   Rohde, Dennis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHM; WEIGHT
AB In the k-nearest neighborhood model (k-NN), we are given a set of points P, and we shall answer queries q by returning the k nearest neighbors of q in P according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many k-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed k-NN is not explicit. We study property testing of k-NN graphs in theory and evaluate it empirically: given a point set P subset of R-delta and a directed graph G = (P, E), is G a k-NN graph, i.e., every point p is an element of P has outgoing edges to its k nearest neighbors, or is it epsilon-far from being a k-NN graph? Here, is an element of-far means that one has to change more than an is an element of-fraction of the edges in order to make G a k-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the k-NN property, with complexity O(root nk(2)/epsilon(2)) measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of Omega(root n/epsilon k). We evaluate our tester empirically on the k-NN models computed by various algorithms and show that it can be used to detect k-NN models with bad accuracy in significantly less time than the building time of the k-NN model.
C1 [Fichtenberger, Hendrik; Rohde, Dennis] TU Dortmund, Dortmund, Germany.
RP Fichtenberger, H (reprint author), TU Dortmund, Dortmund, Germany.
EM hendrik.fichtenberger@tu-dortmund.de; dennis.rohde@cs.tu-dortmund.de
FU European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013) / ERC grant [307696]
FX The research leading to these results has received funding from the
   European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013) / ERC grant agreement no 307696. We thank the
   anonymous reviewers for their comments and questions, which we addressed
   by adding Lemma 12 and Lemma 15 most notably.
CR Ann Arbor Algorithms, 2018, KGRAPH LIB K NEAR NE
   Aumuller Martin, 2017, Similarity Search and Applications. 10th International Conference, SISAP 2017. Proceedings: LNCS 10609, P34, DOI 10.1007/978-3-319-68474-1_3
   Ben-Zwi O, 2007, INFORM PROCESS LETT, V102, P219, DOI 10.1016/j.ipl.2006.12.011
   Bernhardsson Erik, 2018, ANN BENCHMARKS ALGOS
   Bernhardsson Erik, 2018, HFICHTENBERGER ANN B, DOI [10.5281/zenodo.1463824, DOI 10.5281/ZENODO.1463824]
   Bernhardsson Erik, 2018, ANN BENCHMARKS BENCH
   Boytsov L, 2013, LECT NOTES COMPUT SC, V8199, P280, DOI 10.1007/978-3-642-41062-8_28
   CALLAHAN PB, 1995, J ASSOC COMPUT MACH, V42, P67, DOI 10.1145/200836.200853
   Chen J, 2009, J MACH LEARN RES, V10, P1989
   Connor M, 2010, IEEE T VIS COMPUT GR, V16, P599, DOI 10.1109/TVCG.2010.9
   Czumaj A, 2005, SIAM J COMPUT, V35, P91, DOI 10.1137/S0097539703435297
   Czumaj A, 2009, SIAM J COMPUT, V39, P904, DOI 10.1137/060672121
   Czumaj A, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1367064.1367071
   Czumaj Artur, 2000, P 8 ESA, P155, DOI 10.1007/3-540-45253-2_15
   Dasarathy Belur V., 1991, NEAREST NEIGHBOR NOR
   Fichtenberger Hendrik, THEORY BASED EVALUAT
   Fichtenberger Hendrik, 2018, HFICHTENBERGER KNN T, DOI [10.5281/zenodo.1463804, DOI 10.5281/ZENODO.1463804]
   FRIEDMAN JH, 1975, IEEE T COMPUT, V24, P1000, DOI 10.1109/T-C.1975.224110
   Fu Xiping, 2018, UCI MACHINE LEARNING
   FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297
   Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060
   Hellweg F, 2010, LECT NOTES COMPUT SC, V6390, P306, DOI 10.1007/978-3-642-16367-8_24
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Kabatyanskii G. A., 1978, Problems of Information Transmission, V14, P1
   LeCun Yann, 2018, MNIST HANDWRITTEN DI
   Maillo J, 2017, KNOWL-BASED SYST, V117, P3, DOI 10.1016/j.knosys.2016.06.012
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Naidan Bilegsaikhan, 2018, NONMETRIC SPACE LIB
   Parnas Michal, 2001, 33 ANN ACM S THEOR C, P276, DOI [10.1145/380752.380811, DOI 10.1145/380752.380811]
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rubinfeld R, 1996, SIAM J COMPUT, V25, P252, DOI 10.1137/S0097539793255151
   Shakhnarovich G, 2006, NEURAL INFORM PROCES
   WYNER AD, 1965, BELL SYST TECH J, V44, P1061, DOI 10.1002/j.1538-7305.1965.tb04170.x
   Zalando Research, 2018, FASH MNIST MNIST LIK
   ZEGER K, 1994, IEEE T INFORM THEORY, V40, P1647, DOI 10.1109/18.333884
   Zhang SC, 2018, IEEE T NEUR NET LEAR, V29, P1774, DOI 10.1109/TNNLS.2017.2673241
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001030
DA 2019-06-15
ER

PT S
AU Figueiredo, F
   Borges, G
   de Melo, POSV
   Assuncao, R
AF Figueiredo, Flavio
   Borges, Guilherme
   Vaz de Melo, Pedro O. S.
   Assuncao, Renato
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast Estimation of Causal Interactions using Wold Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to accomplish this task, our work is the first to explore the use of Wold processes. By doing so, we are able to develop asymptotically fast MCMC learning algorithms. With N being the total number of events and K the number of processes, our learning algorithm has a O(N ( log (N) + log (K))) cost per iteration. This is much faster than the O((NK2)-K-3) or O(K-3) for the state of the art. Our approach, called GRANGER-BUSCA, is validated on nine datasets. This is an advance in relation to most prior efforts which focus mostly on subsets of the Memetracker data. Regarding accuracy, GRANGER-BUSCA is three times more accurate (in Precision @10) than the state of the art for the commonly explored subsets Memetracker. Due to GRANGER-BUSCA's much lower training complexity, our approach is the only one able to train models for larger, full, sets of data.
C1 [Figueiredo, Flavio; Borges, Guilherme; Vaz de Melo, Pedro O. S.; Assuncao, Renato] Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil.
RP Figueiredo, F (reprint author), Univ Fed Minas Gerais, Belo Horizonte, MG, Brazil.
EM flaviovdf@dcc.ufmg.br; guilherme.borges@dcc.ufmg.br; olmo@dcc.ufmg.br;
   assuncao@dcc.ufmg.br
FU project ATMOSPHERE (atmosphere-eubrazil.eu) - Brazilian Ministry of
   Science, Technology and Innovation [51119]; European Commission under
   the Cooperation Programme, Horizon 2020 [777154]; CNPq; CAPES; Fapemig;
   Microsoft Azure [CRM:0740801]
FX We thank Fabricio Murai and the anonymous reviewers for providing
   comments. We also thank Gabriel Coutinho for discussions on the
   mathematical properties of GRANGER-BUSCA, as well as Alexandre Souza for
   providing pointers to prior studies. This work has been partially
   supported by the project ATMOSPHERE (atmosphere-eubrazil.eu), funded by
   the Brazilian Ministry of Science, Technology and Innovation (Project
   51119 -MCTI/RNP 4th Coordinated Call) and by the European Commission
   under the Cooperation Programme, Horizon 2020 grant agreement no 777154.
   Funding was also provided by the authors' individual grants from CNPq,
   CAPES and Fapemig. Computational resources were provided by the
   Microsoft Azure for Data Science Research Award (CRM:0740801).
CR Achab M., 2017, ICML
   Alves R., 2016, KDD
   Arpaci-Dusseau R. H., 2015, OPERATING SYSTEMS 3
   Bacry E., 2015, MARKET MICROSTRUCTUR, V1, P1
   Bao Y., 2017, ML FOR HC
   Barabasi A.-L., 2016, NETWORK SCI
   Bergstra James S, 2011, NIPS
   Blundell C., 2012, NIPS
   Chen S., 2017, ARXIV170704928
   Chevallier J., 2015, MATH MODELS METHODS, V25
   Choi E., 2015, ICDM
   Cox D. R., 1955, J ROYAL STAT SOC B
   Daley D., 1982, J APPL PROBABILITY, V19
   Daley D. J., 2003, INTRO THEORY POINT P, VI
   de Melo P. O. S. Vaz, 2013, WWW
   Dhamala M, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.018701
   Didelez V., 2008, J ROYAL STAT SOC B, V70
   Du N., 2015, KDD
   Etesami J., 2016, UAI
   Fenwick P. M., 1994, SOFTWARE PRACTICE EX, V24
   Granger C. W., 1969, ECONOMETRICA J ECONO
   Guttorp P, 2012, INT STAT REV, V80, P253, DOI 10.1111/j.1751-5823.2012.00181.x
   Hallac D., 2017, KDD
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Hawkes A. G., 1971, J ROYAL STAT SOC B
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Horn R. A., 1990, MATRIX ANAL
   Isham V., 1977, J APPL PROBABILITY, V14
   Kleinberg J., 2009, KDD
   Kumar S., 2016, ICDM
   Leskovec J., 2010, ICWSM
   Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727
   Li A. Q., 2014, KDD
   Li S., 2017, CIKM
   Linderman S., 2014, ICML
   Linderman S. W., 2015, ARXIV150703228
   Monti RP, 2014, NEUROIMAGE, V103, P427, DOI 10.1016/j.neuroimage.2014.07.033
   Namaki A., 2011, PHYSICA A, V390
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Panzarasa P., 2009, J ASS INFORM SCI TEC, V60
   Paranjape A., 2017, WSDM
   Rizoiu M. - A., 2018, FRONTIERS MULTIMEDIA
   Silva J, 2009, IEEE T PATTERN ANAL, V31, P563, DOI 10.1109/TPAMI.2008.232
   Steyvers M., 2007, HDB LATENT SEMANTIC, V427
   Terenin A., 2017, NIPS
   Vaz de Melo P. O. S., 2015, ACM T KNOWLEDGE DISC, V9
   Wold H., 1948, SCANDINAVIAN ACTUARI, V1948
   Xu H., 2016, ICML
   Yang Y., 2017, NIPS
   Yin H., 2017, KDD
   Yu H. - F., 2015, WWW
   Yuan J., 2015, WWW
   Zhou K., 2013, AISTATS
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303001
DA 2019-06-15
ER

PT S
AU Figurnov, M
   Mohamed, S
   Mnih, A
AF Figurnov, Michael
   Mohamed, Shakir
   Mnih, Andriy
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Implicit Reparameterization Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.
C1 [Figurnov, Michael; Mohamed, Shakir; Mnih, Andriy] DeepMind, London, England.
RP Figurnov, M (reprint author), DeepMind, London, England.
EM mfigurnov@google.com; shakir@google.com; amnih@google.com
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   BAYDIN AG, 2015, ARXIV150205767
   Bhattacharjee G., 1970, J R STAT SOC C-APPL, V19, P285
   Blei D. M., 2006, ICML, P113, DOI DOI 10.1145/1143844.1143859
   Blei David, 2005, NIPS, P147
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blundell Charles, 2015, INT C MACH LEARN
   Burda Y., 2016, INT C LEARN REPR
   Davidson T. R., 2018, C UNC ART INT
   Devroye L, 1986, NONUNIFORM RANDOM VA
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   Gal Y., 2016, INT C MACH LEARN, P1050
   Gal Y., 2016, ADV NEURAL INFORM PR, P1019
   Glasserman P., 2013, MONTE CARLO METHODS, V53
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Graves Alex, 2016, ARXIV160705690
   Hill G. W., 1977, ACM Transactions on Mathematical Software, V3, P279, DOI 10.1145/355744.355753
   Hoffman M., 2010, ADV NEURAL INFORM PR, P856
   Hoffman M., 2015, INT C ART INT STAT, P361
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310
   Jang Eric, 2017, INT C LEARN REPR
   Jankowiak  M., 2018, INT C MACH LEARN
   Jankowiak  M., 2018, ARXIV180601856
   Kingma Diederik, 2014, INT C LEARN REPR
   Knowles D. A., 2015, ARXIV150901631
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Maddison Chris J., 2017, INT C LEARN REPR
   Mardia K. V., 2009, DIRECTIONAL STAT, P494
   Mnih Andriy, 2014, INT C MACH LEARN
   Molchanov  D., 2017, INT C MACH LEARN
   MOORE RJ, 1982, J R STAT SOC C-APPL, V31, P330
   Naesseth  C., 2017, INT C ART INT STAT, P489
   Nalisnick  E., 2016, ADV NEUR INF PROC SY, V2
   Nalisnick  E., 2017, INT C LEARN REPR
   Paisley  J., 2012, INT C MACH LEARN
   Paszke  A., 2017, ADV NEUR INF PROC SY
   Ranganath  R., 2014, AISTATS, P814
   Rezende D. J., 2014, INT C MACH LEARN
   Roeder  G., 2017, ADV NEURAL INFORM PR
   Ruiz F. R., 2016, ADV NEURAL INFORM PR
   Ruschendorf  L., 2013, MATH RISK ANALA, P3
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   Srivastava  A., 2017, INT C LEARN REPR
   Srivastava  A., 2018, ARXIV180407944
   SURI R, 1988, MANAGE SCI, V34, P39, DOI 10.1287/mnsc.34.1.39
   Teh Y.-W., 2007, ADV NEURAL INFORM PR, P1353
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   von Mises R, 1918, PHYS Z, V19, P490
   Wallach H. M, 2009, ADV NEURAL INFORM PR, P1973
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
   Zhang H, 2018, INT C LEARN REPR
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300041
DA 2019-06-15
ER

PT S
AU Finn, C
   Xu, K
   Levine, S
AF Finn, Chelsea
   Xu, Kelvin
   Levine, Sergey
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Probabilistic Model-Agnostic Meta-Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.
C1 [Finn, Chelsea; Xu, Kelvin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Finn, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM cbfinn@eecs.berkeley.edu; kelvinxu@eecs.berkeley.edu;
   svlevine@eecs.berkeley.edu
FU NSF Graduate Research Fellowship; NSF [IIS-1651843]; Office of Naval
   Research; NVIDIA
FX We thank Marvin Zhang and Dibya Ghosh for feedback on an early draft of
   this paper. This research was supported by an NSF Graduate Research
   Fellowship, NSF IIS-1651843, the Office of Naval Research, and NVIDIA.
CR Barber D., 1998, NEURAL INFORM PROCES
   Blundell C, 2015, ARXIV150505424
   Braun D. A., 2010, BEHAV BRAIN RES
   Daume III H., 2009, C UNC ART INT UAI
   Duan  Y., 2016, ARXIV161102779
   Edwards H., 2017, INT C LEARN REPR ICL
   Fei-Fei L., 2003, C COMP VIS PATT REC
   Finn C., 2017, ARXIV170904905
   Finn C., 2017, INT C MACH LEARN ICM
   Finn C., 2017, ARXIV171011622
   Fortunato M., 2017, ARXIV170402798
   Gao J., 2008, ACM SIGKDD C KNOWL D
   Garcia V., 2017, ARXIV171104043
   Grant E., 2018, INT C LEARN REPR ICL
   Grant E., 2017, NIPS WORKSH COGN INF
   Graves A., 2011, NEURAL INFORM PROCES
   Higgins I., 2017, INT C LEARN REPR ICL
   Hinton G. E., 1993, C COMP LEARN THEOR
   Hochreiter S., 2001, INT C ART NEUR NETW
   Hoffman Matthew D., 2013, J MACHINE LEARNING R
   Johnson M., 2016, NEURAL INFORM PROCES
   Kingma D. P., 2013, ARXIV13126114
   Lacoste A., 2017, ARXIV171205016
   Lake B., 2015, SCIENCE
   Lawrence N, 2004, P 21 INT C MACH LEAR, DOI 10.1145/1015330.1015382
   Li Z., 2017, ARXIV170709835
   MacKay D. J. C., 1992, NEURAL COMPUTATION
   Mishra N., 2018, INT C LEARN REPR
   Neal R M, 1995, THESIS
   Nichol A., 2018, ARXIV180302999
   Ravi S., 2017, INT C LEARN REPR ICL
   Rezende D. J, 2014, ARXIV14014082
   Santoro A., 2016, INT C MACH LEARN ICM
   Santos R. J., 1996, LINEAR ALGEBRA ITS A
   SCHMIDHUBER J, 1987, THESIS
   Shu R., 2018, ARXIV180508913
   Snell J., 2017, NEURAL INFORM PROCES
   Sung F., 2017, CORR
   Tenenbaum J. B., 1999, THESIS
   Vinyals O., 2016, NEURAL INFORM PROCES
   Wan J., 2012, C COMP VIS PATT REC
   Wang Y.-X., 2016, EUR C COMP VIS ECCV
   Yu K., 2005, INT C MACH LEARN ICM
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004011
DA 2019-06-15
ER

PT S
AU Finocchiaro, J
   Frongillo, R
AF Finocchiaro, Jessica
   Frongillo, Rafael
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Convex Elicitation of Continuous Real Properties
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CLASSIFICATION
AB A property or statistic of a distribution is said to be elicitable if it can be expressed as the minimizer of some loss function in expectation. Recent work shows that continuous real-valued properties are elicitable if and only if they are identifiable, meaning the set of distributions with the same property value can be described by linear constraints. From a practical standpoint, one may ask for which such properties do there exist convex loss functions. In this paper, in a finite-outcome setting, we show that in fact essentially every elicitable real-valued property can be elicited by a convex loss function. Our proof is constructive, and leads to convex loss functions for new properties.
C1 [Finocchiaro, Jessica; Frongillo, Rafael] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
RP Finocchiaro, J (reprint author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
EM jessica.finocchiaro@colorado.edu; raf@colorado.edu
FU National Science Foundation [CCF-1657598]
FX We would like to thank Bo Waggoner and Arpit Agarwal for their insights
   and the discussion which led to this project, and we thank Krisztina
   Dearborn for consultation on analysis results. Additionally, we would
   like to thank our reviewers for their feedback and suggestions. This
   project was funded by National Science Foundation Grant CCF-1657598.
CR Abbott S., 2001, UNDERSTANDING ANAL
   Abernethy J. D., 2011, ADV NEURAL INFORM PR, V24, P2600
   Agarwal A., 2015, JMLR WORKSHOP C P, P1
   Apostol T. M., 1974, MATH ANAL
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Buja A, 2005, LOSS FUNCTIONS BINAR
   Fissler T, 2016, ANN STAT, V44, P1680, DOI 10.1214/16-AOS1439
   Frongillo R., 2015, ADV NEURAL INFORM PR, V29
   Frongillo R., 2015, J MACH LEARN RES WOR, P1
   Frongillo R., 2017, PREPRINT
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Lambert N. S., 2018, ELICITATION EVALUATI
   Lambert N. S., 2009, P 10 ACM C EL COMM E, P109
   Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129
   Lambert NS, 2015, J ECON THEORY, V156, P389, DOI 10.1016/j.jet.2014.03.012
   Osband K. H., 1985, PROVIDING INCENTIVES
   Pouso R. L., 2012, ARXIV12031462
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Rockafellar R., 1997, PRINCETON MATH SERIE, V28
   SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229
   Steinwart I., 2014, J MACHINE LEARNING R, P482
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   Williamson RC, 2016, J MACH LEARN RES, V17, P1
   Witkowski J., 2018, P 32 AAAI C ART INT
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005002
DA 2019-06-15
ER

PT S
AU Fischer, V
   Kohler, J
   Pfeil, T
AF Fischer, Volker
   Koehler, Jan
   Pfeil, Thomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The streaming rollout of deep networks - towards fully model-parallel
   execution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NEURAL-NETWORKS
AB Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network's architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.
C1 [Fischer, Volker; Koehler, Jan; Pfeil, Thomas] Bosch Ctr Artificial Intelligence, Renningen, Germany.
RP Fischer, V (reprint author), Bosch Ctr Artificial Intelligence, Renningen, Germany.
EM volker.fischer@de.bosch.com; jan.koehler@de.bosch.com;
   thomas.pfeil@de.bosch.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Al-Rfou, 2016, ARXIV160502688 THEAN
   Amodei D., 2016, INT C MACH LEARN, P173
   Breuel TM, 2013, PROC INT CONF DOC, P683, DOI 10.1109/ICDAR.2013.140
   Brueckner Raymond, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4823, DOI 10.1109/ICASSP.2014.6854518
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira Joao, 2018, P EUR C COMP VIS ECC, P649
   Cho K., 2014, ARXIV14091259
   Chung Junyoung, 2017, INT C LEARN REPR ICL
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan Y., 2016, INT C MACH LEARN, P1329
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fan Y., 2014, 15 ANN C INT SPEECH
   Fernandez S, 2007, LECT NOTES COMPUT SC, V4669, P220
   Figurnov Michael, 2017, P IEEE C COMP VIS PA
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A., 2009, ADV NEURAL INFORM PR, P545
   Graves A., 2014, ARXIV14105401
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Greff Klaus, 2017, INT C LEARN REPR ICL
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Han S., 2016, INT C LEARN REPR ICL
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Z., 2015, ARXIV150801991
   Kim Y.-D., 2015, ARXIV151106530
   Koutnik J., 2014, P 31 INT C MACH LEAR, P1863
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao Qianli, 2016, ARXIV160403640
   Lin CM, 2014, NEURAL COMPUT APPL, V24, P487, DOI 10.1007/s00521-012-1242-5
   Little William A, 1974, HIGH TEMPERATURE SUP, P145
   Mathieu M., 2014, INT C LEARN REPR ICL
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Milan  A., 2017, AAAI, P4225
   Minsky  M., 1969, PERCEPTRONS INTRO CO
   Neil Daniel, 2016, ADV NEURAL INFORM PR
   Osindero Simon, 2017, INT C MACH LEARN ICM
   Pascanu R., 2013, ARXIV13126026
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Pundak G, 2017, INTERSPEECH, P1303, DOI 10.21437/Interspeech.2017-429
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shelhamer Evan, 2016, ECCV WORKSH
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Teerapittayanon Surat, 2016, INT C PATT REC ICPR
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   Torres Jordi, 2018, INT C LEARN REPR ICL
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Williams R. J., 1995, BACKPROPAGATION THEO, V1, P433, DOI DOI 10.1080/02673039508720837
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Zamir A. R., 2016, ARXIV161209508
   Zilly J. G., 2016, ARXIV160703474
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304008
DA 2019-06-15
ER

PT S
AU Flennerhag, S
   Yin, HJ
   Keane, J
   Elliot, M
AF Flennerhag, Sebastian
   Yin, Hujun
   Keane, John
   Elliot, Mark
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Breaking the Activation Function Bottleneck through Adaptive
   Parameterization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half the number of iterations.
C1 [Flennerhag, Sebastian; Yin, Hujun; Keane, John; Elliot, Mark] Univ Manchester, Manchester, Lancs, England.
   [Flennerhag, Sebastian; Yin, Hujun] Alan Turing Inst, London, England.
RP Flennerhag, S (reprint author), Univ Manchester, Manchester, Lancs, England.; Flennerhag, S (reprint author), Alan Turing Inst, London, England.
EM sflennerhag@turing.ac.uk; hujun.yin@manchester.ac.uk;
   john.keane@manchester.ac.uk; mark.elliot@manchester.ac.uk
FU ESRC via the North West Doctoral Training Centre [ES/J500094/1]
FX The authors would like to thank anonymous reviewers for their comments.
   This work was supported by ESRC via the North West Doctoral Training
   Centre, grant number ES/J500094/1.
CR Al-Shedivat Maruan, 2018, INT C LEARN REPR
   Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Bengio Samy, 1995, OPTIMALITY BIOL ARTI, P6
   Bengio Yoshua, 1991, LEARNING SYNAPTIC LE
   Bertinetto L., 2016, ADV NEURAL INFORM PR, P523
   Brock Andrew, 2018, INT C LEARN REPR
   Canziani  A., 2016, ARXIV160507678
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J, 2014, ARXIV14123555
   Clevert Djork-Arne, 2015, INT C LEARN REPR
   Cooijmans Tim, 2016, INT C LEARN REPR
   Cybenko George, 1989, MCSS
   Dauphin Yann N, 2017, INT C MACH LEARN
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Fernando Chrisantha, 2016, GECCO
   Finn Chelsea, 2017, INT C MACH LEARN
   Frankle Jonathan, 2018, ARXIV180303635
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gomez F, 2005, LECT NOTES COMPUT SC, V3697, P383
   Grave Edouard, 2017, INT C LEARN REPR
   Graves A, 2013, ARXIV13080850
   Ha David, 2017, INT C LEARN REPR
   Ha David, 2018, INT C LEARN REPR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Inan Hakan, 2017, INT C LEARN REPR
   Jaderberg Max, 2017, INT C MACH LEARN
   Kingma D. P., 2015, INT C LEARN REPR
   Klambauer G., 2017, ADV NEURAL INFORM PR, P972
   Krause Ben, 2016, ARXIV160907959
   Krause Ben, 2017, ARXIV170907432
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lee Yoonho, 2017, INT C MACH LEARN
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Melis Gabor, 2018, INT C LEARN REPR
   Merity Stephen, 2018, INT C LEARN REPR
   Merity Stephen, 2017, INT C LEARN REPR
   Mikolov T., 2012, THESIS
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Mikolov Tomas, 2012, PREPRINT
   Novak R., 2018, INT C LEARN REPR
   Press O., 2017, P 15 C EUR CH ASS CO, V2, P157, DOI DOI 10.18653/V1/E17-2025
   Radford  A., 2017, ARXIV170401444
   Ravi Sachin, 2017, INT C LEARN REPR
   Saxe A.M., 2013, ARXIV13126120
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Stanley KO, 2009, ARTIF LIFE, V15, P185, DOI 10.1162/artl.2009.15.2.15202
   Suarez Joseph, 2017, ADV NEURAL INFORM PR, P3269
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   Wu Yuhuai, 2016, ADV NEURAL INFORM PR, P2864
   Yang Z., 2018, INT C LEARN REPR
   Zaremba Wojciech, 2015, INT C LEARN REPR
   Zilly J. G., 2016, ARXIV160703474
   Zoph Barret, 2017, INT C LEARN REPR
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002030
DA 2019-06-15
ER

PT S
AU Fletcher, AK
   Pandit, P
   Rangan, S
   Sarkar, S
   Schniter, P
AF Fletcher, Alyson K.
   Pandit, Parthe
   Rangan, Sundeep
   Sarkar, Subrata
   Schniter, Philip
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Plug-in Estimation in High-Dimensional Linear Inverse Problems: A
   Rigorous Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MESSAGE-PASSING ALGORITHMS; RECOVERY
AB Estimating a vector x from noisy linear measurements Ax + w often requires use of prior knowledge or structural constraints on x for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or "plug-in" denoiser function that can be designed in a modular manner based on the prior knowledge about x. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this "plug-and-play" VAMP can be exactly predicted for high-dimensional right-rotationally invariant random A and Lipschitz denoisers. The method is demonstrated on applications in image recovery and parametric bilinear estimation.
C1 [Fletcher, Alyson K.] UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.
   [Pandit, Parthe] UC Los Angeles, Dept ECE, Los Angeles, CA USA.
   [Rangan, Sundeep] NYU, Dept ECE, New York, NY 10003 USA.
   [Sarkar, Subrata; Schniter, Philip] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
RP Fletcher, AK (reprint author), UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.
EM akfletcher@ucla.edu; parthepandit@ucla.edu; srangan@nyu.edu;
   sarkar.51@osu.edu; schniter.1@osu.edu
FU National Science Foundation [1738285, 1738286, 1116589, 1302336,
   1547332, CCF-1527162]; Office of Naval Research [N00014-15-1-2677];
   industrial affiliates of NYU WIRELESS
FX A. K. Fletcher and P. Pandit were supported in part by the National
   Science Foundation under Grants 1738285 and 1738286 and the Office of
   Naval Research under Grant N00014-15-1-2677. S. Rangan was supported in
   part by the National Science Foundation under Grants 1116589, 1302336,
   and 1547332, and the industrial affiliates of NYU WIRELESS. The work of
   P. Schniter was supported in part by the National Science Foundation
   under Grant CCF-1527162.
CR Ahmed A, 2014, IEEE T INFORM THEORY, V60, P1711, DOI 10.1109/TIT.2013.2294644
   Anderson M. R., 2014, ADV NEURAL INFORM PR, P1745
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Berthier R., 2017, ARXIV170803950
   Borgerding M, 2017, IEEE T SIGNAL PROCES, V65, P4293, DOI 10.1109/TSP.2017.2708040
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Caltagirone F, 2014, IEEE INT SYMP INFO, P1812, DOI 10.1109/ISIT.2014.6875146
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen S, 2017, PR ELECTROMAGN RES S, P2035, DOI 10.1109/PIERS-FALL.2017.8293472
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100
   Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223
   Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Fletcher A. K., 2017, P NEUR INF PROC SYST, P2542
   Fletcher A. K., 2018, 180610466 ARXIV
   Fletcher A, 2016, IEEE INT SYMP INFO, P190, DOI 10.1109/ISIT.2016.7541287
   Haykin S, 1994, BLIND DECONVOLUTION
   Javanmard A, 2013, INF INFERENCE, V2, P115, DOI 10.1093/imaiai/iat004
   Kamilov US, 2014, IEEE T INFORM THEORY, V60, P2969, DOI 10.1109/TIT.2014.2309005
   Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733
   Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002
   Ma JJ, 2017, IEEE ACCESS, V5, P2020, DOI 10.1109/ACCESS.2017.2653119
   Ma YT, 2017, IEEE INT SYMP INFO, P231, DOI 10.1109/ISIT.2017.8006524
   Matsushita R., 2013, ADV NEURAL INFORM PR, V26, P917
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Opper M, 2005, J MACH LEARN RES, V6, P2177
   Parker JT, 2016, IEEE J-STSP, V10, P795, DOI 10.1109/JSTSP.2016.2539123
   Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056
   Rangan S., 2011, P IEEE INT S INF THE, P2174
   Rangan S, 2017, IEEE INT SYMP INFO, P1588, DOI 10.1109/ISIT.2017.8006797
   Rangan S, 2017, IEEE T SIGNAL PROCES, V65, P4577, DOI 10.1109/TSP.2017.2713759
   Rangan S, 2014, IEEE INT SYMP INFO, P236, DOI 10.1109/ISIT.2014.6874830
   Rangan S, 2013, IEEE INT SYMP INFO, P664, DOI 10.1109/ISIT.2013.6620309
   Rangan  Sundeep, 2016, ARXIV161003082
   Scharf L. L., 1991, STAT SIGNAL PROCESSI, V63
   Schniter P., 2017, P INT BIOM ASTR SIGN, P77
   Sun P, 2018, IEEE T SIGNAL PROCES, V66, P2772, DOI 10.1109/TSP.2018.2812720
   Taeb A., 2013, ARXIV13032389
   Takeuchi K, 2017, IEEE INT SYMP INFO, P501, DOI 10.1109/ISIT.2017.8006578
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Vila J, 2015, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2015.7178325
   Wang XR, 2017, INT CONF ACOUST SPEE, P1323, DOI 10.1109/ICASSP.2017.7952371
   Xie  J., 2012, P INT C NEUR INF PRO, P341
   Xu  L., 2014, ADV NEURAL INFORM PR, P1790
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhu H, 2011, IEEE T SIGNAL PROCES, V59, P2002, DOI 10.1109/TSP.2011.2109956
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002003
DA 2019-06-15
ER

PT S
AU Foster, DJ
   Sekhari, A
   Sridharan, K
AF Foster, Dylan J.
   Sekhari, Ayush
   Sridharan, Karthik
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Uniform Convergence of Gradients for Non-Convex Learning and
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We investigate 1) the rate at which refined properties of the empirical risk-in particular, gradients-converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed.
   Moving to non-smooth models we show--in contrast to the smooth case-that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.
C1 [Foster, Dylan J.; Sekhari, Ayush; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA.
RP Foster, DJ (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM djfoster@cornell.edu; sekhari@cs.cornell.edu; sridharan@cs.cornell.edu
FU NSF [CDSE-MSS 1521544]; Alfred P. Sloan Fellowship; NDSEG PhD
   fellowship; Facebook PhD fellowship; NSF CAREER [1750575]
FX K.S acknowledges support from the NSF under grants CDS&E-MSS 1521544 and
   NSF CAREER Award 1750575, and the support of an Alfred P. Sloan
   Fellowship. D.F. acknowledges support from the NDSEG PhD fellowship and
   Facebook PhD fellowship.
CR Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6241
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Borwein J. M., 2010, CONVEX ANAL NONLINEA
   Chi Jin, 2018, ADV NEURAL INFORM PR
   Clarke FH, 1990, OPTIMIZATION NONSMOO, V5
   Davis Damek, 2018, ARXIV181007590
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Golowich Noah, 2018, C LEARN THEOR
   Gonen Alon, 2017, C LEARN THEOR
   Hardt Moritz, 2017, INT C LEARN REPR
   Hardt Moritz, 2018, J MACHINE LEARNING R
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058
   Kakade S., 2011, ADV NEURAL INFORM PR, P927
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Ledoux M., 1991, PROBABILITY BANACH S
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Li  X., 2018, APPL COMPUTATIONAL H
   Li Yuanzhi, 2017, ADV NEURAL INFORM PR, P597
   Liu Huikang, 2016, PMLR, V48, P1158
   Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1
   Mohri M., 2012, FDN MACHINE LEARNING
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI 10.1214/aop/1176988477
   PISIER G, 1975, ISRAEL J MATH, V20, P326, DOI 10.1007/BF02760337
   Polyak Boris Teodorovich, 1963, ZH VYCH MAT MAT FIZ, V3, P643
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Song Mei, 2016, ANN STAT
   Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725
   Tibshirani  R., 2015, STAT LEARNING SPARSI
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P699
   Zeyuan Allen-Zhu, 2018, ADV NEURAL INFORM PR
   Zhang Chiyuan, 2017, INT C LEARN REPR
   Zhang Huishuai, 2016, INT C MACH LEARN
   Zhou Dongruo, 2018, ADV NEURAL INFORM PR
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003031
DA 2019-06-15
ER

PT S
AU Foster, DJ
   Krishnamurthy, A
AF Foster, Dylan J.
   Krishnamurthy, Akshay
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Contextual bandits with surrogate losses: Margin bounds and efficient
   algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive new margin-based regret bounds in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a root dT-type mistake bound against benchmark policies induced by d-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds.
C1 [Foster, Dylan J.] Cornell Univ, Ithaca, NY 14853 USA.
   [Krishnamurthy, Akshay] Microsoft Res, Nyc, NY USA.
RP Foster, DJ (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM djfoster@cs.cornell.edu; akshay@cs.umass.edu
FU NDSEG PhD fellowship; Facebook PhD fellowship
FX We thank Haipeng Luo, Karthik Sridharan, Chen-Yu Wei, and Chicheng Zhang
   for several helpful discussions. D.F. acknowledges the support of the
   NDSEG PhD fellowship and Facebook PhD fellowship.
CR AbbasiYadkori  Y., 2011, ADV NEURAL INFORM PR
   Agarwal Alekh, 2016, ARXIV160603966
   Agarwal Alekh, 2012, ARTIFICIAL INTELLIGE
   Agarwal Alekh, 2014, INT C MACH LEARN
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Auer Peter, 2002, SIAM J COMPUTING
   Bartlett P. L., 2006, J AM STAT ASS
   Beygelzimer Alina, 2017, INT C MACH LEARN
   Boucheron S., 2013, CONCENTRATION INEQUA
   Boucheron Stephane, 2005, ESAIM PROBABILITY ST
   Bubeck Sebastien, 2018, DISCRETE COMPUTATION
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi Nicolo, 2017, C LEARN THEOR
   Cheng X., 2018, ARXIV180501648
   Chu W., 2011, INT C ART INT STAT
   Dalalyan A. S, 2017, ARXIV171000095
   Daniely Amit, 2013, C LEARN THEOR
   Duchi John C., 2012, SIAM J OPTIMIZATION
   Foster Dylan J., 2018, C LEARN THEOR
   Foster Dylan J., 2015, ADV NEURAL INFORM PR
   Freund Yoav, 1997, J COMPUTER SYSTEM SC
   Hazan Elad, 2011, ADV NEURAL INFORM PR
   Hazan Elad, 2007, C LEARN THEOR
   Kakade Sham M., 2012, J MACHINE LEARNING R
   Kakade Sham M, 2009, ADV NEURAL INFORM PR
   Kakade Sham M., 2008, INT C MACH LEARN
   Langford  John, 2008, ADV NEURAL INFORM PR
   Lovasz Laszlo, 2007, RANDOM STRUCTURES AL
   Lykouris Thodoris, 2018, C LEARN THEOR
   Narayanan Hariharan, 2017, J MACHINE LEARNING R
   Neu Gergely, 2013, INT C ALG LEARN THEO
   Pisier Gilles, 1975, ISRAEL J MATH
   Raginsky Maxim, 2017, C LEARN THEOR
   Rakhlin Alexander, 2015, ARXIV150106598
   Rakhlin Alexander, 2016, INT C MACH LEARN
   Rakhlin Alexander, 2009, C LEARN THEOR
   Rakhlin Alexander, 2015, PROBABILITY THEORY R
   Rakhlin Alexander, 2017, C LEARN THEOR
   Rakhlin Alexander, 2015, J MACHINE LEARNING R
   Rakhlin Alexander, 2010, ADV NEURAL INFORM PR
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Slivkins Aleksandrs, 2011, C LEARN THEOR
   Srebro Nathan, 2011, ADV NEURAL INFORM PR
   Syrgkanis Vasilis, 2016, ADV NEURAL INFORM PR
   Syrgkanis Vasilis, 2016, INT C MACH LEARN
   Szepesvari Csaba, 2013, INT C MACH LEARN
   Tewari Ambuj, 2017, MOBILE HLTH
   Tong Zhang, 2004, J MACHINE LEARNING R
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302062
DA 2019-06-15
ER

PT S
AU Frecon, J
   Salzo, S
   Pontil, M
AF Frecon, Jordan
   Salzo, Saverio
   Pontil, Massimiliano
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bilevel Learning of the Group Lasso Structure
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OPTIMIZATION; SPARSITY; REGULARIZERS; REGRESSION; SELECTION
AB Regression with group-sparsity penalty plays a central role in high-dimensional prediction problems. However, most existing methods require the group structure to be known a priori. In practice, this may be a too strong assumption, potentially hampering the effectiveness of the regularization method. To circumvent this issue, we present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets. Our approach relies on an approximation scheme where the lower level problem is replaced by a smooth dual forward-backward algorithm with Bregman distances. We provide guarantees regarding the convergence of the approximate procedure to the exact problem and demonstrate the well behaviour of the proposed method on synthetic experiments. Finally, a preliminary application to genes expression data is tackled with the purpose of unveiling functional groups.
C1 [Frecon, Jordan; Salzo, Saverio; Pontil, Massimiliano] Ist Italiano Tecnol, Computat Stat & Machine Learning, Genoa, Italy.
   [Pontil, Massimiliano] UCL, Dept Comp Sci, London, England.
RP Frecon, J (reprint author), Ist Italiano Tecnol, Computat Stat & Machine Learning, Genoa, Italy.
FU SAP SE
FX We wish to thank Luca Franceschi and the anonymous referees for their
   useful comments. We also would like to thank Giorgio Valentini for
   providing the gene expression dataset. This work was supported in part
   by SAP SE.
CR Ahmed A, 2009, P NATL ACAD SCI USA, V106, P11878, DOI 10.1073/pnas.0901910106
   Bauschke H.H., 2017, CONVEX ANAL MONOTONE
   Bauschke HH, 2017, MATH OPER RES, V42, P330, DOI 10.1287/moor.2016.0817
   Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187
   Calatroni L., 2016, VARIATIONAL METHODS, P252
   Chen GHG, 1997, SIAM J OPTIMIZ, V7, P421, DOI 10.1137/S1052623495290179
   Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090
   Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Dempe S., 2002, FDN BILEVEL PROGRAMM
   Franceschi L., 2017, P MACHINE LEARNING R, V70, P1165
   Franceschi L., 2018, P 35 INT C MACH LEAR, V80, P1568
   Griewank A, 2008, EVALUATING DERIVATIV
   Hernandez- Lobato D., 2013, ADV NEURAL INFORM PR, V26, P746
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Jenatton R, 2011, J MACH LEARN RES, V12, P2777
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Kunisch K, 2013, SIAM J IMAGING SCI, V6, P938, DOI 10.1137/120882706
   Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896
   Ma SG, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-60
   Maclaurin D., 2015, P 32 INT C MACH LEAR, P2113
   Maurer A, 2012, J MACH LEARN RES, V13, P671
   Micchelli CA, 2013, ADV COMPUT MATH, V38, P455, DOI 10.1007/s10444-011-9245-9
   Ochs P, 2016, J MATH IMAGING VIS, V56, P175, DOI 10.1007/s10851-016-0663-7
   Nguyen QV, 2017, VIETNAM J MATH, V45, P519, DOI 10.1007/s10013-016-0238-3
   Re M., 2009, INT M COMP INT METH
   Reddi S. J., 2016, ADV NEURAL INFORM PR, V29, P1145
   Shervashidze N, 2015, IEEE T SIGNAL PROCES, V63, P4894, DOI 10.1109/TSP.2015.2446432
   Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhao P, 2009, ANN STAT, V37, P3468, DOI 10.1214/07-AOS584
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002081
DA 2019-06-15
ER

PT S
AU Freksen, C
   Kamma, L
   Larsen, KG
AF Freksen, Casper
   Kamma, Lior
   Larsen, Kasper Green
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fully Understanding The Hashing Trick
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID JOHNSON-LINDENSTRAUSS; RANDOM PROJECTIONS
AB Feature hashing, also known as the hashing trick, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix A : R-n -> R-m (where m << n) in order to reduce the dimension of the data from n to m while approximately preserving the Euclidean norm. Every column of A contains exactly one non-zero entry, equals to either -1 or 1.
   Weinberger et al. showed tail bounds on parallel to Ax parallel to(2)(2). Specifically they showed that for every epsilon, delta, if parallel to x parallel to(infinity)/parallel to x parallel to(2) is sufficiently small, and m is sufficiently large, then
   Pr[ vertical bar parallel to Ax parallel to(2)(2) - parallel to x parallel to(2)(2) vertical bar < epsilon parallel to x parallel to(2)(2)] >= 1 - delta.
   These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters parallel to x parallel to(infinity)/parallel to x parallel to(2), m, epsilon, delta remained an open question.
   We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants "hiding" in the asymptotic notation are, in fact, very close to 1, thus further illustrating the tightness of the presented bounds in practice.
C1 [Freksen, Casper; Kamma, Lior; Larsen, Kasper Green] Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.
RP Freksen, C (reprint author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.
EM cfreksen@cs.au.dk; lior.kamma@cs.au.dk; larsen@cs.au.dk
FU Villum Young Investigator Grant
FX This work was supported by a Villum Young Investigator Grant.
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096
   Andoni A., 2015, ADV NEURAL INFORM PR, P1225
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6
   Clarkson KL, 2009, ACM S THEORY COMPUT, P205
   Cohen  M.B., 2018, 1 S SIMPL ALG SOSA 2
   Dahlgaard  S., 2017, ADV NEURAL INFORM PR, V30, P6615
   Dahlgaard S, 2015, ANN IEEE SYMP FOUND, P1292, DOI 10.1109/FOCS.2015.83
   Dalessandro B, 2013, BIG DATA, V1, P110, DOI 10.1089/big.2013.0010
   Dasgupta A, 2010, ACM S THEORY COMPUT, P341
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dheeru D., 2017, UCI MACHINE LEARNING
   Freksen  C.B., 2017, 28 INT S ALG COMP IS
   Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI [DOI 10.4086/T0C.2012.V008A014, 10.4086/toc.2012.v008a014]
   Hegde C., 2008, ADV NEURAL INF PROCE, V20, P641
   Jayram TS, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483706
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902
   Larsen KG, 2017, ANN IEEE SYMP FOUND, P633, DOI 10.1109/FOCS.2017.64
   Maillard O., 2009, ADV NEURAL INFORM PR, V22, P1213
   Matousek J, 2008, RANDOM STRUCT ALGOR, V33, P142, DOI 10.1002/rsa.20218
   Meng X, 2013, P 45 ANN ACM S THEOR, P91
   Nelson J, 2013, P 45 ACM S THEOR COM, P101
   Paul S, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2641760
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
   Suthaharan Shan, 2015, MACHINE LEARNING MOD
   Thorup M, 2013, ANN IEEE SYMP FOUND, P90, DOI 10.1109/FOCS.2013.18
   Thorup M, 2012, SIAM J COMPUT, V41, P293, DOI 10.1137/100800774
   Vempala  S.S., 2005, DIMACS SERIES DISCRE
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305041
DA 2019-06-15
ER

PT S
AU Fried, D
   Hu, RH
   Cirik, V
   Rohrbach, A
   Andreas, J
   Morency, LP
   Berg-Kirkpatrick, T
   Saenko, K
   Klein, D
   Darrell, T
AF Fried, Daniel
   Hu, Ronghang
   Cirik, Volkan
   Rohrbach, Anna
   Andreas, Jacob
   Morency, Louis-Philippe
   Berg-Kirkpatrick, Taylor
   Saenko, Kate
   Klein, Dan
   Darrell, Trevor
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Speaker-Follower Models for Vision-and-Language Navigation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach-speaker-driven data augmentation, pragmatic reasoning and panoramic action space-dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.
C1 [Fried, Daniel; Hu, Ronghang; Rohrbach, Anna; Andreas, Jacob; Klein, Dan; Darrell, Trevor] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Cirik, Volkan; Morency, Louis-Philippe; Berg-Kirkpatrick, Taylor] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Saenko, Kate] Boston Univ, Boston, MA 02215 USA.
RP Fried, D (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
RI Hu, Ronghang/Q-8559-2019
FU US DoD; DARPA XAI; D3M; NSF [IIS-1833355]; Oculus VR; Berkeley
   Artificial Intelligence Research (BAIR) Lab; Huawei / Berkeley AI
   fellowship
FX This work was partially supported by US DoD and DARPA XAI and D3M, NSF
   awards IIS-1833355, Oculus VR, and the Berkeley Artificial Intelligence
   Research (BAIR) Lab. DF was supported by a Huawei / Berkeley AI
   fellowship. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the author(s) and do not
   necessarily reflect the views of the sponsors, and no official
   endorsement should be inferred.
CR Anderson  P., 2018, P IEEE C COMP VIS PA
   Andreas J., 2016, P C EMP METH NAT LAN
   Andreas J., 2015, P C EMP METH NAT LAN
   Artzi Y, 2013, T ASS COMPUT LINGUIS, V1, P49
   Bahdanau D., 2015, P INT C LEARN REPR I
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Branavan S, 2009, P JOINT C 47 ANN M A, P82
   Chang A., 2017, INT C 3D VIS 3DV
   Chen D, 2012, P 50 ANN M ASS COMP, V1, P430
   Chen K., 2017, P IEEE INT C COMP VI
   Cirik V., 2018, 32 AAAI C ART INT AA
   Cohn- Gordon R., 2018, P C N AM CHAPT ASS C
   Das A., 2018, P IEEE C COMP VIS PA
   Frank M. C., 2009, P ANN C COGN SCI SOC
   Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633
   Fried D., 2018, P C N AM CHAPT ASS C
   Goodman ND, 2013, TOP COGN SCI, V5, P173, DOI 10.1111/tops.12007
   Grice H. P., 1975, SYNTAX SEMANTICS, V3, P41, DOI DOI 10.1017/S0022226700005296
   Gulcehre C., 2015, ARXIV150303535
   Guu K., 2017, P ANN M ASS COMP LIN
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hermann K. M., 2017, CORR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hu R., 2016, P IEEE C COMP VIS PA
   Hu R., 2017, P IEEE C COMP VIS PA
   Hu R., 2016, P EUR C COMP VIS ECC
   Kocisky T., 2016, P 2016 C EMP METH NA, P1078
   Liu C, 2017, PROCEEDINGS OF THE ASME INTERNAL COMBUSTION ENGINE FALL TECHNICAL CONFERENCE, 2017, VOL 2
   Long R., 2016, P ANN M ASS COMP LIN
   Luo R., 2017, P IEEE C COMP VIS PA
   Mao J., 2016, P IEEE C COMP VIS PA
   McClosky D., 2006, P N AM CHAPT ASS COM, P152, DOI DOI 10.3115/1220835.1220855
   Mei H., 2016, P C ART INT AAAI
   Misra D., 2017, P C EMP METH NAT LAN
   Monroe W., 2017, T ASS COMPUTATIONAL, V5, P325
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pathak Deepak, 2018, ARXIV180408606
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Plummer B., 2015, P IEEE INT C COMP VI
   Radosavovic I., 2017, ARXIV171204440
   Rohrbach A., 2016, P EUR C COMP VIS ECC
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/TIT.1965.1053799
   Sennrich  R., 2016, P 54 ANN M ASS COMP, V1, P86
   Silver D., 2017, ARXIV171201815
   Smith N. J., 2013, ADV NEURAL INFORM PR, P3039
   Sukhbaatar S., 2017, ARXIV170305407
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tellex S., 2011, 25 AAAI C ART INT, V1, P2
   Vasudevan A. B., 2018, P IEEE WINT C APPL C
   Vedantam R., 2017, P IEEE C COMP VIS PA, V3
   WANG MZ, 2016, P EUR C COMP VIS ECC, V9912, P696, DOI DOI 10.1007/978-3-319-46484-8_42
   Wang X., 2018, ARXIV180307729
   Weber T., 2017, ARXIV170706203
   Yu L., 2017, P IEEE C COMP VIS PA
   Yu L., 2018, P IEEE C COMP VIS PA
NR 58
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303032
DA 2019-06-15
ER

PT S
AU Friedman, T
   Van den Broeck, G
AF Friedman, Tal
   Van den Broeck, Guy
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Approximate Knowledge Compilation by Online Collapsed Importance
   Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE
AB We introduce collapsed compilation, a novel approximate inference algorithm for discrete probabilistic graphical models. It is a collapsed sampling algorithm that incrementally selects which variable to sample next based on the partial compilation obtained so far. This online collapsing, together with knowledge compilation inference on the remaining variables, naturally exploits local structure and context-specific independence in the distribution. These properties are used implicitly in exact inference, but are difficult to harness for approximate inference. Moreover, by having a partially compiled circuit available during sampling, collapsed compilation has access to a highly effective proposal distribution for importance sampling. Our experimental evaluation shows that collapsed compilation performs well on standard benchmarks. In particular, when the amount of exact inference is equally limited, collapsed compilation is competitive with the state of the art, and outperforms it on several benchmarks.
C1 [Friedman, Tal; Van den Broeck, Guy] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
RP Friedman, T (reprint author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
EM tal@cs.ucla.edu; guyvdb@cs.ucla.edu
FU NSF [IIS-1657613, IIS-1633857, CCF-1837129]; DARPA XAI
   [N66001-17-2-4032]
FX We thank Jonas Vlasselaer and Wannes Meert for initial discussions.
   Additionally, we thank Arthur Choi, Yujia Shen, Steven Holtzen, and
   YooJung Choi for helpful feedback. This work is partially supported by a
   gift from Intel, NSF grants #IIS-1657613, #IIS-1633857, #CCF-1837129,
   and DARPA XAI grant #N66001-17-2-4032.
CR Bekker J., 2015, ADV NEURAL INFORM PR, P2242
   Bidyuk B, 2007, J ARTIF INTELL RES, V28, P1
   Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115
   Chakraborty S., 2014, P 28 AAAI C ART INT, P1722
   Chan Hei, 2006, P 22 C UNC ART INT, P63
   Chavira M, 2006, INT J APPROX REASON, V42, P4, DOI 10.1016/j.ijar.2005.10.001
   Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002
   Choi Arthur, 2006, P NAT C ART INT, V21, P1107
   Choi Arthur, 2005, P 21 C UNC ART INT U, P128
   Choi Arthur, 2017, ICML
   Choi Arthur, 2013, ECSQARU
   Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357
   Darwiche A, 2001, J ACM, V48, P608, DOI 10.1145/502090.502091
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Darwiche Adnan, 1999, P IJCAI
   Darwiche Adnan, 2011, P IJCAI
   De Raedt L, 2015, MACH LEARN, V100, P5, DOI 10.1007/s10994-015-5494-z
   Dechter Rina, 2002, P 18 C UNC ART INT U, P128
   Ermon S., 2013, ADV NEURAL INFORM PR, P2085
   Fierens D, 2015, THEOR PRACT LOG PROG, V15, P358, DOI 10.1017/S1471068414000076
   Gogate V, 2011, ARTIF INTELL, V175, P694, DOI 10.1016/j.artint.2010.10.009
   Gogate Vibhav, 2007, P 11 C ART INT STAT, P147
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lowd Daniel, 2010, NIPS, P1477
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Tokdar ST, 2010, WIRES COMPUT STAT, V2, P54, DOI 10.1002/wics.56
   Van den Broeck G, 2015, FOUND TRENDS DATABAS, V7, pI, DOI 10.1561/1900000052
   Van den Broeck Guy, 2015, P 29 C ART INT AAAI
   VandenBroeck G., 2013, THESIS
   Vlasselaer J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1852
   Vlasselaer J, 2016, ARTIF INTELL, V232, P43, DOI 10.1016/j.artint.2015.12.001
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002056
DA 2019-06-15
ER

PT S
AU Friesen, AL
   Domingos, P
AF Friesen, Abram L.
   Domingos, Pedro
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Submodular Field Grammars: Representation, Inference, and Application to
   Image Parsing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ENERGY MINIMIZATION
AB Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production A -> BC is linear in the length of A, in an image there are an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the resulting model a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we show promising improvements in accuracy when using SFGs for scene understanding, and demonstrate exponential improvements in inference time compared to traditional methods, while returning comparable minima.
C1 [Friesen, Abram L.; Domingos, Pedro] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.
RP Friesen, AL (reprint author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.
EM afriesen@cs.washington.edu; pedrod@cs.washington.edu
FU ONR [N00014-16-1-2697]; AFRL [FA8750-13-2-0019]
FX AF would like to thank Robert Gens, Rahul Kidambi, and Gena Barnabee for
   useful discussions, insights, and assistance with this document. The
   DGX-1 used for this research was donated by NVIDIA. This research was
   partly funded by ONR grant N00014-16-1-2697 and AFRL contract
   FA8750-13-2-0019. The views and conclusions contained in this document
   are those of the authors and should not be interpreted as necessarily
   representing the official policies, either expressed or implied, of ONR,
   AFRL, or the United States Government.
CR Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chandrasekaran V., 2008, P 24 C UNC ART INT, P70
   Chen L., 2015, P INT C LEARN REPR
   Chen Liang- Chieh, 2016, ARXIV160600915CSCV
   Delong A, 2012, INT J COMPUT VISION, V100, P38, DOI 10.1007/s11263-012-0531-x
   Gens R., 2013, P 30 INT C MACH LEAR, V28, P873
   Gould Stephen, 2009, P IEEE INT C COMP VI
   GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271
   Hopcroft JE, 1979, INTRO AUTOMATA THEOR
   IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388
   Jurafsky D., 2000, SPEECH LANGUAGE PROC
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031
   Kumar M. Pawan, 2009, P 25 C UNC ART INT, P313
   Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143
   Lempitsky Victor, 2011, NEURAL INFORM PROCES
   Poon H., 2011, P 27 C UNC ART INT, P337
   Russell Chris, 2010, 26 C UNC ART INT
   Sharma A., 2014, P ADV NEUR INF PROC, P2447
   Shotton J., 2006, P EUR C COMP VIS ECC, V3951
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Zhao Yibiao, 2011, ADV NEURAL INFORM PR
   Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304033
DA 2019-06-15
ER

PT S
AU Fromm, J
   Patel, S
   Philipose, M
AF Fromm, Josh
   Patel, Shwetak
   Philipose, Matthai
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Heterogeneous Bitwidth Binarization in Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recent work has shown that fast, compact low-bitwidth neural networks can be surprisingly accurate. These networks use homogeneous binarization: all parameters in each layer or (more commonly) the whole model have the same low bitwidth (e.g., 2 bits). However, modern hardware allows efficient designs where each arithmetic instruction can have a custom bitwidth, motivating heterogeneous binarization, where every parameter in the network may have a different bitwidth. In this paper, we show that it is feasible and useful to select bitwidths at the parameter granularity during training For instance a heterogeneously quantized version of modern networks such as AlexNet and MobileNet, with the right mix of 1-, 2- and 3-bit parameters that average to just 1.4 bits can equal the accuracy of homogeneous 2-bit versions of these networks. Further, we provide analyses to show that the heterogeneously binarized systems yield FPGA- and ASIC-based implementations that are correspondingly more efficient in both circuit area and energy efficiency than their homogeneous counterparts.
C1 [Fromm, Josh] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Patel, Shwetak] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA.
   [Philipose, Matthai] Microsoft Res, Redmond, WA 98052 USA.
RP Fromm, J (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM jwfromm@uw.edu; shwetak@cs.washington.edu; matthaip@microsoft.com
CR Alemdar H, 2017, IEEE IJCNN, P2547, DOI 10.1109/IJCNN.2017.7966166
   Chen  W., 2015, INT C MACH LEARN, P2285
   Courbariaux M, 2016, ARXIV160202830
   Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, P3123
   Dong Yinpeng, 2017, ARXIV170801001
   Ehliar A, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), P131, DOI 10.1109/FPT.2014.7082765
   Gong Y., 2014, ARXIV14126115
   Han S., 2015, ARXIV151000149
   Howard Andrew G., 2017, ARXIV170404861
   Hubara I., 2016, ARXIV160907061
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li  F., 2016, ARXIV160504711
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang W., 2017, AAAI, P2625
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Zhou S., 2016, ARXIV160606160
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304005
DA 2019-06-15
ER

PT S
AU Frongillo, R
   Waggoner, B
AF Frongillo, Rafael
   Waggoner, Bo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bounded-Loss Private Prediction Markets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Prior work has investigated variations of prediction markets that preserve participants' (differential) privacy, which formed the basis of useful mechanisms for purchasing data for machine learning objectives. Such markets required potentially unlimited financial subsidy, however, making them impractical. In this work, we design an adaptively-growing prediction market with a bounded financial subsidy, while achieving privacy, incentives to produce accurate predictions, and precision in the sense that market prices are not heavily impacted by the added privacy-preserving noise. We briefly discuss how our mechanism can extend to the data-purchasing setting, and its relationship to traditional learning algorithms.
C1 [Waggoner, Bo] Microsoft Res, Redmond, WA 98052 USA.
RP Waggoner, B (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM raf@colorado.edu; bwag@colorado.edu
CR Abernethy J., 2012, P 25 COLT, P1
   Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12, DOI DOI 10.1145/2465769.2465777
   Abernethy J. D., 2011, ADV NEURAL INFORM PR, V24, P2600
   Abernethy Jacob, 2014, P 15 ACM C EC COMP, P395
   Abernethy Jacob D., 2014, P 15 ACM C EC COMP, P413, DOI [10.1145/2600057.2602900, DOI 10.1145/2600057.2602900]
   Banerjee A, 2005, IEEE T INFORM THEORY, V51, P2664, DOI 10.1109/TIT.2005.850145
   Cummings R, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P143, DOI 10.1145/2940716.2940721
   Frongillo R., 2012, ADV NEURAL INFORM PR, V25, P3275
   Frongillo Rafael, 2015, ADV NEURAL INFORM PR, P3016
   SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229
   Talagrand M, 2014, UPPER LOWER BOUNDS S, V60
   Waggoner Bo, 2015, ADV NEURAL INFORM PR, V28, P3492
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005005
DA 2019-06-15
ER

PT S
AU Fruit, R
   Pirotta, M
   Lazaric, A
AF Fruit, Ronan
   Pirotta, Matteo
   Lazaric, Alessandro
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Near Optimal Exploration-Exploitation in Non-Communicating Markov
   Decision Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with S-C communicating states, A actions and Gamma(C) <= S-C possible communicating next states, we derive a (O) over tilde (D-C root Gamma(C)S(C)AT) regret bound, where D-C is the diameter (i.e., the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with existing optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.
C1 [Fruit, Ronan; Pirotta, Matteo] Inria Lille, Sequel Team, Lille, France.
   [Lazaric, Alessandro] Facebook AI Res, Menlo Pk, CA USA.
RP Fruit, R (reprint author), Inria Lille, Sequel Team, Lille, France.
EM ronan.fruit@inria.fr; matteo.pirotta@inria.fr; lazaric@fb.com
FU French Ministry of Higher Education and Research; Nord-Pas-de-Calais
   Regional Council; French National Research Agency (ANR) under project
   ExTra-Learn [ANR-14-CE24-0010-01]
FX This research was supported in part by French Ministry of Higher
   Education and Research, Nord-Pas-de-Calais Regional Council and French
   National Research Agency (ANR) under project ExTra-Learn (n.
   ANR-14-CE24-0010-01).
CR Abbasi-Yadkori Y, 2015, P 3 ANN C ADV COGN S, P2
   Abbasi-Yadkori Yasin, 2015, UAI, P1
   Agrawal Shipra, 2017, NIPS, P1184
   Audibert JY, 2007, LECT NOTES ARTIF INT, V4754, P150
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Bartlett Peter L, 2009, P 25 C UNC ART INT U, P35
   Brockman G., 2016, CORR
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   Fruit Ronan, 2018, CORR
   Fruit Ronan, 2017, NIPS, P3169
   Gopalan Aditya, 2015, P 28 C LEARN THEOR C, V40, P861
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Maillard Odalric-Ambrym, 2013, P 30 INT C MACH LEAR, V28, P543
   Maurer Andreas, 2009, COLT
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moore Andrew William, 1990, TECHNICAL REPORT
   Osband I., 2013, ADV NEURAL INFORM PR, P3003
   Osband  Ian, 2017, P 34 INT C MACH LEAR, P2701
   Osband Ian, 2016, CORR
   Puterman M. L., 1994, MARKOV DECISION PROC
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton R.S., 2018, REINFORCEMENT LEARNI
   Talebi Mohammad Sadegh, 2018, ALT, V83, P770
   Theocharous Georgios, 2017, CORR
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Yi Ouyang, 2017, ADV NEURAL INFORM PR, P1333
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303003
DA 2019-06-15
ER

PT S
AU Fu, J
   Singh, A
   Ghosh, D
   Yang, L
   Levine, S
AF Fu, Justin
   Singh, Avi
   Ghosh, Dibya
   Yang, Larry
   Levine, Sergey
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Variational Inverse Control with Events: A General Framework for
   Data-Driven Reward Definition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.
C1 [Fu, Justin; Singh, Avi; Ghosh, Dibya; Yang, Larry; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Fu, J (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM justinfu@berkeley.edu; avisingh@berkeley.edu; dibyaghosh@berkeley.edu;
   larrywyang@berkeley.edu; svlevine@berkeley.edu
FU ONR Young Investigator Program award; National Science Foundation
   [IIS-1651843, IIS-1614653, IIS-1700696]; Berkeley DeepDrive
FX This research was supported by an ONR Young Investigator Program award,
   the National Science Foundation through IIS-1651843, IIS-1614653, and
   IIS-1700696, Berkeley DeepDrive, and donations from Google, Amazon, and
   NVIDIA.
CR Amodei Dario, 2016, ABS160606565 ARXIV
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Finn C., 2016, ICRA
   Finn Chelsea, 2016, ABS161103852
   Fu Justin, 2018, INT C LEARN REPR ICL
   Haarnoja Tuomas, 2017, INT C MACH LEARN ICM
   Hadfield-Menell Dylan, 2017, NIPS
   Ho Jonathan, 2016, ADV NEURAL INFORM PR
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Kappen Hilbert J, 2009, OPTIMAL CONTROL GRAP
   Levine Sergey, 2016, J MACHINE LEARNING J
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nachum Ofir, 2017, ADV NEURAL INFORM PR
   Ng Andrew, 2000, INT C MACH LEARN ICM
   O'Donoghue Brendan, 2016, COMBINING POLICY GRA
   Peng Xue Bin, 2017, ABS171006537 CORR
   Pinto L, 2016, IEEE INT CONF ROBOT, P3406, DOI 10.1109/ICRA.2016.7487517
   Rawlik Konrad, 2012, ROBOTICS SCI SYSTEMS
   Russell S J, 2003, ARTIFICIAL INTELLIGE
   Rusu Andrei A, 2017, C ROB LEARN CORL
   Schulman J., 2015, INT C MACH LEARN ICM
   Schulman John, 2017, EQUIVALENCE POLICY G
   Singh S, 2010, P INT S INSP BIOL S
   Sorg Jonathan, 2010, NIPS
   Todorov Emo, 2007, ADV NEURAL INFORM PR
   Todorov Emo, 2008, IEEE C DEC CONTR CDC
   Toussaint Marc, 2009, INT C MACH LEARN ICM
   Tung Hsiao-Yu Fish, 2018, C COMP VIS PATT REC
   Ziebart B. D., 2010, THESIS
   Ziebart Brian, 2008, AAAI C ART INT AAAI
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003013
DA 2019-06-15
ER

PT S
AU Fujii, K
   Soma, T
AF Fujii, Kaito
   Soma, Tasuku
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast greedy algorithms for dictionary selection with generalized
   sparsity constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OVERCOMPLETE DICTIONARIES
AB In dictionary selection, several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods, but it can also handle more complex sparsity constraints, such as average sparsity. Using numerical experiments, we show that our algorithm outperforms the known methods for dictionary selection, achieving competitive performances with dictionary learning algorithms in a smaller running time.
C1 [Fujii, Kaito; Soma, Tasuku] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.
RP Fujii, K (reprint author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.
EM kaito_fujii@mist.i.u-tokyo.ac.jp; tasuku_soma@mist.i.u-tokyo.ac.jp
FU JSPS KAKENHI [JP 18J12405]; ACT-I, JST; JST CREST, Japan [JPMJCR14D2]
FX The authors would thank Taihei Oki and Nobutaka Shimizu for their
   stimulating discussions. K.F. was supported by JSPS KAKENHI Grant Number
   JP 18J12405. T.S. was supported by ACT-I, JST. This work was supported
   by JST CREST, Grant Number JPMJCR14D2, Japan.
CR Agarwal A, 2016, SIAM J OPTIMIZ, V26, P2775, DOI 10.1137/140979861
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Arora S., 2014, P 27 C LEARN THEOR, P779
   Balkanski E., 2016, P 33 INT C MACH LEAR, P2207
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862
   Chen Lin, 2018, P 21 INT C ART INT S, P1896
   Cong Y, 2017, IEEE T IMAGE PROCESS, V26, P185, DOI 10.1109/TIP.2016.2619260
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Das A., 2011, P 28 INT C MACH LEAR, P1057
   Dumitrescu B., 2018, DICT LEARNING ALGORI
   Elenberg E., 2017, NIPS, V30, P4047
   Elenberg E. R., 2016, P NIPS WORKSH LEARN
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Everingham M., PASCAL VISUAL OBJECT
   Foucart S, 2013, MATH INTRO COMPRESSI
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Huang J., 2009, J MACHINE LEARNING R, V12, P3371
   Kale S., 2017, P 35 INT C MACH LEAR, P1
   Khanna R., 2017, ICML, P1837
   Krause A., 2010, P 27 INT C MACH LEAR, P567
   Liberty E., 2017, APPROXIMATION RANDOM, V81
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477
   Rusu C, 2014, IEEE SIGNAL PROC LET, V21, P6, DOI 10.1109/LSP.2013.2288788
   Stan S., 2017, P 34 INT C MACH LEAR, P3241
   Streeter M., 2009, P ADV NEUR INF PROC, P1577
   Zhou M., 2009, ADV NEURAL INFORM PR, V22, P2295
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304073
DA 2019-06-15
ER

PT S
AU Fusi, N
   Sheth, R
   Elibol, M
AF Fusi, Nicolo
   Sheth, Rishit
   Elibol, Melih
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Probabilistic Matrix Factorization for Automated Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data preprocessing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.
C1 [Fusi, Nicolo; Sheth, Rishit; Elibol, Melih] Microsoft Res, Cambridge, England.
   [Elibol, Melih] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.
RP Fusi, N (reprint author), Microsoft Res, Cambridge, England.
EM nfusi@microsoft.com; rishet@microsoft.com; elibol@cs.berkeley.edu
CR Bergstra J., 2013, P 30 INT C MACH LEAR, P115
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Feurer M., 2015, ADV NEURAL INFORM PR, V28, P2962
   Feurer M., 2015, P 29 AAAI C ART INT, P1128
   Grunewalder S., 2010, AISTATS, P273
   Guyon I., 2016, P MACHINE LEARNING R, V64, P21
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   Lawrence Neil, 2009, ICML
   Leite Rui, 2012, Machine Learning and Data Mining in Pattern Recognition. Proceedings 8th International Conference, MLDM 2012, P117, DOI 10.1007/978-3-642-31537-4_10
   Li L., 2016, ARXIV160306560
   Malitsky Yuri, 2014, 7 ANN S COMB SEARCH
   Misir M, 2017, ARTIF INTELL, V244, P291, DOI 10.1016/j.artint.2016.12.001
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Mockus J, 1975, OPT TECHN IFIP TECHN, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55]
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Perrone Valerio, 2017, ARXIV171202902
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7
   Schilling N, 2015, LECT NOTES ARTIF INT, V9285, P87, DOI 10.1007/978-3-319-23525-7_6
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Springenberg Jost Tobias, 2016, ADV NEURAL INFORM PR, V29, P4134, DOI DOI 10.1152/JN.00333.2004
   Srinivas N, 2009, ARXIV09123995
   Stern D, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P179
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Vanschoren J., 2013, ACM SIGKDD EXPLORATI, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]
   Wistuba Martin, 2015, P 2015 INT WORKSH ME, P15
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303035
DA 2019-06-15
ER

PT S
AU Gabrie, M
   Manoel, A
   Luneau, C
   Barbier, J
   Macris, N
   Krzakala, F
   Zdeborova, L
AF Gabrie, Marylou
   Manoel, Andre
   Luneau, Clement
   Barbier, Jean
   Macris, Nicolas
   Krzakala, Florent
   Zdeborova, Lenka
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Entropy and mutual information in models of deep neural networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.
C1 [Gabrie, Marylou; Barbier, Jean; Krzakala, Florent] PSL Univ, Lab Phys Stat, Ecole Normale Super, Paris, France.
   [Manoel, Andre] Univ Paris Saclay, CEA, INRIA, Parietal Team, St Aubin, France.
   [Manoel, Andre] Owkin Inc, New York, NY USA.
   [Manoel, Andre; Zdeborova, Lenka] Univ Paris Saclay, CNRS, Inst Phys Theor, CEA, St Aubin, France.
   [Luneau, Clement; Barbier, Jean; Macris, Nicolas] Ecole Polytech Fed Lausanne, Lab Theorie Commun, Lausanne, Switzerland.
   [Barbier, Jean] Abdus Salaam Int Ctr Theoret Phys, Trieste, Italy.
   [Krzakala, Florent; Zdeborova, Lenka] Duke Univ, Dept Math, Durham, NC 27706 USA.
   [Krzakala, Florent] Sorbonne Univ, Paris, France.
   [Krzakala, Florent] LightOn Inc, Paris, France.
RP Gabrie, M (reprint author), PSL Univ, Lab Phys Stat, Ecole Normale Super, Paris, France.
EM marylou.gabrie@ens.fr
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU ERC under the European Union's FP7 Grant [307087-SPARCS]; European
   Union's Horizon 2020 Research and Innovation Program [714608-SMiLe];
   French Agence Nationale de la Recherche [ANR-17-CE23-0023-01 PAIL];
   "Chaire de recherche sur les modeles et sciences des donnees", Fondation
   CFM pour la Recherche-ENS; Labex DigiCosme; Swiss National Science
   Foundation [200021E-175541]
FX The authors would like to thank Leon Bottou, Antoine Maillard, Marc
   Mezard, Leo Miolane, and Galen Reeves for insightful discussions. This
   work has been supported by the ERC under the European Union's FP7 Grant
   Agreement 307087-SPARCS and the European Union's Horizon 2020 Research
   and Innovation Program 714608-SMiLe, as well as by the French Agence
   Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL.
   Additional funding is acknowledged by MG from "Chaire de recherche sur
   les modeles et sciences des donnees", Fondation CFM pour la
   Recherche-ENS; by AM from Labex DigiCosme; and by CL from the Swiss
   National Science Foundation under grant 200021E-175541. We gratefully
   acknowledge the support of NVIDIA Corporation with the donation of the
   Titan Xp GPU used for this research.
CR Achille A, 2018, IEEE T PATTERN ANAL
   Achille A., 2017, ICML 2017 WORKSH PRI
   Advani M. S, 2017, ARXIV171003667
   Alemi A., 2017, INT C LEARN REPR ICL
   AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530
   Baldassi C, 2007, P NATL ACAD SCI USA, V104, P11079, DOI 10.1073/pnas.0700324104
   Barbier J., 2018, IEEE INT S INF THEOR
   Barbier J., 2017, 55 ANN ALL C COMM CO
   Barbier Jean, 2017, ARXIV170803395
   Barbier Jean, 2017, PROBABILITY THEORY R
   Belghazi M. I., 2018, INT C MACH LEARN ICM
   Chalk M., 2016, ADV NEURAL INFORM PR
   Chechik G, 2005, J MACH LEARN RES, V6, P165
   Dauphin Y., 2014, ADV NEURAL INFORM PR
   Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Engel A., 2001, STAT MECH LEARNING
   Fletcher A. K., 2017, ARXIV170606549
   GARDNER E, 1989, J PHYS A-MATH GEN, V22, P1983, DOI 10.1088/0305-4470/22/12/004
   Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221
   Kabashima Y, 2008, J PHYS C SER, V95
   Kolchinsky A., 2017, ARXIV170502436
   KOLCHINSKY A, 2017, ENTROPY-SWITZ, V19, DOI DOI 10.3390/E19070361
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Louart C., 2017, IEEE INT C AC SPEECH
   Manoel A, 2017, IEEE INT SYMP INFO, P2098, DOI 10.1109/ISIT.2017.8006899
   MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018
   Mezard M., 1987, SPIN GLASS THEORY
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Moczulski M., 2016, INT C LEARN REPR ICL
   Opper  M., 2001, ADV MEAN FIELD METHO
   Pennington J., 2017, ADV NEURAL INFORM PR
   Raghu M., 2017, INT C MACH LEARN ICM
   Rangan S., 2011, IEEE INT S INF THEOR
   Rangan S, 2017, IEEE INT SYMP INFO, P1588, DOI 10.1109/ISIT.2017.8006797
   Reeves G., 2017, 55 ANN ALL C COMM CO
   Saxe A., 2014, INT C LEARN REPR ICL
   Saxe A. M., 2018, INT C LEARN REPR ICL
   Schoenholz S. S., 2017, INT C LEARN REPR ICL
   SEUNG HS, 1992, PHYS REV A, V45, P6056, DOI 10.1103/PhysRevA.45.6056
   Shwartz-Ziv R., 2017, ARXIV170300810
   Tishby N., 1999, 37 ANN ALL C COMM CO
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Tulino AM, 2013, IEEE T INFORM THEORY, V59, P4243, DOI 10.1109/TIT.2013.2250578
   Yang Z., 2015, IEEE INT C COMP VIS
   Zdeborova L, 2016, ADV PHYS, V65, P453, DOI 10.1080/00018732.2016.1211393
   Zhao S., 2017, ARXIV170602262
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301078
DA 2019-06-15
ER

PT S
AU Gaillard, P
   Wintenberger, O
AF Gaillard, Pierre
   Wintenberger, Olivier
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient online algorithms for fast-rate regret bounds under sparsity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of online convex optimization in two different settings: arbitrary and i.i.d. sequence of convex loss functions. In both settings, we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. First, the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second, their rates are faster than the slow-rate (1)/root T under additional convexity assumptions on the loss functions. In the adversarial setting, we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the Lojasiewicz's assumption, we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate (1)/root T- for general convex risk to (1)/T for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk.
C1 [Gaillard, Pierre] PSL Res Univ, ENS, INRIA, Paris, France.
   [Wintenberger, Olivier] Sorbonne Univ, CNRS, LPSM, Paris, France.
RP Gaillard, P (reprint author), PSL Res Univ, ENS, INRIA, Paris, France.
EM pierre.gaillard@inria.fr; olivier.wintenberger@upmc.fr
CR Agarwal A., 2012, ADV NEURAL INFORM PR, P1538
   [Anonymous], 2014, INTRO HIGH DIMENSION
   Audibert J.-Y., 2008, ADV NEURAL INFORM PR, P41
   Bunea F, 2007, ELECTRON J STAT, V1, P169, DOI 10.1214/07-EJS008
   Catoni O., 1999, TECH REPOT, P510
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Duchi J. C., 2010, P 23 ANN C LEARN THE, P14
   Foster D. J., 2016, 29 ANN C LEARN THEOR, P960
   Gaillard P., 2017, 20 INT C ART INT STA
   Gerchinovitz S., 2011, THESIS
   Gerchinovitz S, 2013, J MACH LEARN RES, V14, P729
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Kale S., 2017, ARXIV170604690
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Koolen W. M., 2015, COLT, V40, P1155
   Koolen Wouter M., 2016, ADV NEURAL INFORM PR, V29, P4457
   Langford J, 2009, J MACH LEARN RES, V10, P777
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   LOJASIEWICZ S, 1993, ANN I FOURIER, V43, P1575
   Lojasiewicz S., 1963, EQUATIONS DERIVEES P, V117, P87, DOI DOI 10.1006/JDEQ.1997.3393
   Mehta N. A., 2017, ARTIF INTELL, P1085
   Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854
   Roulet V., 2017, ADV NEURAL INFORM PR, P1119
   Steinhardt J., 2014, ARXIV14124182
   Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
   Vovk V. G., 1990, P COMP LEARN THEOR 1
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Wintenberger O, 2017, MACH LEARN, V106, P119, DOI 10.1007/s10994-016-5592-6
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Yang YH, 2004, ECONOMET THEOR, V20, P176, DOI 10.1017/S0266466604201086
   Zhang Y., 2014, COLT, V35, P921
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001056
DA 2019-06-15
ER

PT S
AU Gallo, N
   Ihler, A
AF Gallo, Nicholas
   Ihler, Alexander
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Lifted Weighted Mini-Bucket
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many graphical models, such as Markov Logic Networks (MLNs) with evidence, possess highly symmetric substructures but no exact symmetries. Unfortunately, there are few principled methods that exploit these symmetric substructures to perform efficient approximate inference. In this paper, we present a lifted variant of the Weighted Mini-Bucket elimination algorithm which provides a principled way to (i) exploit the highly symmetric substructure of MLN models, and (ii) incorporate high-order inference terms which are necessary for high quality approximate inference. Our method has significant control over the accuracy-time trade-off of the approximation, allowing us to generate any-time approximations. Experimental results demonstrate the utility of this class of approximations, especially in models with strong repulsive potentials.
C1 [Gallo, Nicholas; Ihler, Alexander] Univ Calif Irvine, Irvine, CA 92637 USA.
RP Gallo, N (reprint author), Univ Calif Irvine, Irvine, CA 92637 USA.
EM ngallo1@uci.edu; ihler@ics.uci.edu
FU NSF [IIS-1526842, IIS-1254071]; United States Air Force
   [FA9453-16-C-0508]; DARPA [W911NF-18-C-0015]
FX This work is sponsored in part by NSF grants IIS-1526842, IIS-1254071,
   by the United States Air Force under Contract No. FA9453-16-C-0508, and
   DARPA Contract No. W911NF-18-C-0015.
CR Berkholz C, 2013, LECT NOTES COMPUT SC, V8125, P145, DOI 10.1007/978-3-642-40450-4_13
   Bui H. B., 2012, AAAI
   Bui H. H., 2012, ARXIV12074814
   Bui H. H., 2014, ARXIV14064200
   Dechter R, 1999, ARTIF INTELL, V113, P41, DOI 10.1016/S0004-3702(99)00059-4
   Dechter R., 1997, P 13 C UNC ART INT, P132
   Forouzan S., 2015, UAI, P268
   Gallo N., 2018, AAAI
   Liu Q., 2011, INT C MACH LEARN ICM, P849
   Mladenov M., 2014, P UAI, P603
   Mladenov M., 2015, UAI, P602
   Mladenov M., 2012, JMLR W CP, P788
   Mladenov M., 2014, ARTIF INTELL, P623
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Sen Prithviraj, 2009, P 25 C UNC ART INT, P496
   Singla P., 2008, P 23 AAAI C ART INT, P1094
   Singla P., 2014, AAAI, P2497
   Smith D., 2016, ARXIV160609637
   Taghipour N., 2012, JMLR, P1194
   V d Broeck G, 2014, ARXIV14120315
   Van den Broeck G., 2013, ADV NEURAL INFORM PR, P2868
   Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004084
DA 2019-06-15
ER

PT S
AU Gamarnik, D
   Zadik, I
AF Gamarnik, David
   Zadik, Ilias
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI High Dimensional Linear Regression using Lattice Basis Reduction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFORMATION; RECOVERY
AB We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector beta* from n noisy linear observations Y = X beta* + W is an element of R-n, for known X is an element of R-nxp and unknown W is an element of R-n. Unlike most of the literature on this model we make no sparsity assumption on beta*. Instead we adopt a regularization based on assuming that the underlying vectors beta* have rational entries with the same denominator Q is an element of Z(>0). We call this Q-rationality assumption. We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm. We establish that under the Q-rationality assumption, our algorithm recovers exactly the vector beta* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise, even when the learner has access to only one observation (n = 1). Furthermore, we prove that in the case of the Gaussian white noise for W, n = o (p/ log p) and Q sufficiently large, our algorithm tolerates a nearly optimal information-theoretic level of the noise.
C1 [Gamarnik, David] MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Zadik, Ilias] MIT, Operat Res Ctr, Cambridge, MA 02139 USA.
RP Gamarnik, D (reprint author), MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM gamarnik@mit.edu; izadik@mit.edu
CR Bora A., 2017, P 34 INT C MACH LEAR, V70, P537
   Borno M. A., 2011, REDUCTION SOLVING SO
   Brunel L., 1999, P 1999 IEEE INF THEO
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Cover T. M., 2006, WILEY SERIES TELECOM
   Donoho D. L., 2006, COUNTING FACES RANDO
   Donoho D, 2009, PHILOS T R SOC A, V367, P4273, DOI 10.1098/rsta.2009.0152
   Donoho DL, 2013, IEEE T INFORM THEORY, V59, P7434, DOI 10.1109/TIT.2013.2274513
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9452, DOI 10.1073/pnas.0502258102
   Erdos P., 1985, ACTA ARITH, V5, P524
   Foucart S, 2013, MATH INTRO COMPRESSI
   FRIEZE AM, 1986, SIAM J COMPUT, V15, P536, DOI 10.1137/0215038
   Gamarnik D., 2017, SPARSE HIGH DIMENSIO
   Gamarnik D., 2017, C LEARN THEOR COLT
   HARDY G. H., 1975, INTRO THEORY NUMBERS
   Hassibi A., 1998, IEEE T SIGNAL PROCES
   Hassibi B., 2002, 2002 IEEE INT C AC S
   Krzakala F, 2012, PHYS REV X, V2, DOI 10.1103/PhysRevX.2.021005
   LAGARIAS JC, 1985, J ACM, V32, P229, DOI 10.1145/2455.2461
   Lempel A., 1979, Computing Surveys, V11, P285
   LENSTRA AK, 1982, MATH ANN, V261, P515, DOI 10.1007/BF01457454
   MERKLE RC, 1978, IEEE T INFORM THEORY, V24, P525, DOI 10.1109/TIT.1978.1055927
   Shamir A., 1982, 23rd Annual Symposium on Foundations of Computer Science, P145, DOI 10.1109/SFCS.1982.5
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301080
DA 2019-06-15
ER

PT S
AU Ganea, OE
   Becigneul, G
   Hofmann, T
AF Ganea, Octavian-Eugen
   Becigneul, Gary
   Hofmann, Thomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hyperbolic Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID POINCARE BALL MODEL
AB Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, mostly because of the absence of corresponding hyperbolic neural network layers. This makes it hard to use hyperbolic embeddings in downstream tasks. Here, we bridge this gap in a principled manner by combining the formalism of Mobius gyrovector spaces with the Riemannian geometry of the Poincare model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks such as gated recurrent units. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.
C1 [Ganea, Octavian-Eugen; Becigneul, Gary; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Ganea, OE; Becigneul, G (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM octavian.ganea@inf.ethz.ch; gary.becigneul@inf.ethz.ch
FU Swiss National Science Foundation (SNSF) [167176]; Max Planck ETH Center
   for Learning Systems
FX This research is funded by the Swiss National Science Foundation (SNSF)
   under grant agreement number 167176. Gary Becigneul is also funded by
   the Max Planck ETH Center for Learning Systems.
CR Abadi M, 2016, TENSORFLOW SYSTEM LA
   Albert Ungar Abraham, 2008, ANAL HYPERBOLIC GEOM
   Bahdanau D., 2015, P INT C LEARN REPR I
   Birman GS, 2001, J MATH ANAL APPL, V254, P321, DOI 10.1006/jmaa.2000.7280
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Bowman S. R., 2015, P 2015 C EMP METH NA, P632
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Cannon J. W., 1997, FLAVORS GEOMETRY, V31, P59
   Desa C, 2018, ARXIV180403329
   Ganea Octavian-Eugen, 2018, P 35 INT C MACH LEAR
   Gromov M., 1987, MATH SCI RES I PUBL, V8, P75, DOI DOI 10.1007/978-1-4613-9586-7_3
   Hamann Matthias, 2017, MATH P CAMBRIDGE PHI, P1
   Hopper Christopher, 2010, RICCI FLOW RIEMANNIA
   Kim Y, 2014, P 2014 C EMP METH NA, P1746, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]
   Kingma D., 2015, P INT C LEARN REPR I
   Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106
   Lamping J., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P401
   Lebanon Guy, 2004, P 21 INT C MACH LEAR, P66
   Nickel M., 2011, P 28 INT C MACH LEAR, P809
   Nickel Maximillian, 2017, ADV NEURAL INFORM PR, V30, P6341
   Rocktaschel Tim, 2015, P INT C LEARN REPR I
   Spivak M, 1979, COMPREHENSIVE INTRO
   Tallec Corentin, 2018, P INT C LEARN REPR I
   Ungar AA, 2001, COMPUT MATH APPL, V41, P135, DOI 10.1016/S0898-1221(01)85012-4
   Ungar Abraham Albert, 2014, ANAL HYPERBOLIC GEOM
   Ungar Abraham Albert, 2008, SYNTHESIS LECT MATH, V1, P1
   Vendrov Ivan, 2016, P INT C LEARN REPR I
   Vermeer J, 2005, TOPOL APPL, V152, P226, DOI 10.1016/j.topol.2004.10.012
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305037
DA 2019-06-15
ER

PT S
AU Gao, H
   Shou, Z
   Zareian, A
   Zhang, HW
   Chang, SF
AF Gao, Hang
   Shou, Zheng
   Zareian, Alireza
   Zhang, Hanwang
   Chang, Shih-Fu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Low-shot Learning via Covariance-Preserving Adversarial Augmentation
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the "variability" of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.
C1 [Gao, Hang; Shou, Zheng; Zareian, Alireza; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA.
   [Zhang, Hanwang] Nanyang Technol Univ, Singapore, Singapore.
RP Gao, H (reprint author), Columbia Univ, New York, NY 10027 USA.
EM hg2469@columbia.edu; zs2262@columbia.edu; az2407@columbia.edu;
   hanwangzhang@ntu.edu.sg; sc250@columbia.edu
FU U.S. DARPA AIDA Program [FA8750-18-2-0014]
FX This work was supported by the U.S. DARPA AIDA Program No.
   FA8750-18-2-0014. The views and conclusions contained in this document
   are those of the authors and should not be interpreted as representing
   the official policies, either expressed or implied, of the U.S.
   Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Government purposes notwithstanding any
   copyright notation here on.
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Finn C, 2017, ARXIV170303400
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goldberger J., 2005, ADV NEURAL INFORM PR, P513
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2013, ARXIV13126211
   Gurumurthy Swaminathan, 2017, IEEE C COMP VIS PATT, V1
   Hariharan B., 2016, ARXIV160602819
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang  Xun, 2018, ARXIV180404732
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Koch G., 2015, ICML DEEP LEARN WORK, V2
   Lei D, 1996, J MANAGE, V22, P549, DOI 10.1016/S0149-2063(96)90024-0
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Mroueh  Youssef, 2017, ARXIV170208398
   Mroueh  Youssef, 2017, ADV NEURAL INFORM PR, P2510
   Odena A., 2016, ARXIV161009585
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Rezende D.J., 2016, ARXIV160305106
   Salakhutdinov R., 2012, JMLR P, P195
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Santoro A., 2016, ARXIV160506065
   Shmelkov K., 2017, ARXIV170806977
   Snell J., 2017, ADV NEURAL INFORM PR, P4080
   TOBIN J, 1958, ECONOMETRICA, V26, P24, DOI 10.2307/1907382
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Wang  Y.-X., 2018, ARXIV180105401
   Xian Y., 2017, ARXIV171200981
   Xu PL, 1998, GEOPHYS J INT, V135, P505, DOI 10.1046/j.1365-246X.1998.00652.x
   Yang  Y., 2017, ARXIV171106025, P11
   Zhu J.-Y., 2017, ADV NEURAL INFORM PR, V30, P465
   Zhu J Y, 2017, ARXIV170310593
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301001
DA 2019-06-15
ER

PT S
AU Gao, HY
   Wang, ZY
   Ji, SW
AF Gao, Hongyang
   Wang, Zhengyang
   Ji, Shuiwang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI ChannelNets: Compact and Efficient Convolutional Neural Networks via
   Channel-Wise Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.
C1 [Gao, Hongyang; Wang, Zhengyang; Ji, Shuiwang] Texas A&M Univ, College Stn, TX 77843 USA.
RP Gao, HY (reprint author), Texas A&M Univ, College Stn, TX 77843 USA.
EM hongyang.gao@tamu.edu; zhengyang.wang@tamu.edu; sji@tamu.edu
FU National Science Foundation [IIS-1633359, DBI-1641223]
FX This work was supported in part by National Science Foundation grants
   IIS-1633359 and DBI-1641223.
CR Chen  W., 2015, INT C MACH LEARN, P2285
   Chollet F., 2016, XCEPTION DEEP LEARNI
   Deng  J., 2009, P IEEE C COMP VIS PA
   Han  Song, 2015, INT C LEARN REPR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Howard Andrew G., 2017, ARXIV170404861
   Iandola F., 2016, ARXIV160207360
   Ioffe S., 2015, ARXIV150203167
   Jaderberg  M., 2014, P BRIT MACH VIS C
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebedev V., 2014, ARXIV14126553
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin M., 2013, ARXIV13124400
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Sandler M., 2018, ARXIV180104381
   See  A., 2016, P 20 SIGNLL C COMP N, P291
   Sifre L., 2014, THESIS
   Simonyan K., 2015, P INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Zhang X, 2017, ARXIV170701083
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305023
DA 2019-06-15
ER

PT S
AU Gao, R
   Xie, LY
   Xie, Y
   Xu, H
AF Gao, Rui
   Xie, Liyan
   Xie, Yao
   Xu, Huan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Robust Hypothesis Testing Using Wasserstein Uncertainty Sets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector.
C1 [Gao, Rui; Xie, Liyan; Xie, Yao; Xu, Huan] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
RP Gao, R (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
EM rgao32@gatech.edu; lxie49@gatech.edu; yao.xie@isye.gatech.edu;
   huan.xu@isye.gatech.edu
CR Arjovsky M, 2017, ARXIV170107875
   Ben-Tal  A., 2001, LECT MODERN CONVEX O, V2
   Blanchet J., 2016, ARXIV160401446
   Cao Yang, 2017, INT S INF THEOR ISIT
   Esfahani P. M., 2015, MATH PROGRAM, V24, P1
   Gao  R., 2016, ARXIV160402199
   Goldenshluger A, 2015, ELECTRON J STAT, V9, P1645, DOI 10.1214/15-EJS1054
   Gul G, 2017, IEEE T INFORM THEORY, V63, P5572, DOI 10.1109/TIT.2017.2693198
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Hellinger E, 1909, J REINE ANGEW MATH, V136, P210
   Huber P. J., 1981, ROBUST STAT
   HUBER PJ, 1965, ANN MATH STAT, V36, P1753, DOI 10.1214/aoms/1177699803
   Juditsky A, 2016, ELECTRON J STAT, V10, P2204, DOI 10.1214/16-EJS1170
   Kwapisz J. R., 2010, ACM SIGKDD EXPLORATI, V12, P74, DOI DOI 10.1145/1964897.1964918
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Levy B. C., 2008, PRINCIPLES SIGNAL DE
   Levy BC, 2009, IEEE T INFORM THEORY, V55, P413, DOI 10.1109/TIT.2008.2008128
   Lockhart J.W., 2011, P 5 INT WORKSH KNOWL, P25
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Montgomery DC, 2009, INTRO STAT QUALITY C
   Nemirovski A, 2006, SIAM J OPTIMIZ, V17, P969, DOI 10.1137/050622328
   Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37
   Ramdas A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19020047
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Shafieezadeh-Abadeh S, 2015, ADV NEURAL INFORM PR, P1576
   Sinha Aman, 2017, ARXIV171010571
   Topsoe F, 2000, IEEE T INFORM THEORY, V46, P1602, DOI 10.1109/18.850703
   Weiss G., 2012, AAAI WORKSH ACT CONT, P98
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002045
DA 2019-06-15
ER

PT S
AU Gao, YX
   Chen, L
   Li, BC
AF Gao, Yuanxiang
   Chen, Li
   Li, Baochun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Post: Device Placement with Cross-Entropy Minimization and Proximal
   Policy Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.
C1 [Gao, Yuanxiang; Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada.
   [Gao, Yuanxiang] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu, Sichuan, Peoples R China.
   [Chen, Li] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
RP Gao, YX (reprint author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada.; Gao, YX (reprint author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu, Sichuan, Peoples R China.
EM yuanxiang@ece.utoronto.ca; li.chen@louisiana.edu; bli@ece.toronto.edu
CR Bahdanau D., 2015, P INT C LEARN REPR I
   Boer P. D., 2005, ANN OPERATIONS RES
   Boyd S., 2004, CONVEX OPTIMIZATION
   Gao Y., 2018, P INT C MACH LEARN I
   Goschin S., 2013, P INT C MACH LEARN I
   He K., 2016, P IEEE COMP VIS PATT
   Heess N., 2017, ARXIV170702286
   Jozefowicz R., 2016, ARXIV160202410
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Mannor S., 2003, P INT C MACH LEARN I
   Mirhoseini A., 2018, P INT C LEARN REPR I
   Mirhoseini A., 2017, P INT C MACH LEARN I
   Papoulis A., 2002, PROBABILITY RANDOM V
   Rubinstein R., 2004, CROSS ENTROPY METHOD
   Shulman J., 2017, P INT C MACH LEARN I
   Shulman J., 2015, P INT C MACH LEARN I
   Sutskever  I., 2014, ADV NEURAL INFORM PR
   Sutton R., 2000, ADV NEURAL INFORM PR
   Szegedy C., 2016, P IEEE COMP VIS PATT
   Szita I., 2006, NEURAL COMPUTATION
   Wu Y., 2016, ARXIV160908144
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004052
DA 2019-06-15
ER

PT S
AU Gardner, JR
   Pleiss, G
   Bindel, D
   Weinberger, KQ
   Wilson, AG
AF Gardner, Jacob R.
   Pleiss, Geoff
   Bindel, David
   Weinberger, Kilian Q.
   Wilson, Andrew Gordon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU
   Acceleration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHM
AB Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n(3)) to O(n(2)). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.
C1 [Gardner, Jacob R.; Pleiss, Geoff; Bindel, David; Weinberger, Kilian Q.; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA.
RP Gardner, JR (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM jrg365@cornell.edu; geoff@cs.cornell.edu; bindel@cs.cornell.edu;
   kqw4@cornell.edu; andrew@cornell.edu
FU NSF [IIS-1563887]; Facebook Research; National Science Foundation
   [III-1618134, III-1526012, IIS-1149882, IIS-1724282, TRIPODS-1740822];
   Bill and Melinda Gates Foundation; Office of Naval Research; SAP America
   Inc.
FX JRG and AGW are supported by NSF IIS-1563887 and by Facebook Research.
   GP and KQW are supported in part by the III-1618134, III-1526012,
   IIS-1149882, IIS-1724282, and TRIPODS-1740822 grants from the National
   Science Foundation. In addition, they are supported by the Bill and
   Melinda Gates Foundation, the Office of Naval Research, and SAP America
   Inc.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Asuncion A., 2007, UCI MACHINE LEARNING
   Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349
   Bach F., 2013, COLT
   Bonilla E. V., 2008, NIPS
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chaudhari Pratik, 2016, ARXIV161101838
   Chen Tianqi, 2015, ARXIV151201274
   Cunningham J. P., 2008, ICML
   Cutajar K., 2016, ICML
   Datta B. N., 2010, NUMERICAL LINEAR ALG, V116
   Demmel J.W, 1997, APPL NUMERICAL LINEA, V56
   Dong K., 2017, NIPS
   Fitzsimons J. K., 2016, ARXIV160800117
   Gardner J. R., 2018, AISTATS
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Golub GH, 2010, PRINC SER APPL MATH, P1
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Harbrecht H, 2012, APPL NUMER MATH, V62, P428, DOI 10.1016/j.apnum.2011.10.001
   He K., 2016, CVPR
   Hensman J., 2015, ICML
   Hensman J., 2013, UAI
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Huang G, 2017, CVPR
   HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866
   Ioffe S., 2015, ICML
   Izmailov P., 2018, UNCERTAINTY ARTIFICI
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Keskar N. S., 2016, ARXIV160904836
   Krizhevsky A., 2012, NIPS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lanczos C, 1950, ITERATION METHOD SOL
   Matthews AGD, 2017, J MACH LEARN RES, V18, P1
   Murray I., 2009, ICML WORKSH NUM MATH
   OLEARY DP, 1980, LINEAR ALGEBRA APPL, V29, P293, DOI 10.1016/0024-3795(80)90247-5
   Paige C. C., 1970, BIT (Nordisk Tidskrift for Informationsbehandling), V10, P183, DOI 10.1007/BF01936866
   PARLETT BN, 1980, LINEAR ALGEBRA APPL, V29, P323, DOI 10.1016/0024-3795(80)90248-7
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pleiss G., 2018, ICML
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Saad  Y., 2003, ITERATIVE METHODS SP, V82
   Saatci Y., 2012, THESIS
   Snelson E., 2006, NIPS
   Titsias M, 2009, ARTIF INTELL, P567
   Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974
   Van der Vorst H. A., 2003, ITERATIVE KRYLOV MET, V13
   Wathen AJ, 2015, NUMER ALGORITHMS, V70, P709, DOI 10.1007/s11075-015-9970-0
   Williams C. K., 2006, GAUSSIAN PROCESSES M, V1
   Wilson A. G., 2015, ARXIV151101870
   Wilson A. G., 2016, AISTATS
   Wilson A. G., 2016, NIPS
   Wilson A. G., 2015, ICML
   Wilson Andrew Gordon, 2014, THESIS
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002015
DA 2019-06-15
ER

PT S
AU Garg, S
   Sharan, V
   Zhang, BH
   Valiant, G
AF Garg, Shivam
   Sharan, Vatsal
   Zhang, Brian Hu
   Valiant, Gregory
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Spectral View of Adversarially Robust Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features. Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints. We establish strong connections between adversarially robust features and a natural spectral property of the geometry of the dataset and metric of interest. This connection can be leveraged to provide both robust features, and a lower bound on the robustness of any function that has significant variance across the dataset. Finally, we provide empirical evidence that the adversarially robust features given by this spectral approach can be fruitfully leveraged to learn a robust (and accurate) model.
C1 [Garg, Shivam; Sharan, Vatsal; Zhang, Brian Hu; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA.
RP Garg, S (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM shivamgarg@stanford.edu; vsharan@stanford.edu; bhz@stanford.edu;
   gvaliant@stanford.edu
FU NSF [CCF-1704417, 1813049]; ONR Young Investigator Award
   [N00014-18-1-2295]
FX This work was supported by NSF awards CCF-1704417 and 1813049, and an
   ONR Young Investigator Award (N00014-18-1-2295).
CR Athalye A., 2018, ARXIV180200420
   Brown TB, 2017, CSCV171209665 ARXIV
   Bubeck Sebastien, 2018, ARXIV180510204
   Chung Fan R. K., 1997, SPECTRAL GRAPH THEOR
   Evtimov I., 2017, ARXIV170708945
   Fawzi Alhussein, 2018, ARXIV180208686
   Gilmer Justin, 2018, ARXIV180102774
   Goodfellow IJ, 2014, ARXIV14126572
   Hein Matthias, 2017, ADV NEURAL INFORM PR, P2263
   Kingma D. P., 2014, ARXIV14126980
   Kolter J. Z., 2017, ARXIV171100851
   Kurakin A., 2016, ARXIV160702533
   Madry Aleksander, 2017, ARXIV170606083
   Papernot  N., 2016, ARXIV160507277
   Peck Jonathan, 2017, ADV NEURAL INFORM PR, P804
   Raghunathan Aditi, 2018, ARXIV180109344
   Schmidt Ludwig, 2018, ARXIV180411285
   Sinha Aman, 2017, ARXIV171010571
   Spielman DA, 2007, ANN IEEE SYMP FOUND, P29, DOI 10.1109/FOCS.2007.56
   Sutskever I., 2017, ARXIV170707397
   Szegedy C, 2013, ARXIV13126199
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004067
DA 2019-06-15
ER

PT S
AU Garg, VK
   Dekel, O
   Xiao, L
AF Garg, Vikas K.
   Dekel, Ofer
   Xiao, Lin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning SMaLL Predictors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DNF; ALGORITHM; SELECTION; CLASSIFICATION; REGRESSION
AB We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We supplement the theoretical foundations of our work with an extensive empirical evaluation.
C1 [Garg, Vikas K.] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Dekel, Ofer; Xiao, Lin] Microsoft Res, Redmond, WA USA.
RP Garg, VK (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM vgarg@csail.mit.edu; oferd@microsoft.com; lin.xiao@microsoft.com
CR Aiolli F, 2005, J MACH LEARN RES, V6, P817
   Bertsimas D, 2007, OPER RES, V55, P252, DOI 10.1287/opre.1060.0360
   Blum A., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P253, DOI 10.1145/195058.195147
   Boyd S., 2004, CONVEX OPTIMIZATION
   BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5
   Bshouty N. H., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P286, DOI 10.1145/307400.307472
   Bshouty NH, 2005, J COMPUT SYST SCI, V71, P250, DOI 10.1016/j.jcss.2004.10.010
   Chen  W., 2015, INT C MACH LEARN, P2285
   Cord O., 2001, GENETIC FUZZY SYSTEM, V19
   Dekel O., 2007, P NEUR INF PROC SYST, P345
   Dekel O, 2008, SIAM J COMPUT, V37, P1342, DOI 10.1137/060666998
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Feldman V., 2012, C LEARN THEOR COLT
   Garg V. K., 2018, UAI
   Gupta C., 2017, INT C MACH LEARN, V70, P1331
   Han S, 2016, ICLR
   Hauser JR, 2010, J MARKETING RES, V47, P485, DOI 10.1509/jmkr.47.3.485
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Jackson JC, 1997, J COMPUT SYST SCI, V55, P414, DOI 10.1006/jcss.1997.1533
   Jalali A, 2017, SIAM J OPTIMIZ, V27, P2634, DOI 10.1137/16M1087424
   Juditsky A, 2012, OPTIMIZATION FOR MACHINE LEARNING, P149
   Khot S, 2008, ANN IEEE SYMP FOUND, P231, DOI 10.1109/FOCS.2008.37
   Klivans AR, 2004, J COMPUT SYST SCI, V68, P303, DOI 10.1016/j.jcss.2003.07.007
   Kumar Ashish, 2017, 34 INT C MACH LEARN, P1935
   Kusner M., 2014, P 31 INT C MACH LEAR, P622
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   MANSOUR Y, 1995, J COMPUT SYST SCI, V50, P543, DOI 10.1006/jcss.1995.1043
   Nakkiran P, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1473
   Nan F., 2016, ADV NEURAL INFORM PR, P2334
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748
   Pilanci M, 2015, MATH PROGRAM, V151, P63, DOI 10.1007/s10107-015-0894-1
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Sakai Y, 2000, THEOR COMPUT SYST, V33, P17, DOI 10.1007/s002249910002
   Servedio RA, 2004, INFORM COMPUT, V193, P57, DOI 10.1016/j.ic.2004.04.003
   Tan MK, 2014, J MACH LEARN RES, V15, P1371
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Verbeurgt K., 1998, P 9 C ALG LEARN THEO, P385
   Wang T, 2017, J MACH LEARN RES, V18, P1
   Zhong K., 2017, AISTATS, P1255
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003066
DA 2019-06-15
ER

PT S
AU Garg, VK
   Kalai, A
AF Garg, Vikas K.
   Kalai, Adam
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Supervising Unsupervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm, remove the outliers, and provably circumvent Kleinberg's impossibility result. Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.
C1 [Garg, Vikas K.] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Kalai, Adam] Microsoft Res, Redmond, WA USA.
RP Garg, VK (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM vgarg@csail.mit.edu; noreply@microsoft.com
CR Ackerman  Margareta, 2008, NIPS
   Balcan  Maria-Florina, 2015, WORKSH COMP LEARN TH
   Balcan  Maria-Florina, 2017, C LEARN THEOR, P213
   Bousmalis  Konstantinos, 2016, NIPS
   Chen  Wenlin, 2015, ICML
   Fusi  Nicolo, 2018, PROBABILISTIC MATRIX
   Ganin  Yaroslav, 2015, ICML
   Gupta C., 2017, INT C MACH LEARN, V70, P1331
   Han S, 2016, ICLR
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   KEARNS MJ, 1994, MACH LEARN, V17, P115
   Kingma D. P., 2014, NIPS
   Kleinberg J., 2003, ADV NEURAL INFORM PR, P463
   Long M., 2016, NIPS
   Luo J.-H., 2017, ICCV
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rohrbach  Marcus, 2013, NIPS
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Siddharth  N., 2017, NIPS
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Thornton C., 2013, P 19 ACM SIGKDD INT, P847, DOI [10.1145/2487575.2487629, DOI 10.1145/2487575.2487629]
   Thrun S, 1995, BIOL TECHNOLOGY INTE, P165
   Thrun S., 2012, LEARNING LEARN
   Tseng GC, 2007, BIOINFORMATICS, V23, P2247, DOI 10.1093/bioinformatics/btm320
   Zadeh R. B., 2009, P 25 C UNC ART INT, P639
   Zeiler M.D., 2012, ARXIV12125701
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305004
DA 2019-06-15
ER

PT S
AU Garipov, T
   Izmailov, P
   Podoprikhin, D
   Vetrov, D
   Wilson, AG
AF Garipov, Timur
   Izmailov, Pavel
   Podoprikhin, Dmitrii
   Vetrov, Dmitry
   Wilson, Andrew Gordon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.
C1 [Garipov, Timur] Samsung AI Ctr Moscow, Moscow, Russia.
   [Garipov, Timur] Skolkovo Inst Sci & Technol, Moscow, Russia.
   [Izmailov, Pavel; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA.
   [Podoprikhin, Dmitrii] Natl Res Univ, Higher Sch Econ, Samsung HSE Lab, Moscow, Russia.
   [Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia.
RP Garipov, T (reprint author), Samsung AI Ctr Moscow, Moscow, Russia.
FU Ministry of Education and Science of the Russian Federation
   [14.756.31.0001]; Samsung Research, Samsung Electronics; NSF
   [IIS-1563887]; Facebook Research
FX Timur Garipov was supported by Ministry of Education and Science of the
   Russian Federation (grant 14.756.31.0001). Timur Garipov and Dmitrii
   Podoprikhin were supported by Samsung Research, Samsung Electronics.
   Andrew Gordon Wilson and Pavel Izmailov were supported by Facebook
   Research and NSF IIS-1563887.
CR Auer P, 1996, ADV NEUR IN, V8, P316
   Choromanska A., 2015, ARTIF INTELL, P192
   Daniel Freeman  C, 2017, INT C LEARN REPR
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Dinh L., 2017, P MACHINE LEARNING R, P1019
   Draxler  Felix, 2018, P MACHINE LEARNING R, V80, P1309
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Gotmare  Akhilesh, 2018, ARXIV180606977
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Huang  Gao, 2017, INT C LEARN REPR
   Jonsson H., 1998, CLASSICAL QUANTUM DY
   Keskar N. S., 2017, INT C LEARN REPR
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Lee  Stefan, 2016, ADV NEURAL INFORM PR, P2119
   Li  Hao, 2017, ARXIV171209913
   Loshchilov I, 2017, INT C LEARN REPR
   Russakovsky  Olga, 2012, INT J COMPUT VISION, V115, P211
   Simonyan K, 2014, ARXIV14091556
   Smith L. N, 2017, ARXIV170204283
   Xie  Jingjing, 2013, ARXIV13062759
   Zagoruyko S., 2016, BMVC
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003035
DA 2019-06-15
ER

PT S
AU Ge, R
   Lee, H
   Risteski, A
AF Ge, Rong
   Lee, Holden
   Risteski, Andrej
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal
   Distributions using Simulated Tempering Langevin Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REVERSIBLE DIFFUSION-PROCESSES; METASTABILITY; ASYMPTOTICS
AB A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric distributions, such as latent-variable generative models. However sampling (even very approximately) can be #P-hard.
   Classical results (going back to [BE85]) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mixes in polynomial time. However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes. In this case, Langevin diffusion suffers from torpid mixing.
   We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge, this is the first result that proves fast mixing for multimodal distributions in this setting.
   For the analysis, we introduce novel techniques for proving spectral gaps based on decomposing the action of the generator of the diffusion. Previous approaches rely on decomposing the state space as a partition of sets, while our approach can be thought of as decomposing the stationary measure as a mixture of distributions (a "soft partition").
   Additional materials for the paper can be found at http : //tiny. cc/glr17. Note that the proof and results have been improved and generalized from the precursor at http: //www.arxiv. org/abs/1710 . 02736. See Section ?? for a comparison.
C1 [Ge, Rong] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.
   [Lee, Holden] Princeton Univ, Dept Math, Princeton, NJ 08544 USA.
   [Risteski, Andrej] MIT, Appl Math, Cambridge, MA 02139 USA.
   [Risteski, Andrej] IDSS, Cambridge, MA USA.
RP Ge, R (reprint author), Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.
EM rongge@cs.duke.edu; holdenl@princeton.edu; risteski@mit.edu
CR Bakry D., 1985, SPRINGER LECT NOTES, V1123, P177, DOI DOI 10.1007/BFB0075847
   Bakry D., 2013, ANAL GEOMETRY MARKOV, V348
   Bakry D, 2008, ELECTRON COMMUN PROB, V13, P60, DOI 10.1214/ECP.v13-1352
   BHATTACHARYA RN, 1978, ANN PROBAB, V6, P541, DOI 10.1214/aop/1176995476
   Bovier A, 2005, J EUR MATH SOC, V7, P69
   Bovier A, 2004, J EUR MATH SOC, V6, P399
   Bovier A, 2002, COMMUN MATH PHYS, V228, P219, DOI 10.1007/s002200200609
   Bubeck S, 2018, DISCRETE COMPUT GEOM, V59, P757, DOI 10.1007/s00454-018-9992-1
   Cheng X., 2017, ARXIV170703663
   Dalalyan A., 2017, P 2017 C LEARN THEOR, V65, P678
   Dalalyan A. S, 2017, ARXIV171000095
   Dalalyan Arnak S, 2016, J ROYAL STAT SOC B
   Del Moral P, 2012, FOUND TRENDS MACH LE, V3, P225, DOI 10.1561/2200000026
   Durmus  A., 2016, HIGH DIMENSIONAL BAY
   Durmus Alain, 2018, ARXIV180209188
   Dwivedi Raaz, 2018, P 2018 C LEARN THEOR
   Giraud F, 2017, BERNOULLI, V23, P670, DOI 10.3150/14-BEJ680
   Kingma D. P., 2013, ARXIV13126114
   Liang FM, 2005, PHYSICA A, V356, P468, DOI 10.1016/j.physa.2005.04.007
   LOVASZ L, 1993, RANDOM STRUCT ALGOR, V4, P359, DOI 10.1002/rsa.3240040402
   Madras N, 2002, ANN APPL PROBAB, V12, P581
   Mangoubi Oren, 2017, ARXIV170807114
   Mangoubi Oren, 2017, ARXIV171102621
   MARINARI E, 1992, EUROPHYS LETT, V19, P451, DOI 10.1209/0295-5075/19/6/002
   Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Park S, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.016703
   Paulin Daniel, 2015, ARXIV150908775
   Raginsky Maxim, 2017, C LEARN THEOR, P1674
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Schweizer Nikolaus, 2012, NONASYMPTOTIC ERROR
   Sontag D., 2011, NIPS, P1008
   Vempala Santosh, 2005, COMBINATORIAL COMPUT, V52, P2
   Woodard DB, 2009, ANN APPL PROBAB, V19, P617, DOI 10.1214/08-AAP555
   Zheng ZR, 2003, STOCH PROC APPL, V104, P131, DOI 10.1016/S0304-4149(02)00232-6
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002040
DA 2019-06-15
ER

PT S
AU Ge, YX
   Li, ZW
   Zhao, HY
   Yin, GJ
   Yi, S
   Wang, XG
   Li, HS
AF Ge, Yixiao
   Li, Zhuowan
   Zhao, Haiyu
   Yin, Guojun
   Yi, Shuai
   Wang, Xiaogang
   Li, Hongsheng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI FD-GAN: Pose-guided Feature Distilling GAN for Robust Person
   Re-identification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.(double dagger double dagger)
C1 [Ge, Yixiao; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.
   [Li, Zhuowan; Zhao, Haiyu; Yin, Guojun; Yi, Shuai] SenseTime Res, Hong Kong, Peoples R China.
   [Li, Zhuowan] Johns Hopkins Univ, Baltimore, MD 21218 USA.
   [Yin, Guojun] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
RP Li, HS (reprint author), Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.
EM yxge@link.cuhk.edu.hk; zli110@jhu.edu; zhaohaiyu@sensetime.com;
   gjyin@mail.ustc.edu.cn; yishuai@sensetime.com; xgwang@ee.cuhk.edu.hk;
   hsli@ee.cuhk.edu.hk
FU SenseTime Group Limited; General Research Fund - Research Grants Council
   of Hong Kong [CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015,
   CUHK14239816, CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217];
   Hong Kong Innovation and Technology Support Program [ITS/121/15FX]
FX This work is supported by SenseTime Group Limited, the General Research
   Fund sponsored by the Research Grants Council of Hong Kong (Nos.
   CUHK14213616, CUHK14206114, CUHK14205615, CUHK14203015, CUHK14239816,
   CUHK419412, CUHK14207814, CUHK14208417, CUHK14202217), the Hong Kong
   Innovation and Technology Support Program (No. ITS/121/15FX).
CR Bak S., 2017, CVPR
   Bousmalis Konstantinos, 2017, CVPR
   Chen D., 2018, CVPR
   CHEN DP, 2016, CVPR, P1268, DOI DOI 10.1109/CVPR.2016.142
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Cheng D., 2016, CVPR
   Chung D., 2017, ICCV
   Deng J., 2009, CVPR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   He K., 2016, CVPR
   Huang X., 2017, CVPR
   Isola  Phillip, 2017, CVPR
   Kaneko T., 2017, CVPR
   Li  Dangwei, 2017, CVPR
   Li H., 2016, CVPR
   Li J., 2017, CVPR
   Li  W., 2014, CVPR
   Li Wei, 2017, IJCAI
   Liao S., 2015, CVPR
   Liu Z., 2017, ICCV
   Ma L., 2017, ADV NEURAL INFORM PR, P405
   Mirza M., 2014, ARXIV14111784
   Nguyen V., 2017, ICCV
   Nowozin Sebastian, 2016, NIPS
   Radford  A., 2015, ARXIV151106434
   Ristani E., 2016, ECCV
   Salimans T., 2016, NIPS
   Shen Y., 2018, CVPR
   Shen Y., 2018, PERSON REIDENTIFICAT
   Shi Z., 2015, CVPR
   Siarohin A., 2017, CVPR
   Su  Chi, 2017, ICCV
   Sun Yifan, 2017, ICCV
   Tran Luan, 2017, CVPR
   Wei L., 2018, CVPR
   Wu A., 2017, ICCV
   Xiao  Tong, 2017, CVPR
   Yu H. X., 2017, ICCV
   Zanfir M., 2018, CVPR
   Zhang Han, 2017, ICCV
   Zhang  Li, 2016, CVPR
   Zhao H., 2017, CVPR
   Zhao  Liming, 2017, ARXIV170707256
   Zheng L., 2015, CVPR
   Zheng Z., 2018, TCSVT
   Zheng  Zhedong, 2017, ICCV
   Zhong  Zhun, 2017, CVPR
   Zhou J., 2017, CVPR
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301023
DA 2019-06-15
ER

PT S
AU Geffner, T
   Domke, J
AF Geffner, Tomas
   Domke, Justin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Using Large Ensembles of Control Variates for Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.
C1 [Geffner, Tomas; Domke, Justin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Geffner, T (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM tgeffner@cs.umass.edu; domke@cs.umass.edu
CR Agarwal A, 2012, IEEE T INFORM THEORY, V58, P3235, DOI 10.1109/TIT.2011.2182178
   AUEB M. T. R., 2015, ADV NEURAL INFORM PR, P2638
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Challis Edward, 2011, P 14 INT C ART INT S, P199
   Fabius Otto, 2014, ARXIV14126581
   Furmston Thomas, 2010, P AISTATS, P241
   Grathwohl Will, 2017, ARXIV171100123
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jang Eric, 2016, ARXIV161101144
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Jordan Michael I., EXPONENTIAL FAMILY C
   Kingma D. P., 2015, ADV NEURAL INFORM PR, P2575
   Kingma D. P., 2013, ARXIV13126114
   Li Steven Cheng-Xian, 2016, P 29 INT C NEUR INF, P1804
   Liang Percy, 2007, P 2007 JOINT C EMP M
   Maddison Chris J, 2016, ARXIV161100712
   Miller Andrew C, 2017, ARXIV170507880
   Mnih Andriy, 2014, ARXIV14020030
   Paisley John, 2012, ARXIV12066430
   Ranganath  R, 2015, JMLR WORKSHOP C P, P762
   Ranganath  R., 2014, AISTATS, P814
   Rezende D. J, 2014, ARXIV14014082
   Roeder Geoffrey, 2017, ARXIV170309194
   Ruiz Francisco, 2016, ADV NEURAL INFORM PR, P460
   Ruiz Francisco JR, 2016, ARXIV160301140
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Tucker G, 2017, ADV NEURAL INFORM PR, P2627
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang C, 2013, INT CONF SMART GRID, P181, DOI 10.1109/SmartGridComm.2013.6687954
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Winn J, 2005, J MACH LEARN RES, V6, P661
   Zhang Cheng, 2017, ARXIV171105597
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004051
DA 2019-06-15
ER

PT S
AU Geirhos, R
   Temme, CRM
   Rauber, J
   Schutt, HH
   Bethge, M
   Wichmann, FA
AF Geirhos, Robert
   Temme, Carlos R. Medina
   Rauber, Jonas
   Schuett, Heiko H.
   Bethge, Matthias
   Wichmann, Felix A.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Generalisation in humans and deep neural networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISCRIMINATION; INFORMATION; CONTRAST
AB We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.
C1 [Geirhos, Robert; Temme, Carlos R. Medina; Schuett, Heiko H.; Wichmann, Felix A.] Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany.
   [Geirhos, Robert; Rauber, Jonas; Bethge, Matthias; Wichmann, Felix A.] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.
   [Geirhos, Robert; Rauber, Jonas] Int Max Planck Res Sch Intelligent Syst, Tubingen, Germany.
   [Schuett, Heiko H.] Univ Tubingen, Grad Sch Neural & Behav Sci, Tubingen, Germany.
   [Schuett, Heiko H.] Univ Potsdam, Dept Psychol, Potsdam, Germany.
   [Bethge, Matthias; Wichmann, Felix A.] Bernstein Ctr Computat Neurosci Tubingen, Tubingen, Germany.
   [Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Wichmann, Felix A.] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
RP Geirhos, R (reprint author), Univ Tubingen, Neural Informat Proc Grp, Tubingen, Germany.; Geirhos, R (reprint author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.; Geirhos, R (reprint author), Int Max Planck Res Sch Intelligent Syst, Tubingen, Germany.
EM robert.geirhos@bethgelab.org
FU German Federal Ministry of Education and Research (BMBF) through the
   Bernstein Computational Neuroscience Program Tubingen [FKZ: 01GQ1002];
   German Research Foundation (DFG) [Sachbeihilfe Wi 2103/4-1, SFB 1233];
   International Max Planck Research School for Intelligent Systems
   (IMPRS-IS); Bosch Forschungsstiftung (Stifterverband) [T113/30057/17];
   Centre for Integrative Neuroscience Tubingen [EXC 307]; Intelligence
   Advanced Research Projects Activity (IARPA) via Department of
   Interior/Interior Business Center (DoI/IBC) [D16PC00003]
FX This work has been funded, in part, by the German Federal Ministry of
   Education and Research (BMBF) through the Bernstein Computational
   Neuroscience Program Tubingen (FKZ: 01GQ1002) as well as the German
   Research Foundation (DFG; Sachbeihilfe Wi 2103/4-1 and SFB 1233 on
   "Robust Vision"). The authors thank the International Max Planck
   Research School for Intelligent Systems (IMPRS-IS) for supporting R.G.
   and J.R.; J.R. acknowledges support by the Bosch Forschungsstiftung
   (Stifterverband, T113/30057/17); M.B. acknowledges support by the Centre
   for Integrative Neuroscience Tubingen (EXC 307) and by the Intelligence
   Advanced Research Projects Activity (IARPA) via Department of
   Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.
CR Abadi M., 2016, ARXIV160304467
   Berardino Alexander, 2017, ADV NEURAL INFORM PR, P3533
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BOX GEP, 1976, J AM STAT ASSOC, V71, P791, DOI 10.2307/2286841
   Brainard David H, 1996, HUMAN COLOR VISION C
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Carandini M, 1997, J NEUROSCI, V17, P8621
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Chen Z, 2016, LIFELONG MACHINE LEA
   Cichy RM, 2016, NEUROIMAGE
   Dekel Ron, 2017, ARXIV170104674
   Delorme A, 2000, VISION RES, V40, P2187, DOI 10.1016/S0042-6989(00)00083-3
   DERRINGTON AM, 1984, J PHYSIOL-LONDON, V357, P241, DOI 10.1113/jphysiol.1984.sp015499
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Dodge Samuel, 2017, ARXIV170502498
   Dodge Samuel, 2017, ARXI171004744
   Dodge Samuel F., 2016, 8 INT C QUAL MULT EX, V2016, P1, DOI DOI 10.1109/QOMEX.2016.7498955
   Donahue J., 2014, P INT C MACH LEARN, P647
   DOUGLAS RJ, 1991, TRENDS NEUROSCI, V14, P286, DOI 10.1016/0166-2236(91)90139-L
   Flachot A, 2018, J OPT SOC AM A, V35, pB334, DOI 10.1364/JOSAA.35.00B334
   Gal Y., 2016, INT C MACH LEARN, P1050
   Geirhos Robert, 2017, ARXIV170606969
   Geirhos Robert, 2018, ARXIV181112231
   Gerstner Wulfram, 2005, 23 PROBLEMS SYSTEMS, P135
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henning GB, 2002, J OPT SOC AM A, V19, P1259, DOI 10.1364/JOSAA.19.001259
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jozwik KM, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01726
   Karimi-Rouzbahani H, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13756-8
   Kawaguchi K., 2017, ARXIV171005468
   Kheradpisheh Saeed Reza, 2016, ARXIV150803929
   Kietzmann Tim C, 2017, BIORXIV
   Kleiner M, 2007, PERCEPTION, V36, P14
   Koenderink J, 2017, J VISION, V17, DOI 10.1167/17.2.7
   Kriegeskorte N, 2015, ANNU REV VIS SC, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896
   Kummerer M, 2016, ARXIV161001563
   Lake Brenden M, 2017, BEHAV BRAIN SCI, V40
   Lamme VAF, 1998, CURR OPIN NEUROBIOL, V8, P529, DOI 10.1016/S0959-4388(98)80042-1
   Lin Tsung-Yi, 2015, ECCV, P740
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   NACHMIAS J, 1974, VISION RES, V14, P1039, DOI 10.1016/0042-6989(74)90175-8
   Neyshabur B., 2017, ADV NEURAL INFORM PR, P5949
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Pelli DG, 1999, J OPT SOC AM A, V16, P647, DOI 10.1364/JOSAA.16.000647
   POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509
   Pramod RT, 2016, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2016.177
   R Core Team, 2016, R LANG ENV STAT COMP
   Ren M., 2016, ARXIV161104520
   Rosch Eleanor, 1999, CONCEPTS CORE READIN, V189
   Rosenfeld Amir, 2018, ARXIV180301485
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schutt HH, 2017, J VISION, V17, DOI 10.1167/17.12.12
   Shwartz-Ziv R., 2017, ARXIV170300810
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K., 2015, ARXIV14091556
   Sporns O, 2004, NEUROINFORMATICS, V2, P145, DOI 10.1385/NI:2:2:145
   Stockman A, 2000, VISION RES, V40, P1711, DOI 10.1016/S0042-6989(00)00021-3
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thrun S, 1996, ADV NEUR IN, V8, P640
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   vanderSchaaf A, 1996, VISION RES, V36, P2759, DOI 10.1016/0042-6989(96)00002-8
   Vasiljevic I., 2016, ARXIV161105760
   Wallis TSA, 2017, J VISION, V17, DOI 10.1167/17.12.5
   Wichmann FA, 2006, VISION RES, V46, P1520, DOI 10.1016/j.visres.2005.11.008
   Wichmann FA, 2010, J VISION, V10, DOI 10.1167/10.4.6
   Wichmann Felix A, 2017, ELECT IMAGING HUMAN, V2017, P36
   Wichmann Felix A, 1999, THESIS
   Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zhang C, 2016, ARXIV161103530
   Zhou YR, 2017, INT CONF ACOUST SPEE, P1213, DOI 10.1109/ICASSP.2017.7952349
NR 79
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002012
DA 2019-06-15
ER

PT S
AU George, T
   Laurent, C
   Bouthillier, X
   Ballas, N
   Vincent, P
AF George, Thomas
   Laurent, Cesar
   Bouthillier, Xavier
   Ballas, Nicolas
   Vincent, Pascal
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast Approximate Natural Gradient Descent in a Kronecker-factored
   Eigenbasis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.
C1 [George, Thomas; Laurent, Cesar; Bouthillier, Xavier; Vincent, Pascal] Univ Montreal, Mila, Montreal, PQ, Canada.
   [Ballas, Nicolas; Vincent, Pascal] Facebook AI Res, Menlo Pk, CA USA.
   [Vincent, Pascal] CIFAR, Toronto, ON, Canada.
RP George, T (reprint author), Univ Montreal, Mila, Montreal, PQ, Canada.
EM thomas.george@umontreal.ca; cesar.laurent@umontreal.ca;
   xavier.bouthillier@umontreal.ca; ballasn@fb.com; pascal@fb.com
FU Facebook; CIFAR; Calcul Quebec; Compute Canada
FX The experiments were conducted using PyTorch (Paszke et al. (2017)). The
   authors would like to thank Facebook, CIFAR, Calcul Quebec and Compute
   Canada, for research funding and computational resources.
CR Amari Shun-Ichi, 1998, NEURAL COMPUTATION
   Ba Jimmy, 2017, ICLR
   Becker S., 1988, P 1988 CONN MOD SUMM
   Bottou Leon, 2016, OPTIMIZATION METHODS
   Desjardins Guillaume, 2015, NIPS
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fujimoto Yuki, 2018, Artificial Intelligence and Soft Computing. 17th International Conference, ICAISC 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10841), P47, DOI 10.1007/978-3-319-91253-0_5
   Gehring Jonas, 2017, ICLR
   Goyal Priya, 2017, ARXIV170602677
   Grosse Roger, 2016, ICML
   He K., 2016, CVPR
   He K., 2015, ICCV
   Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ioffe S., 2015, ICML
   Kingma D. P., 2015, ICLR
   Laurent Cesar, 2018, ICLR WORKSH
   Le Roux Nicolas, 2008, NIPS
   Liu Dong C, 1989, MATH PROGRAMMING
   Martens James, 2015, ICML
   Ollivier Yann, 2015, INFORM INFERENCE J I
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Schraudolph Nicol N, 2001, INT C ART NEUR NETW
   Simonyan Karen, 2015, ICLR
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Zeiler M.D., 2012, ARXIV12125701
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004014
DA 2019-06-15
ER

PT S
AU Ghalebi, E
   Mirzasoleiman, B
   Grosu, R
   Leskovec, J
AF Ghalebi, Elahe
   Mirzasoleiman, Baharan
   Grosu, Radu
   Leskovec, Jure
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dynamic Network Model from Partial Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID STOCHASTIC BLOCKMODELS
AB Can evolving networks be inferred and modeled without directly observing their nodes and edges? In many applications, the edges of a dynamic network might not be observed, but one can observe the dynamics of stochastic cascading processes (e.g., information diffusion, virus propagation) occurring over the unobserved network. While there have been efforts to infer networks based on such data, providing a generative probabilistic model that is able to identify the underlying time-varying network remains an open question. Here we consider the problem of inferring generative dynamic network models based on network cascade diffusion data. We propose a novel framework for providing a non-parametric dynamic network model-based on a mixture of coupled hierarchical Dirichlet processes- based on data capturing cascade node infection times. Our approach allows us to infer the evolving community structure in networks and to obtain an explicit predictive distribution over the edges of the underlying network-including those that were not involved in transmission of any cascade, or are likely to appear in the future. We show the effectiveness of our approach using extensive experiments on synthetic as well as real-world networks.
C1 [Ghalebi, Elahe; Grosu, Radu] TU Wien, Vienna, Austria.
   [Mirzasoleiman, Baharan; Leskovec, Jure] Stanford Univ, Stanford, CA 94305 USA.
RP Ghalebi, E (reprint author), TU Wien, Vienna, Austria.
EM eghalebi@cps.tuwien.ac.at; baharanm@cs.stanford.edu;
   radu.grosu@tuwien.ac.at; jure@cs.stanford.edu
FU SNSF [P2EZP2_172187]
FX This research was partially supported by SNSF P2EZP2_172187.
CR Ahmed A, 2009, P NATL ACAD SCI USA, V106, P11878, DOI 10.1073/pnas.0901910106
   Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   Airoldi EM, 2009, 1 HARV U DEP STAT
   ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3
   Bourigault S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P573, DOI 10.1145/2835776.2835817
   Cai Diana, 2016, ADV NEURAL INFORM PR, P4249
   Crane H., 2016, ARXIV160304571
   Djolonga Josip, 2017, ARXIV170901006
   Fox Emily B, STICKY HDP HMM BAYES
   Gomez-Rodriguez M., 2013, P 6 ACM INT C WEB SE, P23, DOI DOI 10.1145/2433396.2433402
   Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741
   Gomez-Rodriguez Manuel, 2012, P 29 INT C MACH LEAR, P1587
   Gomez-rodriguez Manuel, 2011, P 28 INT C MACH LEAR
   Herlau T., 2016, ADV NEURAL INFORM PR, V29, P4260
   Hodas Nathan Oken, 2013, CORRABS13085015
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Hoover Douglas N, 1979, PREPRINT, V2
   Ishiguro K., 2010, P ANN C NEUR INF PRO, V23, P919
   Kemp C., 2006, AAAI, V3, P5
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Leskovec J., 2007, P 24 INT C MACH LEAR, P497, DOI DOI 10.1145/1273496.1273559
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Li C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P577, DOI 10.1145/3038912.3052643
   Lloyd J., 2012, P ADV NEUR INF PROC, V25, P998
   Lyons R, 2003, PUBL MATH IHES, P167
   Miller K.T., 2009, ADV NEURAL INFORM PR, V22, P1276
   Myers S.A., 2010, ADV NEURAL INFORM PR, P1741
   Namaki A, 2011, PHYSICA A, V390, P3835, DOI 10.1016/j.physa.2011.06.033
   Palla K, 2016, BAYESIAN NONPARAMETR
   Palla Konstantina, 2012, P 29 INT COF INT C M, P395
   Rodriguez M Gomez, 2010, P 16 ACM SIGKDD INT, P1019, DOI [DOI 10.1145/1835804.1835933, 10.1145/1835804.1835933]
   Rodriguez Manuel Gomez, 2012, ARXIV12051671
   Rodriguez Manuel Gomez, 2011, ARXIV11050697
   Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004
   Spielman DA, 2014, SIAM J MATRIX ANAL A, V35, P835, DOI 10.1137/090771430
   Wang Jia, 2017, ARXIV171110162
   Williamson RC, 2016, J MACH LEARN RES, V17, P1
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004042
DA 2019-06-15
ER

PT S
AU Ghassami, A
   Kiyavash, N
   Huang, BW
   Zhang, K
AF Ghassami, AmirEmad
   Kiyavash, Negar
   Huang, Biwei
   Zhang, Kun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multi-domain Causal Structure Learning in Linear Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE
AB We study the problem of causal structure learning in linear systems from observational data given in multiple domains, across which the causal coefficients and/or the distribution of the exogenous noises may vary. The main tool used in our approach is the principle that in a causally sufficient system, the causal modules, as well as their included parameters, change independently across domains. We first introduce our approach for finding causal direction in a system comprising two variables and propose efficient methods for identifying causal direction. Then we generalize our methods to causal structure learning in networks of variables. Most of previous work in structure learning from multi-domain data assume that certain types of invariance are held in causal modules across domains. Our approach unifies the idea in those works and generalizes to the case that there is no such invariance across the domains. Our proposed methods are generally capable of identifying causal direction from fewer than ten domains. When the invariance property holds, two domains are generally sufficient.
C1 [Ghassami, AmirEmad] Univ Illinois, Dept ECE, Urbana, IL 61801 USA.
   [Kiyavash, Negar] Georgia Inst Technol, Sch ISyE & ECE, Atlanta, GA 30332 USA.
   [Huang, Biwei; Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA.
RP Ghassami, A (reprint author), Univ Illinois, Dept ECE, Urbana, IL 61801 USA.
EM ghassam2@illinois.edu
FU Army grant [W911NF-15-1-0281]; NSF [NSF CCF 1065022]; United States Air
   Force [FA8650-17-C-7715]; National Science Foundation under EAGER Grant
   [IIS-1829681]; National Institutes of Health [NIH-1R01EB022858-01,
   FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02,
   FAIN-U54HG008540]; Department of Defense [FA8702-15-D-0002]
FX This work was supported in part by Army grant W911NF-15-1-0281 and NSF
   grant NSF CCF 1065022. This material is partially based upon work
   supported by United States Air Force under Contract No.
   FA8650-17-C-7715, by National Science Foundation under EAGER Grant No.
   IIS-1829681, and National Institutes of Health under Contract No.
   NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087,
   NIH-5U54HG008540-02, and FAIN-U54HG008540, and work funded and supported
   by the Department of Defense under Contract No. FA8702-15-D-0002 with
   Carnegie Mellon University for the operation of the Software Engineering
   Institute, a federally funded research and development center. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   views of the United States Air Force or the National Institutes of
   Health or the National Science Foundation. We thank Clark Glymour and
   Malcolm Forster for helpful discussions, and appreciate the comments
   from anonymous reviewers, which greatly helped to improve the paper.
CR Andersson SA, 1997, ANN STAT, V25, P505
   Bird CM, 2008, NAT REV NEUROSCI, V9, P182, DOI 10.1038/nrn2335
   Bollen K. A., 1989, WILEY SERIES PROBABI
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Daniusis  Povilas, 2010, P 26 C UNC ART INT U
   Eberhardt Frederick, 2007, THESIS
   Ghassami  AmirEmad, 2017, ADV NEURAL INFORM PR, P3015
   Gretton A., 2008, ADV NEURAL INFORM PR, V20, P585
   HOOVER KD, 1990, ECON PHILOS, V6, P207, DOI 10.1017/S026626710000122X
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   Huang  Biwei, 2017, P IEEE 17 INT C DAT
   Pearl J., 2009, CAUSALITY
   Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Pourahmadi M, 2011, STAT SCI, V26, P369, DOI 10.1214/11-STS358
   Reichenbach  Hans, 1991, DIRECTION TIME, V65
   Scholkopf  B., 2012, P 29 INT C MACH LEAR
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Shimizu S, 2011, J MACH LEARN RES, V12, P1225
   Spirtes P., 2000, CAUSATION PREDICTION
   Tian J, 2001, P 17 C UNC ART INT, P512
   Wang  Yuhao, 2018, ARXIV180205631
   Xu  Pan, 2016, ADV NEURAL INFORM PR, P1064
   Zhang  K., 2015, P 15 C THEOR ASP RAT
   Zhang  K., 2013, ICML
   Zhang  K., 2006, P 13 INT C NEUR INF
   Zhang  Kun, 2009, P 25 C UNC ART INT U
   Zhang  Kun, 2017, P INT JOINT C ART IN
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000074
DA 2019-06-15
ER

PT S
AU Ghiasi, G
   Lin, TY
   Le, QV
AF Ghiasi, Golnaz
   Lin, Tsung-Yi
   Le, Quoc V.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI DropBlock: A regularization method for convolutional networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78:13% accuracy, which is more than 1:6% improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from 36:8% to 38:4%
C1 [Ghiasi, Golnaz; Lin, Tsung-Yi; Le, Quoc V.] Google Brain, Mountain View, CA 94043 USA.
RP Ghiasi, G (reprint author), Google Brain, Mountain View, CA 94043 USA.
CR Cubuk Ekin D, 2018, CORR
   Deng J., 2009, CVPR
   DeVries Terrance, 2017, CORR
   Gal Y., 2016, ADV NEURAL INFORM PR, P1019
   Gastaldi Xavier, 2017, CORR
   Goodfellow I. J., 2013, INT C MACH LEARN
   Han D, 2017, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR.2017.668
   Hariharan B., 2011, ICCV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, CVPR
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe S., 2015, CORR
   Jie Hu, 2018, CVPR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Krueger David, 2016, CORR
   Larsson Gustav, 2017, INT C LEARN REPR
   Lin T.-Y., 2014, ECCV
   Lin T. Y., 2017, ICCV
   Lin T.-Y, 2017, CVPR
   Peng Chao, 2018, CVPR
   Real Esteban, 2018, CORR
   Shen Zhiqiang, 2017, ICCV
   Simonyan Karen, 2015, ADV NEURAL INFORM PR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2016, CVPR
   Szegedy C., 2015, CVPR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy  Christian, 2017, AAAI
   Tompson J., 2015, CVPR
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang Xiaolong, 2017, CVPR
   Wu Yuxin, 2018, ECCV
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yamada Y., 2018, CORR
   Zhou Bolei, 2018, CVPR
   Zoph Barret, 2017, CVPR
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005032
DA 2019-06-15
ER

PT S
AU Ghoshdastidar, D
   von Luxburg, U
AF Ghoshdastidar, Debarghya
   von Luxburg, Ulrike
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Practical Methods for Graph Two-Sample Testing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COMMUNITY DETECTION
AB Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question.
   In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods.
C1 [Ghoshdastidar, Debarghya] Univ Tubingen, Dept Comp Sci, Tubingen, Germany.
   [von Luxburg, Ulrike] Univ Tubingen, Dept Comp Sci, Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Ghoshdastidar, D (reprint author), Univ Tubingen, Dept Comp Sci, Tubingen, Germany.
EM ghoshdas@informatik.uni-tuebingen.de;
   luxburg@informatik.uni-tuebingen.de
FU German Research Foundation (Research Unit 1735); Institutional Strategy
   of the University of Tubingen (DFG) [ZUK 63]
FX This work is supported by the German Research Foundation (Research Unit
   1735) and the Institutional Strategy of the University of Tubingen (DFG,
   ZUK 63).
CR Anderson TW, 1984, INTRO MULTIVARIATE S
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   Arias-Castro E, 2014, ANN STAT, V42, P940, DOI 10.1214/14-AOS1208
   Bassett DS, 2008, J NEUROSCI, V28, P9239, DOI 10.1523/JNEUROSCI.1929-08.2008
   Berry AC, 1941, T AM MATH SOC, V49, P122, DOI 10.2307/1990053
   Bickel PJ, 2016, J R STAT SOC B, V78, P253, DOI 10.1111/rssb.12117
   Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Clarke R, 2008, NAT REV CANCER, V8, P37, DOI 10.1038/nrc2294
   Dua D., 2017, UCI MACHINE LEARNING
   Erdos L, 2012, ADV MATH, V229, P1435, DOI 10.1016/j.aim.2011.12.010
   Ghoshdastidar D., 2017, C LEARN THEOR COLT
   Ghoshdastidar D., 2017, ARXIV170700833
   Ginestet CE, 2017, ANN APPL STAT, V11, P725, DOI 10.1214/16-AOAS1015
   Ginestet CE, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00051
   Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hyduke DR, 2013, MOL BIOSYST, V9, P167, DOI 10.1039/c2mb25453k
   Kondor R., 2016, ADV NEURAL INFORM PR
   Landman BA, 2011, NEUROIMAGE, V54, P2854, DOI 10.1016/j.neuroimage.2010.11.047
   Lee JO, 2014, DUKE MATH J, V163, P117, DOI 10.1215/00127094-2414767
   Lei J, 2016, ANN STAT, V44, P401, DOI 10.1214/15-AOS1370
   Leskovec J., 2005, ACM SIGKDD INT C KNO
   Leskovec J., 2014, SNAP DATASETS STANFO
   Lovasz L., 2012, LARGE NETWORKS GRAPH
   Mukherjee S. S., 2017, ADV NEURAL INFORM PR
   Ng Andrew Y., 2002, ADV NEURAL INFORM PR
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Tang M., 2016, J COMPUTATIONAL GRAP, V26, P344
   Tang M, 2017, BERNOULLI, V23, P1599, DOI 10.3150/15-BEJ789
   Tracy CA, 1996, COMMUN MATH PHYS, V177, P727, DOI 10.1007/BF02099545
   Yang J., 2013, P 6 ACM INT C WEB SE, P587, DOI DOI 10.1145/2433396.2433471
   Zhang B, 2009, BIOINFORMATICS, V25, P526, DOI 10.1093/bioinformatics/btn660
NR 33
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303005
DA 2019-06-15
ER

PT S
AU Gillen, S
   Jung, C
   Kearns, M
   Roth, A
AF Gillen, Stephen
   Jung, Christopher
   Kearns, Michael
   Roth, Aaron
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Learning with an Unknown Fairness Metric
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability [?], which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who "knows unfairness when he sees it" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O (root T) regret bound to the best fair policy.
C1 [Gillen, Stephen; Jung, Christopher; Kearns, Michael; Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA.
RP Gillen, S (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
EM stepe@math.upenn.edu; chrjung@cis.upenn.edu; mkearns@cis.upenn.edu;
   aaroth@cis.upenn.edu
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Berk R., 2017, ARXIV170309207
   Celis L Elisa, 2018, ARXIV180208674
   Chouldechova Alexandra, 2017, ARXIV170300056
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Friedler Sorelle A, 2016, ARXIV160907236
   Gillen Stephen, 2018, ARXIV180206936
   Hajian S, 2013, IEEE T KNOWL DATA EN, V25, P1445, DOI 10.1109/TKDE.2012.72
   Hardt Moritz, 2016, ADV NEURAL INFORM PR
   Hebert-Johnson Ursula, 2017, ARXIV171108513
   Jabbari Shahin, 2017, INT C MACH LEARN, P1617
   Jain P., 2009, ADV NEURAL INFORM PR, P761
   Joseph M., 2016, CORR
   Joseph Matthew, 2016, FAIRNESS LEARNING CL, P325
   Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8
   Kearns M., 2017, ARXIV171105144
   Kim Michael P, 2018, ARXIV180303239
   Kleinberg Jon, 2017, P 2017 ACM C INN THE
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Liu Yang, 2017, ARXIV170701875
   Lobel Ilan, 2017, P 2017 ACM C EC COMP, P585, DOI [10.1145/3033274.3085100, DOI 10.1145/3033274.3085100]
   Rothblum Guy N, 2018, ARXIV180303242
   Zafar MB, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1171, DOI 10.1145/3038912.3052660
   Zemel R., 2013, JMLR P, P325
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302060
DA 2019-06-15
ER

PT S
AU Gillenwater, J
   Kulesza, A
   Mariet, Z
   Vassilvitskii, S
AF Gillenwater, Jennifer
   Kulesza, Alex
   Mariet, Zelda
   Vassilvitskii, Sergei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Maximizing Induced Cardinality Under a Determinantal Point Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Determinantal point processes (DPPs) are well-suited to recommender systems where the goal is to generate collections of diverse, high-quality items. In the existing literature this is usually formulated as finding the mode of the DPP (the so-called MAP set). However, the MAP objective inherently assumes that the DPP models "optimal" recommendation sets, and yet obtaining such a DPP is nontrivial when there is no ready source of example optimal sets. In this paper we advocate an alternative framework for applying DPPs to recommender systems. Our approach assumes that the DPP simply models user engagements with recommended items, which is more consistent with how DPPs for recommender systems are typically trained. With this assumption, we are able to formulate a metric that measures the expected number of items that a user will engage with. We formalize this optimization of this metric as the Maximum Induced Cardinality (MIC) problem. Although the MIC objective is not submodular, we show that it can be approximated by a submodular function, and that empirically it is well-optimized by a greedy algorithm.
C1 [Gillenwater, Jennifer; Kulesza, Alex; Vassilvitskii, Sergei] Google Res NYC, New York, NY 10011 USA.
   [Mariet, Zelda] MIT, Cambridge, MA 02139 USA.
RP Gillenwater, J (reprint author), Google Res NYC, New York, NY 10011 USA.
EM jengi@google.com; kulesza@google.com; zelda@csail.mit.edu;
   sergeiv@google.com
CR Chen L., 2017, LARG SCAL REC SYST W
   Dupuy C., 2018, C ART INT STAT AISTA
   Feige U., 2009, SIAM J COMPUTING SIC, V39
   Friedland S., 2013, LINEAR ALGEBRA ITS A, V438
   Gartrell M., 2017, AAAI C ART INT
   Gillenwater J., 2014, NEURAL INFORM PROCES
   Gillenwater J., 2012, NEURAL INFORM PROCES
   HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049
   Herlocker J., 2004, ACM T INFORM SYSTEMS, V22
   Hurley N, 2011, ACM T INTERNET TECHN, V10, DOI 10.1145/1944339.1944341
   Kathuria T., 2017, SAMPLING GREEDY MAP
   Kulesza A, 2011, C UNC ART INT UAI
   Kulesza A, 2012, FDN TRENDS MACHINE L, V5
   Mariet Z., 2015, INT C MACH LEARN ICM
   Nemhauser G., 1978, MATH PROGRAMMING, V14
   Nikolov A., 2016, S THEOR COMP STOC
   Smyth Barry, 2001, INT C CAS BAS REAS
   Suhubi E., 2003, FUNCTIONAL ANAL
   Swaminathan A., 2017, NEURAL INFORM PROCES
   Wilhelm M., 2018, C INF KNOWL MAN CIKM
   Zhang M., 2016, STAT SIGN PROC WORKS
   Ziegler C., 2005, INT C WORLD WID WEB
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001045
DA 2019-06-15
ER

PT S
AU Gimelfarb, M
   Sanner, S
   Lee, CG
AF Gimelfarb, Michael
   Sanner, Scott
   Lee, Chi-Guhn
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reinforcement Learning with Multiple Experts: A Bayesian Model
   Combination Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms.
C1 [Gimelfarb, Michael; Sanner, Scott; Lee, Chi-Guhn] Univ Toronto, Mech & Ind Engn, Toronto, ON, Canada.
RP Gimelfarb, M (reprint author), Univ Toronto, Mech & Ind Engn, Toronto, ON, Canada.
EM mike.gimelfarb@mail.utoronto.ca; ssanner@mie.utoronto.ca;
   cglee@mie.utoronto.ca
CR Asmuth J., 2009, P 25 C UNC ART INT U, P19
   Bertsekas D. P., 2004, STOCHASTIC OPTIMAL C
   Brockman G, 2016, ARXIV160601540
   Brys T, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3352
   Christiano P. F., 2017, ADV NEURAL INFORM PR, P4302
   Dearden R, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P761
   Devlin S., 2011, P 10 INT C AUT AG MU, V1, P225
   Devlin S., 2012, P 11 INT C AUT AG MU, V1, P433
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1
   Downey C., 2010, P 27 INT C MACH LEAR, P311
   Eck A., 2013, AAMAS, P1123
   Geva S., 1993, IEEE Control Systems Magazine, V13, P40, DOI 10.1109/37.236324
   Griffith S., 2013, ADV NEURAL INFORM PR, V26, P2625
   Grzes M., 2009, P AAMAS 2009 WORKSH, V115
   Grzes M, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P565
   Grzes M, 2010, NEURAL NETWORKS, V23, P541, DOI 10.1016/j.neunet.2010.01.001
   Harutyunyan A, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1913
   Hsu W.-S., 2016, NEURIPS, P4536
   Konidaris G., 2006, P 23 INT C MACH LEAR, P489
   Lantz B, 2013, MACHINE LEARNING R
   Li  Y., 2017, ARXIV170107274
   Maclin R., 2005, P 20 NAT C ART INT, V20, P819
   Marom O., 2018, AAAI
   Minka T. P., 2000, BAYESIAN MODEL AVERA, P1
   Mnih V., 2013, ARXIV13125602
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   O'Hagan A, 2004, WAG UR FRON, V3, P31
   Omar F., 2016, ONLINE BAYESIAN LEAR
   Philipp P, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P962
   Randlov J., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P463
   Suay HB, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P429
   Sutton R. S., 2018, REINFORCEMENT LEARNI, V1
   Taylor ME, 2007, J MACH LEARN RES, V8, P2125
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Tenorio-Gonzalez AC, 2010, LECT NOTES ARTIF INT, V6433, P483
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Wiewiora E, 2003, J ARTIF INTELL RES, V19, P205, DOI 10.1613/jair.1190
   Wiewiora E., 2003, P 20 INT C MACH LEAR, P792
NR 38
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004012
DA 2019-06-15
ER

PT S
AU Goel, V
   Weng, J
   Poupart, P
AF Goel, Vik
   Weng, Jameson
   Poupart, Pascal
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Video Object Segmentation for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information. Our code is available at https://github.com/vik-goel/MOREL.
C1 [Goel, Vik] Univ Waterloo, Cheriton Sch Comp Sci, Waterloo AI Inst, Toronto, ON, Canada.
   Vector Inst, Toronto, ON, Canada.
RP Goel, V (reprint author), Univ Waterloo, Cheriton Sch Comp Sci, Waterloo AI Inst, Toronto, ON, Canada.
EM v5goel@uwaterloo.ca; jj2weng@uwaterloo.ca; ppoupart@uwaterloo.ca
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Battaglia P., 2016, ADV NEURAL INFORM PR, P4502
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237
   Broekens  Joost, 2017, ARXIV170500470
   Diuk C., 2008, P 25 INT C MACH LEAR, P240, DOI DOI 10.1145/1390156.1390187
   Dubey  Rachit, 2018, INVESTIGATING HUMAN
   Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173
   Friedman N., 1997, P 13 C UNC ART INT, P175, DOI DOI 10.1016/J.CVIU.2007.08.003
   Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595
   Ha D., 2018, ARXIV180310122
   Higgins I., 2017, ARXIV170708475
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Iyer  Rahul, 2018, TRANSPARENCY EXPLANA
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Jaderberg  Max, 2017, ICLR
   Kansky  K., 2017, INT C MACH LEARN, V70, P1809
   Khan O. Z., 2009, ICAPS
   Kingma D. P., 2015, INT C LEARN REPR
   Lange S., 2010, 2010 INT JOINT C NEU, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li  Yuezhang, 2017, GLOB C ART INT, V50, P20
   Mahmoudieh  Parsa, 2017, SELF SUPERVISION REI
   Mirowski  Piotr, 2017, ICLR
   Mnih V., 2013, ARXIV13125602
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Racaniere Sebastien, 2017, ADV NEURAL INFORM PR, P5694
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Scholz  Jonathan, 2014, P 31 INT C MACH LEAR, P1089
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman  J., 2017, ARXIV170706347
   Simonyan K, 2013, ARXIV13126034
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Van Hasselt H., 2016, AAAI, P2094
   Vijayanarasimhan Sudheendra, 2017, ARXIV170407804
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
   Wu, 2017, ADV NEURAL INFORM PR, P5285
   Zhou T., 2017, CVPR, V2, P7
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000021
DA 2019-06-15
ER

PT S
AU Goetz, J
   Tewari, A
   Zimmerman, P
AF Goetz, Jack
   Tewari, Ambuj
   Zimmerman, Paul
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Active Learning for Non-Parametric Regression Using Purely Random Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.
C1 [Goetz, Jack; Tewari, Ambuj; Zimmerman, Paul] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Goetz, J (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM jrgoetz@umich.edu; tewaria@umich.edu; paulzim@umich.edu
FU NSF [DMS-1646108]; Sloan Research Fellowship
FX JG acknowledges the support of NSF via grant DMS-1646108. AT
   acknowledges the support of a Sloan Research Fellowship.
CR Attenberg J., 2011, ACM SIGKDD EXPLORATI, V12, P36, DOI DOI 10.1145/1964897.1964906
   Awasthi P., 2014, STOC, P449
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Breiman L., 2017, CLASSIFICATION REGRE
   Breiman L., 2000, 579 UCB STAT DEP
   Bull AD, 2013, ANN STAT, V41, P41, DOI 10.1214/12-AOS1064
   Chaudhuri  K., 2015, ADV NEURAL INFORM PR, P1090
   Chaudhuri  K., 2017, INT C MACH LEARN, P694
   Cramer C. J, 2013, ESSENTIALS COMPUTATI
   Dasgupta S., 2008, ADV NEURAL INFORM PR, P353
   Efromovich S, 2008, SCAND J STAT, V35, P266, DOI 10.1111/j.1467-9469.2007.00582.x
   Genuer R, 2012, J NONPARAMETR STAT, V24, P543, DOI 10.1080/10485252.2012.677843
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Gyorfi L, 2006, DISTRIBUTION FREE TH
   Hanneke S, 2015, J MACH LEARN RES, V16, P3487
   Hoang T. N., 2014, P ICML, P739
   Lakshminarayanan B., 2014, ADV NEURAL INFORM PR, P3140
   Liu H., 2017, STRUCT MULTIDISCIP O, P1
   Mourtada J., 2018, ARXIV180305784
   Mourtada J., 2017, ADV NEURAL INFORM PR, P3761
   Sabato S., 2014, ADV NEURAL INFORM PR, P469
   Sourati J, 2017, J MACH LEARN RES, V18
   Willett R., 2006, ADV NEURAL INF PROCE, P179
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302054
DA 2019-06-15
ER

PT S
AU Golan, I
   El-Yaniv, R
AF Golan, Izhak
   El-Yaniv, Ran
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Anomaly Detection Using Geometric Transformations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of anomaly detection in images, and present a new detection technique. Given a sample of images, all known to belong to a "normal" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects). The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images. We present extensive experiments using the proposed detector, which indicate that our technique consistently improves all known algorithms by a wide margin.
C1 [Golan, Izhak; El-Yaniv, Ran] Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel.
RP Golan, I (reprint author), Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel.
EM izikgo@cs.technion.ac.il; rani@cs.technion.ac.il
FU Israel Science Foundation [710/18]
FX This research was partially supported by the Israel Science Foundation
   (grant No. 710/18).
CR An J., 2015, TECH REP
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Burnaev E, 2015, PROC SPIE, V9875, DOI 10.1117/12.2228794
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   Deecke L., 2018, ANOMALY DETECTION GE
   El-Yaniv R., 2010, ABS10104466 CORR
   El-Yaniv R., 2011, ADV NEURAL INFORM PR, P1665
   El-Yaniv R., 2007, ADV NEURAL INF PROCE, P377
   El-Yaniv R, 2007, LECT NOTES COMPUT SC, V4539, P157, DOI 10.1007/978-3-540-72927-3_13
   Elson J., 2007, P 14 ACM C COMP COMM
   Geifman Y., 2017, P ADV NEURAL INF PRO, P4878
   Geifman Y., 2018, CORR
   Geifman Y., 2018, ABS181107579 CORR
   Geifman Y., 2017, CORR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Iwata T., 2016, ADV NEURAL INFORM PR, P1136
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT, P150, DOI [10.1007/0-387-22440-8_7, DOI 10.1007/978-1-4757-1904-8_7]
   Kim J, 2012, J MACH LEARN RES, V13, P2529
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2009, THESIS
   LeCun Y., 2010, MNIST HANDWRITTEN DI, V2
   Liang S., 2018, INT C LEARN REPR
   Minka Thomas, 2000, ESTIMATING DIRICHLET
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Radford  A., 2015, ARXIV151106434
   Ruff L., 2018, INT C MACH LEARN, P4390
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Scholkopf B, 2000, ADV NEUR IN, V12, P582
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Taylor A, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P130, DOI 10.1109/DSAA.2016.20
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Wang SQ, 2018, PATTERN RECOGN, V74, P198, DOI 10.1016/j.patcog.2017.09.012
   Wicker N, 2008, COMPUT STAT DATA AN, V52, P1315, DOI 10.1016/j.csda.2007.07.011
   Wiener Y., 2012, ADV NEURAL INFORM PR, P2051
   Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177
   Xiao  H., 2017, FASHION MNIST NOVEL
   Yosinski J., 2015, ARXIV150606579
   Zagoruyko S., 2016, BMVC
   Zhai S., 2016, P 33 INT C MACH LEAR, P1100
   Zimek Arthur, 2012, Statistical Analysis and Data Mining, V5, P363, DOI 10.1002/sam.11161
   Zong B., 2018, INT C LEARN REPR
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004033
DA 2019-06-15
ER

PT S
AU Gong, CY
   He, D
   Tan, X
   Qin, T
   Wang, LW
   Liu, TY
AF Gong, Chengyue
   He, Di
   Tan, Xu
   Qin, Tao
   Wang, Liwei
   Liu, Tie-Yan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI FRAGE: Frequency-Agnostic Word Representation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop FRequency-AGnostic word Embedding (FRAGE) which is a neat, simple yet effective way to learn word representation using adversarial training We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation, and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.
C1 [Gong, Chengyue] Peking Univ, Beijing, Peoples R China.
   [He, Di; Wang, Liwei] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China.
   [Tan, Xu; Qin, Tao; Liu, Tie-Yan] Microsoft Res Asia, Beijing, Peoples R China.
   [Wang, Liwei] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China.
RP Gong, CY (reprint author), Peking Univ, Beijing, Peoples R China.
EM cygong@pku.edu.cn; di_he@pku.edu.cn; xu.tan@microsoft.com;
   taoqin@microsoft.com; wanglw@cis.pku.edu.cn; tie-yan.liu@microsoft.com
FU National Basic Research Program of China (973 Program) [2015CB352502];
   NSFC [61573026]; BJNSF [L172037]; Microsoft Research Asia
FX This work is supported by National Basic Research Program of China (973
   Program) (grant no. 2015CB352502), NSFC (61573026) and BJNSF (L172037)
   and a grant from Microsoft Research Asia. We would like to thank the
   anonymous reviewers for their valuable comments on our paper.
CR Al-Rfou R., 2013, P 17 C COMP NAT LANG, P183
   Arjovsky M., 2017, ARXIV170107875
   Arora S., 2016, SIMPLE TOUGH TO BEAT
   Bahdanau D., 2014, ARXIV14090473
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   CONNEAU A., 2017, ARXIV171004087
   Dai Andrew M., 2015, ADV NEURAL INFORM PR, P3079
   Edunov Sergey, 2017, ARXIV171104956
   Ganin Y., 2015, INT C MACH LEARN, P1180
   Gehring J., 2017, ARXIV170503122
   Gehring J., 2016, ARXIV161102344
   Gong C., 2018, CORR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grave Edouard, 2016, CORR
   Hoffer E., 2018, ICLR
   Jozefowicz R., 2016, ARXIV160202410
   Kalchbrenner N., 2016, ARXIV161010099
   Kim Y, 2016, P 30 AAAI C ART INT, P2741
   Krause B., 2017, CORR
   Lai S., 2015, P 29 AAAI, P2267
   Lamb A. M., 2016, ADV NEURAL INFORM PR, P4601
   Lample G., 2017, ARXIV171100043
   Larson RR, 2010, J AM SOC INF SCI TEC, V61, P852, DOI 10.1002/asi.21234
   Maas A. L., 2011, P 49 ANN M ASS COMP, V1, P142
   Merity S., 2016, CORR
   Merity S., 2017, CORR
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Mu J., 2017, ARXIV170201417
   Mu J., 2017, CORR
   Ott M., 2018, ARXIV180300047
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Radford A., 2015, ARXIV151106434
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Sennrich  R., 2015, ARXIV150807909
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A., 2018, CORR
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Wang Y., DUAL TRANSFER LEARNI
   Wu Y., 2016, ARXIV160908144
   Yang Z., 2017, CORR
   Zhu J Y, 2017, ARXIV170310593
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301033
DA 2019-06-15
ER

PT S
AU Gonzalez-Garcia, A
   van de Weijer, J
   Bengio, Y
AF Gonzalez-Garcia, Abel
   van de Weijer, Joost
   Bengio, Yoshua
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Image-to-image translation for cross-domain disentanglement
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.
C1 [Gonzalez-Garcia, Abel] Comp Vis Ctr, Barcelona, Spain.
   [van de Weijer, Joost] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona, Spain.
   [Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
RP Gonzalez-Garcia, A (reprint author), Comp Vis Ctr, Barcelona, Spain.
EM agonzalez@cvc.uab.es
FU CHISTERA project M2CR [PCIN2015-251];  [TIN2016-79717-R]
FX We acknowledge the Spanish project TIN2016-79717-R and the CHISTERA
   project M2CR (PCIN2015-251).
CR Almahairi A., 2018, ICML
   Arjovsky  M., 2017, ICML
   Aubry M., 2014, CVPR
   Aytar Y., 2017, IEEE T PAMI
   Badrinarayanan V., 2017, IEEE T PAMI
   Barrow H., 1978, COMPUT VIS SYST
   Bengio Y., 2013, IEEE T PAMI
   Bousmalis Konstantinos, 2017, CVPR
   Bousmalis  Konstantinos, 2016, NIPS
   Chen Xi, 2016, NIPS
   Donahue J., 2017, ICLR
   Dumoulin V., 2017, ICLR
   Eigen D., 2015, ICCV
   Feutry C., 2018, ARXIV180209386
   Fidler S., 2012, NIPS
   Ganin  Yaroslav, 2015, ICML
   Goodfellow I., 2014, NIPS
   Gulrajani  Ishaan, 2017, NIPS
   Hinton G. E., 2011, ICANN
   Huang X., 2018, ECCV
   Iizuka S., 2016, ACM TOG
   Ioffe S., 2015, ICML
   Isola  Phillip, 2017, CVPR
   Ji X., 2017, ACM MULTIMEDIA
   Kingma D. P., 2014, NIPS
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lee H.-Y., 2018, ECCV
   Li C., 2016, ECCV
   Lin J., 2018, CVPR
   Liu Y.-C., 2018, CVPR
   Long  J., 2015, CVPR
   Ma L., 2018, CVPR
   Mathieu M., 2016, ICLR
   Mathieu M. F., 2016, NIPS
   Pang K., 2017, BMVC
   Radford A., 2015, ICLR
   Reed S., 2014, ICML
   Reed S. E., 2015, NIPS
   Rifai S., 2012, ECCV
   Ronneberger O., 2015, ICMICCAI
   Salimans T., 2016, NIPS
   Tappen M. F., 2003, NIPS
   Tenenbaum J. B., 1997, NIPS
   Tran Luan, 2017, CVPR
   Tylecek R., 2013, GCPR
   Wang Y., 2018, CVPR
   Zhang R., 2018, CVPR
   Zhang Richard, 2016, ECCV
   Zhu J, 2017, CVPR
   Zhu J.-Y., 2017, NIPS
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301029
DA 2019-06-15
ER

PT S
AU Gopakumar, S
   Gupta, S
   Rana, S
   Nguyen, V
   Venkatesh, S
AF Gopakumar, Shivapratap
   Gupta, Sunil
   Rana, Santu
   Vu Nguyen
   Venkatesh, Svetha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Algorithmic Assurance: An Active Approach to Algorithmic Testing using
   Bayesian Optimisation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EFFICIENT GLOBAL OPTIMIZATION; SEARCH
AB We introduce algorithmic assurance, the problem of testing whether machine learning algorithms are conforming to their intended design goal. We address this problem by proposing an efficient framework for algorithmic testing. To provide assurance, we need to efficiently discover scenarios where an algorithm decision deviates maximally from its intended gold standard. We mathematically formulate this task as an optimisation problem of an expensive, black-box function. We use an active learning approach based on Bayesian optimisation to solve this optimisation problem. We extend this framework to algorithms with vector-valued outputs by making appropriate modification in Bayesian optimisation via the EXP3 algorithm. We theoretically analyse our methods for convergence. Using two real-world applications, we demonstrate the efficiency of our methods. The significance of our problem formulation and initial solutions is that it will serve as the foundation in assuring humans about machines making complex decisions.
C1 [Gopakumar, Shivapratap; Gupta, Sunil; Rana, Santu; Vu Nguyen; Venkatesh, Svetha] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
RP Gupta, S (reprint author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM sunil.gupta@deakin.edu.au
RI Rana, Santu/R-2992-2019
OI Rana, Santu/0000-0003-2247-850X
FU Australian Government through the Australian Research Council (ARC); ARC
   Australian Laureate Fellowship [FL170100006]
FX This research was partially funded by the Australian Government through
   the Australian Research Council (ARC). Prof Venkatesh is the recipient
   of an ARC Australian Laureate Fellowship (FL170100006).
CR Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bull AD, 2011, J MACH LEARN RES, V12, P2879
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Hoffman M. W., 2011, UNCERTAINTY ARTIFICI, P327
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Nogueira J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1967, DOI 10.1109/IROS.2016.7759310
   Rana Santu, 2017, P 34 INT C MACH LEAR, V70, P2883
   Rasmussen C., 1999, P ADV NEURAL INFORM, P554
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seldin Y., 2012, EWRL, P103
   Snoek J., 2015, P 32 INT C MACH LEAR, P2171
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Thanh Dai Nguyen, 2017, Advances in Knowledge Discovery and Data Mining. 21st Pacific-Asia Conference, PAKDD 2017. Proceedings: LNAI 10235, P578, DOI 10.1007/978-3-319-57529-2_45
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000001
DA 2019-06-15
ER

PT S
AU Gottlieb, LA
   Kaufman, E
   Kontorovich, A
   Nivasch, G
AF Gottlieb, Lee-Ad
   Kaufman, Eran
   Kontorovich, Aryeh
   Nivasch, Gabriel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning convex polytopes with margin
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INTERSECTIONS; COMPLEXITY; APPROXIMABILITY; HALFSPACES; HARDNESS
AB We present an improved algorithm for properly learning convex polytopes in the realizable PAC setting from data with a margin. Our learning algorithm constructs a consistent polytope as an intersection of about t log t halfspaces with margins in time polynomial in t (where t is the number of halfspaces forming an optimal polytope).
   We also identify distinct generalizations of the notion of margin from hyperplanes to polytopes and investigate how they relate geometrically; this result may be of interest beyond the learning setting.
C1 [Gottlieb, Lee-Ad; Kaufman, Eran; Nivasch, Gabriel] Ariel Univ, Ariel, Israel.
   [Kontorovich, Aryeh] Ben Gurion Univ Negev, Beer Sheva, Israel.
RP Gottlieb, LA (reprint author), Ariel Univ, Ariel, Israel.
EM leead@ariel.ac.il; erankfmn@gmail.com; karyeh@bgu.sc.il;
   gabrieln@ariel.ac.il
FU Israel Science Foundation [755/15]
FX We thank Sasho Nikolov, Bernd Gartner and David Eppstein for helpful
   discussions. L. Gottlieb and A. Kontorovich were supported in part by
   the Israel Science Foundation (grant No. 755/15).
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   AMALDI E, 1995, THEOR COMPUT SCI, V147, P181, DOI 10.1016/0304-3975(94)00254-G
   Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1
   Anderson Joseph, 2013, COLT 2013, P1020
   Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746
   Anthony Martin, 1999, NEURAL NETWORK LEARN, DOI [10.1017/CBO9780511624216, DOI 10.1017/CBO9780511624216]
   Arriaga RI, 2006, MACH LEARN, V63, P161, DOI 10.1007/s10994-006-6265-7
   Barasz Mihaly, 2010, INNOVATIONS IN COMPU, P42
   Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43
   Ben-David S, 2003, J COMPUT SYST SCI, V66, P496, DOI 10.1016/S0022-0000(03)00038-2
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Chvatal Vasek, NOTES KHACHIYAN KALA
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Goel Surbhi, 2018, ARXIV170906010V4
   Hanneke Steve, 2017, OPTIMALITY SVM NOVEL
   Hegedus T., 1994, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94, P228, DOI 10.1145/180139.181124
   Hellerstein L, 2007, THEOR COMPUT SCI, V384, P66, DOI 10.1016/j.tcs.2007.05.018
   HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011
   Jain S, 2003, J COMPUT SYST SCI, V67, P546, DOI 10.1016/S0022-0000(03)00067-9
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kane Daniel M., 2013, COLT, P522
   Kantchelian A., 2014, NIPS, P3248
   Kearns Micheal, 1997, INTRO COMPUTATIONAL
   Khot S, 2011, J COMPUT SYST SCI, V77, P129, DOI 10.1016/j.jcss.2010.06.010
   Klivans AR, 2008, J COMPUT SYST SCI, V74, P35, DOI 10.1016/j.jcss.2007.04.012
   Klivans AR, 2009, J COMPUT SYST SCI, V75, P2, DOI 10.1016/j.jcss.2008.07.008
   Kwek S, 1998, ALGORITHMICA, V22, P53, DOI 10.1007/PL00013834
   LONG PM, 1994, INFORM COMPUT, V113, P230, DOI 10.1006/inco.1994.1071
   MATOUSEK J, 2002, GRAD TEXT M, V212, pR5
   MEGIDDO N, 1988, DISCRETE COMPUT GEOM, V3, P325, DOI 10.1007/BF02187916
   Nikolov Aleksandar, 2018, THEORETICAL COMPUTER
   Rademacher Luis, 2009, COLT 2009
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   VERSHYNIN R., 2010, CORR
   Zuckerman D., 2007, THEORY COMPUTING, V3, P103, DOI [10.4086/toc.2007.v003a006, DOI 10.4086/T0C.2007.V003A006]
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000023
DA 2019-06-15
ER

PT S
AU Gower, RM
   Hanzely, F
   Richtarik, P
   Stich, SU
AF Gower, Robert M.
   Hanzely, Filip
   Richtarik, Peter
   Stich, Sebastian U.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Accelerated Stochastic Matrix Inversion: General Theory and Speeding up
   BFGS Rules for Faster Second-Order Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID QUASI-NEWTON METHODS; EFFICIENCY; ALGORITHM
AB We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning. As an application of our general theory, we develop the first accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.
C1 [Gower, Robert M.] Telecom ParisTech, Paris, France.
   [Hanzely, Filip; Richtarik, Peter] KAUST, Thuwal, Saudi Arabia.
   [Stich, Sebastian U.] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Richtarik, Peter] Univ Edinburgh, Moscow Inst Phys & Technol, Edinburgh, Midlothian, Scotland.
RP Gower, RM (reprint author), Telecom ParisTech, Paris, France.
EM robert.gower@telecom-paristech.fr; filip.hanzely@kaust.edu.sa;
   peter.richtarik@kaust.edu.sa; sebastian.stich@epfl.ch
CR Agarwal N, 2017, J MACH LEARN RES, V18
   Berahas Albert S, 2016, ADV NEURAL INFORM PR, P1055
   Berahas Albert S, 2017, ABS170506211 CORR
   BROYDEN CG, 1967, MATH COMPUT, V21, P368, DOI 10.2307/2003239
   Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362
   Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Curtis  Frank, 2016, INT C MACH LEARN, P632
   DESOER CA, 1963, J SOC IND APPL MATH, V11, P442, DOI 10.1137/0111031
   FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317
   GOLDFARB D, 1970, MATH COMPUT, V24, P23, DOI 10.2307/2004873
   Gower R., 2016, JMLR WORKSHOP C P, P1869
   Gower RM, 2017, SIAM J MATRIX ANAL A, V38, P1380, DOI 10.1137/16M1062053
   Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487
   Gower Robert Mansel, 2015, ARXIV151206890
   Kaczmarz S., 1937, B ACAD POLON SCI L A, V35, P355
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu J, 2015, MATH COMPUT, V85, P153, DOI 10.1090/mcom/2971
   Loizou Nicolas, 2017, ARXIV171209677
   Mokhtari A, 2015, J MACH LEARN RES, V16, P3151
   Moritz P., 2016, ARTIF INTELL, P249
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182
   Pedersen Gert K, 1996, GRADUATE TEXTS MATH
   Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106
   Richtarik Peter, 2017, ARXIV170601108
   Richtarik  Peter, 2017, STOCHASTIC REF UNPUB
   Schraudolph N. N., 2007, P 11 INT C ART INT S, P436
   SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840
   Stich SU, 2016, MATH PROGRAM, V156, P549, DOI 10.1007/s10107-015-0908-z
   Stich Sebastian U, 2014, THESIS
   Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4
   Tu Stephen, 2017, P 34 INT C MACH LEAR, V70, P3482
   Wang X, 2017, SIAM J OPTIMIZ, V27, P927, DOI 10.1137/15M1053141
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Xu P., 2016, ADV NEURAL INFORM PR, V29, P3000
   Xu  Peng, 2017, ARXIV170807164
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301059
DA 2019-06-15
ER

PT S
AU Graber, C
   Meshi, O
   Schwing, A
AF Graber, Colin
   Meshi, Ofer
   Schwing, Alexander
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Structured Prediction with Nonlinear Output Transformations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep structured models are widely used for tasks like semantic segmentation, where explicit correlations between variables provide important prior information which generally helps to reduce the data needs of deep nets. However, current deep structured models are restricted by oftentimes very local neighborhood structure, which cannot be increased for computational complexity reasons, and by the fact that the output configuration, or a representation thereof, cannot be transformed further. Very recent approaches which address those issues include graphical model inference inside deep nets so as to permit subsequent non-linear output space transformations. However, optimization of those formulations is challenging and not well understood. Here, we develop a novel model which generalizes existing approaches, such as structured prediction energy networks, and discuss a formulation which maintains applicability of existing inference techniques.
C1 [Graber, Colin; Schwing, Alexander] Univ Illinois, Urbana, IL 61801 USA.
   [Meshi, Ofer] Google, Menlo Pk, CA USA.
RP Graber, C (reprint author), Univ Illinois, Urbana, IL 61801 USA.
EM cgraber2@illinois.edu; meshi@google.com; aschwing@illinois.edu
FU National Science Foundation [1718221]; Samsung; 3M; IBM-ILLINOIS Center
   for Cognitive Computing Systems Research (C3SR)
FX This material is based upon work supported in part by the National
   Science Foundation under Grant No. 1718221, Samsung, 3M, and the
   IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR). We
   thank NVIDIA for providing the GPUs used for this research.
CR Alvarez J., 2012, P ECCV
   Amos B., 2017, P ICML
   Babu B. R., 2009, CHARACTER RECOGNITIO
   Belanger D., 2016, P ICML
   Belanger D., 2017, P ICML
   Borenstein E., 2002, P ECCV
   Boykov Y., 2001, PAMI
   Chambolle A., 2011, J MATH IMAGING VISIO
   Chen L.-C., 2015, P ICLR
   Chen Liang Chieh, 2015, P ICML
   Ciliberto Carlo, 2018, ARXIV180602402
   Deng J., 2009, P CVPR
   Finley T., 2008, P ICML
   Globerson A., 2006, P NIPS
   Gygli M., 2017, P ICML
   Hazan T., 2016, JMLR
   Hazan T., 2010, P NIPS
   Hazan T., 2010, T INFORM THEORY
   He K., 2016, P CVPR
   Huiskes M. J., 2008, P ACM INT C MULT INF
   Jegelka S., 2011, P NIPS
   Kalchbrenner N., 2016, P ICML
   Kappes J. H., 2015, IJCV
   Komodakis N., 2011, P CVPR
   Komodakis N., 2009, P CVPR
   Krizhevsky A., 2012, P NIPS
   Kulesza A., 2008, P NIPS
   Lafferty J., 2001, P ICML
   Leblond R., 2018, INT C LEARN REPR
   Lin G., 2015, P NIPS
   McCormick S. T., 2008, SUBMODULAR FUNCTION, P321
   Meltzer T., 2009, P UAI
   Meshi O., 2016, P ICML
   Meshi O., 2015, P NIPS
   Meshi O., 2017, P NIPS
   Meshi O., 2010, P ICML
   Nam J., 2017, P NIPS
   Nguyen K., 2017, WACV
   Pearl J., 1982, P AAAI
   Pletscher P., 2010, P ECML PKDD
   Schrijver A., 2004, COMBINATORIAL OPTIMI
   Schwing A. G., 2014, P ICML
   Schwing A.G., 2015, FULLY CONNECTED DEEP
   Schwing A. G., 2012, P NIPS
   Schwing A. G., 2011, P CVPR
   Shelhamer E., 2016, PAMI
   Shimony S. E., 1994, ARTIFICIAL INTELLIGE
   Song Y., 2016, P ICML
   Sontag D., 2008, P NIPS
   Stobbe P., 2010, P NIPS
   Sutskever I., 2014, P NIPS
   Taskar B., 2003, P NIPS
   Tompson J., 2014, P NIPS
   Tsochantaridis I., 2005, JMLR
   Tu L., 2018, P ICLR
   Wainwright M, 2008, GRAPHICAL MODELS EXP
   Wainwright M. J., 2003, P C CONTR COMM COMP
   Werner T., 2007, PAMI
   Zheng S., 2015, P ICCV
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000079
DA 2019-06-15
ER

PT S
AU Greenberg, CS
   Monath, N
   Kobren, A
   Flaherty, P
   McGregor, A
   McCallum, A
AF Greenberg, Craig S.
   Monath, Nicholas
   Kobren, Ari
   Flaherty, Patrick
   McGregor, Andrew
   McCallum, Andrew
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Compact Representation of Uncertainty in Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB For many classic structured prediction problems, probability distributions over the dependent variables can be efficiently computed using widely-known algorithms and data structures (such as forward-backward, and its corresponding trellis for exact probability distributions in Markov models). However, we know of no previous work studying efficient representations of exact distributions over clusterings. This paper presents definitions and proofs for a dynamic-programming inference procedure that computes the partition function, the marginal probability of a cluster, and the MAP clustering-all exactly. Rather than the Nth Bell number, these exact solutions take time and space proportional to the substantially smaller powerset of N. Indeed, we improve upon the time complexity of the algorithm introduced by Kohonen and Corander [11] for this problem by a factor of N. While still large, this previously unknown result is intellectually interesting in its own right, makes feasible exact inference for important real-world small data applications (such as medicine), and provides a natural stepping stone towards sparse-trellis approximations that enable further scalability (which we also explore). In experiments, we demonstrate the superiority of our approach over approximate methods in analyzing real-world gene expression data used in cancer treatment.
C1 [Greenberg, Craig S.; Monath, Nicholas; Kobren, Ari; McGregor, Andrew; McCallum, Andrew] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Greenberg, Craig S.] NIST, Gaithersburg, MD 20899 USA.
   [Flaherty, Patrick] Univ Massachusetts, Dept Math & Stat, Amherst, MA 01003 USA.
RP Greenberg, CS (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.; Greenberg, CS (reprint author), NIST, Gaithersburg, MD 20899 USA.
EM csgreenberg@cs.umass.edu; nmonath@cs.umass.edu; akobren@cs.umass.edu;
   flaherty@math.umass.edu; mcgregor@cs.umass.edu; mccallum@cs.umass.edu
FU Center for Intelligent Information Retrieval; DARPA [FA8750-13-2-0020];
   National Science Foundation Graduate Research Fellowship [NSF-1451512];
   National Science Foundation [1637536]
FX This work was supported in part by the Center for Intelligent
   Information Retrieval, in part by DARPA under agreement number
   FA8750-13-2-0020, in part by the National Science Foundation Graduate
   Research Fellowship under Grant No. NSF-1451512 and in part by the
   National Science Foundation Grant 1637536. Any opinions, findings and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect those of the sponsor.
CR Bansal Nikhil, 2004, MACHINE LEARNING
   Bell E. T., 1934, ANN MATH
   Blundell Charles, 2010, C UNC ART INT
   Cancer Genome Atlas Network, 2012, NATURE
   Dechter Rina, 1999, BUCKET ELIMINATION U
   Geman Stuart, 1984, IEEE T PATTERN ANAL
   Heller Katherine A, 2005, INT C MACH LEARN
   Hubert L., 2001, SIAM MONOG DISCR MAT
   Jensen Robert E., 1969, OPERATIONS RES
   Kappes Jorg Hendrik, 2015, INT C SCAL SPAC VAR
   Kohonen Jukka, 2016, COMMUNICATIONS STAT
   LeCun Yann, 2006, TUTORIAL ENERGY BASE
   Lehmann Brian D, 2014, J PATHOLOGY
   Lovasz L., 1993, COMBINATORIAL PROBLE
   Papandreou George, 2011, INT C COMP VIS
   Poole David, 1996, J ARTIFICIAL INTELLI
   Reddy D. R., 1977, SPEECH UNDERSTANDING
   Rouzier Roman, 2005, CLIN CANC RES
   Saddiki Hachem, 2014, BIOINFORMATICS
   Sorlie Therese, 2003, P NATL ACAD SCI
   Sorlie Therese, 2001, P NATL ACAD SCI
   Van Os BJ, 2004, J CLASSIFICATION
   Yersal Ozlem, 2014, WORLD J CLIN ONCOLOG
   Zanella Giacomo, 2016, ADV NEURAL INFORM PR
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003021
DA 2019-06-15
ER

PT S
AU Grill, JB
   Valko, M
   Munos, R
AF Grill, Jean-Bastien
   Valko, Michal
   Munos, Remi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Optimistic Optimization of a Brownian
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID APPROXIMATION
AB We address the problem of optimizing a Brownian motion. We consider a (random) realization W of a Brownian motion with input space in [0, 1]. Given W, our goal is to return an epsilon-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order log(2 )(1/epsilon). This improves over previous results of Al-Mharmah and Calvin (1996) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive-each query depends on previous values-and is an instance of the optimism-in-the-face-of-uncertainty principle.
C1 [Grill, Jean-Bastien] INRIA Lille, SequeL Team, Lille, France.
   [Valko, Michal] Nord Europe, Lille, France.
   [Munos, Remi] DeepMind Paris, Paris, France.
RP Grill, JB (reprint author), INRIA Lille, SequeL Team, Lille, France.
EM jbgrill@google.com; michal.valko@inria.fr; munos@google.com
FU European CHIST-ERA project DELTA; French Ministry of Higher Education
   and Research; Nord-Pas-de-Calais Regional Council; Inria;
   Otto-von-Guericke-Universitat Magdeburg associated-team north-European
   project Allocate; French National Research Agency project ExTra-Learn
   [ANR-14-CE24-0010-01]; French National Research Agency project BoB
   [ANR-16CE23-0003]; FMJH Program PGMO; Criteo, a doctoral grant of Ecole
   Normale Superieure
FX This research was supported by European CHIST-ERA project DELTA, French
   Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional
   Council, Inria and Otto-von-Guericke-Universitat Magdeburg
   associated-team north-European project Allocate, French National
   Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.
   ANR-16CE23-0003), FMJH Program PGMO with the support to this program
   from Criteo, a doctoral grant of Ecole Normale Superieure, and Maryse &
   Michel Grill.
CR AlMharmah H, 1996, J GLOBAL OPTIM, V8, P81
   Basu Kinjal, 2018, ARXIV170506808
   Calvin JM, 2017, J COMPLEXITY, V39, P17, DOI 10.1016/j.jco.2016.11.002
   Chapelle Olivier, 2011, NEURAL INFORM PROCES
   DENISOV IV, 1983, TEOR VEROYA PRIMEN, V28, P785
   DURRETT RT, 1977, ANN PROBAB, V5, P117, DOI 10.1214/aop/1176995895
   Hefter M, 2017, COMMUN MATH SCI, V15, P2121
   Munos Remi, 2011, NEURAL INFORM PROCES
   Russo Daniel, 2018, FDN TRENDS MACHINE L
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 10
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303004
DA 2019-06-15
ER

PT S
AU Grover, A
   Achim, T
   Ermon, S
AF Grover, Aditya
   Achim, Tudor
   Ermon, Stefano
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Streamlining Variational Inference for Constraint Satisfaction Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID RANDOM K-SAT; SURVEY PROPAGATION; ALGORITHM; THRESHOLD
AB Several algorithms for solving constraint satisfaction problems are based on survey propagation, a variational inference scheme used to obtain approximate marginal probability estimates for variable assignments. These marginals correspond to how frequently each variable is set to true among satisfying assignments, and are used to inform branching decisions during search; however, marginal estimates obtained via survey propagation are approximate and can be self-contradictory. We introduce a more general branching strategy based on streamlining constraints, which sidestep hard assignments to variables. We show that streamlined solvers consistently outperform decimation-based solvers on random k-SAT instances for several problem sizes, shrinking the gap between empirical performance and theoretical limits of satisfiability by 16.3% on average for k = 3, 4, 5, 6.
C1 [Grover, Aditya; Achim, Tudor; Ermon, Stefano] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA.
RP Grover, A (reprint author), Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA.
EM adityag@cs.stanford.edu; tachim@cs.stanford.edu; ermon@cs.stanford.edu
FU NSF [1651565, 1522054, 1733686]; Microsoft Research PhD Fellowship;
   Stanford Data Science Scholarship; FLI
FX This research was supported by NSF (#1651565, #1522054, #1733686) and
   FLI. AG is supported by a Microsoft Research PhD Fellowship and a
   Stanford Data Science Scholarship. We are grateful to Neal Jean for
   helpful comments on early drafts.
CR Achim  T., 2016, INT C MACH LEARN
   Achlioptas D, 2004, J AM MATH SOC, V17, P947, DOI 10.1090/S0894-0347-04-00464-3
   Achlioptas  D., 2015, UNCERTAINTY ARTIFICI
   Banko  M., 2007, INT JOINT C ART INT
   Biere A, 2009, FRONT ARTIF INTEL AP, V185, P457, DOI 10.3233/978-1-58603-929-5-457
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Braunstein A, 2005, RANDOM STRUCT ALGOR, V27, P201, DOI 10.1002/rsa.20057
   Chen  M., 2014, AM CONTR C
   Chen M, 2017, IEEE T AUTOMAT CONTR, V62, P1451, DOI 10.1109/TAC.2016.2577619
   Coja-Oghlan A, 2016, ADV MATH, V288, P985, DOI 10.1016/j.aim.2015.11.007
   Comes C, 2004, LECT NOTES COMPUT SC, V3258, P274
   Daude H, 2008, LECT NOTES COMPUT SC, V4957, P12, DOI 10.1007/978-3-540-78773-0_2
   Ding  J., 2015, S THEOR COMP
   Do MB, 2001, ARTIF INTELL, V132, P151, DOI 10.1016/S0004-3702(01)00128-X
   Ermon  S., 2014, INT C MACH LEARN
   Euzenat J., 2007, ONTOLOGY MATCHING, V333
   Gableske  O., 2013, PRAGMATICS SAT POS
   Gomes C. P., 2007, AAAI C ART INT
   Grover  A., 2016, ADV NEURAL INFORM PR
   Hromkovic  J., 2013, ALGORITHMICS HARD PR
   Kroc  L., 2009, S APPL COMP
   Maneva E, 2007, J ACM, V54, DOI 10.1145/1255443.1255445
   Marino R, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12996
   Mezard M, 2002, SCIENCE, V297, P812, DOI 10.1126/science.1073287
   Mitchell  D., 1992, AAAI C ART INT
   Molloy M, 2003, SIAM J COMPUT, V32, P935, DOI 10.1137/S0097539700368667
   Nuijten  W., 1994, THESIS
   Pittel B, 2016, COMB PROBAB COMPUT, V25, P236, DOI 10.1017/S0963548315000097
   Ren  H., 2018, INT JOINT C ART INT
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Selman  B., 1994, AAAI C ART INT
   Singla  P., 2006, INT C DAT MIN
   Stewart  R., 2017, AAAI C ART INT
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wei  W., 2004, AAAI C ART INT
   Zhao  S., 2016, AAAI C ART INT
   Zhou  Z., 2014, AM CONTR C
   Zhou  Z., 2017, ADV NEURAL INFORM PR
   Zhou ZY, 2018, AUTOMATICA, V89, P28, DOI 10.1016/j.automatica.2017.11.035
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005016
DA 2019-06-15
ER

PT S
AU Gueguen, L
   Sergeev, A
   Kadlec, B
   Liu, R
   Yosinski, J
AF Gueguen, Lionel
   Sergeev, Alex
   Kadlec, Ben
   Liu, Rosanne
   Yosinski, Jason
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Faster Neural Networks Straight from JPEG
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID IMAGE; RETRIEVAL
AB The simple, elegant approach of training convolutional neural networks (CNNs) directly from RGB pixels has enjoyed overwhelming empirical success. But could more performance be squeezed out of networks by using different input representations? In this paper we propose and explore a simple idea: train CNNs directly on the blockwise discrete cosine transform (DCT) coefficients computed and available in the middle of the JPEG codec. Intuitively, when processing JPEG images using CNNs, it seems unnecessary to decompress a blockwise frequency representation to an expanded pixel representation, shuffle it from CPU to GPU, and then process it with a CNN that will learn something similar to a transform back to frequency representation in its first layers. Why not skip both steps and feed the frequency domain into the network directly? In this paper, we modify libjpeg to produce DCT coefficients directly, modify a ResNet-50 network to accommodate the differently sized and strided input, and evaluate performance on ImageNet. We find networks that are both faster and more accurate, as well as networks with about the same accuracy but 1.77x faster than ResNet-50.
C1 [Gueguen, Lionel; Sergeev, Alex; Kadlec, Ben] Uber, San Francisco, CA 94105 USA.
   [Liu, Rosanne; Yosinski, Jason] Uber AI Labs, San Francisco, CA USA.
RP Gueguen, L (reprint author), Uber, San Francisco, CA 94105 USA.
EM lgueguen@uber.com; asergeev@uber.com; bkadlec@uber.com;
   rosanne@uber.com; yosinskig@uber.com
CR Adler Amir, 2016, ARXIV161009615
   [Anonymous], 2018, JPEG LIB TURB
   Brock Andrew, 2017, 5 INT C LEARN REPR T
   Chollet F., 2016, CORR
   Dan Fu, 2016, USING COMPRESSION SP
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Feng GC, 2003, PATTERN RECOGN, V36, P977, DOI 10.1016/S0031-3203(02)00114-0
   Gueguen L, 2008, IEEE T KNOWL DATA EN, V20, P562, DOI 10.1109/TKDE.2007.190718
   Hafed ZM, 2001, INT J COMPUT VISION, V43, P167, DOI 10.1023/A:1011183429707
   He K., 2015, CORR
   Hudson G, 2017, IEEE MULTIMEDIA, V24, P96, DOI 10.1109/MMUL.2017.38
   Iandola F., 2016, ARXIV160207360
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   Mandal MK, 1999, IMAGE VISION COMPUT, V17, P513, DOI 10.1016/S0262-8856(98)00143-7
   Mnih V., 2013, ARXIV E PRINTS
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sergeev Alex, 2017, MEET HOROVOD UBERS O
   Shen B, 1996, P SOC PHOTO-OPT INS, V2670, P404, DOI 10.1117/12.234779
   Song Han, 2015, CORR
   Srom Martin, 2018, GPUJP
   Torfason Robert, 2018, ARXIV180306131
   Ulicny Matej, 2017, P 19 IR MACH VIS IM
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Yosinski J., 2015, DEEP LEARN WORKSH IN
   Zeiler M. D., 2013, ARXIV13112901
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303089
DA 2019-06-15
ER

PT S
AU Gunasekar, S
   Lee, JD
   Soudry, D
   Srebro, N
AF Gunasekar, Suriya
   Lee, Jason D.
   Soudry, Daniel
   Srebro, Nathan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Implicit Bias of Gradient Descent on Linear Convolutional Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We show that gradient descent on full width linear convolutional networks of depth L converges to a linear predictor related to the l(2/L) bridge penalty in the frequency domain. This is in contrast to fully connected linear networks, where regardless of depth, gradient descent converges to the l(2) maximum margin solution.
C1 [Gunasekar, Suriya; Srebro, Nathan] TTI, Chicago, IL 60637 USA.
   [Lee, Jason D.] USC Los Angeles, Los Angeles, CA USA.
   [Soudry, Daniel] Technion, Haifa, Israel.
RP Gunasekar, S (reprint author), TTI, Chicago, IL 60637 USA.
EM suriya@ttic.edu; jasonlee@marshall.usc.edu; daniel.soudry@gmail.com;
   nati@ttic.edu
CR Andrychowicz Marcin, 2016, ADV NEURAL INFORM PR
   Bartlett P. L., 2003, J MACHINE LEARNING R
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Chaudhari Pratik, 2016, ARXIV161101838
   Dinh Laurent, 2017, INT C MACH LEARN
   Ge Dongdong, 2011, MATH PROGRAMMING
   Gunasekar Suriya, 2018, CHARACTERIZING IMPLI
   Gunasekar Suriya, 2017, NIPS
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hoffer Elad, 2017, ADV NEURAL INFORM PR
   Ji Ziwei, 2018, ARXIV180307300
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Kakade Sham M, 2009, ADV NEURAL INFORM PR
   Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR
   Keskar Nitish Shirish, 2016, INT C LEARN REPR
   Le Smith Kindermans, 2018, ICLR
   Lee Jason, 2018, ARXIV180301905
   Lee Jason D., 2016, 29 ANN C LEARN THEOR
   Li Yuanzhi, 2017, ARXIV171209203
   Muresan Marian, 2009, CONCRETE APPROACH CL, V14
   Neyshabur B., 2015, ADV NEURAL INFORM PR, P2422
   Neyshabur Behnam, 2017, GEOMETRY OPTIMIZATIO
   Neyshabur Behnam, 2015, INT C LEARN REPR
   Nguyen Quynh, 2017, ARXIV170408045
   Rockafellar R Tyrrell, 1979, P LONDON MATH SOC
   Soudry Daniel, 2017, ARXIV171010345
   Telgarsky Matus, 2013, JMLR, V28, P307
   Wilson Ashia C, 2017, ADV NEURAL INFORM PR
   Zhang Chiyuan, 2017, INT C LEARN REPR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004006
DA 2019-06-15
ER

PT S
AU Guo, DL
   Yu, AJ
AF Guo, Dalin
   Yu, Angela J.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Why so gloomy? A Bayesian explanation of human pessimism bias in the
   multi-armed bandit task
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which is also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability (mean and variance of the reward rate distribution). Surprisingly, we find subjects significantly underestimate prior mean of reward rates - based on their self-report on their reward expectation of non-chosen arms at the end of a game. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure - humans assume reward rates can change over time even though they are truly fixed. We find that the "pessimism bias" in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constants. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial Q-values. While it seems sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are stable, and utilizes a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a simplified decision policy.
C1 [Guo, Dalin; Yu, Angela J.] Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.
RP Guo, DL (reprint author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.
EM dag082@ucsd.edu; ajyu@ucsd.edu
FU NSF CRCNS grant [BCS-1309346]
FX We thank Shunan Zhang, Henry Qiu, Alvita Tran, Joseph Schilz, and
   numerous undergraduate research assistants who helped in the data
   collection. We thank Samer Sabri for helpful input with the writing.
   This work was in part funded by an NSF CRCNS grant (BCS-1309346) to AJY.
CR Averbeck BB, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004164
   Bateson M, 2016, CURR OPIN BEHAV SCI, V12, P115, DOI 10.1016/j.cobeha.2016.09.013
   Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954
   BERK RA, 1983, AM SOCIOL REV, V48, P386, DOI 10.2307/2095230
   Cohen JD, 2007, PHILOS T R SOC B, V362, P933, DOI 10.1098/rstb.2007.2098
   Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766
   Dezza IC, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17237-w
   Frazier  P, 2008, ADV NEURAL INFORM PR, V20
   Gershman SJ, 2018, COGNITION, V173, P34, DOI 10.1016/j.cognition.2017.12.014
   Gershman SJ, 2015, TOP COGN SCI, V7, P391, DOI 10.1111/tops.12138
   Harle KM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186473
   Harle KM, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01910
   Ide JS, 2013, J NEUROSCI, V33, P2039, DOI 10.1523/JNEUROSCI.2201-12.2013
   Montague PR, 1996, J NEUROSCI, V16, P1936
   Nickerson RS, 2002, PSYCHOL REV, V109, P330, DOI 10.1037//0033-295X.109.2.330
   Rescorla R. A, 1972, CLASSICAL CONDITION, V21, P64, DOI [10.1101/gr.110528.110, DOI 10.1037/A0030892]
   Ryali  C, 2018, ADV NEURAL INFORM PR
   Schonberg T, 2007, J NEUROSCI, V27, P12860, DOI 10.1523/JNEUROSCI.2496-07.2007
   Schultz W, 1998, J NEUROPHYSIOL, V80, P1
   Sharot T, 2011, CURR BIOL, V21, pR941, DOI 10.1016/j.cub.2011.10.030
   Shenoy P., 2010, ADV NEURAL INFORM PR, V23, P2146
   Speekenbrink M, 2015, TOP COGN SCI, V7, P351, DOI 10.1111/tops.12145
   Stankevicius A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003605
   Steyvers M, 2009, J MATH PSYCHOL, V53, P168, DOI 10.1016/j.jmp.2008.11.002
   Yu A., 2009, ADV NEURAL INFORM PR, P1873, DOI DOI 10.1371/JOURNAL.PONE.0099909
   Yu A J., 2014, DECISION, V1, P275, DOI [10.1037/dec0000013, DOI 10.1037/DEC0000013]
   Zhang  S, 2013, ADV NEURAL INFORM PR, V26
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305021
DA 2019-06-15
ER

PT S
AU Guo, DD
   Chen, B
   Zhang, H
   Zhou, MY
AF Guo, Dandan
   Chen, Bo
   Zhang, Hao
   Zhou, Mingyuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Poisson gamma dynamical systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.
C1 [Guo, Dandan; Chen, Bo; Zhang, Hao] Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian, Shaanxi, Peoples R China.
   [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.
RP Guo, DD (reprint author), Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian, Shaanxi, Peoples R China.
EM gdd_xidian@126.com; bchen@mail.xidian.edu.cn; zhanghao_xidian@163.com;
   mingyuan.zhou@mccombs.utexas.edu
FU Program for Young Thousand Talent by Chinese Central Government; 111
   Project [B18039]; NSFC [61771361]; NSFC for Distinguished Young Scholars
   [61525105]; Innovation Fund of Xidian University; U.S. National Science
   Foundation [IIS-1812699]
FX D. Guo, B. Chen, and H. Zhang acknowledge the support of the Program for
   Young Thousand Talent by Chinese Central Government, the 111 Project
   (No. B18039), NSFC (61771361), NSFC for Distinguished Young Scholars
   (61525105) and the Innovation Fund of Xidian University. M. Zhou
   acknowledges the support of Award IIS-1812699 from the U.S. National
   Science Foundation.
CR Acharya  A., 2015, AISTATS
   ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.1093/biomet/35.3-4.246
   Charlin Laurent, 2015, P 9 ACM C REC SYST, P155
   Cong  Y., 2017, ICML
   Cong YL, 2017, BAYESIAN ANAL, V12, P1017, DOI 10.1214/17-BA1052
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025
   Gan Zhe, 2015, ADV NEURAL INFORM PR, P2467
   Ghahramani Z, 1999, ADV NEUR IN, V11, P431
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Gong C. Y., 2017, NIPS
   Gopalan  P., 2014, AISTATS
   Han  S., 2014, NIPS, P2663
   Henao  R., 2015, NIPS, P2800
   Hermans M., 2013, ADV NEURAL INFORM PR, P190
   Hosseini SA, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P847, DOI 10.1145/3097983.3098197
   Kalman R., 1963, SIAM J CONTROL     A, V1, P152, DOI DOI 10.1137/0301010
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   LI C, 2016, P 30 AAAI C ART INT, P1788
   Ma Y.-A., 2015, ADV NEURAL INFORM PR, V2, P2917
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Ranganath  R., 2014, AISTATS, P762
   Schein  A., 2016, NIPS
   Sutskever  I., 2007, AISTATS
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Wang C., 2008, P 24 C UNC ART INT, P579
   Wang J. M., 2006, NIPS
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Zhang  H., 2018, ICLR
   Zhou  M., 2015, NIPS, P3043
   Zhou M., 2012, J MACHINE LEARNING R, V15, P1462
   Zhou M, 2015, P MACH LEARN RES, P1135
   Zhou M, 2016, ACSR ADV COMPUT, V44, P1
   Zhou MY, 2018, BAYESIAN ANAL, V13, P1061, DOI 10.1214/17-BA1070
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003004
DA 2019-06-15
ER

PT S
AU Guo, DY
   Tang, DY
   Duan, N
   Zhou, M
   Yin, J
AF Guo, Daya
   Tang, Duyu
   Duan, Nan
   Zhou, Ming
   Yin, Jian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dialog-to-Action: Conversational Question Answering Over a Large-Scale
   Knowledge Base
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.
C1 [Guo, Daya; Yin, Jian] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou, Guangdong, Peoples R China.
   [Tang, Duyu; Duan, Nan; Zhou, Ming] Microsoft Res Asia, Beijing, Peoples R China.
   [Guo, Daya] Microsoft Res, Beijing, Peoples R China.
RP Guo, DY (reprint author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou, Guangdong, Peoples R China.; Guo, DY (reprint author), Microsoft Res, Beijing, Peoples R China.
EM guody5@mail2.sysu.edu.cn; dutang@microsoft.com; nanduan@microsoft.com;
   mingzhou@microsoft.com; issjyin@mail.sysu.edu.cn
FU National Natural Science Foundation of China [61472453, U1401256,
   U1501252, U1611264, U1711261, U1711262]
FX This work is supported by the National Natural Science Foundation of
   China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262).
   Thanks to the anonymous reviewers and Junwei Bao for their helpful
   comments and suggestions.
CR Artzi Y, 2013, T ASS COMPUT LINGUIS, V1, P49
   Bahdanau D., 2015, P ICLR
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Chen Bo, 2018, P 56 ANN M ASS COMP
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Dodge J, 2015, ARXIV151106931
   Guu K., 2017, P 55 ANN M ASS COMP, P1051
   Iyer Srinivasan, 2018, ARXIV180809588
   Iyyer M, 2017, P 55 ANN M ASS COMP, P1821
   Krishnamurthy Jayant, 2017, P 2017 C EMP METH NA, P1516
   Long Reginald, 2016, P 54 ANN M ASS COMP, V1, P1456
   Lu W., 2008, P C EMP METH NAT LAN, P783
   Luo Liangchen, 2019, P 33 AAAI C ART INT
   Luong T., 2015, P 2015 C EMP METH NA, P1412, DOI DOI 10.18653/V1/D15-1166
   Madotto Andrea, 2018, ARXIV180408217
   Miller A, 2016, ARXIV160603126
   Misra S., 2016, P C EMP METH NAT LAN, P1775
   Saha A., 2018, ARXIV180110314
   Shum Heung-Yeung, 2018, ARXIV180101957
   Socher R., 2013, ADV NEURAL INFORM PR, P926
   Suhr Alane, 2018, P ANN M ASS COMP LIN, P2072
   Suhr Alane, 2018, ARXIV180406868
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Vlachos Andreas, 2014, T ASS COMPUTATIONAL, V2, P547
   Weston Jason E, 2016, ADV NEURAL INFORM PR, P829
   Yin P., 2017, P 55 ANN M ASS COMP, P440, DOI DOI 10.18653/V1/P17-1041
   Yin Pengcheng, 2018, ARXIV181013337
   Yin Pengcheng, 2018, P 2018 C EMP METH NA, P7
   Zettlemoyer L. S., 2009, P JOINT C 47 ANN M A, V2, P976
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302092
DA 2019-06-15
ER

PT S
AU Guo, WB
   Huang, S
   Tao, YZ
   Xing, XY
   Lin, L
AF Guo, Wenbo
   Huang, Sui
   Tao, Yunzhe
   Xing, Xinyu
   Lin, Lin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Explaining Deep Learning Models - A Bayesian Non-parametric Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.
C1 [Guo, Wenbo; Xing, Xinyu; Lin, Lin] Penn State Univ, University Pk, PA 16802 USA.
   [Huang, Sui] Netflix Inc, Los Gatos, CA USA.
   [Tao, Yunzhe] Columbia Univ, New York, NY 10027 USA.
RP Guo, WB (reprint author), Penn State Univ, University Pk, PA 16802 USA.
EM wzg13@ist.psu.edu; shuang@netflix.com; y.tao@columbia.edu;
   xxing@ist.psu.edu; llin@psu.edu
FU NSF [CNS-1718459]; NVIDIA Corporation
FX We gratefully acknowledge the funding from NSF grant CNS-1718459 and the
   support of NVIDIA Corporation with the donation of the GPU. We also
   would like to thank anonymous reviewers, Kaixuan Zhang, Xinran Li and
   Chenxin Ma for their helpful comments.
CR Bach S., 2015, PLOS ONE
   Chollet F., 2015, KERAS
   Cron A. J., 2011, AM STAT
   Dabkowski P., 2017, P 31 C NEUR INF PROC
   Deng J., 2009, P 22 C COMP VIS PATT
   Fong R., 2017, P 16 INT C COMP VIS
   Frosst N., 2017, ARXIV171109784
   Gan C., 2015, P 28 C COMP VIS PATT
   Hans C., 2011, J AM STAT ASS
   Hennig C., 2010, ADV DATA ANAL CLASSI
   Ishwaran H., 2001, J AM STAT ASS
   Kim B., 2016, P 30 C NEUR INF PROC
   Knight W., 2017, DARK SECRET HEART AI
   Knight W., 2017, FINANCIAL WORLD WANT
   Koh P. W., 2017, P 34 INT C MACH LEAR
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li J., 2016, ARXIV161208220
   Li Q., 2010, BAYESIAN ANAL
   Liang F., 2016, TECHNOMETRICS
   Lipton Zachary C, 2016, ARXIV160603490
   Lundberg S. M., 2017, P 31 C NEUR INF PROC
   Marin JM, 2005, HDB STAT
   Ribeiro M. T., 2016, P 22 INT C KNOWL DIS
   Selvaraju R. R., 2016, ARXIVORGABS161002391
   Shrikumar A, 2017, P 34 INT C MACH LEAR
   Simonyan K, 2013, ARXIV13126034
   Simonyan K., 2015, P 3 INT C LEARN REPR
   SMITH AFM, 1993, J ROY STAT SOC B MET, V55, P3
   Springenberg J. T., 2015, P 3 INT C LEARN REPR
   Srivastava S., 2015, P 18 INT C ART INT S
   Suchard M., 2010, J COMPUTATIONAL GRAP
   Sundararajan M., 2016, ARXIV161102639
   Wu Mike, 2018, P 32 AAAI C ART INT
   Xiao H., 2017, ARXIV170807747
   Yang H., 2011, MULTIPLE BAYES UNPUB
   Zeiler M. D., 2014, P 13 EUR C COMP VIS
   Zintgraf L. M., 2017, P 5 INT C LEARN REPR
   Zou H., 2005, J ROYAL STAT SOC B
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304052
DA 2019-06-15
ER

PT S
AU Guo, XY
   Li, S
AF Guo, Xiangyu
   Li, Shi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Distributed k-Clustering for Data with Heavy Noise
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we consider the k-center/median/means clustering with outliers problems (or the (k, z)-center/median/means problems) in the distributed setting. Most previous distributed algorithms have their communication costs linearly depending on z, the number of outliers. Recently Guha et al. [10] overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with 2z outliers. For the case where z is large, the extra z outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible (1 + epsilon)z, while maintaining the O(1)-approximation ratio and independence of communication cost on z. The problems we consider include the (k, z)-center problem, and (k, z)-median/means problems in Euclidean metrics. Implementation of the our algorithm for (k, z)-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution.
C1 [Guo, Xiangyu; Li, Shi] SUNY Buffalo, Buffalo, NY 14260 USA.
RP Guo, XY (reprint author), SUNY Buffalo, Buffalo, NY 14260 USA.
EM xiangyug@buffalo.edu; shil@buffalo.edu
FU NSF [CCF-1566356, CCF-1717134]
FX This research was supported by NSF grants CCF-1566356 and CCF-1717134.
CR Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15
   Balcan M. - F., 2013, NIPS, P1995
   Byrka J, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/2981561
   Charikar M, 2001, SIAM PROC S, P642
   Chen Jiecao, 2016, 29 ADV NEURAL INFORM, P3720
   Chen Jiecao, 2018, CORR
   Chen K, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P826
   Ene A., 2011, P 17 ACM SIGKDD INT, P681, DOI DOI 10.1145/2020408.2020515
   Guha S, 2017, PROCEEDINGS OF THE 29TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'17), P143, DOI 10.1145/3087556.3087568
   Hu Ding, 2016, P MACHINE LEARNING R, P1339
   Im Sungjin, 2015, P SPAA, P65
   Krishnaswamy R, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P646, DOI 10.1145/3188745.3188882
   Malkomes G., 2015, P NIPS, P1063
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002039
DA 2019-06-15
ER

PT S
AU Guo, XX
   Wu, H
   Cheng, Y
   Rennie, S
   Tesauro, G
   Feris, RS
AF Guo, Xiaoxiao
   Wu, Hui
   Cheng, Yu
   Rennie, Steven
   Tesauro, Gerald
   Feris, Rogerio Schmidt
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dialog-based Interactive Image Retrieval
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID RELEVANCE FEEDBACK
AB Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.
C1 [Guo, Xiaoxiao; Wu, Hui; Cheng, Yu; Tesauro, Gerald; Feris, Rogerio Schmidt] IBM Res AI, Yorktown Hts, NY 10598 USA.
   [Tesauro, Gerald] Fusemachines Inc, New York, NY USA.
RP Guo, XX (reprint author), IBM Res AI, Yorktown Hts, NY 10598 USA.
EM xiaoxiao.guo@ibm.com; wuhu@us.ibm.com; chengyu@us.ibm.com;
   srennie@gmail.com; gtesauro@us.ibm.com; rsferis@us.ibm.com
CR Antol S., 2015, ICCV
   Barrett DP, 2016, IEEE T PATTERN ANAL, V38, P2069, DOI 10.1109/TPAMI.2015.2505297
   Berg Tamara L, 2010, ECCV
   Bordes  A., 2017, ICLR
   Das  Abhishek, 2017, CVPR
   Das  Abhishek, 2017, ICCV
   de Vries Harm, 2017, CVPR
   FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146
   Frome A., 2013, NIPS
   Gordo A., 2016, ECCV
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Guo  Xiaoxiao, 2014, NIPS
   Guo  Xiaoxiao, 2017, ICLR
   He K., 2016, CVPR
   Hu  Ronghang, 2016, CVPR
   Huang  Junshi, 2015, ICCV
   Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Kim Yoon, 2014, EMNLP
   Kovashka A, 2017, ADV COMPUT VIS PATT, P89, DOI 10.1007/978-3-319-50077-5_5
   Kovashka  Adriana, 2012, CVPR
   Kovashka  Adriana, 2013, ICCV
   Kulkarni  G., 2011, CVPR
   Li  Cheng, 2014, SIGIR
   Li  Cheng, 2016, CIKM
   Li  Shuang, 2017, CVPR
   Li X., 2016, ARXIV161205688
   Liu  Z., 2016, CVPR
   Maji  Subhransu, 2012, ECCV
   Parikh D., 2011, ICCV
   Plummer Bryan A, 2015, ICCV
   Rennie Steven J., 2017, CVPR
   Rohrbach A., 2016, ECCV
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Seo Paul Hongsuck, 2017, NIPS
   Serban  I., 2016, AAAI
   Shi Z., 2015, CVPR
   Strub  Florian, 2017, IJCAI
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tapaswi  Makarand, 2016, CVPR
   Tellex  Stefanie, 2009, ACM INT C IM VID RET
   Thomee B, 2012, INT J MULTIMED INF R, V1, P71, DOI 10.1007/s13735-012-0014-4
   Vaquero Daniel A, 2009, WACV
   Vedantam  Ramakrishna, 2017, CVPR
   Vinyals O, 2015, CVPR
   Wang J, 2014, CVPR
   Wang L., 2016, CVPR
   Wang  Xuanhui, 2018, CIKM
   Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008
   Wu  Hong, 2004, ICPR
   Xu K, 2015, ICML
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yu A, 2017, ADV COMPUT VIS PATT, P119, DOI 10.1007/978-3-319-50077-5_6
   Yu  Licheng, 2016, ECCV
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300063
DA 2019-06-15
ER

PT S
AU Guo, YW
   Zhang, C
   Zhang, CS
   Chen, YR
AF Guo, Yiwen
   Zhang, Chao
   Zhang, Changshui
   Chen, Yurong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sparse DNNs with Improved Adversarial Robustness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under l(2) attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.
C1 [Guo, Yiwen; Chen, Yurong] Intel Labs China, Beijing, Peoples R China.
   [Guo, Yiwen; Zhang, Changshui] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing, Peoples R China.
   [Zhang, Chao] Peking Univ, Ctr Data Sci, Acad Adv Interdisciplinary Studies, Beijing, Peoples R China.
RP Guo, YW (reprint author), Intel Labs China, Beijing, Peoples R China.; Guo, YW (reprint author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing, Peoples R China.
EM yiwen.guo@intel.com; pkuzc@pku.edu.cn; zcs@mail.tsinghua.edu.cn;
   yurong.chen@intel.com
FU NSFC [61876095, 61751308, 61473167]; Beijing Natural Science Foundation
   [L172037]
FX We would like to thank anonymous reviewers for their constructive
   suggestions. Changshui Zhang is supported by NSFC (Grant No. 61876095,
   No. 61751308 and No. 61473167) and Beijing Natural Science Foundation
   (Grant No. L172037).
CR Bartlett Peter, 2017, NIPS
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Carlini  Nicholas, 2017, SP
   Cisse  Moustapha, 2017, ICML
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   Denil  Misha, 2013, NIPS
   Dhillon Guneet S, 2018, ICLR
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Galloway  Angus, 2018, ICLR
   Goodfellow I., 2015, ICLR
   Gopalakrishnan  Soorya, 2018, ICLR WORKSH
   Guo Yiwen, 2016, NIPS
   Han S., 2015, NIPS
   He K., 2016, CVPR
   Hein Matthias, 2017, NIPS
   Ji Gao, 2017, ICLR WORKSH
   Jia  Yangqing, 2014, MM
   Lu  J., 2017, ICCV
   Madry Aleksander, 2018, ICLR
   Marzi  Zhinus, 2018, ARXIV180104695
   Molchanov D., 2017, ICML
   Moosavi-Dezfooli S. - M., 2016, CVPR
   Neklyudov  Kirill, 2017, NIPS
   Papernot  Nicolas, 2016, SP
   Park  Jongsoo, 2017, ICLR
   Szegedy C., 2014, ICLR
   Tramer  Florian, 2018, ICLR
   Ullrich  Karen, 2017, ICLR
   Wang  Luyu, 2018, ICLR WORKSH SUBM
   Weng T.-W., 2018, ICLR
   Xie  C., 2018, ICLR
   Ye  Shaokai, 2018, ICLR WORKSH SUBM
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300023
DA 2019-06-15
ER

PT S
AU Gupta, A
   Murali, A
   Gandhi, D
   Pinto, L
AF Gupta, Abhinav
   Murali, Adithyavairavan
   Gandhi, Dhiraj
   Pinto, Lerrel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Robot Learning in Homes: Improving Generalization and Reducing Dataset
   Bias
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3 K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28 K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.
C1 [Gupta, Abhinav; Murali, Adithyavairavan; Gandhi, Dhiraj; Pinto, Lerrel] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
RP Gupta, A; Murali, A; Gandhi, D; Pinto, L (reprint author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
EM abhinavg@cs.cmu.edu; amurali@cs.cmu.edu; dgandhi@cs.cmu.edu;
   lerrelp@cs.cmu.edu
FU ONR MURI [N000141612007]; Sloan Research Fellowship; Uber Fellowship
FX This work was supported by ONR MURI N000141612007. Abhinav was supported
   in part by Sloan Research Fellowship and Adithya was partly supported by
   a Uber Fellowship.
CR Agarwal  Pulkit, 2016, LEARNING POKE POKING
   Bicchi  Antonio, 2000, INT C ROB AUT
   Bohg  Jeannette, 2014, IEEE T ROBOTICS
   Bousmalis K, 2018, IEEE INT CONF ROBOT, P4243
   Calandra  Roberto, 2017, C ROB LEARN
   Chen  Tao, 2018, NUERAL INFORM PROCES
   Dahl  George, 2010, IEEE T AUDIO SPEECH
   Deisenroth Marc Peter, 2011, RSS
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Erickson  Zackory, 2017, C ROB LEARN
   Fang K, 2018, IEEE INT CONF ROBOT, P3516, DOI 10.1109/ICRA.2018.8461041
   Finn Chelsea, 2016, ADV NEURAL INFORM PR
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Hawes N, 2017, IEEE ROBOT AUTOM MAG, V24, P146, DOI 10.1109/MRA.2016.2636359
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Keselman  Leonid, 2017, ARXIV170505548
   Kingma D. P., 2014, ARXIV14126980
   Lenz  Ian, 2015, IJRR
   Levine S., 2016, ABS160302199 CORR
   Levine S, 2016, J MACH LEARN RES, V17
   Mahler J, 2016, IEEE INT CONF ROBOT, P1957, DOI 10.1109/ICRA.2016.7487342
   Mahler Jeffrey, 2017, RSS
   Misra  Ishan, 2016, CVPR
   Murali  Adithyavairavan, 2018, INT S EXP ROB
   Murali  Adithyavairavan, 2018, INT C ROB AUT
   Nair  Ashvin, 2017, INT C ROB AUT
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   NGUYEN VD, 1988, INT J ROBOT RES, V7, P3, DOI 10.1177/027836498800700301
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Peng Xue Bin, 2017, ARXIV171006537
   Pinto  L., 2018, ROBOTICS SCI SYSTEMS
   Pinto  Lerrel, 2017, INT C ROB AUT
   Pinto  Lerrel, 2015, ABS150906825 CORR
   Redmon J., 2016, ARXIV161208242
   Sharma  Pratyusha, 2018, ARXIV181007121
   Sun  Chen, 2017, ICCV
   Tobin  Josh, 2017, DOMAIN RANDOMIZATION
   Veloso M, 2012, IEEE INT C INT ROBOT, P5446, DOI 10.1109/IROS.2012.6386300
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yahya  Ali, 2017, IROS
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003063
DA 2019-06-15
ER

PT S
AU Gupta, A
   Mendonca, R
   Liu, YX
   Abbeel, P
   Levine, S
AF Gupta, Abhishek
   Mendonca, Russell
   Liu, YuXuan
   Abbeel, Pieter
   Levine, Sergey
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Meta-Reinforcement Learning of Structured Exploration Strategies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm - model agnostic exploration with structured noise (MAESN) - to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.
C1 [Gupta, Abhishek; Mendonca, Russell; Liu, YuXuan; Abbeel, Pieter; Levine, Sergey] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Gupta, A (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM abhigupta@eecs.berkeley.edu; russellm@berkeley.edu;
   yuxuanliu@berkeley.edu; pabbeel@eecs.berkeley.edu;
   svlevine@eecs.berkeley.edu
FU National Science Foundation [IIS-1651843, IIS-1614653]; ONR PECASE
   award; ONR Young Investigator Program
FX The authors would like to thank Chelsea Finn, Gregory Kahn, Ignasi
   Clavera for thoughtful discussions and Justin Fu, Marvin Zhang for
   comments on an early version of the paper. This work was supported by a
   National Science Foundation Graduate Research Fellowship for Abhishek
   Gupta, ONR PECASE award for Pieter Abbeel, and the National Science
   Foundation through IIS-1651843 and IIS-1614653, as well as an ONR Young
   Investigator Program award for Sergey Levine.
CR Bellemare Marc G., 2016, NIPS
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Duan Y., 2016, CORR
   Finn C., 2017, ICML
   Florensa C., 2017, CORR
   Fortunato M., 2017, CORR
   Gomez S., 2016, NIPS
   Hausman K., 2018, P INT C LEARN REPR I
   Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87
   Houthooft R., 2016, NIPS
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   Kolter J. Z., 2007, RSS
   Levine S, 2016, J MACH LEARN RES, V17
   LI Z, 2017, CORR
   Lopes M., 2012, NIPS
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Osband I., 2016, NIPS
   Plappert M., 2017, CORR
   Rajeswaran A., 2017, CORR
   Ravi S., 2017, P INT C LEARN REPR I
   Santoro A., 2016, ICML
   SCHMIDHUBER J, 1987, THESIS
   Schulman John, 2015, ICML
   Singh S. P., 2004, NIPS
   Stadie B. C., 2015, CORR
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Thrun S, 1998, LEARNING TO LEARN, P3
   Vinyals O., 2016, NIPS
   Wang J., 2016, CORR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305033
DA 2019-06-15
ER

PT S
AU Gupta, MR
   Bahri, D
   Cotter, A
   Canini, K
AF Gupta, Maya R.
   Bahri, Dara
   Cotter, Andrew
   Canini, Kevin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Diminishing Returns Shape Constraints for Interpretability and
   Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs. We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks. We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability.
C1 [Gupta, Maya R.; Bahri, Dara; Cotter, Andrew; Canini, Kevin] Google AI, 1600 Charleston Rd, Mountain View, CA 94043 USA.
RP Gupta, MR (reprint author), Google AI, 1600 Charleston Rd, Mountain View, CA 94043 USA.
EM mayagupta@google.com; dbahri@google.com; acotter@google.com;
   canini@google.com
CR Amos B., 2017, P ICML
   ARCHER NP, 1993, DECISION SCI, V24, P60, DOI 10.1111/j.1540-5915.1993.tb00462.x
   Barlow R, 1972, STAT INFERENCE ORDER
   Canini K., 2016, ADV NEURAL INFORM PR
   Chen Y., 2016, J ROYAL STAT SOC B
   Cotter A., 2016, 29 ANN C LEARN THEOR, P729
   Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Dugas C., 2009, J MACHINE LEARNING R
   Fard M. Milani, 2016, ADV NEURAL INFORM PR
   Garcia E. K., 2009, ADV NEURAL INFORM PR
   Garcia E, 2012, IEEE T IMAGE PROCESS, V21, P4128, DOI 10.1109/TIP.2012.2200902
   Groeneboom P., 2014, NONPARAMETRIC ESTIMA
   Gupta M. R., 2016, J MACHINE LEARNING R, V17, P1
   HASTIE T, 1990, GEN ADDITIVE MODELS
   Kahneman D., 2000, CHOICES VALUES FRAME
   Kay H, 2000, AICHE J, V46, P2426, DOI 10.1002/aic.690461211
   Kim J., 2004, P IEEE INT C COMP AI
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Magnani A., 2009, OPTIMIZATION ENG
   Minin A, 2010, NEURAL NETWORKS, V23, P471, DOI 10.1016/j.neunet.2009.09.002
   Patton F. L., 1926, DIMINISHING RETURNS
   Pya N., 2015, STAT COMPUTING
   Qu YJ, 2011, IEEE T NEURAL NETWOR, V22, P2447, DOI 10.1109/TNN.2011.2167348
   Shingo S, 1986, ZERO QUALITY CONTROL
   Sill J., 1998, ADV NEURAL INFORM PR
   Smith A., 1776, INQUIRY NATURE CAUSE
   Smith A., 1817, PRINCIPLES POLITICAL
   Vandenberghe L., 2008, CONVEX OPTIMIZATION
   You S., 2017, ADV NEURAL INFORM PR
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001038
DA 2019-06-15
ER

PT S
AU Gupta, N
   Sidford, A
AF Gupta, Neha
   Sidford, Aaron
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Exploiting Numerical Sparsity for Efficient Learning : Faster
   Eigenvector Computation and Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix A is an element of R-nxd where every row a is an element of R-d has parallel to a parallel to(2)(2) <= L and numerical sparsity at most s, i.e. parallel to a parallel to(2)(1)/parallel to a parallel to(2)(2) <= s, we provide faster algorithms for these problems in many parameter settings.
   For top eigenvector computation, we obtain a running time of (O) over tilde (nd + r(s + root rs)/gap(2)) where gap > 0 is the relative gap between the top two eigenvectors of A(inverted perpendicular) A and r is the stable rank of A. This running time improves upon the previous best unaccelerated running time of O(nd + rd/gap(2)) as r <= d and s <= d.
   For regression, we obtain a running time of (O) over tilde (nd (nL/mu) root snL/mu) where > 0 is the smallest eigenvalue of A(inverted perpendicular)A. This running time improves upon the previous best unaccelerated running time of (O) over tilde (nd + nLd/mu). This result expands the regimes where regression can be solved in nearly linear time from when L/mu = (O) over tilde (1) to when L/mu = (O) over tilde (d(2/3) / (sn)(1/3)).
   Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point [9] / catalyst [15]. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and l(p) norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning.
C1 [Gupta, Neha] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Sidford, Aaron] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.
RP Gupta, N (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM nehagupta@cs.stanford.edu; sidford@stanford.edu
CR Achlioptas D., 2013, 27 ANN C NEUR INF PR, P1565
   Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097
   Agarwal Naman, 2017, ARXIV171108426
   Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Arora S, 2006, LECT NOTES COMPUT SC, V4110, P272
   Bottou U, 2004, ADV NEUR IN, V16, P217
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Drineas P, 2011, INFORM PROCESS LETT, V111, P385, DOI 10.1016/j.ipl.2011.01.010
   Frostig R., 2015, P 32 INT C MACH LEAR, P2540
   Garber D., 2016, INT C MACH LEARN, P2626
   Gittens Alex, 2009, ARXIV09114108
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Konecny J, 2017, OPTIM METHOD SOFTW, V32, P993, DOI 10.1080/10556788.2017.1298596
   Kundu Abhisek, 2014, ARXIV14040320
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Musco C, 2017, ARXIV170404163
   Musco C., 2015, ADV NEURAL INFORM PR, V28, P1396
   Nguyen Nam H, 2010, ARXIV10054732
   Nguyen NH, 2009, MATRIX SPARSIFICATIO
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shamir O., 2015, P 32 INT C MACH LEAR, P144
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305030
DA 2019-06-15
ER

PT S
AU Gur, Y
   Momeni, A
AF Gur, Yonatan
   Momeni, Ahmadreza
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adaptive Learning with Unknown Information Flows
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process.
C1 [Gur, Yonatan] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA.
   [Momeni, Ahmadreza] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Gur, Y (reprint author), Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA.
EM ygur@stanford.edu; amomenis@stanford.edu
CR Agrawal S, 2013, P 16 INT C ART INT S, P99
   Audibert J. Y., 2010, COLT 23 C LEARN THEO, P13
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Bastani H., 2017, ARXIV170409011
   Bastani H., 2015, PREPRINT
   Besbes O., 2014, ADV NEURAL INFORM PR, P199
   Bubeck Sebastien, 2013, P 26 C LEARN THEOR, P122
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   GOLDENSHLUGER A., 2013, STOCHASTIC SYSTEMS, V3, P230
   Jadbabaie A, 2015, P 18 INT C ART INT S, P398
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Komiyama J., 2013, AS C MACH LEARN, P100
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Sani  Amir, 2014, ADV NEURAL INFORM PR, V27, P810
   Seldin  Y., 2014, INT C MACH LEARN ICM, P1287
   Shah V., 2018, ARXIV180205693
   Strehl A. L., 2006, P 23 INT C MACH LEAR, P889
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Traca S., 2015, ARXIV150505629
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Wang CC, 2005, IEEE T AUTOMAT CONTR, V50, P338, DOI 10.1109/TAC.2005.844079
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002006
DA 2019-06-15
ER

PT S
AU Ha, D
   Schmidhuber, J
AF Ha, David
   Schmidhuber, Jurgen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Recurrent World Models Facilitate Policy Evolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PRIMARY VISUAL-CORTEX; REPRESENTATION; CREATIVITY; CURIOSITY
AB A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatiotemporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io
C1 [Ha, David] Google Brain, Tokyo, Japan.
   [Schmidhuber, Jurgen] IDSIA USI & SUPSI, Swiss AI Lab, NNAISENSE, Manno, Switzerland.
RP Ha, D (reprint author), Google Brain, Tokyo, Japan.
EM hadavid@google.com; juergen@idsia.ch
FU SNF project RNNAISSANCE [200021_165675]; ERC Advanced Grant [742870]
FX We would like to thank Blake Richards, Kory Mathewson, Chris Olah, Kai
   Arulkumaran, Denny Britz, Kyle McDonald, Ankur Handa, Elwin Ha, Nikhil
   Thorat, Daniel Smilkov, Alex Graves, Douglas Eck, Mike Schuster, Rajat
   Monga, Vincent Vanhoucke, Jeff Dean and Natasha Jaques for their
   thoughtful feedback. This work was partially funded by SNF project
   RNNAISSANCE (200021_165675) and by an ERC Advanced Grant (no: 742870).
CR Alvernaz Samuel, 2017, 2017 IEEE Conference on Computational Intelligence and Games (CIG), P1, DOI 10.1109/CIG.2017.8080408
   Bartol TM, 2015, ELIFE, V4, DOI 10.7554/eLife.10778
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Bousmalis K., 2017, ARXIV170907857
   Brockman G, 2016, ARXIV160601540
   Carter S., 2016, DISTILL
   Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011
   Chiappa S., 2017, ARXIV170402254
   Consalvo M., 2007, CHEATING GAINING ADV
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Denton E. L., 2017, ADV NEURAL INFORM PR, P4417
   Depeweg S., 2016, ARXIV160507127
   Dosovitskiy A, 2016, ARXIV161101779
   Fernando C, 2017, ARXIV170108734
   Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173
   French R. M., 1994, ADV NEURAL INFORM PR, V6, P1176
   Gal  Yarin, 2016, DAT EFF MACH LEARN W
   Gauci J, 2010, NEURAL COMPUT, V22, P1860, DOI 10.1162/neco.2010.06-09-1042
   Gemici Mevlana, 2017, ARXIV170204649
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gomez F, 2008, J MACH LEARN RES, V9, P937
   Gomez FJ, 2005, GECCO 2005: Genetic and Evolutionary Computation Conference, Vols 1 and 2, P491
   Gottlieb J, 2013, TRENDS COGN SCI, V17, P585, DOI 10.1016/j.tics.2013.09.001
   Graves A, 2013, ARXIV13080850
   Graves A., 2015, HALLUCINATION RECURR
   Guzdial M. O. R. Matthew, 2017, P 26 INT JOINT C ART, P3707
   Ha D., 2017, EVOLVING STABLE STRA
   Ha David, 2017, INT C LEARN REPR
   Ha David, 2018, INT C LEARN REPR
   Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398
   Hansen Nikolaus, 2016, ARXIV160400772
   Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713
   Hein D., 2017, ARXIV170909480
   Higgins I., 2017, ARXIV170708475
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hunermann J., 2017, SELF DRIVING CARS BR
   Jang S., 2017, REINFORCEMENT CAR RA
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Keller GB, 2012, NEURON, V74, P809, DOI 10.1016/j.neuron.2012.03.040
   KELLEY HJ, 1960, ARSJ-AM ROCKET SOC J, V30, P947, DOI 10.2514/8.5282
   Kempka M, 2016, IEEE CONF COMPU INTE
   Khan M., 2016, CAR RACING USING REI
   Kingma D.P., 2013, ARXIV13126114
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Klimov O., 2016, CARRACING V0
   Koutnik J, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1061
   Lau B., 2016, USING KERAS DEEP DET
   Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025
   Leinweber M, 2017, NEURON, V95, P1420, DOI 10.1016/j.neuron.2017.08.036
   LIN L, 1993, THESIS
   Linnainmaa S., 1970, THESIS
   Maus GW, 2013, NEURON, V78, P554, DOI 10.1016/j.neuron.2013.03.010
   McAllister R., 2017, ADV NEURAL INFORM PR, P2037
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Mobbs D, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00055
   MUNRRO PW, 1987, P 9 ANN C COGN SCI S, P165
   Nagabandi  A., 2017, ARXIV170802596
   Nguyen D., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P357, DOI 10.1109/IJCNN.1989.118723
   Nortmann N, 2015, CEREB CORTEX, V25, P1427, DOI 10.1093/cercor/bht318
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271
   Paquette P., 2016, DOOMTAKECOVER V0
   Parker M, 2012, IEEE T COMP INTEL AI, V4, P44, DOI 10.1109/TCIAIG.2012.2184109
   Pathak Deepak, 2017, INT C MACH LEARN ICM, V2017
   Pi HJ, 2013, NATURE, V503, P521, DOI 10.1038/nature12676
   Prieur L., 2017, DEEP Q LEARNING RACE
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Racaniere Sebastien, 2017, ADV NEURAL INFORM PR, P5694
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Rechenberg I, 1978, SIMULATIONSMETHODEN, P83
   Rezende D. J, 2014, ARXIV14014082
   ROBINSON T, 1989, PROCEEDINGS OF THE 1, P836
   Salimans T., 2017, ARXIV170303864
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Schmidhuber J., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P253, DOI 10.1109/IJCNN.1990.137723
   Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X
   SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605
   Schmidhuber J., 1990, FKI12690 TU MUNCH
   Schmidhuber J., 1991, ADV NEURAL INFORMATI, V3, P500
   Schmidhuber J., 1990, P 1 INT C SIM AD BEH, P222
   Schmidhuber J., 2015, ARXIV151109249
   Schmidhuber J., 2018, ARXIV180208864
   Schmidhuber J., 1994, FKI94 TUM DEP INF
   Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658
   Schmidhuber J, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00313
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Schwefel H.-P., 1977, NUMERICAL OPTIMIZATI
   Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004
   Shazeer N., 2017, INT C LEARN REPR
   Silver D., 2016, ARXIV161208810
   Srivastava RK, 2013, NEURAL NETWORKS, V41, P130, DOI 10.1016/j.neunet.2013.01.022
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Suarez J., 2017, ADV NEURAL INFORM PR, P3269
   Such F. P., 2017, ARXIV171206567
   Sutton R., 1998, INTRO REINFORCEMENT
   Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216
   Van Den Oord A., 2016, ARXIV160903499
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Wahlstrom N., 2015, ARXIV150202251
   Wahlstrom N., 2015, 17 IFAC S SYST ID SY
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Watters  N., 2017, ARXIV170601433
   Werbos P. J., 1982, System Modeling and Optimization. Proceedings of the 10th IFIP Conference, P762
   Werbos P. J., 1987, P IEEE INT C SYST MA
   WERBOS PJ, 1989, PROCEEDINGS OF THE 28TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-3, P260, DOI 10.1109/CDC.1989.70114
   Wiering M, 2012, ADAPT LEARN OPTIM, V12, P1, DOI 10.1007/978-3-642-27645-3
   Wu Yan, 2018, INT C LEARN REPR
NR 107
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302046
DA 2019-06-15
ER

PT S
AU Ha, JS
   Park, YJ
   Chae, HJ
   Park, SS
   Choi, HL
AF Ha, Jung-Su
   Park, Young-Jin
   Chae, Hyeok-Joo
   Park, Soon-Seo
   Choi, Han-Lim
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adaptive Path-Integral Autoencoder: Representation Learning and Planning
   for Dynamical Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional sequential raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data. The supplementary video(1) and the implementation code(2) are available online.
C1 [Ha, Jung-Su; Park, Young-Jin; Chae, Hyeok-Joo; Park, Soon-Seo; Choi, Han-Lim] Korea Adv Inst Sci & Technol, Dept Aerosp Engn & KI Robot, Daejeon 305701, South Korea.
RP Ha, JS (reprint author), Korea Adv Inst Sci & Technol, Dept Aerosp Engn & KI Robot, Daejeon 305701, South Korea.
EM jsha@lics.kaist.ac.kr; yjpark@lics.kaist.ac.kr; hjchae@lics.kaist.ac.kr;
   sspark@lics.kaist.ac.kr; hanlimc@kaist.ac.kr
FU Agency for Defense Development [UD150047JD]
FX This work was supported by the Agency for Defense Development under
   contract UD150047JD.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Anh Le Tuan, 2018, INT C LEARN REPR ICL
   Banijamali Ershad, 2018, INT C ART INT STAT A
   Bellman  R., 2013, DYNAMIC PROGRAMMING
   Burda Yuri, 2016, INT C LEARN REPR ICL
   Chen NT, 2016, IEEE-RAS INT C HUMAN, P629, DOI 10.1109/HUMANOIDS.2016.7803340
   Cremer Chris, 2018, ARXIV180103558
   Cremer Chris, 2017, ICLR WORKSH
   Fraccaro Marco, 2017, ADV NEURAL INFORM PR, P3604
   Gardiner CW, 1985, HDB STOCHASTIC METHO, V4
   Genewein T, 2015, FRONT ROBOT AI, DOI 10.3389/frobt.2015.00027
   Ha Jung-Su, 2018, ROBOTICS AUTOMATION
   Hjelm Devon, 2016, ADV NEURAL INFORM PR, P4691
   Ichter Brian, 2018, INT C ROB AUT ICRA
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Jonschkowski R, 2015, AUTON ROBOT, V39, P407, DOI 10.1007/s10514-015-9459-7
   Kappen HJ, 2016, J STAT PHYS, V162, P1244, DOI 10.1007/s10955-016-1446-7
   Karkus Peter, 2017, ADV NEURAL INFORM PR, P4697
   Karl M., 2017, INT C LEARN REPR ICL
   Kim Yoon, 2018, ARXIV180202550
   Kingma D, 2014, INT C LEARN REPR ICL
   Krishnan R. G., 2017, AAAI, P2101
   Krishnan Rahul G, 2018, INT C ART INT STAT A
   Lesort T., 2018, ARXIV180204181
   Maddison Chris J, 2017, ADV NEURAL INFORM PR
   Mnih A, 2016, INT C MACH LEARN, P2188
   Naesseth Christian A, 2018, INT C ART INT STAT A
   Okada M., 2017, ARXIV170609597
   Rainforth Tom, 2018, INT C MACH LEARN ICM
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Ruiz HC, 2017, IEEE T SIGNAL PROCES, V65, P3191, DOI 10.1109/TSP.2017.2686340
   Tamar A., 2016, ADV NEURAL INFORM PR, V29, P2154
   Tassa Y., 2018, ARXIV180100690
   Thijssen S, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.032104
   Todorov E, 2008, IEEE DECIS CONTR P, P4286, DOI 10.1109/CDC.2008.4739438
   Todorov E, 2009, P NATL ACAD SCI USA, V106, P11478, DOI 10.1073/pnas.0710743106
   Vernaza P, 2012, INT J ROBOT RES, V31, P1739, DOI 10.1177/0278364912457436
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Zhang Clark, 2018, ARXIV180601968
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003048
DA 2019-06-15
ER

PT S
AU Haber, N
   Mrowca, D
   Wang, S
   Li, FF
   Yamins, DLK
AF Haber, Nick
   Mrowca, Damian
   Wang, Stephanie
   Li Fei-Fei
   Yamins, Daniel L. K.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Play With Intrinsically-Motivated, Self-Aware Agents
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFANTS; ATTENTION; EXPERIENCE; CURIOSITY
AB Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a "world-model" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit "self-model" that allows the agent to track the error map of its world-model. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments.
C1 [Haber, Nick; Yamins, Daniel L. K.] Dept Psychol, Stanford, CA 94305 USA.
   [Haber, Nick] Dept Pediat, Stanford, CA 94305 USA.
   [Haber, Nick] Dept Biomed Data Sci, Stanford, CA 94305 USA.
   [Mrowca, Damian; Wang, Stephanie; Li Fei-Fei; Yamins, Daniel L. K.] Dept Comp Sci, Stanford, CA 94305 USA.
   [Yamins, Daniel L. K.] Wu Tsai Neurosci Inst, Stanford, CA 94305 USA.
RP Haber, N (reprint author), Dept Psychol, Stanford, CA 94305 USA.; Haber, N (reprint author), Dept Pediat, Stanford, CA 94305 USA.; Haber, N (reprint author), Dept Biomed Data Sci, Stanford, CA 94305 USA.
EM nhaber@stanford.edu; mrowca@stanford.edu
FU James S. McDonnell Foundation; Simons Foundation; Sloan Foundation;
   Berry Foundation postdoctoral fellowship; Stanford Department of
   Biomedical Data Science [NLM T-15 LM007033-35]; ONR - MURI (Stanford
   Lead) [N00014-16-1-2127]; ONR - MURI (UCLA Lead) [1015 G TA275]
FX This work was supported by grants from the James S. McDonnell
   Foundation, Simons Foundation, and Sloan Foundation (DLKY), a Berry
   Foundation postdoctoral fellowship and Stanford Department of Biomedical
   Data Science NLM T-15 LM007033-35 (NH), ONR - MURI (Stanford Lead)
   N00014-16-1-2127 and ONR - MURI (UCLA Lead) 1015 G TA275 (LF).
CR Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Achiam J., 2017, ABS170301732 CORR
   Agrawal P., 2016, NIPS
   Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008
   Begus K, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108817
   Boeing A, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P281
   Chentanez N, 2005, ADV NEURAL INFORM PR, V17, P1281
   Ebert F., 2017, CORL, V78, P344
   Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33
   FANTZ RL, 1964, SCIENCE, V146, P668, DOI 10.1126/science.146.3644.668
   Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324
   Frank M., 2014, FRONT NEUROROBOT
   Gidaris N. K. Spyros, 2018, ICLR
   Gilad-Bachrach R., 2005, ADV NEURAL INFORM PR, V5, P443
   Gopnik A., 2009, SCI CRIB MINDS BRAIN
   Gottlieb J, 2013, TRENDS COGN SCI, V17, P585, DOI 10.1016/j.tics.2013.09.001
   Goupil L, 2016, P NATL ACAD SCI USA, V113, P3492, DOI 10.1073/pnas.1515129113
   Held D., 2017, ABS170506366 CORR
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   HONG S, 2017, CVPR, P2224, DOI DOI 10.1109/CVPR.2017.239
   Hurley KB, 2015, J COGN DEV, V16, P11, DOI 10.1080/15248372.2013.833922
   Hurley KB, 2010, INFANT BEHAV DEV, V33, P619, DOI 10.1016/j.infbeh.2010.07.015
   Jaderberg M., 2016, ABS161105397 CORR
   Kalchbrenner N., 2017, P 34 INT C MACH LEAR, P1771
   Kidd C, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0036399
   Kingma D. P., 2014, ABS14126980 CORR
   Kulkarni T. D., 2016, ADV NEURAL INFORM PR, P3675
   Lin L. - J., 1992, THESIS
   Machado Marlos C, 2017, ARXIV170300956
   Mitash C, 2017, IEEE INT C INT ROBOT, P545, DOI 10.1109/IROS.2017.8202206
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mrowca D., 2018, ARXIV180608047
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271
   Oudeyer PY, 2016, TOP COGN SCI, V8, P492, DOI 10.1111/tops.12196
   Pathak D., 2017, P INT C MACH LEARN, P2778
   Popov Ivaylo, 2017, ARXIV170403073
   Saxe R, 2003, NEUROIMAGE, V19, P1835, DOI 10.1016/S1053-8119(03)00230-1
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Sener O., 2017, ABS170800489 CORR
   Settles B., 2011, ACTIVE LEARNING, V18
   Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031
   Sokolov EN, 1963, PERCEPTION CONDITION
   Twomey K. E., 2017, DEV SCI
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
NR 46
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002089
DA 2019-06-15
ER

PT S
AU Hajiramezanali, E
   Dadaneh, SZ
   Karbalayghareh, A
   Zhou, MY
   Qian, XN
AF Hajiramezanali, Ehsan
   Dadaneh, Siamak Zamani
   Karbalayghareh, Alireza
   Zhou, Mingyuan
   Qian, Xiaoning
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bayesian multi-domain learning for cancer subtype discovery from
   next-generation sequencing count data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without "negative transfer" effects often seen in existing multi-task learning and transfer learning methods.
C1 [Hajiramezanali, Ehsan; Dadaneh, Siamak Zamani; Karbalayghareh, Alireza; Qian, Xiaoning] Texas A&M Univ, College Stn, TX 77843 USA.
   [Zhou, Mingyuan] Univ Texas Austin, Austin, TX 78712 USA.
RP Hajiramezanali, E (reprint author), Texas A&M Univ, College Stn, TX 77843 USA.
EM ehsanr@tamu.edu; siamak@tamu.edu; alireza.kg@tamu.edu;
   Mingyuan.Zhou@mccombs.utexas.edu; xqian@ece.tamu.edu
FU NSF [CCF-1553281, IIS-1812641, IIS-1812699]
FX We would like to thank Dr. Sahar Yarian for insightful discussions. We
   also thank Texas A&M High Performance Research Computing and Texas
   Advanced Computing Center for providing computational resources to
   perform experiments in this work. This work was supported in part by the
   NSF Awards CCF-1553281, IIS-1812641, and IIS-1812699.
CR Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Cam L.L., 1960, PAC J MATH, V10, P1181, DOI [10.2140/pjm.1960.10.1181, DOI 10.2140/PIM.L960.10.1181]
   Chelba C, 2006, COMPUT SPEECH LANG, V20, P382, DOI 10.1016/j.csl.2005.05.005
   Chin L, 2008, NATURE, V455, P1061, DOI 10.1038/nature07385
   Dadaneh SZ, 2018, J AM STAT ASSOC, V113, P81, DOI 10.1080/01621459.2017.1328358
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Ghahramani Z., 2006, ADV NEURAL INFORM PR, P475
   Hajiramezanali  E., 2018, ARXIV180302527
   Jacob L., 2009, ADV NEURAL INFORM PR, V21, P745
   Kang Z., 2011, P 28 INT C MACH LEAR, P521
   Karbalayghareh  A., 2018, IEEE T SIGNAL PROCES
   Kumar A., 2012, ARXIV12066417
   Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pardoe D., 2010, P 27 INT C MACH LEAR, P863
   Passos  A., 2012, ARXIV12066486
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Rai P, 2010, P 13 INT C ART INT S, P613
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Teh Y. W., 2005, ADV NEURAL INFORM PR, P1385
   Thibaux R., 2007, INT C ART INT STAT, V11, P564
   Williamson  S., 2010, IBP COMPOUND DIRICHL
   Xue Y, 2007, J MACH LEARN RES, V8, P35
   Zhou  M., 2012, ADV NEURAL INFORM PR, P2546
   Zhou M., 2009, ADV NEURAL INFORM PR, V22, P2295
   Zhou MY, 2018, BAYESIAN ANAL, V13, P1061, DOI 10.1214/17-BA1070
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003065
DA 2019-06-15
ER

PT S
AU Halloran, JT
   Rocke, DM
AF Halloran, John T.
   Rocke, David M.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Concave Conditional Likelihood Models for Improved Analysis of
   Tandem Mass Spectra
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FALSE DISCOVERY RATE; PEPTIDE IDENTIFICATION
AB The most widely used technology to identify the proteins present in a complex biological sample is tandem mass spectrometry, which quickly produces a large collection of spectra representative of the peptides (i.e., protein subsequences) present in the original sample. In this work, we greatly expand the parameter learning capabilities of a dynamic Bayesian network (DBN) peptide-scoring algorithm, Didea [25], by deriving emission distributions for which its conditional log-likelihood scoring function remains concave. We show that this class of emission distributions, called Convex Virtual Emissions (CVEs), naturally generalizes the log-sum-exp function while rendering both maximum likelihood estimation and conditional maximum likelihood estimation concave for a wide range of Bayesian networks. Utilizing CVEs in Didea allows efficient learning of a large number of parameters while ensuring global convergence, in stark contrast to Didea's previous parameter learning framework (which could only learn a single parameter using a costly grid search) and other trainable models [12, 13, 14] (which only ensure convergence to local optima). The newly trained scoring function substantially outperforms the state-of-the-art in both scoring function accuracy and downstream Fisher kernel analysis. Furthermore, we significantly improve Didea's runtime performance through successive optimizations to its message passing schedule and derive explicit connections between Didea's new concave score and related MS/MS scoring functions.
C1 [Halloran, John T.; Rocke, David M.] Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.
RP Halloran, JT (reprint author), Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.
EM jthalloran@ucdavis.edu; dmrocke@ucdavis.edu
FU National Center for Advancing Translational Sciences (NCATS), National
   Institutes of Health [UL1 TR001860]
FX This work was supported by the National Center for Advancing
   Translational Sciences (NCATS), National Institutes of Health, through
   grant UL1 TR001860.
CR Ajit P. Singh, 2012, UNCERTAINTY ARTIFICI
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Craig R, 2004, BIOINFORMATICS, V20, P1466, DOI 10.1093/bioinformatics/bth092
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Deng L., 2006, SYNTH LECT SPEECH AU, V2, P1
   Diament BJ, 2011, J PROTEOME RES, V10, P3871, DOI 10.1021/pr101196n
   Eng JK, 2008, J PROTEOME RES, V7, P4598, DOI 10.1021/pr800420s
   Eng JK, 2013, PROTEOMICS, V13, P22, DOI 10.1002/pmic.201200439
   ENG JK, 1994, J AM SOC MASS SPECTR, V5, P976, DOI 10.1016/1044-0305(94)80016-2
   Geer LY, 2004, J PROTEOME RES, V3, P958, DOI 10.1021/pr0499491
   Halloran John T, 2018, Methods Mol Biol, V1807, P163, DOI 10.1007/978-1-4939-8561-6_12
   Halloran JT, 2016, J PROTEOME RES, V15, P2749, DOI 10.1021/acs.jproteome.6b00290
   Halloran John T., 2018, ADV NEURAL INFORM PR
   Halloran John T, 2017, ADV NEURAL INF PROCE, P5728
   Halloran John T., 2014, UNCERTAINTY ARTIFICI
   Jeffry Howbert  J, 2014, MOL CELLULAR PROTEOM, pmcp
   Kall L, 2007, NAT METHODS, V4, P923, DOI 10.1038/NMETH1113
   Keich U, 2015, J PROTEOME RES, V14, P3148, DOI 10.1021/acs.jproteome.5b00081
   Kim  Sangtae, 2014, NATURE COMMUNICATION, V5
   Klammer AA, 2009, J PROTEOME RES, V8, P2106, DOI 10.1021/pr8011107
   McIlwain  Sean, 2014, J PROTEOME RES
   Murphy KP, 2002, ADV NEUR IN, V14, P833
   Park CY, 2008, J PROTEOME RES, V7, P3022, DOI 10.1021/pr800127y
   Pearl J, 1988, PROBABILISTIC REASON
   Spivak M, 2012, MOL CELLULAR PROTEOM, V11
   Spivak M, 2009, J PROTEOME RES, V8, P3737, DOI 10.1021/pr801109k
   Wenger C. D., 2013, J PROTEOME RES
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305044
DA 2019-06-15
ER

PT S
AU Hamilton, WL
   Bajaj, P
   Zitnik, M
   Jurafsky, D
   Leskovec, J
AF Hamilton, William L.
   Bajaj, Payal
   Zitnik, Marinka
   Jurafsky, Dan
   Leskovec, Jure
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Embedding Logical Queries on Knowledge Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DATABASE; NETWORKS
AB Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict what drugs are likely to target proteins involved with both diseases X and Y?-a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries-a flexible but tractable subset of first-order logic-on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.
C1 [Hamilton, William L.; Bajaj, Payal; Zitnik, Marinka; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Jurafsky, Dan] Stanford Univ, Dept Linguist, Stanford, CA 94305 USA.
RP Hamilton, WL (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM wleif@stanford.edu; pbajaj@stanford.edu; jure@cs.stanford.edu;
   jurafsky@stanford.edu; marinka@cs.stanford.edu
FU NSF [IIS-1149837]; DARPA SIMPLEX; Stanford Data Science Initiative;
   Huawei; Chan Zuckerberg Biohub; SAP Stanford Graduate Fellowship; NSERC
   PGS-D grant
FX The authors thank Alex Ratner, Stephen Bach, and Michele Catasta for
   their helpful discussions and comments on early drafts. This research
   has been supported in part by NSF IIS-1149837, DARPA SIMPLEX, Stanford
   Data Science Initiative, Huawei, and Chan Zuckerberg Biohub. WLH was
   also supported by the SAP Stanford Graduate Fellowship and an NSERC
   PGS-D grant.
CR Abiteboul Serge, 1995, FDN DATABASES LOGICA
   Ashburner M, 2000, NAT GENET, V25, P25, DOI 10.1038/75556
   Bach SH, 2017, J MACH LEARN RES, V18
   Berant J., 2013, EMNLP
   Bordes A., 2014, EMNLP
   Bordes A., 2013, NIPS
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Brown AS, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.29
   Cavallo R., 1987, VLDB
   Chatr-aryamontri A, 2015, NUCLEIC ACIDS RES, V43, pD470, DOI 10.1093/nar/gku1204
   Cohen W., 2016, ARXIV160506523
   Dalvi N., 2007, VLDB
   Das R., 2016, EACL
   Das R., 2018, ICLR
   Demeester T., 2016, EMNLP
   Getoor L, 2007, INTRO STAT RELATIONA
   Gilmer J., 2017, ICML
   Guu Kelvin, 2015, EMNLP
   Hamilton W. L., 2017, IEEE DATA ENG B
   Hu Z., 2016, ACL
   Indyk P., 1998, ACM S THEOR COMP
   KAHN AB, 1962, COMMUN ACM, V5, P558, DOI 10.1145/368996.369025
   Krompass D, 2014, LECT NOTES COMPUT SC, V8797, P114, DOI 10.1007/978-3-319-11915-1_8
   Kuhn M, 2016, NUCLEIC ACIDS RES, V44, pD1075, DOI 10.1093/nar/gkv1075
   Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591
   Menche J, 2015, SCIENCE, V347, DOI 10.1126/science.1257601
   Neelakantan A., 2015, AAAI
   Nickel M., 2011, ICML
   Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592
   Paszke  A., 2017, NIPS AUT WORKSH
   Qi C. R., 2017, CVPR
   Queralt-Rosinach N., 2015, DATABASE, V2015
   Ramanathan G., 2015, AAAI SPRING SERIES
   Rocktaschel T., 2017, NIPS
   Rocktaschel T., 2017, ARXIV171209687
   Rocktaschel T., 2015, P C N AM CHAPT ASS C, P1119
   Rocktaschel Tim, 2014, P ACL 2014 WORKSH SE, P45
   Rolland T, 2014, CELL, V159, P1212, DOI 10.1016/j.cell.2014.10.050
   Szklarczyk D, 2017, NUCLEIC ACIDS RES, V45, pD362, DOI 10.1093/nar/gkw937
   Szklarczyk D, 2016, NUCLEIC ACIDS RES, V44, pD380, DOI 10.1093/nar/gkv1277
   Tatonetti NP, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3003377
   Thulasiraman K., 2011, GRAPHS THEORY ALGORI
   Wang M., 2017, NIPS
   Wu L., 2017, AAAI
   Yang B., 2015, ICLR
   Zaheer M., 2017, NIPS
   Zhang Y., 2018, AAAI
   Zhou T, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046115
   Zitnik M., 2018, BIOINFORMATICS
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302007
DA 2019-06-15
ER

PT S
AU Han, B
   Yao, QM
   Yu, XR
   Niu, G
   Xu, M
   Hu, WH
   Tsang, IW
   Sugiyama, M
AF Han, Bo
   Yao, Quanming
   Yu, Xingrui
   Niu, Gang
   Xu, Miao
   Hu, Weihua
   Tsang, Ivor W.
   Sugiyama, Masashi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Co-teaching: Robust Training of Deep Neural Networks with Extremely
   Noisy Labels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called "Co-teaching" for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.
C1 [Han, Bo; Yu, Xingrui; Tsang, Ivor W.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.
   [Han, Bo; Niu, Gang; Xu, Miao; Sugiyama, Masashi] RIKEN, Tokyo, Japan.
   [Yao, Quanming] 4Paradigm Inc, Beijing, Peoples R China.
   [Hu, Weihua] Stanford Univ, Stanford, CA 94305 USA.
   [Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan.
RP Han, B (reprint author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.; Han, B (reprint author), RIKEN, Tokyo, Japan.
FU JST CREST [JPMJCR1403]; ARC [FT130100746, DP180100106, LP150100671];
   NSFC [61671481]; NVIDIA Corporation; RIKEN-AIP
FX MS was supported by JST CREST JPMJCR1403. IWT was supported by ARC
   FT130100746, DP180100106 and LP150100671. BH would like to thank the
   financial support from RIKEN-AIP. XRY was supported by NSFC Project No.
   61671481. QY would give special thanks to Weiwei Tu and Yuqiang Chen
   from 4Paradigm Inc. We gratefully acknowledge the support of NVIDIA
   Corporation with the donation of the Titan Xp GPU used for this
   research.
CR Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Arpit Devansh, 2017, ICML
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Blum A, 2003, J ACM, V50, P506, DOI 10.1145/792538.792543
   Blum A., 1998, COLT
   Chapelle Olivier, 2006, IEEE T NEURAL NETWOR, V20, P542, DOI DOI 10.1109/TNN.2009.2015974
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   Deng J., 2013, CVPR
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fan Y., 2018, ICLR
   Freund Y., 1995, EUROPEAN COLT
   Freund Y., 1999, J JAPANESE SOC ARTIF, V14, P1612
   Goldberger J., 2017, ICLR
   Gong C., 2016, AAAI
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hinton G., 2015, ARXIV150302531
   Jiang L., 2018, ICML
   Kingma D. P., 2015, ICLR
   Kiryo R., 2017, NIPS
   Krizhevsky A., 2012, NIPS
   Laine S., 2017, ICLR
   Li  Yuncheng, 2017, ICCV
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Ma X., 2018, ICML
   Maas A. L., 2013, ICML
   Malach E., 2017, NIPS
   Masnadi-Shirazi H., 2009, NIPS
   Menon A., 2015, ICML
   Menon A., 2015, NIPS
   Miyato T., 2016, ICLR
   Natarajan N., 2013, NIPS
   Patrini G., 2017, CVPR
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Reed S., 2015, ICLR
   Ren M., 2018, ICML
   Rodrigues F., 2018, AAAI
   Sanderson T., 2014, AISTATS
   Szegedy C., 2016, CVPR
   Tanaka D., 2018, CVPR
   Veit  Andreas, 2017, CVPR
   Wang Y., 2018, CVPR
   Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1
   Yu X., 2018, ECCV
   Yu X., 2018, CVPR
   Zhang Chiyuan, 2017, ICLR
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003012
DA 2019-06-15
ER

PT S
AU Han, B
   Yao, JC
   Niu, G
   Zhou, MY
   Tsang, I
   Zhang, Y
   Sugiyama, M
AF Han, Bo
   Yao, Jiangchao
   Niu, Gang
   Zhou, Mingyuan
   Tsang, Ivorw.
   Zhang, Ya
   Sugiyama, Masashi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Masking: A New Perspective of Noisy Supervision
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called "Masking" that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly.
C1 [Han, Bo; Yao, Jiangchao; Tsang, Ivorw.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.
   [Han, Bo; Niu, Gang; Sugiyama, Masashi] RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan.
   [Yao, Jiangchao; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China.
   [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.
   [Sugiyama, Masashi] Univ Tokyo, Grad Sch Frontier Sci, Tokyo, Japan.
RP Han, B; Yao, JC (reprint author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.; Han, B (reprint author), RIKEN, Ctr Adv Intelligence Project, Tokyo, Japan.; Yao, JC (reprint author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China.
FU International Research Center for Neurointelligence (WPI-IRCN) at The
   University of Tokyo Institutes for Advanced Study; ARC [FT130100746,
   DP180100106, LP150100671]; U.S. National Science Foundation
   [IIS-1812699]; High Technology Research and Development Program of China
   [2015AA015801]; NSFC [61521062]; STCSM [18DZ2270700]; RIKEN-AIP;
   SJTU-CMIC; UTS-CAI
FX MS was supported by the International Research Center for
   Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for
   Advanced Study. IWT was supported by ARC FT130100746, DP180100106 and
   LP150100671. MZ acknowledges the support of Award IIS-1812699 from the
   U.S. National Science Foundation. YZ was supported by the High
   Technology Research and Development Program of China (2015AA015801),
   NSFC (61521062), and STCSM (18DZ2270700). BH would like to thank the
   financial support from RIKEN-AIP. JY would like to thank the financial
   support SJTU-CMIC and UTS-CAI. We gratefully acknowledge the support of
   NVIDIA Corporation with the donation of the Titan Xp GPU used for this
   research.
CR Ait-Sahalia Y, 2010, J AM STAT ASSOC, V105, P1504, DOI 10.1198/jasa.2010.tm10163
   Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Arpit Devansh, 2017, ICML
   Azadi  S., 2016, ICLR
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cha  Y., 2012, SIGIR
   Deng J., 2013, CVPR
   Dgani  Y., 2018, ISBI
   Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361
   Goldberger J., 2017, ICLR
   Goodfellow I., 2014, NIPS
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodman  N., 1952, PHILOS REV, P160
   Grigolini P, 2009, PHYSICA A, V388, P4192, DOI 10.1016/j.physa.2009.06.024
   Gulrajani  Ishaan, 2017, NIPS
   Han  B., 2016, ECML PKDD
   Hendrycks  D., 2018, NIPS
   Huang  J., 2007, NIPS
   Jiang L., 2018, ICML
   Laine S., 2017, ICLR
   Li W., 2017, ARXIV170802862
   Li  Yuncheng, 2017, ICCV
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Liu  W., 2011, CVPR
   Ma X., 2018, ICML
   Malach E., 2017, NIPS
   Masnadi-Shirazi H., 2009, NIPS
   Menon A., 2015, ICML
   Michalski R.S, 2013, MACHINE LEARNING ART
   Miyato T., 2016, ICLR
   Natarajan N., 2013, NIPS
   Patrini G., 2017, CVPR
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Reed S., 2015, ICLR
   Ren M., 2018, ICML
   Rockova  V., 2017, ARXIV170800085
   Rodrigues F., 2018, AAAI
   Sanderson T., 2014, AISTATS
   Sukhbaatar  S., 2015, ICLR WORKSH
   Tanaka D., 2018, CVPR
   Tarvainen  A., 2017, NIPS
   Tran  D., 2017, NIPS
   Veit  Andreas, 2017, CVPR
   Wang Y., 2018, CVPR
   Welinder P., 2010, NIPS
   Xiao  T., 2015, CVPR
   Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1
   Zhang Chiyuan, 2017, ICLR
   Zhang  Z., 2018, NIPS
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000035
DA 2019-06-15
ER

PT S
AU Han, I
   Avron, H
   Shin, J
AF Han, Insu
   Avron, Haim
   Shin, Jinwoo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Stochastic Chebyshev Gradient Descent for Spectral Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID LOW-RANK; MATRIX
AB A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. Unfortunately, computing the gradient of a spectral function is generally of cubic complexity, as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus, one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However, here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper, we develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are variance-optimal, which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums, and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments.
C1 [Han, Insu; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
   [Avron, Haim] Tel Aviv Univ, Dept Appl Math, Tel Aviv, Israel.
   [Shin, Jinwoo] AItrics, Seoul, South Korea.
RP Han, I (reprint author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
EM insu.han@kaist.ac.kr; haimav@post.tau.ac.il; jinwoos@kaist.ac.kr
FU National Research Foundation of Korea(NRF) - Korea government(MS IT)
   [2018RIA5A1059921]; Israel Science Foundation [1272/17]
FX This work was supported by the National Research Foundation of
   Korea(NRF) grant funded by the Korea government(MS IT)
   (2018RIA5A1059921). Haim Avron acknowledges the support of the Israel
   Science Foundation (grant no. 1272/17)
CR Adams Ryan P, 2018, ARXIV180203451
   Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Broniatowski Michel, 2014, ARXIV14035113
   Budincsevity Norbert, 2016, WEATHER SZEGED 2006
   DAVIDSON ER, 1975, J COMPUT PHYS, V17, P87, DOI 10.1016/0021-9991(75)90065-0
   Dong Kun, 2017, ADV NEURAL INFORM PR, P6330
   Friedlander MP, 2016, SIAM J SCI COMPUT, V38, pA1616, DOI 10.1137/15M1034283
   Garber D., 2015, ARXIV150905647
   Han I, 2015, ICML, P908
   Han I, 2017, SIAM J SCI COMPUT, V39, pA1558, DOI 10.1137/16M1078148
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   HUTCHINSON MF, 1989, COMMUN STAT SIMULAT, V18, P1059, DOI 10.1080/03610918908812806
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Koltchinskii V, 2015, J MACH LEARN RES, V16, P1757
   Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68
   Lewis AS, 1996, MATH OPER RES, V21, P576, DOI 10.1287/moor.21.3.576
   Lu CY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2380155
   Mason J. C, 2002, CHEBYSHEV POLYNOMIAL
   Mohan K, 2012, J MACH LEARN RES, V13, P3441
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roosta-Khorasani F, 2015, FOUND COMPUT MATH, V15, P1187, DOI 10.1007/s10208-014-9220-1
   Trefethen L., 2013, APPROXIMATION THEORY
   Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974
   Vinck M, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.051139
   Wilson A., 2015, INT C MACH LEARN, P1775
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P1080
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001090
DA 2019-06-15
ER

PT S
AU Han, K
   Wen, HG
   Zhang, YZ
   Fu, D
   Culurciello, E
   Liu, ZM
AF Han, Kuan
   Wen, Haiguang
   Zhang, Yizhen
   Fu, Di
   Culurciello, Eugenio
   Liu, Zhongming
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Predictive Coding Network with Local Recurrent Processing for
   Object Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NEURAL-NETWORKS; VISUAL-CORTEX; VISION; MODEL
AB Inspired by "predictive coding" - a theory in neuroscience, we develop a bidirectional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to refine representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classification with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.
C1 [Han, Kuan; Wen, Haiguang; Zhang, Yizhen; Fu, Di; Culurciello, Eugenio; Liu, Zhongming] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Culurciello, Eugenio; Liu, Zhongming] Purdue Univ, Weldon Sch Biomed Engn, W Lafayette, IN 47907 USA.
   [Han, Kuan; Wen, Haiguang; Zhang, Yizhen; Fu, Di; Liu, Zhongming] Purdue Univ, Purdue Inst Integrat Neurosci, W Lafayette, IN 47907 USA.
RP Liu, ZM (reprint author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.; Liu, ZM (reprint author), Purdue Univ, Weldon Sch Biomed Engn, W Lafayette, IN 47907 USA.; Liu, ZM (reprint author), Purdue Univ, Purdue Inst Integrat Neurosci, W Lafayette, IN 47907 USA.
EM zmliu@purdue.edu
FU NIH [R01MH104402]; College of Engineering at Purdue University
FX The research was supported by NIH R01MH104402 and the College of
   Engineering at Purdue University.
CR Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Boehler CN, 2008, P NATL ACAD SCI USA, V105, P8742, DOI 10.1073/pnas.0801999105
   Bressler SL, 2008, J NEUROSCI, V28, P10056, DOI 10.1523/JNEUROSCI.1776-08.2008
   Buffalo EA, 2010, P NATL ACAD SCI USA, V107, P361, DOI 10.1073/pnas.0907658106
   Camprodon JA, 2010, J COGNITIVE NEUROSCI, V22, P1262, DOI 10.1162/jocn.2009.21253
   Chen Y, 2017, ADV NEURAL INFORM PR, P4470
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   DeVries T, 2017, ARXIV170804552
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Dumoulin V., 2016, ARXIV160307285
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Garcia-Garcia A., 2017, ARXIV170406857
   George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532
   Goodfellow I.J., 2013, ARXIV13024389
   Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015
   Han Kuan, 2017, BIORXIV
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G. E, 2012, ARXIV12070580
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Huang YP, 2011, WIRES COGN SCI, V2, P580, DOI 10.1002/wcs.142
   Ioffe S., 2015, ARXIV150203167
   Jastrzebski Stanislaw, 2017, ARXIV171004773
   Jehee JFM, 2006, J PHYSIOLOGY-PARIS, V100, P125, DOI 10.1016/j.jphysparis.2006.09.011
   Koivisto M, 2011, J NEUROSCI, V31, P2488, DOI 10.1523/JNEUROSCI.3074-10.2011
   Kriegeskorte N, 2015, ANNU REV VIS SC, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Larsson G., 2016, ARXIV160507648
   Lee C. Y., 2015, ARTIF INTELL, P562
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao Qianli, 2016, ARXIV160403640
   Lin M., 2013, ARXIV13124400
   Lotter W., 2016, ARXIV160508104
   Lu Jiasen, 2017, IEEE C COMP VIS PATT, V6, P2
   McClure Patrick, 2018, BIORXIV
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   O'Reilly RC, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00124
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Romero Adriana, 2014, ARXIV14126550
   Seeliger K, 2018, NEUROIMAGE, V181, P775, DOI 10.1016/j.neuroimage.2018.07.043
   Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8
   Shen Guohua, 2017, BIORXIV
   Shi JX, 2018, HUM BRAIN MAPP, V39, P2269, DOI 10.1002/hbm.24006
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Simonyan K, 2014, ARXIV14091556
   Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551
   Spratling MW, 2017, COGN COMPUT, V9, P151, DOI 10.1007/s12559-016-9445-1
   Spratling MW, 2017, BRAIN COGNITION, V112, P92, DOI 10.1016/j.bandc.2015.11.003
   Spratling MW, 2012, NEURAL COMPUT, V24, P60, DOI 10.1162/NECO_a_00222
   Spratling MW, 2008, FRONT COMPUT NEUROSC, V2, P1, DOI 10.3389/neuro.10.004.2008
   Srivastava R.K., 2015, ADV NEURAL INFORM PR, P2377
   St-Yves Ghislain, 2018, BIORXIV
   Stollenga MF, 2014, ADV NEURAL INFORM PR, P3545
   Szegedy C., 2015, CVPR
   VANESSEN DC, 1983, TRENDS NEUROSCI, V6, P370, DOI 10.1016/0166-2236(83)90167-4
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268
   Wen Haiguang, 2018, ARXIV180204762
   Wyatte D, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00674
   Wyatte D, 2012, J COGNITIVE NEUROSCI, V24, P2248, DOI 10.1162/jocn_a_00282
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yang Yibo, 2018, ARXIV180210419
   Zagoruyko S, 2016, ARXIV160507146
   Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196
NR 70
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003073
DA 2019-06-15
ER

PT S
AU Han, YJ
   Jiao, JT
   Lee, CZ
   Weissman, T
   Wu, YH
   Yu, TC
AF Han, Yanjun
   Jiao, Jiantao
   Lee, Chuan-Zheng
   Weissman, Tsachy
   Wu, Yihong
   Yu, Tiancheng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Entropy Rate Estimation for Markov Chains with Large State Space
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FUNCTIONALS
AB Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on S elements with independent samples, the optimal sample complexity scales sublinearly with S as Theta(S/log S) as shown by Valiant and Valiant [4]. Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with S states from a sample path of n observations. We show that
   Provided the Markov chain mixes not too slowly, i.e., the relaxation time is at most O(S/ln(3)S), consistent estimation is achievable when n >> S-2/log S.
   Provided the Markov chain has some slight dependency, i.e., the relaxation time is at least 1+Omega(ln(2) S/root S), consistent estimation is impossible when n less than or similar to S-2/logS.
   Under both assumptions, the optimal estimation accuracy is shown to be Theta(S-2/n log S). In comparison, the empirical entropy rate requires at least Omega(S-2) samples to be consistent, even when the Markov chain is memoryless. In addition to synthetic experiments, we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora, which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure.
C1 [Han, Yanjun; Lee, Chuan-Zheng; Weissman, Tsachy] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Jiao, Jiantao] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Wu, Yihong] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06511 USA.
   [Yu, Tiancheng] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
RP Han, YJ (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM yjhan@stanford.edu; jiantao@berkeley.edu; czlee@stanford.edu;
   tsachy@stanford.edu; yihong.wu@yale.edu; thueeyutc14@foxmail.com
CR Acharya J., 2017, P INT C MACH LEARN, V70, P11
   Antos A, 2001, RANDOM STRUCT ALGOR, V19, P163, DOI 10.1002/rsa.10019
   BILLINGSLEY P, 1961, ANN MATH STAT, V32, P12, DOI 10.1214/aoms/1177705136
   Bordenave C, 2010, ALEA-LAT AM J PROBAB, V7, P41
   Brown P. F., 1992, Computational Linguistics, V18, P31
   Cai HX, 2004, IEEE T INFORM THEORY, V50, P1551, DOI 10.1109/TIT.2004.830771
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Ciuperca Gabriela, 2005, P INT S APPL STOCH M
   Cover T. M., 2006, ELEMENTS INFORM THEO
   COVER TM, 1978, IEEE T INFORM THEORY, V24, P413, DOI 10.1109/TIT.1978.1055912
   Daskalakis Constantinos, 2017, ARXIV170406850
   Effros M, 2002, IEEE T INFORM THEORY, V48, P1061, DOI 10.1109/18.995542
   Falahatgar M, 2016, IEEE INT SYMP INFO, P2689, DOI 10.1109/ISIT.2016.7541787
   Han Yanjun, 2017, ARXIV171102141
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Hsu D. J., 2015, ADV NEURAL INFORM PR, V28, P1459
   Jiang Qian, 2009, CONSTRUCTION TRANSIT
   Jiao JT, 2017, IEEE T INFORM THEORY, V63, P6774, DOI 10.1109/TIT.2017.2733537
   Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]
   Jiao JT, 2013, IEEE T INFORM THEORY, V59, P6220, DOI 10.1109/TIT.2013.2267934
   Jurafsky D., 2009, SPEECH LANGUAGE PROC
   Kamath S, 2016, IEEE INT SYMP INFO, P685, DOI 10.1109/ISIT.2016.7541386
   KIEFFER JC, 1991, IEEE T INFORM THEORY, V37, P263, DOI 10.1109/18.75241
   Kontoyiannis I, 1998, IEEE T INFORM THEORY, V44, P1319, DOI 10.1109/18.669425
   Krumme C, 2013, SCI REP-UK, V3, DOI 10.1038/srep01645
   Kuchaiev Oleksii, 2017, ABS170310722 CORR
   Lanctot JK, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P409
   Levin David A, 2016, ARXIV161205330
   Mitzenmacher M., 2005, PROBABILITY COMPUTIN
   Montenegro R, 2006, FOUND TRENDS THEOR C, V1, P237, DOI 10.1561/0400000003
   Paninski L, 2004, IEEE T INFORM THEORY, V50, P2200, DOI 10.1109/TIT.2004.833360
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Paulin D, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-4039
   SHANNON CE, 1951, BELL SYST TECH J, V30, P50, DOI 10.1002/j.1538-7305.1951.tb01366.x
   Shields Paul C, 1996, GRADUATE STUDIES MAT
   Song CM, 2010, SCIENCE, V327, P1018, DOI 10.1126/science.1177170
   Takaguchi T, 2011, PHYS REV X, V1, DOI 10.1103/PhysRevX.1.011008
   Tao T, 2012, TOPICS RANDOM MATRIX, V132
   Tatwawadi Kedar, 2018, ARXIV180501355
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Valiant G, 2011, ACM S THEORY COMPUT, P685
   Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81
   Valiant P., 2013, P ADV NEUR INF PROC, P2157
   Wang Chunyan, 2012, SCI REPORTS, V2
   Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468
   WYNER AD, 1989, IEEE T INFORM THEORY, V35, P1250, DOI 10.1109/18.45281
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
   Zoph B., 2016, ABS161101578 CORR
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004035
DA 2019-06-15
ER

PT S
AU Han, YL
   Gmytrasiewicz, P
AF Han, Yanlin
   Gmytrasiewicz, Piotr
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Others' Intentional Models in Multi-Agent Settings Using
   Interactive POMDPs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.
C1 [Han, Yanlin; Gmytrasiewicz, Piotr] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
RP Han, YL (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM yhan37@uic.edu; piotr@uic.edu
CR Ballard Dana, 1996, THESIS
   Del Moral P., 1996, MARKOV PROCESS RELAT, V2, P555
   Doshi Prashant, 2009, J ARTIFICIAL INTELLI, V34, P297
   Doshi-Velez F, 2015, IEEE T PATTERN ANAL, V37, P394, DOI 10.1109/TPAMI.2013.191
   Doucet A, 2001, STAT ENG IN, P3
   Fudenberg D., 1998, THEORY LEARNING GAME, V2
   Gilks W. R., 1996, MARKOV CHAIN MONTE C, V1, P19
   Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579
   Gmytrasiewicz PJ, 2000, AUTON AGENT MULTI-AG, V3, P319, DOI 10.1023/A:1010028119149
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Harsanyi JC, 1967, MANAGE SCI, V14, P159, DOI DOI 10.1287/MNSC.14.3.159
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   Karkus Peter, 2017, ADV NEURAL INFORM PR, P4697
   Liu Miao, 2011, P INT C MACH LEARN, P769
   Panella A., 2016, P 13 AAAI C ART INT, P2530
   PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000016
DA 2019-06-15
ER

PT S
AU Hand, P
   Leong, O
   Voroninski, V
AF Hand, Paul
   Leong, Oscar
   Voroninski, Vladislav
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Phase Retrieval Under a Generative Prior
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SIGNAL RECOVERY; CRYSTALLOGRAPHY
AB We introduce a novel deep learning inspired formulation of the phase retrieval problem, which asks to recover a signal y(0) is an element of 2 R-n from m quadratic observations, under structural assumptions on the underlying signal. As is common in many imaging problems, previous methodologies have considered natural signals as being sparse with respect to a known basis, resulting in the decision to enforce a generic sparsity prior. However, these methods for phase retrieval have encountered possibly fundamental limitations, as no computationally efficient algorithm for sparse phase retrieval has been proven to succeed with fewer than O (k(2) log n) generic measurements, which is larger than the theoretical optimum of O (k log n). In this paper, we propose a new framework for phase retrieval by modeling natural signals as being in the range of a deep generative neural network G : R-k -> R-n. We introduce an empirical risk formulation that has favorable global geometry for gradient methods, as soon as m = O (kd(2) log n), under the model of a d-layer fully-connected neural network with random weights. Specifically, when suitable deterministic conditions on the generator and measurement matrix are met, we construct a descent direction for any point outside of a small neighborhood around the true k-dimensional latent code and a negative multiple thereof. This formulation for structured phase retrieval thus benefits from two effects: generative priors can more tightly represent natural signals than sparsity priors, and this empirical risk formulation can exploit those generative priors at an information theoretically optimal sample complexity, unlike for a sparsity prior. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms both sparse and general phase retrieval methods.
C1 [Hand, Paul] Northeastern Univ, Boston, MA 02115 USA.
   [Leong, Oscar] Rice Univ, Houston, TX 77251 USA.
   [Voroninski, Vladislav] Helm Ai, Menlo Pk, CA USA.
RP Hand, P (reprint author), Northeastern Univ, Boston, MA 02115 USA.
EM p.hand@northeastern.edu; oscar.f.leong@rice.edu; vlad@helm.ai
FU NSF Graduate Research Fellowship [DGE-1450681]; NSF [DMS-1464525]
FX OL acknowledges support by the NSF Graduate Research Fellowship under
   Grant No. DGE-1450681. PH acknowledges funding by the grant NSF
   DMS-1464525.
CR Arora  Sanjeev, 2015, ABS151105653 CORR
   Bahmani  S., 2015, ADV NEURAL INFORM PR, P523
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Bora A., 2017, ARXIV170303208
   Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903
   Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443
   Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Candes Emmanuel J., 2017, IEEE T INFORM THEORY, V61, P195
   Chandra  Rohan, 2017, AS C SIGN SYST COMP
   Dainty J.C., 1987, IMAGE RECOVERY THEOR, P231
   Drury S. W., 2001, HONOURS ANAL LECT NO
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   GERCHBERG RW, 1972, OPTIK, V35, P237
   Goldstein T., 2016, ARXIV161007531
   Gunaydin  Harun, 2017, ARXIV170504286
   Hand  Paul, 2016, ARXIV161105985
   Hand Paul, 2017, ARXIV170507576
   HARRISON RW, 1993, J OPT SOC AM A, V10, P1046, DOI 10.1364/JOSAA.10.001046
   Jaganathan K, 2013, IEEE INT SYMP INFO, P1022, DOI 10.1109/ISIT.2013.6620381
   Kingma D. P., 2014, ARXIV14126980
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee  Jaehoon, 2018, INT C LEARN REPR ICL
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mark A, 2013, PROOF RIP SUBGAUSSIA
   Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642
   MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394
   Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442
   Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725
   Vershynin R., 2012, COMPRESSED SENSING T
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Walther A., 1963, Optica Acta, V10, P41
   Wang G, 2018, IEEE T INFORM THEORY, V64, P773, DOI 10.1109/TIT.2017.2756858
   Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733
   Wendel J. G., 1962, MATH SCAND, V11, P109
   Yeh LH, 2015, OPT EXPRESS, V23, P33214, DOI 10.1364/OE.23.033214
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003067
DA 2019-06-15
ER

PT S
AU Hanin, B
   Rolnick, D
AF Hanin, Boris
   Rolnick, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI How to Start Training: The Effect of Initialization and Architecture
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We identify and study two common failure modes for early training in deep ReLU nets. For each, we give a rigorous proof of when it occurs and how to avoid it, for fully connected, convolutional, and residual architectures. We show that the first failure mode, exploding or vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly scaling the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.
C1 [Hanin, Boris] Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.
   [Rolnick, David] MIT, Dept Math, Cambridge, MA 02139 USA.
RP Hanin, B (reprint author), Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.
EM bhanin@math.tamu.edu; drolnick@mit.edu
CR Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Arpit Devansh, 2017, ARXIV170605394
   Billingsley P., 2008, PROBABILITY MEASURE
   Chollet  F., 2018, KERAS
   Ge Rong, 2017, ARXIV170400708
   Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221
   Hanin Boris, 2018, ADV NEURAL INFORM PR
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henaff M., 2016, INT C MACH LEARN, P2034
   Ioffe S., 2015, ARXIV150203167
   JIN C, 2017, ARXIV170300887
   Jing  L., 2017, P 34 INT C MACH LEAR, V70, P1733
   Kadmon J, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.041030
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Klambauer G., 2017, ADV NEURAL INFORM PR, P972
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Le Quoc V, 2015, ARXIV150400941
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pennington  Jeffrey, 2017, ADV NEURAL INFORM PR, P4788
   Poole B, 2016, ADV NEURAL INFORM PR, V29, P3360
   Raghu M., 2017, P 34 INT C MACH LEAR, P2847
   Saxe A.M., 2013, ARXIV13126120
   Schoenholz Samuel S, 2016, ARXIV161101232
   Schoenholz  SamuelS, 2017, ARXIV171006570
   Shalev-Shwartz Shai, 2017, P INT C MACH LEARN, V70, P3067
   Shanmugam R., 2015, STAT SCI ENG
   Taki Masato, 2017, ARXIV170902956
   Veit A., 2016, ADV NEURAL INFORM PR, P550
   Wu Yuhuai, 2017, ADV NEURAL INFORM PR, V30, P5285
   Yang  G., 2017, ADV NEURAL INFORM PR, P7103
   Yang Greg, 2018, WORKSH INT C LEARN R
   Zhang Chiyuan, 2017, INT C LEARN REPR
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300053
DA 2019-06-15
ER

PT S
AU Hanin, B
AF Hanin, Boris
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Which Neural Net Architectures Give Rise to Exploding and Vanishing
   Gradients?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta, given by the sum of the reciprocals of the hidden layer widths. When beta is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.
C1 [Hanin, Boris] Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.
RP Hanin, B (reprint author), Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.
EM bhanin@math.tamu.edu
CR [Anonymous], 1991, SEPP HOCHREITERS FUN
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Balduzzi D., 2017, ARXIV170208591
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Choromanska A., 2015, ARTIF INTELL, P192
   Hanin  Boris, 2018, ADV NEURAL INFORM PR, P32
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henaff M., 2016, INT C MACH LEARN, P2034
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Hochreiter S., 1991, UNTERSUCHUNGEN DYNAM, P91
   Klambauer G., 2017, ADV NEURAL INFORM PR, P972
   Mishkin Dmytro, 2015, ARXIV151106422
   Pennington  Jeffrey, 2018, ARXIV180209979
   Pennington  Jeffrey, 2017, ADV NEURAL INFORM PR, P4788
   Poole B, 2016, ADV NEURAL INFORM PR, V29, P3360
   Raghu M., 2017, P 34 INT C MACH LEAR, P2847
   Schoenholz  SamuelS, 2017, ARXIV171006570
   Srivastava R. K., 2015, HIGHWAY NETWORKS
   Xie Di, 2017, ARXIV170301827
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300054
DA 2019-06-15
ER

PT S
AU Hannah, R
   Liu, YL
   O'Connor, D
   Yin, W
AF Hannah, Robert
   Liu, Yanli
   O'Connor, Daniel
   Yin, Wotao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Breaking the Span Assumption Yields Fast Finite-Sum Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of n smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the "span assumption": Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number kappa = O(n), the span assumption prevents algorithms from converging to an approximate solution of accuracy epsilon in less than n ln(1/epsilon) iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to Omega (1 + (ln(n/kappa))+) times faster. In particular, to obtain an accuracy epsilon = 1/n(alpha) for kappa = n(beta) and alpha, beta is an element of (0, 1), modified SVRG requires O(n) iterations, whereas algorithms that follow the span assumption require O(n ln(n)) iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield faster algorithms in the big data regime.
C1 [Hannah, Robert] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.
   Univ San Francisco, Dept Math, San Francisco, CA 94117 USA.
RP Hannah, R (reprint author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.
EM RobertHannah89@gmail.com; yanli@math.ucla.edu;
   daniel.v.oconnor@gmail.com; WotaoYin@math.ucla.edu
FU AFOSR MURI [FA9550-18-1-0502]; NSF [DMS-1720237]; ONR [N000141712162]
FX This work was supported in part by grants: AFOSR MURI FA9550-18-1-0502,
   NSF DMS-1720237, and ONR N000141712162.
CR Allen-Zhu Z., 2018, ARXIV180203866
   Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Arjevani Y., 2014, ARXIV14106387
   Arjevani Y., 2016, ARXIV160503529CSMATH
   Arjevani Y., 2016, ARXIV160609333CSMATH
   Defazio  A., 2014, P 31 INT C MACH LEAR, P1125
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Defazio Aaron, 2016, ADV NEURAL INFORM PR, P676
   Eberts M., 2011, ADV NEURAL INFORM PR, V24, P1539
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lan Guanghui, 2017, MATH PROGRAM, P1
   Lan  Guanghui, 2015, ARXIV150702000
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Lin H., 2015, ARXIV150602186
   Lin Q., 2014, ARXIV14071296
   Mairal J., 2013, P INT C MACH LEARN, P783
   Nesterov Yu, 2013, INTRO LECT CONVEX OP
   Nguyen L. M., 2017, ARXIV170300102CSMATH
   Shalev- Shwartz S., 2015, ARXIV150206177
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Sridharan K., 2009, ADV NEURAL INFORM PR, V21, P1545
   Woodworth B., 2016, ARXIV160508003
   Xiao L., 2014, ARXIV14034699
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302033
DA 2019-06-15
ER

PT S
AU Hansen, SS
   Sprechmann, P
   Pritzel, A
   Barreto, A
   Blundell, C
AF Hansen, Steven S.
   Sprechmann, Pablo
   Pritzel, Alexander
   Barreto, Andre
   Blundell, Charles
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast deep reinforcement learning using online adjustments from the past
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer. EVA shifts the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning. We show that EVA is performant on a demonstration task and Atari games.
C1 [Hansen, Steven S.; Sprechmann, Pablo; Pritzel, Alexander; Barreto, Andre; Blundell, Charles] DeepMind, London, England.
RP Hansen, SS (reprint author), DeepMind, London, England.
EM stevenhansen@google.com; psprechmann@google.com; apritzel@google.com;
   andrebarreto@google.com; cblundell@google.com
CR Bakker B., 2003, P 2003 IEEE RSJ INT, V1, P430
   Barth-Maron Gabriel, 2018, ARXIV180408617
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Blundell C., 2016, ARXIV160604460
   Brea Johanni, 2017, ARXIV171106677
   Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429
   Espeholt Lasse, 2018, ARXIV180201561
   Gabel T, 2005, LECT NOTES ARTIF INT, V3620, P206
   Gershman SJ, 2017, ANNU REV PSYCHOL, V68, P101, DOI 10.1146/annurev-psych-122414-033625
   Grave E., 2016, ARXIV161204426
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hausknecht M.J., 2015, ARXIV150706527
   Kaiser Lukasz, 2016, LEARNING REMEMBER RA
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   Merity S., 2016, ARXIV160907843
   Mnih  V., 2016, INT C MACH LEARN
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960
   Munos Remi, 1998, NIPS, P1024
   Nishio Daichi, 2018, ARXIV180101968
   Oh  J., 2016, P 33 INT C MACH LEAR, P2790
   Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829
   Pritzel Alexander, 2017, ICML
   Santamaria JC, 1997, ADAPT BEHAV, V6, P163, DOI 10.1177/105971239700600201
   Sarkin Jain Mika, 2018, ICLR 2018 WORKSH
   Savinov N., 2018, ARXIV180300653
   Schaul Tom, 2015, CORR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Silver David, 2008, P 25 INT C MACH LEAR, P968
   Sprechmann Pablo, 2018, ICLR
   Stepleton Tom, 2017, PYCOLAB GAME ENGINE
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wayne Greg, 2018, ARXIV180310760
   Xiao Chenjun, 2018, AAAI
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005017
DA 2019-06-15
ER

PT S
AU Hanzely, F
   Mishchenko, K
   Richtarik, P
AF Hanzely, Filip
   Mishchenko, Konstantin
   Richtarik, Peter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI SEGA: Variance Reduction via Gradient Sketching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COORDINATE DESCENT; OPTIMIZATION
AB We propose a randomized first order optimization method-SEGA (SkEtched GrAdient)-which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient. In each iteration, SEGA updates the current estimate of the gradient through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent.
C1 [Hanzely, Filip; Mishchenko, Konstantin; Richtarik, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
   [Richtarik, Peter] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Richtarik, Peter] Moscow Inst Phys & Technol, Dolgoprudnyi, Russia.
RP Hanzely, F (reprint author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
CR Allen-Zhu  Z, 2016, P 33 INT C MACH LEAR, P1110
   Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Allen- Zhu Zeyuan, 2017, INNOVATIONS THEORETI
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bergou El Houcine, 2018, RANDOM DIRECT UNPUB
   Chambolle Antonin, 2018, SIAM J OPTIMIZ, V28
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1
   D'Aspremont A, 2008, SIAM J OPTIMIZ, V19, P1171, DOI 10.1137/060676386
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Devolder O, 2014, MATH PROGRAM, V146, P37, DOI 10.1007/s10107-013-0677-5
   Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993
   Gower R., 2016, JMLR WORKSHOP C P, P1869
   Gower RM, 2017, SIAM J MATRIX ANAL A, V38, P1380, DOI 10.1137/16M1062053
   Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487
   Gower Robert M, 2018, ARXIV180502632
   Gower Robert M, 2018, ARXIV180204079
   Gower Robert M, 2016, ARXIV161206255
   Gower Robert Mansel, 2015, ARXIV151206890
   Hanzely Filip, 2018, ARXIV180909354
   HOOKE R, 1961, J ACM, V8, P212, DOI 10.1145/321062.321069
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Kolda TG, 2003, SIAM REV, V45, P385, DOI 10.1137/S0036144502428893
   Konecny Jakub, 2014, ARXIV14100390
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Lin Q., 2014, ADV NEURAL INFORM PR, V27, P3059
   Loizou Nicolas, 2017, NIPS WORKSH OPT MACH
   Loizou Nicolas, 2017, ARXIV171209677
   Necoara Ion, 2018, ARXIV180104873
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nguyen L. M., 2017, P 34 INT C MACH LEAR, P2613
   Qu Z., 2015, ADV NEURAL INFORM PR, P865
   Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360
   Qu Z, 2016, OPTIM METHOD SOFTW, V31, P858, DOI 10.1080/10556788.2016.1190361
   Qu Zheng, 2016, P 33 INT C MACH LEAR, V48, P1823
   Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1
   Richtarik Peter, 2017, ARXIV170601108
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt  M., 2011, ADV NEURAL INFORM PR, P1458
   Shalev-Shwartz S., 2012, ARXIV12112717
   Tu Stephen, 2017, P 34 INT C MACH LEAR, V70, P3482
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302012
DA 2019-06-15
ER

PT S
AU Hao, Y
   Orlitsky, A
   Suresh, AT
   Wu, YH
AF Hao, Yi
   Orlitsky, Alon
   Suresh, Ananda T.
   Wu, Yihong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Data Amplification: A Unified and Competitive Approach to Property
   Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NONPARAMETRIC-ESTIMATION; MINIMAX ESTIMATION; ENTROPY; NUMBER
AB Estimating properties of discrete distributions is a fundamental problem in statistical learning. We design the first unified, linear-time, competitive, property estimator that for a wide class of properties and for all underlying distributions uses just 2n samples to achieve the performance attained by the empirical estimator with n root log n samples. This provides off-the-shelf, distribution-independent, "amplification" of the amount of data available relative to common-practice estimators. We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases, its performance with n samples is even as good as that of the empirical estimator with n log n samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property.
C1 [Hao, Yi; Orlitsky, Alon] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
   [Suresh, Ananda T.] Google Res, New York, NY 10011 USA.
   [Wu, Yihong] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06511 USA.
RP Hao, Y (reprint author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM yih179@eng.ucsd.edu; alon@eng.ucsd.edu; theertha@google.com;
   yihong.wu@yale.edu
CR Acharya J., 2017, P INT C MACH LEARN, V70, P11
   Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435
   Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86
   Bustamante J, 2017, BERNSTEIN OPERATORS
   CANONNE C. L, 2017, SURVEY DISTRIBUTION
   CARLTON AG, 1969, PSYCHOL BULL, V71, P108, DOI 10.1037/h0026857
   CHAO A, 1984, SCAND J STAT, V11, P265
   CHAO A., 2005, ENCY STAT SCI
   CHUNG F. R, 2017, COMPLEX GRAPHS NETWO
   Colwell RK, 2012, J PLANT ECOL-UK, V5, P3, DOI 10.1093/jpe/rtr044
   Cover T. M., 2012, ELEMENTS INFORM THEO
   GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344
   Haas P. J., 1995, VLDB '95. Proceedings of the 21st International Conference on Very Large Data Bases, P311
   HAN Y, 2018, ARXIV180208405
   Hao Y, 2018, IEEE INT SYMP INFO, P1076, DOI 10.1109/ISIT.2018.8437702
   Ionita-Laza I, 2009, P NATL ACAD SCI USA, V106, P5008, DOI 10.1073/pnas.0807815106
   Jiao JT, 2016, IEEE INT SYMP INFO, P750
   Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]
   KAMATH S, 2015, C LEARN THEOR, P1066
   KORNEICHUK E N. P, 1991, EXACT CONSTANTS APPR, V38
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lehmann EL, 2006, TESTING STAT HYPOTHE
   Loh WY, 2011, WIRES DATA MIN KNOWL, V1, P14, DOI 10.1002/widm.8
   MCNEIL DR, 1973, J AM STAT ASSOC, V68, P92, DOI 10.2307/2284147
   Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Renyi A., 1961, MEASURES ENTROPY INF
   Sarndal C.-E., 2003, MODEL ASSISTED SURVE
   SMITH EP, 1984, BIOMETRICS, V40, P119, DOI 10.2307/2530750
   TIMAN A. F, 2014, THEORY APPROXIMATION
   Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81
   Valiant P., 2013, P ADV NEUR INF PROC, P2157
   Watson GN, 1995, TREATISE THEORY BESS
   WU Y, 2015, ARXIV150401227
   Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003039
DA 2019-06-15
ER

PT S
AU Hao, Y
   Orlitsky, A
   Pichapati, V
AF Hao, Yi
   Orlitsky, Alon
   Pichapati, Venkatadheeraj
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On Learning Markov Chains
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The problem of estimating an unknown discrete distribution from its samples is a fundamental tenet of statistical learning. Over the past decade, it attracted significant research effort and has been solved for a variety of divergence measures. Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider two problems related to the min-max risk (expected loss) of estimating an unknown k-state Markov chain from its n sequential samples: predicting the conditional distribution of the next sample with respect to the KL-divergence, and estimating the transition matrix with respect to a natural loss induced by KL or a more general f -divergence measure. For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is Omega(k log log n/n) and O(k(2) log log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f -divergences, including KL-, L-2-, Chi-squared, Hellinger, and Alpha-divergences.
C1 [Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
RP Hao, Y (reprint author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM yih179@ucsd.edu; alon@ucsd.edu; dheerajpv7@ucsd.edu
CR AitSahlia Farid, 2012, ELEMENTARY PROBABILI
   Bishop C. M., 2006, PATTERN RECOGNITION
   Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010
   Braess D, 2002, LECT NOTES ARTIF INT, V2533, P380
   Chung F., 2006, COMPLEX GRAPHS NETWO
   COVER TM, 1972, IEEE T INFORM THEORY, V18, P216, DOI 10.1109/TIT.1972.1054738
   Crooks Gavin E., 2017, MEASURES ENTROPY INF, V4
   Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299
   Falahatgar M, 2016, IEEE INT SYMP INFO, P2689, DOI 10.1109/ISIT.2016.7541787
   Falahatgar Moein, 2016, NIPS, P4860
   GILBERT EN, 1971, IEEE T INFORM THEORY, V17, P304, DOI 10.1109/TIT.1971.1054638
   KAMATH S, 2015, C LEARN THEOR, P1066
   KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Levin D. A., 2017, MARKOV CHAINS MIXING
   Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355
   Nikulin M.S, 2001, ENCY MATH, V151
   Norris J, 2017, COMB PROBAB COMPUT, V26, P603, DOI 10.1017/S0963548317000074
   Orlitsky Alon, 2015, ADV NIPS, P2143
   Paninski L., 2004, P ADV NEUR INF PROC, P1033
   Valiant G, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P142, DOI 10.1145/2897518.2897641
   Wolfer Geoffrey, 2018, ARXIV180905014
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300060
DA 2019-06-15
ER

PT S
AU Harer, JA
   Ozdemir, O
   Lazovich, T
   Reale, CP
   Russell, RL
   Kim, LY
   Chin, P
AF Harer, Jacob A.
   Ozdemir, Onur
   Lazovich, Tomo
   Reale, Christopher P.
   Russell, Rebecca L.
   Kim, Louis Y.
   Chin, Peter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Repair Software Vulnerabilities with Generative Adversarial
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.
C1 [Harer, Jacob A.; Ozdemir, Onur; Lazovich, Tomo; Reale, Christopher P.; Russell, Rebecca L.; Kim, Louis Y.] Draper, Cambridge, MA 02139 USA.
   [Harer, Jacob A.; Chin, Peter] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.
   [Lazovich, Tomo] Lightmatter, Boston, MA USA.
RP Harer, JA (reprint author), Draper, Cambridge, MA 02139 USA.; Harer, JA (reprint author), Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.
EM jharer@draper.com; oozdemir@draper.com; tomo@lightmatter.ai;
   creale@draper.com; rrussell@draper.com; lkim@draper.com;
   spchin@cs.bu.edu
FU Air Force Research Laboratory (AFRL) as part of the DARPA MUSE program
FX This project was sponsored by the Air Force Research Laboratory (AFRL)
   as part of the DARPA MUSE program.
CR Arjovsky M., 2017, INT C LEARN REPR ICL
   Arjovsky Martin, 2017, INT C MACH LEARN ICM
   Chen X., 2016, NEURAL INFORM PROCES
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Devlin Jacob, 2017, ARXIV171011054
   Gomez A. N., 2018, INT C LEARN REPR ICL
   Goodfellow I., 2014, NEURAL INFORM PROCES
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Gulrajani I., 2017, NEURAL INFORM PROCES
   Gupta R., 2017, AAAI, P1345
   Harer J. A., 2018, ARXIV180304497
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jang E, 2017, INT C LEARN REPR ICL
   Ji Jianshu, 2017, ANN M ASS COMP LING, V1, P753
   Lample G., 2018, INT C LEARN REPR ICL
   LaToza T. D., 2006, INT C SOFTW ENG ICSE
   Le X. B. D., 2016, SOFTWARE ANAL EVOLUT
   Long F., 2016, PRINCIPLES PROGRAMMI
   Luong M. - T., 2015, EMPIRICAL METHODS NA
   Maddison C. J, 2017, INT C LEARN REPR ICL
   Mirza M., 2014, ARXIV14111784
   MITRE, COMM VULN EXP
   Monperrus M, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3105906
   Okun V., 2013, TECHNICAL REPORT
   Press O., 2017, 1 WORKSH SUBW CHAR L
   Radford A., 2016, INT C LEARN REPR ICL
   Rajeswar S., 2017, 2 WORKSH REPR LEARN
   Schmaltz A., 2017, EMPIRICAL METHODS NA
   Shrivastava A., 2017, COMPUTER VISION PATT
   Vincent P., 2008, INT C MACH LEARN ICM
   Williams R. J., 1989, NEURAL COMPUTATION
   Xie  Z., 2016, ARXIV160309727
   Yang Z., 2018, N AM CHAPTER ASS COM
   Yu L., 2017, ASS ADVANCEMENT ARTI
   Yuan Z., 2016, N AM CHAPTER ASS COM
   Zhang Y., 2017, INT C MACH LEARN ICM
   Zhu JY, 2017, INT C COMP VIS ICCV
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002048
DA 2019-06-15
ER

PT S
AU Haris, A
   Simon, N
   Shojaie, A
AF Haris, Asad
   Simon, Noah
   Shojaie, Ali
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Wavelet regression and additive models for irregularly spaced data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ORTHONORMAL BASES; SHRINKAGE
AB We present a novel approach for nonparametric regression using wavelet basis functions. Our proposal, waveMesh, can be applied to non-equispaced data with sample size not necessarily a power of 2. We develop an efficient proximal gradient descent algorithm for computing the estimator and establish adaptive minimax convergence rates. The main appeal of our approach is that it naturally extends to additive and sparse additive models for a potentially large number of covariates. We prove minimax optimal convergence rates under a weak compatibility condition for sparse additive models. The compatibility condition holds when we have a small number of covariates. Additionally, we establish convergence rates for when the condition is not met. We complement our theoretical results with empirical studies comparing waveMesh to existing methods.
C1 [Haris, Asad; Simon, Noah; Shojaie, Ali] Univ Washington, Dept Biostat, Seattle, WA 98195 USA.
RP Haris, A (reprint author), Univ Washington, Box 357232, Seattle, WA 98195 USA.
EM aharis@uw.edu; nrsimon@uw.edu; ashojaie@uw.edu
FU National Institutes of Health; National Science Foundation
FX We thank three anonymous referees for insightful comments that
   substantially improved the manuscript. We thank Professor Sylvain Sardy
   for providing software. This work was partially supported by National
   Institutes of Health grants to A.S. and N.S., and National Science
   Foundation grants to A.S.
CR Amato U, 2001, STAT COMPUT, V11, P373, DOI 10.1023/A:1011929305660
   Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942
   Cai TT, 1998, ANN STAT, V26, P1783
   Cai TT, 1999, STAT PROBABIL LETT, V42, P313, DOI 10.1016/S0167-7152(98)00223-5
   Cencov N., 1962, SOV MATH, V3, P1559
   Chui Charles K, 1992, INTRO WAVELETS, P38
   Dalalyan AS, 2017, BERNOULLI, V23, P552, DOI 10.3150/15-BEJ756
   DAUBECHIES I, 1993, SIAM J MATH ANAL, V24, P499, DOI 10.1137/0524031
   DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705
   Daubechies I., 1992, 10 LECT WAVELETS, V61
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512
   Hall P, 1997, ANN STAT, V25, P1912
   Kovac A, 2000, J AM STAT ASSOC, V95, P172, DOI 10.2307/2669536
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Meyer  Yves, 1985, SEMINARE BOURBAKI, V662, P1985
   Nason G., 2010, WAVELET METHODS STAT
   Nesterov Y, 2007, TECHNICAL REPORT
   Nunes MA, 2006, STAT COMPUT, V16, P143, DOI 10.1007/s11222-006-6560-y
   Ogden T., 2012, ESSENTIAL WAVELETS S
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Pensky M, 2001, ANN I STAT MATH, V53, P681, DOI 10.1023/A:1014640632666
   Percival D.B., 2006, WAVELET METHODS TIME, V4
   Petersen A, 2016, J COMPUT GRAPH STAT, V25, P1005, DOI 10.1080/10618600.2015.1073155
   Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x
   Sardy S, 2004, J COMPUT GRAPH STAT, V13, P283, DOI 10.1198/1061860043434
   Sardy S, 1999, STAT COMPUT, V9, P65, DOI 10.1023/A:1008818328241
   Schnaidt Grez German A, 2018, ARXIV E PRINTS
   SILVERMAN BW, 1985, J R STAT SOC B, V47, P1
   Strang G, 1996, WAVELETS FILTER BANK
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani RJ, 2013, ELECTRON J STAT, V7, P1456, DOI 10.1214/13-EJS815
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Vidakovic  Brani, 2009, STAT MODELING WAVELE, V503
   Wahba G., 1990, SPLINE MODELS OBSERV
   Zhang SL, 2003, ANN STAT, V31, P152
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003052
DA 2019-06-15
ER

PT S
AU Harris, DG
   Li, S
   Pensyl, T
   Srinivasan, A
   Trinh, K
AF Harris, David G.
   Li, Shi
   Pensyl, Thomas
   Srinivasan, Aravind
   Khoa Trinh
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Approximation algorithms for stochastic clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
DE clustering; k-center; k-median; lottery; approximation algorithms
ID RACE
AB We consider stochastic settings for clustering, and develop provably-good (approximation) algorithms for a number of these notions. These algorithms allow one to obtain better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including providing fairer clustering and clustering which has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results.
C1 [Harris, David G.; Srinivasan, Aravind] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Li, Shi] SUNY Buffalo, Buffalo, NY USA.
   [Pensyl, Thomas] Bandwidth Inc, Raleigh, NC USA.
   [Srinivasan, Aravind] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
   [Khoa Trinh] Google, Mountain View, CA 94043 USA.
RP Harris, DG (reprint author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM davidgharris29@gmail.com; shil@buffalo.edu; tpensyl@bandwidth.com;
   srin@cs.umd.edu; khoatrinh@google.com
FU NSF [CNS-1010789, CCF-1422569, CCF-1749864, CCF-1566356, CCF-1717134];
   Adobe, Inc.
FX Research supported in part by NSF Awards CNS-1010789, CCF-1422569 and
   CCF-1749864, CCF-1566356, CCF-1717134 and by research awards from Adobe,
   Inc.
CR Ahmadian S., 2016, CORR
   Alipour S, 2018, PODS'18: PROCEEDINGS OF THE 37TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P425, DOI 10.1145/3196959.3196969
   Ayres I, 2015, RAND J ECON, V46, P891, DOI 10.1111/1756-2171.12115
   Badger E., 2016, WASHINGTON POST
   Bertrand M, 2004, AM ECON REV, V94, P991, DOI 10.1257/0002828042002561
   Byrka J, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/2981561
   Charikar M, 2012, LECT NOTES COMPUT SC, V7391, P194, DOI 10.1007/978-3-642-31594-7_17
   Datta Amit, 2015, Proceedings on Privacy Enhancing Technologies, V1, P92, DOI 10.1515/popets-2015-0007
   Guha S, 1999, J ALGORITHMS, V31, P228, DOI 10.1006/jagm.1998.0993
   Harris D. G., 2017, ABS170906995 ARXIV
   Harris D. G., 2017, LIPICS LEIBNIZ INT P, V81
   HOCHBAUM DS, 1986, J ACM, V33, P533, DOI 10.1145/5925.5933
   Huang LX, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P110
   Ibarra I. A., 2018, AM EC ASS PAPERS P, V1
   Jain K., 2002, P 34 ANN ACM S THEOR, P731, DOI DOI 10.1016/J.0RL.2006.03.0
   Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003
   Kempe D, 2015, THEORY COMPUT, V11, P105, DOI [10.4086/toc.2015.v011a004, DOI 10.4086/TOC.2015.V011A004]
   Krishnaswamy R, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P646, DOI 10.1145/3188745.3188882
   Lanier J., 2014, WHO OWNS FUTURE
   Moshkovitz D., 2015, THEORY COMPUT, V11, P221, DOI DOI 10.4086/TOC.2015.V011A007
   Schulman KA, 1999, NEW ENGL J MED, V340, P618, DOI 10.1056/NEJM199902253400806
   Srinivasan A, 2001, ANN IEEE SYMP FOUND, P588, DOI 10.1109/SFCS.2001.959935
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000053
DA 2019-06-15
ER

PT S
AU Hashimoto, TB
   Guu, K
   Oren, Y
   Liang, P
AF Hashimoto, Tatsunori B.
   Guu, Kelvin
   Oren, Yonatan
   Liang, Percy
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Retrieve-and-Edit Framework for Predicting Structured Outputs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.
C1 [Hashimoto, Tatsunori B.; Oren, Yonatan; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Guu, Kelvin] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Hashimoto, TB (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM thashim@stanford.edu; kguu@stanford.edu; yonatano@stanford.edu;
   pliang@cs.stanford.edu
FU DARPA CwC program under ARO prime [W911NF-15-1-0462]
FX This work was funded by the DARPA CwC program under ARO prime contract
   no. W911NF-15-1-0462.
CR Allamanis M., 2015, JMLR P, P2123
   Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Bahdanau  D., 2015, INT C LEARN REPR ICL
   Balog M., 2016, ARXIV161101989
   Bowman S.R., 2016, P 20 SIGNLL C COMP N, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Chaidaroon S, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P75, DOI 10.1145/3077136.3080816
   Chelba C., 2013, ARXIV13123005
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Davidson T. R., 2018, ARXIV180400891
   Feng F., 2014, P 22 ACM INT C MULT, P7, DOI DOI 10.1145/2647868.2654902
   Goldenshluger A., 1997, MATH METH STAT, V6, P135
   Gu J., 2016, ASS COMPUTATIONAL LI
   Gu J., 2017, ARXIV170507267
   Guu K., 2018, T ASS COMPUTATIONAL
   Hayati S. A., 2018, EMPIRICAL METHODS NA
   Krizhevsky A., 2011, EUR S ART NEUR NETW, P489
   Kushman N., 2013, HUMAN LANGUAGE TECHN, P826
   Kuznetsova P., 2013, ACL, P790
   LEI T, 2016, P 2016 C N AM CHAPT, P1279
   Li J., 2017, ARXIV170106547
   Liang P., 2010, P 27 INT C MACH LEAR, P639
   Ling W., 2016, P 54 ANN M ASS COMP, V1, P599, DOI DOI 10.18653/V1/P16-1057
   Maddison C., 2014, INT C MACH LEARN ICM, P649
   Mason R., 2014, COMPUTATIONAL NATURA, P2
   Papineni K., 2002, ASS COMPUTATIONAL LI
   Rabinovich M., 2017, ASS COMPUTATIONAL LI
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P373, DOI 10.1145/2766462.2767738
   Shao L., 2017, P 2017 C EMP METH NA, P2210
   Shen D., 2018, ASS COMPUTATIONAL LI, P2041
   Song Y., 2016, ARXIV161007149
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Sumita E., 1991, ASS COMPUTATIONAL LI
   Sun W., 2018, ARXIV180706473
   Tan M., 2015, ARXIV151104108
   Wu Y., 2018, ARXIV180607042
   Wu Y., 2016, ARXIV160908144
   Xu J., 2018, EMPIRICAL METHODS NA
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yan R, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/2911451.2911542
   Yin P., 2017, P 55 ANN M ASS COMP, P440, DOI DOI 10.18653/V1/P17-1041
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004059
DA 2019-06-15
ER

PT S
AU Hassidim, A
   Singer, Y
AF Hassidim, Avinatan
   Singer, Yaron
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Optimization for Approximate Submodularity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MAXIMIZATION; ALGORITHMS; QUERIES
AB We consider the problem of maximizing a submodular function when given access to its approximate version. Submodular functions are heavily studied in a wide variety of disciplines since they are used to model many real world phenomena and are amenable to optimization. There are many cases however in which the phenomena we observe is only approximately submodular and the optimization guarantees cease to hold. In this paper we describe a technique that yields strong guarantees for maximization of monotone submodular functions from approximate surrogates under cardinality and intersection of matroid constraints. In particular, we show tight guarantees for maximization under a cardinality constraint and 1/(1 + P) approximation under intersection of P matroids.
C1 [Hassidim, Avinatan] Bar Ilan Univ, Ramat Gan, Israel.
   [Hassidim, Avinatan] Google, Ramat Gan, Israel.
   [Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
RP Hassidim, A (reprint author), Bar Ilan Univ, Ramat Gan, Israel.; Hassidim, A (reprint author), Google, Ramat Gan, Israel.
EM avinatan@cs.biu.ac.il; yaron@seas.harvard.edu
FU BSF [2014389]; NSF [CCF 1452961, CCF 1301976, 1540428]; Google Research
   award; Facebook research award;  [1394/16]
FX A.H. is supported by 1394/16 and by a BSF grant. Y.S. is supported by
   NSF grant CAREER CCF 1452961, NSF CCF 1301976, BSF grant 2014389, NSF
   USICCS proposal 1540428, a Google Research award, and a Facebook
   research award.
CR Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2
   Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753
   Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Balcan MF, 2011, ACM S THEORY COMPUT, P793
   Bshouty NH, 2002, J MACH LEARN RES, V2, P359, DOI 10.1162/153244302760200669
   Buchbinder N, 2014, P 25 ANN ACM SIAM S, V25, P1433
   Buchbinder N, 2012, ANN IEEE SYMP FOUND, P649, DOI 10.1109/FOCS.2012.73
   Buchfuhrer  D., 2010, P 11 ACM EC, P33
   Buchfuhrer D, 2010, PROC APPL MATH, V135, P518
   Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182
   Chambers C. P., 2016, ECONOMETRIC SOC MONO
   Chekuri C, 2011, ACM S THEORY COMPUT, P783
   Chekuri C, 2011, ANN IEEE SYMP FOUND, P807, DOI 10.1109/FOCS.2011.34
   Chekuri Chandra, 2015, P 2015 C INN THEOR C, P201
   Chen  Lin, 2018, P 35 INT C MACH LEAR, P813
   Djolonga J., 2014, ADV NEURAL INFORM PR
   DOBZINSKI S., 2012, P 13 ACM C EL COMM, P405
   Dobzinski S, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1064, DOI 10.1145/1109557.1109675
   Dobzinski S, 2011, ACM S THEORY COMPUT, P129
   Dobzinski Shahar, 2005, P 37 ANN ACM S THEOR, P610
   Dobzinski  Shahar, 2008, FOCS
   Dughmi S, 2011, ACM S THEORY COMPUT, P149
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Feige U., 2015, P 29 AAAI C ART INT, P872
   Feige U, 2006, ANN IEEE SYMP FOUND, P667
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Feldman V, 2009, J MACH LEARN RES, V10, P163
   Fisher Marshall L, 1978, ANAL APPROXIMATIONS
   Goldman S. A., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory
   Golovin  D., 2010, IPSN
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Gomes  R., 2010, INT C MACH LEARN ICM
   Gomez Rodriguez  M., 2011, ACM TKDD, V5
   Hassani Hamed, 2017, NIPS, P5843
   Hassidim  Avinatan, 2017, SUBMODULAR OPTIMIZAT
   Horel T, 2016, ADV NEUR IN, V29
   Jackson J., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P42, DOI 10.1109/SFCS.1994.365706
   Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589
   Jegelka  S., 2011, INT C MACH LEARN ICM
   Kempe D., 2003, ACM SIGKDD C KNOWL D
   Khot S, 2005, LECT NOTES COMPUT SC, V3828, P92
   Kohli  P., 2013, IEEE C COMP VIS PATT
   Krause  A., 2007, INT C MACH LEARN ICM
   Kumar  Ravi, 2013, SPAA
   Lee J, 2009, ACM S THEORY COMPUT, P323
   Leskovec  J., 2007, ACM SIGKDD C KNOWL D
   Li  Qiang, 2017, ADV NEURAL INFORM PR, V30, P3804
   Lin  H., 2011, ACL HLT
   Lin  H., 2011, P INTERSPEECH
   Lucier Brendan, 2013, Web and Internet Economics. 9th International Conference, WINE 2013. Proceedings: LNCS 8289, P347, DOI 10.1007/978-3-642-45046-4_28
   Mirrokni V, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P70
   Mokhtari Aryan, 2018, INT C ART INT STAT, P1886
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Nemhauser G. L., 1978, MATH PROGRAMMING STU, V8
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Papadimitriou C, 2011, ACM S THEORY COMPUT, P119
   Papadimitriou C, 2008, ANN IEEE SYMP FOUND, P250, DOI 10.1109/FOCS.2008.54
   Qian  Chao, 2017, ADV NEURAL INFORM PR, V30, P3563
   Schapira M, 2008, LECT NOTES COMPUT SC, V5385, P351, DOI 10.1007/978-3-540-92185-1_41
   Shamir E., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P357
   Streeter  M., 2009, ADV NEURAL INFORM PR
   Vondrak J, 2008, ACM S THEORY COMPUT, P67
NR 62
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300037
DA 2019-06-15
ER

PT S
AU Haug, L
   Tschiatschek, S
   Singla, A
AF Haug, Luis
   Tschiatschek, Sebastian
   Singla, Adish
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Teaching Inverse Reinforcement Learners via Features and Demonstrations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.
C1 [Haug, Luis] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Tschiatschek, Sebastian] Microsoft Res, Cambridge, England.
   [Singla, Adish] Max Planck Inst Software Syst, Saarbrucken, Germany.
RP Haug, L (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM lhaug@inf.ethz.ch; setschia@microsoft.com; adishs@mpi-sws.org
CR Abbeel P., 2004, ICML
   Aodha O. M., 2018, CVPR
   Brown D. S., 2018, CORR
   Cakmak M., 2012, AAAI
   Cakmak M, 2014, ARTIF INTELL, V217, P198, DOI 10.1016/j.artint.2014.08.005
   Chen Y., 2018, CORR
   Chen Y., 2018, NEURAL INFORM PROCES
   Haug L., 2018, CORR
   Kamalaruban P., 2018, LEARN INSTR WORKSH N
   Liu W., 2017, ICML, P2149
   Mayer M., 2017, DARTS DAGSTUHL ARTIF, V3
   Mei S., 2015, AAAI, P2871
   Patil K. R., 2014, ADV NEURAL INFORM PR, P2465
   Rafferty AN, 2016, COGNITIVE SCI, V40, P1290, DOI 10.1111/cogs.12290
   Sermanet P, 2018, IEEE INT CONF ROBOT, P1134, DOI 10.1109/ICRA.2018.8462891
   Singla A., 2013, NIPS WORKSH DAT DRIV
   Singla A., 2014, P 31 INT C MACH LEAR, P154
   Stadie B. C., 2017, CORR
   Yeo T., 2019, AAAI
   Zhu X, 2015, AAAI, P4083
   Zhu X., 2018, CORR
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003006
DA 2019-06-15
ER

PT S
AU Havasi, M
   Hernandez-Lobato, JM
   Murillo-Fuentes, JJ
AF Havasi, Marton
   Hernandez-Lobato, Jose Miguel
   Jose Murillo-Fuentes, Juan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inference in Deep Gaussian Processes using Stochastic Gradient
   Hamiltonian Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.
C1 [Havasi, Marton; Hernandez-Lobato, Jose Miguel] Univ Cambridge, Dept Engn, Cambridge, England.
   [Hernandez-Lobato, Jose Miguel] Microsoft Res, Cambridge, England.
   [Hernandez-Lobato, Jose Miguel] Alan Turing Inst, London, England.
   [Jose Murillo-Fuentes, Juan] Univ Seville, Dept Signal Theory & Commun, Seville, Spain.
RP Havasi, M (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM mh740@cam.ac.uk; jmh233@cam.ac.uk; murillo@us.es
FU Intel; EPSRC; Spanish government [TEC2016-78434-C3-R]; European Union
   (MINECO/FEDER, UE)
FX We want to thank Adri a Gariga-Alonso, John Bronskill, Robert Peharz and
   Siddharth Swaroop for their helpful comments and thank Intel and EPSRC
   for their generous support.; Juan Jose Murillo-Fuentes acknowledges
   funding from the Spanish government (TEC2016-78434-C3-R) and the
   European Union (MINECO/FEDER, UE).
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Bui T., 2016, P 33 INT C INT C MAC, P1472
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Cheng C.-A., 2017, ADV NEURAL INFORM PR, P5190
   Cheng C.-A., 2016, ADV NEURAL INF PROCE, P4410
   Cramer D., 1998, FUNDAMENTAL STAT SOC, P10001
   Cutajar K., 2016, ARXIV161004386
   Damianou A., 2015, THESIS
   Damianou A. C., 2013, P AISTATS, V31, P207
   Dunlop Matthew M., 2017, ARXIV171111280
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s
   Hensman J, 2015, ADV NEURAL INFORM PR, V28, P1648
   Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, V24, P280
   Hernandez-Lobato J. M., 2015, INT C MACH LEARN ICM, P1861
   Hoffman Matthew D, 2017, INT C MACH LEARN, P1510
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Neal R. M., 1993, PROBABILISTIC INFERE
   Neath R. C., 2013, I MATH STAT COLLECTI, P43, DOI 10.1214/12-IMSCOLL1003
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Salimbeni H., 2017, ADV NEURAL INFORM PR, P4591
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Springenberg Jost Tobias, 2016, ADV NEURAL INFORM PR, V29, P4134, DOI DOI 10.1152/JN.00333.2004
   Titsias M, 2009, ARTIF INTELL, P567
   Turner R. E., 2011, BAYESIAN TIME SERIES, V1, P3
   WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Williams CKI, 1996, ADV NEUR IN, V8, P514
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002009
DA 2019-06-15
ER

PT S
AU Havens, AJ
   Jiang, ZH
   Sarkar, S
AF Havens, Aaron J.
   Jiang, Zhanhong
   Sarkar, Soumik
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Robust Policy Learning in the Presence of Unknown Adversaries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.
C1 [Havens, Aaron J.; Jiang, Zhanhong; Sarkar, Soumik] Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.
RP Havens, AJ (reprint author), Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.
EM ajhavens@iastate.edu; zhjiang@iastate.edu; soumiks@iastate.edu
FU U.S. AFOSR under the YIP [FA9550-17-1-0220]
FX This work has been supported in part by the U.S. AFOSR under the YIP
   grant FA9550-17-1-0220. Any opinions, findings and conclusions or
   recommendations expressed in this publication are those of the authors
   and do not necessarily reflect the views of the sponsoring agencies.
CR Antoniou A, 2016, IEEE IJCNN, P2879, DOI 10.1109/IJCNN.2016.7727563
   Brockman G., 2016, OPENAI GYM
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chi-Sheng Shih, 2016, IET Cyber-Physical Systems: Theory & Applications, V1, P3, DOI 10.1049/iet-cps.2016.0025
   Fox E, 2011, IEEE T SIGNAL PROCES, V59, P1569, DOI 10.1109/TSP.2010.2102756
   Frans K., 2017, ARXIV E PRINTS
   Goodfellow I. J., 2014, ARXIV E PRINTS
   Havens Aaron J, 2018, ARXIV180706064
   Huang Sandy, 2017, ARXIV170202284
   Kos Jernej, 2017, ARXIV170506452
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2015, ARXIV E PRINTS
   Lin Y., 2017, ARXIV E PRINTS
   Madry Aleksander, 2017, ARXIV170606083
   Mandlekar Ajay, 2017, IEEE INT C INT ROB S
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pattanaik A., 2017, ARXIV E PRINTS
   Pinto L., 2017, ARXIV E PRINTS
   Rajeswaran Aravind, 2016, ARXIV161001283
   Rawat DB, 2015, SOUTHEASTCON 2015 IE, P1, DOI 10.1109/SECON.2015.7132891
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman  J., 2017, ARXIV170706347
   Schulman J., 2015, ARXIV E PRINTS
   Sutton R. S., 1992, IEEE Control Systems Magazine, V12, P19, DOI 10.1109/37.126844
   Sutton R. S., 2017, REINFORCEMENT LEARNI
   Van Hasselt H., 2016, AAAI, P2094
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004047
DA 2019-06-15
ER

PT S
AU Hayes, J
   Ohrimenko, O
AF Hayes, Jamie
   Ohrimenko, Olga
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Contamination Attacks and Mitigation in Multi-Party Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Machine learning is data hungry; the more data a model has access to in training, the more likely it is to perform well at inference time. Distinct parties may want to combine their local data to gain the benefits of a model trained on a large corpus of data. We consider such a case: parties get access to the model trained on their joint data but do not see each others individual datasets. We show that one needs to be careful when using this multi-party model since a potentially malicious party can taint the model by providing contaminated data. We then show how adversarial training can defend against such attacks by preventing the model from learning trends specific to individual parties data, thereby also guaranteeing party-level membership privacy.
C1 [Hayes, Jamie] UCL, London, England.
   [Hayes, Jamie; Ohrimenko, Olga] Microsoft Res, Cambridge, England.
RP Hayes, J (reprint author), UCL, London, England.; Hayes, J (reprint author), Microsoft Res, Cambridge, England.
EM j.hayes@cs.ucl.ac.uk; oohrim@microsoft.com
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Alfeld S., 2016, P 13 AAAI C ART INT, P1452
   Allen J., 2018, CORR
   Biggio B, 2012, P 29 INT C INT C MAC, P1467
   Bittau A., 2017, ACM S OP SYST PRINC
   Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen X., 2017, ARXIV171205526
   Dowlin N., 2016, P 33 INT C MACH LEAR, V48, P201, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?
   Dwork C., 2012, C INN THEOR COMP SCI
   Edwards H., 2016, INT C LEARN REPR ICL, V2
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow IJ, 2014, ARXIV14126572
   Gu T., 2017, ARXIV170806733
   Hamm J, 2016, P 33 INT C MACH LEAR, P555
   Hamm J, 2017, J MACH LEARN RES, V18
   Hayes J., 2018, P PRIV ENH TECHN POP
   Hesamifard Ehsan, 2018, Proceedings on Privacy Enhancing Technologies, V2018, P123, DOI 10.1515/popets-2018-0024
   Hitaj B, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P603, DOI 10.1145/3133956.3134012
   Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057
   Kim Y, 2014, P 2014 C EMP METH NA, P1746, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]
   Koh Pang Wei, 2017, P MACHINE LEARNING R, P1885
   Kurakin A., 2016, ARXIV160702533
   Long Y., 2018, ARXIV180204889
   Louppe G., 2017, NIPS, P982
   McMahan H. B., 2016, CORR
   McMahan H. B., 2018, INT C LEARN REPR ICL
   Mohassel P, 2017, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2017.12
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nasr M, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P634, DOI 10.1145/3243734.3243855
   Nikolaenko V., 2013, P 2013 ACM SIGSAC C, P801, DOI DOI 10.1145/2508859.2516751
   Ohrimenko O, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P619
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Pappachan P., 2013, WORKSH HARDW ARCH SU
   Pathak  M., 2010, ADV NEURAL INFORM PR, P1876
   Rajkumar A., 2012, P 15 INT C ART INT S, V22, P933
   Schuster F, 2015, P IEEE S SECUR PRIV, P38, DOI 10.1109/SP.2015.10
   Shen S., 2016, P 32 ANN C COMP SEC, P508, DOI DOI 10.1145/2991079.2991125
   Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41
   Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687
   Song CZ, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P587, DOI 10.1145/3133956.3134077
   Xiao H., 2015, JMLR W CP, P1689
   Xiao H, 2015, NEUROCOMPUTING, V160, P53, DOI 10.1016/j.neucom.2014.08.081
   Zafar M. B., 2017, ARTIF INTELL, P962
   Zemel R., 2013, JMLR P, P325
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001017
DA 2019-06-15
ER

PT S
AU Hazan, E
   Hu, W
   Li, YZ
   Li, ZY
AF Hazan, Elad
   Hu, Wei
   Li, Yuanzhi
   Li, Zhiyuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Improper Learning with an Approximation Oracle
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHMS
AB We study the following question: given an efficient approximation algorithm for an optimization problem, can we learn efficiently in the same setting? We give a formal affirmative answer to this question in the form of a reduction from online learning to offline approximate optimization using an efficient algorithm that guarantees near optimal regret. The algorithm is efficient in terms of the number of oracle calls to a given approximation oracle - it makes only logarithmically many such calls per iteration. This resolves an open question by Kalai and Vempala, and by Garber. Furthermore, our result applies to the more general improper learning problems.
C1 [Hazan, Elad; Hu, Wei; Li, Zhiyuan] Princeton Univ, Princeton, NJ 08544 USA.
   [Hazan, Elad] Google AI Princeton, Princeton, NJ USA.
   [Li, Yuanzhi] Stanford Univ, Stanford, CA 94305 USA.
RP Hazan, E (reprint author), Princeton Univ, Princeton, NJ 08544 USA.; Hazan, E (reprint author), Google AI Princeton, Princeton, NJ USA.
EM ehazan@cs.princeton.edu; huwei@cs.princeton.edu; yuanzhil@stanford.edu;
   zhiyuanli@cs.princeton.edu
CR Awerbuch  B., 2004, P 36 ANN ACM S THEOR, P45
   Balcan M.-F., 2006, P 7 ACM C EL COMM, P29
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Dudik M., 2016, ARXIV161101688
   Fujita T, 2013, LECT NOTES ARTIF INT, V8139, P68
   Garber  D., 2017, ADV NEURAL INFORM PR, V30, P627
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P128, DOI 10.1145/2897518.2897536
   Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135
   PREKOPA A, 1973, ACTA SCI MATH, V34, P334
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000018
DA 2019-06-15
ER

PT S
AU Hazan, E
   Lee, H
   Singh, K
   Zhang, C
   Zhang, Y
AF Hazan, Elad
   Lee, Holden
   Singh, Karan
   Zhang, Cyril
   Zhang, Yi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Spectral Filtering for General Linear Dynamical Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.
C1 [Hazan, Elad; Lee, Holden; Singh, Karan; Zhang, Cyril; Zhang, Yi] Princeton Univ, Princeton, NJ 08544 USA.
   [Hazan, Elad; Singh, Karan; Zhang, Cyril; Zhang, Yi] Google AI Princeton, Princeton, NJ USA.
RP Hazan, E (reprint author), Princeton Univ, Princeton, NJ 08544 USA.; Hazan, E (reprint author), Google AI Princeton, Princeton, NJ USA.
EM ehazan@cs.princeton.edu; holdenl@princeton.edu; karans@cs.princeton.edu;
   cyril.zhang@cs.princeton.edu; y.zhang@cs.princeton.edu
CR Abbasi-Yadkori Y, 2011, 24 ANN C LEARN THEOR, V19, P1
   Anava Oren, 2013, P C LEARN THEOR, V30, P172
   Beckermann B, 2017, SIAM J MATRIX ANAL A, V38, P1227, DOI 10.1137/16M1096426
   Belanger David, 2015, P ICML, P833
   Boley DL, 1998, LINEAR ALGEBRA APPL, V284, P41, DOI 10.1016/S0024-3795(98)10101-5
   Box G. E. P., 1994, TIME SERIES ANAL FOR
   Brockwell P. J., 2009, TIME SERIES THEORY M
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   CHOI MD, 1983, AM MATH MON, V90, P301, DOI 10.2307/2975779
   Dean S, 2017, ARXIV171001688
   Hamilton J, 1994, TIME SERIES ANAL
   Hardt Moritz, 2016, ARXIV160905191
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan Elad, 2017, ADV NEURAL INFORM PR, P6705
   Hilbert D, 1894, ACTA MATH, V18, P155
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Kuznetsov Vitaly, 2016, P MACH LEARN RES COL, V49, P1190
   Kuznetsov Vitaly, 2017, ADV NEURAL INFORM PR, V30, P5671
   Ljung L., 1998, SYSTEM IDENTIFICATIO
   Moon T, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P1126, DOI 10.1109/ISIT.2007.4557121
   Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674
   Van Overschee P, 2012, SUBSPACE IDENTIFICAT
   Zhou K., 1996, ROBUST OPTIMAL CONTR, V40
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304063
DA 2019-06-15
ER

PT S
AU He, L
   Bian, A
   Jaggi, M
AF He, Lie
   Bian, An
   Jaggi, Martin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI COLA: Decentralized Linear Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OPTIMIZATION; CONVERGENCE; ALGORITHM
AB Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices.
C1 [He, Lie; Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Bian, An] Swiss Fed Inst Technol, Zurich, Switzerland.
RP He, L (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM lie.he@epfl.ch; ybian@inf.ethz.ch; martin.jaggi@epfl.ch
FU SNSF [200021_175796]; Microsoft Research JRC project 'Coltrain'; Google
   Focused Research Award
FX We thank Prof. Bharat K. Bhargava for fruitful discussions. We
   acknowledge funding from SNSF grant 200021_175796, Microsoft Research
   JRC project 'Coltrain', as well as a Google Focused Research Award.
CR Agarwal A., 2011, P ADV NEUR INF PROC, P873
   Bianchi P, 2016, IEEE T AUTOMAT CONTR, V61, P2947, DOI 10.1109/TAC.2015.2512043
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397
   Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027
   Dunner Celestine, 2018, ICML 2018 P 35 INT C, P1357
   Dunner Celestine, 2016, P 33 INT C MACH LEAR, P783
   Gargiani Matilde, 2017, THESIS
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Jaggi M., 2014, ADV NEURAL INFORM PR, P3068
   Jakovetic D, 2012, 2012 20TH TELECOMMUNICATIONS FORUM (TELFOR), P867, DOI 10.1109/TELFOR.2012.6419345
   Konecny J., 2016, ARXIV161005492
   Konecny Jakub, 2015, ARXIV151103575
   Lee Ching-pei, 2018, ACM INT C KNOWL DISC
   Lee Ching-pei, 2017, ARXIV170903043
   Lian Xiangru, 2018, ICML 2018 P 35 INT C
   Lian Xiangru, 2017, ADV NEURAL INFORM PR, P5336
   Ma Chenxin, 2015, P ICML, P1973
   McMahan H. B., 2017, ARTIF INTELL, P1273
   Mokhtari A, 2016, J MACH LEARN RES, V17
   Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Paszke Adam, 2017, NIPS WORKSH AUT
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Reddi Sashank J., 2016, ARXIV160806879
   Rockafellar R. T., 2015, CONVEX ANAL
   Scaman K., 2017, P 34 INT C MACH LEAR, P3027
   Scaman Kevin, 2018, ARXIV180600291
   Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X
   Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432
   Sirb B, 2018, SIAM J OPTIMIZ, V28, P1232, DOI 10.1137/16M1081257
   Smith Virginia, 2018, JMLR, V18, P1
   Smith Virginia, 2017, NIPS 2017 ADV NEURAL, V30
   Stich Sebastian U., 2018, NIPS 2018 ADV NEURAL
   Tang Hanlin, 2018, ARXIV180307068
   Tang Hanlin, 2018, NIPS 2018 ADV NEURAL
   TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412
   Wang Jialei, 2017, C LEARN THEOR, P1882
   Wei Ermin, 2013, O 1 K CONVERGENCE AS
   Wu TY, 2018, IEEE T SIGNAL INF PR, V4, P293, DOI 10.1109/TSIPN.2017.2695121
   Yang Tianbao, 2013, NIPS 2014 ADV NEUR P, V27
   Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
   Zhang Y., 2015, P 32 INT C MACH LEAR, P362
   Zinkevich M., 2010, ADV NEURAL INFORM PR, V23, P2595
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304054
DA 2019-06-15
ER

PT S
AU He, LF
   Chen, K
   Xu, WW
   Zhou, JY
   Wang, F
AF He, Lifang
   Chen, Kun
   Xu, Wanwan
   Zhou, Jiayu
   Wang, Fei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Boosted Sparse and Low-Rank Tensor Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REGULARIZATION; SELECTION
AB We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.
C1 [He, Lifang; Wang, Fei] Weill Cornell Med, New York, NY 10065 USA.
   [Chen, Kun; Xu, Wanwan] Univ Connecticut, Storrs, CT 06269 USA.
   [Zhou, Jiayu] Michigan State Univ, E Lansing, MI 48824 USA.
RP Chen, K (reprint author), Univ Connecticut, Storrs, CT 06269 USA.
EM lifanghescut@gmail.com; kun.chen@uconn.edu; wanwan.xu@uconn.edu;
   dearjiayu@gmail.com; few2001@med.cornell.edu
FU NSF [IIS-1716432, IIS-1750326, IIS-1718798, DMS-1613295, IIS-1749940,
   IIS-1615597]; ONR [N00014-18-1-2585, N00014-17-1-2265]; Michael J. Fox
   Foundation [14858]; Michael J. Fox Foundation for Parkinson's Research;
   Abbvie; Avid; Biogen; Bristol-Mayers Squibb; Covance; GE; Genentech;
   GlaxoSmithKline; Lilly; Lundbeck; Merk; Meso Scale Discovery; Pfizer;
   Piramal; Roche; Sanofi; Servier; TEVA; UCB; Golub Capital; 
   [1R01AI130460]
FX This work is supported by NSF No. IIS-1716432 (Wang), IIS-1750326
   (Wang), IIS-1718798 (Chen), DMS-1613295 (Chen), IIS-1749940 (Zhou),
   IIS-1615597 (Zhou), ONR N00014-18-1-2585 (Wang), and N00014-17-1-2265
   (Zhou), and Michael J. Fox Foundation grant number 14858 (Wang). Lifang
   He's research is supported in part by 1R01AI130460. Data used in the
   preparation of this article were obtained from the Parkinson's
   Progression Markers Initiative (PPMI) database
   (http://www.ppmi-info.org/data).For up-to-date information on the study,
   visit http://www.ppmi-info.org.PPMI - a public-private partnership -is
   funded by the Michael J. Fox Foundation for Parkinson's Research and
   funding partners, including Abbvie, Avid, Biogen, Bristol-Mayers Squibb,
   Covance, GE, Genentech, GlaxoSmithKline, Lilly, Lundbeck, Merk, Meso
   Scale Discovery, Pfizer, Piramal, Roche, Sanofi, Servier, TEVA, UCB and
   Golub Capital.
CR Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439
   Chen K, 2012, J R STAT SOC B, V74, P203, DOI 10.1111/j.1467-9868.2011.01002.x
   da Silva Alex Pereira, 2015, ARXIV150805273
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Guo WW, 2012, IEEE T IMAGE PROCESS, V21, P816, DOI 10.1109/TIP.2011.2165291
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Kampa K, 2014, J GLOBAL OPTIM, V59, P439, DOI 10.1007/s10898-013-0134-2
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Minasian A, 2014, IEEE T WIREL COMMUN, V13, P6118, DOI 10.1109/TWC.2014.2320977
   Mishra A, 2017, J COMPUT GRAPH STAT, V26, P814, DOI 10.1080/10618600.2017.1340891
   Phan AH, 2015, IEEE T SIGNAL PROCES, V63, P5924, DOI 10.1109/TSP.2015.2458785
   Signoretto M, 2014, MACH LEARN, V94, P303, DOI 10.1007/s10994-013-5366-3
   Song  Xiaonan, 2017, AAAI, P2562
   Su Y, 2012, IEEE T SYST MAN CY B, V42, P1560, DOI 10.1109/TSMCB.2012.2195171
   Tan  Xu, 2012, P INT C INT SCI INT, P573
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vaughan G, 2017, BIOMETRICS, V73, P1332, DOI 10.1111/biom.12669
   Wang F, 2014, P 20 ACM SIGKDD INT, P145
   Yu  Rose, 2016, INT C MACH LEARN, P373
   Zhao P, 2007, J MACH LEARN RES, V8, P2701
   Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301004
DA 2019-06-15
ER

PT S
AU He, TY
   Tan, X
   Xia, YC
   He, D
   Qin, T
   Chen, ZB
   Liu, TY
AF He, Tianyu
   Tan, Xu
   Xia, Yingce
   He, Di
   Qin, Tao
   Chen, Zhibo
   Liu, Tie-Yan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Layer-Wise Coordination between Encoder and Decoder for Neural Machine
   Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Specifically, we design a layer-wise attention and mixed attention mechanism, and further share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline.
C1 [He, Tianyu; Chen, Zhibo] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei, Anhui, Peoples R China.
   [Tan, Xu; Xia, Yingce; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China.
   [He, Di] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China.
   [He, Tianyu] Microsoft Res Asia, Beijing, Peoples R China.
RP He, TY (reprint author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei, Anhui, Peoples R China.
EM hetianyu@mail.ustc.edu.cn; xuta@microsoft.com; yingce.xia@microsoft.com;
   di_he@pku.edu.cn; taoqin@microsoft.com; chenzhibo@ustc.edu.cn;
   tie-yan.liu@microsoft.com
FU National Key Research and Development Program of China [2016YFC0801001];
   National Program on Key Basic Research Projects (973 Program)
   [2015CB351803]; NSFC [61571413, 61632001, 61390514]
FX This work was partially supported by the National Key Research and
   Development Program of China under Grant No. 2016YFC0801001, the
   National Program on Key Basic Research Projects (973 Program) under
   Grant 2015CB351803, NSFC under Grant 61571413, 61632001, 61390514. We
   thank all the anonymous reviewers for their valuable comments on our
   paper.
CR Bahdanau D., 2015, ICLR 2015
   Bahdanau D., 2017, 5 INT C LEARN REPR I
   Cettolo M., 2014, P 11 IWSLT
   Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.0134, 10.1109/CIS.2016.133]
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Ding Y., 2017, PROCEEDINGS OF THE 5, V1, P1150
   Gehring J., 2017, P 34 INT C MACH LEAR, P1243
   Gu J., 2017, CORR
   Hassan H., 2018, CORR
   Hu B., 2014, ADV NEURAL INFORM PR, P2042
   Huang P.-S., 2017, ARXIV170605565
   Kalchbrenner N., 2016, CORR
   Kingma D. P., 2014, ARXIV14126980
   Koehn P., 2017, STAT MACHINE TRANSLA, P13
   Lin  Z., 2017, STRUCTURED SELF ATTE
   Lu Z., 2013, ADV NEURAL INFORM PR, P1367
   Luong M., 2015, EMNLP
   Luong T., 2015, P 2015 C EMP METH NA, P1412, DOI DOI 10.18653/V1/D15-1166
   Pang Liang, 2016, AAAI, P2793
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Parikh A., 2016, P 2016 C EMP METH NA, P2249, DOI DOI 10.18653/V1/D16-1244
   Paulus R., 2017, CORR
   Ranzato M., 2015, CORR
   Sennrich R., 2016, P 54 ANN M ASS COMP, V1
   Sennrich R., 2016, P 1 C MACH TRANSL WM, V2, P371
   Shazeer Noam, 2017, CORR
   Shen Y., 2018, P 2018 C N AM CHAPT, V1, P1294
   Song K., 2018, P 27 INT C COMP LING, P3064
   Sutskever I., 2014, NEURAL INFORM PROCES, V2014, P3104
   Tu Z., 2017, T ASS COMPUT LINGUIS, V5, P87
   Tu Z., 2017, AAAI, P3097
   Vaswani A., 2018, CORR
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Wang Y., 2018, AAAI
   Wu L., 2018, EMNLP
   Wu Y., 2016, CORR
   Xia Y., 2018, P 35 INT C MACH LEAR, P5379
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002049
DA 2019-06-15
ER

PT S
AU He, XX
   Zhou, ZM
   Thiele, L
AF He, Xiaoxi
   Zhou, Zimu
   Thiele, Lothar
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multi-Task Zipping via Layer-wise Neuron Sharing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both within-model and cross-model to fit in mobile storage and memory. Previous studies squeeze the redundancy within a single model. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with < 0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8x lower than that of training a single VGG-16 network. Moreover, experiments show that MTZ is also able to effectively merge multiple residual networks.
C1 [He, Xiaoxi; Zhou, Zimu; Thiele, Lothar] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Zhou, ZM (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM hex@ethz.ch; zzhou@tik.ee.ethz.ch; thiele@ethz.ch
CR Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, P3123
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Dong  X., 2017, P ADV NEUR INF PROC, P4860
   Georgiev Petko, 2017, P ACM INT MOB WEAR U, V1
   Guo Y., 2016, ADV NEURAL INFORM PR, P1379
   Han S., 2016, P INT C LEARN REPR
   Hassibi B., 1993, ADV NEURAL INFORMATI, V5, P164
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, ARXIV150302531
   Hu H., 2016, ARXIV160703250
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu Yongxi, 2017, P IEEE C COMP VIS PA, P5334
   Mathur A, 2017, MOBISYS'17: PROCEEDINGS OF THE 15TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P68, DOI 10.1145/3081333.3081359
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Rebuffi Sylvestre-Alvise, 2017, ADV NEURAL INFORM PR, P506
   Romero Adriana, 2014, ARXIV14126550
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2014, ARXIV14091556
   Soomro K., 2012, ARXIV12120402
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Yang Yongxin, 2016, P INT C LEARN REPR
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zagoruyko S, 2016, ARXIV160507146
   Zhang Yu, 2017, ARXIV170708114
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000051
DA 2019-06-15
ER

PT S
AU Heidari, H
   Ferrari, C
   Gummadi, KP
   Krause, A
AF Heidari, Hoda
   Ferrari, Claudio
   Gummadi, Krishna P.
   Krause, Andreas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated
   Decision Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID UTILITY
AB We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems-namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics, and is justified by the Rawlsian conception of fairness behind a veil of ignorance. The convex formulation of our welfare-based measures of fairness allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of individual fairness. Furthermore and perhaps most importantly, our work provides both heuristic justification and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level inequality.
C1 [Heidari, Hoda; Ferrari, Claudio; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Gummadi, Krishna P.] MPI SWS, Saarbrucken, Germany.
RP Heidari, H (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM hheidari@inf.ethz.ch; ferraric@ethz.ch; gummadi@mpi-sws.org;
   krausea@ethz.ch
FU CTI grant [27248.1 PFES-ES]; European Research Council (ERC) Advanced
   Grant "Foundations for Fair Social Computing" [789373]
FX H. Heidari and A. Krause acknowledge support from CTI grant no. 27248.1
   PFES-ES. Krishna P. Gummadi was supported in part by a European Research
   Council (ERC) Advanced Grant "Foundations for Fair Social Computing"
   (No. 789373).
CR Amiel Y, 2003, RES EC INEQ, V9, P35
   Angwin J, 2016, PROPUBLICA
   ATKINSON AB, 1970, J ECON THEORY, V2, P244, DOI 10.1016/0022-0531(70)90039-6
   Barry-Jester AM, 2015, NEW SCI SENTENCING
   Calders T, 2013, IEEE DATA MINING, P71, DOI 10.1109/ICDM.2013.114
   Carlsson F, 2005, ECONOMICA, V72, P375, DOI 10.1111/j.0013-0427.2005.00421.x
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   Cowell FA, 2001, EUR ECON REV, V45, P941, DOI 10.1016/S0014-2921(01)00121-0
   DAGUM C, 1990, J ECONOMETRICS, V43, P91, DOI 10.1016/0304-4076(90)90109-7
   Dalton H, 1920, ECON J, V30, P348, DOI 10.2307/2223525
   Debreu Gerard, 1959, TECHNICAL REPORT
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Freeman Samuel, 2016, STANFORD ENCY PHILOS
   GORMAN WM, 1968, REV ECON STUD, V35, P367, DOI 10.2307/2296766
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Harsanyi JC, 1955, J POLIT ECON, V63, P309, DOI 10.1086/257678
   Harsanyi JC, 1953, J POLIT ECON, V61, P434, DOI 10.1086/257416
   KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185
   Kamiran Faisal, 2009, INT C COMP CONTR COM, P1, DOI DOI 10.1109/IC4.2009.4909197
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Kleinberg Jon, 2017, P 8 INN THEOR COMP S
   Larson Jeff, 2016, DATA ANAL WE ANAL CO
   Levin S., 2016, GUARDIAN
   Miller Claire Cain, 2015, NY TIMES
   Moulin H., 2004, FAIR DIVISION COLLEC
   Petrasic Kevin, 2017, WHITE CASE
   Pigou A. C., 1912, WEALTH WELFARE
   Rawls J., 2009, THEORY JUSTICE
   ROBERTS KWS, 1980, REV ECON STUD, V47, P421, DOI 10.2307/2297002
   Rudin Cynthia, 2013, WIRED MAGAZINE
   Schwartz Joseph, 1980, SOCIOL METHODOL, V11, P1
   SEN A, 1977, ECONOMETRICA, V45, P1539, DOI 10.2307/1913949
   Speicher Till, 2018, P INT C KNOWL DISC D
   Sweeney L., 2013, DISCRIMINATION ONLIN, V11, P10
   VARIAN HR, 1974, J ECON THEORY, V9, P63, DOI 10.1016/0022-0531(74)90075-1
   Zafar M. B., 2017, ADV NEURAL INFORM PR, P228
   Zafar MB, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1171, DOI 10.1145/3038912.3052660
   Zafar MuhammadBilal, 2017, P 20 INT C ART INT S
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301027
DA 2019-06-15
ER

PT S
AU Hendrycks, D
   Mazeika, M
   Wilson, D
   Gimpel, K
AF Hendrycks, Dan
   Mazeika, Mantas
   Wilson, Duncan
   Gimpel, Kevin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe
   Noise
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The growing importance of massive datasets used for deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. Numerous previous works assume that no source of labels can be trusted. We relax this assumption and assume that a small subset of the training data is trusted. This enables substantial label corruption robustness performance gains. In addition, particularly severe label noise can be combated by using a set of trusted data with clean labels. We utilize trusted data by proposing a loss correction technique that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.
C1 [Hendrycks, Dan] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Mazeika, Mantas] Univ Chicago, Chicago, IL 60637 USA.
   [Wilson, Duncan] Foundat Res Inst, Basel, Switzerland.
   [Gimpel, Kevin] Toyota Technol Inst, Chicago, IL USA.
RP Hendrycks, D (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM hendrycks@berkeley.edu; mantas@ttic.edu; duncanw@nevada.unr.edu;
   kgimpel@ttic.edu
CR Biggio  B, 2011, ACML
   Charikar  Moses, 2017, STOC
   Frenay  Benoit, 2014, IEEE T NEURAL NETW L
   Gimpel  Kevin, 2011, ACL
   Guo  Chuan, 2017, ICML
   Hendrycks  Dan, 2017, ICLR
   Hendrycks  Dan, 2016, 160608415 ARXIV
   Kingma Diederik P, 2014, ICLR
   Larsen  J, 1998, AC SPEECH SIGN PROC
   Li  Bo, 2016, NIPS
   Li  Yuncheng, 2017, ICCV
   Loshchilov I, 2016, ICLR
   Maas A. L., 2011, P 49 ANN M ASS COMP
   Menon Aditya Krishna, 2016, CORR
   Mnih V., 2012, ICML
   Natarajan  Nagarajan, 2013, ADV NEURAL INFORM PR, V26
   Nettleton David F, 2010, ARTIF INTELL REV
   Patrini  Giorgio, 2016, CVPR
   Pechenizkiy  M, 2006, 19 IEEE S COMP BAS M
   Reed  Scott, 2014, ICLR WORKSH
   Ren Thu Mengye, 2018, ICML
   Socher R., 2013, C EMP METH NAT LANG
   Srivastava N., 2014, J MACHINE LEARNING R
   Steinhardt  Jacob, 2017, NIPS
   Sukhbaatar  Sainbayar, 2014, ICLR WORKSH
   Veit  Andreas, 2017, CVPR
   Xiao  Tong, 2015, CVPR
   Xie  Saining, 2016, CVPR
   Zagoruyko S., 2016, BMVC
   Zhu  Xingquan, 2004, ARTIFICIAL INTELLIGE
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005007
DA 2019-06-15
ER

PT S
AU Heo, J
   Lee, HB
   Kim, S
   Lee, J
   Kim, KJ
   Yang, E
   Hwang, SJ
AF Heo, Jay
   Lee, Hae Beom
   Kim, Saehoon
   Lee, Juho
   Kim, Kwang Joon
   Yang, Eunho
   Hwang, Sung Ju
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Uncertainty-Aware Attention for Reliable Interpretation and Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with "I don't know" decision show that UA yields networks with high reliability as well.
C1 [Heo, Jay; Lee, Hae Beom; Yang, Eunho; Hwang, Sung Ju] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Heo, Jay; Lee, Hae Beom; Kim, Saehoon; Lee, Juho; Yang, Eunho; Hwang, Sung Ju] AItrics, Seoul, South Korea.
   [Kim, Kwang Joon] Yonsei Univ, Coll Med, Seoul, South Korea.
   [Heo, Jay] UNIST, Ulsan, South Korea.
   [Lee, Juho] Univ Oxford, Oxford, England.
RP Heo, J (reprint author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.; Heo, J (reprint author), AItrics, Seoul, South Korea.; Heo, J (reprint author), UNIST, Ulsan, South Korea.
EM jayheo@kaist.ac.kr; haebeom.lee@kaist.ac.kr; shkim@aitrics.com;
   juho.lee@stats.ox.ac.uk; preppie@yuhs.ac; eunhoy@kaist.ac.kr;
   sjhwang82@kaist.ac.kr
FU Machine Learning and Statistical Inference Framework for Explainable
   Artificial Intelligence - Institution for Information & Communications &
   Technology Promotion (IITP) [2017-0-01779]; Basic Science Research
   Program through the National Research Foundation of Korea (NRF) -
   Ministry of Education of South Korea [2015R1D1A1A01061019]; European
   Research Council under the European Union's Seventh Framework Programme
   (FP7/2007-2013) ERC grant [617071]
FX This work was supported by a Machine Learning and Statistical Inference
   Framework for Explainable Artificial Intelligence (No.2017-0-01779)
   funded by Institution for Information & Communications & Technology
   Promotion (IITP) and Basic Science Research Program through the National
   Research Foundation of Korea (NRF) funded by the Ministry of Education
   (2015R1D1A1A01061019) of South Korea. Juho Lee is funded by the European
   Research Council under the European Union's Seventh Framework Programme
   (FP7/2007-2013) ERC grant agreement no. 617071.
CR Ayhan M. S., 2018, MIDL
   Bandanau D., 2015, ICLR
   Choi Edward, 2016, NIPS
   Futoma  J., 2017, ICML
   Gal  Y., THEORETICALLY GROUND
   Gal Y., 2015, BAYESIAN CONVOLUTION
   Gal Y., 2017, NIPS
   Gal  Yarin, 2016, ICML
   Guo  Chuan, 2017, ICML
   He K., 2016, CVPR
   Ivanovitch Silva D. J. S. L. A. C., 2012, IN CINC
   Johnson A. E., MIMIC 3 FREELY ACCES
   Kendall A, 2015, BAYESIAN SEGNET MODE
   Kendall A., 2017, NIPS
   Kingma D. P., 2015, VARIATIONAL DROPOUT
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, NIPS
   Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2010, MNIST HANDWRITTEN DI
   Maddison C. J., 2016, CONCRETE DISTRIBUTIO
   Naeini M. P., 2015, AAAI
   Sohn K., 2015, NIPS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar S., 2015, NIPS
   Tanno  R., 2017, BAYESIAN IMAGE QUALI
   van der Westhuizen  J., 2017, BAYESIAN LSTMS MED
   Xu K, 2015, ICML
   Zhu  L., 2017, DEEP CONFIDENT PREDI
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300084
DA 2019-06-15
ER

PT S
AU Herzig, R
   Raboh, M
   Chechik, G
   Berant, J
   Globerson, A
AF Herzig, Roei
   Raboh, Moshiko
   Chechik, Gal
   Berant, Jonathan
   Globerson, Amir
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Mapping Images to Scene Graphs with Permutation-Invariant Structured
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple interrelated objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state-of-the-art results on the Visual Genome scene-graph labeling benchmark, outperforming all recent approaches.
C1 [Herzig, Roei; Raboh, Moshiko; Globerson, Amir] Tel Aviv Univ, Tel Aviv, Israel.
   [Chechik, Gal] Bar Ilan Univ, NVIDIA Res, Ramat Gan, Israel.
   [Berant, Jonathan] Tel Aviv Univ, AI2, Tel Aviv, Israel.
RP Herzig, R (reprint author), Tel Aviv Univ, Tel Aviv, Israel.
EM roeiherzig@mail.tau.ac.il; mosheraboh@mail.tau.ac.il;
   gal.chechik@biu.ac.il; joberant@cs.tau.ac.il; gamir@post.tau.ac.il
FU ISF Centers of Excellence grant; Yandex Initiative in Machine Learning
FX This work was supported by the ISF Centers of Excellence grant, and by
   the Yandex Initiative in Machine Learning. Work by GC was performed
   while at Google Brain Research.
CR Bahdanau  D., 2015, INT C LEARN REPR ICL
   Belanger David, 2017, P 34 INT C MACH LEAR, V70, P429
   Bello I., 2016, ARXIV161109940
   Bui Hung Hai, 2013, UAI, P132
   CHEN D, 2014, P 2014 C EMP METH NA, V2014, P740, DOI DOI 10.3115/V1/D14-1082
   Chen Liang Chieh, 2014, P 2 INT C LEARN REPR
   Chen Liang Chieh, 2015, P ICML
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gilmer J., 2017, ARXIV170401212
   Gygli Michael, 2017, P MACHINE LEARNING R, V70, P1341
   Jianwei Yang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11205), P690, DOI 10.1007/978-3-030-01246-5_41
   Johnson J., 2018, ARXIV180401622
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Khalil Elias, 2017, ADV NEURAL INFORM PR, P6351
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Liao Wentong, 2016, ARXIV160905834
   Lin G., 2015, ADV NEURAL INFORM PR, P361
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Meshi O., 2010, P 27 INT C MACH LEAR, P783
   Newell A, 2017, ADV NEURAL INFORM PR, P2274
   Newell Alejandro, 2017, ADV NEURAL INFORM PR, V30, P1172
   Pei W., 2015, P 53 ANN M ASS COMP, P313
   Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213
   Raposo David, 2017, ARXIV170205068
   Schwing Alexander G, 2015, ARXIV E PRINTS
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Taskar B, 2004, ADV NEUR IN, V16, P25
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Zaheer M, 2017, ADV NEURAL INFORM PR, P3394
   Zellers Rowan, 2017, ARXIV171106640
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001073
DA 2019-06-15
ER

PT S
AU Hoffer, E
   Banner, R
   Golan, I
   Soudry, D
AF Hoffer, Elad
   Banner, Ron
   Golan, Itay
   Soudry, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Norm matters: efficient and accurate normalization schemes in deep
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used L-2 batch-norm, using normalization in L-1 and L-infinity spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.(2)
C1 [Hoffer, Elad; Golan, Itay; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel.
   [Banner, Ron] Intel Artificial Intelligence Prod Grp AIPG, Santa Clara, CA USA.
RP Hoffer, E (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM elad.hoffer@gmail.com; ron.banner@intel.com; itaygolan@gmail.com;
   daniel.soudry@gmail.com
FU Israel Science Foundation [31/1031]; Taub foundation
FX This research was supported by the Israel Science Foundation (grant No.
   31/1031), and by the Taub foundation. A Titan Xp used for this research
   was donated by the NVIDIA Corporation.
CR Arpit Devansh, 2016, INT C MACH LEARN, P1168
   Ba J. L., 2016, ARXIV160706450
   Bahdanau D., 2014, ARXIV14090473
   Bos S., 1996, Artificial Neural Networks - ICANN 96. 1996 International Conference Proceedings, P551
   Bos S, 1996, IEEE IJCNN, P241, DOI 10.1109/ICNN.1996.548898
   Cooijmans T., 2016, ARXIV160309025
   Courbariaux M., 2014, ARXIV14127024
   Das D., 2018, ARXIV180200930
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Eidnes L., 2017, ARXIV170904054
   Gitman I., 2017, CORR
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729
   Huang L., 2017, ARXIV171002338
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Ioffe Sergey, 2017, ADV NEURAL INFORM PR, P1942
   Kingma D. P., 2014, ARXIV14126980
   Klambauer G., 2017, ADV NEURAL INFORM PR, P971
   Koster U., 2017, ADV NEURAL INFORM PR, P1740
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krogh A., 1992, NEURAL INFORM PROCES, P950
   Micikevicius P., 2018, INT C LEARN REPR
   Rota Bulo S., 2017, ARXIV171202616
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Smith S. L., 2017, ARXIV171100489
   Soudry D., 2014, ADV NEURAL INFORM PR, P963
   Soudry D., 2018, INT C LEARN REPR
   Takeru Miyato M. K. Y. Y., 2018, INT C LEARN REPR
   Ulyanov D., 2016, CORR
   van Laarhoven T., 2017, ARXIV170605350
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Venkatesh G, 2017, INT CONF ACOUST SPEE, P2861, DOI 10.1109/ICASSP.2017.7952679
   Wu S., 2018, ARXIV E PRINTS
   Wu Y.X., 2018, ARXIV180308494
   Xiang S., 2017, ARXIV170403971
   Zhang C, 2016, ARXIV161103530
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302019
DA 2019-06-15
ER

PT S
AU Hoffman, J
   Mohri, M
   Zhang, NS
AF Hoffman, Judy
   Mohri, Mehryar
   Zhang, Ningshan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Algorithms and Theory for Multiple-Source Adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DOMAIN ADAPTATION
AB We present a number of novel contributions to the multiple-source adaptation problem. We derive new normalized solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. We also provide new guarantees that hold in the case where the conditional probabilities for the source domains are distinct. Moreover, we give new algorithms for determining the distribution-weighted combination solution for the cross-entropy loss and other losses. We report the results of a series of experiments with real-world datasets. We find that our algorithm outperforms competing approaches by producing a single robust model that performs well on any target mixture distribution. Altogether, our theory, algorithms, and empirical results provide a full solution for the multiple-source adaptation problem with very practical benefits.
C1 [Hoffman, Judy] Univ Calif Berkeley, CS Dept, Berkeley, CA 94720 USA.
   [Mohri, Mehryar] Courant Inst, New York, NY 10012 USA.
   [Mohri, Mehryar] Google, New York, NY 10012 USA.
   [Zhang, Ningshan] NYU, New York, NY 10012 USA.
RP Hoffman, J (reprint author), Univ Calif Berkeley, CS Dept, Berkeley, CA 94720 USA.
EM jhoffman@eecs.berkeley.edu; mohri@cims.nyu.edu; nzhang@stern.nyu.edu
FU NSF [IIS-1618662]
FX We thank Cyril Allauzen for comments on a previous draft of this paper.
   This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.
CR Arndt C., 2004, SIGNALS COMMUNICATIO
   Blanchard G., 2011, ADV NEURAL INFORM PR, V1, P2178
   Blitzer J., 2007, ANN M ASS COMP LING, V7, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Cortes C., 2015, INT C KNOWL DISC DAT, P169
   Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Crammer K, 2008, J MACH LEARN RES, V9, P1757
   Donahue J., 2014, P INT C MACH LEARN, P647
   Dredze Mark, 2008, P 25 INT C MACH LEAR, P264
   Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411
   Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   Ganin Y., 2015, INT C MACH LEARN, P1180
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong B., 2013, P 30 INT C MACH LEAR, V28, P222
   Gong B., 2013, ADV NEURAL INFORM PR, P1286
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50
   Hoffman  Judy, 2013, ICLR
   Huang J., 2006, ADV NEURAL INFORM PR, P601
   Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Liao H, 2013, INT CONF ACOUST SPEE, P7947, DOI 10.1109/ICASSP.2013.6639212
   Mansour Y, 2008, P 22 ANN C NEUR INF, P1041
   Mansour Y., 2009, UAI, P367
   Mansour  Yishay, 2009, COLT
   Mingsheng L., 2015, INT C MACH LEARN, P97
   Muandet K., 2013, P 30 INT C MACH LEAR, P10
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pei Z., 2018, AAAI, P3934
   RENYI A., 1961, P 4 BERK S MATH STAT, V1, P547, DOI DOI 10.1021/JP106846B
   Roark B, 2012, P ACL 2012 SYST DEM, P61
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Shai Ben-David, 2006, P 20 ANN C NEUR INF, P137
   Sriperumbudur BK, 2012, NEURAL COMPUT, V24, P1391, DOI 10.1162/NECO_a_00283
   Taigman  Y., 2017, ICLR
   Tao P. D., 1997, ACTA MATH VIETNAMICA, V22, P289
   Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Xu Z, 2014, LECT NOTES COMPUT SC, V8691, P628, DOI 10.1007/978-3-319-10578-9_41
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhang K., 2015, P 29 AAAI C ART INT, P3150
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002076
DA 2019-06-15
ER

PT S
AU Hoffman, MD
   Johnson, MJ
   Tran, D
AF Hoffman, Matthew D.
   Johnson, Matthew J.
   Tran, Dustin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific
   Language
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.(1)
C1 [Hoffman, Matthew D.] Google AI, Mountain View, CA 94043 USA.
   [Johnson, Matthew J.; Tran, Dustin] Google Brain, Mountain View, CA USA.
RP Hoffman, MD (reprint author), Google AI, Mountain View, CA 94043 USA.
EM mhoffman@google.com; mattjj@google.com; trandustin@google.com
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Blei DM, 2005, P 18 INT C NEUR INF
   Carette Jacques, 2016, Practical Aspects of Declarative Languages. 18th International Symposium, PADL 2016. Proceedings: LNCS 9585, P135, DOI 10.1007/978-3-319-28228-2_9
   Cook SR, 2006, J COMPUT GRAPH STAT, V15, P675, DOI 10.1198/106186006X136976
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Diehl S., 2013, PYREWRITE PYTHON TER
   Gehr T, 2016, LECT NOTES COMPUT SC, V9779, P62, DOI 10.1007/978-3-319-41528-4_4
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Goodman N. D., 2014, DESIGN IMPLEMENTATIO
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jaakkola T., 1996, INT WORKSH ART INT S, V82, P4
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Khan M. E, 2016, C UNC ART INT UAI
   Khan M. E., 2017, ARTIFICIAL INTELLIGE
   Khan M. E., 2015, ADV NEURAL INFORM PR, P3402
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kucukelbir A., 2016, ARXIV160300788
   Maclaurin D, 2014, AUTOGRAD REVERSE MOD
   Murray L. M, 2018, ARTIFICIAL INTELLIGE
   Narayanan Praveen, 2016, Functional and Logic Programming. 13th International Symposium, FLOPS 2016. Proceedings: LNCS 9613, P62, DOI 10.1007/978-3-319-29604-3_5
   Narayanan P., 2017, P ACM PROGRAMMING LA, V1, P11
   Nipkow T., 1999, TERM REWRITING ALL
   Radul A., 2013, RULES EXTENSIBLE PAT
   Rozenberg G, 1997, HDB GRAPH GRAMMARS C, V1
   SPIEGELHALTER D, 1995, BUGS BAYESIAN INFERE
   Sussman G. J, 2018, SCMUTILS
   Tran D, 2018, NEURAL INFORM PROCES
   Tristan J.-B, 2014, NEURAL INFORM PROCES
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Winn J, 2005, J MACH LEARN RES, V6, P661
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005031
DA 2019-06-15
ER

PT S
AU Hong, S
   Yan, XC
   Huang, T
   Lee, H
AF Hong, Seunghoon
   Yan, Xinchen
   Huang, Thomas
   Lee, Honglak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Hierarchical Semantic Image Manipulation through Structured
   Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation on natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representation for manipulation. Initialized with coarse-level bounding boxes, our structure generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.
C1 [Hong, Seunghoon; Yan, Xinchen; Huang, Thomas; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Lee, Honglak] Google Brain, Mountain View, CA USA.
RP Hong, S (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM hongseu@umich.edu; xcyan@umich.edu; thomaseh@umich.edu;
   honglak@umich.edu
FU ONR [N00014-13-1-0762]; NSF CAREER [IIS-1453651]; DARPA Explainable AI
   (XAI) program [313498]; Sloan Research Fellowship; Adobe Research
   Fellowship; Google PhD Fellowship
FX This work was supported in part by ONR N00014-13-1-0762, NSF CAREER
   IIS-1453651, DARPA Explainable AI (XAI) program #313498, Sloan Research
   Fellowship, and Adobe Research Fellowship and Google PhD Fellowship to
   X. Yan.
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chen L.-C., 2018, ARXIV 1802 02611
   Chen Qifeng, 2017, ICCV
   Cordts M., 2016, CVPR
   Denton E, 2016, ARXIV161106430
   Goodfellow I., 2014, NIPS
   Gupta A., 2010, ECCV
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Hoiem  Derek, 2005, ICCV
   Hong S., 2018, CVPR
   Isola  Phillip, 2017, CVPR
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2012, NIPS
   Li Y., 2017, CVPR
   Liu C., 2016, DENSE IMAGE CORRES C, P15
   Liu M. -Y., 2017, NIPS
   Pathak D., 2016, CVPR
   Radford A., 2016, ICLR
   Reed S., 2016, NIPS
   Sangkloy P., 2017, CVPR
   Simonyan K, 2014, ARXIV14091556
   Szegedy C., 2015, CVPR
   Vondrick C., 2016, NIPS
   Wang T.-C., 2017, ICCV
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xian W., 2018, CVPR
   Yan X., 2016, ECCV
   Yang J., 2017, ICLR
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208
   Yeh Raymond A, 2017, P IEEE C COMP VIS PA, P5485
   Zhou  Bolei, 2017, CVPR
   Zhu J.-Y., 2017, ICCV
   Zhu Jun-Yan, 2016, ECCV
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302070
DA 2019-06-15
ER

PT S
AU Hong, ZW
   Shann, TY
   Su, SY
   Chang, YH
   Fu, TJ
   Lee, CY
AF Hong, Zhang-Wei
   Shann, Tzu-Yun
   Su, Shih-Yang
   Chang, Yi-Hsiang
   Fu, Tsu-Jui
   Lee, Chun-Yi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Diversity-Driven Exploration Strategy for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off-and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure regularization to the loss function, the proposed methodology significantly enhances an agent's exploratory behavior, and thus prevents the policy from being trapped in local optima. We further propose an adaptive scaling strategy to enhance the performance. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results validate that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.
C1 [Hong, Zhang-Wei; Shann, Tzu-Yun; Su, Shih-Yang; Chang, Yi-Hsiang; Fu, Tsu-Jui; Lee, Chun-Yi] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
RP Hong, ZW (reprint author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM williamd4112@gapp.nthu.edu.tw; arielshann@gapp.nthu.edu.tw;
   at7788546@gapp.nthu.edu.tw; shawn420@gapp.nthu.edu.tw;
   rayfu1996ozig@gapp.nthu.edu.tw; cylee@gapp.nthu.edu.tw
FU Ministry of Science and Technology (MOST) in Taiwan; MediaTek Inc.;
   NVIDIA Corporation
FX The authors would like to thank Ministry of Science and Technology
   (MOST) in Taiwan and MediaTek Inc. for their funding support, and NVIDIA
   Corporation and NVAITC for their support of GPUs.
CR Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Conti, 2017, ARXIV171206560
   Fortunato, 2018, P INT C LEARN REPR I
   Haarnoja T., 2018, ARXIV180101290
   Haarnoja Tuomas, 2018, P 35 INT C MACH LEAR, V80, P1861
   Lehman J, 2011, GENET EVOL COMPUT, P37
   Lehman J, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P211
   Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025
   Lillicrap, 2016, ARXIV150902971
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Osband I., 2017, ARXIV170307608
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Pathak D., 2017, P INT C MACH LEARN, P2778
   Peters J, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P1607
   Plappert, 2018, P INT C LEARN REPR I
   Schulman, 2017, ARXIV170706347
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Silver  D., 2014, ICML, P387
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stadie BC, 2015, ARXIV150700814
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tang Haoran, 2017, ADV NEURAL INFORM PR, P2750
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Wu, 2017, ADV NEURAL INFORM PR, P5285
   Zhang M, 2016, IEEE INT CONF ROBOT, P520, DOI 10.1109/ICRA.2016.7487174
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005010
DA 2019-06-15
ER

PT S
AU Hoshen, Y
AF Hoshen, Yedid
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Non-Adversarial Mapping with VAEs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The study of cross-domain mapping without supervision has recently attracted much attention. Much of the recent progress was enabled by the use of adversarial training as well as cycle constraints. The practical difficulty of adversarial training motivates research into non-adversarial methods. In a recent paper, it was shown that cross-domain mapping is possible without the use of cycles or GANs. Although promising, this approach suffers from several drawbacks including costly inference and an optimization variable for every training example preventing the method from using large training sets. We present an alternative approach which is able to achieve non-adversarial mapping using a novel form of Variational Auto-Encoder. Our method is much faster at inference time, is able to leverage large datasets and has a simple interpretation.
C1 [Hoshen, Yedid] Facebook AI Res, Tel Aviv, Israel.
RP Hoshen, Y (reprint author), Facebook AI Res, Tel Aviv, Israel.
CR Arjovsky Martin, 2017, ICLR
   Benaim Sagie, 2017, NIPS
   Bojanowski Piotr, 2018, ICML
   Chen Qifeng, 2017, ICCV
   Choi Yunjey, 2018, CVPR
   Fabius Otto, 2014, ICLR WORKSH
   Gulrajani  Ishaan, 2017, NIPS
   Hoshen Yedid, 2018, INT C LEARN REPR
   Hoshen Yedid, 2018, ICLR WORKSH
   Hoshen Yedid, 2018, ECCV
   Karras Tero, 2017, ARXIV171010196
   Kim Taeksoo, 2017, ICML
   Kingma Diederik P, 2014, ICLR
   Liu M. -Y., 2017, NIPS
   Liu Ming-Yu, 2016, NIPS
   Miller Erik G, 2000, CVPR
   Miyato Takeru, 2018, ICLR
   Radford A., 2016, ICLR
   Tolstikhin Ilya, 2018, ICLR
   Wolf Lior, 2018, ARXIV180106126
   Yi Z, 2017, ICCV
   Yu Aron, 2014, CVPR
   Zhang Han, 2017, ARXIV171010916
   Zhu J.-Y., 2017, ICCV
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002011
DA 2019-06-15
ER

PT S
AU Hoskins, JG
   Musco, C
   Musco, C
   Tsourakakis, CE
AF Hoskins, Jeremy G.
   Musco, Cameron
   Musco, Christopher
   Tsourakakis, Charalampos E.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inferring Networks From Random Walk-Based Node Similarities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GRAPH; ALGORITHM; TREES; RESISTANCE
AB Digital presence in the world of online social media entails significant privacy risks [31, 56]. In this work we consider a privacy threat to a social network in which an attacker has access to a subset of random walk-based node similarities, such as effective resistances (i.e., commute times) or personalized PageRank scores. Using these similarities, the attacker seeks to infer as much information as possible about the network, including unknown pairwise node similarities and edges.
   For the effective resistance metric, we show that with just a small subset of measurements, one can learn a large fraction of edges in a social network. We also show that it is possible to learn a graph which accurately matches the underlying network on all other effective resistances. This second observation is interesting from a data mining perspective, since it can be expensive to compute all effective resistances or other random walk-based similarities. As an alternative, our graphs learned from just a subset of effective resistances can be used as surrogates in a range of applications that use effective resistances to probe graph structure, including for graph clustering, node centrality evaluation, and anomaly detection.
   We obtain our results by formalizing the graph learning objective mathematically, using two optimization problems. One formulation is convex and can be solved provably in polynomial time. The other is not, but we solve it efficiently with projected gradient and coordinate descent. We demonstrate the effectiveness of these methods on a number of social networks obtained from Facebook. We also discuss how our methods can be generalized to other random walk-based similarities, such as personalized PageRank scores. Our code is available at https://github.com/cnmusco/graph-similarity-learning.
C1 [Hoskins, Jeremy G.] Yale Univ, Dept Math, New Haven, CT 06520 USA.
   [Musco, Cameron] Microsoft Res, Cambridge, MA USA.
   [Musco, Christopher] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
   [Tsourakakis, Charalampos E.] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.
   [Tsourakakis, Charalampos E.] Harvard Univ, Boston, MA 02115 USA.
RP Hoskins, JG (reprint author), Yale Univ, Dept Math, New Haven, CT 06520 USA.
EM jeremy.hoskins@yale.edu; camusco@microsoft.com; cmusco@cs.princeton.edu;
   ctsourak@bu.edu
CR Abebe Rediet, 2014, PRIVATE LINK PREDICT
   Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1
   Al Hasan M, 2011, SOCIAL NETWORK DATA ANALYTICS, P243
   Andersen Reid, 2006, 47 ANN S FDN COMP SC
   Angluin D, 2008, J COMPUT SYST SCI, V74, P546, DOI 10.1016/j.jcss.2007.06.006
   Backstrom L., 2007, P 16 INT C WORLD WID, P181, DOI DOI 10.1145/1242572.1242598
   BATAGELJ V, 1990, INT J COMPUT MATH, V34, P171, DOI 10.1080/00207169008803874
   Ben-Hamou Anna, 2018, P ACM SIAM S DISCR A
   Blondel VD, 2004, SIAM REV, V46, P647, DOI 10.1137/S0036144502415960
   Boyd Stephen, 2004, SIAM REV
   Brin Sergey, 1999, TECHNICAL REPORT
   Castro R, 2004, STAT SCI, V19, P499, DOI 10.1214/088342304000000422
   Chandra A. K., 1996, Computational Complexity, V6, P312, DOI 10.1007/BF01270385
   Chen D, 2010, PROC APPL MATH, V135, P1309
   Cooper C, 2014, SOC NETW ANAL MIN, V4, DOI 10.1007/s13278-014-0168-6
   CULBERSON JC, 1989, INFORM PROCESS LETT, V30, P215, DOI 10.1016/0020-0190(89)90216-0
   Desper R, 1999, J COMPUT BIOL, V6, P37, DOI 10.1089/cmb.1999.6.37
   FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.1111/j.1558-5646.1985.tb00420.x
   Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46
   Ghosh A, 2008, SIAM REV, V50, P37, DOI 10.1137/050645452
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Grant M., 2014, CVX MATLAB SOFTWARE
   Haveliwala Taher H., 2003, IEEE T KNOWLEDGE DAT
   Jeh G., 2002, P 8 ACM SIGKDD INT C, P538, DOI DOI 10.1145/775047.775126
   Jeh G., 2003, P 12 INT C WORLD WID, P271, DOI DOI 10.1145/775152.775191
   Kalofolias Vassilis, 2016, J MACHINE LEARNING R
   Kannan Sampath, 2015, INT C AUT LANG PROGR
   KATZ L, 1953, PSYCHOMETRIKA, V18, P39
   Katzir Liran, 2011, P 20 INT C WORLD WID
   KLEIN DJ, 1993, J MATH CHEM, V12, P81, DOI 10.1007/BF01164627
   Kleinberg J. M., 1999, Computing and Combinatorics. 5th Annual International Conference, COCOON'99. Proceedings (Lecture Notes in Computer Science Vol.1627), P1
   Korolova A., 2008, P 17 ACM C INF KNOWL, P289
   Leskovec J., 2012, ADV NEURAL INFORM PR, P539
   Leskovec J., 2014, SNAP DATASETS STANFO
   Leskovec Jure, 2010, P 19 INT C WORLD WID, DOI 10.1145/1772690.1772756
   Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591
   Mcrae BH, 2008, ECOLOGY, V89, P2712, DOI 10.1890/07-1861.1
   McRae BH, 2006, EVOLUTION, V60, P1551, DOI 10.1111/j.0014-3820.2006.tb00500.x
   MOSEK ApS, 2017, MOSEK OPT SUIT
   Perozzi Bryan, 2014, P 20 ACM INT C KNOWL
   Pilliod David, 2015, EFFECTS CHANGING CLI, V5
   Rattigan M., 2005, SIGKDD EXPLORATIONS, V7, P41, DOI DOI 10.1145/1117454.1117460
   Reyzin L, 2007, INFORM PROCESS LETT, V101, P98, DOI 10.1016/j.ipl.2006.08.013
   Saerens M, 2004, LECT NOTES COMPUT SC, V3201, P371
   Sarkar P., 2007, P 23 C UNC ART INT, P335
   Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029
   Spielman Daniel A., 2012, U LECT
   Stone EA, 2009, LINEAR ALGEBRA APPL, V431, P1869, DOI 10.1016/j.laa.2009.06.024
   Sun J, 2006, SIAM REV, V48, P681, DOI 10.1137/S0036144504443821
   Tong H., 2006, FAST RANDOM WALK RES
   Tsourakakis Charalampos E, 2017, ARXIV170907308
   von Luxburg U, 2014, J MACH LEARN RES, V15, P1751
   Von Luxburg Ulrike, 2010, ADV NEURAL INFORM PR
   Wittmann Dominik M., 2009, THEORETICAL COMPUTER
   Yen Luh, 2007, ADV KNOWLEDGE DISCOV
   Zheleva E., 2012, SYNTHESIS LECT DATA, V3, P1
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303068
DA 2019-06-15
ER

PT S
AU Hou, QB
   Jiang, PT
   Wei, YC
   Cheng, MM
AF Hou, Qibin
   Jiang, Peng-Tao
   Wei, Yunchao
   Cheng, Ming-Ming
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Self-Erasing Network for Integral Object Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions from spreading to unexpected background regions. In particular, SeeNet leverages two self-erasing strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of our SeeNet over other state-of-the-art methods.
C1 [Hou, Qibin; Jiang, Peng-Tao; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
   [Wei, Yunchao] UIUC, Urbana, IL USA.
RP Cheng, MM (reprint author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
EM andrewhoux@gmail.com; cmm@nankai.edu.cn
FU NSFC [61620106008, 61572264]; national youth talent support program,
   Tianjin Natural Science Foundation for Distinguished Young Scholars
   [17JCJQJC43700]; Huawei Innovation Research Program
FX This research was supported by NSFC (NO. 61620106008, 61572264), the
   national youth talent support program, Tianjin Natural Science
   Foundation for Distinguished Young Scholars (NO. 17JCJQJC43700), and
   Huawei Innovation Research Program.
CR Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chaudhry  Arslan, 2017, BMVC
   Chen L. C., 2017, IEEE TPAMI
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Di Lin, 2016, CVPR
   Everingham M., 2015, IJCV
   Hariharan B., 2011, ICCV
   He K., 2016, CVPR
   Hong  Seunghoon, 2017, CVPR, P3626
   Hou Q., 2018, IEEE TPAMI
   Hou  Qibin, 2017, EMMCVPR
   Hou  Qibin, 2018, ARXIV180309859
   Jiang  Huaizu, 2018, FRONT COMPUT SCI
   Jin  Bin, 2017, P IEEE C COMP VIS PA, P3626
   Kim  Dahun, 2017, ICCV
   Kolesnikov A., 2016, ECCV
   Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599
   Li  Kunpeng, 2018, CVPR
   Lin  Guosheng, 2017, CVPR
   Long  J., 2015, CVPR
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oh Seong Joon, 2017, CVPR
   Papandreou G., 2015, ICCV
   Pathak D., 2015, ICCV
   Pinheiro Pedro O, 2015, CVPR
   Qi  Xiaojuan, 2016, ECCV
   Roy  Anirban, 2017, CVPR
   Russakovsky Olga, 2015, IJCV
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Simonyan Karen, 2015, ICLR
   Simonyan  Karen, 2014, ICLRW
   Torralba A., 2016, CVPR
   Wang  Jingdong, 2017, IJCV
   Wei  Yunchao, 2017, CVPR
   Wei  Yunchao, 2018, CVPR
   Wei  Yunchao, 2016, IEEE TPAMI
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhang  Jianming, 2016, ECCV
   Zhang Xiangyu, 2018, CVPR
   Zhao H., 2017, CVPR
   Zheng S., 2015, ICCV
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300051
DA 2019-06-15
ER

PT S
AU Houthooft, R
   Chen, RY
   Isola, P
   Stadie, BC
   Wolski, F
   Ho, J
   Abbeel, P
AF Houthooft, Rein
   Chen, Richard Y.
   Isola, Phillip
   Stadie, Bradly C.
   Wolski, Filip
   Ho, Jonathan
   Abbeel, Pieter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Evolved Policy Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.
C1 [Houthooft, Rein; Chen, Richard Y.; Isola, Phillip; Stadie, Bradly C.; Wolski, Filip; Ho, Jonathan] OpenAI, San Francisco, CA 94110 USA.
   [Isola, Phillip; Stadie, Bradly C.; Ho, Jonathan; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA USA.
   [Isola, Phillip] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Houthooft, R (reprint author), OpenAI, San Francisco, CA 94110 USA.
CR Brockman G, 2016, ARXIV160601540
   Chen Richard Y, 2017, ARXIV170601502
   Dearden R, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P150
   Duan  Y., 2016, ARXIV161102779
   Duan Y., 2016, INT C MACH LEARN, P1329
   Finn C., 2017, ARXIV171011622
   Finn C, 2017, ARXIV170303400
   Haarnoja T., 2017, ARXIV170208165
   Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398
   Kolter J. Z., 2009, P 26 ANN INT C MACH, P513
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Lake Brenden M, 2017, BEHAV BRAIN SCI, V40
   Mishra N., 2017, ARXIV170703141
   Nachum O., 2017, ADV NEURAL INFORM PR, P2772
   Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2
   O'Donoghue B., 2016, ARXIV161101626
   Ostrovski Georg, 2017, ARXIV170301310
   Pathak Deepak, 2017, INT C MACH LEARN ICM, V2017
   Plappert Matthias, 2018, ARXIV180209464
   Rechenberg I., 1973, EVOLUTIONSSTRATEGIE
   Salimans T., 2017, ARXIV170303864
   Schmidhuber J., 2002, NAT COMP SER, P579
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman  J., 2015, ADV NEURAL INFORM PR, P3528
   Schulman J., 2017, ARXIV170406440
   Schulman  J., 2017, ARXIV170706347
   Schwefel H. -P., 1977, NUMERISCHE OPTIMIERU
   Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004
   SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tang Haoran, 2017, ADV NEURAL INFORM PR
   Wang J. X, 2016, ARXIV161105763
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305042
DA 2019-06-15
ER

PT S
AU Hsieh, JT
   Liu, BB
   Huang, DA
   Fei-Fei, L
   Niebles, JC
AF Hsieh, Jun-Ting
   Liu, Bingbin
   Huang, De-An
   Fei-Fei, Li
   Niebles, Juan Carlos
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Decompose and Disentangle Representations for Video
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we intuitively would do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.
C1 [Hsieh, Jun-Ting; Liu, Bingbin; Huang, De-An; Fei-Fei, Li; Niebles, Juan Carlos] Stanford Univ, Stanford, CA 94305 USA.
RP Hsieh, JT (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM junting@stanford.edu; bingbin@stanford.edu; dahuang@cs.stanford.edu;
   feifeili@cs.stanford.edu; jniebles@cs.stanford.edu
FU Panasonic; Oppo
FX This work was partially funded by Panasonic and Oppo. We thank our
   anonymous reviewers, John Emmons, Kuan Fang, Michelle Guo, and Jingwei
   Ji for their helpful feedback and suggestions.
CR Alahi  A., 2016, CVPR
   Battaglia Peter W., 2016, NIPS
   Chang M., 2018, ICLR
   Chang Michael B, 2017, ICLR
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   De Brabandere  B., 2016, NIPS
   Denton E, 2017, NIPS
   Finn Chelsea, 2016, NIPS
   Fragkiadaki Katerina, 2016, ICLR
   Gao D. J. R., 2016, ACCV
   Ghosh A., 2017, AAAI
   Gregor K., 2015, ARXIV150204623
   Hinton G. E., 2016, NIPS
   Jaderberg M., 2015, NIPS
   Kalchbrenner  N., 2017, ICML
   Karl  M., 2016, ICLR
   Kingma Diederik P, 2014, ICLR
   Kitani Kris M., 2012, ECCV
   Kosiorek A. R., 2018, NIPS
   Lan  T., 2014, ECCV
   Lotter W., 2016, ARXIV160508104
   Mathieu M., 2016, ICLR
   Oh  J., 2015, NIPS
   Oliu  M., 2018, ECCV
   Patraucean V, 2015, ARXIV151106309
   Paxton  C., 2018, ARXIV180400062
   Ranzato M, 2014, ARXIV14126604
   Rezende D.J., 2016, ARXIV160305106
   Schmidhuber J., 2017, NIPS
   Schmidhuber J., 2016, NIPS
   Soran  B., 2015, ICCV
   Srivastava N, 2015, ICML
   Sutskever  I., 2014, NIPS
   Tulyakov  S., 2017, ARXIV171111566
   Tulyakov S., 2017, ARXIV170704993
   Van Amersfoort  J., 2017, ARXIV170108435
   Villegas R., 2017, ARXIV170405831
   Villegas R., 2017, ICLR
   Vondrick C., 2016, NIPS
   Vondrick  C., 2017, CVPR
   Walker  J., 2017, ICCV
   Walker  J., 2016, ECCV
   Xingjian  S., 2015, NIPS
   Xue  T., 2016, NIPS
   Zhou  T., 2016, ECCV
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300048
DA 2019-06-15
ER

PT S
AU Hsieh, YP
   Kavis, A
   Rolland, P
   Cevher, V
AF Hsieh, Ya-Ping
   Kavis, Ali
   Rolland, Paul
   Cevher, Volkan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Mirrored Langevin Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of sampling from constrained distributions, which has posed significant challenges to both non-asymptotic analysis and algorithmic design. We propose a unified framework, which is inspired by the classical mirror descent, to derive novel first-order sampling schemes. We prove that, for a general target distribution with strongly convex potential, our framework implies the existence of a first-order algorithm achieving (O) over tilde (epsilon(-2)d) convergence, suggesting that the state-of-the-art (O) over tilde (epsilon(-6)d(5)) can be vastly improved. With the important Latent Dirichlet Allocation (LDA) application in mind, we specialize our algorithm to sample from Dirichlet posteriors, and derive the first non-asymptotic (O) over tilde (epsilon(-2)d(2)) rate for first-order sampling. We further extend our framework to the mini-batch setting and prove convergence rates when only stochastic gradients are available. Finally, we report promising experimental results for LDA on real datasets.
C1 [Hsieh, Ya-Ping; Kavis, Ali; Rolland, Paul; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
RP Hsieh, YP (reprint author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
EM ya-ping.hsieh@epfl.ch; ali.kavis@epfl.ch; paul.rolland@epfl.ch;
   volkan.cevher@epfl.ch
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation programme [725594]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant agreement no 725594 - time-data).
CR Ahn Sungjin, 2012, P 29 INT COF INT C M, P1771
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Brosse Nicolas, 2017, P MACHINE LEARNING R, V65, P319
   Bubeck Sebastien, 2015, ARXIV150702564
   Chen C., 2015, ADV NEURAL INFORM PR, P2278
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Cheng X., 2017, ARXIV170703663
   Cheng  Xiang, 2018, P 29 INT C ALG LEARN, P186
   Dai Bo, 2016, ARTIF INTELL, P985
   Dalalyan A. S, 2017, ARXIV171000095
   Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Durmus Alain, 2018, ARXIV180209188
   Durmus Alain, 2016, ADV NEURAL INFORM PR, P2047
   Dwivedi R., 2018, ARXIV180102309
   Krichene Walid, 2017, ADV NEURAL INFORM PR, P6799
   Lan SW, 2016, ADV COMPUT VIS PATT, P25, DOI 10.1007/978-3-319-45026-1_2
   Liu Chang, 2016, ADV NEURAL INFORM PR, P3009
   Luu Tung, 2017, SAMPLING NONSMOOTH D
   Ma Y.-A., 2015, ADV NEURAL INFORM PR, V2, P2917
   Mandelbrot B. B., 1983, FRACTAL GEOMETRY NAT, V173
   McCann RJ, 1995, DUKE MATH J, V80, P309, DOI 10.1215/S0012-7094-95-08013-2
   Mertikopoulos P, 2018, SIAM J OPTIMIZ, V28, P163, DOI 10.1137/16M1105682
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639
   Rockafellar R T, 1970, CONVEX ANAL
   Simsekli  Umut, 2016, INT C MACH LEARN, P642
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Villani C., 2003, TOPICS OPTIMAL TRANS, V58
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Xu Pan, 2018, INT C ART INT STAT, P1087
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302086
DA 2019-06-15
ER

PT S
AU Hu, HX
   Chen, LY
   Gong, BQ
   Sha, F
AF Hu, Hexiang
   Chen, Liyu
   Gong, Boqing
   Sha, Fei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Synthesized Policies for Transfer and Adaptation across Tasks and
   Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (epsilon) and tasks (tau), probably more importantly, by learning from only sparse (epsilon, tau) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GRIDWORLD and THOR, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (epsilon, tau) pairs after learning from only 40% of them.
C1 [Hu, Hexiang; Chen, Liyu] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Gong, Boqing] Tencent AI Lab, Bellevue, WA 98004 USA.
   [Sha, Fei] Netflix, Los Angeles, CA 90028 USA.
RP Hu, HX (reprint author), Univ Southern Calif, Los Angeles, CA 90089 USA.
EM hexiangh@usc.edu; liyuc@usc.edu; boqinggo@outlook.com; fsha@netflix.com
FU DARPA [FA8750-18-2-0117]; NSF [IIS-1065243, 1451412,
   1513966/1632803/1833137, 1208500, CCF-1139148]; Google Research Award;
   Alfred P. Sloan Research Fellowship; ARO [W911NF-12-1-0241,
   W911NF-15-1-0484]
FX We appreciate the feedback from the reviewers. This work is partially
   supported by DARPA#FA8750-18-2-0117, NSF IIS-1065243, 1451412,
   1513966/1632803/1833137, 1208500, CCF-1139148, a Google Research Award,
   an Alfred P. Sloan Research Fellowship, gifts from Facebook and Netflix,
   and ARO#W911NF-12-1-0241 and W911NF-15-1-0484.
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Andreas Jacob, 2017, ICML
   Barreto  A., 2017, NIPS
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao  W.-L., 2016, ECCV
   DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613
   Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Hinton G., 2015, ARXIV150302531
   Ho  J., 2016, ADV NEURAL INFORM PR, P4565
   Huang Sandy, 2017, ARXIV170202284
   Jaderberg M., 2016, ABS161105397 CORR
   Kolve  E., 2017, ABS171205474 CORR
   Kulkarni T. D., 2016, ABS160602396 CORR
   Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Oh  J., 2017, ARXIV170605064
   Parisotto Emilio, 2015, ARXIV151106342
   Schaul  T., 2015, ICML
   Schulman  J., 2017, ARXIV170706347
   Silver  D., 2017, ABS171201815 CORR
   Sutton R. S., 1998, IEEE T NEURAL NETWOR, V16, P285
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Teh Yee, 2017, ADV NEURAL INFORM PR, P4499
   Vinyals O., 2017, ARXIV170804782
   Wilson A., 2007, P 24 INT C MACH LEAR, P1015
   Zhang C, 2018, ARXIV180406893
   Zhu  Y., 2017, P IEEE INT C COMP VI, V2, P7
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301018
DA 2019-06-15
ER

PT S
AU Hu, J
   Shen, L
   Albanie, S
   Sun, G
   Vedaldi, A
AF Hu, Jie
   Shen, Li
   Albanie, Samuel
   Sun, Gang
   Vedaldi, Andrea
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gather-Excite: Exploiting Feature Context in Convolutional Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.
C1 [Hu, Jie; Sun, Gang] Momenta, Cambridge, MA 02142 USA.
   [Shen, Li; Albanie, Samuel; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England.
RP Hu, J (reprint author), Momenta, Cambridge, MA 02142 USA.
EM hujie@momenta.ai; lishen@robots.ox.ac.uk; albanie@robots.ox.ac.uk;
   sungang@momenta.ai; vedaldi@robots.ox.ac.uk
FU ESPRC AIMS CDT; ERC [638009-IDIU]
FX The authors would like to thank Andrew Zisserman and Aravindh Mahendran
   for many helpful discussions. Samuel Albanie is supported by ESPRC AIMS
   CDT. Andrea Vedaldi is supported by ERC 638009-IDIU.
CR Bell Sean, 2016, CVPR
   Biederman Irving, 1982, COGNITIVE PSYCHOL
   Chen Liang-Chieh, 2018, IEEE TPAMI
   Chen Long, 2017, CVPR
   Csurka Gabriella, 2004, ECCV WORKSH
   Divvala S. K., 2009, CVPR
   HANSON AR, 1978, COMPUTER VISION SYST
   He K., 2016, ECCV
   He K., 2016, CVPR
   He Kaiming, 2017, ICCV
   Heitz Geremy, 2008, ECCV
   Hinton G.E., 1986, PARALLEL DISTRIBUTED
   Hock Howard S, 1974, PERCEPTION PSYCHOPHY
   Howard Andrew G., 2017, ARXIV170404861
   Hu Jie, 2018, CVPR
   Huang G., 2016, ECCV
   Ke Tsung-Wei, 2017, CVPR
   Kligvasser Idan, 2018, CVPR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Lecun Y., 1998, P IEEE
   Lee H., 2009, ICML
   Lin Guosheng, 2016, CVPR
   Lin M., 2014, ICLR
   Lin T.-Y., 2014, ECCV
   Liu W, 2016, INT WORKS EARTH OB
   Luo W., 2016, NIPS
   Morcos Ari S, 2018, ICLR
   Mottaghi R., 2014, CVPR
   Murphy Kevin P, 2004, NIPS
   Novotny David, 2018, CVPR
   Ren S., 2015, NIPS
   Russakovsky Olga, 2015, IJCV
   Sanchez Jorge, 2013, IJCV
   Simonyan Karen, 2015, ICLR
   Strat Thomas M, 1991, IEEE TPMI
   Szegedy C., 2015, CVPR
   Szegedy Christian, 2016, ICLR WORKSH
   Torralba A., 2003, ICCV
   Torralba Antonio, 2003, IJCV
   Vaswani A., 2017, NIPS
   Wang Fei, 2017, CVPR
   Wang X., 2018, CVPR
   Wolf Lior, 2006, IJCV
   Woo Sanghyun, 2018, ECCV
   Xu K, 2015, ICML
   Yang Jun, 2007, MIR
   Yu F., 2016, ICLR
   Zagoruyko S., 2016, BMVC
   Zhang Xiangyu, 2018, CVPR
   Zoph Barret, 2018, CVPR
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004001
DA 2019-06-15
ER

PT S
AU Hu, SB
   Chen, ZT
   Nia, VP
   Chan, LW
   Geng, YH
AF Hu, Shoubo
   Chen, Zhitang
   Nia, Vahid Partovi
   Chan, Laiwan
   Geng, Yanhui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Causal Inference and Mechanism Clustering of A Mixture of Additive Noise
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.
C1 [Hu, Shoubo; Chan, Laiwan] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Chen, Zhitang; Nia, Vahid Partovi] Huawei Noahs Ark Lab, Hong Kong, Peoples R China.
   [Geng, Yanhui] Huawei Montreal Res Ctr, Montreal, PQ, Canada.
RP Hu, SB (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM sbhu@cse.cuhk.edu.hk; chenzhitang2@huawei.com;
   vahid.partovinia@huawei.com; lwchan@cse.cuhk.edu.hk;
   geng.yanhui@huawei.com
FU Hong Kong Research Grants Council
FX This work is partially supported by the Hong Kong Research Grants
   Council.
CR Daniusis P., 2012, ARXIV12033475
   Ester M, 1996, P 2 INT C KNOWL DISC, P226
   Fukumizu K, 2004, J MACH LEARN RES, V5, P73
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Gretton A., 2005, AISTATS 10, V10, P112
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Janzing D, 2010, IEEE T INFORM THEORY, V56, P5168, DOI 10.1109/TIT.2010.2060095
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Liu FR, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700477
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Mooij JM, 2016, J MACH LEARN RES, V17
   Rasmussen CE, 2000, ADV NEUR IN, V12, P554
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599
   Zhang K., 2009, P 25 C UNC ART INT, P647
   Zhang K., 2015, ARXIV150908056
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305024
DA 2019-06-15
ER

PT S
AU Hu, ZH
   Liang, YT
   Zhang, J
   Li, Z
   Liu, Y
AF Hu, Zehong
   Liang, Yitao
   Zhang, Jie
   Li, Zhao
   Liu, Yang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inference Aided Reinforcement Learning for Incentive Mechanism Design in
   Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs of exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to the existing one-shot mechanisms.
C1 [Hu, Zehong; Li, Zhao] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
   [Liang, Yitao] Univ Calif Los Angeles, Los Angeles, CA USA.
   [Zhang, Jie] Nanyang Technol Univ, Singapore, Singapore.
   [Liu, Yang] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Liu, Yang] Harvard Univ, Cambridge, MA 02138 USA.
RP Hu, ZH (reprint author), Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
EM HUZE0004@e.ntu.edu.sg; yliang@cs.ucla.edu; ZhangJ@ntu.edu.sg;
   lizhao.lz@alibaba-inc.com; yangliu@ucsc.edu
FU National Research Foundation (NRF) Singapore under the Corp
   Lab@University Scheme; NSF [IIS-1657613, IIS-1633857]; DARPA XAI
   [N66001-17-2-4032]; NSF CCF [1718549]
FX This work started when Zehong Hu was at the Rolls-Royce@NTU Corporate
   Lab with support from the National Research Foundation (NRF) Singapore
   under the Corp Lab@UniversityScheme.Yitao is partially supported by NSF
   grants #IIS-1657613, #IIS-1633857 and DARPA XAI grant #N66001-17-2-4032.
   Yang Liu acknowledges supports from NSF CCF #1718549. The authors also
   thank Anxiang Zeng from Alibaba Group for valuable discussions.
CR Chen X, 2015, J MACH LEARN RES, V16, P1
   Dasgupta  Anirban, 2013, P WWW
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Difallah Djellel Eddine, 2015, P WWW
   Engel  Yaakov, 2005, P ICML
   Gasic M, 2014, IEEE-ACM T AUDIO SPE, V22, P28, DOI 10.1109/TASL.2013.2282190
   HOWE J, 2006, WIRED MAG, V14, P14
   Jurca Radu, 2009, J ARTIFICIAL INTELLI, V34, P209
   Liang  Yitao, 2016, P AAMAS
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Liu  Qiang, 2012, P NIPS
   Liu Yang, 2017, P AAAI, P607
   MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Prelec D, 2004, SCIENCE, V306, P462, DOI 10.1126/science.1102081
   Provost  Foster, 2008, P SIGKDD
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Simpson Edwin D, 2015, P WWW
   Slivkins A, 2013, SI GECOM EXCH, V12, P4
   Snow  Rion, 2008, P EMNLP
   Witkowski  Jens, 2012, P ACM EC
   Zhang  Yuchen, 2014, P NIPS
   Zheng YD, 2017, PROC VLDB ENDOW, V10, P541, DOI 10.14778/3055540.3055547
   Zhou  Dengyong, 2014, P ICML
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000005
DA 2019-06-15
ER

PT S
AU Hu, ZT
   Yang, ZC
   Salakhutdinov, R
   Liang, XD
   Qin, LH
   Dong, HY
   Xing, EP
AF Hu, Zhiting
   Yang, Zichao
   Salakhutdinov, Ruslan
   Liang, Xiaodan
   Qin, Lianhui
   Dong, Haoye
   Xing, Eric P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Generative Models with Learnable Knowledge Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.
C1 [Hu, Zhiting] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   Petuum Inc, Pittsburgh, PA USA.
RP Hu, ZT (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM zhitingh@cs.cmu.edu; zichaoy@cs.cmu.edu; rsalakhu@cs.cmu.edu;
   xiaodan1@cs.cmu.edu; eric.xing@petuum.com
FU National Science Foundation [IIS1563887]
FX This material is based upon work supported by the National Science
   Foundation grant IIS1563887. Any opinions, findings and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the views of the National Science
   Foundation.
CR Abdolmaleki A., 2018, ICLR
   Andreas J., 2016, ARXIV160101705
   Bahdanau D., 2014, ARXIV14090473
   Bellare K., 2009, UAI, P43
   Chen X., 2016, NEURIPS
   Dayan P, 1997, NEURAL COMPUT, V9, P271, DOI 10.1162/neco.1997.9.2.271
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Diao Q., 2014, P 20 ACM SIGKDD INT, P193, DOI DOI 10.1145/2623330.2623758
   Fedus W., 2018, ARXIV180107736
   Finn C., 2016, INT C MACH LEARN, P49
   Finn C., 2016, ARXIV161103852
   Fu J., 2017, ARXIV171011248
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Haarnoja T., 2017, ARXIV170208165
   Hinton G., 2015, ARXIV150302531
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Holtzman A., 2018, ACL
   Hu Z., 2017, ICML
   Hu Z., 2018, ARXIV180900794
   Hu Z., 2016, ACL
   Hu Z., 2018, ICLR
   Hu Z., 2016, EMNLP
   Kim  Taesup, 2016, ARXIV160603439
   Kingma D. P., 2013, ARXIV13126114
   Kusner M. J, 2017, ARXIV170301925
   Larochelle H., 2011, AISTATS
   Levine S., 2018, ARXIV180500909
   Li C., 2015, P INT C ADV NEUR INF, P1837
   Liang P., 2009, 26 ANN INT C MACH LE, P641
   Liang X., 2018, NEURIPS
   Liang X., 2017, ICCV
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lopez-Paz  David, 2015, ARXIV151103643
   Ma L., 2018, CVPR
   Ma L., 2017, ADV NEURAL INFORM PR, P405
   Mei S., 2014, INT C MACH LEARN, P253
   Mohamed S., 2016, ARXIV161003483
   Neumann G., 2011, P 28 INT C MACH LEAR, P817
   Oord A. v. d., 2016, ARXIV160106759
   Peters J, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P1607
   Pumarola A., 2018, CVPR
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman J., 2017, ARXIV170406440
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tan B., 2018, CONNECTING DOTS MLE
   Taskar B, 2004, ADV NEUR IN, V16, P25
   Wang D., 2016, ARXIV161101722
   Wang T.-C., 2017, ARXIV171111585
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang Z., 2018, NEURIPS
   Zhai S., 2016, ARXIV161101799
   Zhao J., 2016, ARXIV160903126
   Ziebart B. D., 2008, AAAI, V8, P1433
   Zweig G., 2011, TECHNICAL REPORT
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005011
DA 2019-06-15
ER

PT S
AU Huang, CW
   Tan, S
   Lacoste, A
   Courville, A
AF Huang, Chin-Wei
   Tan, Shawn
   Lacoste, Alexandre
   Courville, Aaron
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Improving Explorability in Variational Inference with Annealed
   Variational Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods. Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective. In our experiments, we demonstrate our method's robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.
C1 [Huang, Chin-Wei; Tan, Shawn; Courville, Aaron] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Huang, Chin-Wei; Lacoste, Alexandre] Element AI, Montreal, PQ, Canada.
RP Huang, CW (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.; Huang, CW (reprint author), Element AI, Montreal, PQ, Canada.
EM chin-wei.huang@umontreal.ca; jing.shan.shawn.tan@umontreal.ca;
   allac@elementai.com; aaron.courville@umontreal.ca
CR Abrol F., 2014, STAT, V1050, P7
   Agakov F. V., 2004, NEURAL INFORM PROCES
   Berg R. v. d., 2018, ARXIV180305649
   Bowman S. R., 2016, SIGNLL C COMP NAT LA
   Burda Y., 2016, INT C LEARN REPR
   Chung J., 2015, ADV NEURAL INFORM PR
   Clevert D. A., 2016, INT C LEARN REPR
   Cremer C., 2018, INT C MACH LEARN
   Dieng A. B., 2017, ADV NEURAL INFORM PR
   Edwards H., 2017, INT C LEARN REPR
   Fraccaro  M., 2016, ADV NEURAL INFORM PR, V2016, P2199
   Gulrajani I., 2017, INT C LEARN REPR
   Huang C. - W., 2018, INT C MACH LEARN
   Huszar F., 2017, ARXIV170208235
   Karl M., 2016, INT C LEARN REPR
   Katahira K, 2008, J PHYS CONF SER, V95, DOI 10.1088/1742-6596/95/1/012015
   Kim Y., 2018, INT C MACH LEARN
   Kingma D. P., 2016, ADV NEURAL INFORM PR
   Kingma Diederik, 2014, INT C LEARN REPR
   Krishnan R. G., 2017, ARXIV171006085
   Krueger D., 2017, ARXIV171004759
   Larochelle H., 2011, INT C ART INT STAT
   Li Y., 2016, ADV NEURAL INFORM PR
   Maaloe L., 2016, INT C MACH LEARN
   Mandt S., 2016, INT C ART INT STAT
   Marino J., 2018, INT C MACH LEARN
   Mescheder L., 2017, INT C MACH LEARN
   Nair V, 2010, INT C MACH LEARN
   Neal R. M., 2001, STAT COMPUTING, V11
   Nowozin S., 2018, INT C LEARN REPR
   Raiko T., 2007, J MACHINE LEARNING R
   Rainforth T., 2018, INT C MACH LEARN
   Ranganath R., 2016, INT C MACH LEARN
   Rezende D. J., 2015, INT C MACH LEARN
   Rezende D. J., 2014, INT C MACH LEARN
   Salimans T, 2015, INT C MACH LEARN
   Salimans  T., 2016, ADV NEURAL INFORM PR
   Shabanian  S., 2017, ARXIV171105717
   Shi J., 2018, INT C LEARN REPR
   Sonderby C. K., 2016, ADV NEURAL INFORM PR, P3738
   Tomczak J. M., 2017, BENELEARN
   Tomczak Jakub M., 2016, ARXIV161109630
   Turner R. E., 2011, BAYESIAN TIME SERIES, V1, P3
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004028
DA 2019-06-15
ER

PT S
AU Huang, HB
   Li, ZH
   He, R
   Sun, ZN
   Tan, TN
AF Huang, Huaibo
   Li, Zhihang
   He, Ran
   Sun, Zhenan
   Tan, Tieniu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI IntroVAE: Introspective Variational Autoencoders for Photographic Image
   Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at 1024(2)), which are comparable to or better than the state-of-the-art GANs.
C1 [He, Ran] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.
   CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.
RP He, R (reprint author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM huaibo.huang@cripac.ia.ac.cn; zhihang.li@nlpr.ia.ac.cn;
   rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn
FU State Key Development Program [2016YFB1001001]; National Natural Science
   Foundation of China [61622310, 61427811]
FX This work is partially funded by the State Key Development Program
   (Grant No. 2016YFB1001001) and National Natural Science Foundation of
   China (Grant No. 61622310, 61427811).
CR Arjovsky M, 2017, ARXIV170107875
   Berthelot D., 2017, ARXIV170310717
   Brock A., 2017, ICLR
   Chen Xi, 2017, ICLR
   Dahl Ryan, 2017, ICCV
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Dinh Laurent, 2017, ICLR
   Donahue J., 2017, ICLR
   Dosovitskiy Alexey, 2016, ADV NEURAL INFORM PR, V29, P658
   Dumoulin V., 2017, ICLR
   Durugkar Ishan, 2017, ICLR
   Gibiansky A., 2017, P NIPS, P2966
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I., 2016, DEEP LEARNING, V1
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Heusel  M., 2017, P ADV NEUR INF PROC, P6626
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Karras Tero, 2018, ICLR
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma Diederik P, 2014, ICLR
   Lample G., 2017, NEURIPS, P5969
   Larsen A. B. L, 2016, P 33 INT C MACH LEAR, P1558
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Liu  M., 2017, ADV NEURAL INFORM PR, P700
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma L., 2017, ADV NEURAL INFORM PR, P405
   Makhzani A., 2015, ARXIV151105644
   Nguyen T., 2017, ADV NEURAL INFORM PR, P2667
   Odena A., 2017, P 34 INT C MACH LEAR, P2642
   Radford A., 2016, ICLR
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Sonderby C. K., 2016, ADV NEURAL INFORM PR, P3738
   Srivastava A., 2017, ADV NEURAL INFORM PR, P3310
   Ulyanov Dmitry, 2018, AAAI
   van den Oord A., 2016, P 33 INT C MACH LEAR, P1747
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Wang Ting- Chun, 2018, CVPR
   Yu F., 2015, ARXIV150603365
   Zhang H., 2017, P IEEE INT C COMP VI, P5907
   Zhang Han, 2017, ARXIV171010916V2
   Zhang Zizhao, 2018, ARXIV180209178
   Zhao J., 2017, ICLR
   Zhao S., 2017, ARXIV170602262
   Zhu J.-Y., 2017, ADV NEURAL INFORM PR, V30, P465
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300006
DA 2019-06-15
ER

PT S
AU Huang, J
   Wu, F
   Precup, D
   Cai, Y
AF Huang, Jessie
   Wu, Fa
   Precup, Doina
   Cai, Yang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning safe policies with expert guidance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ROBUST; OPTIMIZATION
AB We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the "follow-the-perturbed-leader" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.
C1 [Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
   [Wu, Fa] Zhejiang Demet Med Technol, Taizhou, Peoples R China.
RP Huang, J (reprint author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
EM jiexi.huang@mcgill.ca; fa.wu2@mcgill.ca; dprecup@cs.mcgill.ca;
   cai@cs.mcgill.ca
FU Open Philanthropy Fund; NSERC [RGPIN2015-06127]; FRQNT [2017-NC-198956]
FX Doina Precup and Jessie Huang gratefully acknowledge funding from Open
   Philanthropy Fund and NSERC which made this research possible. Yang Cai
   and Fa Wu thank the NSERC for its support through the Discovery grant
   RGPIN2015-06127 and FRQNT for its support through the grant
   2017-NC-198956.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Amin Kareem, 2017, ADV NEURAL INFORM PR, P1813
   Amodei Dario, 2016, ARXIV160606565
   Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374
   Boyd Stephen, 2007, STANFORD EE 364B LEC
   Brockman G, 2016, ARXIV160601540
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273
   Hadfield-Menell Dylan, 2017, ADV NEURAL INFORM PR, P6768
   Hutter M, 2005, J MACH LEARN RES, V6, P639
   Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   KARP RM, 1982, SIAM J COMPUT, V11, P620, DOI 10.1137/0211053
   Khachiyan L, 1979, SOV MATH DOKL, V20, P191
   Morimoto J, 2005, NEURAL COMPUT, V17, P335, DOI 10.1162/0899766053011528
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Syed U., 2008, ADV NEURAL INFORM PR, V20, P1449
   Xu Huan, 2013, ADV NEURAL INFORM PR, P701
   Yang Cai, 2013, 24 ANN ACM SIAM S DI
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003064
DA 2019-06-15
ER

PT S
AU Huang, QY
   Zhang, PC
   Wu, DP
   Zhang, L
AF Huang, Qiuyuan
   Zhang, Pengchuan
   Wu, Dapeng
   Zhang, Lei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Turbo Learning for CaptionBot and DrawingBot
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study in this paper the problems of both image captioning and text-to-image generation, and present a novel turbo learning approach to jointly training an image-to-text generator (a.k.a. CaptionBot) and a text-to-image generator (a.k.a. DrawingBot). The key idea behind the joint training is that image-to-text generation and text-to-image generation as dual problems can form a closed loop to provide informative feedback to each other. Based on such feedback, we introduce a new loss metric by comparing the original input with the output produced by the closed loop. In addition to the old loss metrics used in CaptionBot and DrawingBot, this extra loss metric makes the jointly trained CaptionBot and DrawingBot better than the separately trained CaptionBot and DrawingBot. Furthermore, the turbo-learning approach enables semi-supervised learning since the closed loop can provide pseudo-labels for unlabeled samples. Experimental results on the COCO dataset demonstrate that the proposed turbo learning can significantly improve the performance of both CaptionBot and DrawingBot by a large margin.
C1 [Huang, Qiuyuan; Zhang, Pengchuan; Zhang, Lei] Microsoft Res, Redmond, WA 98052 USA.
   [Wu, Dapeng] Univ Florida, Gainesville, FL USA.
RP Huang, QY (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM qihua@microsoft.com; penzhan@microsoft.com; dpwu@ieee.org;
   leizhang@microsoft.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Anderson P., 2016, ECCV
   Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   COCO, 2017, COCO DAT IM CAPT
   Denton E. L., 2015, NIPS
   Devlin J., 2015, ARXIV150501809
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Gan Z., 2017, P IEEE C COMP VIS PA
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor K., 2015, ICML
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang P.-S., 2013, P 22 ACM INT C C INF, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   Isola  Phillip, 2017, CVPR
   Kalchbrenner N., 2016, NIPS
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros  R., 2014, ARXIV14112539
   Kiros R., 2014, P 31 INT C MACH LEAR, V14, P595
   Ledig C., 2017, CVPR
   Lu Jiasen, 2017, P IEEE C COMP VIS PA, V6
   Mansimov E., 2016, ICLR
   Mao J., 2015, P INT C LEARN REPR
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Pasunuru R., 2017, EMNLP
   Pennington J., 2017, STANFORD GLOVE GLOBA
   Radford A., 2016, ICLR
   Reed S., 2016, NIPS
   Reed S., 2016, ICML
   Reed S. E., 2017, ICML
   Rennie S. J., 2017, P IEEE C COMP VIS PA
   Salimans T., 2016, NIPS
   Salimans T., 2016, ARXIV160603498
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xia Y., 2016, ARXIV161100179
   Xu KS, 2017, IEEE INT CON MULTI, P361, DOI 10.1109/ICME.2017.8019408
   Xu Tao, 2018, ATTNGAN FINE GRAINED
   Yao T., 2017, P INT C COMP VIS
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang Han, 2017, ARXIV171010916
   Zhang Han, 2017, ICCV
   Zhu J-Y, 2017, IEEE ICC
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001003
DA 2019-06-15
ER

PT S
AU Huang, SY
   Qi, SY
   Xiao, YX
   Zhu, YX
   Wu, YN
   Zhu, SC
AF Huang, Siyuan
   Qi, Siyuan
   Xiao, Yinxue
   Zhu, Yixin
   Wu, Ying Nian
   Zhu, Song-Chun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout,
   and Camera Pose Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Holistic 3D indoor scene understanding refers to jointly recovering the i) object bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The existing methods either are ineffective or only tackle the problem partially. In this paper, we propose an end-to-end model that simultaneously solves all three tasks in real-time given only a single RGB image. The essence of the proposed method is to improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of directly estimating the targets, and ii) cooperative training across different modules in contrast to training these modules individually. Specifically, we parametrize the 3D object bounding boxes by the predictions from several modules, i.e., 3D camera pose and object attributes. The proposed method provides two major advantages: i) The parametrization helps maintain the consistency between the 2D image and the 3D world, thus largely reducing the prediction variances in 3D coordinates. ii) Constraints can be imposed on the parametrization to train different modules simultaneously. We call these constraints "cooperative losses" as they enable the joint training and inference. We employ three cooperative losses for 3D bounding boxes, 2D projections, and physical constraints to estimate a geometrically consistent and physically plausible 3D scene. Experiments on the SUN RGB-D dataset shows that the proposed method significantly outperforms prior approaches on 3D object detection, 3D layout estimation, 3D camera pose estimation, and holistic scene understanding.
C1 [Huang, Siyuan; Zhu, Yixin; Wu, Ying Nian; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA.
   [Qi, Siyuan; Xiao, Yinxue; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
RP Huang, SY (reprint author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA.
EM huangsiyuan@ucla.edu; syqi@cs.ucla.edu; yinxuex@ucla.edu;
   yixin.zhu@ucla.edu; ywu@stat.ucla.edu; sczhu@stat.ucla.edu
FU DARPA XAI grant [N66001-17-2-4029]; ONR MURI grant [N00014-16-1-2007];
   ARO grant [W911NF-18-1-0296]; NVIDIA GPU donation grant
FX The work reported herein was supported by DARPA XAI grant
   N66001-17-2-4029, ONR MURI grant N00014-16-1-2007, ARO grant
   W911NF-18-1-0296, and an NVIDIA GPU donation grant. We thank Prof.
   Hongjing Lu from the UCLA Psychology Department for useful discussions
   on the motivation of this work, and three anonymous reviewers for their
   constructive comments.
CR Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Chen Liu, 2018, IEEE C COMP VIS PATT
   Choi Wongun, 2013, IEEE C COMP VIS PATT
   Dahua Lin, 2013, IEEE INT C COMP VIS
   Dai JY, 2017, IEEE ICC
   Gupta Abhinav, 2010, C NEUR INF PROC SYST
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hedau Varsha, 2009, IEEE C COMP VIS PATT
   Izadinia Hamid, 2017, IEEE C COMP VIS PATT
   Jacobs RA, 2002, TRENDS COGN SCI, V6, P345, DOI 10.1016/S1364-6613(02)01948-4
   Kehl Wadim, 2017, IEEE C COMP VIS PATT
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kubricht JR, 2017, TRENDS COGN SCI, V21, P749, DOI 10.1016/j.tics.2017.06.002
   Kundu Abhijit, 2018, IEEE C COMP VIS PATT
   Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495
   LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M
   Lee C, 2017, IEEE ICC
   Mousavian Arsalan, 2017, IEEE C COMP VIS PATT
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8
   Paszke A., 2017, NIPS W
   POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509
   POTTER MC, 1975, SCIENCE, V187, P965, DOI 10.1126/science.1145183
   Ren Shaoqing, 2015, C NEUR INF PROC SYST
   Rezende Danilo Jimenez, 2016, C NEUR INF PROC SYST
   Ruiqi Guo, 2013, IEEE INT C COMP VIS
   Schwing Alexander G, 2013, IEEE INT C COMP VIS
   SCHYNS PG, 1994, PSYCHOL SCI, V5, P195, DOI 10.1111/j.1467-9280.1994.tb00500.x
   Siyuan Huang, 2018, EUR C COMP VIS ECCV
   Song S., 2015, P IEEE C COMP VIS PA
   Song Shuran, 2016, IEEE C COMP VIS PATT
   Song Shuran, 2014, EUR C COMP VIS ECCV
   Song Shuran, 2017, IEEE C COMP VIS PATT
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Tulsiani Shubham, 2015, IEEE C COMP VIS PATT
   Tulsiani Shubham, 2018, IEEE C COMP VIS PATT
   Wu J., 2016, EUR C COMP VIS ECCV
   Wu Jiajun, 2017, C NEUR INF PROC SYST
   Yan Xinchen, 2016, C NEUR INF PROC SYST
   Yibiao Zhao, 2011, C NEUR INF PROC SYST
   Yibiao Zhao, 2013, IEEE C COMP VIS PATT
   Yinda Zhang, 2014, EUR C COMP VIS ECCV
   Yinda Zhang, 2017, IEEE C COMP VIS PATT
   Yinda Zhang, 2017, IEEE INT C COMP VIS
   Zhuo Deng, 2017, IEEE C COMP VIS PATT
   Zou Chuhang, 2018, IEEE C COMP VIS PATT
   Zou Chuhang, 2017, ARXIV171009490
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300020
DA 2019-06-15
ER

PT S
AU Huang, WB
   Zhang, T
   Rong, Y
   Huang, JZ
AF Huang, Wenbing
   Zhang, Tong
   Rong, Yu
   Huang, Junzhou
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adaptive Sampling Towards Fast Graph Representation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ROBUST
AB Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.
C1 [Huang, Wenbing; Rong, Yu; Huang, Junzhou] Tencent AI Lab, Bellevue, WA 98004 USA.
   [Zhang, Tong] Australian Natl Univ, Canberra, ACT, Australia.
RP Huang, WB (reprint author), Tencent AI Lab, Bellevue, WA 98004 USA.
EM hwenbing@126.com; tong.zhang@anu.edu.au; yu.rong@hotmail.com;
   joehhuang@tencent.com
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Atwood J., 2016, ADV NEURAL INFORM PR, V29, P1993
   Bruna J., 2013, ABS13126203 CORR
   Chen J, 2018, ARXIV180110247
   Chen Jianfei, 2018, INT C MACH LEARN
   Defferrard M., 2016, ADV NEURAL INFORM PR, P3844
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Fouss F, 2006, IEEE DATA MINING, P863
   Fout A., 2017, ADV NEURAL INFORM PR, P6533
   Grover Aditya, 2016, KDD, V2016, P855
   Hamilton W., 2017, ADV NEURAL INFORM PR, P1025
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henaff Mikael, 2015, ARXIV150605163
   Kipf T. N., 2016, ARXIV160902907
   Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809
   Monti F., 2017, CVPR, V1, P3
   Niepert M., 2016, INT C MACH LEARN, P2014
   Owen AB, 2013, MONTE CARLO THEORY M
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   Such FP, 2017, IEEE J-STSP, V11, P884, DOI 10.1109/JSTSP.2017.2726981
   Tang J, 2015, P 24 INT C WORLD WID, P1067, DOI DOI 10.1145/2736277.2741093
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Velickovic  P., 2017, ARXIV171010903
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304056
DA 2019-06-15
ER

PT S
AU Huang, ZY
   Liu, JY
   Wang, XN
AF Huang, Zhiyi
   Liu, Jinyan
   Wang, Xiangning
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Optimal Reserve Price against Non-myopic Bidders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful. Previous algorithms, e.g., empirical pricing, do not provide non-trivial regret rounds in this setting in general. We introduce algorithms that obtain a small regret against non-myopic bidders either when the market is large, i.e., no single bidder appears in more than a small constant fraction of the rounds, or when the bidders are impatient, i.e., they discount future utility by some factor mildly bounded away from one. Our approach carefully controls what information is revealed to each bidder, and builds on techniques from differentially private online learning as well as the recent line of works on jointly differentially private algorithms.
C1 [Huang, Zhiyi; Liu, Jinyan; Wang, Xiangning] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
RP Huang, ZY (reprint author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM zhiyi@cs.hku.hk; jyliu@cs.hku.hk; xnwang@cs.hku.hk
FU RGC [HKU17257516E]
FX Supported in part by an RGC grant HKU17257516E.
CR Abernethy J., 2014, P 27 C LEARN THEOR C, V35, P807
   Agarwal Naman, 2017, P 34 INT C MACH LEAR, P32
   Amin Kareem, 2013, ADV NEURAL INF PROCE, P1169
   Amin Kareem, 2014, ADV NEURAL INFORM PR, P622
   Blum A, 2004, THEOR COMPUT SCI, V324, P137, DOI 10.1016/j.tcs.2004.05.012
   Blum A, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1156
   Bubeck Sebastien, 2017, P 2017 ACM C EC COMP, P497
   Cai Yang, 2017, 58 ANN IEEE S FDN CO
   Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Devanur NR, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P426, DOI 10.1145/2897518.2897553
   Devanur Nikhil R, 2015, P S DISCRET ALGORITH, P983
   Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   Epasto A, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1369, DOI 10.1145/3178876.3186042
   Gonczarowski YA, 2017, ACM S THEORY COMPUT, P856, DOI 10.1145/3055399.3055427
   Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225
   Hsu J, 2016, SIAM J COMPUT, V45, P1953, DOI 10.1137/15100271X
   Hsu Justin, 2016, P 27 ANN ACM SIAM S, P580
   Huang  Zhiyi, 2015, P 16 ACM C EC COMP E, P45
   Huang Zhiyi, 2018, P 29 ANN ACM SIAM S
   Immorlica Nicole, 2017, P 2017 ACM C EC COMP, P167
   Jain  Prateek, 2012, C LEARN THEOR, P24
   Kearns M., 2014, P 5 C INN THEOR COMP, P403
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Mirrokni Vahab, 2018, P 2018 ACM C EC COMP, P169
   Mohri M., 2014, ADV NEURAL INFORM PR, P1871
   Morgenstern Jamie, 2016, P 29 ANN C LEARN THE, P1298
   Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR, P136
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Nekipelov D., 2015, P 16 ACM C EC COMP, P1
   Nissim K., 2012, P 3 INN THEOR COMP S, P203, DOI DOI 10.1145/2090236.2090254
   Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P1, DOI 10.1145/2940716.2940723
   Tossou Aristide C. Y., 2017, AAAI, P2653
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302008
DA 2019-06-15
ER

PT S
AU Huggins, JH
   Mackey, L
AF Huggins, Jonathan H.
   Mackey, Lester
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Random Feature Stein Discrepancies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power-even when power is explicitly optimized To address these shortcomings, we introduce feature Stein discrepancies (Phi)SDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct (Phi)SDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations-random (Phi)SDs (R Phi SDs)-which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, R(Phi)SDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.
C1 [Huggins, Jonathan H.] Harvard, Dept Biostat, Cambridge, MA 02138 USA.
   [Mackey, Lester] Microsoft Res New England, Cambridge, MA USA.
RP Huggins, JH (reprint author), Harvard, Dept Biostat, Cambridge, MA 02138 USA.
EM jhuggins@mit.edu; lmackey@microsoft.com
CR Abramowitz  M., 1964, HDB MATH FUNCTIONS
   BACH F, 2017, J MACH LEARN RES, V18
   Briol  F.-X., 2017, INT C MACH LEARN
   Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503
   Chung  F., 2006, COMPLEX GRAPHS NETWO, V107
   Chwialkowski  K., 2016, INT C MACH LEARN
   Chwialkowski  K., 2015, ADV NEURAL INFORM PR
   Eberle A, 2016, PROBAB THEORY REL, V166, P851, DOI 10.1007/s00440-015-0673-1
   Geyer C. J., 1991, COMP SCI STAT, P156, DOI DOI 10.1080/01621459.1995.10476590
   Gorham  J., 2016, 161106972V3 ARXIV
   Gorham  J., 2015, ADV NEURAL INFORM PR
   Gorham  J., 2017, INT C MACH LEARN
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Herb RA, 2011, CONTEMP MATH, V557, P3
   Honorio  J., 2017, 171009953V1 ARXIV
   Jitkrittum  W., 2017, ADV NEURAL INFORM PR
   Liu  Q., 2017, INT C ART INT STAT
   Liu  Q., 2016, ADV NEURAL INFORM PR
   Liu Q, 2016, DESTECH TRANS COMP
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185
   Rahimi A, 2007, ADV NEURAL INFORM PR
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   SERFLING RJ., 1980, APPROXIMATION THEORE
   Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144
   Sutherland D. J., 2015, P UAI, P862
   Wang  D., 2016, LEARNING DRAW SAMPLE
   Welling  M., 2011, INT C MACH LEARN
   Wendland  H., 2005, SCATTERED DATA APPRO
   Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301085
DA 2019-06-15
ER

PT S
AU Hughes, E
   Leibo, JZ
   Phillips, M
   Tuyls, K
   Duenez-Guzman, E
   Castaneda, AG
   Dunning, I
   Zhu, TN
   McKee, K
   Koster, R
   Roff, H
   Graepel, T
AF Hughes, Edward
   Leibo, Joel Z.
   Phillips, Matthew
   Tuyls, Karl
   Duenez-Guzman, Edgar
   Castaneda, Antonio Garcia
   Dunning, Iain
   Zhu, Tina
   McKee, Kevin
   Koster, Raphael
   Roff, Heather
   Graepel, Thore
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inequity aversion improves cooperation in intertemporal social dilemmas
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FAIRNESS; REINFORCEMENT; PREFERENCES; INFORMATION; PUNISHMENT
AB Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.
C1 [Hughes, Edward; Leibo, Joel Z.; Phillips, Matthew; Tuyls, Karl; Duenez-Guzman, Edgar; Castaneda, Antonio Garcia; Dunning, Iain; Zhu, Tina; McKee, Kevin; Koster, Raphael; Roff, Heather; Graepel, Thore] DeepMind, London, England.
RP Hughes, E (reprint author), DeepMind, London, England.
EM edwardhughes@google.com; jzl@google.com; matthew.phillips.12@ucl.ac.uk;
   karltuyls@google.com; duenez@google.com; antoniogc@google.com;
   idunning@google.com; tinazhu@google.com; kevinrmckee@google.com;
   rkoster@google.com; hroff@google.com; thore@google.com
CR Bellemare C, 2008, ECONOMETRICA, V76, P815, DOI 10.1111/j.1468-0262.2008.00860.x
   Bicchieri C, 2010, J BEHAV DECIS MAKING, V23, P161, DOI 10.1002/bdm.648
   Blake R. P., 2015, ONTOGENY FAIRNESS 7, V528, P11
   Brosnan SF, 2014, SCIENCE, V346, P314, DOI 10.1126/science.1251776
   Camerer CF, 2011, BEHAV GAME THEORY EX
   Charness G, 2002, Q J ECON, V117, P817, DOI 10.1162/003355302760193904
   Chentanez N, 2005, ADV NEURAL INFORM PR, V17, P1281
   de Cote E. Munoz, 2006, AAMAS, P783
   de Jong S, 2011, AUTON AGENT MULTI-AG, V22, P103, DOI 10.1007/s10458-010-9122-9
   Dietz T, 2003, SCIENCE, V302, P1907, DOI 10.1126/science.1091015
   Eckel C, 2010, J ECON BEHAV ORGAN, V73, P109, DOI 10.1016/j.jebo.2009.03.026
   Engelmann D, 2004, AM ECON REV, V94, P857, DOI 10.1257/0002828042002741
   Falk A, 2006, GAME ECON BEHAV, V54, P293, DOI 10.1016/j.geb.2005.03.001
   Fehr E, 2002, NATURE, V415, P137, DOI 10.1038/415137a
   Fehr E, 1999, Q J ECON, V114, P817, DOI 10.1162/003355399556151
   Fehr E, 2000, AM ECON REV, V90, P980, DOI 10.1257/aer.90.4.980
   Fehr E, 2007, ANNU REV SOCIOL, V33, P43, DOI 10.1146/annurev.soc.33.040406.131812
   Foerster J. N., 2017, ARXIV170904326
   FREY BS, 1995, J INST THEOR ECON, V151, P286
   Gibbons R., 1992, PRIMER GAME THEORY
   GRICE GR, 1948, J EXP PSYCHOL, V38, P1, DOI 10.1037/h0061016
   Gurerk O, 2006, SCIENCE, V312, P108, DOI 10.1126/science.1123633
   HARDIN G, 1968, SCIENCE, V162, P1243
   Hart HLA, 1955, PHILOS REV, V64, P175, DOI [10.2307/2182586, DOI 10.2307/2182586]
   Henrich J, 2010, SCIENCE, V327, P1480, DOI 10.1126/science.1182238
   Hoppe EI, 2013, REV ECON STUD, V80, P1516, DOI 10.1093/restud/rdt010
   Janssen M., 2010, ECOLOGY SOC, V15
   Janssen MA, 2013, ECOL SOC, V18, DOI 10.5751/ES-05664-180404
   Janssen MA, 2010, SCIENCE, V328, P613, DOI 10.1126/science.1183532
   Kearns Michael J, 2000, P 13 ANN C COMP LEAR, P142
   Kleiman-Weiner Max, 2016, COGSCI
   KLOSKO G, 1987, ETHICS, V97, P353, DOI 10.1086/292843
   Kollock P, 1998, ANNU REV SOCIOL, V24, P183, DOI 10.1146/annurev.soc.24.1.183
   Leibo Joel Z., 2017, P 16 INT C AUT AG MU
   Lerer Adam, 2017, ARXIV170701068
   Littman M., 1994, P 11 INT C MACH LEAR, V157, P157
   LOEWENSTEIN GF, 1989, J PERS SOC PSYCHOL, V57, P426, DOI 10.1037/0022-3514.57.3.426
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   OLIVER P, 1980, AM J SOCIOL, V85, P1356, DOI 10.1086/227168
   Olson Mancur, 1965, LOGIC COLLECTIVE ACT
   Ostrom E, 1998, AM POLIT SCI REV, V92, P1, DOI 10.2307/2585925
   OSTROM E, 1992, AM POLIT SCI REV, V86, P404, DOI 10.2307/1964229
   Ostrom Elinor, 1990, GOVERNING COMMONS EV
   Perolat J., 2017, ADV NEURAL INFORM PR
   Peysakhovich A, 2017, ARXIV170902865
   Peysakhovich A., 2017, CORR
   Rand DG, 2013, TRENDS COGN SCI, V17, P413, DOI 10.1016/j.tics.2013.06.003
   RAWLS J, 1958, PHILOS REV, V67, P164, DOI 10.2307/2182612
   Rousseau J. J., 1755, DISCOURSE ORIGIN INE
   Sandholm TW, 1996, BIOSYSTEMS, V37, P147, DOI 10.1016/0303-2647(95)01551-5
   SCHELLING TC, 1973, J CONFLICT RESOLUT, V17, P381, DOI 10.1177/002200277301700302
   Shapley L. S., 1953, P NATL ACAD SCI US
   Skinner Burrhus F., 1938, BEHAV ORGANISMS EXPT
   Sunehag P., 2017, CORR
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Verbeeck K, 2002, LECT NOTES ARTIF INT, V2564, P81
   Walsh W. E., 2002, P AAAI WORKSH GAM TH, P109
   Wellman M. P., 2006, P AAAI, P1552
   YAMAGISHI T, 1986, J PERS SOC PSYCHOL, V51, P110, DOI 10.1037/0022-3514.51.1.110
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303033
DA 2019-06-15
ER

PT S
AU Huh, D
   Sejnowski, TJ
AF Huh, Dongsung
   Sejnowski, Terrence J.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gradient Descent for Spiking Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ERROR-BACKPROPAGATION; RULE
AB Most large-scale network models use neurons with static nonlinearities that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking neural networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking dynamics and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (approximate to millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory task over extended duration (approximate to second). The results show that the gradient descent approach indeed optimizes networks dynamics on the time scale of individual spikes as well as on behavioral time scales. In conclusion, our method yields a general purpose supervised learning algorithm for spiking neural networks, which can facilitate further investigations on spike-based computations.
C1 [Huh, Dongsung; Sejnowski, Terrence J.] Salk Inst Biol Studies, La Jolla, CA 92037 USA.
RP Huh, D (reprint author), Salk Inst Biol Studies, La Jolla, CA 92037 USA.
EM huh@salk.edu; terry@salk.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Brendel W., 2017, ARXIV170303777
   Deneve S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   DIEHL PU, 2015, IEEE IJCNN
   Ermentrout B., 2008, SCHOLARPEDIA, V3, P1398, DOI [10.4249/scholar-pedia.1398, DOI 10.4249/SCHOLAR-PEDIA.1398]
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Fremaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gutig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gutig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hunsberger Eric, 2015, ARXIV151008829
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI [10.1162/neco.2009.09-07-610, 10.1162/neco.2008.09-07-610]
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pontryagin L, 1962, MATH THEORY OPTIMAL
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende D. J., 2011, ADV NEURAL INF PROCE, P136
   Rueckauer B., 2016, ARXIV161204052
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Sengupta A., 2018, ARXIV180202627
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Welling M., 2016, ARXIV160208323
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Zenke F., 2017, ARXIV170511146
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301042
DA 2019-06-15
ER

PT S
AU Le, H
   Tran, T
   Nguyen, T
   Venkatesh, S
AF Hung Le
   Truyen Tran
   Thin Nguyen
   Venkatesh, Svetha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Variational Memory Encoder-Decoder
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations.
C1 [Hung Le; Truyen Tran; Thin Nguyen; Venkatesh, Svetha] Deakin Univ, Appl AI Inst, Geelong, Vic, Australia.
RP Le, H (reprint author), Deakin Univ, Appl AI Inst, Geelong, Vic, Australia.
EM lethai@deakin.edu.au; truyen.tran@deakin.edu.au;
   thin.nguyen@deakin.edu.au; svetha.venkatesh@deakin.edu.au
CR Bacharoglou AG, 2010, P AM MATH SOC, V138, P2619, DOI 10.1090/S0002-9939-10-10340-2
   Bahdanau D., 2015, P INT C LEARN REPR
   Bornschein Jorg, 2017, ADV NEURAL INFORM PR, P3923
   Bowman S.R., 2016, P 20 SIGNLL C COMP N, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Britz Denny, 2017, P C EMP METH NAT LAN, P392
   Chen B., 2014, P 9 WORKSH STAT MACH, P362, DOI DOI 10.3115/V1/W14-3346
   Chen HS, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1653, DOI 10.1145/3178876.3186077
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Dilokthanakul N., 2016, ARXIV161102648
   Forgues Gabriel, 2014, NIPS MOD MACH LEARN, V2
   Gemici Mevlana, 2017, ARXIV170204649
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves A, 2013, ARXIV13080850
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hershey John R, 2007, IEEE INT C AC SPEECH
   Le H, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1637, DOI 10.1145/3219819.3219981
   Hung Le, 2018, Advances in Knowledge Discovery and Data Mining. 22nd Pacific-Asia Conference, PAKDD 2018. Proceedings: LNAI 10939, P273, DOI 10.1007/978-3-319-93040-4_22
   Jiang Z., 2017, P 26 INT JOINT C ART, P1965
   Jiwei L., 2016, P 2016 C N AM CHAPT, P110
   Kalchbrenner N., 2013, P 2013 C EMP METH NA, P1700
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma  Diederik, 2014, P INT C LEARN REPR
   Levy O., 2014, ADV NEURAL INFORM PR, V27, P2177, DOI DOI 10.1162/153244303322533223
   Lison Pierre, 2017, P ANN SIGDIAL M DISC, P384
   Mazya V, 1996, IMA J NUMER ANAL, V16, P13, DOI 10.1093/imanum/16.1.13
   Nalisnick Eric, 2016, NIPS WORKSH BAYES DE, V2
   Nallapati R., 2016, P 20 SIGNLL C COMP N, P280
   Prakash A., 2017, P 31 AAAI C ART INT, P3274
   Rezende Danilo Jimenez, 2014, P INT C INT C MACH L, pII
   Serban Iulian V., 2017, AAAI, P3295
   Shen Xiaoyu, 2017, P ANN M ASS COMP LIN, V2, P504
   Shu Rui, 2016, ECCV WORKSH ACT ANT, V2
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Vinyals O, 2015, ARXIV150605869
   Wang Liwei, 2017, P ADV NEUR INF PROC, P5756
   Wang  M., 2016, EMNLP, P278
   Wen Tsung-Hsien, 2017, P INT C MACH LEARN, P3732
   Zhang Biao, 2016, P C EMP METH NAT LAN, P521
   Zhao T., 2017, M ASS COMP LING, P654
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301049
DA 2019-06-15
ER

PT S
AU Huo, ZY
   Gu, B
   Huang, H
AF Huo, Zhouyuan
   Gu, Bin
   Huang, Heng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Training Neural Networks Using Features Replay
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves faster convergence, lower memory consumption, and better generalization error than compared methods.
C1 [Huo, Zhouyuan; Gu, Bin; Huang, Heng] Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA.
RP Huang, H (reprint author), Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA.
EM zhouyuan.huo@pitt.edu; jsgubin@gmail.com; heng.huang@pitt.edu
CR Balduzzi David, 2015, AAAI C ART INT, P485
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bottou  L., 2016, ARXIV160604838
   Carreira-Perpinan M. A., 2014, ARTIF INTELL, P10
   Chen J., 2016, ARXIV160400981
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Eldan  R., 2016, C LEARN THEOR, P907
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton Geoffrey, COURSERA LECT SLIDES
   Huang G, 2016, ARXIV160806993
   Huo Zhouyuan, 2018, ARXIV180410574
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg M., 2016, ARXIV160805343
   Johnson Justin, 2017, BENCHMARKS POPULAR C
   Kalchbrenner N., 2014, ARXIV14042188
   Kim Y., 2014, ARXIV14085882
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lillicrap T P, 2015, ARXIV150902971
   Lin M., 2013, ARXIV13124400
   Mnih V., 2013, ARXIV13125602
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nokland Arild, 2016, ADV NEURAL INFORM PR, P1037
   Paszke A., 2017, NIPS W
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Simonyan K, 2014, ARXIV14091556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taylor G., 2016, INT C MACH LEARN, V48, P2722
   Telgarsky M., 2016, ARXIV160204485
   Zadrozny B., 2014, P 31 INT C MACH LEAR, pII
   Zhang X., 2015, ADV NEURAL INFORM PR, V28, P649
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001022
DA 2019-06-15
ER

PT S
AU Nguyen, HL
   Zakynthinou, L
AF Huy Le Nguyen
   Zakynthinou, Lydia
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Improved Algorithms for Collaborative PAC Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MODEL
AB We study a recent model of collaborative PAC learning where k players with k different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and has O((ln(k))(2)) times the worst-case sample complexity for learning a single task. In this work, we design new algorithms for both the realizable and the non-realizable setting, having sample complexity only O(ln(k)) times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks.
C1 [Huy Le Nguyen; Zakynthinou, Lydia] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Nguyen, HL (reprint author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
EM hu.nguyen@northeastern.edu; zakynthinou.1@northeastern.edu
FU NSF CAREER [1750716]; Northeastern University's College of Computer and
   Information Science
FX We thank the anonymous reviewers for their helpful remarks and for
   pointing us to the idea of slightly modifying the algorithms in the
   non-realizable setting so that the optimal error is unknown. This work
   was partially supported by NSF CAREER 1750716 and a Graduate fellowship
   from Northeastern University's College of Computer and Information
   Science.
CR Anthony  M., 2009, NEURAL NETWORK LEARN
   Balcan Maria-Florina, 2012, P 25 ANN C LEARN THE
   Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Blum Avrim, 2017, NIPS, P2389
   Chen Jiecao, 2018, CORR
   Dekel O., 2011, P 28 INT C MACH LEAR, P713
   Finn  C., 2017, P 34 INT C MACH LEAR, P1126
   Koufogiannakis C, 2014, ALGORITHMICA, V70, P648, DOI 10.1007/s00453-013-9771-6
   Mansour Y., 2009, ADV NEURAL INFORM PR, V21, P1041
   Mansour Yishay, 2009, P COLT, P19
   Mitzenmacher M., 2017, PROBABILITY COMPUTIN
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Wang JW, 2016, AER ADV ENG RES, V79, P751
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002020
DA 2019-06-15
ER

PT S
AU Ibarz, B
   Leike, J
   Pohlen, T
   Irving, G
   Legg, S
   Amodei, D
AF Ibarz, Borja
   Leike, Jan
   Pohlen, Tobias
   Irving, Geoffrey
   Legg, Shane
   Amodei, Dario
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reward learning from human preferences and demonstrations in Atari
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NEURAL-NETWORKS
AB To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.
C1 [Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Legg, Shane] DeepMind, London, England.
   [Irving, Geoffrey; Amodei, Dario] OpenAI, San Francisco, CA USA.
RP Ibarz, B (reprint author), DeepMind, London, England.
EM bibarz@google.com; leike@google.com; pohlen@google.com;
   irving@openai.com; legg@google.com; damodei@openai.com
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Akrour Riad, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P116, DOI 10.1007/978-3-642-33486-3_8
   Amodei Dario, 2016, ARXIV160606565
   Andrychowicz M, 2017, ADV NEURAL INFORM PR, P5048
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Bellemare M. G., 2017, INT C MACH LEARN, P449
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Chentanez N, 2005, ADV NEURAL INFORM PR, V17, P1281
   Christiano P. F., 2017, ADV NEURAL INFORM PR, P4302
   Dabney Will, 2017, ARXIV171010044
   Daniel C, 2015, AUTON ROBOT, V39, P389, DOI 10.1007/s10514-015-9454-z
   El Asri L, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P457
   Elo A. E., 1978, RATING CHESSPLAYERS
   Everitt Tom, 2018, THESIS
   Eysenbach Benjamin, 2018, ARXIV180206070
   Goodfellow IJ, 2014, ARXIV14126572
   Gregor Karol, 2016, ARXIV161107507
   Hester Todd, 2018, AAAI
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg Max, 2017, INT C LEARN REPR
   Kingma D. P., 2014, ARXIV14126980
   Knox WB, 2009, K-CAP'09: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON KNOWLEDGE CAPTURE, P9
   Lehman Joel, 2018, ARXIV180303453
   Lin Zhiyu, 2017, ARXIV170903969
   MacGlashan James, 2017, P 34 INT C MACH LEAR, P2285
   Mathewson Kory, 2017, ARXIV170301274
   Mnih V., 2013, ARXIV13125602
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohamed S., 2015, ADV NEURAL INFORM PR, V2, P2125
   Nair A, 2018, IEEE INT CONF ROBOT, P6292
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Orseau L, 2013, LECT NOTES ARTIF INT, V8139, P158
   Pathak D., 2017, P INT C MACH LEARN, P2778
   Pilarski P M, 2011, P IEEE INT C REH ROB, P1
   Salge C, 2014, EMERGENCE COMPLEX CO, V9, P67, DOI 10.1007/978-3-642-53734-9_4
   Saunders W, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P2067
   Schaul  Tom, 2015, ABS151105952 CORR
   Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159
   Sutton R.S., 2018, REINFORCEMENT LEARNI
   Tarvainen Antti, 2017, ARXIV170301780
   Van Hasselt H., 2016, AAAI, P2094
   Vecerik Matej, 2017, ARXIV1707088172
   Wang Z., 2016, SER P MACHINE LEARNI, P1995
   Warnell Garrett, 2017, ARXIV170910163
   Wilson Aaron, 2012, P ADV NEUR INF PROC, P1133
   Wirth C., 2016, 30 AAAI C ART INT AA, P2222
   Wirth Christian, 2017, J MACHINE LEARNING R, V18, P4945
   Wirth Christian, 2013, ECML PKDD WORKSH REI
   Zhang Xiaoqin, 2018, ARXIV180110459
   Zhu Y., 2018, ARXIV180209564
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002055
DA 2019-06-15
ER

PT S
AU Imani, E
   Graves, E
   White, M
AF Imani, Ehsan
   Graves, Eric
   White, Martha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI An Off-policy Policy Gradient Theorem Using Emphatic Weightings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. We develop a new actor-critic algorithm-called Actor Critic with Emphatic weightings (ACE)-that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods-particularly OffPAC and DPG-converge to the wrong solution whereas ACE finds the optimal solution.
C1 [Imani, Ehsan; Graves, Eric; White, Martha] Univ Alberta, Reinforcement Learning & Artificial Intelligence, Dept Comp Sci, Edmonton, AB, Canada.
RP Imani, E (reprint author), Univ Alberta, Reinforcement Learning & Artificial Intelligence, Dept Comp Sci, Edmonton, AB, Canada.
EM imani@ualberta.ca; graves@ualberta.ca; whitem@ualberta.ca
FU Alberta Innovates; Alberta Machine Intelligence Institute
FX The authors would like to thank Alberta Innovates for funding the
   Alberta Machine Intelligence Institute and by extension this research.
   We would also like to thank Hamid Maei, Susan Murphy, and Rich Sutton
   for their helpful discussions and insightful comments.
CR BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bhatnagar S, 2009, AUTOMATICA
   Bhatnagar S., 2008, ADV NEURAL INFORM PR, V20, P105
   Degris T, 2012, P AMER CONTR CONF, P2177
   Degris Thomas, 2012, INT C MACH LEARN
   Greensmith E, 2004, J MACH LEARN RES, V5, P1471
   Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595
   Gu S., 2017, NEURAL INFORM PROCES, P3849
   Gu Shixiang, 2016, ARXIV161102247
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Lillicrap T P, 2015, ARXIV150902971
   LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1023/A:1022628806385
   Maei H., 2011, THESIS
   Maei Hamid Reza, 2018, ARXIV180207842
   Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Peters J, 2005, LECT NOTES ARTIF INT, V3720, P280
   Precup Doina, 2000, THESIS
   Schaul T, 2015, ARXIV151105952
   Silver D, 2014, P 31
   Sutton R., 1988, MACHINE LEARNING
   Sutton R., 2000, ADV NEURAL INFORM PR
   Sutton R. S., 2011, 10 INT C AUT AG MULT, V2, P761
   Sutton Richard S, 2016, J MACHINE LEARNING R
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Thomas Philip, 2014, P 31 INT C MACH LEAR, P441
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Weaver  L., 2001, P 17 C UNC ART INT, P538
   White A., 2015, THESIS
   White Martha, 2017, INT C MACH LEARN, P3742
   Williams Ronald J, 1992, MACHINE LEARNING
   WITTEN IH, 1977, INFORM CONTROL, V34, P286, DOI 10.1016/S0019-9958(77)90354-0
   Yu H., 2015, ANN C LEARN THEOR
   Ziyu Wang, 2016, SAMPLE EFFICIENT ACT
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300010
DA 2019-06-15
ER

PT S
AU Imani, M
   Ghoreishi, SF
   Braga-Neto, UM
AF Imani, Mahdi
   Ghoreishi, Seyede Fatemeh
   Braga-Neto, Ulisses M.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor
   Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications, due to ethical, economic, and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process, parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage, the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm, followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces, as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data.
C1 [Imani, Mahdi; Ghoreishi, Seyede Fatemeh; Braga-Neto, Ulisses M.] Texas A&M Univ, College Stn, TX 77843 USA.
RP Imani, M (reprint author), Texas A&M Univ, College Stn, TX 77843 USA.
EM m.imani88@tamu.edu; f.ghoreishi88@tamu.edu; ulisses@ece.tamu.edu
FU National Science Foundation [CCF-1718924]
FX The authors acknowledge the support of the National Science Foundation,
   through NSF award CCF-1718924.
CR Antos A., 2008, ADV NEURAL INFORM PR, P9
   Asmuth J., 2012, ARXIV12023699
   Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1
   Bittner M, 2000, NATURE, V406, P536, DOI 10.1038/35020115
   Braga-Neto U, 2011, CONF REC ASILOMAR C, P1050, DOI 10.1109/ACSSC.2011.6190172
   Busoniu L, 2010, AUTOM CONTROL ENG SE, P1, DOI 10.1201/9781439821091-f
   Doya K, 2002, NEURAL COMPUT, V14, P1347, DOI 10.1162/089976602753712972
   Drougard N., 2014, AAAI, P2257
   Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377
   Fonteneau R, 2013, IEEE SYMP ADAPT DYNA, P77, DOI 10.1109/ADPRL.2013.6614992
   Gasic M, 2014, IEEE-ACM T AUDIO SPE, V22, P28, DOI 10.1109/TASL.2013.2282190
   Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049
   Gilks WR, 1995, MARKOV CHAIN MONTE C
   Guez  A., 2012, ADV NEURAL INFORM PR, P1025
   Guez A, 2013, J ARTIF INTELL RES, V48, P841
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Imani M., 2018, IEEE T CONTROL SYSTE
   Imani M, 2017, IEEE T SIGNAL PROCES, V65, P359, DOI 10.1109/TSP.2016.2614798
   Imani M, 2018, AUTOMATICA, V95, P172, DOI 10.1016/j.automatica.2018.05.028
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Vien NA, 2013, APPL INTELL, V39, P345, DOI 10.1007/s10489-012-0416-2
   Poupart P., 2006, P 23 INT C MACH LEAR, V148, P697, DOI DOI 10.1145/1143844.1143932
   Powell W. B., 2012, OPTIMAL LEARNING, V841
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ross Stephane, 2008, Uncertain Artif Intell, V2008, P476
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Trevizan FW, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2023
   Wang T., 2005, INT C MACH LEARN ICM, P956
   Wang Y., 2012, ARXIV12066449
   Weeraratna AT, 2002, CANCER CELL, V1, P279, DOI 10.1016/S1535-6108(02)00045-4
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002067
DA 2019-06-15
ER

PT S
AU Insafutdinov, E
   Dosovitskiy, A
AF Insafutdinov, Eldar
   Dosovitskiy, Alexey
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Learning of Shape and Pose with Differentiable Point Clouds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors which we then distill to a single "student" model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.
C1 [Insafutdinov, Eldar] Max Planck Inst Informat, Saarbrucken, Germany.
   [Dosovitskiy, Alexey] Intel Labs, Munich, Germany.
RP Insafutdinov, E (reprint author), Max Planck Inst Informat, Saarbrucken, Germany.
EM eldar@mpi-inf.mpg.de; adosovitskiy@gmail.com
CR Abadi M., 2016, OSDI
   Cashman T. J., 2011, PAMI, V35
   Chang A. X., 2015, TECHNICAL REPORT
   Chen Qifeng, 2017, ICCV
   Choy C. B., 2016, ECCV
   Fan H., 2017, CVPR
   Guzman-Rivera A., 2012, NIPS
   Kato H., 2018, CVPR
   Kingma D. P., 2015, ICLR
   Li J., 2017, SIGGRAPH
   Lin C.-H., 2018, AAAI
   Loper M. M., 2014, ECCV
   Rezende D., 2016, NIPS
   Rhodin H., 2015, ICCV
   Soltani A. A., 2017, CVPR
   Sun X., 2018, CVPR
   Tatarchenko M., 2017, ICCV
   Tulsiani S., 2017, CVPR
   Tulsiani S., 2017, PAMI, V39
   Tulsiani S., 2018, CVPR
   Vicente S., 2014, CVPR
   Wu J, 2016, NIPS
   Wu J., 2018, IJCV
   Yan X., 2016, NIPS
   Yang Y., 2018, CVPR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302079
DA 2019-06-15
ER

PT S
AU Ishida, T
   Niu, G
   Sugiyama, M
AF Ishida, Takashi
   Niu, Gang
   Sugiyama, Masashi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Binary Classification from Positive-Confidence Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SUPPORT
AB Can we learn a binary classifier from only positive data, without any negative data or unlabeled data? We show that if one can equip positive data with confidence (positive-confidence), one can successfully learn a binary classifier, which we name positive-confidence (Pconf) classification. Our work is related to one-class classification which is aimed at "describing" the positive class by clustering-related methods, but one-class classification does not have the ability to tune hyper-parameters and their aim is not on "discriminating" positive and negative classes. For the Pconf classification problem, we provide a simple empirical risk minimization framework that is model-independent and optimization-independent. We theoretically establish the consistency and an estimation error bound, and demonstrate the usefulness of the proposed method for training deep neural networks through experiments.
C1 [Ishida, Takashi; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan.
   [Ishida, Takashi; Niu, Gang; Sugiyama, Masashi] RIKEN, Tokyo, Japan.
RP Ishida, T (reprint author), Univ Tokyo, Tokyo, Japan.; Ishida, T (reprint author), RIKEN, Tokyo, Japan.
EM ishida@msk.u-tokyo.ac.jp; gang.niu@riken.jp; sugi@k.u-tokyo.ac.jp
FU Sumitomo Mitsui Asset Management; JST CREST [JPMJCR1403]
FX TI was supported by Sumitomo Mitsui Asset Management. MS was supported
   by JST CREST JPMJCR1403. We thank Ikko Yamane and Tomoya Sakai for the
   helpful discussions. We also thank anonymous reviewers for pointing out
   numerical issues in our experiments, and for pointing out the necessary
   condition in Theorem 1 in our earlier work of this paper.
CR Bao  H., 2018, ICML
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Boyd S., 2004, CONVEX OPTIMIZATION
   Breunig M. M., 2000, ACM SIGMOD
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chapelle O., 2006, SEMISUPERVISED LEARN
   du Plessis M. C., 2013, TAAI
   du Plessis  M.C., 2014, NIPS
   du Plessis M. C., 2015, ICML
   du Plessis MC, 2017, MACH LEARN, V106, P463, DOI 10.1007/s10994-016-5604-6
   Du Plessis MC, 2014, IEICE T INF SYST, VE97D, P1358, DOI 10.1587/transinf.E97.D.1358
   Elkan  C., 2008, KDD
   Fishman GS, 1996, MONTE CARLO CONCEPTS
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Hido  S., 2008, ICDM
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ishida  T., 2017, NIPS
   Ishida  T., 2018, ARXIV181004327
   Johansson F., 2013, MPMATH PYTHON LIB AR
   Khan S. S., 2009, IR C ART INT COGN SC
   Kingma D. P., 2015, ICLR
   Kipf T. N., 2017, ICLR
   Kiryo R., 2017, NIPS
   Ledoux M., 1991, PROBABILITY BANACH S
   Lu  N., 2018, ARXIV180810585V2
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323
   Menon A., 2015, ICML
   Miyato T., 2016, ICLR
   Mohri M., 2012, FDN MACHINE LEARNING
   Nair V., 2010, ICML
   Natarajan N., 2013, NIPS
   Niu  G., 2016, NIPS
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Oliver  A., 2018, NEURIPS
   Paszke  A., 2017, AUT WORKSH NIPS
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Quadrianto  N., 2008, ICML
   Sakai  T., 2017, ICML
   Sakai T, 2018, MACH LEARN, V107, P767, DOI 10.1007/s10994-017-5678-9
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Scholkopf B., 2001, LEARNING KERNELS
   Scott  C., 2009, AISTATS
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Smola  A., 2009, AISTATS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sugiyama M, 2012, ADAPT COMPUT MACH LE, P1
   Sugiyama M, 2014, NEURAL COMPUT, V26, P84, DOI 10.1162/NECO_a_00534
   Tax  D., 1999, PATTERN RECOGNITION
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Yang  Z., 2016, ICML
   Yu F. X., 2013, ICML
   Yu X., 2018, ECCV
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000042
DA 2019-06-15
ER

PT S
AU Ishikawa, I
   Fujii, K
   Ikeda, M
   Hashimoto, Y
   Kawahara, Y
AF Ishikawa, Isao
   Fujii, Keisuke
   Ikeda, Masahiro
   Hashimoto, Yuka
   Kawahara, Yoshinobu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PATTERNS; KERNELS
AB The development of a metric for structural data is a long-term problem in pattern recognition and machine learning. In this paper, we develop a general metric for comparing nonlinear dynamical systems that is defined with Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric includes the existing fundamental metrics for dynamical systems, which are basically defined with principal angles between some appropriately-chosen subspaces, as its special cases. We also describe the estimation of our metric from finite data. We empirically illustrate our metric with an example of rotation dynamics in a unit disk in a complex plane, and evaluate the performance with real-world time-series data.
C1 [Ishikawa, Isao; Fujii, Keisuke; Ikeda, Masahiro; Hashimoto, Yuka; Kawahara, Yoshinobu] RIKEN Ctr Adv Intelligence Project, Wako, Saitama, Japan.
   [Ishikawa, Isao; Ikeda, Masahiro; Hashimoto, Yuka] Keio Univ, Sch Fundamental Sci & Technol, Tokyo, Japan.
   [Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.
RP Ishikawa, I (reprint author), RIKEN Ctr Adv Intelligence Project, Wako, Saitama, Japan.; Ishikawa, I (reprint author), Keio Univ, Sch Fundamental Sci & Technol, Tokyo, Japan.
EM isao.ishikawa@riken.jp; keisuke.fujii.zh@riken.jp;
   masahiro.ikeda@riken.jp; yukahashimoto@keio.jp;
   ykawahara@sanken.osaka-u.ac.jp
CR Banach S., 1995, THEORIE OPERATIONS L
   Bao QF, 2006, J MATH ANAL APPL, V323, P481, DOI 10.1016/j.jmaa.2005.10.064
   Berger E, 2015, ADV ROBOTICS, V29, P331, DOI 10.1080/01691864.2014.981292
   Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010
   Chaudhry R., 2014, P 52 IEEE C DEC CONT, P5377
   Chen Y., 2015, UCR TIME SERIES CLAS
   De Cock K, 2002, SYST CONTROL LETT, V46, P265, DOI 10.1016/S0167-6911(02)00135-4
   Fujii K, 2017, LECT NOTES ARTIF INT, V10536, P127, DOI 10.1007/978-3-319-71273-4_11
   Kawahara Y., 2016, ADV NEURAL INFORM PR, V29, P911
   Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315
   Kutz JN, 2016, SIAM J APPL DYN SYST, V15, P713, DOI 10.1137/15M1023543
   Martin RJ, 2000, IEEE T SIGNAL PROCES, V48, P1164, DOI 10.1109/78.827549
   Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x
   Mezic I, 2004, PHYSICA D, V197, P101, DOI 10.1016/j.physd.2004.06.015
   Mezic I., 2016, COMP DYNAMICS DISSIP, P454
   Proctor JL, 2016, SIAM J APPL DYN SYST, V15, P142, DOI 10.1137/15M1013857
   Proctor JL, 2015, INT HEALTH, V7, P139, DOI 10.1093/inthealth/ihv009
   Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059
   Semenov EM, 2010, J FUNCT ANAL, V259, P1517, DOI 10.1016/j.jfa.2010.05.011
   SUCHESTON L, 1967, AM MATH MON, V74, P308, DOI 10.2307/2316038
   SUCHESTON L, 1964, MATH Z, V86, P327, DOI 10.1007/BF01110407
   Takeishi N., 2017, P 26 INT JOINT C ART, P2814
   Takeishi N, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.033310
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0
   Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302084
DA 2019-06-15
ER

PT S
AU Ito, S
   Hatano, D
   Sumita, H
   Yabe, A
   Fukunaga, T
   Kakimura, N
   Kawarabayashi, K
AF Ito, Shinji
   Hatano, Daisuke
   Sumita, Hanna
   Yabe, Akihiro
   Fukunaga, Takuro
   Kakimura, Naonori
   Kawarabayashi, Ken-ichi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Regret Bounds for Online Portfolio Selection with a Cardinality
   Constraint
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Online portfolio selection is a sequential decision-making problem in which a learner repetitively selects a portfolio over a set of assets, aiming to maximize long-term return. In this paper, we study the problem with the cardinality constraint that the number of assets in a portfolio is restricted to be at most k, and consider two scenarios: (i) in the full-feedback setting, the learner can observe price relatives (rates of return to cost) for all assets, and (ii) in the bandit-feedback setting, the learner can observe price relatives only for invested assets. We propose efficient algorithms for these scenarios, which achieve sublinear regrets. We also provide regret (statistical) lower bounds for both scenarios which nearly match the upper bounds when k is a constant. In addition, we give a computational lower bound, which implies that no algorithm maintains both computational efficiency, as well as a small regret upper bound.
C1 [Ito, Shinji; Yabe, Akihiro] NEC Corp Ltd, Minato, Japan.
   [Hatano, Daisuke; Fukunaga, Takuro] RIKEN AIP, Tokyo, Japan.
   [Sumita, Hanna] Tokyo Metropolitan Univ, Tokyo, Japan.
   [Fukunaga, Takuro] JST PRESTO, Tokyo, Japan.
   [Kakimura, Naonori] Keio Univ, Tokyo, Japan.
   [Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan.
RP Ito, S (reprint author), NEC Corp Ltd, Minato, Japan.
FU JST ERATO, Japan [JPMJER1201]; JSPS KAKENHI [JP18H05291]
FX This work was supported by JST ERATO Grant Number JPMJER1201, Japan, and
   JSPS KAKENHI Grant Number JP18H05291.
CR Agarwal  A., 2006, P 23 INT C MACH LEAR, P9
   Alon N, 2004, PROBABILISTIC METHOD
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Combes  R., 2015, ADV NEURAL INFORM PR, V28, P2116
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Das P., 2014, 28 AAAI C ART INT
   FENWICK PM, 1994, SOFTWARE PRACT EXPER, V24, P327, DOI 10.1002/spe.4380240306
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gentle JE, 2009, STAT COMPUT SER, P3
   HAKANSSON NH, 1995, HBK OPERAT RES MANAG, V9, P65
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Kalai A., 2002, J MACHINE LEARNING R, V3, P423
   Karp R. M., 1972, COMPLEXITY COMPUTER, P85, DOI [DOI 10.1007/978-1-4684-2001-2_9, 10.1007/978-1-4684-2001-29]
   KELLY JL, 1956, BELL SYST TECH J, V35, P917, DOI 10.1002/j.1538-7305.1956.tb03809.x
   Li B., 2012, P INT C MACH LEARN E, P273
   Li B, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2512962
   Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0
   Matousek J., 2001, LECT NOTES
   Ordentlich E, 1998, MATH OPER RES, V23, P960, DOI 10.1287/moor.23.4.960
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Ye Y., 2018, P 21 INT C ART INT S, P2008
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005019
DA 2019-06-15
ER

PT S
AU Jacot, A
   Gabriel, F
   Hongler, C
AF Jacot, Arthur
   Gabriel, Franck
   Hongler, Clement
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Tangent Kernel: Convergence and Generalization in Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MULTILAYER FEEDFORWARD NETWORKS
AB At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit (12;9), thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function f(theta) (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.
   We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function f(theta) follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping.
   Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.
C1 [Jacot, Arthur; Gabriel, Franck; Hongler, Clement] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Gabriel, Franck] Imperial Coll London, London, England.
RP Jacot, A (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM arthur.jacot@netopera.net; franckrgabriel@gmail.com;
   clement.hongler@gmail.com
FU ERC CG CRITICAL; ERC SG Constamis; NCCR SwissMAP; Blavatnik Family
   Foundation; Latsis Foundation
FX The authors thank K. Kytola for many interesting discussions. The second
   author was supported by the ERC CG CRITICAL. The last author
   acknowledges support from the ERC SG Constamis, the NCCR SwissMAP, the
   Blavatnik Family Foundation and the Latsis Foundation.
CR Belkin  M., 2018, UNDERSTAND DEEP LEAR
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Choromanska A., 2015, ARTIF INTELL, P192
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Dragomir S. S., 2003, SOME GRONWALL TYPE I
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Karakida  R., 2018, UNIVERSAL STAT FISHE
   Lee J. H., 2018, ICLR
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115
   Neal RM, 1996, BAYESIAN LEARNING NE
   Pascanu R., 2014, SADDLE POINT PROBLEM
   Pennington  J., 2017, INT C MACH LEARN, V70, P2798
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Sagun  L., 2017, ABS170604454 CORR
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Zhang  C., 2017, ICLR 2017 P FEB
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003016
DA 2019-06-15
ER

PT S
AU Jain, S
   Huth, AG
AF Jain, Shailee
   Huth, Alexander G.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Incorporating Context into Language Encoding Models for fMRI
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work, we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.
C1 [Jain, Shailee; Huth, Alexander G.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78751 USA.
   [Huth, Alexander G.] Univ Texas Austin, Dept Neurosci, Austin, TX 78751 USA.
RP Jain, S (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78751 USA.
EM shailee@cs.utexas.edu; huth@cs.utexas.edu
FU NSF [IIS-1208203]; NIH NEI [EY019684-01A1]; Burroughs-Wellcome Fund;
   NVIDIA
FX We thank Jack Gallant, Wendy de Heer, Frederic Theunissen, and Thomas
   Griffiths for helping design the fMRI experiment and collecting the data
   used here; Brittany Griffin and Anwar Nuflez for segmenting and
   flattening cortical surfaces; and Niko Kriegeskorte for useful
   discussions. Data collection was supported by NSF grant IIS-1208203 and
   NIH NEI grant EY019684-01A1. This work was supported by grants from the
   Burroughs-Wellcome Fund and NVIDIA. We also acknowledge the Texas
   Advanced Computing Center (TACC) at The University of Texas at Austin
   for providing HPC resources that have contributed to the research
   results reported within this paper.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Agrawal P., 2014, ABS14075104 CORR
   Baldassano C, 2017, NEURON, V95, P709, DOI 10.1016/j.neuron.2017.06.041
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   de Heer WA, 2017, J NEUROSCI, V37, P6539, DOI 10.1523/JNEUROSCI.3267-16.2017
   Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001
   Gao JS, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00023
   Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015
   Huth AG, 2016, NATURE, V532, P453, DOI 10.1038/nature17637
   Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713
   Kell A. J., 2018, NEURON, V98
   Lerner Y, 2011, J NEUROSCI, V31, P2906, DOI 10.1523/JNEUROSCI.3684-10.2011
   McCann B., 2017, ABS170800107 CORR
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876
   Naselaris T, 2011, NEUROIMAGE, V56, P400, DOI 10.1016/j.neuroimage.2010.07.073
   Pereira F, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03068-4
   Peters M., 2017, P 55 ANN M ASS COMP, V1, P1756
   Peters M. E., 2018, ABS180205365 CORR
   Ruder  Sebastian, 2016, ABS160904747 CORR
   SUNDERMEYER Martin, 2012, 13 ANN C INT SPEECH
   Wehbe L., 2014, EMNLP
   Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575
   Wu MCK, 2006, ANNU REV NEUROSCI, V29, P477, DOI 10.1146/annurev.neuro.29.051605.113024
   Xu H., 2016, P 2016 C EMP METH NA, P2017
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001019
DA 2019-06-15
ER

PT S
AU Jaini, P
   Poupart, P
   Yu, YL
AF Jaini, Priyank
   Poupart, Pascal
   Yu, Yaoliang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deep Homogeneous Mixture Models: Representation, Separation, and
   Approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE
AB At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in exact representation size between deep mixture architectures and shallow ones. In contrast, for approximate representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within epsilon accuracy by combining O(1/epsilon(2)) "shallow" architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation.
C1 [Jaini, Priyank; Yu, Yaoliang] Univ Waterloo, Dept Comp Sci, Waterloo, ON, Canada.
   [Jaini, Priyank; Yu, Yaoliang] Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada.
   [Poupart, Pascal] Univ Waterloo, Vector Inst, Waterloo, ON, Canada.
   [Poupart, Pascal] Waterloo AI Inst, Waterloo, ON, Canada.
RP Jaini, P (reprint author), Univ Waterloo, Dept Comp Sci, Waterloo, ON, Canada.; Jaini, P (reprint author), Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada.
EM pjaini@uwaterloo.ca; ppoupart@uwaterloo.ca; yaoliang.yu@uwaterloo.ca
FU NSERC discovery program
FX The authors gratefully acknowledge support from the NSERC discovery
   program.
CR Alon N, 2009, COMB PROBAB COMPUT, V18, P3, DOI 10.1017/S0963548307008917
   Anandkumar Animashree, 2012, ADV NEURAL INFORM PR
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Caron Richard, 2005, TECHNICAL REPORT
   Choi MJ, 2011, J MACH LEARN RES, V12, P1771
   Cohen N., 2016, C LEARN THEOR, P698
   Cohen Nadav, 2017, ARXIV170502302V4
   Cohen Nadav, 2016, ICML
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Delalleau O., 2011, ADV NEURAL INFORM PR, P666
   Dinh L., 2014, ARXIV14108516
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   HACKBUSCH  W., 2012, TENSOR SPACES NUMERI
   Ishteva M, 2015, LECT NOTES COMPUT SC, V9237, P49, DOI 10.1007/978-3-319-22482-4_6
   Le Song, 2013, ICML
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2004, COMP VIS PATT REC 20, V2, P104
   Li JQ, 2000, ADV NEUR IN, V12, P279
   Martens James, 2014, ARXIV14117717
   McLachlan G., 2004, FINITE MIXTURE MODEL
   Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605
   Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879
   Nguyen Hien D, 2016, ARXIV161103974
   Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381
   Poon Hoifung, 2011, UNCERTAINTY ARTIFICI
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Sharir Or, 2018, ARXIV161004167V5
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Zhao QF, 2015, INT CONF MACH LEARN, P116, DOI 10.1109/ICMLC.2015.7340908
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001066
DA 2019-06-15
ER

PT S
AU Jaiswal, A
   Wu, Y
   AbdAlmageed, W
   Natarajan, P
AF Jaiswal, Ayush
   Wu, Yue
   AbdAlmageed, Wael
   Natarajan, Premkumar
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Adversarial Invariance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.
C1 [Jaiswal, Ayush; Wu, Yue; AbdAlmageed, Wael; Natarajan, Premkumar] USC Informat Sci Inst, Marina Del Rey, CA 90292 USA.
RP Jaiswal, A (reprint author), USC Informat Sci Inst, Marina Del Rey, CA 90292 USA.
EM ajaiswal@isi.edu; yue_wu@isi.edu; wamageed@isi.edu; pnataraj@isi.edu
FU Defense Advanced Research Projects Agency [FA8750-16-2-0204]
FX This work is based on research sponsored by the Defense Advanced
   Research Projects Agency under agreement number FA8750-16-2-0204. The
   U.S. Government is authorized to reproduce and distribute reprints for
   governmental purposes notwithstanding any copyright notation thereon.
   The views and conclusions contained herein are those of the authors and
   should not be interpreted as necessarily representing the official
   policies or endorsements, either expressed or implied, of the Defense
   Advanced Research Projects Agency or the U.S. Government.
CR Antoniou A., 2017, ARXIV171104340
   Aubry Mathieu, 2014, P IEEE COMP SOC C CO
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chen  M., 2012, P 29 INT C MACH LEAR, P1627
   Ganin Y, 2016, J MACH LEARN RES, V17
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Jaiswal Ayush, 2018, ARXIV180501049
   Ko T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3586
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Yujia, 2014, ARXIV14125244
   Louizos Christos, 2016, P INT C LEARN REPR
   Masi Iacopo, 2018, IEEE T PATTERN ANAL
   Miao JY, 2016, PROCEDIA COMPUT SCI, V91, P919, DOI 10.1016/j.procs.2016.07.111
   Ruder S., 2017, ARXIV170605098
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xie Q., 2017, ADV NEURAL INFORM PR, P585
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305013
DA 2019-06-15
ER

PT S
AU Jakab, T
   Gupta, A
   Bilen, H
   Vedaldi, A
AF Jakab, Tomas
   Gupta, Ankush
   Bilen, Hakan
   Vedaldi, Andrea
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Learning of Object Landmarks through Conditional Image
   Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.
C1 [Jakab, Tomas; Gupta, Ankush; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England.
   [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Jakab, T (reprint author), Univ Oxford, Visual Geometry Grp, Oxford, England.
EM tomj@robots.ox.ac.uk; ankush@robots.ox.ac.uk; hbilen@ed.ac.uk;
   vedaldi@robots.ox.ac.uk
FU EPSRC AIMS CDT; Clarendon Fund scholarship; ERC [638009-IDIU]
FX We are grateful for the support provided by EPSRC AIMS CDT, ERC
   638009-IDIU, and the Clarendon Fund scholarship. We would like to thank
   James Thewlis for suggestions and support with code and data, and David
   Novotny and Triantafyllos Afouras for helpful advice.
CR [Anonymous], 2015, P ICCV
   Bruna J., 2016, P ICLR
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Charles J., 2013, P BMVC
   CHEN Q, 2017, P ICCV, V1
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chen X., 2014, P NIPS
   Chung J. Son, 2018, INTERSPEECH
   Denton E. L., 2017, P NIPS
   Dosovitskiy A., 2016, P NIPS
   Dosovitskiy Alexey, 2016, ADV NEURAL INFORM PR, V29, P658
   Duchon J, 1977, CONSTRUCTIVE THEORY
   Gatys L. A., 2016, P CVPR
   Goodfellow I. J., 2014, P NIPS
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ionescu C., 2014, PAMI
   Isola P., 2017, P CVPR
   Johnson J., 2016, P ECCV
   Kalchbrenner N., 2016, ARXIV161000527
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koestinger M., 2011, ICCV WORKSH
   Larsson G., 2016, P ECCV
   LeCun Y., 2004, P CVPR
   Ledig C., 2017, P CVPR
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Netzer Y., 2011, NIPS DLW, V2011
   Nguyen A., 2016, P NIPS
   Nguyen A., 2017, P CVPR
   Patraucean V., 2015, ICLR WORKSH
   Pfister T., 2014, P AS C COMP VIS
   Pfister T., 2013, P BMVC
   Pfister T., 2015, P ICCV
   Reed S. E., 2015, P NIPS
   Reed S. E., 2016, ADV NEURAL INFORM PR, P217
   Shu Z., 2018, P ECCV
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava N., 2015, INT C MACH LEARN, P843
   Sun Y., 2013, P CVPR
   Sutskever  I., 2009, ADV NEURAL INFORM PR, P1601
   Suwajanakorn S., 2018, P NIPS
   Thewlis J., 2017, P ICCV
   Thewlis J., 2017, P NIPS
   Villegas R., 2017, ARXIV170405831
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613
   Wahba G, 1990, SPLINE MODELS OBSERV, V59
   Whitney W. F., 2016, ICLR WORKSH
   Wiles O., 2018, P BMVC
   Xue T., 2016, P NIPS
   Yang Y., 2011, P CVPR
   Zhang J., 2014, P ECCV
   Zhang Y., 2018, P CVPR
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhang Zhanpeng, 2016, PAMI
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304006
DA 2019-06-15
ER

PT S
AU Jalalzai, H
   Clemencon, S
   Sabourin, A
AF Jalalzai, Hamid
   Clemencon, Stephan
   Sabourin, Anne
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On Binary Classification in Extreme Regions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in R(d )with d >= 1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.
C1 [Jalalzai, Hamid; Clemencon, Stephan; Sabourin, Anne] Univ Paris Saclay, LTCI Telecom ParisTech, F-75013 Paris, France.
RP Jalalzai, H (reprint author), Univ Paris Saclay, LTCI Telecom ParisTech, F-75013 Paris, France.
EM hamid.jalalzai@telecom-paristech.fr;
   stephan.clemencon@telecom-paristech.fr;
   anne.sabourin@telecom-paristech.fr
CR Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350
   Cai JJ, 2011, ANN STAT, V39, P1803, DOI 10.1214/11-AOS891
   Carpentier  A., 2014, ADV NEURAL INFORM PR, P1089
   DEHAAN L, 1987, STOCH PROC APPL, V25, P83, DOI 10.1016/0304-4149(87)90191-8
   Devroye L., 1996, APPL MATH STOCHASTIC
   Goix N., 2016, ARTIF INTELL, P75
   Goix N, 2017, J MULTIVARIATE ANAL, V161, P12, DOI 10.1016/j.jmva.2017.06.010
   Mendelson S, 2018, PROBAB THEORY REL, V171, P459, DOI 10.1007/s00440-017-0784-y
   NAKAI K, 1992, GENOMICS, V14, P897, DOI 10.1016/S0888-7543(05)80111-9
   Ohannessian Mesrob I, 2012, COLT, P21
   Resnick S., 1987, SPRINGER SERIES OPER
   Roos Teemu, 2006, ADV NEURAL INFORM PR, V18, P1129
   Stephenson A., 2003, EXTREMES, V6, P49, DOI DOI 10.1023/A:1026277229992
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303012
DA 2019-06-15
ER

PT S
AU Jamieson, K
   Jain, L
AF Jamieson, Kevin
   Jain, Lalit
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Bandit Approach to Multiple Testing with False Discovery Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MULTIARMED BANDIT
AB We propose an adaptive sampling approach for multiple testing which aims to maximize statistical power while ensuring anytime false discovery control. We consider n distributions whose means are partitioned by whether they are below or equal to a baseline (nulls), versus above the baseline (actual positives). In addition, each distribution can be sequentially and repeatedly sampled. Inspired by the multi-armed bandit literature, we provide an algorithm that takes as few samples as possible to exceed a target true positive proportion (i.e. proportion of actual positives discovered) while giving anytime control of the false discovery proportion (nulls predicted as actual positives). Our sample complexity results match known information theoretic lower bounds and through simulations we show a substantial performance improvement over uniform sampling and an adaptive elimination style algorithm. Given the simplicity of the approach, and its sample efficiency, the method has promise for wide adoption in the biological sciences, clinical testing for drug discovery, and online A/B/n testing problems.
C1 [Jamieson, Kevin; Jain, Lalit] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.
   [Jamieson, Kevin] Optimizely, San Francisco, CA 94105 USA.
RP Jamieson, K (reprint author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.; Jamieson, K (reprint author), Optimizely, San Francisco, CA 94105 USA.
EM jamieson@cs.washington.edu; lalitj@cs.washington.edu
CR Balsubramani A., 2014, ARXIV E PRINTS
   Benjamini Y, 2001, ANN STAT, V29, P1165
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Cao Tongyi, 2017, ARXIV171108018
   Castro RM, 2014, BERNOULLI, V20, P2217, DOI 10.3150/13-BEJ555
   Chen Lijie, 2017, ARTIF INTELL, P101
   Chen Lijie, 2017, C LEARN THEOR, P535
   Chen S., 2014, ADV NEURAL INFORM PR, V26, P379
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Hao LH, 2008, NATURE, V454, P890, DOI 10.1038/nature07151
   Hartman P, 1941, AM J MATH, V63, P169, DOI 10.2307/2371287
   Haupt J, 2011, IEEE T INFORM THEORY, V57, P6222, DOI 10.1109/TIT.2011.2162269
   Heckel Reinhard, 2018, INT C ART INT STAT, P1057
   Jamieson K. G., 2014, P 27 C LEARN THEOR, V35, P423
   Johari Ramesh, 2015, ARXIV151204922
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Kano Hideaki, 2017, ARXIV171006360
   Karnin Z., 2013, P 30 INT C MACH LEAR, V28, P1238
   Kaufmann E, 2016, J MACH LEARN RES, V17
   Locatelli Andrea, 2016, ICML, P1690
   Malloy ML, 2014, IEEE T INFORM THEORY, V60, P7862, DOI 10.1109/TIT.2014.2363846
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   Optimizely, 2018, ACC EXP MACH LEARN
   Rabinovich Maxim, 2017, ARXIV170505391
   Raginsky Maxim, 2011, ADV NEURAL INFORM PR, P1026
   Ramdas A., 2017, ARXIV E PRINTS
   Rocklin GJ, 2017, SCIENCE, V357, P168, DOI 10.1126/science.aan0693
   Simchowitz Max, 2017, C LEARN THEOR, P1794
   Tanczos Ervin, 2017, ADV NEURAL INFORM PR, V30, P5896
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Yang Fanny, 2017, ADV NEURAL INFORM PR, P5959
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303064
DA 2019-06-15
ER

PT S
AU Jayaraman, B
   Wang, LX
   Evans, D
   Gu, QQ
AF Jayaraman, Bargav
   Wang, Lingxiao
   Evans, David
   Gu, Quanquan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Distributed Learning without Distress: Privacy-Preserving Empirical Risk
   Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Distributed learning allows a group of independent data owners to collaboratively learn a model over their data sets without exposing their private data. We present a distributed learning approach that combines differential privacy with secure multiparty computation. We explore two popular methods of differential privacy, output perturbation and gradient perturbation, and advance the state-of-the-art for both methods in the distributed learning setting. In our output perturbation method, the parties combine local models within a secure computation and then add the required differential privacy noise before revealing the model. In our gradient perturbation method, the data owners collaboratively train a global model via an iterative learning algorithm. At each iteration, the parties aggregate their local gradients within a secure computation, adding sufficient noise to ensure privacy before the gradient updates are revealed. For both methods, we show that the noise can be reduced in the multi-party setting by adding the noise inside the secure computation after aggregation, asymptotically improving upon the best previous results. Experiments on real world data sets demonstrate that our methods provide substantial utility gains for typical privacy requirements.
C1 [Jayaraman, Bargav; Evans, David] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.
   [Wang, Lingxiao; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
RP Jayaraman, B (reprint author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.
EM bj4nq@virginia.edu; lingxw@cs.ucla.edu; evans@virginia.edu;
   qgu@cs.ucla.edu
FU National Science Foundation [1111781, 1717950, 1804603]; Google; Intel;
   Amazon
FX This work was partially supported by the National Science Foundation
   (Awards #1111781, #1717950, and #1804603) and research awards from
   Google, Intel, and Amazon.
CR Abadi Martin, 2016, ACM SIGSAC C COMP CO
   Asuncion A., 2007, UCI MACHINE LEARNING
   Bindschaedler Vincent, 2017, 7 ACM C DAT APPL SEC
   Bonawitz Keith, 2017, ACM SIGSAC C COMP CO
   BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645
   Bun Mark, 2016, THEOR CRYPT C
   Burkhart Martin, 2010, USENIX SEC S
   Carlini Nicholas, 2018, 180208232 ARXIV
   Chase Melissa, 2017, TECHNICAL REPORT
   Chaudhuri Kamalika, 2009, ADV NEURAL INFORM PR
   Chaudhuri Kamalika, 2011, J MACHINE LEARNING R
   Chen YR, 2018, INFORM SCIENCES, V451, P34, DOI 10.1016/j.ins.2018.03.061
   Damgard Ivan, 2009, INT WORKSH PUBL KEY
   Damgard Ivan, 2012, ADV CRYPTOLOGY CRYPT
   Doerner Jack, 2017, ABSENTMINDED CRYPTO
   Dwork C., 2008, INT C THEOR APPL MOD
   Dwork Cynthia, 2004, ADV CRYPTOLOGY CRYPT
   Dwork Cynthia, 2006, THEOR CRYPT C
   Dwork Cynthia, 2006, ANN INT C THEOR APPL
   Fredrikson Matthew, 2014, 23 USENIX SEC S
   Gascon Adria, 2017, Proceedings on Privacy Enhancing Technologies, V2017, P345, DOI 10.1515/popets-2017-0053
   Goldwasser Shafi, 1987, 19 ACM S THEOR COMP
   Gupta Trinabh, 2017, C ACM SPEC INT GROUP
   Heikkila Mikko, 2017, ARXIV170301106
   Hettich S., 1999, UCI MACHINE LEARNING
   Holzer Andreas, 2012, ACM C COMP COMM SEC
   Huang Y, 2012, P IEEE S SECUR PRIV, P272, DOI 10.1109/SP.2012.43
   Huang Yan, 2011, 20 USENIX SEC S
   Jain Prateek, 2012, 25 ANN C LEARN THEOR
   Jain Prateek, 2013, INT C MACH LEARN
   Lindell  Y., 2009, J PRIVACY CONFIDENTI
   Lindell Yehuda, 2000, ADV CRYPTOLOGY CRYPT
   Lindell Yehuda, 2007, ADV CRYPTOLOGY EUROC
   Ma Xu, 2018, INFORM SCI
   Malkhi Dahlia, 2004, USENIX SEC S
   Nikolaenko V, 2013, P IEEE S SECUR PRIV, P334, DOI 10.1109/SP.2013.30
   Orlandi Claudio, 2012, ADV CRYPTOLOGY CRYPT
   Parsa Ismail, 1998, UCI MACHINE LEARNING
   Pathak Manas, 2010, ADV NEURAL INFORM PR
   Pinkas Benny, 2009, INT C THEOR APPL CRY
   Rajkumar Arun, 2012, ARTIFICIAL INTELLIGE
   Ramage Daniel, 2018, 6 INT C LEARN REPR
   Rastogi Aseem, 2014, 35 IEEE S SEC PRIV
   Shi E, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/3146549
   Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41
   Shokri Reza, 2015, ACM C COMP COMM SEC
   Sridharan Karthik, 2009, ADV NEURAL INFORM PR
   Tian Lu, 2016, NIPS WORKSH PRIV MUL
   Vaidya Jaideep, 2008, VLDB J
   Wang Qian, 2018, IEEE T KNOWLEDGE DAT
   Wang X., 2016, EMP TOOLKIT EFFICIEN
   Wang Xiao, 2017, ACM SIGSAC C COMP CO
   Yang Zhiqiang, 2005, SIAM INT C DATA MINI
   Yao Andrew C, 1982, S FDN COMP SCI
   Ye Minwei, 2017, ADV NEURAL INFORM PR
   Zahur Samee, 2015, 20151153 CRYPT EPRIN
   Zhang Jiaqi, 2017, 26 INT JOINT C ART I
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000081
DA 2019-06-15
ER

PT S
AU Jean, N
   Xie, SM
   Ermon, S
AF Jean, Neal
   Xie, Sang Michael
   Ermon, Stefano
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by
   Minimizing Predictive Variance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID POSTERIOR REGULARIZATION
AB Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.
C1 [Jean, Neal; Xie, Sang Michael; Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Jean, N (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM nealjean@cs.stanford.edu; xie@cs.stanford.edu; ermon@cs.stanford.edu
FU NSF [1651565, 1522054, 1733686]; ONR; Sony; FLI; Department of Defense
   (DoD) through the National Defense Science & Engineering Graduate
   Fellowship (NDSEG) Program
FX This research was supported by NSF (#1651565, #1522054, #1733686), ONR,
   Sony, and FLI. NJ was supported by the Department of Defense (DoD)
   through the National Defense Science & Engineering Graduate Fellowship
   (NDSEG) Program. We are thankful to Volodymyr Kuleshov and Aditya Grover
   for helpful discussions.
CR Abadi M., 2016, ARXIV160304467
   Al-Shedivat Maruan, 2016, ARXIV161008936
   Arnold A., 2007, P 7 IEEE INT C DAT M
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chapelle O, 2005, P 10 INT WORKSH ART, P57
   Damianou Andreas C., 2013, J MACHINE LEARNING R
   Eissman Stephan, 2018, P 34 C UNC ART INT
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow IJ, 2014, ARXIV14126572
   Grandvalet Y., 2004, NIPS, V17, P529
   Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894
   Kingma D. P., 2013, ARXIV13126114
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kuleshov Volodymyr, 2017, P C UNC AI UAI
   Laine S., 2017, ICLR
   Lichman M., 2013, UCI MACHINE LEARNING
   Maaloe Lars, 2016, ARXIV160205473
   Miyato  T., 2015, ARXIV150700677
   Miyato  Takeru, 2017, ARXIV170403976
   Odena A, 2018, REALISTIC EVALUATION
   Oshri Barak, 2018, P 24 ACM SIGKDD C
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ren Russell, 2018, P 27 INT JOINT C ART
   Shu R., 2018, INT C LEARN REPR
   Shu Rui, 2018, NIPS
   Singh A., 2009, P ADV NEUR INF PROC, V21, P1513
   Tarvainen A, 2017, P ADV NEUR INF PROC, P1195
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson A. G., 2016, ADV NEURAL INFORM PR, P2586
   Wilson A. G, 2016, P 19 INT C ART INT S, P370
   Wilson Andrew Gordon, 2015, J MACHINE LEARNING R
   Xie M., 2016, AAAI C ART INT
   You J., 2017, P 31 AAAI C ART INT, P4559
   Yu Kai, 2006, INT C MACH LEARN ICM
   Zhao CY, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1342, DOI 10.1109/GlobalSIP.2015.7418417
   ZHOU Z, 2005, IJCAI 2005, P908
   Zhu J, 2014, J MACH LEARN RES, V15, P1799
   Zhu X., 2002, TECHNICAL REPORT
   Zhu X, 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI DOI 10.2200/S00196ED1V01Y200906AIM006
NR 41
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305035
DA 2019-06-15
ER

PT S
AU Jeon, W
   Seo, S
   Kim, KE
AF Jeon, Wonseok
   Seo, Seokin
   Kim, Kee-Eung
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Bayesian Approach to Generative Adversarial Imitation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach can robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions.
C1 [Jeon, Wonseok; Seo, Seokin; Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
   [Kim, Kee-Eung] PROWLER Io, Cambridge, England.
RP Jeon, W (reprint author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
EM wsjeon@ai.kaist.ac.kr; siseo@ai.kaist.ac.kr; kekim@cs.kaist.ac.kr
FU ICT R&D program of MSIT/IITP [2017-0-01778]; Ministry of Trade, Industry
   & Energy (MOTIE, Korea) [10063424]
FX This work was supported by the ICT R&D program of MSIT/IITP. (No.
   2017-0-01778, Development of Explainable Human-level Deep Machine
   Learning Inference Framework) and the Ministry of Trade, Industry &
   Energy (MOTIE, Korea) under Industrial Technology Innovation Program
   (No. 10063424, Development of Distant Speech Recognition and Multi-task
   Dialog Processing Technologies for In-door Conversational Robots).
CR Abadi M., 2016, ARXIV160304467
   Abdolmaleki Abbas, 2018, P 6 INT C LEARN REPR
   Bagnell J.A., 2015, TECHNICAL REPORT
   Choi Jaedeug, 2011, ADV NEURAL INFORM PR, P1989
   Dhariwal P., 2017, OPENAI BASELINES
   Finn C., 2016, ARXIV161103852
   Fu Justin, 2018, P 6 INT C LEARN REPR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Ho Jonathan, 2016, JMLR WORKSHOP C P, P2760
   Kim Kee- Eung, 2018, P 33 AAAI C ART INT
   Kingma D. P., 2014, ARXIV14126980
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Li Yunzhu, 2017, ADV NEURAL INFORM PR, V30, P3815
   Liu Qiang, 2016, INT C MACH LEARN, P276
   Liu Qiang, 2016, ADV NEURAL INFORM PR, V29, P2378
   Liu Y., 2017, ARXIV170402399
   Neu G., 2007, P 23 C UNC ART INT, P295
   Neumann G., 2011, P 28 INT C MACH LEAR, P817
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88
   Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586
   Ratliff ND, 2006, P 23 INT C MACH LEAR, P729
   Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964
   Saatci Yunus, 2017, ADV NEURAL INFO P SY, V30, P3622
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Toussaint M., 2009, P 26 ANN INT C MACH, P1049
   Wang Ziyu, 2017, ADV NEURAL INFORM PR, V30, P5326
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002002
DA 2019-06-15
ER

PT S
AU Jeon, Y
   Kim, J
AF Jeon, Yunho
   Kim, Junmo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Constructing Fast Network through Deconstruction of Convolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks. Code is available at https://github.com/jyh2986/Active-Shift.
C1 [Jeon, Yunho; Kim, Junmo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
RP Jeon, Y (reprint author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
EM jyh2986@kaist.ac.kr; junmo.kim@kaist.ac.kr
CR Chen  L-C., 2015, INT C LEARN REPR ICL
   Choi S, 2018, INT CONF IMAG VIS
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Han S., 2016, INT C LEARN REPR ICL
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Howard Andrew G., 2017, ARXIV170404861
   Hu J., 2018, IEEE C COMP VIS PATT
   Iandola Forrest N., 2016, ARXIV160207360
   Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Juefei-Xu  Felix, 2017, IEEE C COMP VIS PATT
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebedev  Vadim, 2014, INT C LEARN REPR ICL
   Li  Hao, 2017, INT C LEARN REPR ICL
   Sandler  Mark, 2018, IEEE C COMP VIS PATT
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C, 2016, ICLR 2016 WORKSH
   Szegedy C, 2015, IEEE C COMP VIS PATT
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wu  Bichen, 2018, IEEE C COMP VIS PATT
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yu  Fisher, 2016, INT C LEARN REPR ICL
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000045
DA 2019-06-15
ER

PT S
AU Jetley, S
   Lord, NA
   Torr, PHS
AF Jetley, Saumya
   Lord, Nicholas A.
   Torr, Philip H. S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI With Friends Like These, Who Needs Adversaries?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NETWORKS
AB The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks for image classification that shed new light on their behaviour and how it connects to the problem of adversaries. In short, the celebrated performance of these networks and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they use to achieve their classification performance in the first place. We develop this result in two main steps. The first uncovers the fact that classes tend to be associated with specific image-space directions. This is shown by an examination of the class-score outputs of nets as functions of 1D movements along these directions. This provides a novel perspective on the existence of universal adversarial perturbations. The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions. Thus, our analysis resolves the apparent contradiction between accuracy and vulnerability. It provides a new perspective on much of the prior art and reveals profound implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.(1)
C1 [Jetley, Saumya; Lord, Nicholas A.; Torr, Philip H. S.] Univ Oxford, Dept Engn Sci, Oxford, England.
   [Lord, Nicholas A.; Torr, Philip H. S.] FiveAI Ltd, Oxford Res Grp, Oxford, England.
RP Jetley, S (reprint author), Univ Oxford, Dept Engn Sci, Oxford, England.
EM sjetley@robots.ox.ac.uk; nicklord@robots.ox.ac.uk; phst@robots.ox.ac.uk
FU ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [Seebibyte EP/M013774/1];
   EPSRC/MURI [EP/N019474/1]
FX This work was supported by the ERC grant ERC-2012-AdG 321162-HELIOS,
   EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We
   would also like to acknowledge the Royal Academy of Engineering, FiveAI,
   and extend our thanks to Seyed-Mohsen Moosavi-Dezfooli for providing his
   research code for curvature analysis of decision boundaries of DCNs.
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   [Anonymous], 2017, 2017 COMP ADV ATT DE
   Dalal N, 2005, PROC CVPR IEEE, P886
   Das N, 2017, ABS170502900 CORR
   do Carmo M.P., 1976, DIFFERENTIAL GEOMETR
   Fawzi A, 2017, ARXIV170509552
   Fawzi A, 2017, IEEE SIGNAL PROC MAG, V34, P50, DOI 10.1109/MSP.2017.2740965
   Fawzi Alhussein, 2016, ADV NEURAL INFORM PR, V29, P1632
   Gao J, 2017, INT C LEARN REPR
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Lin M, 2013, INT C LEARN REPR
   LU JJ, 2017, P IEEE C COMP VIS PA, P446, DOI DOI 10.1109/ICCV.2017.56
   Madry A, 2018, INT C LEARN REPR
   Maharaj A. V., 2015, IMPROVING ADVERSARIA
   Metzen  J.H., 2017, ARXIV170204267
   Moosavi-Dezfooli S. M, 2018, INT C LEARN REPR
   Moosavi-Dezfooli S. M, 2016, P 2016 IEEE C COMP V
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabour S., 2016, INT C LEARN REPR
   Simonyan K, 2013, ARXIV13126034
   Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8
   Szegedy Christian, 2014, INT C LEARN REPR
   Tanay  T., 2016, ARXIV160807690
   Wang B, 2016, ARXIV161200334
   Xie C, 2017, ABS171101991 CORR
   Zhang H, 2018, INT C LEARN REPR
   Zhao Q, 2016, ABS160305145 CORR
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005034
DA 2019-06-15
ER

PT S
AU Ji, KY
   Liang, YB
AF Ji, Kaiyi
   Liang, Yingbin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Minimax Estimation of Neural Net Distance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice.
C1 [Ji, Kaiyi; Liang, Yingbin] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
RP Ji, KY (reprint author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
EM ji.367@osu.edu; liang.889@osu.edu
FU U.S. National Science Foundation [CCF-1801855]
FX The work was supported in part by U.S. National Science Foundation under
   the grant CCF-1801855.
CR Anthony  M., 2009, NEURAL NETWORK LEARN
   Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Arora S., 2017, P INT C MACH LEARN I
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6241
   Du S. S, 2018, ARXIV180301206
   Dziugaite G. K., 2015, UAI, P258
   Golowich N., 2018, P C LEARN THEOR COLT
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hsu Daniel, 2012, ELECT COMMUNICATIONS, V17
   Kontorovich A., 2014, P ICML, V2, P28
   Ledoux M., 1991, PROBABILITY BANACH S
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Liang Tengyuan, 2017, ARXIV171208244
   Liu Shuang, 2017, ADV NEURAL INFORM PR, P5551
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   MONTGOMERYSMITH SJ, 1990, P AM MATH SOC, V109, P517, DOI 10.2307/2048015
   Muandet K., 2012, ADV NEURAL INFORM PR, V25, P10
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Neyshabur B., 2017, ADV NEURAL INFORM PR, P5949
   Neyshabur B., 2015, COLT, P1376
   Neyshabur B., 2018, P INT C LEARN REPR I
   Oymak S., 2018, ARXIV180201223
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722
   Tolstikhin I., 2016, P INT C NEUR INF PRO, P1930
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang P., 2018, P INT C LEARN REPR I
   Zou S., 2015, IEEE T SIGNAL PROCES, V65, P5034
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303081
DA 2019-06-15
ER

PT S
AU Ji, Y
   Liang, L
   Deng, L
   Zhang, YY
   Zhang, YH
   Xie, Y
AF Ji, Yu
   Liang, Ling
   Deng, Lei
   Zhang, Youyang
   Zhang, Youhui
   Xie, Yuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI TETRIS: TilE-matching the TRemendous Irregular Sparsity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Compressing neural networks by pruning weights with small magnitudes can significantly reduce the computation and storage cost. Although pruning makes the model smaller, it is difficult to get a practical speedup in modern computing platforms such as CPU and GPU due to the irregularity. Structural pruning has attracted a lot of research interest to make sparsity hardware-friendly. Increasing the sparsity granularity can lead to better hardware utilization, but it will compromise the sparsity for maintaining accuracy.
   In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game(2), we cluster the irregularly distributed weights with small value into structured groups by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. The experiments also show ideal speedup, which is proportional to the sparsity, on GPU platforms. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off.
C1 [Ji, Yu; Zhang, Youyang; Zhang, Youhui] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
   [Ji, Yu; Zhang, Youhui] Beijing Innovat Ctr Future Chip, Beijing, Peoples R China.
   [Ji, Yu; Liang, Ling; Deng, Lei; Xie, Yuan] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
RP Zhang, YH (reprint author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.; Zhang, YH (reprint author), Beijing Innovat Ctr Future Chip, Beijing, Peoples R China.
EM jiy15@mails.tsinghua.edu.cn; lingliang@ece.ucsb.edu;
   leideng@ece.ucsb.edu; zhang-yy15@mails.tsinghua.edu.cn;
   zyh02@tsinghua.edu.cn; yuanxie@ece.ucsb.edu
FU Beijing Innovation Center for Future Chip; National Science Foundations
   (NSF) [1725447, 1730309]; Science and Technology Innovation Special Zone
   project
FX This research was collaborative work of Tsinghua University and
   University of California, Santa Barbara. Thanks for the support from
   Beijing Innovation Center for Future Chip, Science and Technology
   Innovation Special Zone project, and the National Science Foundations
   (NSF) under grant numbers 1725447 and 1730309. We also thank OpenAI for
   their open-source library, blocksparse.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Amodei D., 2016, INT C MACH LEARN, P173
   Ardakani A., 2016, ARXIV161101427
   Deng J., 2009, CVPR09
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Han S, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P75, DOI 10.1145/3020078.3021745
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Li Hao, 2016, ARXIV160808710
   Lin CY, 2018, ASIA S PACIF DES AUT, P105, DOI 10.1109/ASPDAC.2018.8297290
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Lu Y., 2017, P 2017 INT C COMP AR, P12
   Luo J.-H., 2017, ARXIV170706342
   Marcel S., 2010, P 18 ACM INT C MULT, P1485
   Narang S., 2017, ARXIV171102782
   Page A, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005448
   Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Redmon J., 2016, 1612 ARXIV
   Scott G., 2016, GPU KERNEL BLOCK SPA
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sun  Xu, 2017, ARXIV170606197
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Wen W., 2017, ARXIV170905027
   Wu Y., 2016, ARXIV160908144
   Xu C., 2017, CORR
   Yu JC, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P548, DOI 10.1145/3079856.3080215
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304015
DA 2019-06-15
ER

PT S
AU Jia, B
   Ray, S
   Safavi, S
   Bento, J
AF Jia, Bei
   Ray, Surjyendu
   Safavi, Sam
   Bento, Jose
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient Projection onto the Perfect Phylogeny Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE; HETEROGENEITY; ALGORITHM; SIMPLEX; HISTORY
AB Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms might do extensive searches over the space of possible trees. At the center of these algorithms is a projection problem that assigns a fitness cost to phylogenetic trees. In order to perform a wide search over the space of the trees, it is critical to solve this projection problem fast. In this paper, we use Moreau's decomposition for proximal operators, and a tree reduction scheme, to develop a new algorithm to compute this projection. Our algorithm terminates with an exact solution in a finite number of steps, and is extremely fast. In particular, it can search over all evolutionary trees with fewer than 11 nodes, a size relevant for several biological problems (more than 2 billion trees) in about 2 hours.
C1 [Jia, Bei; Ray, Surjyendu; Safavi, Sam; Bento, Jose] Boston Coll, Chestnut Hill, MA 02167 USA.
   [Jia, Bei] Element AI, Montreal, PQ, Canada.
RP Jia, B (reprint author), Boston Coll, Chestnut Hill, MA 02167 USA.; Jia, B (reprint author), Element AI, Montreal, PQ, Canada.
EM jiabe@bc.edu; raysc@bc.edu; safavisa@bc.edu; jose.bento@bc.edu
FU NVIDIA hardware grant [NIH/1U01AI124302, NSF/IIS-1741129]
FX This work was partially funded by NIH/1U01AI124302, NSF/IIS-1741129, and
   a NVIDIA hardware grant.
CR Bento J., 2013, ADV NEURAL INFORM PR, P521
   Bento Jose, 2015, AAAI, P3657
   Bonizzoni P., 2014, NATURAL COMPUTING SE, P67, DOI DOI 10.1007/978-3-642-40193-0_4.
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6
   Deshwar AG, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0602-8
   Ding ZH, 2006, J COMPUT BIOL, V13, P522, DOI 10.1089/cmb.2006.13.522
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   El-Kebir M, 2016, CELL SYST, V3, P43, DOI 10.1016/j.cels.2016.07.004
   El-Kebir M, 2015, BIOINFORMATICS, V31, P62, DOI 10.1093/bioinformatics/btv261
   El-Kebir Mohammed, 2016, ARXIV160402605
   Fernandez-Baca D, 2001, COMB OPT (SER), V11, P203
   Franca G, 2016, IEEE INT SYMP INFO, P2104, DOI 10.1109/ISIT.2016.7541670
   Franca Guilherme, 2017, ARXIV171000889
   Garey M. R., 2002, COMPUTERS INTRACTABI, V29
   Ghahramani  Z., 2010, NIPS, P19
   Gong PH, 2011, NEUROCOMPUTING, V74, P2754, DOI 10.1016/j.neucom.2011.02.019
   GUSFIELD D, 1991, NETWORKS, V21, P19, DOI 10.1002/net.3230210104
   Hajirasouliha I, 2014, BIOINFORMATICS, V30, P78, DOI 10.1093/bioinformatics/btu284
   Hao N, 2016, IEEE SYM PARA DISTR, P835, DOI 10.1109/IPDPSW.2016.162
   HUDSON RR, 1983, THEOR POPUL BIOL, V23, P183, DOI 10.1016/0040-5809(83)90013-8
   Jiang YC, 2016, P NATL ACAD SCI USA, V113, pE5528, DOI 10.1073/pnas.1522203113
   Jiao W, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-35
   KIMURA M, 1969, GENETICS, V61, P893
   Liu J., 2009, P 26 ANN INT C MACH, P657, DOI DOI 10.1145/1553374.1553459
   Malikic S, 2015, BIOINFORMATICS, V31, P1349, DOI 10.1093/bioinformatics/btv003
   Mathy Charles JM, SPARTA FAST GLOBAL P
   MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486
   MOREAU JJ, 1962, CR HEBD ACAD SCI, V255, P238
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Popic V, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0647-8
   Prufer H., 1918, ARCH MATH PHYS, V27, P742
   Satas G, 2017, BIOINFORMATICS, V33, pI152, DOI 10.1093/bioinformatics/btx270
   Schuh A, 2012, BLOOD, V120, P4191, DOI 10.1182/blood-2012-05-433540
   Yang Laurence, 2018, ARXIV180704245
   Zoran  D., 2014, ADVANCES IN NEURAL I, P226
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304014
DA 2019-06-15
ER

PT S
AU Jia, Y
   Zhang, Y
   Weiss, RJ
   Wang, Q
   Shen, J
   Ren, F
   Chen, ZF
   Nguyen, P
   Pang, RM
   Moreno, IL
   Wu, YH
AF Jia, Ye
   Zhang, Yu
   Weiss, Ron J.
   Wang, Quan
   Shen, Jonathan
   Ren, Fei
   Chen, Zhifeng
   Nguyen, Patrick
   Pang, Ruoming
   Moreno, Ignacio Lopez
   Wu, Yonghui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Transfer Learning from Speaker Verification to Multispeaker
   Text-To-Speech Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.
C1 [Jia, Ye; Zhang, Yu; Weiss, Ron J.; Wang, Quan; Shen, Jonathan; Ren, Fei; Chen, Zhifeng; Nguyen, Patrick; Pang, Ruoming; Moreno, Ignacio Lopez; Wu, Yonghui] Google Inc, Mountain View, CA 94043 USA.
RP Jia, Y (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM jiaye@google.com; ronw@google.com; ngyuzh@google.com
CR [Anonymous], 1996, ITUT REC P 800 METH
   Bahdanau D., 2015, P ICLR
   BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209
   Chen Yutian, 2018, ARXIV180910460
   Chung JS, 2018, INTERSPEECH, P1086
   Doddipatla R, 2017, INTERSPEECH, P3404, DOI 10.21437/Interspeech.2017-1038
   Gibiansky Andrew, 2017, ADV NEURAL INFORM PR, V30, P2962
   Heigold G, 2016, INT CONF ACOUST SPEE, P5115, DOI 10.1109/ICASSP.2016.7472652
   Nachmani  Eliya, 2018, ARXIV180206984
   Nagrani A., 2017, ARXIV170608612
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Ping W., 2018, P INT C LEARN REPR I
   Shen J., 2018, P IEEE INT C AC SPEE
   Skerry-Ryan R. J., 2018, ARXIV180309047
   Sotelo J., 2017, P INT C LEARN REPR I
   Taigman Yaniv, 2018, P INT C LEARN REPR I
   van den Oord A., 2016, ABS160903499 CORR
   Variani Ehsan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4052, DOI 10.1109/ICASSP.2014.6854363
   Veaux  C., 2017, CSTR VCTK CORPUS ENG
   Wan L, 2018, P ASME INT C OCEAN
   Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452
   Wang Yuxuan, 2018, ARXIV180309017
   Zhou Yanqi, 2018, ARXIV180206006
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304049
DA 2019-06-15
ER

PT S
AU Jiang, CH
   Xu, H
   Liang, XD
   Lin, L
AF Jiang, Chenhan
   Xu, Hang
   Liang, Xiaodan
   Lin, Liang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hybrid Knowledge Routed Modules for Large-scale Object Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP. Codes and trained model can be found in https://github.com/chanyn/HKRM.
C1 [Jiang, Chenhan; Lin, Liang] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China.
   [Xu, Hang] Huawei Noahs Ark Lab, Shenzhen, Peoples R China.
   [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
RP Liang, XD (reprint author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
EM jchcyan@gmail.com; xbjxh@live.com; xdliang328@gmail.com;
   linliang@ieee.org
FU National Key Research and Development Program of China [2018YFC0830103];
   National High Level Talents Special Support Plan (Ten Thousand Talents
   Program); National Natural Science Foundation of China (NSFC) [61622214,
   61836012]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant No. 2018YFC0830103, in part by
   National High Level Talents Special Support Plan (Ten Thousand Talents
   Program), and in part by National Natural Science Foundation of China
   (NSFC) under Grant No. 61622214, and 61836012.
CR Akata Z., 2013, CVPR
   Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X
   Cai Z., 2018, CVPR
   Chen X., 2017, ICCV
   Chen X., 2018, CVPR
   Dai B., 2017, CVPR
   Dai J., 2016, NIPS
   Deng J., 2014, ECCV
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Farhadi A., 2009, CVPR
   FELZENSZWALB PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI DOI 10.1109/TPAMI.2009.167
   Frome A., 2013, NIPS
   Galleguillos C., 2008, CVPR
   Garcia V., 2018, ICLR
   Gould S., 2009, ADV NEURAL INFORM PR, V22
   He K., 2016, CVPR
   Hoffman J., 2014, NIPS
   Hu H., 2018, CVPR
   Hu R., 2018, CVPR
   Jayaraman D., 2014, NIPS
   Kipf T. N., 2017, ICLR
   Krishna R., 2016, INT J COMPUTER VISIO
   Lampert C. H., 2009, CVPR
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li Y., 2016, ICLR
   Li Z., 2017, CVPR
   Lin T.-Y., 2014, ECCV
   Lin T.-Y, 2017, CVPR
   Liu W., 2016, ECCV
   Mao J., 2015, ICCV
   Marino K., 2017, CVPR
   Mensink T., 2012, COMPUTER VISION ECCV
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Misra I., 2017, CVPR
   Mottaghi R., 2014, CVPR
   Niepert M., 2016, INT C MACH LEARN, P2014
   Parikh D., 2011, ICCV
   Paszke A, 2017, NIPS WORKSH
   Redmon J., 2016, CVPR
   Reed S., 2016, CVPR
   Ren S., 2015, NIPS
   Rohrbach  Marcus, 2013, NIPS
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salakhutdinov R., 2011, CVPR
   Simonyan Karen, 2015, ICLR
   Springenberg J. T., 2015, ICLR WORKSH
   Torralba A., 2003, ICCV
   Torres AG, 2004, IEEE IND APPLIC SOC, P1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A., 2017, NIPS
   Wang X., 2018, CVPR
   Wang XH, 2018, ADV EXP MED BIOL, V1078, P3, DOI 10.1007/978-981-13-0950-2_1
   Wu Q., 2016, CVPR
   Yang J., 2017, FASTER PYTORCH IMPLE
   Zhou  Bolei, 2017, CVPR
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301053
DA 2019-06-15
ER

PT S
AU Jiang, F
   Yin, GS
   Francesca, D
AF Jiang, Fei
   Yin, Guosheng
   Francesca, Dominici
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bayesian Model Selection Approach to Boundary Detection with Non-Local
   Priors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CHANGE-POINT
AB Based on non-local prior distributions, we propose a Bayesian model selection (BMS) procedure for boundary detection in a sequence of data with multiple systematic mean changes. The BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. We speed up the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. Extensive simulation studies are conducted to compare the BMS with existing methods, and our approach is illustrated with application to the magnetic resonance imaging guided radiation therapy data.
C1 [Jiang, Fei; Yin, Guosheng] Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Peoples R China.
   [Francesca, Dominici] Harvard Univ, Harvard TH Chan Sch Publ Hlth, Cambridge, MA 02138 USA.
RP Jiang, F (reprint author), Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Peoples R China.
EM feijiang@hku.hk; gyin@hku.hk; fdominic@hsph.harvard.edu
FU Research Grants Council of Hong Kong [27304117, 17326316]
FX The authors would like to thank Dr. Zhou Shouhao from Department of
   Biostatistics, M.D. Anderson Cancer Center for providing the data. The
   research is partially supported by grants from the Research Grants
   Council of Hong Kong (grant number 27304117 for Jiang and 17326316 for
   Yin).
CR Baranowski R., 2016, ARXIV160900293
   Bertolino F, 2000, J ROY STAT SOC D-STA, V49, P503
   Conigliani C, 2000, CAN J STAT, V28, P343, DOI 10.2307/3315983
   De Santis F, 2001, J STAT PLAN INFER, V97, P305, DOI 10.1016/S0378-3758(00)00240-8
   Du C, 2016, J AM STAT ASSOC, V111, P314, DOI 10.1080/01621459.2015.1006365
   Eiauer P., 1978, TECHNOMETRICS, V20, P431
   Fryzlewicz P, 2014, ANN STAT, V42, P2243, DOI 10.1214/14-AOS1245
   Glaz J., 2001, SCAN STAT
   Jeffreys H, 1998, THEORY PROBABILITY
   Jiang F., 2018, BAYESIAN MODEL SELEC
   Johnson VE, 2010, J R STAT SOC B, V72, P143, DOI 10.1111/j.1467-9868.2009.00730.x
   Killick R, 2012, J AM STAT ASSOC, V107, P1590, DOI 10.1080/01621459.2012.737745
   Kirch C., 2014, PREPRINT
   Lavielle M, 2000, BERNOULLI, V6, P845, DOI 10.2307/3318759
   Niu YS, 2012, ANN APPL STAT, V6, P1306, DOI 10.1214/12-AOAS539
   Preuss P, 2015, J AM STAT ASSOC, V110, P654, DOI 10.1080/01621459.2014.920613
   Walker SG, 2004, STAT SCI, V19, P111, DOI 10.1214/088342304000000134
   YAO YC, 1987, ANN STAT, V15, P1321, DOI 10.1214/aos/1176350509
   Yau CY, 2016, J R STAT SOC B, V78, P895, DOI 10.1111/rssb.12139
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302002
DA 2019-06-15
ER

PT S
AU Jiang, H
   Kim, B
   Guan, MY
   Gupta, M
AF Jiang, Heinrich
   Kim, Been
   Guan, Melody Y.
   Gupta, Maya
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI To Trust Or Not To Trust A Classifier
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REJECT; CONFIDENCE
AB Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.
C1 [Jiang, Heinrich; Gupta, Maya] Google Res, Mento Pk, CA USA.
   [Kim, Been] Google Brain, Mountain View, CA USA.
   [Guan, Melody Y.] Stanford Univ, Stanford, CA 94305 USA.
RP Jiang, H (reprint author), Google Res, Mento Pk, CA USA.
EM heinrichj@google.com; beenkim@google.com; mguan@stanford.edu;
   mayagupta@google.com
CR Amodei Dario, 2016, CORR
   Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Balakrishnan Sivaraman, 2013, ADV NEURAL INFORM PR, P2679
   Bartlett PL, 2008, J MACH LEARN RES, V9, P1823
   CHAUDHURI K., 2010, ADV NEURAL INFORM PR, P343
   Chazal Frederic, 2013, UPPER BOUND VOLUME G
   CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406
   Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5
   Cortes Corinna, 2017, ARXIV170303478
   Cortes Corinna, 2016, ADV NEURAL INFORM PR, P1660
   Dasgupta Sanjoy, 2014, ADV NEURAL INFORM PR, P2555
   DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633
   DUBUISSON B, 1993, PATTERN RECOGN, V26, P155, DOI 10.1016/0031-3203(93)90097-G
   El-Yaniv R., 2011, ADV NEURAL INFORM PR, P1665
   El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Friedman J, 2001, ELEMENTS STAT LEARNI
   Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68
   Fumera G, 2000, LECT NOTES COMPUT SC, V1876, P863
   Gal Y., 2016, INT C MACH LEARN, P1050
   Genovese CR, 2012, J MACH LEARN RES, V13, P1263
   Goodfellow IJ, 2014, ARXIV14126572
   Grandvalet Y., 2009, ADV NEURAL INFORM PR, P537
   Guo C, 2017, ARXIV170604599
   HARTIGAN J. A., 1975, CLUSTERING ALGORITHM
   Hendrycks Dan, 2016, ARXIV161002136
   Herbei R, 2006, CAN J STAT, V34, P709, DOI 10.1002/cjs.5550340410
   Jiang Heinrich, 2017, INT C MACH LEARN, P1684
   Jiang Heinrich, 2017, INT C MACH LEARN, P1694
   Kendall A., 2017, ADV NEURAL INFORM PR, P5580, DOI DOI 10.1109/TDEI.2009.5211872
   Krizhevsky A., 2009, TECHNICAL REPORT
   Kuleshov Volodymyr, 2015, ADV NEURAL INFORM PR, P3474
   Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405
   Landgrebe TCW, 2006, PATTERN RECOGN LETT, V27, P908, DOI 10.1016/j.patrec.2005.10.015
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lee JD, 2004, HUM FACTORS, V46, P50, DOI 10.1518/hfes.46.1.50.30392
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Netzer Yuval, 2011, READING DIGITS NATUR
   Niculescu-Mizil A., 2005, P 22 INT C MACH LEAR, P625, DOI DOI 10.1145/1102351.1102430
   Niyogi P, 2008, DISCRETE COMPUT GEOM, V39, P419, DOI 10.1007/s00454-008-9053-2
   Papernot N, 2018, ARXIV180304765
   Parrish N, 2013, J MACH LEARN RES, V14, P3561
   Platt J, 1999, ADV LARGE MARGIN CLA, V10, P61
   Provost F., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P445
   Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184
   Rinaldo A, 2010, ANN STAT, V38, P2678, DOI 10.1214/10-AOS797
   Santos-Pereira CM, 2005, PATTERN RECOGN LETT, V26, P943, DOI 10.1016/j.patrec.2004.09.042
   Simonyan K, 2014, ARXIV14091556
   Singh A, 2009, ANN STAT, V37, P2760, DOI 10.1214/08-AOS661
   Tax DMJ, 2008, PATTERN RECOGN LETT, V29, P1565, DOI 10.1016/j.patrec.2008.03.010
   Tortorella F, 2000, LECT NOTES COMPUT SC, V1876, P611
   Tsybakov AB, 1997, ANN STAT, V25, P948, DOI 10.1214/aos/1069362732
   Varshney KR, 2017, BIG DATA-US, V5, P246, DOI 10.1089/big.2016.0051
   Wang Joseph, 2015, ADV NEURAL INFORM PR
   Wei Fan, 2002, AAAI
   Yuan M, 2010, J MACH LEARN RES, V11, P111
   Zadrozny B, 2002, P 8 ACM SIGKDD INT C, P694, DOI DOI 10.1007/S10994-013-5343-X
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000008
DA 2019-06-15
ER

PT S
AU Jiang, JC
   Lu, ZQ
AF Jiang, Jiechuan
   Lu, Zongqing
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Attentional Communication for Multi-Agent Cooperation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.
C1 [Jiang, Jiechuan; Lu, Zongqing] Peking Univ, Beijing, Peoples R China.
RP Lu, ZQ (reprint author), Peking Univ, Beijing, Peoples R China.
EM jiechuan.jiang@pku.edu.cn; zongqing.lu@pku.edu.cn
FU Peng Cheng Laboratory; NSFC [61872009]
FX This work was supported in part by Peng Cheng Laboratory and NSFC under
   grant 61872009.
CR Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061
   Celikyilmaz Asli, 2018, NAACL
   Cheney DL, 2005, LINGUIST REV, V22, P135, DOI 10.1515/tlir.2005.22.2-4.135
   Foerster Jakob, 2016, ADV NEURAL INFORM PR
   Foerster Jakob, 2018, AAAI C ART INT AAAI
   Gu S., 2017, IEEE INT C ROB AUT I
   Havrylov Serhii, 2017, ADV NEURAL INFORM PR
   Heess Nicolas, 2014, ADV NEURAL INFORM PR
   Kong Xiangyu, 2017, ARXIV171207305
   Lample Guillaume, 2017, AAAI C ART INT AAAI
   Lanctot Marc, 2017, ADV NEURAL INFORM PR
   Le Hoang M, 2017, INT C MACH LEARN ICM
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2016, INT C LEARN REPR ICL
   Lowe R., 2017, ADV NEURAL INFORM PR
   Matignon Laetitia, 2012, AAAI C ART INT AAAI
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mordatch Igor, 2018, AAAI C ART INT AAAI
   Peng P., 2017, ARXIV170310069
   Pipattanasomporn Manisa, 2009, IEEE PES POW SYST C
   Silver D., 2014, INT C MACH LEARN ICM
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Sukhbaatar Sainbayar, 2016, ADV NEURAL INFORM PR
   Yang Yaodong, 2018, INT C MACH LEARN ICM
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001078
DA 2019-06-15
ER

PT S
AU Jiang, N
   Kulesza, A
   Singh, S
AF Jiang, Nan
   Kulesza, Alex
   Singh, Satinder
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Completing State Representations using Spectral Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB A central problem in dynamical system modeling is state discovery-that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to mis-specification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.
C1 [Jiang, Nan] UIUC, Urbana, IL 61801 USA.
   [Kulesza, Alex] Google Res, New York, NY USA.
   [Singh, Satinder] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Jiang, N (reprint author), UIUC, Urbana, IL 61801 USA.
EM nanjiang@illinois.edu; kulesza@google.com; baveja@umich.edu
FU NSF [IIS 1319365]
FX This work was supported by NSF grant IIS 1319365. Any opinions,
   findings, conclusions, or recommendations expressed here are those of
   the authors and do not necessarily reflect the views of the sponsors.
CR BJORCK A, 1973, MATH COMPUT, V27, P579, DOI 10.2307/2005662
   Boots Byron, 2010, P 9 INT C AUT AG MUL, P1369
   Cassandra Anthony Rocco, 1998, EXACT APPROXIMATE AL
   Denis Francois, 2014, P 31 INT C MACH LEAR, P449
   Dheeru D., 2017, UCI MACHINE LEARNING
   Hefny A., 2015, ADV NEURAL INFORM PR, P1963
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   James MR, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P734
   James Michael R., 2004, P 21 INT C MACH LEAR, P53
   Jiang Nan, 2016, 30 AAAI C ART INT
   Kulesza Alex, 2015, P 29 AAAI C ART INT
   Kulesza Alex, 2015, ARTIF INTELL, P517
   Littman M. L., 2001, NIPS, P1555
   Liu YL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1259
   Oh  J., 2016, P 33 INT C MACH LEAR, P2790
   Ong Sylvie CW, 2013, AAAI
   Rosencrantz Matthew, 2004, P 21 INT C MACH LEAR, P88
   Shaban Amirreza, 2015, UAI, P792
   Siddiqi Sajid M., 2010, P 13 INT C ART INT S, P741
   Singh S., 2004, P 20 C UNC ART INT, P512
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304035
DA 2019-06-15
ER

PT S
AU Jiang, P
   Agrawal, G
AF Jiang, Peng
   Agrawal, Gagan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Linear Speedup Analysis of Distributed Deep Learning with Sparse and
   Quantized Communication
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. We show that O(1/root MK) convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the O(1/root pMK) convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only 3% - 5% communication data size.
C1 [Jiang, Peng; Agrawal, Gagan] Ohio State Univ, Columbus, OH 43210 USA.
RP Jiang, P (reprint author), Ohio State Univ, Columbus, OH 43210 USA.
EM jiang.952@osu.edu; agrawal@cse.ohio-state.edu
CR Abadi M., 2016, CORR
   Acero A., 1990, ACOUSTICAL ENV ROBUS
   Agarwal A., 2011, P ADV NEUR INF PROC, P873
   Aji A. F., 2017, CORR
   Alistarh D., 2017, ADV NEURAL INF PROCE, P1709
   Awan AA, 2017, ACM SIGPLAN NOTICES, V52, P193, DOI [10.1145/3155284.3018769, 10.1145/3018743.3018769]
   Chen C.-Y., 2017, CORR
   Chen Tianqi, 2015, CORR
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Collobert R., 2002, TORCH MODULAR MACHIN
   De C. M. Sa, 2015, ADV NEURAL INFORM PR, P2674
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Dinh L., 2017, P MACHINE LEARNING R, P1019
   Garg R., 2009, P 26 ANN INT C MACH, P337
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Gupta Suyog, 2015, CORR
   He K., 2015, CORR
   Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223
   Hoffer E., 2017, ADV NEURAL INFORM PR, P1731
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Keskar N. S., 2016, CORR
   Krizhevsky A., 2009, TECHNICAL REPORT
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li H., 2017, ADV NEURAL INFORM PR, P5811
   Li M, 2014, P 11 USENIX S OP SYS, P583
   Li M., 2016, SCALING DISTRIBUTED
   Li M, 2015, GPS SOLUT, V19, P27, DOI 10.1007/s10291-013-0362-4
   Lian X., 2015, ADV NEURAL INFORM PR, P2737
   Lian X., 2017, ADV NEURAL INFORM PR, P5330
   Lian  X., 2016, ADV NEURAL INFORM PR, P3054
   Lin Y., 2018, INT C LEARN REPR
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Povey  D., 2014, CORR
   Ruan JP, 2018, IEEE INT ULTRA SYM
   Seide  F., 2014, 1 BIT STOCHASTIC GRA
   Seide F, 2016, P 22 ACM SIGKDD INT, P2135, DOI DOI 10.1145/2939672.2945397
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smith S. L., 2018, INT C LEARN REPR
   Strom N., 2015, INTERSPEECH
   Su H., 2015, CORR
   Wang LN, 2018, ACM SIGPLAN NOTICES, V53, P41, DOI [10.1145/3200691.3178491, 10.1145/3178487.3178491]
   Wangni J., 2017, CORR
   Wen  W., 2017, P NIPS, P1509
   Zhang S, 2015, ONCOL TRANSLAT MED, V1, P15
   Zhou S., 2016, CORR
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302053
DA 2019-06-15
ER

PT S
AU Jiang, P
   Gu, FL
   Wang, YH
   Tu, CH
   Chen, BQ
AF Jiang, Peng
   Gu, Fanglin
   Wang, Yunhai
   Tu, Changhe
   Chen, Baoquan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI DifNet: Semantic Segmentation by Diffusion Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep Neural Networks (DNNs) have recently shown state of the art performance on semantic segmentation tasks, however, they still suffer from problems of poor boundary localization and spatial fragmented predictions. The difficulties lie in the requirement of making dense predictions from a long path model all at once, since details are hard to keep when data goes through deeper layers. Instead, in this work, we decompose this difficult task into two relative simple sub-tasks: seed detection which is required to predict initial predictions without the need of wholeness and preciseness, and similarity estimation which measures the possibility of any two nodes belong to the same class without the need of knowing which class they are. We use one branch network for one sub-task each, and apply a cascade of random walks base on hierarchical semantics to approximate a complex diffusion process which propagates seed information to the whole image according to the estimated similarities.
   The proposed DifNet consistently produces improvements over the baseline models with the same depth and with the equivalent number of parameters, and also achieves promising performance on Pascal VOC and Pascal Context dataset. Our DifNet is trained end-to-end without complex loss functions.
C1 [Jiang, Peng; Gu, Fanglin; Wang, Yunhai; Tu, Changhe; Chen, Baoquan] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Chen, Baoquan] Peking Univ, Beijing, Peoples R China.
RP Jiang, P (reprint author), Shandong Univ, Jinan, Shandong, Peoples R China.
EM sdujump@gmail.com; fanglin.gu@gmail.com; cloudseawang@gmail.com;
   chtu@sdu.edu.cn; baoquan.chen@gmail.com
FU National Natural Science Foundation of China [61702301, 61332015]; China
   Postdoctoral Science Foundation [2017M612272]; Fundamental Research
   Funds of Shandong University; National Basic Research grant (973)
   [2015CB352501]
FX This work was supported by the grants of National Natural Science
   Foundation of China (61702301), China Postdoctoral Science Foundation
   funded project (2017M612272), Fundamental Research Funds of Shandong
   University, National Natural Science Foundation of China (61332015) and
   National Basic Research grant (973) (2015CB352501).
CR Bertasius G., 2016, IEEE C COMP VIS PATT
   Bertasius G., 2017, IEEE C COMP VIS PATT
   Chandra S., 2017, IEEE INT C COMP VIS
   Chandra S., 2016, EUR C COMP VIS ECCV
   Chen L.C., 2017, ARXIV170605587
   Chen  L-C., 2015, INT C LEARN REPR ICL
   Chen LC, 2018, IEEE INT C ELECTR TA
   Everingham M., 2015, INT J COMPUTER VISIO
   Hariharan B., 2011, INT C COMP VIS ICCV
   Harley Adam W, 2017, IEEE INT C COMP VIS
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Huo YF, 2017, IEEE INT SYMP ELEC
   Jampani V., 2016, IEEE C COMP VIS PATT
   Jiang P., 2015, IEEE INT C COMP VIS
   Lin G., 2016, IEEE C COMP VIS PATT
   Liu S., 2017, ADV NEURAL INFORM PR
   Liu  W., 2015, ARXIV150604579
   Long Jonathan, 2015, IEEE C COMP VIS PATT
   Mottaghi R., 2014, IEEE C COMP VIS PATT
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Rutagemwa H, 2018, IEEE INT CONF COMM, DOI 10.1109/TVCG.2018.2834341
   Vemulapalli R., 2016, IEEE C COMP VIS PATT
   Wang X, 2018, IEEE INT CONF COMM
   Xie S., 2016, EUR C COMP VIS ECCV
   Xie XF, 2015, IEEE INT VAC ELECT C
   Zheng  S., 2015, IEEE INT C COMP VIS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301060
DA 2019-06-15
ER

PT S
AU Jiang, SL
   Malkomes, G
   Abbott, M
   Moseley, B
   Garnett, R
AF Jiang, Shali
   Malkomes, Gustavo
   Abbott, Matthew
   Moseley, Benjamin
   Garnett, Roman
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient nonmyopic batch active search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISCOVERY
AB Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In these settings, specialized instruments can often evaluate multiple points simultaneously; however, all existing work on active search focuses on sequential acquisition. We bridge this gap, addressing batch active search from both the theoretical and practical perspective. We first derive the Bayesian optimal policy for this problem, then prove a lower bound on the performance gap between sequential and batch optimal policies: the "cost of parallelization." We also propose novel, efficient batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can dramatically speed up computation. We conduct thorough experiments on data from three application domains: a citation network, material science, and drug discovery, testing all proposed policies with a wide range of batch sizes. Our results demonstrate that the empirical performance gap matches our theoretical bound, that nonmyopic policies usually significantly outperform myopic alternatives, and that diversity is an important consideration for batch policy design.
C1 [Jiang, Shali; Malkomes, Gustavo; Abbott, Matthew; Garnett, Roman] WUSTL, CSE, St Louis, MO 63130 USA.
   [Moseley, Benjamin] CMU, Tepper Sch Business, Pittsburgh, PA 15213 USA.
   [Moseley, Benjamin] Relational AI, Pittsburgh, PA 15213 USA.
RP Jiang, SL (reprint author), WUSTL, CSE, St Louis, MO 63130 USA.
EM jiang.s@wustl.edu; luizgustavo@wustl.edu; mbabbott@wustl.edu;
   moseleyb@andrew.cmu.edu; garnett@wustl.edu
FU National Science Foundation (NSF) [IIA-1355406]; Brazilian Federal
   Agency for Support and Evaluation of Graduate Education (CAPES); NSF
   [CNS-1560191, CCF-1830711, CCF-1824303, CCF-1733873]; Google Research
   Award
FX We would like to thank all the anonymous reviewers for valuable
   feedbacks. SJ, GM, and RG were supported by the National Science
   Foundation (NSF) under award number IIA-1355406. GM was also supported
   by the Brazilian Federal Agency for Support and Evaluation of Graduate
   Education (CAPES). MA was supported by NSF under award number
   CNS-1560191. BM was supported by a Google Research Award and by NSF
   under awards CCF-1830711, CCF-1824303, and CCF-1733873.
CR Asadpour A, 2008, LECT NOTES COMPUT SC, V5385, P477, DOI 10.1007/978-3-540-92185-1_53
   Chakraborty S, 2015, IEEE T NEUR NET LEAR, V26, P1747, DOI 10.1109/TNNLS.2014.2356470
   Chen Y., 2013, INT C MACH LEARN ICM, P160
   Cuong N. V., 2013, ADV NEURAL INFORM PR, V26, P1457
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Garnett  R., 2012, P 29 INT C MACH LEAR
   Garnett R, 2015, J COMPUT AID MOL DES, V29, P305, DOI 10.1007/s10822-015-9832-9
   Ginsbourger  D., 2008, TECHNICAL REPORT
   Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131
   Gonzalez  J., 2016, P 19 INT C ART INT S, P648
   Hoi S. C., 2006, P 23 INT C MACH LEAR, P417, DOI DOI 10.1145/1143844.1143897
   Jiang  S., 2017, P 34 INT C MACH LEAR, P1714
   Kushner H.J., 1964, Transactions of the ASME. Series D, Journal of Basic Engineering, V86, P97
   Ma  Y., 2015, P MACHINE LEARNING R, P672
   Ma  Y., 2015, UAI, P542
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Oglic  D., 2017, P 31 AAAI C ART INT, P2443
   Oglic D, 2018, MOL INFORM, V37, DOI 10.1002/minf.201700130
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559
   Sutherland Dougal J., 2013, P 19 ACM SIGKDD INT, P212
   Vanchinathan H. P., 2015, P 21 ACM SIGKDD INT, P1195
   Wang J.J., 2017, THESIS
   Wang  X., 2013, P 19 ACM SIGKDD INT, P731
   Warmuth MK, 2002, ADV NEUR IN, V14, P1449
   Warmuth MK, 2003, J CHEM INF COMP SCI, V43, P667, DOI 10.1021/ci025620t
   Williams K, 2015, J R SOC INTERFACE, V12, DOI 10.1098/rsif.2014.1289
   Wu  J., 2016, ADV NEURAL INFORM PR, P3126
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301012
DA 2019-06-15
ER

PT S
AU Jiao, JT
   Gao, WH
   Han, YJ
AF Jiao, Jiantao
   Gao, Weihao
   Han, Yanjun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Nearest Neighbor Information Estimator is Adaptively Near Minimax
   Rate-Optimal
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MUTUAL INFORMATION; INTEGRAL FUNCTIONALS; FEATURE-SELECTION; DENSITIES;
   ENTROPY
AB We analyze the Kozachenko-Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over Holder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the Holder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the Holder ball for s is an element of (0, 2] and arbitrary dimension d, rendering it the first estimator that provably satisfies this property.
C1 [Jiao, Jiantao] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Gao, Weihao] Univ Illinois, Dept ECE, Coordinated Sci Lab, Urbana, IL USA.
   [Han, Yanjun] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Jiao, JT (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM jiantao@berkeley.edu; wgao9@illinois.edu; yjhan@stanford.edu
CR BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Beirlant J., 1997, INT J MATH STAT SCI, V6, P17
   Berrett Thomas B, 2016, ARXIV160600304
   Biau G., 2015, LECT NEAREST NEIGHBO
   BICKEL PJ, 1988, SANKHYA SER A, V50, P381
   BIRGE L, 1995, ANN STAT, V23, P11, DOI 10.1214/aos/1176324452
   Bu YH, 2016, IEEE INT SYMP INFO, P1118, DOI 10.1109/ISIT.2016.7541473
   Cai TT, 2005, ANN STAT, V33, P2930, DOI 10.1214/009053605000000147
   Cai TT, 2003, ANN STAT, V31, P1140
   Chan C, 2015, P IEEE, V103, P1883, DOI 10.1109/JPROC.2015.2458316
   Delattre S, 2017, J STAT PLAN INFER, V185, P69, DOI 10.1016/j.jspi.2017.01.004
   Donoho D. L., 1990, Journal of Complexity, V6, P290, DOI 10.1016/0885-064X(90)90025-9
   EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462
   Emery M, 2000, LECT NOTES MATH, V1738, P5
   Evans LC, 2015, MEASURE THEORY FINE
   FAN JQ, 1991, ANN STAT, V19, P1273, DOI 10.1214/aos/1176348249
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Gao W., 2017, ADV NEURAL INFORM PR, P5988
   Gao W., 2016, ADV NEURAL INFORM PR, P2460
   Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732
   Gao Weihao, 2016, P 33 INT C MACH LEAR, P2780
   Ginc E, 2008, BERNOULLI, V14, P47, DOI 10.3150/07-BEJ110
   HALL P, 1993, ANN I STAT MATH, V45, P69, DOI 10.1007/BF00773669
   HALL P, 1987, STAT PROBABIL LETT, V6, P109, DOI 10.1016/0167-7152(87)90083-6
   HALL P, 1984, MATH PROC CAMBRIDGE, V96, P517, DOI 10.1017/S0305004100062459
   Han Yanjun, 2016, ARXIV160509124
   Han Yanjun, 2017, ARXIV171003863
   Han Yanjun, 2017, ARXIV171102141
   Hussein Fidah El Haje, 2009, J MATH SCI, V163, P290
   JOE H, 1989, ANN I STAT MATH, V41, P683, DOI 10.1007/BF00057735
   Kandasamy K., 2015, ADV NEURAL INFORM PR, P397
   Karunamuni R.J., 2005, STAT METHODOL, V2, P191, DOI DOI 10.1016/J.STAMET.2005.04.001
   Kerkyacharian G, 1996, ANN STAT, V24, P485
   Kozachenko L. F., 1987, PROBL PEREDACHI INF, V23, P9
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Krishnamurthy Akshay, 2014, P 31 INT C MACH LEAR, P919
   Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689
   Laurent B, 1996, ANN STAT, V24, P659
   Lepski O, 1999, PROBAB THEORY REL, V113, P221, DOI 10.1007/s004409970006
   Lepskii O. V., 1992, ADV SOVIET MATH, V12
   Levit Boris Ya, 1978, PROBL INFORM TRANSM, V14, P65
   Li Pan, 2017, ARXIV170901249
   MACK YP, 1979, J MULTIVARIATE ANAL, V9, P1, DOI 10.1016/0047-259X(79)90065-4
   Mukherjee Rajarshi, 2017, ARXIV170507577
   Mukherjee Rajarshi, 2015, ARXIV150800249
   Mukherjee Rajarshi, 2016, ARXIV160801364
   Muller A. C., 2012, INFORM THEORETIC CLU
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Robins J., 2008, PROBABILITY STAT ESS, P335, DOI DOI 10.1214/193940307000000527
   Robins James, 2016, ANN STAT
   Singh S., 2016, ADV NEURAL INFORM PR, P1217
   Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549
   Steeg G. Ver, 2014, STAT, V1050, P27
   Tchetgen E, 2008, STAT PROBABIL LETT, V78, P3307, DOI 10.1016/j.spl.2008.07.001
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Tsybakov AB, 1996, SCAND J STAT, V23, P75
   VANES B, 1992, SCAND J STAT, V19, P61
   Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303018
DA 2019-06-15
ER

PT S
AU Jin, C
   Liu, LT
   Ge, R
   Jordan, MI
AF Jin, Chi
   Liu, Lydia T.
   Ge, Rong
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On the Local Minima of the Empirical Risk
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OPTIMIZATION
AB Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex nonsmooth losses (such as modern deep networks), the population risk is generally significantly more well-behaved from an optimization point of view than the empirical risk. In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function F (population risk) given only access to an approximation f (empirical risk) that is pointwise close to F (i.e., parallel to F - f parallel to(infinity) <= nu). Our objective is to find the 6-approximate local minima of the underlying function F while avoiding the shallow local minima-arising because of the tolerance nu-which exist only in f. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of f that is guaranteed to achieve our goal as long as nu <= O(is an element of(1.5)/d). We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance nu among all algorithms making a polynomial number of queries of f. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.
C1 [Jin, Chi; Liu, Lydia T.; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Ge, Rong] Duke Univ, Durham, NC 27706 USA.
RP Jin, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM chijin@cs.berkeley.edu; lydiatliu@cs.berkeley.edu; rongge@cs.duke.edu;
   jordan@cs.berkeley.edu
CR Agarwal Alekh, 2010, P 23 ANN C LEARN THE
   Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464
   Anandkumar A, 2016, C LEARN THEOR, P81
   Auer P, 1996, ADV NEUR IN, V8, P316
   Bartlett Peter L., 2003, J MACH LEARN RES, V3
   Belloni Alexandre, 2015, P 28 C LEARN THEOR, P240
   Boucheron S., 2013, CONCENTRATION INEQUA
   Brutzkus A., 2017, P 34 INT C MACH LEAR, P605
   Carmon Yair, 2016, ARXIV161100756
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Dinh L., 2017, ARXIV170304933
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Ge Rong, 2015, P 28 C LEARN THEOR C
   Jin C., 2017, INT C MACH LEARN, V70, P1724
   Jin Chi, 2017, CORR
   Jin Chi, 2018, COMMUNICATION
   Keskar N. S., 2016, ARXIV160904836
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Kleinberg Robert, 2018, CORR
   Loh P.-L., 2013, ADV NEURAL INFORM PR, P476
   Mei Song, 2016, ARXIV160706534
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2004, INTRO LECT CONVEX PR
   Rechenberg I., 1973, EVOLUTIONSSTRATEGIE
   Risteski Andrej, 2016, ADV NEURAL INFORM PR, P4745
   Shamir Ohad, 2013, P 26 ANN C LEARN THE, V30
   Singer Yaron, 2015, P 28 ANN C NEUR INF, P3204
   Sinha  A., 2018, INT C LEARN REPR
   Steinhardt Jacob, 2017, ADV NEURAL INFORM PR
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Zhang  Y., 2017, COLT, P1980
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304087
DA 2019-06-15
ER

PT S
AU Jin, C
   Allen-Zhu, Z
   Bubeck, S
   Jordan, MI
AF Jin, Chi
   Allen-Zhu, Zeyuan
   Bubeck, Sebastien
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Is Q-learning Provably Efficient?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BOUNDS
AB Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [7, 22]. The theoretical question of "whether model-free algorithms can be made sample efficient" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret (O) over tilde(root H(3)SAT), where S and A are the numbers of states and actions, H is the number of steps per episode, and T is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single root H factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes root T regret without requiring access to a "simulator."
C1 [Jin, Chi; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Allen-Zhu, Zeyuan; Bubeck, Sebastien] Microsoft Res, Redmond, WA USA.
RP Jin, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM chijin@cs.berkeley.edu; zeyuan@csail.mit.edu; sebubeck@microsoft.com;
   jordan@cs.berkeley.edu
FU DARPA program on Lifelong Learning Machines; Microsoft Research Gratis
   Traveler program
FX We thank Nan Jiang, Sham M. Kakade, Greg Yang and Chicheng Zhang for
   valuable discussions. This work was supported in part by the DARPA
   program on Lifelong Learning Machines, and Microsoft Research Gratis
   Traveler program.
CR Agrawal Shipra, 2017, NIPS, P1184
   Azar Mohammad, 2017, P 34 INT C MACH LEAR, P263
   Azar Mohammad, 2011, C NEUR INF PROC SYST, P2411
   Azar Mohammad, 2012, P 29 INT C MACH LEAR
   Azar MG, 2013, MACH LEARN, V91, P325, DOI 10.1007/s10994-013-5368-1
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Even-Dar E, 2003, J MACH LEARN RES, V5, P1
   Fazel M, 2018, ARXIV180105039
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Jiang Nan, 2016, ARXIV161009512
   Kakade Sham, 2018, ABS180209184 ARXIV
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   KOENIG S, 1993, PROCEEDINGS OF THE ELEVENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P99
   Lattimore Tor, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P320, DOI 10.1007/978-3-642-34106-9_26
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Nagabandi  A., 2017, ARXIV170802596
   Osband Ian, 2014, ARXIV14020635
   Osband Ian, 2016, ABS160802732 ARXIV
   Pong  V., 2018, ARXIV180209081
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sidford Aaron, 2018, P 29 ANN ACM SIAM S, P770
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Strehl A., 2006, P 23 INT C MACH LEAR, P881, DOI DOI 10.1145/1143844.1143955
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Watkins C. J. C. H., 1989, THESIS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304084
DA 2019-06-15
ER

PT S
AU Jin, YYZ
   Zhang, WR
   Li, P
AF Jin, Yingyezhe
   Zhang, Wenrui
   Li, Peng
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking
   Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels. Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [14] and dynamic neuromorphic N-MNIST [26]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [3] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [16], a challenging spatio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.
C1 [Jin, Yingyezhe; Zhang, Wenrui; Li, Peng] Texas A&M Univ, College Stn, TX 77843 USA.
RP Jin, YYZ (reprint author), Texas A&M Univ, College Stn, TX 77843 USA.
EM jyyz@tamu.edu; zhangwenrui@tamu.edu; pli@tamu.edu
FU National Science Foundation [CCF-1639995]; Semiconductor Research
   Corporation (SRC) [2692.001]
FX This material is based upon work supported by the National Science
   Foundation under Grant No.CCF-1639995 and the Semiconductor Research
   Corporation (SRC) under Task 2692.001. The authors would like to thank
   High Performance Research Computing (HPRC) at Texas A&M University for
   providing computing support. Any opinions, findings, conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of NSF, SRC, Texas A&M University,
   and their contractors.
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cohen Gregory, 2017, ARXIV170205373 EMNIS
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   DIEHL PU, 2015, IEEE IJCNN
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hunsberger Eric, 2015, ARXIV151008829
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Liberman Mark, TI46 SPEECH CORPUS
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D., 2016, ADV NEURAL INFORM PR, P3882
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   Welling M., 2016, ARXIV160208323
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wu Yujie, 2017, ARXIV170602609
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001054
DA 2019-06-15
ER

PT S
AU Jitkrittum, W
   Kanagawa, H
   Sangkloy, P
   Hays, J
   Scholkopf, B
   Gretton, A
AF Jitkrittum, Wittawat
   Kanagawa, Heishiro
   Sangkloy, Patsorn
   Hays, James
   Schoelkopf, Bernhard
   Gretton, Arthur
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Informative Features for Model Comparison
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISTRIBUTIONS
AB Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.
C1 [Jitkrittum, Wittawat; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
   [Kanagawa, Heishiro; Gretton, Arthur] UCL, Gatsby Unit, London, England.
   [Sangkloy, Patsorn; Hays, James] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Jitkrittum, W (reprint author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.
EM wittawat@tuebingen.mpg.de; heishirok@gatsby.ucl.ac.uk;
   patsorn_sangkloy@gatech.edu; hays@gatech.edu;
   bernhard.schoelkopf@tuebingen.mpg.de; arthur.gretton@gmail.com
FU Gatsby Charitable Foundation
FX HK and AG thank the Gatsby Charitable Foundation for the financial
   support.
CR Amos B., 2016, TECHNICAL REPORT
   Arjovsky  M., 2017, ICML
   Baringhaus L., 1988, METRIKA, V35, P339, DOI [10.1007/BF02613322, DOI 10.1007/BF02613322]
   Binkowski M., 2018, ICLR
   Bounliphone W., 2015, ICLR
   BOX GEP, 1976, J AM STAT ASSOC, V71, P791, DOI 10.2307/2286841
   Casella G., 2002, STAT INFERENCE, V2
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chwialkowski K., 2015, P ADV NEUR INF PROC, P1972
   Chwialkowski K., 2016, P INT C MACH LEARN, P2606
   Fernandez VA, 2008, COMPUT STAT DATA AN, V52, P3730, DOI 10.1016/j.csda.2007.12.013
   FRIEDMAN JH, 1979, ANN STAT, V7, P697, DOI 10.1214/aos/1176344722
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gorham J., 2015, ADV NEURAL INFORM PR, P226
   Gretton  A., 2012, ADV NEURAL INFORM PR, P1205
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gulrajani I., 2017, P ADV NEUR INF PROC, P5767
   Hall P, 2002, BIOMETRIKA, V89, P359, DOI 10.1093/biomet/89.2.359
   Harchaoui Z., 2008, TESTING HOMOGENEITY, P609
   Heusel M., 2017, NIPS
   HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196
   Jitkrittum W., 2017, ICML
   Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181
   Jitkrittum W., 2017, NIPS
   Kingma D. P., 2014, ARXIV E PRINTS
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lintusaari J, 2017, SYST BIOL, V66, pE66, DOI 10.1093/sysbio/syw077
   Liu Qiang, 2016, INT C MACH LEARN, P276
   Liu  Z., 2015, P INT C COMP VIS ICC
   Lloyd JR, 2015, ADV NEURAL INFORM PR, P829
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nowozin Sebastian, 2016, NIPS
   Radford A., 2015, ARXIV151106434
   Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x
   Salimans T., 2016, ARXIV E PRINTS
   Serfling RJ, 2009, APPROXIMATION THEORE
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Srivastava A., 2017, ARXIV E PRINTS
   Sutherland D. J., 2016, ICLR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szekely GJ, 2005, J MULTIVARIATE ANAL, V93, P58, DOI 10.1016/j.jmva.2003.12.002
   Yamada M., 2018, ARXIV180206226
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300075
DA 2019-06-15
ER

PT S
AU Johnson, DD
   Gorelik, D
   Mawhorter, R
   Suver, K
   Gu, WQ
   Xing, S
   Gabriel, C
   Sankhagowit, P
AF Johnson, Daniel D.
   Gorelik, Daniel
   Mawhorter, Ross
   Suver, Kyle
   Gu, Weiqing
   Xing, Steven
   Gabriel, Cody
   Sankhagowit, Peter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Latent Gaussian Activity Propagation: Using Smoothness and Structure to
   Separate and Localize Sounds in Large Noisy Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BLIND SEPARATION; SIGNALS
AB We present an approach for simultaneously separating and localizing multiple sound sources using recorded microphone data. Inspired by topic models, our approach is based on a probabilistic model of inter-microphone phase differences, and poses separation and localization as a Bayesian inference problem. We assume sound activity is locally smooth across time, frequency, and location, and use the known position of the microphones to obtain a consistent separation. We compare the performance of our method against existing algorithms on simulated anechoic voice data and find that it obtains high performance across a variety of input conditions.
C1 [Johnson, Daniel D.; Gorelik, Daniel; Mawhorter, Ross; Suver, Kyle; Gu, Weiqing] Harvey Mudd Coll, Dept Math, Claremont, CA 91711 USA.
   [Xing, Steven; Gabriel, Cody; Sankhagowit, Peter] Intel Corp, Hillsboro, OR 97124 USA.
RP Johnson, DD (reprint author), Harvey Mudd Coll, Dept Math, Claremont, CA 91711 USA.
EM ddjohnson@hmc.edu; dgorelik@hmc.edu; rmawhorter@hmc.edu; ksuver@hmc.edu;
   gu@hmc.edu; steven.xing@intel.com; cody.gabriel@intel.com;
   peter.sankhagowit@intel.com
CR Aarabi P, 2002, IEEE T SYST MAN CY C, V32, P474, DOI 10.1109/TSMCB.2002.804369
   Bagchi D, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P496, DOI 10.1109/ASRU.2015.7404836
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Dietz Laura, 2010, TECH REP
   Dorfan Y, 2015, EUR SIGNAL PR CONF, P1256, DOI 10.1109/EUSIPCO.2015.7362585
   Emiya V, 2011, IEEE T AUDIO SPEECH, V19, P2046, DOI 10.1109/TASL.2011.2109381
   Habets EAP, 2007, J ACOUST SOC AM, V122, P3464, DOI 10.1121/1.2799929
   Hyvarinen A., 2004, INDEPENDENT COMPONEN, V46
   Jourjine A, 2000, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.2000.861162
   Lewis Jerad, 2012, ANALOG DEVICES
   Mandel MI, 2015, EUR SIGNAL PR CONF, P2028, DOI 10.1109/EUSIPCO.2015.7362740
   Mandel Michael I, 2009, IEEE T AUDIO SPEECH, V17
   Oldfield R, 2015, MULTIMED TOOLS APPL, V74, P2717, DOI 10.1007/s11042-013-1472-2
   Rickard S, 2007, SIGNALS COMMUN TECHN, P217, DOI 10.1007/978-1-4020-6479-1_8
   Srinivasan S, 2006, SPEECH COMMUN, V48, P1486, DOI 10.1016/j.specom.2006.09.003
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253
   Wang DeLiang, 2017, ARXIV170807524
   Yeredor Arie, 2001, P ICA 01 SAN DIEG, P522
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303046
DA 2019-06-15
ER

PT S
AU Johnson, TB
   Guestrin, C
AF Johnson, Tyler B.
   Guestrin, Carlos
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Training Deep Models Faster with Robust, Approximate Importance Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure (RAIS) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, RAIS provides much of the benefit of exact importance sampling with drastically reduced overhead. Empirically, we find RAIS-SGD and standard SGD follow similar learning curves, but RAIS moves faster through these paths, achieving speed-ups of at least 20% and sometimes much more.
C1 [Johnson, Tyler B.; Guestrin, Carlos] Univ Washington, Seattle, WA 98195 USA.
RP Johnson, TB (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM tbjohns@washington.edu; guestrin@cs.washington.edu
FU PECASE [N00014-13-I -0023]
FX We thank Marco Tulio Ribeiro, Tianqi Chen, Maryam Fazel, Sham Kakade,
   and Ali Shojaie for helpful discussion and feedback. This work was
   supported by PECASE N00014-13-I -0023.
CR Alain G., 2016, 4 INT C LEARN REPR W
   Bengio Y., 2009, P 26 INT C MACH LEAR
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Borsos Z., 2018, ARXIV180204715
   Cohen T., 2016, P 33 INT C MACH LEAR
   Gopal S., 2016, P 33 INT C MACH LEAR
   He K., 2016, EUR C COMP VIS
   Horgan  Daniel, 2018, 6 INT C LEARN REPR
   Ioffe S, 2015, 32 INT C MACH LEARN
   Katharopoulos A., 2018, P 35 INT C MACH LEAR
   Katharopoulos A., 2017, ARXIV170600043
   Krizhevsky A., 2009, TECHNICAL REPORT
   Larochelle H., 2007, P 24 INT C MACH LEAR
   Lecun Y., 1998, P IEEE
   Ma P., 2014, P 31 INT C MACH LEAR
   Mahoney M., 2011, FDN TRENDS MACHINE L, V3
   McCallum A., 2017, ADV NEURAL INFORM PR, P30
   Needell D, 2014, ADV NEUR IN, V27
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Schaul T., 2016, 6 INT C LEARN REPR
   Shalev-Shwartz S., 2016, P 33 INT C MACH LEAR
   Shrivastava A., 2016, P IEEE C COMP VIS PA
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stich S. U., 2017, ADV NEURAL INFORM PR, P30
   Sutskever I., 2013, P 30 INT C MACH LEAR
   Zhang C., 2017, C UNC ART INT
   Zhang C., 2018, ARXIV180402772
   Zhao P., 2015, P 32 INT C MACH LEAR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001079
DA 2019-06-15
ER

PT S
AU Joseph, M
   Roth, A
   Ullman, J
   Waggoner, B
AF Joseph, Matthew
   Roth, Aaron
   Ullman, Jonathan
   Waggoner, Bo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Local Differential Privacy for Evolving Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the "local model" of differential privacy that these systems use.
   In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common ( but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.
C1 [Joseph, Matthew; Roth, Aaron; Waggoner, Bo] Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.
   [Ullman, Jonathan] Northeastern Univ, Comp & Informat Sci, Boston, MA 02115 USA.
RP Joseph, M (reprint author), Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.
EM majos@cis.upenn.edu; aaroth@cis.upenn.edu; jullman@ccs.neu.edu;
   bowaggoner@gmail.com
CR Abowd John M., 2016, CHALLENGE SCI REPROD
   Bassily R., 2015, P 47 ANN ACM S THEOR, P127, DOI DOI 10.1145/2746539.2746632
   Bassily Raef, 2014, ARXIV14057085
   Bassily Raef, 2017, NIPS, P2285
   Bittau Andrea, 2017, ARXIV171000901
   Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148
   Bun Mark, 2017, ARXIV171104740
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Differential Privacy Team Apple, 2017, TECHNICAL REPORT
   Ding BL, 2017, ADV NEUR IN, V30
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   Dwork C, 2009, ACM S THEORY COMPUT, P381
   Erlingsson Ulfar, 2014, P 2014 ACM SIGSAC C, P1054, DOI DOI 10.1145/2660267.2660348
   Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85
   Hsu J, 2012, LECT NOTES COMPUT SC, V7391, P461, DOI 10.1007/978-3-642-31594-7_39
   Kasiviswanathan SP, 2008, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2008.27
   Ligett Katrina, 2017, ADV NEURAL INFORM PR, P2563
   Mishra N., 2006, P 25 ACM SIGMOD SIGA, P143, DOI DOI 10.1145/1142351.1142373
   Rogers Ryan M, 2016, ADV NEURAL INFORM PR, P1921
   Roth A, 2010, ACM S THEORY COMPUT, P765
   Tang J., 2017, ARXIV170902753
   Ullman Jonathan, 2018, TIGHT LOWER BO UNPUB
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302039
DA 2019-06-15
ER

PT S
AU Josz, C
   Ouyang, Y
   Zhang, RY
   Lavaei, J
   Sojoudi, S
AF Josz, C.
   Ouyang, Y.
   Zhang, R. Y.
   Lavaei, J.
   Sojoudi, S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A theory on the absence of spurious solutions for nonconvex and
   nonsmooth optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of nonconvex and nonsmooth optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used l(1) norm to avoid outliers in nonconvex optimization.
C1 [Josz, C.; Sojoudi, S.] Univ Calif Berkeley, Berkeley, CA USA.
   [Ouyang, Y.; Zhang, R. Y.; Lavaei, J.] Univ Calif Berkeley, IEOR, Berkeley, CA USA.
RP Josz, C (reprint author), Univ Calif Berkeley, Berkeley, CA USA.
EM cedric.josz@gmail.com; ouyangyii@gmail.com; ryz@berkeley.edu;
   lavaei@berkeley.edu; sojoudi@berkeley.edu
FU ONR [N00014-17-1-2933 ONR, N00014-18-1-2526]; NSF Award [1808859]; DARPA
   [D16AP00002]; AFOSR Award [FA9550- 17-1-0163]
FX This work was supported by the ONR Awards N00014-17-1-2933 ONR and
   N00014-18-1-2526, NSF Award 1808859, DARPA Award D16AP00002, and AFOSR
   Award FA9550- 17-1-0163. We wish to thank the anonymous reviewers for
   their valuable feedback, as well as Chris Dock for fruitful discussions.
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302045
DA 2019-06-15
ER

PT S
AU Jothimurugesan, E
   Tahmasbi, A
   Gibbons, PB
   Tirthapura, S
AF Jothimurugesan, Ellango
   Tahmasbi, Ashraf
   Gibbons, Phillip B.
   Tirthapura, Srikanta
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Variance-Reduced Stochastic Gradient Descent on Streaming Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present an algorithm STRSAGA that can efficiently maintain a machine learning model over data points that arrive over time, and quickly update the model as new training data are observed. We present a competitive analysis that compares the sub-optimality of the model maintained by STRSAGA with that of an offline algorithm that is given the entire data beforehand. Our theoretical and experimental results show that the risk of STRSAGA is comparable to that of an offline algorithm on a variety of input arrival patterns, and its experimental performance is significantly better than prior algorithms suited for streaming data, such as SGD and SSVRG.
C1 [Jothimurugesan, Ellango; Gibbons, Phillip B.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Tahmasbi, Ashraf; Tirthapura, Srikanta] Iowa State Univ, Ames, IA USA.
RP Jothimurugesan, E (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ejothimu@cs.cmu.edu; tahmasbi@iastate.edu; gibbons@cs.cmu.edu;
   snt@iastate.edu
FU NSF [1527541, 1725702, 1725663]
FX Supported in part by NSF grant 1725663; Supported in part by NSF grants
   1527541 and 1725702
CR Bercu B., 2015, CONCENTRATION INEQUA
   Bertsekas D. P., 2016, NONLINEAR PROGRAMMIN
   Bottou L., 2007, ADV NEURAL INFORM PR, V20, P161
   Bottou L., 2003, NIPS, P217
   Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Dua D., 2017, UCI MACHINE LEARNING
   Frostig R., 2015, P C LEARN THEOR PAR, P728
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Konecny J., 2013, ARXIV13121666
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2
   Mitzenmacher M., 2017, PROBABILITY COMPUTIN
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shah V., 2016, ARXIV160306861
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004046
DA 2019-06-15
ER

PT S
AU Jun, KS
   Li, LH
   Ma, YZ
   Zhu, XJ
AF Jun, Kwang-Sung
   Li, Lihong
   Ma, Yuzhe
   Zhu, Xiaojin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial Attacks on Stochastic Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm. We propose the first attack against two popular bandit algorithms: 6-greedy and UCB, without knowledge of the mean rewards. The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack. The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment. As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat.
C1 [Jun, Kwang-Sung] Boston Univ, Boston, MA 02215 USA.
   [Li, Lihong] Google Brain, Mountain View, CA USA.
   [Ma, Yuzhe; Zhu, Xiaojin] UW Madison, Madison, WI USA.
RP Jun, KS (reprint author), Boston Univ, Boston, MA 02215 USA.
EM kwangsung.jun@gmail.com; lihong@google.com; ma234@wisc.edu;
   jerryzhu@cs.wisc.edu
FU NSF [1837132, 1545481, 1704117, 1623605, 1561512]; MADLab AF Center of
   Excellence [FA9550-18-1-0166]
FX This work is supported in part by NSF 1837132, 1545481, 1704117,
   1623605, 1561512, and the MADLab AF Center of Excellence
   FA9550-18-1-0166.
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Agarwal  A., 2014, P 31 INT C MACH LEAR, P1638
   Agarwal Alekh, 2016, CORR
   Agrawal S., 2012, C LEARN THEOR, V23, P391
   Agrawal S., 2013, P 30 INT C MACH LEAR, P127
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chapelle O, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532128
   Dorigo Marco, 1997, ROBOT SHAPING EXPT B
   Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Greenewald Kristjan, 2017, ADV NEURAL INFORM PR, P5979
   Huang Sandy, 2017, ARXIV170202284
   Joseph Anthony D., 2018, ADVERSARIAL MACHINE
   Kuleshov Volodymyr, 2014, CORR
   Kveton B, 2015, P 32 INT C MACH LEAR, P767
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Lin Y.-C., 2017, P 26 INT JOINT C ART, P3756
   Lykouris T, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P114, DOI 10.1145/3188745.3188918
   Ma Yuzhe, 2018, C DEC GAM THEOR SEC
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Zhu Xiaojin, 2018, ARXIV181104422
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303062
DA 2019-06-15
ER

PT S
AU Kadmon, J
   Ganguli, S
AF Kadmon, Jonathan
   Ganguli, Surya
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Statistical mechanics of low-rank tensor decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE; MODEL
AB Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise.
C1 [Kadmon, Jonathan; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
   [Ganguli, Surya] Google Brain, Mountain View, CA USA.
RP Kadmon, J (reprint author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
EM kadmonj@stanford.edu; sganguli@stanford.edu
FU Center for Theory of Deep Learning at the Hebrew University; Simons
   Foundation; Office of Naval Research; National Institutes of Health;
   Burroughs-Wellcome Foundation; McKnight Foundation; James S. McDonnell
   Foundation
FX We thank Alex Williams for useful discussions. We thank the Center for
   Theory of Deep Learning at the Hebrew University (J.K), and the
   Burroughs-Wellcome, McKnight, James S. McDonnell, and Simons
   Foundations, and the Office of Naval Research and the National
   Institutes of Health (S.G) for support.
CR Acar E, 2007, BIOINFORMATICS, V23, pI10, DOI 10.1093/bioinformatics/btm210
   Advani M, 2016, PHYS REV X, V6, DOI 10.1103/PhysRevX.6.031034
   Advani M, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03014
   Advani Madhu, 2016, P ADV NEUR INF PROC, P3378
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Barbier J, 2017, ANN ALLERTON CONF, P1056, DOI 10.1109/ALLERTON.2017.8262854
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Bethe H.A., 1935, Proceedings of the Royal Society of London, Series A (Mathematical and Physical Sciences), V150, P552, DOI 10.1098/rspa.1935.0122
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   CRISANTI A, 1992, Z PHYS B CON MAT, V87, P341, DOI 10.1007/BF01309287
   CRISANTI A, 1995, J PHYS I, V5, P805, DOI 10.1051/jp1:1995164
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Ganguli S., 2010, ADV NEURAL INFORM PR, P667
   Ganguli S, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.188701
   Harshman R. A., 1970, UCLA WORKING PAPERS
   Hunyadi Borbala, 2017, WILEY INTERDISCIPLIN, V7
   Kabashima Y, 2004, LECT NOTES ARTIF INT, V3244, P479
   Kabashima Y, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/09/L09003
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Lesieur T, 2017, IEEE INT SYMP INFO, P511, DOI 10.1109/ISIT.2017.8006580
   Lesieur T, 2017, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/aa7284
   Lesieur T, 2015, ANN ALLERTON CONF, P680, DOI 10.1109/ALLERTON.2015.7447070
   Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733
   MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018
   MEZARD M, 1985, J PHYS LETT-PARIS, V46, pL771
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Montanari A., 2014, ADV NEURAL INFORM PR, P2897
   Nishimori  Hidetoshi, 2001, STAT PHYS SPIN GLASS, V111
   Rabinowitz NC, 2015, ELIFE, V4, DOI 10.7554/eLife.08998
   Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056
   Rangan S., 2009, ADV NEURAL INFORM PR, P1545
   Seely JS, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005164
   Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524
   THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992
   Williams AH, 2018, NEURON, V98, P1099, DOI 10.1016/j.neuron.2018.05.015
   Yedidia Jonathan S, 2001, ADV NEURAL INFORM PR, V13
   Zdeborova L, 2016, ADV PHYS, V65, P453, DOI 10.1080/00018732.2016.1211393
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002072
DA 2019-06-15
ER

PT S
AU Kaiser, M
   Otte, C
   Runkler, T
   Ek, CH
AF Kaiser, Markus
   Otte, Clemens
   Runkler, Thomas
   Ek, Carl Henrik
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bayesian Alignments of Warped Multi-Output Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information. We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field. The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations. This gives rise to multiple deep Gaussian process models connected via latent generating processes. We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem. We show results for an artificial data set and real-world data of two wind turbines.
C1 [Kaiser, Markus; Runkler, Thomas] Tech Univ Munich, Siemens AG, Munich, Germany.
   [Otte, Clemens] Siemens AG, Munich, Germany.
   [Ek, Carl Henrik] Univ Bristol, Bristol, Avon, England.
RP Kaiser, M (reprint author), Tech Univ Munich, Siemens AG, Munich, Germany.
EM markus.kaiser@siemens.com; clemens.otte@siemens.com;
   thomas.runkler@siemens.com; carlhenrik.ek@bristol.ac.uk
FU German Federal Ministry of Education and Research [01IHB15001]
FX The project this report is based on was supported with funds from the
   German Federal Ministry of Education and Research under project number
   01IHB15001. The sole responsibility for the reports contents lies with
   the authors.
CR Alvarez A.M., 2009, ADV NEURAL INFORM PR, V21, P57
   Alvarez M. A., 2010, INT C ART INT STAT, V9, P25
   Alvarez Mauricio A., 2011, ARXIV11066251CSMATHS
   Bitar E, 2013, P AMER CONTR CONF, P2898
   Boyle P, 2004, ADV NEURAL INFORM PR, V17, P217
   Boyle Phillip, 2005, TECH REP
   Coburn Timothy C., 2000, GEOSTATISTICS NATURA
   Damianou Andreas C., 2012, ARXIV12110358CSMATHS
   Duvenaud David, 2014, AVOIDING PATHOLOGIES
   Hensman James, 2014, ARXIV14112005STAT
   Hensman James, 2014, ARXIV14121370STAT
   Hensman James, 2013, ARXIV13096835CSSTAT
   Journel A, 1978, MINING GEOSTATISTICS
   Lazaro-Gredilla M., 2012, ADV NEURAL INFORM PR, P1619
   Matthews AGD, 2017, J MACH LEARN RES, V18, P1
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Salimbeni Hugh, 2017, ARXIV170508933STAT
   Schepers JG, 2007, J PHYS CONF SER, V75, DOI 10.1088/1742-6596/75/1/012039
   Snelson Edward, 2004, WARPED GAUSSIAN PROC, P337
   Snoek Jasper, 2014, ARXIV14020929CSSTAT
   Soleimanzadeh M, 2011, MECHATRONICS, V21, P720, DOI 10.1016/j.mechatronics.2011.02.008
   Titsias M, 2009, ARTIF INTELL, P567
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001053
DA 2019-06-15
ER

PT S
AU Kakade, SM
   Lee, JD
AF Kakade, Sham M.
   Lee, Jason D.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Provably Correct Automatic Subdifferentiation for Qualified Programs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DIFFERENTIATION
AB The Cheap Gradient Principle [Griewank and Walther, 2008] - the computational cost of computing the gradient of a scalar-valued function is nearly the same (often within a factor of 5) as that of simply computing the function itself - is of central importance in optimization; it allows us to quickly obtain (high dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing subderivatives: widely used ML libraries, including TensorFlow and PyTorch, do not correctly compute (generalized) subderivatives even on simple examples. This work considers the question: is there a Cheap Sub gradient Principle? Our main result shows that, under certain restrictions on our library of nonsmooth functions (standard in nonlinear programming), provably correct generalized subderivatives can be computed at a computational cost that is within a (dimension-free) factor of 6 of the cost of computing the scalar function itself.
C1 [Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA.
   [Lee, Jason D.] Univ Southern Calif, Los Angeles, CA 90089 USA.
RP Kakade, SM (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM sham@cs.washington.edu; jasonlee@marshall.usc.edu
FU Washington Research Foundation Fund for Innovation in Data-Intensive
   Discovery; NSF [CCF-1740551]; ONR [N00014-18-1-2247]; ARO under MURI
   [W911NF-11-1-0303]
FX We thank Dima Drusvyatskiy for many helpful discussions. Sham Kakade
   acknowledges funding from Washington Research Foundation Fund for
   Innovation in Data-Intensive Discovery, the NSF through award
   CCF-1740551, and ONR award N00014-18-1-2247. Jason D. Lee acknowledges
   support of the ARO under MURI Award W911NF-11-1-0303. This is part of
   the collaboration between US DOD, UK MOD and UK Engineering and Physical
   Research Council (EPSRC) under the Multidisciplinary University Research
   Initiative.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Abadie J, 1967, NONLINEAR PROGRAMMIN, P19
   BAUR W, 1983, THEOR COMPUT SCI, V22, P317, DOI 10.1016/0304-3975(83)90110-X
   Baydin Atilim Gunes, 2015, ABS150205767 CORR
   Blum L., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P387, DOI 10.1109/SFCS.1988.21955
   CLARKE FH, 1975, T AM MATH SOC, V205, P247, DOI 10.2307/1997202
   Clarke FH, 2008, NONSMOOTH ANAL CONTR, V178
   Coste  Michel, 2000, INTRO O MINIMAL GEOM
   Demmel JW, 1997, APPL NUMERICAL LINEA
   Fiege Sabrina, 2017, ALGORITHMIC DIFFEREN, P1
   Gould F. J., 1971, NECESSARY SUFFICIENT, V20
   Griewank A, 1995, LECT NOTES ECON MATH, V429, P155
   Griewank A., 1989, MATH PROGRAMMING REC, V6, P83
   Griewank A, 2008, EVALUATING DERIVATIV
   GRIEWANK A, 2012, DOCUMENTA MATH, P389
   Griewank Andreas, 2014, Pesqui. Oper., V34, P621, DOI 10.1590/0101-7438.2014.034.03.0621
   Griewank A, 2013, OPTIM METHOD SOFTW, V28, P1139, DOI 10.1080/10556788.2013.796683
   Khan KA, 2015, OPTIM METHOD SOFTW, V30, P1185, DOI 10.1080/10556788.2015.1025400
   Khan KA, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2491491.2491493
   Khan Kamil A., 2017, OPTIMIZATION METHODS, P1
   Klatte D., 2002, NONCON OPTIM ITS APP, V60
   Maclaurin Dougal, 2015, AUTOGRAD REVERSEMODE
   Mordukhovich BS, 2006, VARIATIONAL ANAL GEN
   Morgenstern Jacques, 1985, SIGA
   Nesterov Y, 2005, MATH PROGRAM, V104, P669, DOI 10.1007/s10107-005-0633-0
   Paszke A., 2017, NIPS W
   PETERSON DW, 1973, SIAM REV, V15, P639, DOI 10.1137/1015075
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Seeger Matthias W., 2017, ABS171008717 CORR
   SHAPIRO A, 1990, J OPTIMIZ THEORY APP, V66, P477, DOI 10.1007/BF00940933
   Trefethen L. N., 1997, NUMERICAL LINEAR ALG, V50
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001065
DA 2019-06-15
ER

PT S
AU Kaliszyk, C
   Urban, J
   Michalewski, H
   Olsak, M
AF Kaliszyk, Cezary
   Urban, Josef
   Michalewski, Henryk
   Olsak, Mirek
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reinforcement Learning of Theorem Proving
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GAME; GO
AB We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.
C1 [Kaliszyk, Cezary] Univ Innsbruck, Innsbruck, Austria.
   [Urban, Josef] Czech Tech Univ, Prague, Czech Republic.
   [Michalewski, Henryk] Univ Warsaw, Inst Math, Polish Acad Sci, Warsaw, Poland.
   [Olsak, Mirek] Charles Univ Prague, Prague, Czech Republic.
RP Kaliszyk, C (reprint author), Univ Innsbruck, Innsbruck, Austria.
FU ERC [714034 SMART]; AI4REASON ERC Consolidator grant [649043]; Czech
   project AIReasoning [CZ.02.1.01/0.0/0.0/15 003/0000466]; European
   Regional Development Fund; Academic Computer Center Cyfronet of the AGH
   University of Science and Technology in Krakow
FX Kaliszyk was supported by ERC grant no. 714034 SMART. Urban was
   supported by the AI4REASON ERC Consolidator grant number 649043, and by
   the Czech project AI&Reasoning CZ.02.1.01/0.0/0.0/15 003/0000466 and the
   European Regional Development Fund. Michalewski and Kaliszyk acknowledge
   support of the Academic Computer Center Cyfronet of the AGH University
   of Science and Technology in Krakow and their Prometheus supercomputer.
CR Alama J, 2014, J AUTOM REASONING, V52, P191, DOI 10.1007/s10817-013-9286-5
   Anthony Thomas, 2017, ADV NEURAL INF PROCE, P5366
   Bachmair L., 1994, Journal of Logic and Computation, V4, P217, DOI 10.1093/logcom/4.3.217
   Barrett C, 2009, FRONT ARTIF INTEL AP, V185, P825, DOI 10.3233/978-1-58603-929-5-825
   Blanchette JC, 2016, J FORMALIZ REASON, V9, P101, DOI 10.6092/issn.1972-5787/4593
   Blanchette JC, 2016, J AUTOM REASONING, V57, P219, DOI 10.1007/s10817-016-9362-8
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   Farber M, 2017, LECT NOTES ARTIF INT, V10395, P563, DOI 10.1007/978-3-319-63046-5_34
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gauthier T., 2017, 21 INT C LOG PROGR A, V46, P125
   Gauthier T., 2015, CERTIFIED PROGRAMS P, P49
   Grabowski A, 2010, J FORMALIZ REASON, V3, P153
   Gransden T, 2015, LECT NOTES ARTIF INT, V9195, P246, DOI 10.1007/978-3-319-21401-6_16
   Irving G., 2016, ADV NEURAL INFORM PR, P2235
   Jakubuv J, 2018, AI COMMUN, V31, P237, DOI 10.3233/AIC-180761
   Jakubuv J, 2017, LECT NOTES ARTIF INT, V10383, P292, DOI 10.1007/978-3-319-62075-6_20
   Kaliszyk C., 2013, EPIC SER, V14, P87
   Kaliszyk C., 2014, EPIC SERIES COMPUTIN, V31, P60
   Kaliszyk C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3084
   Kaliszyk C, 2015, LECT NOTES COMPUT SC, V9450, P88, DOI 10.1007/978-3-662-48899-7_7
   Kaliszyk C, 2015, J AUTOM REASONING, V55, P245, DOI 10.1007/s10817-015-9330-8
   Kaliszyk C, 2014, J AUTOM REASONING, V53, P173, DOI 10.1007/s10817-014-9303-3
   Kaliszyk Cezary, 2015, P 4 C CERT PROGR PRO, P59, DOI 10.1145/2676724.2693176
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Kovacs Laura, 2013, Computer Aided Verification. 25th International Conference, CAV 2013. Proceedings. LNCS 8044, P1, DOI 10.1007/978-3-642-39799-8_1
   Kuhlwein Daniel, 2012, Automated Reasoning. Proceedings 6th International Joint Conference, IJCAR 2012, P378, DOI 10.1007/978-3-642-31365-3_30
   Kuhlwein D, 2015, J AUTOM REASONING, V55, P91, DOI 10.1007/s10817-015-9329-1
   LETZ R, 1994, J AUTOM REASONING, V13, P297, DOI 10.1007/BF00881947
   Loos S. M., 2017, EPIC SERIES COMPUTIN, V46, P85
   McCune W., 1990, INT C AUT DED, P663, DOI DOI 10.1007/3-540-52885-7_131
   Meng J, 2008, J AUTOM REASONING, V40, P35, DOI 10.1007/s10817-007-9085-y
   Otten J, 2003, J SYMB COMPUT, V36, P139, DOI 10.1016/S0747-7171(03)00037-3
   Otten J, 2010, AI COMMUN, V23, P159, DOI 10.3233/AIC-2010-0464
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Piotrowski Bartosz, 2018, Automated Reasoning. 9th International Joint Conference, IJCAR 2018 Held as Part of the Federated Logic Conference, FloC 2018. Proceedings: LNAI 10900, P566, DOI 10.1007/978-3-319-94205-6_37
   Robinson  J., 2001, HDB AUTOMATED REASON
   ROBINSON JA, 1965, J ACM, V12, P23, DOI 10.1145/321250.321253
   Schafer S., 2015, EPIC SERIES COMPUTIN, V36, P263
   Schulz Stephan, 2013, Logic for Programming, Artificial Intelligence, and Reasoning. 19th International Conference, LPAR-19, Proceedings: LNCS 8312, P735, DOI 10.1007/978-3-642-45221-5_49
   Silver D., 2017, CORR
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Urban J., 2015, GLOB C ART INT GCAI, V36, P312
   Urban J, 2008, LECT NOTES ARTIF INT, V5195, P441, DOI 10.1007/978-3-540-71070-7_37
   Urban J, 2006, J AUTOM REASONING, V37, P21, DOI 10.1007/s10817-006-9032-3
   Urban J, 2011, LECT NOTES ARTIF INT, V6793, P263, DOI 10.1007/978-3-642-22119-4_21
   Whalen D., 2016, CORR
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003038
DA 2019-06-15
ER

PT S
AU Kallus, N
   Mao, XJ
   Udell, M
AF Kallus, Nathan
   Mao, Xiaojie
   Udell, Madeleine
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Causal Inference with Noisy and Missing Covariates via Matrix
   Factorization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COMPLETION; BOUNDS
AB Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce bias induced by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates. This flexible and principled framework adapts to missing values, accommodates a wide variety of data types, and can enhance a wide variety of causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.
C1 [Kallus, Nathan; Mao, Xiaojie; Udell, Madeleine] Cornell Univ, Ithaca, NY 14853 USA.
RP Kallus, N (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM kallus@cornell.edu; xm77@cornell.edu; udell@cornell.edu
FU National Science Foundation [1656996]; DARPA Award [FA8750-17-2-0101]
FX This work was supported by the National Science Foundation under Grant
   No. 1656996. This work was supported by the DARPA Award
   FA8750-17-2-0101.
CR ANGRIST JD, 1991, Q J ECON, V106, P979, DOI 10.2307/2937954
   Athey Susan, 2017, ARXIV171010251
   Bennett J., 2007, P KDD CUP WORKSH, V2007, P35
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai TT, 2018, ANN STAT, V46, P60, DOI 10.1214/17-AOS1541
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Cao FL, 2015, IEEE T CIRC SYST VID, V25, P1261, DOI 10.1109/TCSVT.2014.2372351
   Carroll RJ, 2006, MEASUREMENT ERROR NO
   Collins M, 2002, ADV NEUR IN, V14, P617
   Connors AF, 1996, JAMA-J AM MED ASSOC, V276, P889, DOI 10.1001/jama.276.11.889
   FROST PA, 1979, REV ECON STAT, V61, P323, DOI 10.2307/1924606
   Gunasekar S., 2014, INT C MACH LEARN, P1917
   Hansen BB, 2006, J COMPUT GRAPH STAT, V15, P609, DOI 10.1198/106186006X137047
   Hastie T, 2015, J MACH LEARN RES, V16, P3367
   Hsu Daniel, 2012, ELECT COMMUNICATIONS, V17
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Kallus Nathan, 2016, ARXIV161005604
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Kuroki M, 2014, BIOMETRIKA, V101, P423, DOI 10.1093/biomet/ast066
   Little R. J, 2014, STAT ANAL MISSING DA, V333
   Louizos Christos, 2017, ADV NEURAL INFORM PR, P6449
   MCCULLAGH P, 1984, EUR J OPER RES, V16, P285, DOI 10.1016/0377-2217(84)90282-0
   Miao Wang, 2016, ARXIV160908816
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Ng A. Y., 2004, P 21 INT C MACH LEAR, P78, DOI [DOI 10.1145/1015330.1015435, 10.1145/1015330.1015435]
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Schnabel Tobias, 2016, ARXIV160205352
   Schuler A, 2016, BIOCOMPUT-PAC SYM, P144
   Singh AP, 2008, LECT NOTES ARTIF INT, V5212, P358, DOI 10.1007/978-3-540-87481-2_24
   Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107
   Tripathi G, 1999, ECON LETT, V63, P1, DOI 10.1016/S0165-1765(99)00014-2
   Udell Madeleine, 2016, Foundations and Trends in Machine Learning, V9, P1, DOI 10.1561/2200000055
   Udell M., 2017, ARXIV170507474
   van Buuren S, 2011, J STAT SOFTW, V45, P1
   Vershynin  Roman, 2010, ARXIV10113027
   Wickens Michael R, 1972, ECONOMETRICA J ECONO, P759
   Wooldridge J., 2015, INTRO ECONOMETRICS M
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001046
DA 2019-06-15
ER

PT S
AU Kallus, N
AF Kallus, Nathan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Balanced Policy Evaluation and Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BILEVEL OPTIMIZATION; PROPENSITY SCORE
AB We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.
C1 [Kallus, Nathan] Cornell Univ, Ithaca, NY 14853 USA.
   [Kallus, Nathan] Cornell Tech, New York, NY 10044 USA.
RP Kallus, N (reprint author), Cornell Univ, Ithaca, NY 14853 USA.; Kallus, N (reprint author), Cornell Tech, New York, NY 10044 USA.
EM kallus@cornell.edu
FU National Science Foundation [1656996]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 1656996.
CR Athey Susan, 2017, ARXIV170202896
   Austin PC, 2015, STAT MED, V34, P3661, DOI 10.1002/sim.6607
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bennett KP, 2008, LECT NOTES COMPUT SC, V5050, P25
   Bertsimas  D., 2016, ARXIV160502347
   Bertsimas D, 2017, DIABETES CARE, V40, P210, DOI 10.2337/dc16-0826
   Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chernozhukov  V., 2016, ARXIV160800060
   Crammer K., 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628
   Dudik  M., 2011, ARXIV11034601
   Elliott MR, 2008, J OFF STAT, V24, P517
   Fletcher R., 2013, PRACTICAL METHODS OP
   Gretton A., 2006, ADV NEURAL INFORM PR, P513
   HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784
   Imbens GW, 2000, BIOMETRIKA, V87, P706, DOI 10.1093/biomet/87.3.706
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Kallus  N., 2018, CONFOUNDING ROBUST P
   Kallus  N., 2018, INT C ART INT STAT, P1243
   Kallus  N., 2017, P INT C MACH LEARN, P1789
   Kallus  N., 2016, ARXIV161208321
   Kallus N, 2018, J R STAT SOC B, V80, P85, DOI 10.1111/rssb.12240
   Kang JDY, 2007, STAT SCI, V22, P523, DOI 10.1214/07-STS227
   Ledoux M., 1991, PROBABILITY BANACH S
   Li L., 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Lunceford JK, 2004, STAT MED, V23, P2937, DOI 10.1002/SIM.1903
   Ochs P, 2016, J MATH IMAGING VIS, V56, P175, DOI 10.1007/s10851-016-0663-7
   Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robins J. M, 1999, P AM STAT ASS SECT B, P6
   ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910
   Royden H, 1988, REAL ANAL
   Sabach S, 2017, SIAM J OPTIMIZ, V27, P640, DOI 10.1137/16M105592X
   Scharfstein DO, 1999, J AM STAT ASSOC, V94, P1096, DOI 10.2307/2669923
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Sriperumbudur B. K., 2010, ARXIV10030887
   Strehl A., 2010, ADV NEURAL INFORM PR, P2217
   Swaminathan A., 2015, ADV NEURAL INFORM PR, P3231
   Swaminathan Adith, 2015, ICML, P814
   Zhou XL, 2015, J GEOL, V123, P269, DOI [10.1086/681918, 10.1080/01621459.2015.1093947]
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003045
DA 2019-06-15
ER

PT S
AU Kallus, N
   Zhou, A
AF Kallus, Nathan
   Zhou, Angela
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Confounding-Robust Policy Improvement
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches that assume unconfoundedness, i.e., no unobserved confounders affected both treatment assignment and outcomes, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference "status quo" policy, over an uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.
C1 [Kallus, Nathan; Zhou, Angela] Cornell Univ, New York, NY 10021 USA.
   [Kallus, Nathan; Zhou, Angela] Cornell Tech, New York, NY 10044 USA.
RP Kallus, N (reprint author), Cornell Univ, New York, NY 10021 USA.; Kallus, N (reprint author), Cornell Tech, New York, NY 10044 USA.
EM kallus@cornell.edu; az434@cornell.edu
FU National Science Foundation [1656996]; National Defense Science &
   Engineering Graduate Fellowship Program
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 1656996. Angela Zhou is supported through the
   National Defense Science & Engineering Graduate Fellowship Program.
CR Aronow  P., 2012, BIOMETRIKA
   Berge  E., 2002, COCHRANE LIB SYSTEMA
   Beygelzimer  A., 2009, P 15 ACM SIGKDD INT
   Bottou  L., 2013, J MACHINE LEARNING R
   Charnes  A., 1962, NAVAL RES LOGISTICS
   Dudik  M., 2014, STAT SCI
   Fogarty  C., 2017, EXTENDED SENSITIVITY
   Hasegawa  R., 2017, BIOMETRICS
   Hsu JY, 2013, BIOMETRICS, V69, P803, DOI 10.1111/biom.12101
   I. S. T. C. Group, 1997, LANCET
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Kallus  N., 2017, P 34 INT C MACH LEAR
   Kitagawa  T., 2015, EMPIRICAL WELFARE MA
   Ledoux M., 1991, PROBABILITY BANACH S
   Li  L., 2011, P 4 ACM INT C WEB SE
   Lipsitch  M., 2010, EPIDEMIOLOGY
   Manski  C., 2005, ECONOMETRIC I LECT
   Masten  M., 2018, ECONOMETRICA
   Miratrix L. W., 2018, BIOMETRIKA
   Petrik  M., 2016, 29 C NEUR INF PROC S
   Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864
   Rosenbaum P. R., 1983, BIOMETRIKA
   Rosenbaum P.R., 2002, SPRINGER SERIES STAT
   Rubin  D., 1974, J ED PSYCHOL
   RUBIN DB, 1980, J AM STAT ASSOC, V75, P591, DOI 10.2307/2287653
   Still  G., 2018, OPTIMIZATION ONLINE
   Swaminathan  A., 2015, P NIPS
   Swaminathan  A., 2015, J MACHINE LEARNING R
   Thomas  P., 2015, P 32 INT C MACH LEAR
   Wager  S., 2017, EFFICIENT POLICY LEA
   Wager  S., 2017, J AM STAT ASS
   Wang Y, 2017, PROC EUR CONF ANTENN
   Zhang  J., 2017, P 26 INT JOINT C ART, P1340, DOI [10.24963/ijcai.2017/186, DOI 10.24963/IJCAI.2017/186]
   Zhao  Q., 2017, SENSITIVITY ANAL INV
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003079
DA 2019-06-15
ER

PT S
AU Kallus, N
   Puli, AM
   Shalit, U
AF Kallus, Nathan
   Puli, Aahlad Manas
   Shalit, Uri
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Removing Hidden Confounding by Experimental Grounding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EXTERNAL VALIDITY; TRANSPORTABILITY
AB Observational data is increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not suffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which it yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment.
C1 [Kallus, Nathan] Cornell Univ, New York, NY 10021 USA.
   [Kallus, Nathan] Cornell Tech, New York, NY 10044 USA.
   [Puli, Aahlad Manas] NYU, New York, NY USA.
   [Shalit, Uri] Technion, Haifa, Israel.
RP Kallus, N (reprint author), Cornell Univ, New York, NY 10021 USA.; Kallus, N (reprint author), Cornell Tech, New York, NY 10044 USA.
EM kallus@cornell.edu; apm470@nyu.edu; urishalit@technion.ac.il
FU National Science Foundation [1656996]
FX We wish to thank the anonymous reviewers for their helpful suggestions
   and comments. (NK) This material is based upon work supported by the
   National Science Foundation under Grant No. 1656996.
CR Andrews  Isaiah, 2017, TECHNICAL REPORT
   Athey  Susan, 2016, ARXIV160309326
   Bareinboim E, 2013, J CAUSAL INFERENCE, V1, P107, DOI 10.1515/jci-2012-0004
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Crump RK, 2008, REV ECON STAT, V90, P389, DOI 10.1162/rest.90.3.389
   D'Amour  Alexander, 2017, ARXIV171102582
   Hartman E, 2015, J R STAT SOC A STAT, V178, P757, DOI 10.1111/rssa.12094
   Hernan MA, 2008, EPIDEMIOLOGY, V19, P766, DOI 10.1097/EDE.0b013e3181875e61
   Krueger AB, 1999, Q J ECON, V114, P497, DOI 10.1162/003355399556052
   Magliacane  Sara, 2016, ARXIV161110351
   McFowland III Edward, 2018, ARXIV180309159
   Pearl J., 2009, CAUSALITY
   Pearl J, 2014, STAT SCI, V29, P579, DOI 10.1214/14-STS486
   Pearl  Judea, 2015, SOCIOLOGICAL METHODS
   Rossouw JE, 2002, JAMA-J AM MED ASSOC, V288, P321
   Rothwell PM, 2005, LANCET, V365, P82, DOI 10.1016/S0140-6736(04)17670-8
   Stuart EA, 2011, J R STAT SOC A STAT, V174, P369, DOI 10.1111/j.1467-985X.2010.00673.x
   Triantafillou S, 2015, J MACH LEARN RES, V16, P2147
   Vandenbroucke JP, 2009, LANCET, V373, P1233, DOI 10.1016/S0140-6736(09)60708-X
   Wager  S., 2017, J AM STAT ASS
   WORD E, 1990, STATE TENNESSEES STU
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005047
DA 2019-06-15
ER

PT S
AU Kalra, A
   Rashwan, A
   Hsu, W
   Poupart, P
   Doshi, P
   Trimponias, G
AF Kalra, Agastya
   Rashwan, Abdullah
   Hsu, Wilson
   Poupart, Pascal
   Doshi, Prashant
   Trimponias, George
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Structure Learning for Feed-Forward and Recurrent Sum-Product
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which marginal inference is always tractable. These properties follow from the conditions of completeness and decomposability, which must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes a new online structure learning technique for feed-forward and recurrent SPNs. The algorithm is demonstrated on real-world datasets with continuous features and sequence datasets of varying length for which the best network architecture is not obvious.
C1 [Kalra, Agastya; Rashwan, Abdullah; Hsu, Wilson; Poupart, Pascal] Univ Waterloo, Waterloo AI Inst, Cheriton Sch Comp Sci, Waterloo, ON, Canada.
   [Kalra, Agastya; Rashwan, Abdullah; Hsu, Wilson; Poupart, Pascal] Vector Inst, Toronto, ON, Canada.
   [Doshi, Prashant] Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
   [Trimponias, George] Huawei Noahs Ark Lab, Hong Kong, Peoples R China.
RP Kalra, A (reprint author), Univ Waterloo, Waterloo AI Inst, Cheriton Sch Comp Sci, Waterloo, ON, Canada.; Kalra, A (reprint author), Vector Inst, Toronto, ON, Canada.
EM agastya.kalra@gmail.com; arashwan@uwaterloo.ca; wwhsu@uwaterloo.ca;
   ppoupart@uwaterloo.ca; pdoshi@cs.uga.edu; g.trimponias@huawei.com
FU Huawei Technologies; NSERC; NSF [IIS-1815598]
FX This research was funded by Huawei Technologies and NSERC. Prashant
   Doshi acknowledges support from NSF grant #IIS-1815598.
CR Adel T., 2015, P UNC ART INT, P32
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Darwiche Adnan, 2002, P KR, V2, P409
   Dennis Aaron, 2012, ADV NEURAL INFORM PR, P2033
   Dinh Laurent, 2017, INT C LEARN REPR
   Gens R., 2013, P 30 INT C MACH LEAR, V28, P873
   Gens R., 2012, NIPS, P3248
   Jaini Priyank, 2016, P 8 INT C PROB GRAPH, P228
   Melibari Mazen, 2016, C PROB GRAPH MOD, P345
   Peharz Robert, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P612, DOI 10.1007/978-3-642-40991-2_39
   Peharz R., 2015, THESIS
   Poon H., 2011, P 12 C UNC ART INT, P2551
   Rahman Tahrima, 2016, P 32 C UNC ART INT U
   Rashwan Abdullah, 2016, P 19 INT C ART INT S, P1469
   Rooshenas A., 2014, P 31 INT C MACH LEAR, V32, P710
   Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1
   Sang-Woo Lee, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8227, P220, DOI 10.1007/978-3-642-42042-9_28
   VERGARI A, 2015, ECML PKDD, V9285, P343, DOI DOI 10.1007/978-3-319-23525-7_21
   Zhao Han, 2016, ICML
   Zhao Han, 2016, ADV NEURAL INFORM PR, P433
   Zhao QF, 2015, INT CONF MACH LEARN, P116, DOI 10.1109/ICMLC.2015.7340908
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001048
DA 2019-06-15
ER

PT S
AU Kanai, S
   Fujiwara, Y
   Yamanaka, Y
   Adachi, S
AF Kanai, Sekitoshi
   Fujiwara, Yasuhiro
   Yamanaka, Yuki
   Adachi, Shuichi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sigsoftmax: Reanalysis of the Softmax Bottleneck
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Softmax is an output activation function for modeling categorical probability distributions in many applications of deep learning. However, a recent study revealed that softmax can be a bottleneck of representational capacity of neural networks in language modeling (the softmax bottleneck). In this paper, we propose an output activation function for breaking the softmax bottleneck without additional parameters. We re-analyze the softmax bottleneck from the perspective of the output set of log-softmax and identify the cause of the softmax bottleneck. On the basis of this analysis, we propose sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Sigsoftmax can break the softmax bottleneck. The experiments on language modeling demonstrate that sigsoftmax and mixture of sigsoftmax outperform softmax and mixture of softmax, respectively.
C1 [Kanai, Sekitoshi] Keio Univ, NTT Software Innovat Ctr, Tokyo, Japan.
   [Fujiwara, Yasuhiro] NTT Software Innovat Ctr, Tokyo, Japan.
   [Yamanaka, Yuki] NTT Secure Platform Labs, Tokyo, Japan.
   [Adachi, Shuichi] Keio Univ, Tokyo, Japan.
RP Kanai, S (reprint author), Keio Univ, NTT Software Innovat Ctr, Tokyo, Japan.
EM kanai.sekitoshi@lab.ntt.co.jp; fujiwara.yasuhiro@lab.ntt.co.jp;
   yamanaka.yuki@lab.ntt.co.jp; adachi.shuichi@appi.keio.ac.jp
CR AUEB M. T. R., 2016, ADV NEURAL INFORM PR, P4161
   Bishop C. M., 2006, PATTERN RECOGNITION
   Bishop C M, 1995, NEURAL NETWORKS PATT
   Bridle J. S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P227
   Bridle J S, 1990, ADV NEURAL INFORMATI, V2, P211
   Chelba  Ciprian, 2013, TECHNICAL REPORT
   Chen  Binghui, 2017, P CVPR, P5372
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   de Brebisson  Alexandre, 2016, P ICLR
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Grave E., 2017, P ICML PMLR AUG, P1302
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kanai  S., 2017, P NEUR INF PROC SYST, P435
   Krause  Ben, 2017, ARXIV170907432
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Mahoney M., 2011, LARGE TEXT COMPRESSI
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Martins A., 2016, INT C MACH LEARN, P1614
   Memisevic R., 2010, ADV NEURAL INFORM PR, P1603
   Merity  Stephen, 2018, P ICLR
   Merity  Stephen, 2017, P ICLR
   Mikolov T., 2012, THESIS
   Mohassel P, 2017, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2017.12
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ollivier  Yann, 2013, ARXIV13030818
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Press W. H., 2007, NUMERICAL RECIPES
   Shim  Kyuhong, 2017, P NIPS, V30, P5469
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Yang  Zhilin, 2018, P ICLR
   Zaremba W, 2014, ARXIV14092329
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300027
DA 2019-06-15
ER

PT S
AU Kannan, S
   Morgenstern, J
   Roth, A
   Waggoner, B
   Wu, ZS
AF Kannan, Sampath
   Morgenstern, Jamie
   Roth, Aaron
   Waggoner, Bo
   Wu, Zhiwei Steven
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual
   Bandit Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Bandit learning is characterized by the tension between long-term exploration and short-term exploitation. However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a "greedy" algorithm, which always makes the optimal decision for the individuals at hand - but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm. We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve "no regret", perhaps (depending on the specifics of the setting) with a constant amount of initial training data. This suggests that in slightly perturbed environments, exploration and exploitation need not be in conflict in the linear setting.(1)
C1 [Kannan, Sampath; Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA.
   [Morgenstern, Jamie] Georgia Tech, Atlanta, GA USA.
   [Waggoner, Bo] Microsoft Res, New York, NY USA.
   [Wu, Zhiwei Steven] Univ Minnesota, Minneapolis, MN 55455 USA.
RP Kannan, S (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Agarwal  A., 2014, P 31 INT C MACH LEAR, P1638
   Barry-Jester Anna Maria, 2015, NEW SCI
   Bastani H., 2017, ARXIV E PRINTS
   Bietti A., 2018, ARXIV E PRINTS
   Bird S., 2016, WORKSH FAIRN ACC TRA
   Byrnes Nanette, 2016, MIT TECHNOLOGY REV
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Ensign Danielle, 2017, WORKSH FAIRN ACC TRA
   Li L., 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Raghavan Manish, 2018, P 31 C LEARN THEOR C
   Rudin Cynthia, 2013, WIRED MAGAZINE
   Syrgkanis Vasilis, 2016, P 33 INT C MACH LEAR, P2159
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302025
DA 2019-06-15
ER

PT S
AU Kaplan, H
   Stemmer, U
AF Kaplan, Haim
   Stemmer, Uri
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private k-Means with Constant Multiplicative Error
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds.
   Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present-for the first time-efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private coresets for k-means clustering in both models.
C1 [Kaplan, Haim] Tel Aviv Univ, Tel Aviv, Israel.
   [Kaplan, Haim] Google, Mountain View, CA 94043 USA.
   [Stemmer, Uri] Ben Gurion Univ Negev, Beer Sheva, Israel.
RP Kaplan, H (reprint author), Tel Aviv Univ, Tel Aviv, Israel.; Kaplan, H (reprint author), Google, Mountain View, CA 94043 USA.
EM haimk@post.tau.ac.il; u@uri.co.il
FU Koshland fellowship; Israel Science Foundation [950/16, 5219/17]
FX Work done while the second author was a postdoctoral researcher at the
   Weizmann Institute of Science, supported by a Koshland fellowship, and
   by the Israel Science Foundation (grants 950/16 and 5219/17).
CR Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736
   Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Arthur D, 2009, ANN IEEE SYMP FOUND, P405, DOI 10.1109/FOCS.2009.14
   Arya V, 2004, SIAM J COMPUT, V33, P544, DOI [10.1137/S0097539702416402, 10.1137/S00097539702416402]
   Balcan M. F., 2017, P MACHINE LEARNING R, P322
   Barger A, 2016, P 2016 SIAM INT C DA, P342
   Blum A, 2005, PODS, P128, DOI DOI 10.1145/1065167.1065184
   Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45
   Chen K, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1177, DOI 10.1145/1109557.1109687
   Cohen E., 2018, P 32 AAAI C ART INT
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Feldman D, 2017, 2017 16TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN), P3, DOI 10.1145/3055031.3055090
   Feldman D, 2011, ACM S THEORY COMPUT, P569
   Feldman D, 2009, ACM S THEORY COMPUT, P361
   Feldman Dan, 2007, P 23 ACM S COMP GEOM, P11
   Gupta A, 2010, PROC APPL MATH, V135, P1106
   Har-Peled S., 2004, P 36 ANN ACM S THEOR, P291, DOI DOI 10.1145/1007352.1007400
   Har-Peled S, 2007, DISCRETE COMPUT GEOM, V37, P3, DOI 10.1007/s00454-006-1271-x
   Huang ZY, 2018, PODS'18: PROCEEDINGS OF THE 37TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P395, DOI 10.1145/3196959.3196977
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Johnson W. B., 1984, EXTENSIONS LIPSCHITZ
   Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Mohan P., 2012, P 2012 ACM SIGMOD IN, P349, DOI DOI 10.1145/2213836.2213876
   Nissim K., 2018, P MACHINE LEARNING R, V83, P619
   Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803
   Nissim K, 2016, PODS'16: PROCEEDINGS OF THE 35TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P413, DOI 10.1145/2902251.2902296
   Nock R., 2016, P 33 INT C MACH LEAR, P145
   Su D, 2016, CODASPY'16: PROCEEDINGS OF THE SIXTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY AND PRIVACY, P26, DOI 10.1145/2857705.2857708
   Wang Y. X., 2015, WATER RESOUR MANAG, V15, P1
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305045
DA 2019-06-15
ER

PT S
AU Karvonen, T
   Oates, CJ
   Sarkka, S
AF Karvonen, Toni
   Oates, Chris. J.
   Sarkka, Simo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Bayes-Sard Cubature Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID HILBERT-SPACES; QUADRATURE; APPROXIMATION; INTERPOLATION
AB This paper focusses on the formulation of numerical integration as an inferential task. To date, research effort has largely focussed on the development of Bayesian cubature, whose distributional output provides uncertainty quantification for the integral. However, the point estimators associated to Bayesian cubature can be inaccurate and acutely sensitive to the prior when the domain is high-dimensional. To address these drawbacks we introduce Bayes-Sard cubature, a probabilistic framework that combines the flexibility of Bayesian cubature with the robustness of classical cubatures which are well-established. This is achieved by considering a Gaussian process model for the integrand whose mean is a parametric regression model, with an improper prior on each regression coefficient. The features in the regression model consist of test functions which are guaranteed to be exactly integrated, with remaining degrees of freedom afforded to the non-parametric part. The asymptotic convergence of the Bayes-Sard cubature method is established and the theoretical results are numerically verified. In particular, we report two orders of magnitude reduction in error compared to Bayesian cubature in the context of a high-dimensional financial integral.
C1 [Karvonen, Toni; Sarkka, Simo] Aalto Univ, Espoo, Finland.
   [Oates, Chris. J.] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.
   [Oates, Chris. J.] Alan Turing Inst, London, England.
RP Karvonen, T (reprint author), Aalto Univ, Espoo, Finland.
EM toni.karvonen@aalto.fi; chris.oates@ncl.ac.uk; simo.sarkka@aalto.fi
FU Aalto ELEC Doctoral School; Lloyd's Register Foundation programme on
   data-centric engineering; Academy of Finland [266940, 304087, 313708];
   National Science Foundation [DMS-1127914]
FX The authors are grateful for discussion with Aretha Teckentrup,
   Catherine Powell, Fred Hickernell and Filip Tronarp. TK was supported by
   the Aalto ELEC Doctoral School. CJO was supported by the Lloyd's
   Register Foundation programme on data-centric engineering. SS was
   supported by the Academy of Finland projects 266940, 304087 and 313708.
   This material was based upon work partially supported by the National
   Science Foundation under Grant DMS-1127914 to the Statistical and
   Applied Mathematical Sciences Institute. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the National
   Science Foundation.
CR BACH F, 2017, J MACHINE LEARNING R, V18
   Berlinet A., 2011, REPRODUCING KERNEL H
   Bernardo J., 1992, BAYESIAN STAT, V4, P345
   Bezhaev A. Yu., 1991, SOVIET J NUMER ANAL, V6, P95
   Bogachev  V., 1998, MATH SURVEYS MONOGRA, V62
   Briol  F.-X., 2018, STAT SCI
   Chai  H., 2018, ARXIV180204782
   Cockayne  J., 2017, ARXIV170203673V2
   DAVIS  P.J., 2007, METHODS NUMERICAL IN
   DeVore  R., 2018, CONSTRUCTIVE APPROXI
   Diaconis P., 1988, STATISTICAL DECISION, V1, P163
   FUKUMIZU K, 2016, ADV NEURAL INFORM PR, P3288
   Gautschi W, 2004, ORTHOGONAL POLYNOMIA
   Genz A, 1996, J COMPUT APPL MATH, V71, P299, DOI 10.1016/0377-0427(95)00232-4
   Gunter T, 2014, ADV NEURAL INFORM PR, V27, P2789
   Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142
   Hensman J, 2018, J MACH LEARN RES, V18, P1
   Hickernell FJ, 1998, MATH COMPUT, V67, P299, DOI 10.1090/S0025-5718-98-00894-1
   Holtz  M., 2011, LECT NOTES COMPUTATI, V77
   Jagadeeswaran  R., 2018, ARXIV180909803V1
   Kanagawa  M., 2017, ARXIV170900147V1
   Karvonen  T., 2018, ARXIV180910227V1
   Karvonen T, 2017, IEEE INT WORKS MACH
   Karvonen T, 2018, SIAM J SCI COMPUT, V40, pA697, DOI 10.1137/17M1121779
   Kennedy M, 1998, STAT COMPUT, V8, P365, DOI 10.1023/A:1008832824006
   Kennedy MC, 2001, J ROY STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294
   LARKIN F. M., 1972, ROCKY MT J MATH, V2, P379
   Larkin F. M., 1974, INF PROC 74 P IFIP C, V74, P605
   LARKIN FM, 1970, MATH COMPUT, V24, P911, DOI 10.2307/2004625
   Minka T. P, 2000, TECHNICAL REPORT
   Mosamam AM, 2010, J NONPARAMETR STAT, V22, P711, DOI 10.1080/10485250903388886
   O'Hagan  A., 1988, TECHNICAL REPORT
   Oates C. J., 2017, ADV NEURAL INFORM PR, P109
   OETTERSHAGEN J., 2017, THESIS
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   OHAGAN A, 1978, J R STAT SOC B, V40, P1
   Osborne M, 2012, ADV NEURAL INFORM PR, V25, P46
   Owhadi  H., 2015, ARXIV150604208V2
   PATTERSON TN, 1968, MATH COMPUT, V22, P847, DOI 10.2307/2004583
   Portier  F., 2018, ARXIV180101797V3
   Pronzato  L., 2018, ARXIV180810722V1
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   RICHTERDYN N, 1971, SIAM J NUMER ANAL, V8, P583, DOI 10.1137/0708056
   Santner T. J., 2003, DESIGN ANAL COMPUTER
   SARD A, 1949, AM J MATH, V71, P80, DOI 10.2307/2372095
   Sarkka S., 2016, J ADV INFORM FUSION, V11, P31
   SCHOENBERG IJ, 1964, B AM MATH SOC, V70, P143, DOI 10.1090/S0002-9904-1964-11054-5
   WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364
   Wendland H., 2005, CAMBRIDGE MONOGRAPHS, V17
   Xu WT, 2017, SIAM-ASA J UNCERTAIN, V5, P138, DOI 10.1137/15M105358X
   Zhou QP, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aac287
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000039
DA 2019-06-15
ER

PT S
AU Kasai, H
   Mishra, B
AF Kasai, Hiroyuki
   Mishra, Bamdev
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inexact trust-region algorithms on Riemannian manifolds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID RANK MATRIX COMPLETION; OPTIMIZATION METHODS
AB We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a trust-region sub-problem. Addressing large-scale finite-sum problems, we specifically propose sub-sampled algorithms with a fixed bound on sub-sampled Hessian and gradient sizes, where the gradient and Hessian are computed by a random sampling technique. Numerical evaluations demonstrate that the proposed algorithms outperform state-of-the-art Riemannian deterministic and stochastic gradient algorithms across different applications.
C1 [Kasai, Hiroyuki] Univ Electrocommun, Chofu, Tokyo, Japan.
   [Mishra, Bamdev] Microsoft, Hyderabad, India.
RP Kasai, H (reprint author), Univ Electrocommun, Chofu, Tokyo, Japan.
EM kasai@is.uec.ac.jp; bamdevm@microsoft.com
FU JSPS KAKENHI [JP16K00031, JP17H01732]
FX H. Kasai was partially supported by JSPS KAKENHI Grant Numbers
   JP16K00031 and JP17H01732. We thank Nicolas Boumal and Hiroyuki Sato for
   insight discussions and also express our sincere appreciation to Jonas
   Moritz Kohler for sharing his expertise on sub-sampled algorithms in the
   Euclidean case.
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Balzano L., 2010, ALLERTON
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Boumal N., 2018, IMA J NUMER ANAL
   Boumal N, 2015, LINEAR ALGEBRA APPL, V475, P200, DOI 10.1016/j.laa.2015.02.027
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X
   Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5
   Conn A. R., 2000, MOS SIAM SERIES OPTI
   Da Silva C, 2015, LINEAR ALGEBRA APPL, V481, P131, DOI 10.1016/j.laa.2015.04.015
   Defazio A., 2014, NIPS
   Erdogdu M. A., 2015, NIPS
   GABAY D, 1982, J OPTIMIZ THEORY APP, V37, P177, DOI 10.1007/BF00934767
   Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209
   Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999
   Huang W., 2016, ENUMATH 2015
   Huang W, 2015, SIAM J OPTIMIZ, V25, P1660, DOI 10.1137/140955483
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Johnson R., 2013, NIPS
   Kasai  H., 2016, ICML
   Kasai H., 2018, AISTATS
   Kasai H., 2018, ICML
   KASAI H, 2018, JMLR, V18
   Kohler J. M., 2017, ICML
   Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z
   Kueng R, 2014, LINEAR ALGEBRA APPL, V441, P110, DOI 10.1016/j.laa.2013.04.018
   LUENBERGER DG, 1972, MANAGE SCI, V18, P620, DOI 10.1287/mnsc.18.11.620
   Meyer G., 2011, ICML
   Mishra B., 2019, MACHINE LEARNING
   Mishra B, 2014, IEEE DECIS CONTR P, P1137, DOI 10.1109/CDC.2014.7039534
   Nguyen L. M., 2017, ICML
   Nimishakavi M., 2018, NEURIPS
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108
   Porikli F., 2006, ICIP
   Reddi S. J., 2016, ICML
   Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roux N. L., 2012, NIPS
   Sato H., 2017, ARXIV170205594
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalit U, 2012, J MACH LEARN RES, V13, P429
   Theis F. J., 2009, ICA
   Toint P. L., 1981, SPARSE MATRICES THEI, P1981
   Tuzel O., 2006, ECCV
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Xu P., 2017, ARXIV170807164
   Yang WH, 2014, PAC J OPTIM, V10, P415
   Yao  Z., 2018, ARXIV180206925
   Zhang H., 2016, NIPS
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304028
DA 2019-06-15
ER

PT S
AU Kaufmann, E
   Koolen, WM
   Garivier, A
AF Kaufmann, Emilie
   Koolen, Wouter M.
   Garivier, Aurelien
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MULTIARMED BANDIT
AB Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-task in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.
C1 [Kaufmann, Emilie] CNRS, Paris, France.
   [Kaufmann, Emilie] Univ Lille, CRIStAL SequeL Inria Lille, Lille, France.
   [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Garivier, Aurelien] Ecole Normale Super Lyon, UMPA, Lyon, France.
RP Kaufmann, E (reprint author), CNRS, Paris, France.; Kaufmann, E (reprint author), Univ Lille, CRIStAL SequeL Inria Lille, Lille, France.
EM emilie.kaufmann@univ-lille.fr; wmkoolen@cwi.nl;
   aurelien.garivier@ens-lyon.fr
CR Agrawal S., 2012, P 25 C LEARN THEOR
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Chen L., 2017, P 30 C LEARN THEOR C
   CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205
   de la Pena VH, 2009, PROBAB APPL SER, P1
   DEramo C., 2017, AAAI
   DEramo C., 2016, INT C MACH LEARN ICM
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Garivier A., 2017, ARXIV171104454
   Garivier A., 2018, MATH OPERATIONS  JUN
   Garivier A., 2016, P 29 C LEARN THEOR C
   Goldsman D., 1998, COMP SYSTEMS VIA SIM
   Grill J.-B., 2016, ADV NEURAL INFORM PR, P4680
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Huang R., 2017, INT C ALG LEARN THEO
   Imagawa T, 2017, CONF TECHNOL APPL, P202, DOI 10.1109/TAAI.2017.19
   Jamieson K., 2014, P 27 C LEARN THEOR
   Kalyanakrishnan S., 2012, INT C MACH LEARN ICM
   Kaufmann E., 2012, P 23 C ALG LEARN THE
   Kaufmann E., 2018, PREPRINT
   Kaufmann E., 2017, ADV NEURAL INFORM PR
   Kaufmann E, 2016, J MACH LEARN RES, V17
   Kim S.-H., 2005, ACM Transactions on Modeling and Computer Simulation, V15, P155, DOI 10.1145/1060576.1060579
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Locatelli A., 2016, INT C MACH LEARN ICM
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Russo D., 2016, CORR
   Simchowitz M., 2017, P 30 C LEARN THEOR C
   Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   van Hasselt H., 2013, CORR
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000080
DA 2019-06-15
ER

PT S
AU Kawamoto, T
   Tsubaki, M
   Obuchi, T
AF Kawamoto, Tatsuro
   Tsubaki, Masashi
   Obuchi, Tomoyuki
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Mean-field theory of graph neural networks in graph partitioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DYNAMICS
AB A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.
C1 [Kawamoto, Tatsuro; Tsubaki, Masashi] Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Koto Ku, 2-3-26 Aomi, Tokyo, Japan.
   [Obuchi, Tomoyuki] Tokyo Inst Technol, Dept Math & Comp Sci, Meguro Ku, 2-12-1 Ookayama, Tokyo, Japan.
RP Kawamoto, T (reprint author), Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Koto Ku, 2-3-26 Aomi, Tokyo, Japan.
EM kawamoto.tatsuro@aist.go.jp; tsubaki.masashi@aist.go.jp;
   obuchi@c.titech.ac.jp
RI Kawamoto, Tatsuro/M-8130-2018
OI Kawamoto, Tatsuro/0000-0002-9898-839X
FU New Energy and Industrial Technology Development Organization (NEDO);
   JSPS KAKENHI [18K11463]
FX The authors are grateful to Ryo Karakida for helpful comments. This work
   was supported by the New Energy and Industrial Technology Development
   Organization (NEDO) (T.K. and M. T.) and JSPS KAKENHI No. 18K11463 (T.
   O.).
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bruna J., 2013, ABS13126203 CORR
   Bruna  J., 2017, ARXIV170508415
   CRISANTI A, 1988, PHYS REV A, V37, P4865, DOI 10.1103/PhysRevA.37.4865
   Crisanti A, 1990, PREPRINT
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Defferrard M., 2016, ADV NEURAL INFORM PR, P3844
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR
   Gilmer J., 2017, ARXIV170401212
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Grover Aditya, 2016, INT C KNOWL DISC DAT
   Hamilton W., 2017, ADV NEURAL INFORM PR, V30, P1024
   He K., 2016, P IEEE C COMP VIS PA
   Ioffe S., 2015, ARXIV150203167
   Kawamoto T, 2015, EPL-EUROPHYS LETT, V112, DOI 10.1209/0295-5075/112/40007
   Kawamoto T, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.032301
   Kawamoto T, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.062803
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kipf T. N., 2016, ARXIV160902907
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Lei T., 2017, P MACHINE LEARNING R, P2024
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   Moore Cristopher, 2017, ARXIV170200467
   Mossel E, 2015, PROBAB THEORY REL, V162, P431, DOI 10.1007/s00440-014-0576-6
   Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104
   Newman MEJ, 2016, PHYS REV LETT, V117, DOI 10.1103/PhysRevLett.117.078301
   Niepert M., 2016, INT C MACH LEARN, P2014
   Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735
   Opper M, 2016, J PHYS A-MATH THEOR, V49, DOI 10.1088/1751-8113/49/11/114002
   Peixoto TP, 2014, PHYS REV E, V89, DOI 10.1103/PhysRevE.89.012804
   Peixoto Tiago P, 2017, ARXIV170510225
   Poole B, 2016, ADV NEURAL INFORM PR, V29, P3360
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schlichtkrull  M., 2017, ARXIV170306103
   Schoenholz Samuel S, 2016, ARXIV161101232
   SOMPOLINSKY H, 1982, PHYS REV B, V25, P6860, DOI 10.1103/PhysRevB.25.6860
   Tokui  S., 2015, P WORKSH MACH LEARN
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Yang L., 2016, P 25 INT JOINT C ART, P2252
   You J., 2018, ARXIV180208773
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304038
DA 2019-06-15
ER

PT S
AU Kazemi, H
   Soleymani, S
   Taherkhani, F
   Iranmanesh, SM
   Nasrabadi, NM
AF Kazemi, Hadi
   Soleymani, Sobhan
   Taherkhani, Fariborz
   Iranmanesh, Seyed Mehdi
   Nasrabadi, Nasser M.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Image-to-Image Translation Using Domain-Specific
   Variational Information Bound
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.
C1 [Kazemi, Hadi; Soleymani, Sobhan; Taherkhani, Fariborz; Iranmanesh, Seyed Mehdi; Nasrabadi, Nasser M.] West Virginia Univ, Morgantown, WV 26505 USA.
RP Kazemi, H (reprint author), West Virginia Univ, Morgantown, WV 26505 USA.
EM hakazemi@mix.wvu.edu; ssoleyma@mix.wvu.edu;
   fariborztaherkhani@gmail.com; seiranmanesh@mix.wvu.edu;
   nasser.nasrabadi@mail.wvu.edu
CR Alemi A. A., 2016, ARXIV161200410
   Almahairi A., 2018, ARXIV180210151
   Bansal A., 2017, ARXIV170805349
   Bousmalis K., 2017, UNSUPERVISED PIXEL L
   Chen Q., 2017, PHOTOGRAPHIC IMAGE S
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Heusel  M., 2017, P ADV NEUR INF PROC, P6626
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   IIZUKA S, 2016, ACM T GRAPHIC, V35
   Isola P., 2017, IEEE C COMP VIS PATT
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2013, ARXIV13126114
   Krizhevsky A, 2014, ARXIV14045997
   Larsen A. B. L., 2015, ARXIV151209300
   Ledig C., 2016, PHOTO REALISTIC SING
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Liu  M., 2017, ADV NEURAL INFORM PR, P700
   Oord  A.v.d., 2016, ARXIV160106759
   Peng CL, 2017, IEEE T CIRC SYST VID, V27, P288, DOI 10.1109/TCSVT.2015.2502861
   Rezende D. J., 2014, INT C MACH LEARN, V2
   Royer A., 2017, ARXIV171105139
   Sangkloy Patsorn, 2017, IEEE C COMP VIS PATT, V2
   Smolensky P., 1986, TECHNICAL REPORT
   Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414
   Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Zhang R., 2018, ARXIV180103924
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao J., 2016, ARXIV160903126
   Zhao S., 2017, ARXIV170602262
   Zhu J.-Y., 2017, ADV NEURAL INFORM PR, V30, P465
   Zhu J Y, 2017, ARXIV170310593
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
   Zohrizadeh F, 2018, IEEE WINT CONF APPL, P1470, DOI 10.1109/WACV.2018.00165
NR 39
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004086
DA 2019-06-15
ER

PT S
AU Kazemi, SM
   Poole, D
AF Kazemi, Seyed Mehran
   Poole, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI SimplE Embedding for Link Prediction in Knowledge Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at https ://github.com/Mehran-k/SimplE.
C1 [Kazemi, Seyed Mehran; Poole, David] Univ British Columbia, Vancouver, BC, Canada.
RP Kazemi, SM (reprint author), Univ British Columbia, Vancouver, BC, Canada.
EM smkazemi@cs.ubc.ca; poole@cs.ubc.ca
CR Abadi M., 2016, ARXIV160304467
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI DOI 10.1145/1376616.1376746
   Bordes A., 2013, ARXIV13047158
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Cybenko George, 1989, MATH CONTROL SIGNAL, V2, P183
   Das Rajarshi, 2017, NIPS WORKSH AKBC
   Dat Quoc Nguyen, 2017, ARXIV170308098
   De Raedt Luc, 2016, SYNTHESIS LECT ARTIF, V10, P1, DOI DOI 10.2200/S00692ED1V01Y201601AIM032
   Dettmers Tim, 2018, AAAI
   Ding Boyang, 2018, P 56 ANN M ASS COMP
   Dong X., 2014, P 20 ACM SIGKDD INT, P601, DOI DOI 10.1145/2623330.2623623
   Feng J., 2016, P 15 INT C PRINC KNO, P557
   Getoor L, 2007, INTRO STAT RELATIONA
   Guo Shu, 2016, P C EMP METH NAT LAN, P192
   Hayashi K., 2017, ARXIV170205563
   Hitchcock F. L., 1927, J MATH PHYS, V6, P164, DOI DOI 10.1002/SAPM192761164
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Ji G., 2015, P 53 ANN M ASS COMP, P687
   Kadlec Rudolf, 2017, ARXIV170510744
   Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8
   Lin Yankai, 2015, EMNLP
   Lin YC, 2015, ADV SOC SCI EDUC HUM, V39, P2181
   Liu Hanxiao, 2018, AAAI
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Minervini P, 2017, LECT NOTES ARTIF INT, V10534, P668, DOI 10.1007/978-3-319-71249-9_40
   Nguyen Dat Quoc, 2016, MARK
   Nickel M., 2012, P 21 INT C WORLD WID, P271, DOI DOI 10.1145/2187836.2187874
   Nickel M., 2014, ADV NEURAL INFORM PR, P1179
   Nickel M., 2016, P 30 AAAI C ART INT, P1955
   Nickel M., 2011, P 28 INT C MACH LEAR, P809
   Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592
   Rocktaschel Tim, 2014, P ACL 2014 WORKSH SE, P45
   Rocktaschel Tim, 2017, NIPS, P3791
   Santoro A, 2017, NIPS
   Schlichtkrull Michael, 2018, The Semantic Web. 15th International Conference, ESWC 2018. Proceedings: LNCS 10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Seyed Mehran Kazemi, 2018, AAAI
   Socher R., 2013, NIPS
   Trouillon  T., 2016, P 33 INT C MACH LEAR, P2071
   Trouillon Theo, 2017, ARXIV170701475
   Trouillon Theo, 2017, ARXIV170206879
   Wang Q, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1859
   Wang Q, 2017, IEEE T KNOWL DATA EN, V29, P2724, DOI 10.1109/TKDE.2017.2754499
   Wang Y., 2018, AAAI
   Wang Z., 2014, P 28 AAAI C ART INT, P1112
   Wei Zhuoyu, 2015, P 24 ACM INT C INF K, P1331
   Yang B., 2015, ICLR
   Zhang Hanwang, 2017, CVPR, V1, P5
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304031
DA 2019-06-15
ER

PT S
AU Ke, CY
   Honorio, J
AF Ke, Chuyang
   Honorio, Jean
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Information-theoretic Limits for Community Detection in Network Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We analyze the information-theoretic limits for the recovery of node labels in several network models. This includes the Stochastic Block Model, the Exponential Random Graph Model, the Latent Space Model, the Directed Preferential Attachment Model, and the Directed Small-world Model. For the Stochastic Block Model, the non-recoverability condition depends on the probabilities of having edges inside a community, and between different communities. For the Latent Space Model, the non-recoverability condition depends on the dimension of the latent space, and how far and spread are the communities in the latent space. For the Directed Preferential Attachment Model and the Directed Small-world Model, the non-recoverability condition depends on the ratio between homophily and neighborhood size. We also consider dynamic versions of the Stochastic Block Model and the Latent Space Model.
C1 [Ke, Chuyang; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
RP Ke, CY (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM cke@purdue.edu; jhonorio@purdue.edu
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Abbe Emmanuel, 2017, ARXIV170310146
   Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103
   Bandeira AS, 2018, FOUND COMPUT MATH, V18, P345, DOI 10.1007/s10208-016-9341-9
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5
   Cabreros Irineo, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P584, DOI 10.1109/CISS.2016.7460568
   Cheng Y, 2014, IEEE IMAGE PROC, P244, DOI 10.1109/ICIP.2014.7025048
   Cline MS, 2007, NAT PROTOC, V2, P2366, DOI 10.1038/nprot.2007.324
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Deshpande Y, 2016, IEEE INT SYMP INFO, P185, DOI 10.1109/ISIT.2016.7541286
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Goldenberg A, 2010, FOUND TRENDS MACH LE, V2, P129, DOI 10.1561/2200000005
   Hajek B, 2016, IEEE T INFORM THEORY, V62, P2788, DOI 10.1109/TIT.2016.2546280
   Heimlicher Simon, 2012, NIPS WORKSH ALG STAT
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   Jog Varun, 2015, IEEE ALL C COMM CONT
   Kim Bomin, 2017, ARXIV171110421
   Lang K. J., 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Mathai AM, 1992, QUADRATIC FORMS RAND
   Mossel E, 2012, ARXIV12021499
   Newman MEJ, 2002, P NATL ACAD SCI USA, V99, P2566, DOI 10.1073/pnas.012582999
   Rui Wu, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P449, DOI 10.1145/2745844.2745887
   Saad Hussein, 2017, IEEE ALL C COMM CONT
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Sarkar Purnamrita, 2006, ADV NEURAL INFORM PR, P1145
   Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112
   Wang W, 2010, IEEE INT SYMP INFO, P1373, DOI 10.1109/ISIT.2010.5513573
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Xu Jiaming, 2014, P 27 C LEARN THEOR J, P903
   Yu Bin, 1997, FESTSCHRIFT L LECAM, V423, P435
   Yun Se- Young, 2016, ADV NEUR INF PROC SY, P965
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002083
DA 2019-06-15
ER

PT S
AU Ke, NR
   Goyal, A
   Bilaniuk, O
   Binas, J
   Mozer, MC
   Pal, C
   Bengio, Y
AF Ke, Nan Rosemary
   Goyal, Anirudh
   Bilaniuk, Olexa
   Binas, Jonathan
   Mozer, Michael C.
   Pal, Chris
   Bengio, Yoshua
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sparse Attentive Backtracking: Temporal Credit Assignment Through
   Reminding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID HIPPOCAMPAL PLACE CELLS; REVERSE REPLAY; NETWORK; SIMILARITY; ALGORITHM
AB Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly longterm dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.
C1 [Ke, Nan Rosemary; Goyal, Anirudh; Bilaniuk, Olexa; Binas, Jonathan; Pal, Chris; Bengio, Yoshua] Univ Montreal, Mila, Montreal, PQ, Canada.
   [Ke, Nan Rosemary; Pal, Chris] Polytech Montreal, Mila, Montreal, PQ, Canada.
   [Mozer, Michael C.] Univ Colorado, Boulder, CO 80309 USA.
   [Pal, Chris] Element AI, Montreal, PQ, Canada.
   [Bengio, Yoshua] CIFAR, Toronto, ON, Canada.
RP Ke, NR (reprint author), Univ Montreal, Mila, Montreal, PQ, Canada.; Ke, NR (reprint author), Polytech Montreal, Mila, Montreal, PQ, Canada.
FU NSERC; CIFAR; Google; Samsung; SNSF; Nuance; IBM; Canada Research
   Chairs; National Science Foundation [EHR-1631428, SES-1461535]
FX The authors would like to thank Hugo Larochelle, Walter Senn, Alex Lamb,
   Remi Le Priol, Matthieu Courbariaux, Gaetan Marceau Caron, Sandeep
   Subramanian for the useful discussions, as well as NSERC, CIFAR, Google,
   Samsung, SNSF, Nuance, IBM, Canada Research Chairs, National Science
   Foundation awards EHR-1631428 and SES-1461535 for funding. We would also
   like to thank Compute Canada and NVIDIA for computing resources. The
   authors would also like to thank Alex Lamb for code review. The authors
   would also like to express debt of gratitude towards those who
   contributed to Theano over the years (now that it is being sunset), for
   making it such a great tool.
CR Ambrose RE, 2016, NEURON, V91, P1124, DOI 10.1016/j.neuron.2016.07.047
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Bahdanau D., 2014, ARXIV14090473
   Benjamin Aaron S, 2010, SUCCESSFUL REMEMBERI
   Berntsen D, 2013, J EXP PSYCHOL GEN, V142, P426, DOI 10.1037/a0029128
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chung J., 2016, ARXIV160901704
   Ciaramelli E, 2008, NEUROPSYCHOLOGIA, V46, P1828, DOI 10.1016/j.neuropsychologia.2008.03.022
   Cooijmans T., 2016, ARXIV160309025
   Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027
   ElHihi S, 1996, ADV NEUR IN, V8, P493
   FORBUS KD, 1995, COGNITIVE SCI, V19, P141, DOI 10.1207/s15516709cog1902_1
   Foster DJ, 2006, NATURE, V440, P680, DOI 10.1038/nature04587
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Gulcehre  C., 2017, ARXIV170108718
   Gupta AS, 2010, NEURON, V65, P695, DOI 10.1016/j.neuron.2010.01.034
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huang G, 2016, ARXIV160806993
   Kadar Akos, 2018, ARXIV180703595
   Ke Nan Rosemary, 2018, ARXIV180604342
   Kingma D. P., 2014, ARXIV14126980
   Koutnik J., 2014, ARXIV14023511
   Lee Dong-Hyun, 2014, CORR
   Lu Jiasen, 2017, P IEEE C COMP VIS PA, V6
   Luong M.T., 2015, ARXIV150804025
   Mahoney M., 2011, LARGE TEXT COMPRESSI
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790
   Mikolov Tomas, 2012, SUBWORD LANGUAGE MOD, V8
   Mozer Michael C, 2017, ARXIV171004110
   NOVICK LR, 1988, J EXP PSYCHOL LEARN, V14, P510, DOI 10.1037//0278-7393.14.3.510
   Ollivier Yann, 2015, ARXIV150707680
   READ SJ, 1991, J EXP SOC PSYCHOL, V27, P1, DOI 10.1016/0022-1031(91)90008-T
   Scellier Benjamin, 2016, CORR
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wharton CM, 1996, MEM COGNITION, V24, P629, DOI 10.3758/BF03201088
   Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949
   Williams RJ, 1990, NEURAL COMPUT, V2, P490, DOI 10.1162/neco.1990.2.4.490
NR 40
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002021
DA 2019-06-15
ER

PT S
AU Keskin, C
   Izadi, S
AF Keskin, Cem
   Izadi, Shahram
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI SplineNets: Continuous Neural Decision Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.
EM cemkeskin@google.com; shahrami@google.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Baek  Seungryul, 2017, ABS170602003 CORR
   Bicici Ufuk Can, 2018, PATT REC ICPR 2018 2
   Brabandere Bert De, 2016, ABS160509673 CORR
   Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18
   Chen X., 2016, ARXIV160603657
   Denoyer  Ludovic, 2014, ABS14100510 CORR
   Ha  David, 2016, ABS160909106 CORR
   Han Song, 2015, ABS151000149 CORR
   He K., 2015, ABS151203385 CORR
   Hinton G. E, 2012, ABS12070580 CORR
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Iandola F., 2016, ARXIV160207360
   Ioannou  Yani, 2016, ABS160301250 CORR
   Jaderberg  Max, 2015, ABS150602025 CORR
   Kingma D.P., 2013, ARXIV13126114
   Kontschieder  Peter, 2016, P 25 INT JOINT C ART, P4190
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Molchanov  Pavlo, 2016, ABS161106440 CORR
   Murdock C, 2017, PROC CVPR IEEE, P673, DOI 10.1109/CVPR.2017.79
   P. Lillicrap  Timothy, 2015, ABS150902971 CORR
   Rastegari  Mohammad, 2016, ABS160305279 CORR
   Sabour  Sara, 2017, ABS171009829 CORR
   Shazeer  Noam, 2017, ABS170106538 CORR
   Shotton J., 2013, P ADV NEUR INF PROC, P234
   Wu  Jiaxiang, 2015, ABS151206473 CORR
   Xiong C, 2015, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2015.418
   Yang  Tien-Ju, 2016, ABS161105128 CORR
   Zhao TT, 2017, IEEE PHOTONICS J, V9, P2, DOI 10.1109/JPHOT.2016.2644864
   Zhou  Aojun, 2017, ABS170203044 CORR
   Zhou  Shuchang, 2016, ABS160606160 CORR
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302004
DA 2019-06-15
ER

PT S
AU Khadka, S
   Tumer, K
AF Khadka, Shauharda
   Tumer, Kagan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Evolution-Guided Policy Gradient in Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NEURAL-NETWORKS; NEUROEVOLUTION
AB Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.
C1 [Khadka, Shauharda; Tumer, Kagan] Oregon State Univ, Collaborat Robot & Intelligent Syst Inst, Corvallis, OR 97331 USA.
RP Khadka, S (reprint author), Oregon State Univ, Collaborat Robot & Intelligent Syst Inst, Corvallis, OR 97331 USA.
EM khadkas@oregonstate.edu; kagan.tumer@oregonstate.edu
CR Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Ackley D., 1991, SFI STUDIES SCI COMP, V10, P487
   Ahn CW, 2003, IEEE T EVOLUT COMPUT, V7, P367, DOI 10.1109/TEVC.2003.814633
   Andrychowicz M, 2017, ADV NEURAL INFORM PR, P5048
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Brockman G, 2016, ARXIV160601540
   Colas C., 2018, ARXIV180205054
   Conti E., 2017, ARXIV171206560
   Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422
   DeAsis K., 2017, ARXIV170301327
   Dhariwal P., 2017, OPENAI BASELINES
   Drugan M. M., 2018, SWARM EVOLUTIONARY C
   Duan Y., 2016, INT C MACH LEARN, P1329
   Espeholt Lasse, 2018, ARXIV180201561
   Eysenbach Benjamin, 2018, ARXIV180206070
   Fernando C, 2017, ARXIV170108734
   Fernando C, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P109, DOI 10.1145/2908812.2908890
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Fogel DB, 2006, EVOLUTIONARY COMPUTATION: TOWARD A NEW PHILOSOPHY OF MACHINE INTELLIGENCE, 3RD EDITION, P1, DOI 10.1002/0471749214
   Fortunato M., 2017, ARXIV170610295
   Fujimoto Scott, 2018, ARXIV180209477
   Gangwani T., 2017, ARXIV171101012
   Gu S., 2017, NEURAL INFORM PROCES, P3849
   Haarnoja T., 2018, ARXIV180101290
   Harutyunyan A., 2016, ALG LEARN THEOR 27 I, P305
   Henderson P., 2017, ARXIV170906560
   Islam R., 2017, ARXIV170804133
   Jaderberg M., 2017, ARXIV171109846
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lehman J., 2008, ALIFE, P329
   Lillicrap T P, 2015, ARXIV150902971
   Liu H, 2017, ARXIV171100436
   Luders B, 2017, LECT NOTES COMPUT SC, V10199, P886, DOI 10.1007/978-3-319-55849-3_57
   Maei H. R., 2009, ADV NEURAL INFORM PR, P1204
   Mahmood A. R., 2017, ARXIV170203006
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ostrovski Georg, 2017, ARXIV170301310
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pathak Deepak, 2017, INT C MACH LEARN ICM, V2017
   Plappert M., 2017, ARXIV170601905
   Pugh JK, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00040
   Risi S, 2017, IEEE T COMP INTEL AI, V9, P25, DOI 10.1109/TCIAIG.2015.2494596
   Salimans T., 2017, ARXIV170303864
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman J., 2015, ARXIV150602438
   Schulman  J., 2017, ARXIV170706347
   Sherstan C., 2018, ARXIV180108287
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spears W. M., 1993, Machine Learning: ECML-93. European Conference on Machine Learning Proceedings, P442
   Stafylopatis A, 1998, EUR J OPER RES, V108, P306, DOI 10.1016/S0377-2217(97)00372-X
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Such F. P., 2017, ARXIV171206567
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tang Haoran, 2017, ADV NEURAL INFORM PR, P2750
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Turney P, 1996, EVOL COMPUT, V4, pIV, DOI 10.1162/evco.1996.4.3.iv
   Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823
   Wang Z., 2016, ARXIV161101224
   Whiteson S, 2006, J MACH LEARN RES, V7, P877
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301020
DA 2019-06-15
ER

PT S
AU Khan, H
   Yener, B
AF Khan, Haidar
   Yener, Bulent
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning filter widths of spectral decompositions with wavelets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DEEP NEURAL-NETWORKS; SIGNAL
AB Time series classification using deep neural networks, such as convolutional neural networks (CNN), operate on the spectral decomposition of the time series computed using a preprocessing step. This step can include a large number of hyperparameters, such as window length, filter widths, and filter shapes, each with a range of possible values that must be chosen using time and data intensive cross-validation procedures. We propose the wavelet deconvolution (WD) layer as an efficient alternative to this preprocessing step that eliminates a significant number of hyperparameters. The WD layer uses wavelet functions with adjustable scale parameters to learn the spectral decomposition directly from the signal. Using backpropagation, we show the scale parameters can be optimized with gradient descent. Furthermore, the WD layer adds interpretability to the learned time series classifier by exploiting the properties of the wavelet transform. In our experiments, we show that the WD layer can automatically extract the frequency content used to generate a dataset. The WD layer combined with a CNN applied to the phone recognition task on the TIMIT database achieves a phone error rate of 18.1%, a relative improvement of 4% over the baseline CNN. Experiments on a dataset where engineered features are not available showed WD+CNN is the best performing method. Our results show that the WD layer can improve neural network based time series classifiers both in accuracy and interpretability by learning directly from the input signal.
C1 [Khan, Haidar; Yener, Bulent] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.
RP Khan, H (reprint author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.
EM khanh2@rpi.edu; yener@rpi.edu
FU NSF [1302231]
FX This work was supported in part by NSF Award #1302231.
CR Abadi M., 2016, ARXIV160304467
   Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Acar E, 2007, BIOINFORMATICS, V23, pI10, DOI 10.1093/bioinformatics/btm210
   Al-Naymat G., 2009, AUSTRALASIAN DATA MI, P117
   Bagnall A, 2016, PROC INT CONF DATA, P1548, DOI 10.1109/ICDE.2016.7498418
   Brockwell P., 2002, INTRO TIME SERIES FO
   Chen Y., 2015, UCR TIME SERIES CLAS
   Chollet F., 2015, KERAS
   DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gosztolyal G, 2015, INT CONF ACOUST SPEE, P4570, DOI 10.1109/ICASSP.2015.7178836
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang NE, 2008, REV GEOPHYS, V46, DOI 10.1029/2007RG000228
   Jaitly N, 2011, INT CONF ACOUST SPEE, P5884
   Khan H, 2017, IEEE T BIOMEDICAL EN, VPP, P1
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mirowski P, 2009, CLIN NEUROPHYSIOL, V120, P1927, DOI 10.1016/j.clinph.2009.09.002
   Mirowski PW, 2008, MACHINE LEARN SIGN P, P244, DOI 10.1109/MLSP.2008.4685487
   Mohamed A.-R., 2009, NIPS WORKSH DEEP LEA, P1, DOI DOI 10.4249/SCHOLARPEDIA.5947
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oppenheim A. V., 2014, DISCRETE TIME SIGNAL
   Palaz D, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P11
   Palaz Dimitri, 2013, ARXIV13122137, P2
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587
   Polikar Robi, 1996, ENG ULTIMATE GUIDE W, V14, P81
   Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1
   Sainath TN, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P297, DOI 10.1109/ASRU.2013.6707746
   Schafer P, 2015, DATA MIN KNOWL DISC, V29, P1505, DOI 10.1007/s10618-014-0377-7
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stober Sebastian, 2015, DEEP FEATURE LEARNIN, P1
   Toth L, 2014, INTERSPEECH, P1078
   Toth L, 2013, INT CONF ACOUST SPEE, P6985, DOI 10.1109/ICASSP.2013.6639016
   Tuske Z, 2014, INTERSPEECH, P890
   Wang ZG, 2017, IEEE IJCNN, P1578, DOI 10.1109/IJCNN.2017.7966039
   Yu Dong, 2015, AUTOMATIC SPEECH REC, V1976
   Zeiler M. D, 2012, ADADELTA ADAPTIVE LE, P6
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304060
DA 2019-06-15
ER

PT S
AU Khayatkhoei, M
   Elgammal, A
   Singh, M
AF Khayatkhoei, Mahyar
   Elgammal, Ahmed
   Singh, Maneesh
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Disconnected Manifold Learning for Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.
C1 [Khayatkhoei, Mahyar; Elgammal, Ahmed] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.
   [Singh, Maneesh] Verisk Analyt, Jersey City, NJ USA.
RP Khayatkhoei, M (reprint author), Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.
EM m.khayatkhoei@cs.rutgers.edu; elgammal@cs.rutgers.edu;
   maneesh.singh@verisk.com
FU Verisk Analytics; NSF-USA [1409683]
FX This work was supported by Verisk Analytics and NSF-USA award number
   1409683.
CR Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Arjovsky M., 2017, ARXIV170104862
   Arora S., 2017, ARXIV170300573
   Barratt S, 2018, ARXIV180101973
   Che T., 2016, ARXIV161202136
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Ghosh Arnab, 2017, ARXIV170402906
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Gurumurthy Swaminathan, 2017, IEEE C COMP VIS PATT, V1
   Heusel M., 2017, ADV NEURAL INFORM PR, P6629
   Kelley John L, 2017, GEN TOPOLOGY
   Liu  Z., 2015, P INT C COMP VIS ICC
   Lucic M, 2017, ARXIV171110337
   Mescheder Lars, 2018, ARXIV180104406
   Metz L., 2016, ARXIV161102163
   Nagarajan Vaishnavh, 2017, ADV NEURAL INFORM PR, P5591
   Quan Hoang, 2018, INT C LEARN REPR
   Radford  A., 2015, ARXIV151106434
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Srivastava A., 2017, ADV NEURAL INFORM PR, P3310
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Yang MH, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P224
   Yann LeCun, 1998, MNIST DATABASE HANDW
   Yu F., 2015, ARXIV150603365
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001086
DA 2019-06-15
ER

PT S
AU Khoshaman, AH
   Amin, MH
AF Khoshaman, Amir H.
   Amin, Mohammad H.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI GumBolt: Extending Gumbel trick to Boltzmann priors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Boltzmann machines (BMs) are appealing candidates for powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, non-differentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-theart performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables. Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous models.
C1 [Khoshaman, Amir H.] D Wave Syst Inc, Burnaby, BC, Canada.
   [Amin, Mohammad H.] Simon Fraser Univ, D Wave Syst Inc, Burnaby, BC, Canada.
   [Khoshaman, Amir H.] Borealis AI, Edmonton, AB, Canada.
RP Khoshaman, AH (reprint author), D Wave Syst Inc, Burnaby, BC, Canada.; Khoshaman, AH (reprint author), Borealis AI, Edmonton, AB, Canada.
EM khoshaman@gmail.com; mhsamin@dwavesys.com
CR Amin M. H., 2016, QUANTUM BOLTZMANN MA
   Bengio Y., 2013, ARXIV13083432
   BENNETT CH, 1976, J COMPUT PHYS, V22, P245, DOI 10.1016/0021-9991(76)90078-4
   Bishop C. M., 2011, PATTERN RECOGNITION
   Burda Y., 2015, ARXIV150900519
   Chen X., 2016, ARXIV161102731
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Germain M., 2015, ICML, V37, P881
   GOYAL Anirudh Goyal ALIAS PARTH, 2017, ADV NEURAL INFORM PR, P6716
   Grathwohl Will, 2017, ARXIV171100123
   Gregor K., 2013, ARXIV13108499
   Gregor K., 2015, ARXIV150204623
   Gu Shixiang, 2015, ARXIV151105176
   Gulrajani Ishaan, 2016, ARXIV161105013
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jang Eric, 2016, ARXIV161101144
   Khoshaman A, 2019, QUANTUM SCI TECHNOL, V4, DOI 10.1088/2058-9565/aada1f
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2013, ARXIV13126114
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Maaloe L., 2017, ARXIV170400637
   Maddison Chris J, 2016, ARXIV161100712
   Mnih A, 2016, INT C MACH LEARN, P2188
   Mnih Andriy, 2014, ARXIV14020030
   Oord A. v. d., 2016, ARXIV160106759
   Raiko T., 2014, ARXIV14062989
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Rolfe J. T., 2016, ARXIV160902200
   Ross S. M., 2013, APPL PROBABILITY MOD
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   Serra J., 2018, ARXIV180101423
   Shirts MR, 2008, J CHEM PHYS, V129, DOI 10.1063/1.2978177
   Sonderby C. K., 2016, ADV NEURAL INFORM PR, P3738
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tucker G., 2017, ADV NEURAL INFORM PR, P2624
   Vahdat A., 2018, ARXIV180204920
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
   Yeung S., 2017, ARXIV170603643
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304010
DA 2019-06-15
ER

PT S
AU Kim, H
   Jiang, YH
   Kannan, S
   Oh, S
   Viswanath, P
AF Kim, Hyeji
   Jiang, Yihan
   Kannan, Sreeram
   Oh, Sewoong
   Viswanath, Pramod
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Deepcode: Feedback Codes via Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ADDITIVE NOISE CHANNELS; CODING SCHEME; AWGN CHANNEL
AB The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide-ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed.
   We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel-specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research.
C1 [Kim, Hyeji] Samsung AI Ctr Cambridge, Cambridge, England.
   [Jiang, Yihan; Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Champaign, IL USA.
   [Oh, Sewoong] UIUC, Dept Ind & Enterprise Syst Engn, Champaign, IL USA.
   [Viswanath, Pramod] UIUC, Dept Elect Engn, Champaign, IL USA.
RP Kim, H (reprint author), Samsung AI Ctr Cambridge, Cambridge, England.
EM hkim1505@gmail.com; yihanrogerjiang@gmail.com; ksreeram@uw.edu;
   swoh@illinois.edu; pramodv@illinois.edu
FU National Science Foundation [CCF-1553452, RI-1815535]; Army Research
   Office [W911NF-18-1-0384]; Amazon Catalyst award; NSF [1651236, 1703403]
FX We thank Shrinivas Kudekar and Saurabh Tavildar for helpful discussions
   and providing references to the state-of-the-art feedforward codes. We
   thank Dina Katabi for a detailed discussion that prompted work on system
   implementation. This work is in part supported by National Science
   Foundation awards CCF-1553452 and RI-1815535, Army Research Office under
   grant number W911NF-18-1-0384, and Amazon Catalyst award. Y. Jiang and
   S. Kannan would also like to acknowledge NSF awards 1651236 and 1703403.
CR Ben-Yishai A, 2017, IEEE T INFORM THEORY, V63, P2409, DOI 10.1109/TIT.2017.2648821
   Cammerer S., 2018, INT ZUR SEM INF COMM, P51
   Chance Z, 2011, IEEE T INFORM THEORY, V57, P6633, DOI 10.1109/TIT.2011.2165796
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Duman T. M., 1997, Proceeding. 1997 IEEE International Symposium on Information Theory (Cat. No.97CH36074), DOI 10.1109/ISIT.1997.613019
   Elias P., 1955, IRE CONV REC, P37
   Erseghe T., 2014, IEEE T INFORM THEORY, V61
   Farsad N., 2018, IEEE INT C AC SPEECH
   Farsad N, 2018, IEEE T SIGNAL PROCES, V66, P5663, DOI 10.1109/TSP.2018.2868322
   Felix A., 2018, ARXIV180305815
   Gallager RG, 2010, IEEE T INFORM THEORY, V56, P6, DOI 10.1109/TIT.2009.2034896
   He H., 2018, CORR
   Hinton G., 2015, ARXIV150302531
   Huawei H., 2016, 87 3GPP TSGRAN WGI
   Il C. J., 2010, P 16 ANN INT C MOB C
   Kim H., 2018, INT C REPR LEARN ICL
   Kim YH, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P1416, DOI 10.1109/ISIT.2007.4557421
   Kimura D, 2017, IEEE WCNC
   Kosaian J., 2018, ARXIV180601259
   Lapidoth A, 1996, IEEE T INFORM THEORY, V42, P1520, DOI 10.1109/18.532892
   Miwa K., 2012, IEICE TECHNICAL REPO, P1
   Nachmani E., 2018, IEEE J SELECTED TOPI
   Nachmani E, 2016, ANN ALLERTON CONF, P341, DOI 10.1109/ALLERTON.2016.7852251
   O'Shea T. J., 2017, ARXIV170200832
   O'Shea T. J., 2017, CORR
   O'Shea TJ, 2016, IEEE INT SYMP SIGNAL, P223, DOI 10.1109/ISSPIT.2016.7886039
   Polyanskiy Y, 2010, IEEE T INFORM THEORY, V56, P2307, DOI 10.1109/TIT.2010.2043769
   Qi H., 2009, 2009 INT C WIR COMM, P1
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   SCHALKWIJK JP, 1966, IEEE T INFORM THEORY, V12, P183, DOI 10.1109/TIT.1966.1053880
   SCHALKWIJK JP, 1966, IEEE T INFORM THEORY, V12, P172, DOI 10.1109/TIT.1966.1053879
   Seo J, 2018, INT CONF COMPUT NETW, P238, DOI 10.1109/ICCNC.2018.8390279
   SHANNON CE, 1956, IRE T INFORM THEOR, V2, P8, DOI 10.1109/TIT.1956.1056798
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Sun H., 2018, IEEE T SIGNAL PROCES
   Tan  X., 2018, ARXIV180401002
   Zhao J, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P813, DOI 10.1109/IAEAC.2017.8054128
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004004
DA 2019-06-15
ER

PT S
AU Kim, J
   Park, S
   Kwak, N
AF Kim, Jangho
   Park, SeongUk
   Kwak, Nojun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Paraphrasing Complex Network: Network Compression via Factor Transfer
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID AUTOENCODERS
AB Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.
C1 [Kim, Jangho; Park, SeongUk; Kwak, Nojun] Seoul Natl Univ, Seoul, South Korea.
RP Kim, J (reprint author), Seoul Natl Univ, Seoul, South Korea.
EM kjh91@snu.ac.kr; swpark0703@snu.ac.kr; nojunk@snu.ac.kr
FU Next-Generation Information Computing Development Program through the
   NRF of Korea [2017M3C4A7077582]; ICT R&D program of MSIP/IITP, Korean
   Government [2017-0-00306]
FX This work was supported by Next-Generation Information Computing
   Development Program through the NRF of Korea (2017M3C4A7077582) and ICT
   R&D program of MSIP/IITP, Korean Government (2017-0-00306).
CR Alain G, 2014, J MACH LEARN RES, V15, P3563
   Baker B., 2016, ARXIV161102167
   Courbariaux M, 2016, ARXIV160202830
   Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, P3123
   Everingham M., PASCAL VISUAL OBJECT
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   Han S., 2015, ARXIV151000149
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, ARXIV150302531
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Howard Andrew G., 2017, ARXIV170404861
   Huang Gao, 2017, CORR
   Iandola F., 2016, ARXIV160207360
   Krizhevsky Alex, CIFAR10
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Ng WWY, 2016, PATTERN RECOGN, V60, P875, DOI 10.1016/j.patcog.2016.06.013
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero Adriana, 2014, ARXIV14126550
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srinivas S, 2015, ARXIV150706149
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Yim Junho, 2017, IEEE C COMP VIS PATT
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zagoruyko S., 2016, ARXIV161203928
   Zagoruyko S, 2016, ARXIV160507146
   Zhao J., 2016, ARXIV160903126
   Zoph B., 2016, ARXIV161101578
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302075
DA 2019-06-15
ER

PT S
AU Kim, JH
   Jun, J
   Zhang, BT
AF Kim, Jin-Hwa
   Jun, Jaehyun
   Zhang, Byoung-Tak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Bilinear Attention Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.
C1 [Kim, Jin-Hwa] SK T Brain, Seoul, South Korea.
   [Kim, Jin-Hwa; Jun, Jaehyun; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea.
   [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea.
RP Kim, JH (reprint author), SK T Brain, Seoul, South Korea.
EM jnhwkim@sktbrain.com; jhjun@bi.snu.ac.kr; btzhang@bi.snu.ac.kr
FU College of Humanities, Seoul National University; Korea government
   [IITP-2017-0-01772-VTT, IITP-R0126-16-1072-SW.StarLab, 2018-0-00622-RMI,
   KEIT-10060086-RISF]
FX We would like to thank Kyoung-Woon On, Bohyung Han, Hyeonwoo Noh,
   Sungeun Hong, Jaesun Park, and Yongseok Choi for helpful comments and
   discussion. Jin-Hwa Kim was supported by 2017 Google Ph.D. Fellowship in
   Machine Learning and Ph.D. Completion Scholarship from College of
   Humanities, Seoul National University. This work was funded by the Korea
   government (IITP-2017-0-01772-VTT, IITP-R0126-16-1072-SW.StarLab,
   2018-0-00622-RMI, KEIT-10060086-RISF). The part of computing resources
   used in this study was generously shared by Standigm Inc.
CR Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6
   Anderson P, 2017, ARXIV170707998
   Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Cadene Remi, 2017, IEEE INT C COMP VIS, P2612
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Fukui A., 2016, ARXIV160601847
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goyal Yash, 2017, IEEE C COMP VIS PATT
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hinami Ryota, 2017, ARXIV171109509
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Ilievski Ilija, 2017, SIMPLE LOSS FUNCTION
   Jaderberg M, 2015, ADV NEURAL INFORM PR, P2008
   Kim J.H., 2016, ADV NEURAL INFORM PR, P361
   Kim Jin-Hwa, 2017, 5 INT C LEARN REPR
   Kingma D. P., 2015, INT C LEARN REPR
   Krishna R., 2016, ARXIV160207332
   Lu Jiasen, 2016, ARXIV160600061
   Mantling Christopher, 2014, P 2014 C EMP METH NA
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nam H., 2016, IEEE C COMP VIS PATT
   Pirsiavash H., 2009, ADV NEURAL INFORM PR, V22, P1482
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Redmon Joseph, 2017, IEEE COMPUTER VISION
   Ren S., 2017, IEEE T PATTERN ANAL, V39
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Salimans T., 2016, ARXIV160207868
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Teney D., 2017, ARXIV170802711
   Trott Alexander, 2018, INT C LEARN REPR
   Veit A., 2016, ADV NEURAL INFORM PR, P550
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang MZ, 2016, LECT NOTES COMPUT SC, V9912, P696, DOI 10.1007/978-3-319-46484-8_42
   Wang Peng, 2017, P IEEE C COMP VIS PA, P1173
   Wolf Lior, 2007, IEEE C COMP VIS PATT
   Xu Huijuan, 2016, EUR C COMP VIS
   Yeh RA, 2017, ADV NEUR IN, V30
   Young P., 2014, P TACL, V2, P67
   Yu Zhou, 2018, IEEE T NEURAL NETWOR
   Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33
   Zhang Yan, 2018, INT C LEARN REPR
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301054
DA 2019-06-15
ER

PT S
AU Kim, MP
   Reingold, O
   Rothblum, GN
AF Kim, Michael P.
   Reingold, Omer
   Rothblum, Guy N.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fairness Through Computationally-Bounded Awareness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of fair classification within the versatile framework of Dwork et al. [6], which assumes the existence of a metric that measures similarity between pairs of individuals. Unlike earlier work, we do not assume that the entire metric is known to the learning algorithm; instead, the learner can query this arbitrary metric a bounded number of times. We propose a new notion of fairness called metric multifairness and show how to achieve this notion in our setting. Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) "comparison sets" over pairs of individuals. At a high level, metric multifairness guarantees that similar subpopulations are treated similarly, as long as these subpopulations are identified within the class C.
C1 [Kim, Michael P.; Reingold, Omer] Stanford Univ, Stanford, CA 94305 USA.
   [Rothblum, Guy N.] Weizmann Inst Sci, Rehovot, Israel.
RP Kim, MP (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM mpk@cs.stanford.edu; reingold@stanford.edu; rothblum@alum.mit.edu
CR Angwin J, 2016, PROPUBLICA
   Bogdanov A, 2017, INFORM SEC CRYPT TEX, P79, DOI 10.1007/978-3-319-57048-8_3
   Buolamwini J, 2018, C FAIRN ACC TRANSP, V81, P77
   Chouldechova Alexandra, 2017, BIG DATA
   Corbett-Davies Sam, 2017, KDD
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094
   Feldman Vitaly, 2010, P 1 S INN COMP SCI 1
   Feller Avi, 2016, WASHINGTON POST
   Gillen Stephen, 2018, ARXIV180206936
   Goldreich O., 1984, 25th Annual Symposium on Foundations of Computer Science (Cat. No. 84CH2085-9), P464, DOI 10.1109/SFCS.1984.715949
   Gopalan P, 2008, ACM S THEORY COMPUT, P527
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Hebert-Johnson Ursula, 2018, ICML
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Kearns Michael, 2018, ICML
   KEARNS MJ, 1994, MACH LEARN, V17, P115
   Kim Michael P., 2018, ARXIV180512317
   Kleinberg Jon, 2017, ITCS
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Pleiss Geoff, 2017, NIPS
   Rothblum Guy N., 2018, ICML
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Waddell K, 2016, ATLANTIC
   Woodworth Blake, 2017, COLT
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304082
DA 2019-06-15
ER

PT S
AU Kim, S
   Lin, S
   Jeon, S
   Min, D
   Sohn, K
AF Kim, Seungryong
   Lin, Stephen
   Jeon, Sangryul
   Min, Dongbo
   Sohn, Kwanghoon
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Recurrent Transformer Networks for Semantic Correspondence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.
C1 [Kim, Seungryong; Jeon, Sangryul; Sohn, Kwanghoon] Yonsei Univ, Seoul, South Korea.
   [Lin, Stephen] Microsoft Res, Beijing, Peoples R China.
   [Min, Dongbo] Ewha Womans Univ, Seoul, South Korea.
RP Sohn, K (reprint author), Yonsei Univ, Seoul, South Korea.
EM srkim89@yonsei.ac.kr; stevelin@microsoft.com; cheonjsr@yonsei.ac.kr;
   dbmin@ewha.ac.kr; khsohn@yonsei.ac.kr
FU Next-Generation Information Computing Development Program through the
   National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT [NRF-2017M3C4A7069370]
FX This research was supported by Next-Generation Information Computing
   Development Program through the National Research Foundation of Korea
   (NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370).
CR Barnes Connelly, 2010, ECCV
   Bourdev L., 2009, ICCV
   Bristow H., 2015, ICCV
   Butler D., 2012, ECCV
   Choy C., 2016, NIPS
   HaCohen Y., 2011, SIGGRAPH
   Ham B., 2017, IEEE T PAMI
   Ham B., 2016, CVPR
   Han K., 2017, ICCV
   Hariharan B., 2011, ICCV
   Hassner T., 2012, CVPR
   He K., 2016, CVPR
   Hur J., 2015, CVPR
   Jaderberg M., 2015, NIPS
   Kanazawa A., 2016, CVPR
   Kim J., 2013, CVPR
   Kim S., 2018, IEEE T PAMI
   Kim S., 2017, ICCV
   Kim S., 2018, TPAMI
   Kim S., 2017, CVPR
   Liao J., 2017, SIGGRAPH
   Lin Chen- Hsuan, 2017, CVPR
   Lin Y. L., 2014, ECCV
   Liu C., 2011, IEEE T PAMI, V33, P815
   Long J.L., 2014, NIPS
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Novotny D., 2017, CVPR
   Philbin James, 2007, CVPR
   Qiu W., 2014, WACV
   Rocco I., 2017, CVPR
   Rocco I., 2018, CVPR
   Ronneberger O., 2015, MICCAI
   Rubinstein M., 2013, CVPR
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Simonyan Karen, 2015, ICLR
   Taniai T., 2016, CVPR
   Thewlis J., 2017, NIPS
   Ufer N., 2017, CVPR
   Yang F., 2017, CVPR
   Yang H., 2014, CVPR
   Yi K. M., 2016, ECCV
   Yi K. M., 2016, CVPR
   Yu F., 2016, ICLR
   Zhou T., 2015, CVPR
   Zhou T., 2016, CVPR
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000061
DA 2019-06-15
ER

PT S
AU Kingma, DP
   Dhariwal, P
AF Kingma, Diederik P.
   Dhariwal, Prafulla
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Glow: Generative Flow with Invertible 1 x 1 Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 x 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openi/glow.
C1 [Kingma, Diederik P.; Dhariwal, Prafulla] OpenAI, San Francisco, CA 94110 USA.
   [Kingma, Diederik P.] Google AI, San Francisco, CA USA.
RP Kingma, DP (reprint author), OpenAI, San Francisco, CA 94110 USA.; Kingma, DP (reprint author), Google AI, San Francisco, CA USA.
CR Deco G., 1995, Advances in Neural Information Processing Systems 7, P247
   Dinh L., 2016, ARXIV160508803
   Dinh L., 2014, ARXIV14108516
   Gomez A. N., 2017, ADV NEURAL INFORM PR, P2211
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves A, 2013, ARXIV13080850
   Grover A., 2018, AAAI C ART INT
   He K., 2016, ARXIV160305027
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ioffe S., 2015, ARXIV150203167
   Karras Tero, 2017, ARXIV171010196
   Kingma D., 2015, P INT C LEARN REPR 2
   Kingma D. P., 2018, VARIATIONAL AUTOENCO
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Oord A. v. d., 2016, ARXIV160106759
   Oord A. V. D., 2017, ARXIV171110433
   Oord  A.v.d., 2016, ARXIV160106759
   Papamakarios G., 2017, ADV NEURAL INFORM PR, P2335
   Parmar N., 2018, ARXIV180205751
   Reed S., 2017, ARXIV170303664
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salimans T., 2016, ARXIV160207868
   Salimans T., 2017, GRADIENT CHECKPOINTI
   Van Den Oord A., 2016, ARXIV160903499
   van den Oord A, 2016, ARXIV160605328
   Yu F., 2015, ARXIV150603365
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004074
DA 2019-06-15
ER

PT S
AU Kirsch, L
   Kunze, J
   Barber, D
AF Kirsch, Louis
   Kunze, Julius
   Barber, David
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Modular Networks: Learning to Decompose Neural Computation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MIXTURES
AB Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.
C1 [Kirsch, Louis; Kunze, Julius; Barber, David] UCL, Dept Comp Sci, London, England.
RP Kirsch, L (reprint author), UCL, Dept Comp Sci, London, England.
EM mail@louiskirsch.com; juliuskunze@gmail.com; david.barber@ucl.ac.uk
FU Alan Turing Institute under the EPSRC [EP/N510129/1]; ERC Advanced Grant
   [742870]
FX We thank Ilya Feige, Hippolyt Ritter, Tianlin Xu, Raza Habib, Alex
   Mansbridge, Roberto Fierimonte, and our anonymous reviewers for their
   feedback. This work was supported by the Alan Turing Institute under the
   EPSRC grant EP/N510129/1. Furthermore, we thank IDSIA (The Swiss AI Lab)
   for the opportunity to finalize the camera ready version on their
   premises, partially funded by the ERC Advanced Grant (no: 742870).
CR Amodei D., 2015, ICML
   Bengio  E., 2016, ICLR WORKSH
   Bengio  E., 2017, THESIS
   Bengio Y., 2013, ARXIV13083432
   Cho  K., 2014, C EMP METH NAT LANG
   Ciresan D. C., 2012, IEEE C COMP VIS PATT
   Clavera I, 2017, IEEE INT C INT ROBOT, P1537, DOI 10.1109/IROS.2017.8205959
   Clune J, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2863
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Eigen  D., 2013, ICLR WORKSH
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Legenstein RA, 2002, THEOR COMPUT SCI, V287, P239, DOI 10.1016/S0304-3975(02)00097-X
   Li  L., 2008, INT C NAT LANG PROC
   Neal R., 1999, LEARNING GRAPHICAL M, P355
   Neklyudov  K., 2017, ADV NEURAL INFORM PR, P6775
   Pakkenberg B, 2003, EXP GERONTOL, V38, P95, DOI 10.1016/S0531-5565(02)00151-1
   Ramezani M, 2015, IEEE T MED IMAGING, V34, P2, DOI 10.1109/TMI.2014.2340816
   Rosenbaum  C., 2018, ICLR
   Sahni  H., 2017, NIPS WORKSH
   SCHMIDHUBER J, 1989, CONNECTIONISM IN PERSPECTIVE, P439
   Schmidhuber  J., 2012, ARXIV12100118
   Shazeer N., 2017, ICLR
   Sporns O, 2016, ANNU REV PSYCHOL, V67, P613, DOI 10.1146/annurev-psych-122414-033634
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Sternberg S, 2011, COGN NEUROPSYCHOL, V28, P156, DOI 10.1080/02643294.2011.557231
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302042
DA 2019-06-15
ER

PT S
AU Klys, J
   Snell, J
   Zemel, R
AF Klys, Jack
   Snell, Jake
   Zemel, Richard
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Latent Subspaces in Variational Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Variational autoencoders (VAEs) [10, 20] are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face [23] and CelebA [15] datasets.
C1 [Klys, Jack; Snell, Jake; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP Klys, J (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM jackklys@cs.toronto.edu; jsnell@cs.toronto.edu; zemel@cs.toronto.edu
FU Samsung; Natural Sciences and Engineering Research Council of Canada
FX We would like to thank Sageev Oore for helpful discussions. This
   research was supported by Samsung and the Natural Sciences and
   Engineering Research Council of Canada.
CR Bowman S. R., 2015, ARXIV151106349
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Creswell Antonia, 2017, ARXIV171105175
   Edwards Harrison, 2015, ARXIV151105897
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gomez-Bombarelli Rafael, 2016, ACS CENTRAL SCI
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani Ishaan, 2016, ARXIV161105013
   Higgins I., 2017, INT C LEARN REPR
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Lample G., 2017, NEURIPS, P5969
   Larsen A. B. L., 2015, ARXIV151209300
   Liu  Z., 2015, P INT C COMP VIS ICC
   Louizos  C., 2015, ARXIV151100830
   Mathieu Michael, 2016, DISENTANGLING FACTOR
   Paszke A., 2017, NIPS W
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rezende D. J, 2014, ARXIV14014082
   Sohn  K., 2015, ADV NEURAL INFORM PR, P3483
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Susskind Josh M, 2010, TECHNICAL REPORT, P3
   Xiao Taihong, 2018, INT C LEARN REPR WOR
   Xiao Taihong, 2018, ARXIV180310562
   Zhou Shuchang, 2017, P BRIT MACH VIS C BM
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001002
DA 2019-06-15
ER

PT S
AU Knoblauch, J
   Jewson, J
   Damoulas, T
AF Knoblauch, Jeremias
   Jewson, Jack
   Damoulas, Theodoros
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with
   beta-Divergences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present the first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with beta-divergences. The resulting inference procedure is doubly robust for both the parameter and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as beta -> 0. Secondly, we give a principled way of choosing the divergence parameter beta by minimizing expected predictive loss on-line. Reducing False Discovery Rates of CPs from over 90% to 0% on real world data, this offers the state of the art.
C1 [Knoblauch, Jeremias; Damoulas, Theodoros] Univ Warwick, Alan Turing Inst, Dept Stat, Coventry CV4 7AL, W Midlands, England.
   [Jewson, Jack] Univ Warwick, Dept Stat, Coventry CV4 7AL, W Midlands, England.
   [Damoulas, Theodoros] Univ Warwick, Alan Turing Inst, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
RP Knoblauch, J (reprint author), Univ Warwick, Alan Turing Inst, Dept Stat, Coventry CV4 7AL, W Midlands, England.
EM j.knoblauch@warwick.ac.uk; j.e.jewson@warwick.ac.uk;
   t.damoulas@warwick.ac.uk
FU EPSRC as part of the Oxford-Warwick Statistics Programme (OXWASP)
   [EP/L016710/1]; Lloyds Register Foundation programme on Data Centric
   Engineering through the London Air Quality project; Alan Turing
   Institute for Data Science and AI under EPSRC [EP/N510129/1]
FX We would like to cordially thank both Jim Smith and Chris Holmes for
   fruitful discussions and help with some of the theoretical results. JK
   and JJ are funded by EPSRC grant EP/L016710/1 as part of the
   Oxford-Warwick Statistics Programme (OXWASP). TD is funded by the Lloyds
   Register Foundation programme on Data Centric Engineering through the
   London Air Quality project. This work was supported by The Alan Turing
   Institute for Data Science and AI under EPSRC grant EP/N510129/1. In
   collaboration with the Greater London Authority.
CR Adams Ryan Prescott, 2007, ARXIV07103742
   Alvarez M., 2010, ADV NEURAL INFORM PR, P55
   Babanezhad  R., 2015, ADV NEURAL INFORM PR, P2251
   BARRY D, 1993, J AM STAT ASSOC, V88, P309, DOI 10.1080/01621459.1993.10594323
   Basu A, 1998, BIOMETRIKA, V85, P549, DOI 10.1093/biomet/85.3.549
   Bernardo J. M, 2001, BAYESIAN THEORY
   Bissiri PG, 2016, J R STAT SOC B, V78, P1103, DOI 10.1111/rssb.12158
   Cao Y, 2017, IEEE INT SYMP INFO, P1287, DOI 10.1109/ISIT.2017.8006736
   Caron F, 2012, STAT COMPUT, V22, P579, DOI 10.1007/s11222-011-9248-x
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Fearnhead P, 2007, J ROY STAT SOC B, V69, P589, DOI 10.1111/j.1467-9868.2007.00601.x
   Fearnhead Paul, 2017, J AM STAT ASS
   Fox Emily, 2012, ADV NEURAL INFORM PR, P737
   Futami Futoshi, 2018, ARTIFICIAL INTELLIGE
   Ghosh A, 2016, ANN I STAT MATH, V68, P413, DOI 10.1007/s10463-014-0499-0
   Grzegorczyk Marco, 2009, ADV NEURAL INFORM PR, P682
   Hall P, 2005, J R STAT SOC B, V67, P427, DOI 10.1111/j.1467-9868.2005.00510.x
   Harchaoui Z., 2008, ADV NEURAL INFORM PR, V20, P617
   Hernandez-Lobato J., 2016, 33 INT C INT C MACH, P1511
   Huang He, 2016, ADV NEURAL INFORM PR, P2730
   Jewson J, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20060442
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Khaleghi Azadeh, 2012, ADV NEURAL INFORM PR, P3086
   Killick R, 2010, OCEAN ENG, V37, P1120, DOI 10.1016/j.oceaneng.2010.04.009
   Knoblauch Jeremias, 2018, P 27 INT C MACH LEAR
   Konidaris  G., 2010, ADV NEURAL INFORM PR, P1162
   Kummerfeld E., 2013, ADV NEURAL INFORM PR, V26, P1205
   Kurtek S, 2015, BIOMETRIKA, V102, P601, DOI 10.1093/biomet/asv026
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Lei Lihua, 2017, ARTIF INTELL, P148
   Li Y., 2016, ADV NEURAL INFORM PR, P1073
   Lin Kevin, 2017, ADV NEURAL INFORM PR, P6887
   Niekum Scott, 2014, CMURITR1410
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   Pollak M., 2010, SEQUENTIAL ANAL, V29, P146
   Polunchenko AS, 2012, SEQUENTIAL ANAL, V31, P409, DOI 10.1080/07474946.2012.694351
   Ranganath Rajesh, 2016, ADV NEURAL INFORM PR, P496
   Ruanaidh O, 1996, NUMERICAL BAYESIAN M
   Ruggieri E, 2016, COMPUT STAT DATA AN, V97, P71, DOI 10.1016/j.csda.2015.11.010
   Saatci  Y., 2010, P 27 INT C MACH LEAR, P927
   Stimberg F., 2011, ADV NEURAL INFORM PR, P2717
   Tran Dustin, 2017, ADV NEURAL INFORM PR, P2729
   Turner R.D., 2013, P NIPS, P306
   Turner Ryan, 2009, TEMP SEGM WORKSH NIP
   Turner Ryan Darby, 2012, THESIS
   Wilson RC, 2010, NEURAL COMPUT, V22, P2452, DOI 10.1162/NECO_a_00007
   Xuan X., 2007, P 24 INT C MACH LEAR, P1055, DOI DOI 10.1145/1273496.1273629
   Yao Yuling, 2018, ARXIV180202538
   Yilmaz YK, 2011, ADV NEURAL INFORM PR, V24, P2151
   Zhang X., 2011, P 25 ANN C NEUR INF, P1395
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300007
DA 2019-06-15
ER

PT S
AU Kohl, SAA
   Romera-Paredes, B
   Meyer, C
   De Fauw, J
   Ledsam, JR
   Maier-Hein, KH
   Eslami, SMA
   Rezende, DJ
   Ronneberger, O
AF Kohl, Simon A. A.
   Romera-Paredes, Bernardino
   Meyer, Clemens
   De Fauw, Jeffrey
   Ledsam, Joseph R.
   Maier-Hein, Klaus H.
   Eslami, S. M. Ali
   Rezende, Danilo Jimenez
   Ronneberger, Olaf
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Probabilistic U-Net for Segmentation of Ambiguous Images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.
C1 [Kohl, Simon A. A.; Romera-Paredes, Bernardino; Meyer, Clemens; De Fauw, Jeffrey; Ledsam, Joseph R.; Eslami, S. M. Ali; Rezende, Danilo Jimenez; Ronneberger, Olaf] DeepMind, London, England.
   [Kohl, Simon A. A.; Maier-Hein, Klaus H.] German Canc Res Ctr, Div Med Image Comp, Heidelberg, Germany.
RP Kohl, SAA (reprint author), DeepMind, London, England.; Kohl, SAA (reprint author), German Canc Res Ctr, Div Med Image Comp, Heidelberg, Germany.
EM simon.kohl@dkfz.de; brp@google.com; meyerc@google.com;
   defauw@google.com; jledsam@google.com; k.maier-hein@dkfz.de;
   aeslami@google.com; danilor@google.com; olafr@google.com
CR Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204
   BATRA D, 2012, EUR C COMP VIS, V7576, P1
   Bellemare M. G., 2017, ARXIV170510743
   Bouchacourt D., 2016, ADV NEURAL INFORM PR, P352
   Chen C., 2013, AISTATS, P161
   Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Esser P., 2018, ARXIV180404694
   Goodfellow Ian, 2016, ARXIV170100160
   Guzman- Rivera Abner, 2012, NIPS, P1799
   Higgins I., 2017, INT C LEARN REPR
   Ilg E, 2018, ARXIV180207095
   Isola Phillip, 2017, IMAGE TO IMAGE TRANS
   Jimenez Rezende D., 2014, P 31 INT C MACH LEAR
   Kendall A., 2017, ADV NEURAL INFORM PR, P5580, DOI DOI 10.1109/TDEI.2009.5211872
   Kendall A., 2015, ARXIV151102680
   Kingma D. P., 2014, NEURAL INFORM PROCES
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Kirillov A., 2016, ADV NEURAL INFORM PR, P334
   Kirillov A., 2015, ADV NEURAL INFORM PR, P613
   KIRILLOV A, 2015, P IEEE INT C COMP VI, P1814, DOI DOI 10.1109/ICCV.2015.211
   KLEBANOV LB, 2005, N DISTANCES THEIR AP
   Kosub S., 2016, ARXIV161202696
   Lakshminarayanan B., 2017, ADV NEURAL INFORM PR, P6405
   Lee S., 2015, ARXIV151106314
   Lee  Stefan, 2016, ADV NEURAL INFORM PR, P2119
   Lipkus AH, 1999, J MATH CHEM, V26, P263, DOI 10.1023/A:1019154432472
   RAO CR, 1982, THEOR POPUL BIOL, V21, P24, DOI 10.1016/0040-5809(82)90004-1
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Rupprecht C., 2017, INT C COMP VIS ICCV
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Salimans T., 2018, ARXIV180305573
   Sohn  K., 2015, ADV NEURAL INFORM PR, P3483
   Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018
   Zhu J.-Y., 2017, ADV NEURAL INFORM PR, V30, P465
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001050
DA 2019-06-15
ER

PT S
AU Koide, S
   Kawano, K
   Kutsuna, T
AF Koide, Satoshi
   Kawano, Keisuke
   Kutsuna, Takuro
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Edit Operations for Biological Sequences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NETWORKS; SEARCH
AB The evolution of biological sequences, such as proteins or DNAs, is driven by the three basic edit operations: substitution, insertion, and deletion. Motivated by the recent progress of neural network models for biological tasks, we implement two neural network architectures that can treat such edit operations. The first proposal is the edit invariant neural networks, based on differentiable Needleman-Wunsch algorithms. The second is the use of deep CNNs with concatenations. Our analysis shows that CNNs can recognize regular expressions without Kleene star, and that deeper CNNs can recognize more complex regular expressions including the insertion/deletion of characters. The experimental results for the protein secondary structure prediction task suggest the importance of insertion/deletion. The test accuracy on the widely-used CB513 dataset is 71.5%, which is 1.2-points better than the current best result on non-ensemble models.
C1 [Koide, Satoshi; Kawano, Keisuke; Kutsuna, Takuro] Toyota Cent R&D Labs, Nagakute, Aichi, Japan.
RP Koide, S (reprint author), Toyota Cent R&D Labs, Nagakute, Aichi, Japan.
EM koide@mosk.tytlabs.co.jp; kawano@mosk.tytlabs.co.jp;
   kutsuna@mosk.tytlabs.co.jp
RI Kutsuna, Takuro/Q-5626-2019
OI Kutsuna, Takuro/0000-0001-6965-1512
CR ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1016/S0022-2836(05)80360-2
   Busia  A., 2017, ABS170203865 CORR
   Cuturi M, 2017, ICML, P894
   Di Lena P, 2012, BIOINFORMATICS, V28, P2449, DOI 10.1093/bioinformatics/bts475
   Forcada ML, 2001, LECT NOTES ARTIF INT, V2036, P480
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769
   Golkov  V., 2017, P NIPS 16, P4222
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kelley DR, 2016, GENOME RES, V26, P990, DOI 10.1101/gr.200535.115
   Li Z., 2016, P 25 INT JOINT C ART, P2560
   Lin  Z., 2016, P AAAI
   Magnan CN, 2014, BIOINFORMATICS, V30, P2592, DOI 10.1093/bioinformatics/btu352
   Minsky M. L., 1967, COMPUTATION FINITE I
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Peng J., 2009, ADV NEURAL INFORM PR, P1419
   Qi YF, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0031539
   Saigo H, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-246
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Soren Kaae Sonderby O. W., 2016, ARXIV14127828
   Wang S, 2016, SCI REP-UK, V6, DOI 10.1038/srep18962
   Worrall D. E., 2017, P IEEE C COMP VIS PA, P5028
   Zhou J, 2014, INT CONF MACH LEARN, P71, DOI 10.1109/ICMLC.2014.7009094
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305001
DA 2019-06-15
ER

PT S
AU Kondor, R
   Lin, Z
   Trivedi, S
AF Kondor, Risi
   Lin, Zhen
   Trivedi, Shubhendu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional
   Neural Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recent work by Cohen et al. [1] has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch-Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.
C1 [Kondor, Risi; Lin, Zhen] Univ Chicago, Chicago, IL 60637 USA.
   [Trivedi, Shubhendu] Toyota Technol Inst, Nagoya, Aichi, Japan.
RP Kondor, R (reprint author), Univ Chicago, Chicago, IL 60637 USA.
EM risi@uchicago.edu; zlin7@uchicago.edu; shubhendu@ttic.edu
CR Blum L. C., 2009, J AM CHEM SOC
   Boomsma W., 2017, ADV NEURAL INFORM PR, P3436
   Chang A X, 2015, SHAPENET INFORM RICH
   Cohen T. S., 2017, ICLR
   Cohen Taco, 2016, INT C MACH LEARN, P2990
   Cohen TS, 2018, INT C LEARN REPR
   Cruz-Mota J, 2012, INT J COMPUT VISION, V98, P217, DOI 10.1007/s11263-011-0505-4
   Diaconis P., 1988, IMS LECT SERIES, V11
   Esteves C., 2017, LEARNING SO 3 EQUIVA
   Esteves C., 2017, POLAR TRANSFORMER NE
   Gens R., 2014, NIPS 2014, P1
   Gutman B., 2008, 2 MICCAI WORKSH MATH, P56
   Healy DM, 1996, INT CONF ACOUST SPEE, P1323, DOI 10.1109/ICASSP.1996.543670
   Khasanova R., 2017, GRAPH BASED CLASSIFI
   Kondor R., 2018, CORR
   Kondor R., 2018, GEN EQUIVARIANCE CON
   Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5
   Lai W-S, SEMANTIC DRIVEN GENE, P1
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Masci J., 2015, GEODESIC CONVOLUTION
   Montavon G., 2012, NIPS
   Monti F., 2016, GEOMETRIC DEEP LEARN
   Raj A., 2016, LOCAL GROUP INVARIAN
   Ravanbakhsh S., 2017, P INT C MACH LEARN
   Rupp M., 2012, PHYS REV LETT
   Savva Manolis, 2017, EUR WORKSH 3D OBJ RE
   Serre J-P., 1977, GRADUATE TEXTS MATHA, V42
   Su Y-C, 2017, MAKING 360 VIDEO WAT
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Terras A., 1999, LONDON MATH SOC STUD, V43
   Thomas N., 2018, ARXIV180208219CS
   Worrall D. E., 2016, HARMONIC NETWORKS DE
   Zelnik-Manor L, 2005, IEEE I CONF COMP VIS, P1292
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004065
DA 2019-06-15
ER

PT S
AU Kong, WH
   Valiant, G
AF Kong, Weihao
   Valiant, Gregory
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Estimating Learnability in the Sublinear Data Regime
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SIGNAL-TO-NOISE; EIGENVALUES
AB We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data. We show that it is possible to accurately estimate this "learnability" even when given an amount of data that is too small to reliably learn any accurate model. Our first result applies to the setting where the data is drawn from a d-dimensional distribution with isotropic covariance, and the label of each datapoint is an arbitrary noisy function of the datapoint. In this setting, we show that with O(root d) samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. For comparison, even if the labels are noiseless linear functions of the data, a sample size linear in the dimension, d, is required to learn any function correlated with the underlying model. Our estimation approach also applies to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data. In this setting we give a consistent estimator of the fraction of explainable variance that uses o(d) samples. Finally, our techniques also extend to the setting of binary classification, where we obtain analogous results under the logistic model, for estimating the classification accuracy of the best linear classifier. We demonstrate the practical viability of our approaches on synthetic and real data. This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected.
C1 [Kong, Weihao; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA.
RP Kong, WH (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM whkong@stanford.edu; gvaliant@cs.stanford.edu
FU NSF [CCF-1704417]; ONR Young Investigator Award; Sloan Research
   Fellowship
FX This work was supported by NSF award CCF-1704417, an ONR Young
   Investigator Award, and a Sloan Research Fellowship.
CR BAI ZD, 1988, THEOR PROBAB APPL+, V32, P490, DOI 10.1137/1132067
   Bayati Mohsen, 2013, ADV NEURAL INFORM PR, P944
   Bellare M, 1996, IEEE T INFORM THEORY, V42, P1781, DOI 10.1109/18.556674
   Ben-Sasson Eli, 2003, P 35 ANN ACM S THEOR, P612
   Chen X, 2014, ANN IEEE SYMP FOUND, P286, DOI 10.1109/FOCS.2014.38
   Dicker LH, 2014, BIOMETRIKA, V101, P269, DOI 10.1093/biomet/ast065
   Fan JQ, 2012, J R STAT SOC B, V74, P37, DOI 10.1111/j.1467-9868.2011.01005.x
   Goldreich O, 1998, ANN IEEE SYMP FOUND, P426, DOI 10.1109/SFCS.1998.743493
   Guo Zijian, 2017, J AM STAT ASS
   Janson L, 2017, J R STAT SOC B, V79, P1037, DOI 10.1111/rssb.12203
   Kong WH, 2017, ANN STAT, V45, P2218, DOI 10.1214/16-AOS1525
   Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z
   Sun TN, 2012, BIOMETRIKA, V99, P879, DOI 10.1093/biomet/ass043
   Verzelen N, 2018, BERNOULLI, V24, P3683, DOI 10.3150/17-BEJ975
   Verzelen N, 2010, ANN STAT, V38, P704, DOI 10.1214/08-AOS629
   Wencheko E, 2000, STAT PAP, V41, P327, DOI 10.1007/BF02925926
   YIN YQ, 1983, J MULTIVARIATE ANAL, V13, P489, DOI 10.1016/0047-259X(83)90035-0
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305047
DA 2019-06-15
ER

PT S
AU Korba, A
   Garcia, A
   d'Alche-Buc, F
AF Korba, Anna
   Garcia, Alexandre
   d'Alche-Buc, Florence
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Structured Prediction Approach for Label Ranking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose to solve a label ranking problem as a structured output regression task. In this view, we adopt a least square surrogate loss approach that solves a supervised learning problem in two steps: a regression step in a well-chosen feature space and a pre-image (or decoding) step. We use specific feature maps/embeddings for ranking data, which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach, either by resulting in consistent estimators, or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. Their extension to the case of incomplete or partial rankings is also discussed. Finally, we provide empirical results on synthetic and real-world datasets showing the relevance of our method.
C1 [Korba, Anna; Garcia, Alexandre; d'Alche-Buc, Florence] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
RP Korba, A (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
EM anna.korba@telecom-paristech.fr; alexandre.garcia@telecom-paristech.fr;
   florence.dalche-buc@telecom-paristech.fr
CR Aiguzhinov A, 2010, LECT NOTES ARTIF INT, V6332, P16, DOI 10.1007/978-3-642-16184-1_2
   Ailon N, 2010, ALGORITHMICA, V57, P284, DOI 10.1007/s00453-008-9211-1
   Aledo JA, 2017, INFORM FUSION, V35, P38, DOI 10.1016/j.inffus.2016.09.002
   Brazdil PB, 2003, MACH LEARN, V50, P251, DOI 10.1023/A:1021713901879
   Brouard C, 2016, J MACH LEARN RES, V17
   Calauzenes Clement, 2012, ADV NEURAL INFORM PR, V25, P197
   Cao Zhe, 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Cheng W., 2009, P 26 ANN INT C MACH, P161, DOI DOI 10.1145/1553374.1553395
   Cheng W, 2013, NEAREST NEIGHBOR APP
   Cheng W., 2010, P 27 INT C MACH LEAR, P215
   Chiang TH, 2012, AS C MACH LEARN JMLR, V25, P81
   Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412
   Clemencon S, 2017, ARXIV171100070
   Cortes C., 2005, P 22 INT C MACH LEAR, P153
   de Sa CR, 2018, INFORM FUSION, V40, P112, DOI 10.1016/j.inffus.2017.07.001
   Dekel O, 2004, ADV NEUR IN, V16, P497
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Deza MM, 2009, ENCY DISTANCES
   Djuric N, 2014, AAAI, P1788
   Fagin R, 2004, P 23 ACM SIGMOD SIGA, P47, DOI [DOI 10.1145/1055558.1055568, 10.1145/1055558.1055568]
   Fathony R, 2018, INT C MACH LEARN, P1456
   Furnkranz J, 2003, LECT NOTES ARTIF INT, V2837, P145
   Geng X, 2014, PROC CVPR IEEE, P3742, DOI 10.1109/CVPR.2014.478
   Gurrieri M, 2012, ADV COMPUTATIONAL IN, P613, DOI DOI 10.1007/978-3-642-31709-5-62
   Jiao Y, 2016, P INT C MACH LEARN I, P2971
   Kadri H, 2013, P 30 INT C MACH LEAR, P471
   Kamishima T, 2010, PREFERENCE LEARNING, P181, DOI 10.1007/978-3-642-14125-6_9
   Kenkre S, 2011, P 2011 SIAM INT C DA
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109
   Li P, 2017, ARXIV170109083
   Mares M, 2007, LECT NOTES COMPUT SC, V4698, P187
   Merlin VR, 1997, J ECON THEORY, V72, P148, DOI 10.1006/jeth.1996.2205
   Micchelli CA, 2005, J MACH LEARN RES, V6, P1099
   Myrvold W, 2001, INFORM PROCESS LETT, V79, P281, DOI 10.1016/S0020-0190(01)00141-7
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Osokin A, 2017, ADV NEURAL INFORM PR, P301
   Ramaswamy H. G, 2013, ADV NEURAL INFORM PR, P1475
   Sa C. R, 2017, LABEL RANKING FOREST
   Steinwart I, 2008, INFORM SCI STAT, P1
   Vembu S, 2010, PREFERENCE LEARNING, P45, DOI 10.1007/978-3-642-14125-6_3
   Wang D, 2015, IEEE T INFORM THEORY, V61, P6417, DOI 10.1109/TIT.2015.2485270
   Wang QS, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P164, DOI 10.1109/ACPR.2011.6166699
   Yu PLH, 2010, PREFERENCE LEARNING, P83, DOI 10.1007/978-3-642-14125-6_5
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhou Y, 2016, ARXIV160807710
   Zhou YM, 2014, J COMPUT, V9, P557, DOI 10.4304/jcp.9.3.557-565
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003054
DA 2019-06-15
ER

PT S
AU Korbar, B
   Tran, D
   Torresani, L
AF Korbar, Bruno
   Du Tran
   Torresani, Lorenzo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Cooperative Learning of Audio and Video Models from Self-Supervised
   Synchronization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.
C1 [Korbar, Bruno; Torresani, Lorenzo] Dartmouth Coll, Hanover, NH 03755 USA.
   [Du Tran] Facebook Res, Menlo Pk, CA USA.
RP Korbar, B (reprint author), Dartmouth Coll, Hanover, NH 03755 USA.
EM bruno.18@dartmouth.edu; trandu@fb.com; LT@dartmouth.edu
FU NSF [CNS-120552]
FX This work was funded in part by NSF award CNS-120552. We gratefully
   acknowledge NVIDIA and Facebook for the donation of GPUs used for
   portions of this work. We would like to thank Relja Arandjelovic for
   discussions and for sharing informations regarding L<SUP>3</SUP>-Net,
   and members of the Visual Learning Group at Dartmouth for their
   feedback.
CR Arandjelovic Relja, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11205), P451, DOI 10.1007/978-3-030-01246-5_27
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aytar  Y., 2016, ADV NEURAL INFORM PR, P-900
   Bengio Y., 2007, P ADV NEUR INF PROC, P153
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao  Z., 2017, CVPR
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chung J. S., 2016, P ACCV, P251
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Doersch C, 2015, ICCV
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, ADV NEURAL INFORM PR, P3468
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Gemmeke  J.F., 2017, P IEEE ICASSP 2017 N
   Gu Chunhui, 2017, ABS170508421 CORR
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hang Zhao, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11205), P587, DOI 10.1007/978-3-030-01246-5_35
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay Will, 2017, ABS170506950 CORR
   Koch G., 2015, ICML DEEP LEARN WORK, V2
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le Quoc, 2011, ICML
   Lee H., 2006, ADV NEURAL INF PROCE, P801
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Owens Andrew, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11210), P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Piczak KJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1015, DOI 10.1145/2733373.2806390
   Ranzato Marc'Aurelio, 2007, 2007 IEEE COMP SOC C
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sailor HB, 2017, INTERSPEECH, P3107, DOI 10.21437/Interspeech.2017-831
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Soomro K., 2012, ARXIV12120402
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Tran D., 2018, CVPR
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang X., 2015, ICCV
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Yu F., 2015, ABS151107122 CORR
   Zhao Hang, 2017, ABS171209374 CORR
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002032
DA 2019-06-15
ER

PT S
AU Korshunova, I
   Degrave, J
   Huszar, F
   Gal, Y
   Gretton, A
   Dambre, J
AF Korshunova, Iryna
   Degrave, Jonas
   Huszar, Ferenc
   Gal, Yarin
   Gretton, Arthur
   Dambre, Joni
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI BRUNO: A Deep Recurrent Model for Exchangeable Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.
C1 [Korshunova, Iryna; Degrave, Jonas; Dambre, Joni] Univ Ghent, Ghent, Belgium.
   [Huszar, Ferenc] Twitter, San Francisco, CA USA.
   [Gal, Yarin] Univ Oxford, Oxford, England.
   [Gretton, Arthur] UCL, Gatsby Unit, London, England.
   [Degrave, Jonas] DeepMind, London, England.
RP Korshunova, I (reprint author), Univ Ghent, Ghent, Belgium.
EM iryna.korshunova@ugent.be; jonas.degrave@ugent.be; fhuszar@twitter.com;
   yarin@cs.ox.ac.uk; arthur.gretton@gmail.com; joni.dambre@ugent.be
CR Aldous D., 1985, LECT NOTES MATH
   BAILEY RW, 1994, MATH COMPUT, V62, P779, DOI 10.2307/2153537
   Chen F., 2018, ARXIV180207876
   Dinh L., 2017, P 5 INT C LEARN REPR
   Edwards H., 2017, P 5 INT C LEARN REPR
   Ghahramani Z., 2006, ADV NEURAL INFORM PR, V18, P435
   Heller KA, 2006, IEEE COMP SOC C COMP, V2, P2110
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lake B., 2015, SCIENCE
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Papamakarios G., 2017, ADV NEURAL INFORM PR, P2335
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T., 2016, P 30 INT C NEUR INF
   Shah A., 2014, JMLR W CP, P877
   Szabo Z, 2016, J MACH LEARN RES, V17
   Theis L., 2016, P 4 INT C LEARN REPR
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Vinyals O., 2016, P 4 INT C LEARN REPR
   Xiao H., 2017, ABS170807747 ARXIV
   Zaheer M, 2017, ADV NEURAL INFORM PR, P3394
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001071
DA 2019-06-15
ER

PT S
AU Kosiorek, AR
   Kim, H
   Posner, I
   Teh, YW
AF Kosiorek, Adam R.
   Kim, Hyunjik
   Posner, Ingmar
   Teh, Yee Whye
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. It can reliably discover and track objects throughout the sequence of frames, and can also generate future frames conditioning on the current frame, thereby simulating expected motion of objects. This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et al., 2016), including learning in an unsupervised manner, and addresses its shortcomings. We use a moving multi MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how SQAIR overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.
C1 [Kosiorek, Adam R.; Posner, Ingmar] Univ Oxford, Oxford Robot Inst, Appl Artificial Intelligence Lab, Oxford, England.
   [Kosiorek, Adam R.; Kim, Hyunjik; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England.
RP Kosiorek, AR (reprint author), Univ Oxford, Oxford Robot Inst, Appl Artificial Intelligence Lab, Oxford, England.; Kosiorek, AR (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM adamk@robots.ox.ac.uk
FU European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013) ERC grant [617071]
FX We would like to thank Ali Eslami for his help in implementing AIR, Alex
   Bewley and Martin Engelcke for discussions and valuable insights and
   anonymous reviewers for their constructive feedback. Additionally, we
   acknowledge that HK and YWT's research leading to these results has
   received funding from the European Research Council under the European
   Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement
   no. 617071.
CR Chang M., 2018, ICLR
   Chung J, 2015, NIPS
   Clevert Djork-Arne, 2015, CORR
   Courville A., 2016, CORR
   Denton E, 2017, NIPS
   Denton E., 2018, ICML
   Gael J. V, 2009, NIPS
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Ha D., 2018, CORR
   Hinton G. E., 2016, NIPS
   Ilin A., 2017, NIPS
   Jaderberg M., 2015, NIPS, P1, DOI DOI 10.1038/NBT.3343
   Kalchbrenner N., 2016, NIPS
   Kemp C., 2008, P NATL ACAD SCI USA, V105, P31
   Kim H., 2018, ICML
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2013, ARXIV13126114
   Kosiorek A. R., 2017, NIPS
   Kwak S., 2015, ICCV
   LeCun  Y., 1989, NEURAL COMPUTATION, V1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levine S., 2017, CORR
   Maddison Chris J, 2017, ADV NEURAL INFORM PR
   Mnih A., 2016, ICML
   Mnih A., 2014, ICML
   Neiswanger W., 2012, CORR
   Niebles J. C., 2018, NIPS
   Ponce J., 2015, CORR
   Ranzato M., 2014, CORR
   Ristani E., 2016, ECCV
   Salakhutdinov R., 2016, ICLR
   Santoro A, 2017, NIPS
   Schmidhuber J., 2017, NIPS
   Schmidhuber J., 2016, NIPS
   Shi Wenzhe, 2016, CVPR
   Smeulders A. W. M., 2016, CVPR
   Srivastava N, 2015, ICML
   Tieleman T., 2012, LECT 6 5 RMSPROP DIV
   Tulyakov S., 2018, CVPR
   Upcroft B., 2016, ICIP
   Valmadre J., 2017, CVPR
   Vernaza P., 2017, CVPR
   Weber T., 2017, NIPS
   Xiao F, 2016, CVPR
   Zaheer M., 2017, NIPS
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003019
DA 2019-06-15
ER

PT S
AU Kovalev, D
   Gorbunov, E
   Gasanov, E
   Richtarik, P
AF Kovalev, Dmitry
   Gorbunov, Eduard
   Gasanov, Elnur
   Richtarik, Peter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Stochastic Spectral and Conjugate Descent Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EFFICIENCY
AB The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.
C1 [Kovalev, Dmitry; Gorbunov, Eduard; Gasanov, Elnur; Richtarik, Peter] Moscow Inst Phys & Technol, Dolgoprudnyi, Russia.
   [Kovalev, Dmitry; Gasanov, Elnur; Richtarik, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
   [Richtarik, Peter] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
RP Kovalev, D (reprint author), Moscow Inst Phys & Technol, Dolgoprudnyi, Russia.; Kovalev, D (reprint author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
CR Allen-Zhu  Z, 2016, P 33 INT C MACH LEAR, P1110
   BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141
   Birgin EG, 2014, J STAT SOFTW, V60, DOI 10.18637/jss.v060.i03
   Csiba D., 2015, 32 INT C MACH LEARN, P674
   Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993
   Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487
   Gower Robert Mansel, 2015, ARXIV151206890
   Lee Ching-Pei, 2016, ARXIV160708320
   Lee Yin Tat, 2013, FOCS
   Leventhal D, 2010, MATH OPER RES, V35, P641, DOI 10.1287/moor.1100.0456
   Loizou Nicolas, 2017, ARXIV171209677
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182
   Nutini Julie, 2015, ICML
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360
   Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6
   Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1
   Richtarik Peter, 2017, ARXIV170601108
   Tu Stephen, 2017, ICML
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303036
DA 2019-06-15
ER

PT S
AU Krijthe, JH
   Loog, M
AF Krijthe, Jesse H.
   Loog, Marco
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Pessimistic Limits and Possibilities of Margin-based Losses in
   Semi-supervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CLASSIFICATION
AB Consider a classification problem where we have both labeled and unlabeled data available. We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing, it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements are possible.
C1 [Krijthe, Jesse H.] Radboud Univ Nijmegen, Nijmegen, Netherlands.
   [Loog, Marco] Delft Univ Technol, Delft, Netherlands.
   [Loog, Marco] Univ Copenhagen, Copenhagen, Denmark.
RP Krijthe, JH (reprint author), Radboud Univ Nijmegen, Nijmegen, Netherlands.
EM jkrijthe@gmail.com; m.loog@tudelft.nl
FU Project P23 of the Dutch COMMIT research programme
FX We thank Alexander Mey for his constructive feedback on an earlier
   version of this manuscript. This work was funded by Project P23 of the
   Dutch COMMIT research programme.
CR Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Ben-David  Shai, 2008, P 21 ANN C LEARN THE, P33
   Cozman F, 2006, SEMISUPERVISED LEARN, P56
   Darnstadt M., 2013, 30 INT S THEOR ASP C, P185, DOI DOI 10.4230/LIPICS.STACS.2013.185
   Elworthy D., 1994, ASS COMPUT LINGUIST, P53
   Fung G., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P77
   HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kawakita M, 2014, NEURAL NETWORKS, V53, P146, DOI 10.1016/j.neunet.2014.01.016
   Krijthe JH, 2017, MACH LEARN, V106, P993, DOI 10.1007/s10994-017-5626-8
   Krijthe JH, 2017, PATTERN RECOGN, V63, P115, DOI 10.1016/j.patcog.2016.09.009
   Li YF, 2015, IEEE T PATTERN ANAL, V37, P175, DOI 10.1109/TPAMI.2014.2299812
   Loog M, 2016, IEEE T PATTERN ANAL, V38, P462, DOI 10.1109/TPAMI.2015.2452921
   Loog M, 2010, LECT NOTES ARTIF INT, V6322, P291, DOI 10.1007/978-3-642-15883-4_19
   Poggio T., 2003, NOTICES AMS, V50, P537
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rifkin R, 2003, NATO SCI SERIES 3, V190, P131
   Seeger Matthias, 2001, TECHNICAL REPORT
   Shi M, 2011, BIOINFORMATICS, V27, P3017, DOI 10.1093/bioinformatics/btr502
   Sion M., 1958, PAC J MATH, V8, P171, DOI DOI 10.2140/PJM.1958.8.171
   Sokolovska N., 2008, P 25 INT C MACH LEAR, P984
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Weston J, 2005, BIOINFORMATICS, V21, P3241, DOI 10.1093/bioinformatics/bti497
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301075
DA 2019-06-15
ER

PT S
AU Kroer, C
   Sandholm, T
AF Kroer, Christian
   Sandholm, Tuomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Unified Framework for Extensive-Form Game Abstraction with Bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees-while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how is an element of-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.
C1 [Kroer, Christian; Sandholm, Tuomas] Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Kroer, C (reprint author), Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM ckroer@cs.cmu.edu; sandholm@cs.cmu.edu
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]; Facebook Fellowship
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082. Christian Kroer is supported by a
   Facebook Fellowship.
CR Basak A, 2016, LECT NOTES COMPUT SC, V9996, P251, DOI 10.1007/978-3-319-47413-7_15
   Basak A, 2016, LECT NOTES ARTIF INT, V10003, P13, DOI 10.1007/978-3-319-46840-2_2
   Basilico N., 2011, AAAI C ART INT AAAI
   Billings D., 2003, P INT JOINT C ARTIFI
   Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown N., 2017, P ANN C NEUR INF PRO, P689
   Brown N., 2014, AAAI C ART INT AAAI
   Brown N., 2017, SCIENCE
   Brown N., 2015, INT C AUT AG MULT SY
   Brown Noam, 2015, P INT JOINT C ARTIFI
   Burch N., 2012, P ANN C NEUR INF PRO, P1880
   Cermak J., 2017, P INT JOINT C ART IN, P936
   Farina Gabriele, 2017, INT C MACH LEARN ICM
   Ganzfried S., 2014, AAAI C ART INT AAAI
   Gilpin A., 2007, INT C AUT AG MULT SY, P1168
   Gilpin A., 2008, INT C AUT AG MULT SY
   Gilpin  A., 2007, P AAAI C ART INT AAA
   Gilpin A., 2006, P NAT C ART INT AAAI, V21, P1007
   Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324
   Gilpin  Andrew, 2008, P AAAI C ART INT AAA
   Hawkin J., 2011, AAAI C ART INT AAAI
   Hawkin J., 2012, AAAI C ART INT AAAI
   Hoda S., 2010, MATH OPERATIONS RES, V35
   Johanson  Michael, 2013, INT C AUT AG MULT SY
   Kroer C., 2016, P ACM C EC COMP EC
   Kroer C., 2015, INT C AUT AG MULT SY
   Kroer C., 2015, P ACM C EC COMP EC
   Kroer C., 2017, P ACM C EC COMP EC
   Kroer  Christian, 2014, P ACM C EC COMP EC
   Lanctot  M., 2009, P ANN C NEUR INF PRO
   Lanctot M., 2012, INT C MACH LEARN ICM
   Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Sandholm T., 2015, AAAI C ART INT AAAI
   Sandholm  Tuomas, 2012, P ACM C EL COMM EC
   Shi J., 2000, COMPUTERS GAMES, P333
   Waugh K., 2009, THESIS
   Waugh K., 2015, AAAI C ART INT AAAI
   Wellman M. P., 2005, P NAT C ART INT AAAI
   Zinkevich  M., 2007, P ANN C NEUR INF PRO
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300057
DA 2019-06-15
ER

PT S
AU Kroer, C
   Farina, G
   Sandholm, T
AF Kroer, Christian
   Farina, Gabriele
   Sandholm, Tuomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Solving Large Sequential Games with the Excessive Gap Technique
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB There has been tremendous recent progress on equilibrium-finding algorithms for zero-sum imperfect-information extensive-form games, but there has been a puzzling gap between theory and practice. First-order methods have significantly better theoretical convergence rates than any counterfactual-regret minimization (CFR) variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-ofthe-art variant of the excessive gap technique-instantiated with the dilated entropy distance function-can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the Libratus poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.
C1 [Kroer, Christian; Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Kroer, C (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM ckroer@cs.cmu.edu; gfarina@cs.cmu.edu; sandholm@cs.cmu.edu
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]; Facebook Fellowship
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082. Christian Kroer is supported by a
   Facebook Fellowship.
CR Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown N., 2017, P ANN C NEUR INF PRO, P689
   Brown N., 2017, SCIENCE
   Brown N., 2017, AAAI C ART INT AAAI
   Brown N., 2015, INT C AUT AG MULT SY
   Brown Noam, 2017, INT C MACH LEARN ICM
   Cermak J, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1813
   Chambolle A., 2011, J MATH IMAGING VISIO
   Farina Gabriele, 2017, INT C MACH LEARN ICM
   Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324
   Hoda S., 2010, MATH OPERATIONS RES, V35
   Johanson.  M., 2013, TECHNICAL REPORT
   Johanson  Michael, 2011, P INT JOINT C ARTIFI
   Juditsky A., 2011, STOCHASTIC SYSTEMS, V1, P17, DOI DOI 10.1214/10-SSY011
   Koller D, 1997, ARTIF INTELL, V94, P167, DOI 10.1016/S0004-3702(97)00023-4
   Koller D., 1996, GAME ECON BEHAV, V14, P2
   Kroer C., 2017, ARXIV170204849
   Kroer C., 2015, P ACM C EC COMP EC
   Kroer C., 2017, P INT JOINT C ARTIFI
   Lanctot  M., 2009, P ANN C NEUR INF PRO
   Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Nemirovski A., 2004, SIAM J OPTIMIZATION, V15
   Nesterov Y., 2005, MATH PROGRAMMING, V103
   Nesterov Y., 2005, SIAM J OPTIMIZATION, V16
   Romanovskii I., 1962, SOVIET MATH, V3
   Tammelin O., 2015, P 24 INT JOINT C ART
   vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050
   Zinkevich  M., 2007, P ANN C NEUR INF PRO
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300080
DA 2019-06-15
ER

PT S
AU Kumar, A
   Sattigeri, P
   Wadhawan, K
   Karlinsky, L
   Feris, R
   Freeman, WT
   Wornell, G
AF Kumar, Abhishek
   Sattigeri, Prasanna
   Wadhawan, Kahini
   Karlinsky, Leonid
   Feris, Rogerio
   Freeman, William T.
   Wornell, Gregory
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Co-regularized Alignment for Unsupervised Domain Adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep neural networks, trained with large amount of labeled data, can fail to generalize well when tested with examples from a target domain whose distribution differs from the training data distribution, referred as the source domain. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples. The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and observe that it provides significant performance improvements on several domain adaptation benchmarks.
C1 [Kumar, Abhishek; Sattigeri, Prasanna; Wadhawan, Kahini; Karlinsky, Leonid; Feris, Rogerio] MIT, IBM Res, IBM Watson AI Lab, Cambridge, MA 02139 USA.
   [Freeman, William T.; Wornell, Gregory] MIT, Cambridge, MA 02139 USA.
RP Kumar, A (reprint author), MIT, IBM Res, IBM Watson AI Lab, Cambridge, MA 02139 USA.
EM abhishk@us.ibm.com; psattig@us.ibm.com; kahini.wadhawan@ibm.com;
   leonidka@il.ibm.com; rsferis@us.ibm.com; billf@mit.edu; gww@mit.edu
CR Balcan M.-F., 2005, ADV NEURAL INFORM PR, P89
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Blum  A., 1998, C LEARN THEOR
   Bousmalis K., 2017, ARXIV170907857
   Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chapelle O, 2005, P 10 INT WORKSH ART, P57
   Chen M, 2011, P ADV NEUR INF PROC, P2456
   Daume III Hal, 2010, ADV NEURAL INFORM PR
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1
   DRUCKER H, 1994, NEURAL COMPUT, V6, P1289, DOI 10.1162/neco.1994.6.6.1289
   Dumoulin  Vincent, 2017, P INT C LEARN REPR T
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   French  Geoffrey, 2018, INT C LEARN REPR
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin  Yaroslav, 2015, ICML
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grandvalet Y., 2005, ADV NEURAL INFORM PR, V17, P529
   Kumar A., 2011, P 28 INT C MACH LEAR, P393
   Kumar A., 2011, ADV NEURAL INFORM PR, P1413
   Laine S, 2016, ARXIV161002242
   Lee  Stefan, 2016, ADV NEURAL INFORM PR, P2119
   Liu  M., 2017, ADV NEURAL INFORM PR, P700
   Liu Y, 1999, NEURAL NETWORKS, V12, P1399, DOI 10.1016/S0893-6080(99)00073-8
   Long M., 2015, ARXIV150202791
   Miyato  Takeru, 2017, ARXIV170403976
   Murez  Zak, 2017, ARXIV171200479
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Rosen B. E., 1996, Connection Science, V8, P373, DOI 10.1080/095400996116820
   Rosenberg David S, 2007, P 11 INT C ART INT S, P396
   Saito K., 2017, ARXIV170208400
   Saito  Kuniaki, 2018, IEEE C COMP VIS PATT
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shu R., 2018, INT C LEARN REPR
   Sindhwani  Vikas, 2005, P WORKSH LEARN MULT
   Sindhwani  Vikas, 2008, P 25 INT C MACH LEAR
   Sridharan  Karthik, 2008, COLT
   Sun B. C., 2014, BMVC, P3
   SUN BC, 2016, EUR C COMP VIS, V9915, P443, DOI DOI 10.1007/978-3-319-49409-8_35
   Tzeng E., 2017, IEEE C COMP VIS PATT, V1, P4
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163
   Yan  Hongliang, 2017, P IEEE C COMP VIS PA, P2272
   Zhou ZH, 2005, IEEE T KNOWL DATA EN, V17, P1529, DOI 10.1109/TKDE.2005.186
   Zolna  Konrad, 2018, INT C LEARN REPR
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003086
DA 2019-06-15
ER

PT S
AU Kumar, A
   Gupta, S
   Fouhey, D
   Levine, S
   Malik, J
AF Kumar, Ashish
   Gupta, Saurabh
   Fouhey, David
   Levine, Sergey
   Malik, Jitendra
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Visual Memory for Robust Path Following
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Humans routinely retrace paths in a novel environment both forwards and backwards despite uncertainty in their motion. This paper presents an approach for doing so. Given a demonstration of a path, a first network generates a path abstraction. Equipped with this abstraction, a second network observes the world and decides how to act to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following and homing under actuation noise and environmental changes. Our experiments show that our approach outperforms classical approaches and other learning based baselines.
C1 [Kumar, Ashish; Gupta, Saurabh; Fouhey, David; Levine, Sergey; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Kumar, A (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM ashish_kumar@berkeley.edu; sgupta@eecs.berkeley.edu;
   dfouhey@eecs.berkeley.edu; svlevine@eecs.berkeley.edu;
   malik@eecs.berkeley.edu
CR Anderson  Peter, 2018, ARXIV180706757
   Armeni  Iro, 2016, CVPR
   Axelrod  Brian, 2017, RSS
   Bowman Sean L, 2017, ICRA
   Brahmbhatt  Samarth, 2018, CVPR
   Canny J., 1988, COMPLEXITY ROBOT MOT
   Chang  Angel, 2017, 3DV
   Chaplot Devendra Singh, 2018, ICLR
   Clement  Lee, 2017, JFR
   Cummins  Mark, 2008, IJRR
   Daftry  Shreyansh, 2016, ISER
   Davison Andrew J, 1998, ECCV
   Engel J., 2014, ECCV
   Fuentes-Pacheco J., 2015, ARTIFICIAL INTELLIGE
   Furgale  Paul, 2010, JFR
   Gandhi  Dhiraj, 2017, IROS
   Giusti  Alessandro, 2016, RAL
   Gupta  Saurabh, 2017, CVPR
   Izadi Shahram, 2011, UIST
   Kahn  Gregory, 2017, ARXIV170910489
   Kar  Abhishek, 2017, NIPS
   Kavraki Lydia E, 1996, RA
   Kendall A, 2015, ICCV
   Khan  Arbaaz, 2018, ICLR
   Lavalle Steven M, 2000, ALGORITHMIC COMPUTAT
   Lowe D. G., 2004, IJCV
   Mirowski P., 2018, ARXIV180400168
   Mirowski  Piotr, 2017, ICLR
   Nister  David, 2004, CVPR
   Parisotto  Emilio, 2018, ICLR
   Pathak  Deepak, 2018, ICLR
   Pillai  Sudeep, 2017, ARXIV170510279
   Pomerleau Dean A, 1989, NIPS, pAlvinn
   Sadeghi  Fereshteh, 2017, RSS
   Savinov  Nikolay, 2018, ICLR
   Schonberger J. L., 2016, ECCV
   Schonberger J. L., 2016, CVPR
   Schonberger Johannes L, 2018, CVPR
   Song  Shuran, 2018, CVPR
   Swedish  Tristan, 2018, CVPR
   Zhang J., 2017, ARXIV170609520
   Zhou T., 2017, CVPR
   Zhu Y., 2017, ICRA
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300071
DA 2019-06-15
ER

PT S
AU Kumar, R
   Purohit, M
   Svitkina, Z
AF Kumar, Ravi
   Purohit, Manish
   Svitkina, Zoya
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Improving Online Algorithms via ML Predictions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this work we study the problem of using machine-learned predictions to improve the performance of online algorithms. We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions. These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.
C1 [Kumar, Ravi; Purohit, Manish; Svitkina, Zoya] Google, Mountain View, CA 94043 USA.
RP Kumar, R (reprint author), Google, Mountain View, CA 94043 USA.
EM ravi.k53@gmail.com; mpurohit@google.com; zoya@cs.cornell.edu
CR Bansal N, 2004, ALGORITHMICA, V40, P305, DOI 10.1007/s00453-004-1115-0
   Bansal N., 2001, Performance Evaluation Review, V29, P279
   Becchetti Luca, 2001, STOC, P94
   Bent Russell, 2009, ONLINE STOCHASTIC CO
   Borodin A., 1998, ONLINE COMPUTATION C
   Bubeck Sebastien, 2012, COLT
   Crovella ME, 1997, IEEE ACM T NETWORK, V5, P835, DOI 10.1109/90.650143
   HarcholBalter M, 1997, ACM T COMPUT SYST, V15, P253, DOI 10.1145/263326.263344
   Im S, 2018, J ACM, V65, DOI 10.1145/3136754
   Im SJ, 2014, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2014.63
   KARLIN AR, 1994, ALGORITHMICA, V11, P542, DOI 10.1007/BF01189993
   Karlin AR, 2003, ALGORITHMICA, V36, P209, DOI 10.1007/s00453-003-1013-x
   KARLIN AR, 1988, ALGORITHMICA, V3, P79, DOI 10.1007/BF01762111
   Khanafer A, 2013, IEEE INFOCOM SER, P1492
   Kodialam Rohan, 2014, SIAM UNDERGRADUATE R, V7, P233
   Kouvelis P., 2013, ROBUST DISCRETE OPTI, V14
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Lykouris Thodoris, 2018, ICML, P3302
   Mahdian M, 2012, ACM T ALGORITHMS, V8, DOI 10.1145/2071379.2071381
   Medina Andres Munoz, 2017, P NIPS, P1856
   Meyerson A, 2005, ANN IEEE SYMP FOUND, P274, DOI 10.1109/SFCS.2005.72
   Mirrokni Vahab S., 2012, P 23 ANN ACM SIAM S, P1690, DOI DOI 10.1137/1.9781611973099.134
   MOTWANI R, 1994, THEOR COMPUT SCI, V130, P17, DOI 10.1016/0304-3975(94)90151-1
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004024
DA 2019-06-15
ER

PT S
AU Kumaraswamy, R
   Schlegel, M
   White, A
   White, M
AF Kumaraswamy, Raksha
   Schlegel, Matthew
   White, Adam
   White, Martha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Context-Dependent Upper-Confidence Bounds for Directed Exploration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration, either through visitation estimates or upper-confidence bounds, as opposed to data-inefficient strategies like epsilon-greedy that use random, undirected exploration. Most data-efficient exploration methods require significant computation, typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches-because they summarize past interactions-with the computation closer to that of model-free approaches. In this work, we provide a novel, computationally efficient, incremental exploration strategy, leveraging this property of least-squares temporal difference learning (LSTD). We derive upper-confidence bounds on the action-values learned by LSTD, with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states, and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.
C1 [Kumaraswamy, Raksha; Schlegel, Matthew; White, Adam; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
   [White, Adam] DeepMind, London, England.
RP Kumaraswamy, R (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM kumarasw@ualberta.ca; mkschleg@ualberta.ca; adamwhite@google.com;
   whitem@ualberta.ca
CR Abbasi-Yadkori Y., 2014, UNCERTAINTY ARTIFICI
   Auer P., 2006, ADV NEURAL INFORM PR
   Bartlett P. L., 2009, C UNC ART INT
   Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646
   Bradtke SJ, 1996, MACH LEARN, V22, P33, DOI 10.1023/A:1018056104778
   Brafman R., 2003, J MACHINE LEARNING R
   Chu W., 2011, INT C ART INT STAT
   Grande R., 2014, INT C MACH LEARN
   Jaksch T., 2010, J MACHINE LEARNING R
   Jong N., 2007, ABSTRACTION REFORMUL
   Jung T., 2010, MACHINE LEARNING ECM
   Kaelbling L. P., 1996, J ARTIFICIAL INTELLI
   Kaelbling Leslie Pack, 1993, LEARNING EMBEDDED SY
   Kakade S., 2003, INT C MACH LEARN
   Kawaguchi K., 2016, AAAI C ART INT
   Kearns M. J., 2002, MACHINE LEARNING
   Lagoudakis M. G., 2003, J MACHINE LEARNING R
   Levine N., 2017, ADV NEURAL INFORM PR, P3138
   Li L., 2009, INT C AUT AG MULT SY
   Li L., 2010, WORLD WID WEB C
   Martin J., 2017, INT JOINT C ART INT
   Meuleau N., 1999, MACHINE LEARNING
   MEYER CD, 1973, SIAM J APPL MATH, V24, P315
   Miller K. S., 1981, MATH MAG, V54, P67, DOI DOI 10.2307/2690437
   Moerland T. M., 2017, ADV NEURAL INFORM PR
   Nouri A., 2009, ADV NEURAL INFORM PR
   Ortner R., 2012, ADV NEURAL INFORM PR
   Osband I., 2013, ADV NEURAL INFORM PR
   Osband I., 2017, INT C MACH LEARN
   Osband I., 2016, ADV NEURAL INFORM PR
   Osband I., 2016, INT C MACH LEARN
   Ostrovski G., 2017, INT C MACH LEARN
   Pazis J., 2013, AAAI C ART INT
   Plappert M., 2017, PARAMETER SPACE NOIS
   Singh S. P., 2000, MACHINE LEARNING
   Strehl A., 2004, INT C MACH LEARN
   Strehl A. L., 2006, INT C MACH LEARN
   Sutton R., 2008, C UNC ART INT
   Sutton R., 1988, MACHINE LEARNING
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Szita I., 2010, INT C MACH LEARN
   Szita I., 2008, INT C MACH LEARN
   van Seijen H., 2015, INT C MACH LEARN
   White M., 2010, ADV NEURAL INFORM PR
   White M., 2017, INT C MACH LEARN
   Wiering M. A., 1998, SIMULATION ADAPTIVE
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304076
DA 2019-06-15
ER

PT S
AU Kurutach, T
   Tamar, A
   Yang, G
   Russell, S
   Abbeel, P
AF Kurutach, Thanard
   Tamar, Aviv
   Yang, Ge
   Russell, Stuart
   Abbeel, Pieter
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Plannable Representations with Causal InfoGAN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FRAMEWORK
AB In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans - a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation(3).
C1 [Kurutach, Thanard; Tamar, Aviv; Russell, Stuart; Abbeel, Pieter] Univ Calif Berkeley, Berkeley AI Res, Berkeley, CA 94720 USA.
   [Yang, Ge] Univ Chicago, Dept Phys, Chicago, IL 60637 USA.
RP Kurutach, T (reprint author), Univ Calif Berkeley, Berkeley AI Res, Berkeley, CA 94720 USA.
FU ONR PECASE [N000141612723]; Siemens; Technion Viterbi scholarship
FX This work was funded in part by ONR PECASE N000141612723 and Siemens.
   Thanard Kurutach and Aviv Tamar were supported by ONR PECASE
   N000141612723. Aviv Tamar was partially supported by the Technion
   Viterbi scholarship. The authors wish to thank Tom Zahavy and Aravind
   Srinivas for sharing their code for our comparisons with MDP state
   aggregation baselines, and Elinor Tamar for timely assistance in
   preprocessing the rope data.
CR Agrawal Pulkit, 2016, ADV NEURAL INFORM PR, P5074
   Andrychowicz M, 2017, ADV NEURAL INFORM PR, P5048
   Baram  N., 2016, SPATIO TEMPORAL ABST
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Corneil  D., 2018, ARXIV180204325
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Denton Emily L, 2017, ADV NEURAL INFORM PR, P4414
   Donahue J., 2016, ARXIV160509782
   Fikes R. E., 1981, READINGS ARTIFICIAL, P231
   Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324
   Finn C., 2016, INT C MACH LEARN, P49
   Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Jang Eric, 2016, ARXIV161101144
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Kansky  K., 2017, INT C MACH LEARN, V70, P1809
   Karras Tero, 2017, ARXIV171010196
   Kingma D. P., 2013, ARXIV13126114
   Konidaris G, 2018, J ARTIF INTELL RES, V61, P215, DOI 10.1613/jair.5575
   Lerer  A., 2016, ARXIV160301312
   Levine  S., 2016, JMLR, V17
   Liu  M., 2017, ARXIV171204065
   Machado Marlos C, 2017, ARXIV170300956
   Mahadevan S, 2007, J MACH LEARN RES, V8, P2169
   Mannor S., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair Ashvin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2146, DOI 10.1109/ICRA.2017.7989247
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Oh Junhyuk, 2017, ADV NEURAL INFORM PR, P6118
   Pong  V., 2018, ABS180209081 CORR
   Riedmiller M., 2018, ARXIV180210567
   Russell S., 2010, ARTIFICIAL INTELLIGE
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Santoro A, 2017, ADV NEURAL INFORM PR, P4974
   Savinov N., 2018, ARXIV180300653
   Simester DI, 2006, MANAGE SCI, V52, P683, DOI 10.1287/mnsc.1050.0504
   Srinivas  A., 2016, ARXIV160505359
   Srinivas  A., 2018, ARXIV180400645
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Tamar A, 2016, ADV NEURAL INFORM PR, P2146
   Vallati M, 2015, AI MAG, V36, P90, DOI 10.1609/aimag.v36i3.2571
   Wang  W., 2017, ARXIV171108534
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Watters  N., 2017, ARXIV170601433
   Wu Jiajun, 2017, ADV NEURAL INFORM PR, P152
   Zhu J.-Y., 2017, ADV NEURAL INFORM PR, V30, P465
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003030
DA 2019-06-15
ER

PT S
AU Kusupati, A
   Singh, M
   Bhatia, K
   Kumar, A
   Jain, P
   Varma, M
AF Kusupati, Aditya
   Singh, Manish
   Bhatia, Kush
   Kumar, Ashish
   Jain, Prateek
   Varma, Manik
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated
   Recurrent Neural Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at [30].
C1 [Kusupati, Aditya; Jain, Prateek; Varma, Manik] Microsoft Res India, Bengaluru, India.
   [Singh, Manish] Indian Inst Technol Delhi, New Delhi, India.
   [Bhatia, Kush; Kumar, Ashish] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Kusupati, A (reprint author), Microsoft Res India, Bengaluru, India.
EM t-vekusu@microsoft.com; singhmanishiitd@gmail.com; kush@cs.berkeley.edu;
   ashish_kumar@berkeley.edu; prajain@microsoft.com; manik@microsoft.com
FU NSF [IIS-1619362]; AFOSR [FA9550-17-1-0308]
FX We are grateful to Ankit Anand, Niladri Chatterji, Kunal Dahiya, Don
   Dennis, Inderjit S. Dhillon, Dinesh Khandelwal, Shishir Patil, Adithya
   Pratapa, Harsha Vardhan Simhadri and Raghav Somani for helpful
   discussions and feedback. KB acknowledges the support of the NSF through
   grant IIS-1619362 and of the AFOSR through grant FA9550-17-1-0308.
CR Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070
   Altun K, 2010, PATTERN RECOGN, V43, P3605, DOI 10.1016/j.patcog.2010.04.019
   Anguita D., 2012, LECT NOTES COMPUTER, V7657, P216, DOI DOI 10.1007/978-3-642-35395-6_
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Bhatia  K., EXTREME CLASSIFICATI
   Bradbury  J., 2016, ARXIV161101576
   Campos  V., 2018, INT C LEARN REPR
   Chen GG, 2015, INT CONF ACOUST SPEE, P5236, DOI 10.1109/ICASSP.2015.7178970
   Cho K., 2014, ARXIV14091259
   Collins J., 2016, ARXIV161109913
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Golowich  N., 2017, ARXIV171206541
   Guoguo Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4087, DOI 10.1109/ICASSP.2014.6854370
   Gupta  C., 2017, P INT C MACH LEARN
   Han S, 2016, ICLR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Inan  Hakan, 2016, ARXIV161101462
   Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016
   Jing  L., 2017, ARXIV170602761
   Jing  L., 2017, INT C MACH LEARN
   Jose  C., 2018, INT C MACH LEARN, V80, P2380
   Kanai  S., 2017, P NEUR INF PROC SYST, P435
   Kepuska Z, 2009, NONLINEAR ANAL-THEOR, V71, pE2772, DOI 10.1016/j.na.2009.06.089
   Kingma D. P., 2014, ARXIV14126980
   Kumar  A., 2017, P INT C MACH LEARN
   Kusupati  A., 2017, EDGEML LIB ML LIB MA
   Le Q. V., 2015, ARXIV150400941
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   McAuley J., 2013, P 7 ACM C REC SYST, V13, P165, DOI DOI 10.1145/2507157.2507163
   Melis G., 2017, ARXIV170705589
   Merity S., 2017, ARXIV170802182
   Mhammedi  Z., 2017, INT C MACH LEARN
   Mikolov  T., 2012, SLT, V12, P8
   Narang  S., 2017, ARXIV170405119
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sainath TN, 2015, 16 ANN C INT SPEECH
   Siri Team Apple, 2017, HEY SIR ON DEV DNN
   Srivastava R. K., 2015, ARXIV150500387
   STCI Microsoft, WAK DAT
   Susto GA, 2015, IEEE T IND INFORM, V11, P812, DOI 10.1109/TII.2014.2349359
   Vorontsov  E., 2017, INT C MACH LEARN
   Wang ZS, 2017, IEEE T VLSI SYST, V25, P2763, DOI 10.1109/TVLSI.2017.2717950
   Warden P., 2018, ARXIV180403209
   Wisdom S., 2016, ADV NEURAL INFORM PR, P4880
   Ye  J., 2017, ARXIV171205134
   Yelp Inc, 2017, YELP DAT CHALL
   Zaremba W, 2014, ARXIV14092329
   Zhang  J., 2018, INT C MACH LEARN
NR 54
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003056
DA 2019-06-15
ER

PT S
AU La Tour, TD
   Moreau, T
   Jas, M
   Gramfort, A
AF La Tour, Tom Dupre
   Moreau, Thomas
   Jas, Mainak
   Gramfort, Alexandre
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multivariate Convolutional Sparse Coding for Electromagnetic Brain
   Signals
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHM
AB Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8-12 Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.
C1 [La Tour, Tom Dupre; Jas, Mainak] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
   [Moreau, Thomas; Gramfort, Alexandre] Univ Paris Saclay, INRIA, Saclay, France.
RP La Tour, TD (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
FU ERC [SLAB ERC-YStG-676943]; ANR THALAMEEG [ANR-14-NEUC-0002-01]
FX This work was supported by the ERC Starting Grant SLAB ERC-YStG-676943
   and by the ANR THALAMEEG ANR-14-NEUC-0002-01.
CR Barthelemy Q, 2013, J NEUROSCI METH, V215, P19, DOI 10.1016/j.jneumeth.2013.02.001
   Barthelemy Q, 2012, IEEE T SIGNAL PROCES, V60, P1597, DOI 10.1109/TSP.2012.2183129
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Brockmeier AJ, 2016, IEEE T BIO-MED ENG, V63, P43, DOI 10.1109/TBME.2015.2499241
   Buzsaki G., 2006, RHYTHMS BRAIN
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Chalasani R., 2013, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2013.6706854
   Cole S. R., 2017, TRENDS COGN SCI
   Cole S. R., 2018, CYCLE BY CYCLE ANAL
   Cole SR, 2017, J NEUROSCI, V37, P4830, DOI 10.1523/JNEUROSCI.2208-16.2017
   Durka PJ, 2005, J NEUROSCI METH, V148, P49, DOI 10.1016/j.jneumeth.2005.04.001
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Garcia-Cardona C., 2017, ARXIV170902893
   Gips B, 2017, J NEUROSCI METH, V275, P66, DOI 10.1016/j.jneumeth.2016.11.001
   Gramfort A, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00267
   Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027
   Grosse R., 2013, 23 C UNC ART INT UAI, P149
   Hari R., 2017, MEG EEG PRIMER
   Hari R, 2006, PROG BRAIN RES, V159, P253, DOI 10.1016/S0079-6123(06)59017-X
   Hastie  T., 2015, STAT LEARNING SPARSI
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hitziger S., 2017, IEEE T SIGNAL PROCES
   Jas M., 2017, ADV NEURAL INFORM PR, P1
   Jones SR, 2016, CURR OPIN NEUROBIOL, V40, P72, DOI 10.1016/j.conb.2016.06.010
   Jost P., 2006, ACOUSTICS SPEECH SIG, V5
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Li YY, 2009, INVERSE PROBL IMAG, V3, P487, DOI 10.3934/ipi.2009.3.487
   Locatello F., 2018, INT C MACH LEARN ICM, P3204
   Mailhe B., 2008, P 16 EUR SIGN PROC C, P1
   Mazaheri A, 2008, J NEUROSCI, V28, P7781, DOI 10.1523/JNEUROSCI.1631-08.2008
   Moreau T., 2018, INT C MACH LEARN ICM
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Pachitariu M, 2013, ADV NEURAL INFORM PR, P1745
   Python Software Foundation, 2017, PYTH LANG REF VERS 3
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Sorel M., 2016, DIGITAL SIGNAL PROCE
   TUOMISTO T, 1983, NUOVO CIMENTO D, V2, P471, DOI 10.1007/BF02455946
   van Ede F., 2018, TRENDS NEUROSCIENCES
   Wohlberg B, 2016, IEEE SW SYMP IMAG, P57, DOI 10.1109/SSIAI.2016.7459174
   Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260
   Wright S., 1999, NUMERICAL OPTIMIZATI, V35
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303030
DA 2019-06-15
ER

PT S
AU Lacombe, T
   Cuturi, M
   Oudot, S
AF Lacombe, Theo
   Cuturi, Marco
   Oudot, Steve
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Large Scale computation of Means and Clusters for Persistence Diagrams
   using Optimal Transport
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BARYCENTERS
AB Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of complex data. Despite several appealing properties, incorporating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. Indeed, this was recently exemplified in a string of papers which show that the simple task of averaging a few PDs can be computationally prohibitive. We propose in this article a tractable framework to carry out standard tasks on PDs at scale, notably evaluating distances, estimating barycenters and performing clustering. This framework builds upon a reformulation of PD metrics as optimal transport (OT) problems. Doing so, we can exploit recent computational advances: the OT problem on a planar grid, when regularized with entropy, is convex can be solved in linear time using the Sinkhom algorithm and convolutions. This results in scalable computations that can stream on GPUs. We demonstrate the efficiency of our approach by carrying out clustering with diagrams metrics on several thousands of PDs, a scale never seen before in the literature.
C1 [Lacombe, Theo; Oudot, Steve] Inria Saclay, Datashape, Palaiseau, France.
   [Cuturi, Marco] Google Brain, Mountain View, CA USA.
   [Cuturi, Marco] ENSAE, CREST, Palaiseau, France.
RP Lacombe, T (reprint author), Inria Saclay, Datashape, Palaiseau, France.
EM theo.lacombe@inria.fr; cuturi@google.com; steve.oudot@inria.fr
FU AMX; Ecole polytechnique; Chaire d'Excellence de l'Idex Paris-Saclay
FX We thank the anonymous reviewers for the fruitful discussion. TL was
   supported by the AMX, Ecole polytechnique. MC acknowledges the support
   of a Chaire d'Excellence de l'Idex Paris-Saclay.
CR Adams H, 2017, J MACH LEARN RES, V18
   Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741
   Altschuler J., 2017, ADV NEURAL INFORM PR, P1961
   Anderes E, 2016, MATH METHOD OPER RES, V84, P389, DOI 10.1007/s00186-016-0549-x
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439
   Bubenik P, 2015, J MACH LEARN RES, V16, P77
   Carlier G, 2015, ESAIM-MATH MODEL NUM, V49, P1621, DOI 10.1051/m2an/2015033
   Carriere M., 2017, 34 INT C MACH LEARN
   Carriere M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692
   Chazal F, 2014, GEOMETRIAE DEDICATA, V173, P193, DOI 10.1007/s10711-013-9937-z
   Chazal F, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P237, DOI 10.1145/1542362.1542407
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   Cuturi M., 2014, INT C MACH LEARN, P685
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Dvurechensky P., 2018, P MACHINE LEARNING R, V80, P1367
   Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454
   Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO
   Frechet M, 1948, ANN I H POINCARE, V10, P215
   Guittet K., 2002, THESIS
   Hiraoka Y, 2016, P NATL ACAD SCI USA, V113, P7035, DOI 10.1073/pnas.1520877113
   Kerber M., 2017, J EXPT ALGORITHMICS, V22, P1
   LI C, 2014, P IEEE C COMP VIS PA, P1995
   Lucet Y, 2010, SIAM REV, V52, P505, DOI 10.1137/100788458
   Lum PY, 2013, SCI REP-UK, V3, DOI 10.1038/srep01236
   Makarenko N, 2016, OPEN ENG, V6, P326, DOI 10.1515/eng-2016-0044
   Mileyko Y, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/12/124007
   Moreau J.J., 1965, B SOC MATH FRANCE, V93, P273, DOI 10.24033/bsmf.1625
   Nicolau M, 2011, P NATL ACAD SCI USA, V108, P7265, DOI 10.1073/pnas.1102826108
   Obayashi I., 2018, J APPL COMPUT TOPOLO, V1, P421, DOI DOI 10.1007/S41468-018-0013-5
   Peyre G., 2018, COMPUTATIONAL OPTIMA
   Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106
   Santambrogio F., 2015, OPTIMAL TRANSPORT AP
   Schrijver A., 1998, THEORY LINEAR INTEGE
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Turner  K., 2013, ARXIV13078300
   Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7
   Villani C., 2003, TOPICS OPTIMAL TRANS
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Zeppelzauer M, 2016, LECT NOTES COMPUT SC, V9667, P77, DOI 10.1007/978-3-319-39441-1_8
   Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004034
DA 2019-06-15
ER

PT S
AU Lage, I
   Ross, AS
   Kim, B
   Gershman, SJ
   Doshi-Velez, F
AF Lage, Isaac
   Ross, Andrew Slavin
   Kim, Been
   Gershman, Samuel J.
   Doshi-Velez, Finale
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Human-in-the-Loop Interpretability Prior
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.
C1 [Lage, Isaac; Ross, Andrew Slavin; Doshi-Velez, Finale] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA.
   [Kim, Been] Google Brain, Mountain View, CA USA.
   [Gershman, Samuel J.] Harvard Univ, Dept Psychol, Cambridge, MA 02138 USA.
RP Lage, I (reprint author), Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA.
EM isaaclage@g.harvard.edu; andrew_ross@g.harvard.edu; beenkim@google.com;
   gershman@fas.harvard.edu; finale@seas.harvard.edu
FU NIH [5T32LM012411-02]; Google Faculty Research Award; Harvard Dean's
   Competitive Fund
FX IL acknowledges support from NIH 5T32LM012411-02. All authors
   acknowledge support from the Google Faculty Research Award and the
   Harvard Dean's Competitive Fund. All authors thank Emily Chen and
   Jeffrey He for their support with the experimental interface, and Weiwei
   Pan and the Harvard DTaK group for many helpful discussions and
   insights.
CR Altendorf E.E., 2005, P 21 C UNC ART INT, P18
   Bach Francis R, 2010, ADV NEURAL INFORM PR, P118
   Caruana R, 2015, P 21 ACM SIGKDD INT, P1721, DOI [DOI 10.1145/2783258.2788613, 10.1145/2783258.2788613]
   Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939
   Christiano Paul F, 2017, ADV NEURAL INFORM PR, V30, P4299
   Chu W, 2004, IEEE T NEURAL NETWOR, V15, P29, DOI 10.1109/TNN.2003.820830
   Dheeru D., 2017, UCI MACHINE LEARNING
   Doshi-Velez F., 2017, RIGOROUS SCI INTERPR
   FREITAS A. A, 2014, ACM SIGKDD EXPLORATI, V15, P1, DOI DOI 10.1145/2594473.2594475
   Hinton G., 2010, MOMENTUM, V9, P926
   Kim Been, 2014, ADV NEURAL INFORM PR, P1952
   Kosorukoff Alex, 2001, P IEEE INT C SYST MA, V5
   Lakkaraju Himabindu, 2016, KDD, V2016, P1675
   Lavrac N, 1999, ARTIF INTELL MED, V16, P3, DOI 10.1016/S0933-3657(98)00062-1
   Lipton Z. C, 2016, ABS160603490 CORR
   Little G., 2010, P 23 ANN ACM S US IN, P57, DOI DOI 10.1145/1866029.1866040
   Ma Yifei, 2012, ABS12093694 CORR
   Masood Muhammad A., 2018, PARTICLE BASED VARIA
   Menaka Narayanan Emily, 2018, DO HUMANS UNDERSTAND
   Poursabzi-Sangdeh Forough, 2018, ABS180207810 CORR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Rokach Lior, 2014, INTRO DECISION TREES, P1
   Ross Andrew, 2018, 2018 ICML WORKSH HUM
   Ross Andrew, 2017, WORKSH TRANSP INT MA
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N, 2009, ARXIV09123995
   Tamuz Omer, 2011, ABS11051033 CORR
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Ustun B, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1125, DOI 10.1145/3097983.3098161
   Wilson Andrew G, 2015, ADV NEURAL INFORM PR, V28, P2854
   Wirth C, 2017, J MACH LEARN RES, V18
   Wu Mike, 2018, P 32 AAAI C ART INT
   Zhu X., 2003, ICML 2003 WORKSH CON, P58
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004069
DA 2019-06-15
ER

PT S
AU Laha, A
   Chemmengath, SA
   Agrawal, P
   Khapra, MM
   Sankaranarayanan, K
   Ramaswamy, HG
AF Laha, Anirban
   Chemmengath, Saneem A.
   Agrawal, Priyanka
   Khapra, Mitesh M.
   Sankaranarayanan, Karthik
   Ramaswamy, Harish G.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On Controllable Sparse Alternatives to Softmax
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.
C1 [Laha, Anirban; Chemmengath, Saneem A.; Agrawal, Priyanka; Sankaranarayanan, Karthik] IBM Res, Yorktown Hts, NY 10598 USA.
   [Khapra, Mitesh M.; Ramaswamy, Harish G.] IIT Madras, Robert Bosch Ctr DS & AI, Chennai, Tamil Nadu, India.
   [Khapra, Mitesh M.; Ramaswamy, Harish G.] IIT Madras, Dept CSE, Chennai, Tamil Nadu, India.
RP Laha, A; Chemmengath, SA (reprint author), IBM Res, Yorktown Hts, NY 10598 USA.
EM anirlaha@in.ibm.com; saneem.cg@in.ibm.com
CR Aly M., 2005, NEURAL NETWORKS, P1
   Bach Francis, 2011, FDN TRENDS R MACHINE
   Bahdanau D., 2015, P INT C LEARN REPR I
   Bridle J. S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P227
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Gao B., 2017, ARXIV E PRINTS
   Kapoor A., 2012, ADV NEURAL INFORM PR, P2645
   Klein Guillaume, 2017, CORR
   Martins A., 2016, INT C MACH LEARN, P1614
   Nema Preksha, 2017, P 55 ANN M ASS COMP
   Niculae Vlad, 2017, P NEUR INF PROC SYST, V30, P3340
   Rush A.M., 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044
   Sorower M. S., 2010, LIT SURVEY ALGORITHM
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Vincent Pascal, 2016, P INT C LEARN REPR I
   Vincent Pascal, 2015, ADV NEURAL INFORM PR, V1, P1108
   Xu K, 2015, INT C MACH LEARN, V32, P2048
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000088
DA 2019-06-15
ER

PT S
AU Lan, X
   Zhu, XT
   Gong, SG
AF Lan, Xu
   Zhu, Xiatian
   Gong, Shaogang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Knowledge Distillation by On-the-Fly Native Ensemble
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.
C1 [Lan, Xu; Gong, Shaogang] Queen Mary Univ London, London, England.
   [Zhu, Xiatian] Vis Semant Ltd, London, England.
RP Lan, X (reprint author), Queen Mary Univ London, London, England.
FU China Scholarship Council; Royal Society Newton Advanced Fellowship
   Programme [NA150459]; Innovate UK Industrial Challenge Project on
   Developing and Commercialising Intelligent Video Analytics Solutions for
   Public Safety [98111-571149]; Vision Semantics Limited
FX This work was partly supported by the China Scholarship Council, Vision
   Semantics Limited, the Royal Society Newton Advanced Fellowship
   Programme (NA150459), and Innovate UK Industrial Challenge Project on
   Developing and Commercialising Intelligent Video Analytics Solutions for
   Public Safety (98111-571149).
CR Anil Rohan, 2018, INT C LEARN REPR
   Ba J., 2014, ADV NEURAL INFORM PR
   Bucilua Cristian, 2006, P 12 ACM SIGKDD
   Chaudhari Pratik, 2016, ENTROPY SGD BIASING
   Furlanello Tommaso, 2018, BORN NEURAL NETWORKS
   Girshick R, 2015, IEEE INT C COMP VIS
   Guo YQ, 2017, IEEE INT C INTELL TR
   Han  S., 2016, INT C LEARN REPR
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hinton Geoffrey, 2015, DISTILLING KNOWLEDGE
   Huang G., 2016, DENSELY CONNECTED CO
   Huang  G., 2017, CONDENSENET EFFICIEN
   Huang  Gao, 2017, INT C LEARN REPR
   Huang GL, 2017, IEEE ICC
   Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353
   Keskar N. S., 2016, LARGE BATCH TRAINING
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lan Xu, 2018, P AS C COMP VIS
   Lan Xu, 2018, EUR C COMP VIS
   Lee C. Y., 2015, ARTIF INTELL, P562
   Li Hao, 2017, INT C LEARN REPR
   Li Shen, 2017, SQUEEZE AND EXCITATI
   Long Jonathan, 2015, IEEE C COMP VIS PATT
   Rastegari Mohammad, 2016, EUR C COMP VIS
   Romero A., 2014, FITNETS HINTS THIN D
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, VERY DEEP CONVOLUTIO
   Szegedy C., 2016, IEEE C COMP VIS PATT
   Szegedy C, 2015, IEEE C COMP VIS PATT
   Xie Saining, 2017, IEEE C COMP VIS PATT
   Yim Junho, 2017, IEEE C COMP VIS PATT
   Zagoruyko S., 2016, WIDE RESIDUAL NETWOR
   Zhang Ying, 2017, DEEP MUTUAL LEARNING
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002010
DA 2019-06-15
ER

PT S
AU Lange-Hegermann, M
AF Lange-Hegermann, Markus
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Algorithmic Linearly Constrained Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COMPLEXITY
AB We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Grobner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathematics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computer algebra in a novel way, we combine noisy observations with precise algebraic computations.
C1 [Lange-Hegermann, Markus] Ostwestfalen Lippe Univ Appl Sci, Dept Elect Engn & Comp Sci, Lemgo, Germany.
RP Lange-Hegermann, M (reprint author), Ostwestfalen Lippe Univ Appl Sci, Dept Elect Engn & Comp Sci, Lemgo, Germany.
EM markus.lange-hegermann@hs-owl.de
CR Adams William W., 1994, GRADUATE STUDIES MAT
   Barakat  Mohamed, 2010, P 19 INT S MATH THEO, P1657
   BAYER D, 1988, J SYMB COMPUT, V6, P135, DOI 10.1016/S0747-7171(88)80039-7
   Bertinet  A., 2004, REPRODUCING KERNEL H
   Buchberger B, 2006, J SYMB COMPUT, V41, P475, DOI 10.1016/j.jsc.2005.09.007
   Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626
   Chyzak F, 2005, APPL ALGEBR ENG COMM, V16, P319, DOI 10.1007/s00200-005-0188-6
   Decker W., 2016, SINGULAR 4 1 0 COMPU
   Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218
   Dong  Kun, 2017, ARXIV171103481
   Duvenaud D, 2014, THESIS
   Eisenbud D., 1995, GRADUATE TEXTS MATH, V150
   Garnett  Roman, 2015, JMLR WORKSHOP C P, V37, P1025
   Gerdt VP, 2005, NATO SC S SS III C S, V196, P199
   Graepel  Thore, 2003, ICML, P234
   Grayson DR, MACAULAY2 SOFTWARE S
   Greuel G-M, 2002, SINGULAR INTRO COMMU
   Hensman  James, 2013, P 29 C UNC ART INT
   Honkela A, 2015, P NATL ACAD SCI USA, V112, P13115, DOI 10.1073/pnas.1420404112
   Izmailov  Pavel, 2017, ARXIVMATH171007324
   Jang Phillip A, 2017, ADV NEURAL INFORM PR, V30, P3940
   Jaynes E. T., 2003, PROBABILITY THEORY L
   JAYNES ET, 1968, IEEE T SYST SCI CYB, VSSC4, P227, DOI 10.1109/TSSC.1968.300117
   Jidling  Carl, 2017, ARXIV170300787
   Lee J, 2017, ARXIV171100165
   Levandovskyy V, 2005, THESIS
   Levandovskyy  Viktor, 2003, P 2003 INT S SYMB AL, P176, DOI DOI 10.1145/860854.860895
   Macedo  Ives, 2008, TECH REP
   MAYR E, 1989, LECT NOTES COMPUT SC, V349, P400
   MAYR EW, 1982, ADV MATH, V46, P305, DOI 10.1016/0001-8708(82)90048-2
   OBERST U, 1990, ACTA APPL MATH, V20, P1, DOI 10.1007/BF00046908
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Quadrat A, 2013, ACTA APPL MATH, V127, P27, DOI 10.1007/s10440-012-9791-2
   Quadrat  Alban, 2010, COURS CIRM, V1, P279
   Quadrat  Alban, 2010, THESIS
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robertz D, 2015, MULTIDIM SYST SIGN P, V26, P349, DOI 10.1007/s11045-014-0280-9
   Robertz  Daniel, 2006, THESIS
   Robertz  Daniel, 2003, JANETORE MAPLE PACKA
   Scheuerer M, 2012, STOCH MODELS, V28, P433, DOI 10.1080/15326349.2012.699756
   Seiler Werner M., 2010, PAMM, V10, P633
   Simon-Gabriel  C.-J., 2016, ARXIV160405251
   Snelson E, 2003, ADV NEURAL INFORM PR, V14, P337
   Solin  Arno, 2015, ARXIV150904634
   Sturmfels B., 2005, NOT AM MATH SOC, V52, P2
   Thewes  Silja, 2015, DESIGN EXPT DOE POWE
   Titsias M, 2009, ARTIF INTELL, P567
   Treves  F., 1967, DOVER BOOKS MATH
   Wahlstrom  Niklas, 2013, P 38 INT C AC SPEECH
   Wilson A., 2013, P 30 INT C MACH LEAR, P1067
   Wilson A. G., 2015, ARXIV151101870
   Wilson Andrew G., 2015, ARXIV151102222
   Zerz E, 2010, COMMUN ALGEBRA, V38, P2037, DOI 10.1080/00927870903015226
   Zerz  Eva, 2000, LECT NOTES CONTROL I, V256
NR 54
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302017
DA 2019-06-15
ER

PT S
AU Lattimore, T
   Kveton, B
   Li, S
   Szepesvari, C
AF Lattimore, Tor
   Kveton, Branislav
   Li, Shuai
   Szepesvari, Csaba
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI TopRank: A Practical Algorithm for Online Stochastic Ranking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, and (d) outperforms existing algorithms empirically.
C1 [Lattimore, Tor; Szepesvari, Csaba] DeepMind, London, England.
   [Kveton, Branislav] Google, Mountain View, CA USA.
   [Li, Shuai] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Szepesvari, Csaba] Univ Alberta, Edmonton, AB, Canada.
RP Lattimore, T (reprint author), DeepMind, London, England.
CR Agichtein E., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P19, DOI 10.1145/1148170.1148177
   Chuklin A, 2015, CLICK MODELS WEB SEA
   Combes R., 2015, P 2015 ACM SIGMETRIC
   Grotov A., 2015, P 6 INT C CLEF ASS
   Katariya S., 2016, INT C MACH LEARN, P1215
   Kveton B., 2015, P 32 INT C MACH LEAR
   Kveton B, 2015, ADV NEURAL INFORM PR, P1450
   Lagree P, 2016, ADV NEURAL INFORM PR, P1597
   Lattimore Tor, 2017, P MACHINE LEARNING R, P728
   Li S, 2016, P 33 INT C MACH LEAR, P1245
   Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3
   Radlinski F., 2008, P 25 INT C MACH LEAR, P784, DOI DOI 10.1145/1390156.1390255
   Slivkins A, 2013, J MACH LEARN RES, V14, P399
   Uchiya T, 2010, LECT NOTES ARTIF INT, V6331, P375, DOI 10.1007/978-3-642-16108-7_30
   Zoghi M, 2017, INT C MACH LEARN, P4199
   Zoghi M, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/2911451.2911500
   Zong S., 2016, P 32 C UNC ART INT
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303090
DA 2019-06-15
ER

PT S
AU Laue, S
   Mitterreiter, M
   Giesen, J
AF Laue, Soeren
   Mitterreiter, Matthias
   Giesen, Joachim
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Computing Higher Order Derivatives of Matrix and Tensor Expressions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup of up to two orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives on CPUs and a speedup of about three orders of magnitude on GPUs.
C1 [Laue, Soeren; Mitterreiter, Matthias; Giesen, Joachim] Friedrich Schiller Univ Jena, Jena, Germany.
RP Laue, S (reprint author), Friedrich Schiller Univ Jena, Jena, Germany.
EM soeren.laue@uni-jena.de; matthias.mitterreiter@uni-jena.de;
   joachim.giesen@uni-jena.de
FU Deutsche Forschungsgemeinschaft (DFG) [LA 2971/1-1]; AWS Cloud Credits
   for Research program
FX Soren Laue has been funded by Deutsche Forschungsgemeinschaft (DFG)
   under grant LA 2971/1-1. This work has also been supported by the AWS
   Cloud Credits for Research program and by a gift from Google.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Baydin AG, 2018, J MACH LEARN RES, V18
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Broomhead D. S., 1988, Complex Systems, V2, P321
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   COX DR, 1958, J ROY STAT SOC B, V20, P215
   Gebremedhin AH, 2009, INFORMS J COMPUT, V21, P209, DOI 10.1287/ijoc.1080.0286
   Giles MB, 2008, ADV AUTOMATIC DIFFER, P35, DOI DOI 10.1007/978-3-540-68942-3
   Griewank A, 2008, EVALUATING DERIVATIV
   Hascoet L, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2450153.2450158
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Maclaurin Dougal, 2015, ICML AUTOML WORKSH
   Magnus JR, 2007, MATRIX DIFFERENTIAL
   Paszke  A., 2017, NIPS AUT WORKSH
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ricci MG, 1901, MATH ANN, V54, P125
   Scholkopf B, 2002, ADAPTIVE COMPUTATION
   Seeger Matthias W., 2017, NIPS AUT WORKSH
   Theano Development Team, 2016, ARXIV E PRINTS
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Walther A, 2012, CH CRC COMP SCI SER, P181
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302074
DA 2019-06-15
ER

PT S
AU Law, HCL
   Sejdinovic, D
   Cameron, E
   Lucas, TCD
   Flaxman, S
   Battle, K
   Fukumizu, K
AF Law, Ho Chung Leon
   Sejdinovic, Dino
   Cameron, Ewan
   Lucas, Tim C. D.
   Flaxman, Seth
   Battle, Katherine
   Fukumizu, Kenji
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Variational Learning on Aggregate Outputs with Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PLASMODIUM-FALCIPARUM; AFRICA
AB While a typical supervised learning framework assumes that the inputs and the outputs are measured at the same levels of granularity, many applications, including global mapping of disease, only have access to outputs at a much coarser level than that of the inputs. Aggregation of outputs makes generalization to new inputs much more difficult. We consider an approach to this problem based on variational learning with a model of output aggregation and Gaussian processes, where aggregation leads to intractability of the standard evidence lower bounds. We propose new bounds and tractable approximations, leading to improved prediction accuracy and scalability to large datasets, while explicitly taking uncertainty into account. We develop a framework which extends to several types of likelihoods, including the Poisson model for aggregated count data. We apply our framework to a challenging and important problem, the fine-scale spatial modelling of malaria incidence, with over 1 million observations.
C1 [Law, Ho Chung Leon; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England.
   [Cameron, Ewan; Lucas, Tim C. D.; Battle, Katherine] Univ Oxford, Big Data Inst, Oxford, England.
   [Flaxman, Seth] Imperial Coll London, Dept Math, London, England.
   [Flaxman, Seth] Imperial Coll London, Data Sci Inst, London, England.
   [Fukumizu, Kenji] Inst Stat Math, Tachikawa, Tokyo, Japan.
   [Sejdinovic, Dino] Alan Turing Inst, London, England.
RP Law, HCL (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM ho.law@stats.ox.ac.uk; dino.sejdinovic@stats.ox.ac.uk;
   dr.ewan.cameron@gmail.com; timcdlucas@gmail.com;
   s.flaxman@imperial.ac.uk; kather-ine.battle@bdi.ox.ac.uk;
   fukumizu@ism.ac.jp
FU EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; JSPS KAKENHI
   [26280009]; ERC [FP7/617071]; Alan Turing Institute [EP/N510129/1]; Bill
   and Melinda Gates Foundation;  [OPP1152978];  [OPP1132415]; 
   [OPP1106023]
FX We thank Kaspar Martens for useful discussions, and Dougal Sutherland
   for providing the code base in which this work was based on. HCLL is
   supported by the EPSRC and MRC through the OxWaSP CDT programme
   (EP/L016710/1). HCLL and KF are supported by JSPS KAKENHI 26280009. EC
   and KB are supported by OPP1152978, TL by OPP1132415 and the MAP
   database by OPP1106023. DS is supported in part by the ERC (FP7/617071)
   and by The Alan Turing Institute (EP/N510129/1). The data were provided
   by the Malaria Atlas Project supported by the Bill and Melinda Gates
   Foundation.
CR Ancarani LU, 2008, J MATH PHYS, V49, DOI 10.1063/1.2939395
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bhatt S, 2015, NATURE, V526, P207, DOI 10.1038/nature15535
   Cheplygina V, 2015, PATTERN RECOGN LETT, V59, P11, DOI 10.1016/j.patrec.2015.03.008
   Gething PW, 2016, NEW ENGL J MED, V375, P2435, DOI 10.1056/NEJMoa1606701
   Goovaerts P, 2010, MATH GEOSCI, V42, P535, DOI 10.1007/s11004-010-9286-5
   Haussmann Manuel, 2017, P IEEE C COMP VIS PA, P6570
   Hensman J, 2015, P 18 INT C ART INT S, P351
   Hensman J., 2013, GAUSSIAN PROCESSES B
   Ho Chung Leon Law, 2018, INT C ART INT STAT A, P1167
   Howitt R, 2003, EUR REV AGRIC ECON, V30, P359, DOI 10.1093/erae/30.3.359
   Keil P, 2013, METHODS ECOL EVOL, V4, P82, DOI 10.1111/j.2041-210x.2012.00264.x
   Kingma D. P., 2014, ARXIV14126980
   Kotzias D., 2015, ACM SIGKDD INT C KNO, P597, DOI DOI 10.1145/2783258.2783380
   Kueck H., 2005, P 21 C UNC ART INT, P332
   Law H. C. L., 2017, NIPS
   Lloyd C., 2015, INT C MACH LEARN, P1814
   Melnikov Vitalik, 2016, JOINT EUR C MACH LEA, P756
   Muandet Krikamol, 2016, ARXIV160509522
   Musicant DR, 2007, IEEE DATA MINING, P252, DOI 10.1109/ICDM.2007.50
   Nickisch H, 2008, J MACH LEARN RES, V9, P2035
   Patrini Giorgio, 2014, NIPS
   Quadrianto N, 2009, J MACH LEARN RES, V10, P2349
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Smola AJ, 2001, ADV NEUR IN, V13, P619
   Szabo Z, 2016, J MACH LEARN RES, V17
   Teh Y.-W., 2007, ADV NEURAL INFORM PR, P1353
   Titsias M, 2009, ARTIF INTELL, P567
   Warrel DA, 2017, OXFORD TXB MED
   Weiss DJ, 2015, MALARIA J, V14, DOI 10.1186/s12936-015-0574-x
   Xavier A, 2018, SPAT STAT-NETH, V23, P91, DOI 10.1016/j.spasta.2017.11.005
   Yu F. X., 2014, ARXIV14025902
   Yu F. X., 2013, ARXIV13060886
   Zaheer M., 2017, NIPS
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000057
DA 2019-06-15
ER

PT S
AU Lazic, N
   Lu, T
   Boutilier, C
   Ryu, M
   Wong, E
   Roy, B
   Imwalle, G
AF Lazic, Nevena
   Lu, Tyler
   Boutilier, Craig
   Ryu, Moonkyung
   Wong, Eehern
   Roy, Binz
   Imwalle, Greg
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Data center cooling using model-predictive control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SYSTEMS
AB Despite the impressive recent advances in reinforcement learning (RL) algorithms, their deployment to real-world physical systems is often complicated by unexpected events, limited data, and the potential for expensive failures. In this paper, we describe an application of RL "in the wild" to the task of regulating temperatures and airflow inside a large-scale data center (DC). Adopting a data-driven, model-based approach, we demonstrate that an RL agent with little prior knowledge is able to effectively and safely regulate conditions on a server floor after just a few hours of exploration, while improving operational efficiency relative to existing PID controllers.
C1 [Lazic, Nevena; Lu, Tyler; Boutilier, Craig; Ryu, Moonkyung] Google Res, Mountain View, CA 94043 USA.
   [Wong, Eehern; Roy, Binz; Imwalle, Greg] Google Cloud, Mountain View, CA USA.
RP Lazic, N (reprint author), Google Res, Mountain View, CA 94043 USA.
EM nevena@google.com; tylerlu@google.com; cboutilier@google.com;
   mkryu@google.com; ejwong@google.com; binzroy@google.com;
   gregi@google.com
CR Abbasi-Yadkori Yasin, 2015, UAI, P1
   Abbasi-Yadkori Yasin, 2011, COMPUTATIONAL LEARNI
   Abeille Marc, 2017, AISTATS
   ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X
   ASTROM KJ, 1973, AUTOMATICA, V9, P185, DOI 10.1016/0005-1098(73)90073-3
   Barroso Luiz Andre, 2013, SYNTHESIS LECT COMPU, V8, P3
   Bittanti S, 2006, COMMUN INF SYST, V6, P299
   Box G.E., 2015, TIME SERIES ANAL FOR
   Chen J., 2000, CONTROL ORIENTED SYS, V19
   Dean S, 2017, ARXIV171001688
   Feng JJ, 2015, ENERG BUILDINGS, V87, P199, DOI 10.1016/j.enbuild.2014.11.037
   Gao J, 2014, MACHINE LEARNING APP
   Hardt Moritz, 2016, ARXIV160905191
   Hayes M. H., 1996, STAT DIGITAL SIGNAL
   HELMICKI AJ, 1991, IEEE T AUTOMAT CONTR, V36, P1163, DOI 10.1109/9.90229
   Ibrahimi M, 2012, ADV NEURAL INFORM PR, V25, P2636
   Kelman A., 2011, P 2011 IFAC WORLD C
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Ljung L, 1999, SYSTEM IDENTIFICATIO
   Ljung Lennart, 1983, THEORY PRACTICE RECU, V5
   Ma YD, 2012, IEEE T CONTR SYST T, V20, P796, DOI 10.1109/TCST.2011.2124461
   Ma YD, 2012, IEEE CONTR SYST MAG, V32, P44, DOI 10.1109/MCS.2011.2172532
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pearl J, 1988, PROBABILISTIC REASON
   Pinto Lerrel, 2017, CORR
   Shehabi Arman, 2016, TECHNICAL REPORT
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Yi Ouyang, 2017, ARXIV170904047
   Zhou R., 2012, P MECH ENG C EXP, V7, P1789
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303078
DA 2019-06-15
ER

PT S
AU Le, L
   Patterson, A
   White, M
AF Le, Lei
   Patterson, Andrew
   White, Martha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Supervised autoencoders: Improving generalization performance with
   unsupervised regularizers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generalization performance is a central goal in machine learning, with explicit generalization strategies needed when training over-parametrized models, like large neural networks. There is growing interest in using multiple, potentially auxiliary tasks, as one strategy towards this goal. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that jointly predicts targets and inputs (reconstruction). We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error-particularly as an improvement on simplistic regularization such as norms. We then demonstrate empirically that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can improve generalization.
C1 [Le, Lei] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
   [Patterson, Andrew; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
RP Le, L (reprint author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
EM leile@iu.edu; ap3@ualberta.ca; whitem@ualberta.ca
CR Abu-Mostafa Yaser S, 1990, J COMPLEXITY
   Baldi  Pierre, 2014, ARXIV150901240V2
   Baldi  Pierre, 1989, NEURAL NETWORKS
   Bartlett Peter L, 2002, J MACHINE LEARNING R
   Baxter  Jonathan, 2000, J ARTIFICIAL INTELLI
   Baxter  Jonathan, 1995, ANN C LEARN THEOR
   Ben-David  S, 2003, LECT NOTES COMPUTER
   Bengio Y., 2007, ADV NEURAL INFORM PR
   Bousquet  Olivier, 2002, J MACHINE LEARNING R
   Carreira-Perpinan Miguel A, 2014, INT C ART INT STAT
   Caruana  R, 1997, ADV NEURAL INFORM PR
   Caruana Rich, 1997, MACHINE LEARNING
   Deterding David Henry, 1990, THESIS
   Gao  Bin-Bin, 2017, IEEE T IMAGE PROCESS
   Gogna  Anupriya, 2016, NEURAL INFORM PROCES
   Gottlieb  Lee-Ad, 2016, THEORETICAL COMPUTER
   Graham  Benjamin, 2014, ARXIV14114000V2CSLG
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kakade Sham M, 2008, ADV NEURAL INFORM PR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lecun Y., 1998, P IEEE
   Lee Giles  C, 2015, EMNLP
   Liu  Tongliang, 2017, IEEE T PATTERN ANAL
   Maurer  Andreas, 2015, ARXIV150901240V2
   Maurer  Andreas, 2013, ANN C LEARN THEOR
   Maurer  Andreas, 2006, J MACHINE LEARNING R
   Mohri M., 2012, FDN MACHINE LEARNING
   Mohri  Mehryar, 2015, NIPS WORKSH FEAT EXT
   Moosavi-Dezfooli S.-M., 2016, IEEE C COMP VIS PATT
   Morgan  N, 1990, ADV NEURAL INFORM PR
   Noh  Hyeonwoo, 2017, ADV NEURAL INFORM PR
   Ranzato M, 2006, ADV NEURAL INFORM PR
   Ranzato  Marc'Aurelio, 2008, INT C MACH LEARN
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Srivastava N., 2014, J MACHINE LEARNING R
   Vincent P., 2010, J MACHINE LEARNING R
   Wager  Stefan, 2013, ADV NEURAL INFORM PR
   Weston  Jason, 2008, INT C MACH LEARN
   Yaeger  Larry, 1997, ADV NEURAL INFORM PR
   Zhang  Tong, 2002, J MACHINE LEARNING R
   Zhang Y, 2016, P MACH LEARN RES NEW, V48, P612
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300011
DA 2019-06-15
ER

PT S
AU Le, T
   Yamada, M
AF Le, Tam
   Yamada, Makoto
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence
   Diagrams
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID HOMOLOGY; SHAPES
AB Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, persistent homology is a well-known tool to extract robust topological features, and outputs as persistence diagrams (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the Wasserstein metric. However, Wasserstein distance is not negative definite. Thus, it is limited to build positive definite kernels upon the Wasserstein distance without approximation. In this work, we rely upon the alternative Fisher information geometry to propose a positive definite kernel for PDs without approximation, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.
C1 [Le, Tam; Yamada, Makoto] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
   [Yamada, Makoto] Kyoto Univ, Kyoto, Japan.
RP Le, T (reprint author), RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
EM tam.le@riken.jp; makoto.yamada@riken.jp
FU JSPS KAKENHI [17K12745]; JST PRESTO program [JPMJPR165A]
FX We thank Ha Quang Minh, and anonymous reviewers for their comments. TL
   acknowledges the support of JSPS KAKENHI Grant number 17K12745. MY was
   supported by the JST PRESTO program JPMJPR165A.
CR Adams Henry, 2017, J MACHINE LEARNING R, V18, P218
   Amari S.-i., 2007, METHODS INFORM GEOME, V191
   Anirudh Rushil, 2016, P IEEE C COMP VIS PA, P68
   [Anonymous], 1972, NATURE, V239, P488
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Berg C., 1984, HARMONIC ANAL SEMIGR
   Bubenik P, 2015, J MACH LEARN RES, V16, P77
   Cang Z, 2015, MOL BASED MATH BIOL, V3
   Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x
   Carriere M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692
   Carriere Mathieu, 2017, P 34 INT C MACH LEAR, V70, P664
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chazal F., 2015, P 32 INT C MACH LEAR, V37, P2143
   Chen Chao, 2016, INT C MACH LEARN, P2732
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   de Silva V, 2007, ALGEBR GEOM TOPOL, V7, P339, DOI 10.2140/agt.2007.7.339
   Di Fabio B, 2015, LECT NOTES COMPUT SC, V9279, P294, DOI 10.1007/978-3-319-23231-7_27
   Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454
   Edelsbrunner H, 2008, CONTEMP MATH, V453, P257
   Elliott S. R., 1983, PHYS AMORPHOUS MAT
   Feragen A, 2015, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR.2015.7298922
   Francois N, 2013, PHYS REV LETT, V111, DOI 10.1103/PhysRevLett.111.148001
   GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004
   Harchaoui Z., 2009, ADV NEURAL INFORM PR, P609
   Hertzsch JM, 2007, SMALL, V3, P202, DOI 10.1002/smll.200600361
   Hofer Christoph, 2017, ADV NEURAL INFORM PR, P1633
   Istas J, 2012, ESAIM-PROBAB STAT, V16, P222, DOI 10.1051/ps/2011106
   Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422
   Kasson PM, 2007, BIOINFORMATICS, V23, P1753, DOI 10.1093/bioinformatics/btm250
   Kusano G, 2018, J MACH LEARN RES, V18
   Kusano Genki, 2016, P 33 INT C MACH LEAR, P2004
   Kwitt Roland, 2015, ADV NEURAL INFORM PR, P3070
   Lafferty J, 2005, J MACH LEARN RES, V6, P129
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Le T, 2015, P 32 INT C MACH LEAR, P2002
   Le T, 2015, MACH LEARN, V99, P169, DOI 10.1007/s10994-014-5446-z
   Lee H, 2011, I S BIOMED IMAGING, P841, DOI 10.1109/ISBI.2011.5872535
   Lee J M, 2006, RIEMANNIAN MANIFOLDS, V176, DOI 10.1007/b98852
   Levy P, 1965, PROCESSUS STOCHASTIQ
   Mendelson S, 2004, J MACH LEARN RES, V4, P759, DOI 10.1162/1532443041424337
   Minh HQ, 2006, LECT NOTES ARTIF INT, V4005, P154, DOI 10.1007/11776420_14
   Morariu VI, 2009, ADV NEURAL INFORM PR, V1, P1113
   Muller Claus, 2012, ANAL SPHERICAL SYMME, V129
   Nakamura T, 2015, NANOTECHNOLOGY, V26, DOI 10.1088/0957-4484/26/30/304001
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Petri G, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0873
   Peyre G., 2017, COMPUTATIONAL OPTIMA
   Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106
   Schoenberg IJ., 1942, DUKE MATH J, V9, P96, DOI [10.1215/S0012-7094-42-00908-6, DOI 10.1215/S0012-7094-42-00908-6]
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Singh G, 2008, J VISION, V8, DOI 10.1167/8.8.11
   Smola AJ, 2001, ADV NEUR IN, V13, P308
   Turner K, 2014, INF INFERENCE, V3, P310, DOI 10.1093/imaiai/iau011
   Villani C., 2003, TOPICS OPTIMAL TRANS, V58
   Xia KL, 2014, INT J NUMER METH BIO, V30, P814, DOI 10.1002/cnm.2655
   Ying Guo, 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P267
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004055
DA 2019-06-15
ER

PT S
AU Lee, D
   Liu, SF
   Gu, JW
   Liu, MY
   Yang, MH
   Kautz, J
AF Lee, Donghoon
   Liu, Sifei
   Gu, Jinwei
   Liu, Ming-Yu
   Yang, Ming-Hsuan
   Kautz, Jan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Context-Aware Synthesis and Placement of Object Instances
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Learning to insert an object instance into an image in a semantically coherent manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network. Code is available at https://github.com/NVlabs/Instance_Insertion.
C1 [Lee, Donghoon] Seoul Natl Univ, Seoul, South Korea.
   [Lee, Donghoon; Yang, Ming-Hsuan] Google Cloud AI, Sunnyvale, CA USA.
   [Lee, Donghoon; Liu, Sifei; Gu, Jinwei; Liu, Ming-Yu; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA.
   [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA.
RP Lee, D (reprint author), Seoul Natl Univ, Seoul, South Korea.; Lee, D (reprint author), Google Cloud AI, Sunnyvale, CA USA.; Lee, D (reprint author), NVIDIA, Santa Clara, CA 95051 USA.
EM donghoon.lee@rllab.snu.ac.kr; sifeil@nvidia.com; jinweig@nvidia.com;
   mingyul@nvidia.com; mhyang@ucmerced.edu; jkautz@nvidia.com
FU NSF CAREER Grant [1149783]
FX This work was conducted in NVIDIA. Ming-Hsuan Yang is supported in part
   by the NSF CAREER Grant #1149783 and gifts from NVIDIA.
CR Bar M, 1996, PERCEPTION, V25, P343, DOI 10.1068/p250343
   Bousmalis  K., 2017, IEEE C COMP VIS PATT
   Cordts M., 2016, IEEE C COMP VIS PATT
   Divvala S. K., 2009, IEEE C COMP VIS PATT
   Goodfellow I., 2014, NEURAL INFORM PROCES
   Hong  S., 2018, IEEE C COMP VIS PATT
   Huang J, 2017, IEEE ICC
   Huang  X., 2018, EUR C COMP VIS
   Isola P., 2017, IEEE C COMP VIS PATT
   Jaderberg  M., 2015, NEURAL INFORM PROCES
   Karras Tero, 2018, INT C LEARN REPR
   Kingma D. P., 2013, ARXIV13126114
   Larsen A. B. L., 2016, INT C MACH LEARN
   Li Y., 2017, IEEE C COMP VIS PATT
   Lin  C.-H., 2018, IEEE C COMP VIS PATT
   Liu  M.-Y., 2016, NEURAL INFORM PROCES
   LIU MY, 2017, NEURAL INFORM PROCES, pNIL_
   Ouyang  X., 2018, ARXIV180402047
   Pathak  D., 2016, IEEE C COMP VIS PATT
   Qi  X., 2018, IEEE C COMP VIS PATT
   Radford  A., 2015, ARXIV151106434
   Redmon J., 2018, ARXIV180402767
   Reed  S., 2016, NEURAL INFORM PROCES
   Rezende D. J, 2014, ARXIV14014082
   Shrivastava A., 2017, IEEE C COMP VIS PATT
   Sun YH, 2017, IEEE ICC
   Taigman  Y., 2017, INT C LEARN REPR
   Tan  F., 2018, IEEE WINT C APPL COM
   Tobin  J., 2017, IEEE RSJ INT C INT R
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Wang  T.-C., 2018, NEURAL INFORM PROCES
   Wang  T.-C., 2018, IEEE C COMP VIS PATT
   Wang X., 2017, IEEE C COMP VIS PATT
   Zhang  H., 2017, IEEE INT C COMP VIS
   Zhu  J.-Y., 2017, NEURAL INFORM PROCES
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005001
DA 2019-06-15
ER

PT S
AU Lee, HB
   Lee, J
   Kim, S
   Yang, E
   Hwang, SJ
AF Lee, Hae Beom
   Lee, Juho
   Kim, Saehoon
   Yang, Eunho
   Hwang, Sung Ju
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI DropMax: Adaptive Variational Softmax
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.
C1 [Lee, Hae Beom; Yang, Eunho; Hwang, Sung Ju] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Lee, Hae Beom; Lee, Juho; Kim, Saehoon; Yang, Eunho; Hwang, Sung Ju] AItrics, Seoul, South Korea.
   [Lee, Juho] Univ Oxford, Oxford, England.
RP Lee, HB (reprint author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.; Lee, HB (reprint author), AItrics, Seoul, South Korea.
EM haebeom.lee@kaist.ac.kr; juho.lee@stats.ox.ac.uk; shkim@aitrics.com;
   eunhoy@kaist.ac.kr; sjhwang82@kaist.ac.kr
FU Engineering Research Center Program through the National Research
   Foundation of Korea (NRF) - Korean Government MSIT
   [NRF-2018R1A5A1059921]; Samsung Research Funding Center of Samsung
   Electronics [SRFC-IT150203]; Machine Learning and Statistical Inference
   Framework for Explainable Artificial Intelligence [2017-0-01779]; Basic
   Science Research Program through the National Research Foundation of
   Korea (NRF) - Ministry of Education [2015R1D1A1A01061019]; European
   Research Council under the European Union's Seventh Framework Programme
   (FP7/2007-2013) ERC grant [617071]
FX This research was supported by the Engineering Research Center Program
   through the National Research Foundation of Korea (NRF) funded by the
   Korean Government MSIT (NRF-2018R1A5A1059921), Samsung Research Funding
   Center of Samsung Electronics (SRFC-IT150203), Machine Learning and
   Statistical Inference Framework for Explainable Artificial Intelligence
   (No. 2017-0-01779), and Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Education (2015R1D1A1A01061019). Juho Lee's research leading to these
   results has received funding from the European Research Council under
   the European Union's Seventh Framework Programme (FP7/2007-2013) ERC
   grant agreement no. 617071.
CR Abadi M., 2016, ARXIV160304467
   Ba J., 2013, NIPS
   Bandanau D., 2015, ICLR
   Bouthillier X., 2015, ARXIV E PRINTS
   Gal Y., 2015, ARXIV E PRINTS
   Gal Y., 2016, NIPS
   Gal Y., 2017, NIPS
   Gal  Yarin, 2016, ICML
   He K., 2016, CVPR
   Huang G, 2017, CVPR
   Jean S., 2015, ACL
   Kendall A., 2017, NIPS
   Kingma D. P., 2015, NIPS
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Lampert C. H., 2009, CVPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Luong M.-T., 2015, ACL
   Maddison C. J., 2017, ICLR
   Martins A. F. T., 2016, ICML
   Molchanov D., 2017, ICML
   Sohn K., 2015, NIPS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vincent P., 2016, ICLR
   Wah C., 2011, TECHNICAL REPORT
   Wang S., 2013, ICML
   Xu K, 2015, ICML
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300085
DA 2019-06-15
ER

PT S
AU Lee, J
   Raginsky, M
AF Lee, Jaeho
   Raginsky, Maxim
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Minimax statistical learning with Wasserstein distances
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove generalization bounds that involve the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.
C1 [Lee, Jaeho] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
RP Lee, J (reprint author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM jlee620@illinois.edu; maxim@illinois.edu
FU NSF [CIF-1527 388, CIF-1302438]; NSF CAREER award [1254041]
FX This work was supported in part by NSF grant nos. CIF-1527 388 and
   CIF-1302438, and in part by the NSF CAREER award 1254041.
CR Ambrosio L., 2008, LECT MATH
   Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Blanchet J., 2016, 161005627V2 ARXIV
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   Duchi J. C., 2016, 161003425 ARXIV
   Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1
   Farnia  F., 2016, P ADV NEUR INF PROC, P4240
   Fournier N, 2015, PROBAB THEORY REL, V162, P707, DOI 10.1007/s00440-014-0583-7
   Gao R., 2016, 160402199 ARXIV
   Goodfellow I. J., 2014, 14126572V3 ARXIV
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Koltchinskii V, 2011, LECT NOTES MATH, V2033, P1, DOI 10.1007/978-3-642-22147-7
   Mansour  Y., 2009, P 22 ANN C LEARN THE
   Shafieezadeh-Abadeh S, 2015, ADV NEURAL INFORM PR, P1576
   Sinha  A., 2018, INT C LEARN REPR
   Talagrand  M., 2014, UPPER LOWER BOUNDS S
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Villani C., 2003, TOPICS OPTIMAL TRANS
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302068
DA 2019-06-15
ER

PT S
AU Lee, J
   Kim, GH
   Poupart, P
   Kim, KE
AF Lee, Jongmin
   Kim, Geon-Hyeong
   Poupart, Pascal
   Kim, Kee-Eung
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Monte-Carlo Tree Search for Constrained POMDPs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MARKOV DECISION-PROCESSES
AB Monte-Carlo Tree Search (MCTS) has been successfully applied to very large POMDPs, a standard model for stochastic sequential decision-making problems. However, many real-world problems inherently have multiple goals, where multi-objective formulations are more natural. The constrained POMDP (CPOMDP) is such a model that maximizes the reward while constraining the cost, extending the standard POMDP model. To date, solution methods for CPOMDPs assume an explicit model of the environment, and thus are hardly applicable to large-scale real-world problems. In this paper, we present CC-POMCP (Cost-Constrained POMCP), an online MCTS algorithm for large CPOMDPs that leverages the optimization of LP-induced parameters and only requires a black-box simulator of the environment. In the experiments, we demonstrate that CC-POMCP converges to the optimal stochastic action selection in CPOMDP and pushes the state-of-the-art by being able to scale to very large problems.
C1 [Lee, Jongmin; Kim, Geon-Hyeong; Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
   [Poupart, Pascal] Univ Waterloo, Waterloo AI Inst, Waterloo, ON, Canada.
   [Poupart, Pascal] Vector Inst, Toronto, ON, Canada.
   [Kim, Kee-Eung] PROWLER Io, Cambridge, England.
RP Lee, J (reprint author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
EM jmlee@ai.kaist.ac.kr; ghkim@ai.kaist.ac.kr; ppoupart@uwaterloo.ca;
   kekim@cs.kaist.ac.kr
FU ICT R&D program of MSIT/IITP of Korea [2017-0-01778]; DAPA/ADD of Korea
   [UD170018CD]; NRF of Korea
FX This work was supported by the ICT R&D program of MSIT/IITP of Korea
   (No. 2017-0-01778) and DAPA/ADD of Korea (UD170018CD). J. Lee
   acknowledges the Global Ph.D. Fellowship Program by NRF of Korea
   (NRF-2018-Global Ph.D. Fellowship Program).
CR Altman E., 1999, STOCH MODEL SER
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Feinberg EA, 2002, J MATH ANAL APPL, V273, P93, DOI 10.1016/S0022-247X(02)00213-5
   Gelly S, 2011, ARTIF INTELL, V175, P1856, DOI 10.1016/j.artint.2011.03.007
   Guez A, 2013, J ARTIF INTELL RES, V48, P841
   Isom J. D., 2008, P 23 AAAI C ART INT, P291
   Kim D., 2011, P 22 INT JOINT C ART, P1968
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Kocsis Levente, 2006, 1 U TART
   Lee Jongmin, 2017, P 26 INT JOINT C ART, P2088
   Oh Eunsoo, 2011, P 27 C UNC ART INT U, P565
   PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441
   Piunovskiy AB, 2000, OPER RES LETT, V27, P119, DOI 10.1016/S0167-6377(00)00039-0
   Poupart P., 2015, P 29 AAAI C ART INT, P3342
   Sammie Katt, 2017, P 34 INT C MACH LEAR, P1819
   Silver D, 2010, ADV NEURAL INFORM PR, V23, P2164
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Smith Trey, 2004, P 20 C UNC ART INT, P520
   Sondik E. J., 1971, THESIS
   Undurti A, 2010, IEEE INT CONF ROBOT, P3966, DOI 10.1109/ROBOT.2010.5509743
   Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002047
DA 2019-06-15
ER

PT S
AU Lee, K
   Lee, K
   Lee, H
   Shin, J
AF Lee, Kimin
   Lee, Kibok
   Lee, Honglak
   Shin, Jinwoo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Simple Unified Framework for Detecting Out-of-Distribution Samples and
   Adversarial Attacks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.
C1 [Lee, Kimin; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Seoul, South Korea.
   [Lee, Kibok; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Lee, Honglak] Google Brain, Mountain View, CA USA.
   [Shin, Jinwoo] Altrics, Rosheim, France.
RP Lee, K (reprint author), Korea Adv Inst Sci & Technol, Seoul, South Korea.
FU Institute for Information & communications Technology Promotion (IITP) -
   Korea government (MSIT) [R0132-15-1005]; National Research Council of
   Science & Technology (NST) - Korea government (MSIP) [CRC-15-05-ETRI];
   DARPA Explainable AI (XAI) program [313498]; Sloan Research Fellowship;
   Kwanjeong Educational Foundation Scholarship
FX This work was supported in part by Institute for Information &
   communications Technology Promotion (IITP) grant funded by the Korea
   government (MSIT) (No.R0132-15-1005, Content visual browsing technology
   in the online and offline environments), National Research Council of
   Science & Technology (NST) grant by the Korea government (MSIP) (No.
   CRC-15-05-ETRI), DARPA Explainable AI (XAI) program #313498, Sloan
   Research Fellowship, and Kwanjeong Educational Foundation Scholarship.
CR Amodei  D., 2016, ICML
   Amodei Dario, 2016, ARXIV160606565
   Carlini Nicholas, 2017, ACM WORKSH AISEC
   Chrabaszcz Patryk, 2017, ARXIV170708819
   Deng J., 2009, CVPR
   Dezfooli Moosavi, 2016, CVPR
   Evtimov Ivan, 2018, CVPR
   Feinman R., 2017, ARXIV170300410
   Gal Yarin, 2017, ICML
   Girshick R., 2015, ICCV
   Goodfellow I., 2015, ICLR
   Guo C., 2017, ARXIV171100117
   He K., 2016, CVPR
   Hendrycks  Dan, 2017, ICLR
   Huang G, 2017, CVPR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kurakin A., 2016, ARXIV160702533
   Lasserre Julia A, 2006, CVPR
   Lee K., 2018, ICLR
   Lee K., 2017, ICML
   Lee Kibok, 2018, CVPR
   Liang Shiyu, 2018, ICLR
   Ma Xingjun, 2018, ICLR
   McCloskey Michael, 1989, PSYCHOL LEARNING MOT
   Mensink Thomas, 2013, IEEE T PATTERN ANAL
   Murphy KP, 2012, MACHINE LEARNING PRO
   Netzer Y., 2011, NIPS WORKSH
   Rebuffi Sylvestre-Alvise, 2017, CVPR
   Sharif Mahmood, 2016, ACM SIGSAC
   van der Maaten  Laurens, 2008, J MACHINE LEARNING R
   Vinyals O., 2016, NIPS
   Yu F., 2015, ARXIV150603365
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001069
DA 2019-06-15
ER

PT S
AU Lee, K
   Choi, S
   Oh, S
AF Lee, Kyungjae
   Choi, Sungjoon
   Oh, Songhwai
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Maximum Causal Tsallis Entropy Imitation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Shannon entropy is less effective.
C1 [Lee, Kyungjae; Oh, Songhwai] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
   [Lee, Kyungjae; Oh, Songhwai] Seoul Natl Univ, ASRI, Seoul, South Korea.
   [Choi, Sungjoon] Kakao Brain, Jeju, South Korea.
RP Lee, K (reprint author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.; Lee, K (reprint author), Seoul Natl Univ, ASRI, Seoul, South Korea.
EM kyungjae.lee@rllab.snu.ac.kr; sam.choi@kakaobrain.com;
   songhwai@snu.ac.kr
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science and ICT [NRF-2017R1A2B2006136];
   Brain Korea 21 Plus Project
FX This work was supported in part by Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Science and ICT (NRF-2017R1A2B2006136) and by the Brain
   Korea 21 Plus Project in 2018.
CR Abbeel P., 2004, P 21 INT C MACH LEAR
   Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156
   Brier GW., 1950, MONTHLY WEATHER REVI, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
   Choi J., 2013, P 23 INT JOINT C ART
   Choi J, 2015, IEEE T CYBERNETICS, V45, P793, DOI 10.1109/TCYB.2014.2336867
   Chow Y., 2018, P INT C MACH LEARN, P978
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   Haarnoja T., 2017, INT C MACH LEARN, P1352
   Hausman K., 2017, ADV NEURAL INFORM PR, P1235
   Heess N., 2012, JMLR WORKSH C P EWRL, P43
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Kyungjae Lee, 2018, IEEE Robotics and Automation Letters, V3, P1466, DOI 10.1109/LRA.2018.2800085
   Levine S., 2011, ADV NEURAL INFORM PR, P19
   Li Yunzhu, 2017, ADV NEURAL INFORM PR, V30, P3815
   Martins A., 2016, INT C MACH LEARN, P1614
   MILLAR PW, 1983, LECT NOTES MATH, V976, P75
   Puterman M. L., 2014, MARKOV DECISION PROC
   Ramachandran Deepak, 2007, P 20 INT JOINT C ART
   Ratliff N., 2006, P 23 INT C MACH LEAR
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Syed U., 2007, ADV NEURAL INFORM PR, P1449
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Vamplew P, 2017, NEUROCOMPUTING, V263, P74, DOI 10.1016/j.neucom.2016.09.141
   Wang Ziyu, 2017, ADV NEURAL INFORM PR, V30, P5326
   Wulfmeier M, 2015, ARXIV150704888
   Zheng J., 2014, P 28 AAAI C ART INT
   Ziebart B. D., 2008, AAAI, V8, P1433
   Ziebart B. D., 2010, THESIS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304042
DA 2019-06-15
ER

PT S
AU Lee, SW
   Heo, YJ
   Zhang, BT
AF Lee, Sang-Woo
   Heo, Yu-Jung
   Zhang, Byoung-Tak
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Answerer in Questioner's Mind: Information Theoretic Approach to
   Goal-Oriented Visual Dialog
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose "Answerer in Questioner's Mind" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: "MNIST Counting Dialog" and "GuessWhat?!". In our experiments, AQM outperforms comparative algorithms by a large margin.
C1 [Lee, Sang-Woo] Naver Corp, Clova AI Res, Seongnam, South Korea.
   [Lee, Sang-Woo; Heo, Yu-Jung; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea.
   [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea.
RP Lee, SW (reprint author), Naver Corp, Clova AI Res, Seongnam, South Korea.; Lee, SW (reprint author), Seoul Natl Univ, Seoul, South Korea.
FU Institute for Information & Communications Technology Promotion
   [R0126-16-1072-SW.StarLab, 2017-0-01772-VTT, 2018-0-00622-RMI]; Korea
   Evaluation Institute of Industrial Technology - Korea government (MSIP,
   DAPA) [10060086-RISF]
FX The authors would like to thank Jin-Hwa Kim, Tong Gao, Cheolho Han,
   Wooyoung Kang, Jaehyun Jun, Hwiyeol Jo, Byoung-Hee Kim, Kyoung Woon On,
   Sungjae Cho, Joonho Kim, Seungjae Jung, Hanock Kwak, Donghyun Kwak,
   Christina Baek, Minjoon Seo, Marco Baroni, and Jung-Woo Ha for helpful
   comments and editing. This work was supported by the Institute for
   Information & Communications Technology Promotion
   (R0126-16-1072-SW.StarLab, 2017-0-01772-VTT, 2018-0-00622-RMI) and Korea
   Evaluation Institute of Industrial Technology (10060086-RISF) grant
   funded by the Korea government (MSIP, DAPA).
CR Andreas J., 2016, P 2016 C EMP METH NA, P1173
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Batali John, 1998, APPROACHES EVOLUTION, V405, P426
   Bordes  A., 2017, ICLR
   Bruner J, 1981, ADV INFANCY RES
   Chandrasekaran Arjun, 2017, ARXIV170400717
   Chattopadhyay P., 2017, ARXIV170805122
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi Edward, 2018, ICLR
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Das Abhishek, 2017, ARXIV170306585
   Das A, 2017, IEEE INT C ELECTR TA
   Evtimova Katrina, 2018, ICLR
   Finn  C., 2017, P 34 INT C MACH LEAR, P1126
   Foerster J. N., 2017, ARXIV170904326
   Fried Daniel, 2018, P N AM CHAPT ASS COM, V1, P1951
   Han Cheolho, 2017, 2017 IJCAI WORKSH LI
   Hernandez-Leal P, 2017, L N INST COMP SCI SO, V179, P3, DOI 10.1007/978-3-319-49622-1_1
   Kendall A., 2017, ADV NEURAL INFORM PR
   Kim Jin-Hwa, 2017, ARXIV171205558
   Kottur Satwik, 2017, P 2017 C EMP METH NA, P2962
   Lazaridou Angeliki, 2018, ICLR
   Lemon O., 2006, P 11 C EUR CHAPT ASS, P119
   Li Xuijun, 2017, ARXIV170301008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   MacKay D. J, 2003, INFORM THEORY INFERE
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Monroe W., 2017, T ASS COMPUTATIONAL, V5, P325
   Mordatch Igor, 2017, ARXIV170304908
   Ng AY, 2002, ADV NEUR IN, V14, P841
   Polifroni Joseph, 2006, 5 INT C LANG RES EV
   PREMACK D, 1978, BEHAV BRAIN SCI, V1, P515, DOI 10.1017/S0140525X00076512
   Redmon J., 2016, ARXIV161208242
   Rothe Anselm, 2017, ADV NEURAL INFORM PR, P1046
   Seo Paul Hongsuck, 2017, ADV NEURAL INFORM PR
   Serban I. V., 2015, ARXIV150704808
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Strub Florian, 2017, P IEEE C COMP VIS PA
   Strub Florian, 2017, ARXIV170305423
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vinyals Oriol, 2015, ICML DEEP LEARN WORK
   Wen T.H., 2016, ARXIV160404562
   Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008
   Yu Licheng, 2017, COMPUTER VISION PATT, V2
   Zhang Byoung-Tak, 2013, AAAI SPRING S LIF MA, P62
   Zhao Tiancheng, 2016, ARXIV160602560
NR 46
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302058
DA 2019-06-15
ER

PT S
AU Lee, S
   Bareinboim, E
AF Lee, Sanghack
   Bareinboim, Elias
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Structural Causal Bandits: Where to Intervene?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MULTIARMED BANDIT
AB We study the problem of identifying the best action in a sequential decision-making setting when the reward distributions of the arms exhibit a non-trivial dependence structure, which is governed by the underlying causal model of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we show that whenever the underlying causal model is not taken into account during the decision-making process, the standard strategies of simultaneously intervening on all variables or on all the subsets of the variables may, in general, lead to suboptimal policies, regardless of the number of interventions performed by the agent in the environment. We formally acknowledge this phenomenon and investigate structural properties implied by the underlying causal model, which lead to a complete characterization of the relationships between the arms' distributions. We leverage this characterization to build a new algorithm that takes as input a causal structure and finds a minimal, sound, and complete set of qualified arms that an agent should play to maximize its expected reward. We empirically demonstrate that the new strategy learns an optimal policy and leads to orders of magnitude faster convergence rates when compared with its causal-insensitive counterparts.
C1 [Lee, Sanghack; Bareinboim, Elias] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
RP Lee, S (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM lee2995@purdue.edu; eb@purdue.edu
FU IBM Research; Adobe Research; NSF [IIS-1704352, IIS-1750807]
FX This research is supported in parts by grants from IBM Research, Adobe
   Research, NSF IIS-1704352, and IIS-1750807 (CAREER).
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113
   Bareinboim Elias, 2015, ADV NEURAL INFORM PR, P1342
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Combes R., 2014, P 31 INT C MACH LEAR, P521
   Dani V., 2008, COLT, P355
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Forney A., 2017, P 34 INT C MACH LEAR, V70, P1156
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18
   Kocaoglu Murat, 2017, ADV NEURAL INFORM PR, P7021
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lattimore  Finnian, ADV NEURAL INFORM PR, V29, P1181
   Lee  Sanghack, 2018, R36 PURD U DEP COMP
   Magureanu  S., 2014, P C LEARN THEOR COLT, P975
   Ortega PA, 2014, COMPLEX ADAPT SYST M, V2, DOI 10.1186/2194-3206-2-2
   Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.2307/2337329
   Pearl J., 2000, CAUSALITY MODELS REA
   Pearl J, 2009, CAUSALITY MODELS REA
   Raja  S., 2017, P 34 INT C MACH LEAR, P3057
   Spirtes P., 2001, CAUSATION PREDICTION
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567
   Zhang  J., 2017, P 26 INT JOINT C ART, P1340, DOI [10.24963/ijcai.2017/186, DOI 10.24963/IJCAI.2017/186]
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302057
DA 2019-06-15
ER

PT S
AU Lee, W
   Yu, H
   Yang, H
AF Lee, Wonyeol
   Yu, Hangyeol
   Yang, Hongseok
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reparameterization Gradient for Non-differentiable Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary's contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.
C1 [Lee, Wonyeol; Yu, Hangyeol; Yang, Hongseok] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
RP Lee, W (reprint author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon, South Korea.
EM wonyeol@kaist.ac.kr; yhk1344@kaist.ac.kr; hongseok.yang@kaist.ac.kr
FU Engineering Research Center Program through the National Research
   Foundation of Korea (NRF) - Korean Government MSIT
   [NRF-2018R1A5A1059921]; National Research Foundation of Korea (NRF) -
   Ministry of Science, ICT [2017M3C4A7068177]
FX We thank Hyunjik Kim, George Tucker, Frank Wood and anonymous reviewers
   for their helpful comments, and Shin Yoo and Seongmin Lee for allowing
   and helping us to use their cluster machines. This research was
   supported by the Engineering Research Center Program through the
   National Research Foundation of Korea (NRF) funded by the Korean
   Government MSIT (NRF-2018R1A5A1059921), and also by Next-Generation
   Information Computing Development Program through the National Research
   Foundation of Korea (NRF) funded by the Ministry of Science, ICT
   (2017M3C4A7068177).
CR Davidson-Pilon C., 2015, BAYESIAN METHODS HAC
   Diaconis P., 2013, ADV MODERN STAT THEO, P102
   FLANDERS H, 1973, AM MATH MON, V80, P615, DOI 10.2307/2319163
   Goodman N. D., 2008, P 24 C UNC ART INT U
   Gordon A. D., 2014, INT C SOFTW ENG ICS
   Grathwohl W., 2018, P 6 INT C LEARN REPR
   Gu S., 2016, P 4 INT C LEARN REPR
   Gu S., 2017, P 5 INT C LEARN REPR
   Gumbel E. J, 1954, STAT THEORY EXTREME, V33
   Jang E., 2017, P 5 INT C LEARN REPR
   Jimenez Rezende D., 2014, P 31 INT C MACH LEAR
   Kingma D., 2015, P 3 INT C LEARN REPR
   Kingma D. P., 2016, P 30 INT C NEUR INF
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Knowles D. A., 2015, STOCHASTIC GRADIENT
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Le T. A., 2018, P 6 INT C LEARN REPR
   Li Y., 2016, P 30 INT C NEUR INF
   Maclaurin D., 2016, THESIS
   Maddison C. J., 2014, P 28 INT C NEUR INF
   Maddison C. J., 2017, P 5 INT C LEARN REPR
   Maddison C. J., 2017, P 31 INT C NEUR INF
   Mansinghka V. K., 2014, VENTURE HIGHER ORDER
   Miller Andrew C, 2017, ARXIV170507880
   Mnih A., 2016, P 33 INT C INT C MAC
   Naesseth C. A., 2017, P 20 INT C ART INT
   Naesseth C. A., 2018, P 21 INT C ART INT S
   Paisley J. W., 2012, P 29 INT C MACH LEAR
   Ranganath R., 2014, P 17 INT C ART INT S
   Rezende D. J., 2015, P 32 INT C INT C MAC
   Ruiz F. J. R., 2016, P 30 INT C NEUR INF
   Shumway R.H., 2005, SPRINGER TEXTS STAT
   Soudjani S. E. Z., 2017, P 14 INT C QUANT EV
   Tucker G., 2017, P 31 INT C NEUR INF
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wingate D., 2013, CORR
   Wood F., 2014, P 17 INT C ART INT S
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000009
DA 2019-06-15
ER

PT S
AU Lei, YW
   Tang, K
AF Lei, Yunwen
   Tang, Ke
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Stochastic Composite Mirror Descent: Optimal Bounds with High
   Probabilities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID APPROXIMATION; ALGORITHMS
AB We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable, which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings.
C1 [Lei, Yunwen; Tang, Ke] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China.
RP Tang, K (reprint author), Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China.
EM leiyw@sustc.edu.cn; tangk3@sustc.edu.cn
FU National Key Research and Development Program of China [2017YFB1003102];
   National Natural Science Foundation of China [61806091, 61672478];
   Science and Technology Innovation Committee Foundation of Shenzhen
   [ZDSYS201703031748284]; Shenzhen Peacock Plan [KQTD2016112514355531]
FX This work is supported in part by the National Key Research and
   Development Program of China (Grant No. 2017YFB1003102), the National
   Natural Science Foundation of China (Grant Nos. 61806091 and 61672478),
   the Science and Technology Innovation Committee Foundation of Shenzhen
   (Grant No. ZDSYS201703031748284) and Shenzhen Peacock Plan (Grant No.
   KQTD2016112514355531).
CR Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2
   Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Ben London, 2017, ADV NEURAL INFORM PR, P2931
   Bottou L., 1998, ONLINE LEARNING NEUR, P9
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391
   Duchi  J., 2009, ADV NEURAL INFORM PR, P495
   Duchi J. C., 2010, P 23 ANN C LEARN THE, P14
   Hardt M, 2016, P 33 INT C MACH LEAR, P1225
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2014, J MACH LEARN RES, V15, P2489
   Lacoste-Julien  S., 2012, ARXIV12122002
   Lei  Y., 2018, APPL COMPUTATIONAL H, DOI [10.1016/j.acha.2018.05.005, DOI 10.1016/J.ACHA.2018.05.005]
   Lei  Y., 2018, J MACHINE LEARNING R, V18, P1
   Lei YW, 2018, SIAM J IMAGING SCI, V11, P547, DOI 10.1137/17M1136225
   Lin  J., 2016, INT C MACH LEARN, P2340
   Lin  J., 2016, ADV NEURAL INFORM PR, P4556
   Moulines  E., 2011, ADV NEURAL INFORM PR, P451
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Nguyen L. M., 2018, ARXIV180203801
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Rakhlin A., 2012, P 29 INT C MACH LEAR, P449
   Rosasco Lorenzo, 2015, ADV NEURAL INFORM PR, P1630
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089
   Steinwart I, 2007, ANN STAT, V35, P575, DOI 10.1214/009053606000001226
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Ying YM, 2006, IEEE T INFORM THEORY, V52, P4775, DOI 10.1109/TIT.2006.883632
   Ying YM, 2017, APPL COMPUT HARMON A, V42, P224, DOI 10.1016/j.acha.2015.08.007
   Zhang T., 2004, P 21 INT C MACH LEAR, P919, DOI DOI 10.1145/1015330.1015332
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
   Zhou  Z., 2017, ADV NEURAL INFORM PR, P7043
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 44
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301050
DA 2019-06-15
ER

PT S
AU Lenssen, JE
   Fey, M
   Libuschewski, P
AF Lenssen, Jan Eric
   Fey, Matthias
   Libuschewski, Pascal
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Group Equivariant Capsule Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present group equivariant capsule networks, a framework to introduce guaranteed equivariance and invariance properties to the capsule network idea. Our work can be divided into two contributions. First, we present a generic routing by agreement algorithm defined on elements of a group and prove that equivariance of output pose vectors, as well as invariance of output activations, hold under certain conditions. Second, we connect the resulting equivariant capsule networks with work from the field of group convolutional networks. Through this connection, we provide intuitions of how both methods relate and are able to combine the strengths of both approaches in one deep neural network architecture. The resulting framework allows sparse evaluation of the group convolution operator, provides control over specific equivariance and invariance properties, and can use routing by agreement instead of pooling operations. In addition, it is able to provide interpretable and equivariant representation vectors as output capsules, which disentangle evidence of object existence from its pose.
C1 [Lenssen, Jan Eric; Fey, Matthias; Libuschewski, Pascal] TU Dortmund Univ, Comp Graph Grp, D-44227 Dortmund, Germany.
RP Lenssen, JE (reprint author), TU Dortmund Univ, Comp Graph Grp, D-44227 Dortmund, Germany.
EM janeric.lenssen@udo.edu; matthias.fey@udo.edu;
   pascal.libuschewski@udo.edu
FU Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research
   Center [SFB 876]
FX Part of the work on this paper has been supported by Deutsche
   Forschungsgemeinschaft (DFG) within the Collaborative Research Center
   SFB 876 Providing Information by Resource-Constrained Analysis, projects
   B2 and A6.
CR Cohen T. S., 2018, ARXIV E PRINTS
   Cohen T. S., 2017, INT C LEARN REPR ICL
   Cohen Taco, 2016, INT C MACH LEARN, P2990
   Cohen Taco S., 2018, INT C LEARN REPR ICL
   Dieleman S., 2016, P 33 INT C MACH LEAR, V48, P1889
   Fey M., 2018, IEEE C COMP VIS PATT
   Gilmer J, 2017, P 34 INT C MACH LEAR, V34, P1263
   Henriques J. F., 2017, P INT C MACH LEARN, P1461
   Hinton Geoffrey, 2018, INT C LEARN REPR ICL
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540
   Nielsen F., 2012, MATRIX INFORM GEOMET
   Sabour S., 2017, ADV NEURAL INFORM PR, P3859
   Weiler M., 2018, IEEE C COMP VIS PATT
   Worrall Daniel E., 2017, IEEE C COMP VIS PATT
   Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003040
DA 2019-06-15
ER

PT S
AU Leung, D
   Drton, M
AF Leung, Dennis
   Drton, Mathias
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Algebraic tests of general Gaussian latent tree models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BOOTSTRAP
AB We consider general Gaussian latent tree models in which the observed variables are not restricted to be leaves of the tree. Extending related recent work, we give a full semi-algebraic description of the set of covariance matrices of any such model. In other words, we find polynomial constraints that characterize when a matrix is the covariance matrix of a distribution in a given latent tree model. However, leveraging these constraints to test a given such model is often complicated by the number of constraints being large and by singularities of individual polynomials, which may invalidate standard approximations to relevant probability distributions. Illustrating with the star tree, we propose a new testing methodology that circumvents singularity issues by trading off some statistical estimation efficiency and handles cases with many constraints through recent advances on Gaussian approximation for maxima of sums of high-dimensional random vectors. Our test avoids the need to maximize the possibly multimodal likelihood function of such models and is applicable to models with larger number of variables. These points are illustrated in numerical experiments.
C1 [Leung, Dennis] Univ Southern Calif, Dept Data Sci & Operat, Los Angeles, CA 90089 USA.
   [Drton, Mathias] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
   [Drton, Mathias] Univ Copenhagen, Dept Math Sci, Copenhagen, Denmark.
RP Leung, D (reprint author), Univ Southern Calif, Dept Data Sci & Operat, Los Angeles, CA 90089 USA.
EM dmhleung@uw.edu; md5@uw.edu
CR Anderson T. W., 2003, WILEY SERIES PROBABI
   BEKKER PA, 1987, PSYCHOMETRIKA, V52, P125, DOI 10.1007/BF02293960
   BOLLEN KA, 1993, SOCIOL METHODOL, V23, P147, DOI 10.2307/271009
   Buhlmann P, 2002, STAT SCI, V17, P52, DOI 10.1214/ss/1023798998
   Chernozhukov V, 2013, ARXIV13127614
   Chernozhukov V, 2013, ANN STAT, V41, P2786, DOI 10.1214/13-AOS1161
   Choi MJ, 2011, J MACH LEARN RES, V12, P1771
   Drton M, 2008, ANN STAT, V36, P2261, DOI 10.1214/07-AOS522
   Drton M, 2007, PROBAB THEORY REL, V138, P463, DOI 10.1007/s00440-006-0033-2
   Drton M, 2016, BERNOULLI, V22, P38, DOI 10.3150/14-BEJ620
   Drton M, 2009, ANN STAT, V37, P979, DOI 10.1214/07-AOS571
   Drton Mathias, 2009, OBERWOLFACH SEMINARS, V39
   HALL P, 1995, BIOMETRIKA, V82, P561
   Lahiri S. N., 2003, SPRINGER SERIES STAT
   Mourad R, 2013, J ARTIF INTELL RES, V47, P157, DOI 10.1613/jair.3879
   Semple C., 2003, OXFORD LECT SERIES M, V24
   Shiers N, 2016, BIOMETRIKA, V103, P531, DOI 10.1093/biomet/asw032
   Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107
   Wishart J, 1928, B J PSYCHOL-GEN SECT, V19, P180, DOI 10.1111/j.2044-8295.1928.tb00508.x
   Zhang DN, 2017, ANN STAT, V45, P1895, DOI 10.1214/16-AOS1512
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000077
DA 2019-06-15
ER

PT S
AU Levin, R
   Sevekari, A
   Woodruff, DP
AF Levin, Roie
   Sevekari, Anish
   Woodruff, David P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Robust Subspace Approximation in a Stream
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study robust subspace estimation in the streaming and distributed settings. Given a set of n data points {a(i)}(i=1)(n) in R-d and an integer k, we wish to find a linear subspace S of dimension k for which Sigma(i) M(dist(S; a(i))) is minimized, where dist(S; x) := min(y is an element of S) parallel to x - y parallel to(2), and M( is some loss function. When M is the identity function, S gives a subspace that is more robust to outliers than that provided by the truncated SVD. Though the problem is NP-hard, it is approximable within a (1 + epsilon) factor in polynomial time when k and epsilon are constant. We give the first sublinear approximation algorithm for this problem in the turnstile streaming and arbitrary partition distributed models, achieving the same time guarantees as in the offline case. Our algorithm is the fi rst based entirely on oblivious dimensionality reduction, and significantly simplifies prior methods for this problem, which held in neither the streaming nor distributed models.
C1 [Levin, Roie; Woodruff, David P.] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
   [Sevekari, Anish] Carnegie Mellon Univ, Dept Math Sci, Pittsburgh, PA 15213 USA.
RP Levin, R (reprint author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
EM roiel@cs.cmu.edu; asevekar@andrew.cmu.edu; dwoodruf@cs.cmu.edu
FU National Science Foundation [CCF-1815840]
FX We would like to thank Ainesh Bakshi for many helpful discussions. D.
   Woodruff thanks partial support from the National Science Foundation
   under Grant No. CCF-1815840. Part of this work was also done while D.
   Woodruff was visiting the Simons Institute for the Theory of Computing.
CR Backurs Arturs, 2016, SODA
   Basu Saugata, 1994, J ACM
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27
   Clarkson KL, 2009, ACM S THEORY COMPUT, P205
   Clarkson Kenneth L., 2015, SODA
   Deshpande A, 2007, ACM S THEORY COMPUT, P641, DOI 10.1145/1250790.1250884
   Deshpande Amit, 2011, SODA
   Deshpande Amit, 2007, STOC
   Ding Chris H. Q., 2006, ICML
   Feldman D, 2010, PROC APPL MATH, V135, P630
   Feldman Dan, 2010, SODA
   Feldman Dan, 2011, STOC
   Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718
   Guruswami V, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2737729
   Indyk P, 2001, ANN IEEE SYMP FOUND, P10, DOI 10.1109/SFCS.2001.959878
   Kannan Ravi, 2014, P 27 C LEARN THEOR C, P1040
   Monemizadeh Morteza, 2010, SODA
   Muthukrishnan S., 2005, FDN TRENDS THEORETIC, V1
   Shyamalkumar ND, 2012, DISCRETE COMPUT GEOM, V47, P44, DOI 10.1007/s00454-011-9384-2
   Song Zhao, 2016, CORR
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005028
DA 2019-06-15
ER

PT S
AU Levy, KY
   Yurtsever, A
   Cevher, V
AF Levy, Kfir Y.
   Yurtsever, Alp
   Cevher, Volkan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Adaptive Methods, Universality and Acceleration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a novel method for convex unconstrained optimization that, without any modifications, ensures: (i) accelerated convergence rate for smooth objectives, (ii) standard convergence rate in the general (non-smooth) setting, and (iii) standard convergence rate in the stochastic optimization setting. To the best of our knowledge, this is the first method that simultaneously applies to all of the above settings.
   At the heart of our method is an adaptive learning rate rule that employs importance weights, in the spirit of adaptive online learning algorithms [12, 20], combined with an update that linearly couples two sequences, in the spirit of [2]. An empirical examination of our method demonstrates its applicability to the above mentioned scenarios and corroborates our theoretical findings.
C1 [Levy, Kfir Y.] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Levy, KY (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM yehuda.levy@inf.ethz.ch; alp.yurtsever@epfl.ch; volkan.cevher@epfl.ch
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation programme [725594 - time-data]; ETH Zurich
   Postdoctoral Fellowship; Marie Curie Actions for People COFUND program
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant agreement no 725594 - time-data). K.Y.L. is supported
   by the ETH Zurich Postdoctoral Fellowship and Marie Curie Actions for
   People COFUND program.
CR Allen-Zhu Z., 2017, STOC
   Allen-Zhu Z., 2017, P 8 INN THEOR COMP S
   Arjevani Y., 2016, J MACHINE LEARNING R, V17, P4303
   Attouch H., 2015, ARXIV150701367
   Aujol J., 2017, OPTIMAL RATE CONVERG
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bubeck S., 2015, ARXIV150608187
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Cohen M. B., 2018, ARXIV180512591
   Cutkosky A., 2018, ARXIV180206293
   Diakonikolas J., 2017, ARXIV170604680
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Flammarion N., 2015, P 28 C LEARN THEOR C, P658
   Foucart S., 2013, MATH INTRO COMPRESSI, V1
   Frostig R., 2015, P 32 INT C MACH LEAR, P2540
   Hu C., 2009, ADV NEURAL INF PROCE, P781
   Lan GH, 2015, MATH PROGRAM, V149, P1, DOI 10.1007/s10107-013-0737-x
   Lan GH, 2012, MATH PROGRAM, V133, P365, DOI 10.1007/s10107-010-0434-y
   Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597
   Levy K., 2017, ADV NEURAL INFORM PR, P1612
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Nemirovskii A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Nesterov Yurii E., 2003, INTRO LECT CONVEX OP
   Neumaier A, 2016, MATH PROGRAM, V158, P1, DOI 10.1007/s10107-015-0911-4
   Orabona F, 2015, LECT NOTES ARTIF INT, V9355, P287, DOI 10.1007/978-3-319-24486-0_19
   Scieur D., 2016, ADV NEURAL INFORM PR, P712
   Shalev-Shwartz S., 2014, P 31 INT C MACH LEAR, P64
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Yurtsever A., 2015, ADV NEURAL INFORM PR, P3150
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001007
DA 2019-06-15
ER

PT S
AU Li, CX
   Welling, M
   Zhu, J
   Zhang, B
AF Li, Chongxuan
   Welling, Max
   Zhu, Jun
   Zhang, Bo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Graphical Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.
C1 [Li, Chongxuan; Zhu, Jun; Zhang, Bo] Tsinghua Univ, State Key Lab Intell Tech & Sys, Dept Comp Sci & Technol, Inst Artificial Intelligence,BNRist Ctr,THBI Lab, Beijing, Peoples R China.
   [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands.
   [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada.
RP Zhu, J (reprint author), Tsinghua Univ, State Key Lab Intell Tech & Sys, Dept Comp Sci & Technol, Inst Artificial Intelligence,BNRist Ctr,THBI Lab, Beijing, Peoples R China.
EM licx14@mails.tsinghua.edu.cn; M.Welling@uva.nl;
   dcszj@mail.tsinghua.edu.cn; dcszb@mail.tsinghua.edu.cn
FU National Key Research and Development Program of China [2017YFA0700900];
   National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant
   of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support
   Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA
   NVAIL Program; Project from Siemens; China Scholarship Council
FX The work was supported by the National Key Research and Development
   Program of China (No. 2017YFA0700900), the National NSF of China (Nos.
   61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp.
   Stan (No. 2016ZXFB00001), the Youth Top-notch Talent Support Program,
   Tsinghua Tiangong Institute for Intelligent Computing, the NVIDIA NVAIL
   Program and a Project from Siemens. This work was done when C. Li
   visited the university of Amsterdam. During this period, he was
   supported by China Scholarship Council.
CR Abadi M, 2016, TENSORFLOW SYSTEM LA
   Arjovsky M, 2017, ARXIV170107875
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Csiszar I., 2004, Foundations and Trends in Communications and Information Theory, V1, P1
   Denton E. L., 2017, ADV NEURAL INFORM PR, P4417
   Dilokthanakul N., 2016, ARXIV161102648
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Huszar F., 2017, ARXIV170208235
   Jang Eric, 2016, ARXIV161101144
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kalchbrenner N., 2016, ARXIV161000527
   Karaletsos  Theofanis, 2016, ARXIV161205048
   Kingma D. P., 2013, ARXIV13126114
   Koller D., 2009, PROBABILISTIC GRAPHI
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li  Yingzhen, 2015, ADV NEURAL INFORM PR, P2323
   Lin  Wu, 2018, ARXIV180305589
   Liu  Z., 2015, P INT C COMP VIS ICC
   Makhzani A., 2015, ARXIV151105644
   Mathieu M, 2015, ARXIV151105440
   Mescheder L., 2017, ARXIV170104722
   Minka T. P, 2005, TECHNICAL REPORT
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Mohamed S., 2016, ARXIV161003483
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Radford  A., 2015, ARXIV151106434
   Saatci Yunus, 2017, ADV NIPS, V30, P3622
   Saito  Masaki, 2016, ARXIV161106624
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Srivastava N., 2015, INT C MACH LEARN, P843
   Stuhlmuller A., 2013, ADV NEURAL INFORM PR, P3048
   Tolstikhin  Ilya, 2017, ARXIV1711015582
   Tran  Dustin, 2017, ARXIV170208896
   Tulyakov S., 2017, ARXIV170704993
   Villegas R., 2017, ARXIV170608033
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613
   Warde-Farley  David, 2016, IMPROVING GENERATIVE
   Xue T., 2016, ADV NEURAL INFORM PR, P91
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000056
DA 2019-06-15
ER

PT S
AU Li, CY
   Liang, XD
   Hu, ZT
   Xing, EP
AF Li, Christy Y.
   Liang, Xiaodan
   Hu, Zhiting
   Xing, Eric P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report
   Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection precision of medical abnormality terminologies, and improved human evaluation performance.
C1 [Li, Christy Y.] Duke Univ, Durham, NC 27706 USA.
   [Liang, Xiaodan; Hu, Zhiting] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Li, Christy Y.; Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA.
RP Liang, XD (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM yl558@duke.edu; xiaodan1@cs.cmu.edu; zhitingh@cs.cmu.edu;
   epxing@cs.cmu.edu
CR Abigail See C. D. M., 2017, ACL
   [Anonymous], 2018, JIEBA CHINESE STUTTE
   Bahdanau Dzmitry, 2017, ICLR
   Banerjee S., 2005, ACL WORKSH
   Bosmans J. M., 2011, RADIOLOGY
   Bowman S. R., 2016, CONLL
   Dayan P., 1993, NEURIPS
   Demner-Fushman D., 2015, J AM MED INFORM ASS
   Donahue J., 2015, CVPR
   Goergen S. K., 2013, J MED IMAGING RAD ON
   He K., 2016, CVPR
   Hong Y., 2013, J DIGITAL IMAGING
   Hu Z., 2017, ICML
   Hu Z., 2018, ARXIV180900794
   Huang G, 2017, CVPR
   Jing B., 2018, ACL
   Karpathy A., 2015, CVPR
   Li L., 2017, ICLR
   Liang X., 2017, ICCV
   Lin C.-Y., 2013, ACL
   Liu S., 2017, P IEEE INT C COMP VI, V3
   Liu Y., 2017, AAAI
   Lu Jiasen, 2017, CVPR
   Lu Jiasen, 2018, CVPR
   Papineni K., 2002, ACL
   Paulus R., 2018, ICLR
   Ranzato Marc Aurelio, 2016, ICLR
   Rennie Steven J., 2017, CVPR
   Simonyan Karen, 2015, ICLR
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tan B., 2018, CONNECTING DOTS MLE
   Vaswani A., 2017, NEURIPS
   Vedantam Ramakrishna, 2015, CVPR
   Vinyals O, 2015, CVPR
   Wang X., 2018, ACL
   Wang Xiaolong, 2017, CVPR
   Williams R. J., 1992, REINFORCEMENT LEARNI, P5
   Wiseman S., 2017, ICCV
   Wu Y., 2016, ARXIV160908144
   Wu Z. Y. Y. Y. Y., 2016, NEURIPS
   Xu K, 2015, ICML
   Yarats D., 2017, EMNLP
   You Q., 2016, CVPR
   Ziqiang Cao S. L., 2018, ACL
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301051
DA 2019-06-15
ER

PT S
AU Li, H
   Xu, Z
   Taylor, G
   Studer, C
   Goldstein, T
AF Li, Hao
   Xu, Zheng
   Taylor, Gavin
   Studer, Christoph
   Goldstein, Tom
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Visualizing the Loss Landscape of Neural Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.
C1 [Li, Hao; Xu, Zheng; Goldstein, Tom] Univ Maryland, College Pk, MD 20742 USA.
   [Taylor, Gavin] US Naval Acad, Annapolis, MD 21402 USA.
   [Studer, Christoph] Cornell Univ, Ithaca, NY 14853 USA.
RP Li, H (reprint author), Univ Maryland, College Pk, MD 20742 USA.
EM haoli@cs.umd.edu; xuzh@cs.umd.edu; taylor@usna.edu; studer@cornell.edu;
   tomg@cs.umd.edu
FU Office of Naval Research [N00014-17-1-2078]; DARPA Lifelong Learning
   Machines [FA8650-18-2-7833]; DARPA YFA [D18AP00055]; Sloan Foundation;
   ONR [N0001418WX01582]; DOD HPC Modernization Program; Xilinx, Inc.; NSF
   [ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, ECCS-1824379]
FX Li, Xu, and Goldstein were supported by the Office of Naval Research
   (N00014-17-1-2078), DARPA Lifelong Learning Machines (FA8650-18-2-7833),
   DARPA YFA (D18AP00055), and the Sloan Foundation. Taylor was supported
   by ONR (N0001418WX01582), and the DOD HPC Modernization Program. Studer
   was supported in part by Xilinx, Inc. and by the NSF under grants
   ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-1824379.
CR Balduzzi D, 2017, ICML
   Ballard AJ, 2017, PHYS CHEM CHEM PHYS, V19, P12585, DOI 10.1039/c7cp01108c
   Blum Avrim, 1989, NIPS
   Chaudhari Pratik, 2017, ICLR
   Choromanska Anna, 2015, AISTATS
   Dauphin Yann N., 2014, NIPS
   DeSoham, 2017, AISTATS
   Dinh Laurent, 2017, ICML
   Dziugaite Gintare Karolina, 2017, UAI
   Freeman C Daniel, 2017, ICLR
   Gallagher M, 2003, IEEE T SYST MAN CY B, V33, P28, DOI 10.1109/TSMCB.2003.808183
   Glorot X., 2010, AISTATS
   Goldstein T., 2016, ARXIV161007531
   Goodfellow I., 2015, ICLR
   Goyal Priya, 2017, ARXIV170602677
   Haeffele Benjamin D, 2017, CVPR
   Hardt Moritz, 2017, ICLR
   He K., 2016, CVPR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Hoffer Elad, 2017, NIPS
   Huang G, 2017, CVPR
   Im Daniel Jiwoong, 2016, ARXIV161204010
   Ioffe S., 2015, ICML
   Kawaguchi K., 2017, ARXIV171005468
   Keskar Nitish Shirish, 2017, ICLR
   Krogh Anders, 1992, NIPS
   Li Yuanzhi, 2017, ARXIV170509886
   Liao Qianli, 2017, ARXIV170309833
   Lipton Zachary C, 2016, ICLR WORKSH
   Lorch Eliana, 2016, ICML WORKSH VIS DEEP
   Neyshabur Behnam, 2017, NIPS
   Nguyen Quynh, 2017, ICML
   Safran Itay, 2016, ICML
   Simonyan Karen, 2015, ICLR
   Smith L. N, 2017, ARXIV170204283
   Soltanolkotabi Mahdi, 2017, ARXIV170704926
   Soudry  D., 2017, ARXIV170205777
   Swirszcz Grzegorz, 2016, ARXIV161106310
   Tian Yuandong, 2017, ICML
   Xie Bo, 2017, AISTATS
   Yun Chulhee, 2017, ICLR
   Zagoruyko S., 2016, BMVC
   Zhang Chiyuan, 2017, ICLR
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000085
DA 2019-06-15
ER

PT S
AU Li, J
   Liu, Y
   Yin, R
   Zhang, H
   Ding, LZ
   Wang, WP
AF Li, Jian
   Liu, Yong
   Yin, Rong
   Zhang, Hua
   Ding, Lizhong
   Wang, Weiping
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multi-Class Learning: From Theory to Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.
C1 [Li, Jian; Liu, Yong; Yin, Rong; Zhang, Hua; Wang, Weiping] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
   [Li, Jian; Yin, Rong] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China.
   [Wang, Weiping] Natl Engn Res Ctr Informat Secur, Shanghai, Peoples R China.
   [Wang, Weiping] Natl Engn Lab Informat Secur Technol, Shanghai, Peoples R China.
   [Ding, Lizhong] IIAI, Abu Dhabi, U Arab Emirates.
RP Liu, Y (reprint author), Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China.
EM lijian9026@iie.ac.cn; liuyong@iie.ac.cn; yinrong@iie.ac.cn;
   lizhong.ding@inceptioniai.org; wangweiping@iie.ac.cn
FU National Natural Science Foundation of China [61703396, 61673293,
   61602467]; National Key Research and Development Program of China
   [2016YFB1000604]; Science and Technology Project of Beijing
   [Z181100002718004]; Excellent Talent Introduction of Institute of
   Information Engineering of CAS [Y7Z0111107]
FX This work is supported in part by the National Natural Science
   Foundation of China (No.61703396, No.61673293, No.61602467), the
   National Key Research and Development Program of China
   (No.2016YFB1000604), the Science and Technology Project of Beijing
   (No.Z181100002718004) and the Excellent Talent Introduction of Institute
   of Information Engineering of CAS (Y7Z0111107).
CR Alexander Zien, 2007, P 24 INT C MACH LEAR, P1191
   Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   BOTTOU L, 1994, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1994.576879
   Cortes C., 2013, P 30 INT C MACH LEAR, P46
   Cortes C., 2013, ADV NEURAL INFORM PR, P2760
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   D. McAllester, 2013, ARXIV 13
   Daniely A., 2014, COLT, P287
   Franc  V., 2005, THESIS
   Guermeur Y, 2002, PATTERN ANAL APPL, V5, P168, DOI 10.1007/s100440200015
   Hardt M, 2016, P 33 INT C MACH LEAR, P1225
   Hill SI, 2007, J ARTIF INTELL RES, V30, P525, DOI 10.1613/jair.2251
   Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Koltchinskii V, 2001, ADV NEUR IN, V13, P245
   Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019
   Kuznetsov V., 2014, ADV NEURAL INF PROCE, P2501
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lei Y, 2015, ADV NEURAL INFORM PR, P2035
   Liu  Y., 2013, P 22 ACM INT C INF K, P2189
   Liu Y., 2011, P 20 ACM C INF KNOWL, P2205
   Liu  Y., 2015, P 29 AAAI C ART INT, P2814
   Liu  Y., 2017, P 21 AAAI C ART INT, P2280
   Liu Y, 2014, ADV INTEL SYS RES, V100, P324
   Maximov Yu, 2016, Pattern Recognition and Image Analysis, V26, P673, DOI 10.1134/S105466181604009X
   Moh  M., 2012, FDN MACHINE LEARNING
   Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1007/BF00114804
   Orabona F., 2011, P 28 INT C MACH LEAR, P249
   Orabona F, 2010, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2010.5540137
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341
   Vapnik V. N., 2000, NATURE STAT LEARNING
   Xu C, 2016, IEEE T IMAGE PROCESS, V25, P1495, DOI 10.1109/TIP.2016.2524207
   Yong Liu, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P290, DOI 10.1007/978-3-662-44851-9_19
   Yousefi  N., 2016, ARXIV160205916
   Zhang T, 2004, J MACH LEARN RES, V5, P1225
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301056
DA 2019-06-15
ER

PT S
AU Li, J
   Mantiuk, RK
   Wang, JL
   Ling, S
   Le Callet, P
AF Li, Jing
   Mantiuk, Rafal K.
   Wang, Junle
   Ling, Suiyi
   Le Callet, Patrick
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference
   Aggregation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper we present a hybrid active sampling strategy for pairwise preference aggregation, which aims at recovering the underlying rating of the test candidates from sparse and noisy pairwise labelling. Our method employs Bayesian optimization framework and Bradley-Terry model to construct the utility function, then to obtain the Expected Information Gain (EIG) of each pair. For computational efficiency, Gaussian-Hermite quadrature is used for estimation of EIG. In this work, a hybrid active sampling strategy is proposed, either using Global Maximum (GM) EIG sampling or Minimum Spanning Tree (MST) sampling in each trial, which is determined by the test budget. The proposed method has been validated on both simulated and real-world datasets, where it shows higher preference aggregation ability than the state-of-the-art methods.
C1 [Li, Jing; Ling, Suiyi; Le Callet, Patrick] Univ Nantes, IPI Lab, LS2N, Nantes, France.
   [Mantiuk, Rafal K.] Univ Cambridge, Comp Lab, Cambridge, England.
   [Wang, Junle] Tencent Games, Turing Lab, Shenzhen, Peoples R China.
RP Li, J (reprint author), Univ Nantes, IPI Lab, LS2N, Nantes, France.
EM jingli.univ@gmail.com; rkm38@cam.ac.uk; wangjunle@gmail.com;
   suiyi.ling@univ-nantes.fr; patrick.lecallet@univ-nantes.fr
CR Ailon N., 2009, ADV NEURAL INFORM PR, P25
   [Anonymous], 2012, BT50013 ITUR
   [Anonymous], 2008, P910 ITUT
   Bradley R. A., 1955, BIOMETRIKA, V42, P450
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Chen XF, 2013, KEY ENG MATER, V538, P193, DOI 10.4028/www.scientific.net/KEM.538.193
   Cortes C., 2007, P 24 INT C MACH LEAR, P169
   Crammer K, 2002, ADV NEUR IN, V14, P641
   Dangauthier P., 2008, ADV NEURAL INFORM PR, P337
   DAVIS  P.J., 2007, METHODS NUMERICAL IN
   DYKSTRA O, 1960, BIOMETRICS, V16, P176, DOI 10.2307/2527550
   Emerson P, 2013, SOC CHOICE WELFARE, V40, P353, DOI 10.1007/s00355-011-0603-9
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933
   Herbrich R., 2007, NEURAL INF PROCESS S, P569
   Jamieson K. G., 2011, P NEUR INF PROC SYST, P2240
   Le Callet P., 2005, SUBJECTIVE QUALITY A
   Le Callet Patrick, 2012, EUROPEAN NETWORK QUA
   Li J., 2018, IS T ELECT IMAGING H
   Li J., 2013, IS T SPIE ELECT IMAG
   Li JY, 2013, INT CONF MEAS, P1, DOI 10.1109/ICMTMA.2013.11
   Li J, 2012, IEEE IMAGE PROC, P629, DOI 10.1109/ICIP.2012.6466938
   LINDLEY DV, 1956, ANN MATH STAT, V27, P986, DOI 10.1214/aoms/1177728069
   Lu T., 2011, P 28 INT C MACH LEAR, P145
   Luce R. D, 2005, INDIVIDUAL CHOICE BE
   MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244
   Negahban Sahand, 2012, ADV NEURAL INFORM PR, P2474
   Pfeiffer T., 2012, AAAI
   Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567
   PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x
   Qin T., 2010, ADV NEURAL INFORM PR
   Shah N. B., 2016, J MACH LEARN RES, V17, P2049
   Sheikh H.R., LIVE IMAGE QUALITY A
   Silverstein DA, 1998, IMAGE PROCESSING IMAGE QUALITY IMAGE CAPTURE SYSTEMS CONFERENCE, P242
   Soufiani H.A., 2012, NIPS, P126
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Wauthier F. L., 2013, INT C MACH LEARN, P109
   Xu Q., 2011, P 19 ACM INT C MULT, P393, DOI DOI 10.1145/2072298.2072350
   Xu Q., 2018, AAAI
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303047
DA 2019-06-15
ER

PT S
AU Li, JN
   Wong, YK
   Zhao, Q
   Kankanhalli, MS
AF Li, Junnan
   Wong, Yongkang
   Zhao, Qi
   Kankanhalli, Mohan S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Unsupervised Learning of View-invariant Action Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ACTION RECOGNITION; ENSEMBLE
AB The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.
C1 [Li, Junnan] Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore, Singapore.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
   [Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN USA.
RP Li, JN (reprint author), Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore, Singapore.
EM lijunnan@u.nus.edu; yongkang.wong@nus.edu.sg; qzhao@cs.umn.edu;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU National Research Foundation, Prime Minister's Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative
FX This research is supported by the National Research Foundation, Prime
   Minister's Office, Singapore under its Strategic Capability Research
   Centres Funding Initiative.
CR Bengio Y., 2007, P ADV NEUR INF PROC, P153
   Bengio Y., 2014, P 31 INT C MACH LEAR, P226
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cheng  G., 2015, ARXIV150105964
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Ganin Y., 2015, INT C MACH LEARN, P1180
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gidaris S., 2018, ICLR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Haque A, 2016, LECT NOTES COMPUT SC, V9905, P160, DOI 10.1007/978-3-319-46448-0_10
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Isik L, 2018, J NEUROPHYSIOL, V119, P631, DOI 10.1152/jn.00642.2017
   Jaimez M, 2015, IEEE INT CONF ROBOT, P98, DOI 10.1109/ICRA.2015.7138986
   Kingma D. P., 2015, ICLR
   Kong Y, 2017, IEEE T IMAGE PROCESS, V26, P3028, DOI 10.1109/TIP.2017.2696786
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li BL, 2012, PROC CVPR IEEE, P1362, DOI 10.1109/CVPR.2012.6247822
   Li J, 2018, AGING MENT HEALTH, V22, P1548, DOI [10.1080/13607863.2017.1377686, 10.1155/2017/5234214]
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Li RN, 2012, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2012.6248011
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/CVPRW.2010.5543273
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Lu CW, 2014, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2014.104
   Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Patraucean  V., 2016, ICLR WORKSH
   Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768
   Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Ranzato M, 2014, ARXIV14126604
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Springenberg J. T., 2014, 14126806 ARXIV
   Srivastava N., 2015, INT C MACH LEARN, P843
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang PC, 2017, PROC CVPR IEEE, P416, DOI 10.1109/CVPR.2017.52
   Wang XL, 2017, IEEE I CONF COMP VIS, P1338, DOI 10.1109/ICCV.2017.149
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233
   Zhang Z, 2013, PROC CVPR IEEE, P2690, DOI 10.1109/CVPR.2013.347
NR 64
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301026
DA 2019-06-15
ER

PT S
AU Li, P
   He, N
   Milenkovic, O
AF Li, Pan
   He, Niao
   Milenkovic, Olgica
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Quadratic Decomposable Submodular Function Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem is closely related to decomposable submodular function minimization and arises in many learning on graphs and hypergraphs settings, such as graph-based semi-supervised learning and PageRank. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.(1)
C1 [Li, Pan; He, Niao; Milenkovic, Olgica] UIUC, Champaign, IL 61820 USA.
RP Li, P (reprint author), UIUC, Champaign, IL 61820 USA.
EM panli2@illinois.edu; niaohe@illinois.edu; milenkov@illinois.edu
FU NIH [1u01 CA198943A]; NSF [CCF 15-27636]
FX The authors gratefully acknowledge many useful suggestions by the
   reviewers. This work was supported in part by the NIH grant 1u01
   CA198943A and the NSF grant CCF 15-27636.
CR Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Chakrabarty  D., 2014, ADV NEURAL INFORM PR, P802
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9
   Chan  T., 2017, ARXIV171101560
   Chan THH, 2018, J ACM, V65, DOI 10.1145/3178123
   Ene  A., 2017, ADV NEURAL INFORM PR, P2874
   Ene  A., 2015, P 32 INT C MACH LEAR, P787
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Fujishige S, 2011, PAC J OPTIM, V7, P3
   Gammerman A., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P148
   Gleich DF, 2015, SIAM J MATRIX ANAL A, V36, P1507, DOI 10.1137/140985160
   Hein M., 2013, ADV NEURAL INFORM PR, P2427
   Jegelka  S., 2013, ADV NEURAL INFORM PR, P1313
   Joachims T., 2003, P 20 INT C MACH LEAR, P290
   Johnson R, 2007, J MACH LEARN RES, V8, P1489
   Kumar S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1150, DOI 10.1109/ICCV.2003.1238478
   Li P., 2017, ADV NEURAL INFORM PR, P2305
   Li P, 2018, P INT C MACH LEARN
   Li  P., 2018, ADV NEURAL INFORM PR
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Nishihara R., 2014, ADV NEURAL INFORM PR, P640
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Page L., 1999, TECH REP
   Shor N. Z., 2012, MINIMIZATION METHODS, V3
   Stobbe  P., 2010, P NIPS, P2208
   Yoshida  Y., 2017, ARXIV170808781
   Zhang  C., 2017, INT C MACH LEARN ICM, P4026
   Zhou D., 2007, P ADV NEUR INF PROC, P1601
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, INT C MACH LEARN, V20, P912
   Zhu X., 2003, ICML 2003 WORKSH CON, V3
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301008
DA 2019-06-15
ER

PT S
AU Li, P
   Milenkovic, O
AF Li, Pan
   Milenkovic, Olgica
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Revisiting Decomposable Submodular Function Minimization with Incidence
   Relations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates.
C1 [Li, Pan; Milenkovic, Olgica] UIUC, Champaign, IL 61820 USA.
RP Li, P (reprint author), UIUC, Champaign, IL 61820 USA.
EM panli2@illinois.edu; milenkov@illinois.edu
FU NSF [CCF 15-27636]; NSF Purdue [4101-38050]; NFT STC center Science of
   Information
FX The authors gratefully acknowledge many useful suggestions by the
   reviewers. This work was supported in part by the NSF grant CCF
   15-27636, the NSF Purdue 4101-38050 and the NFT STC center Science of
   Information.
CR Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Chakrabarty  D., 2014, ADV NEURAL INFORM PR, P802
   Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9
   Chekuri C, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1085
   Djolonga  J., 2015, ICML, P1804
   Ene  A., 2017, ADV NEURAL INFORM PR, P2874
   Ene  A., 2015, P 32 INT C MACH LEAR, P787
   Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Fujishige S., 1992, JAPAN J IND APPL MAT, V9, P369
   Hajnal A., 1970, COMBINATORIAL THEORY, VII, P601
   Hein M., 2013, ADV NEURAL INFORM PR, P2427
   Jegelka  S., 2013, ADV NEURAL INFORM PR, P1313
   KARGER DR, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P21
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Kierstead HA, 2010, COMBINATORICA, V30, P217, DOI 10.1007/s00493-010-2483-5
   Kipf T. N., 2016, ARXIV160902907
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025
   Krause A., 2007, AAAI, P1650
   Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li P., 2017, ADV NEURAL INFORM PR, P2305
   Li  P., 2018, P INT C MACH LEARN, P3014
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   MEYER W, 1973, AM MATH MON, V80, P920, DOI 10.2307/2319405
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nishihara R., 2014, ADV NEURAL INFORM PR, P640
   Stobbe  P., 2010, P NIPS, P2208
   Wei K., 2015, ICML, P1954
   WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381
   Yadati  N., 2018, ARXIV180902589
   ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302026
DA 2019-06-15
ER

PT S
AU Li, R
   Kahou, SE
   Schulz, H
   Michalski, V
   Charlin, L
   Pal, C
AF Li, Raymond
   Kahou, Samira Ebrahimi
   Schulz, Hannes
   Michalski, Vincent
   Charlin, Laurent
   Pal, Chris
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Towards Deep Conversational Recommendations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale dataset consisting of real-world dialogues centered around recommendations. To address this issue and to facilitate our exploration here, we have collected REDIAL, a dataset consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We make this data available to the community for further research. Second, we use this dataset to explore multiple facets of conversational recommendations. In particular we explore new neural architectures, mechanisms, and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.
C1 [Li, Raymond; Kahou, Samira Ebrahimi; Pal, Chris] Ecole Polytech Montreal, Montreal, PQ, Canada.
   [Li, Raymond; Pal, Chris] Element AI, Montreal, PQ, Canada.
   [Kahou, Samira Ebrahimi; Schulz, Hannes] Microsoft Res Montreal, Montreal, PQ, Canada.
   [Michalski, Vincent] Univ Montreal, Montreal, PQ, Canada.
   [Michalski, Vincent; Charlin, Laurent; Pal, Chris] Mila, Montreal, PQ, Canada.
   [Charlin, Laurent] HEC Montreal, Montreal, PQ, Canada.
RP Li, R (reprint author), Ecole Polytech Montreal, Montreal, PQ, Canada.; Li, R (reprint author), Element AI, Montreal, PQ, Canada.
CR Cho K, 2014, ARXIV14061078
   Christakopoulou K., 2016, P 22 ACM SIGKDD INT, P815, DOI DOI 10.1145/2939672.2939746
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Das Abhishek, 2017, C COMP VIS PATT REC, V2
   Das Abhishek, 2017, ARXIV170306585CSCV
   Dodge Jesse, 2015, ARXIV151106931CSCL
   Goker Mehmet H., 2011, ARXIV11070029CSIR
   Greco C, 2017, LECT NOTES ARTIF INT, V10640, P372, DOI 10.1007/978-3-319-70169-1_28
   Gulcehre C., 2016, P 54 ANN M ASS COMP, V1, P140
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   He Ji, 2015, ARXIV151104636
   Jacomy M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098679
   Johansson Pontus, 2004, THESIS
   Kingma D. P., 2014, ARXIV14126980
   Krause Ben, 2017, ARXIV170909816CSCL
   Li Xuijun, 2017, ARXIV170301008
   Liu Chia- Wei, 2016, ARXIV160308023
   Marlin B. M., 2007, P 23 C UNC ART INT U, P267
   Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726
   Serban I. V, 2016, P 30 AAAI C ART INT, P3776
   Serban Iulian Vlad, 2015, ARXIV151205742CSCL
   Sordoni A., 2015, P 24 ACM INT C INF K, V19, P553, DOI DOI 10.1145/2806416.2806493
   Subramanian  S., 2018, ICLR
   Suglia Alessandro, 2017, P DYN SEARCH COMPL T
   Sun Yueming, 2018, ARXIV180603277
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Warnestal Pontus, 2007, SIGDIAL WORKSH DISC
   Widyantoro Dwi H., 2014, 2014 2nd International Conference on Information and Communication Technology (ICoICT), P160, DOI 10.1109/ICoICT.2014.6914058
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004030
DA 2019-06-15
ER

PT S
AU Li, S
   Xiao, S
   Zhu, SX
   Du, N
   Xie, Y
   Song, L
AF Li, Shuang
   Xiao, Shuai
   Zhu, Shixiang
   Du, Nan
   Xie, Yao
   Song, Le
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Temporal Point Processes via Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.
C1 [Li, Shuang; Zhu, Shixiang; Xie, Yao; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Xiao, Shuai; Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China.
   [Du, Nan] Google Brain, Mountain View, CA USA.
RP Li, S (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM sli370@gatech.edu; yao.xie@isye.gatech.edu; lsong@cc.gatech.edu
FU NSF [CCF-1442635, CMMI-1538746, DMS-1830210, IIS-1218749, IIS-1639792
   EAGER, CNS-1704701, CCF-1836822, IIS-1841351 EAGER]; NSF CAREER Award
   [CCF-1650913]; Atlanta Police Foundation fund; S.F. Express fund; NIH
   BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR
   [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; Siemens
FX This project was supported in part by NSF grants CCF-1442635,
   CMMI-1538746, DMS-1830210, NSF CAREER Award CCF-1650913, Atlanta Police
   Foundation fund, and an S.F. Express fund awarded to Yao Xie. This
   project was supported in part by NSF IIS-1218749, NIH BIGDATA
   1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF
   CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS,
   NSF CCF-1836822, NSF IIS-1841351 EAGER, and Siemens awarded to Le Song.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Adams R. A., 2003, SOBOLEV SPACES, V140
   Berlinet A., 2011, REPRODUCING KERNEL H
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Daley D. J., 2007, INTRO THEORY POINT P
   Du N., 2016, P 22 ACM SIGKDD INT, P1555, DOI DOI 10.1145/2939672.2939875
   Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147
   Dziugaite G. K., 2015, ARXIV150503906
   Farajtabar M, 2015, P ADV NEUR INF PROC, P1954
   Genton MG, 2002, J MACH LEARN RES, V2, P299, DOI 10.1162/15324430260185646
   Grandell Jan, 2006, DOUBLY STOCHASTIC PO, V529
   Gretton A., 2007, ADV NEURAL INFORM PR, V19, P513
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Kim Beomjoon, 2013, ROBOTICS SCI SYSTEMS
   Kingma D. P., 2014, ARXIV14126980
   Kingman J, 1993, POISSON PROCESSES
   Mei Hongyuan, 2017, ADV NEURAL INFORM PR, P6757
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Omi T, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.012303
   Pfanzagl Johann, 2011, PARAMETRIC STAT THEO
   Rodriguez M Gomez, 2010, P 16 ACM SIGKDD INT, P1019, DOI [DOI 10.1145/1835804.1835933, 10.1145/1835804.1835933]
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Sriperumbudur  B., 2010, J MACHINE LEARNING R, P773
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Xiao Shuai, 2017, ADV NEURAL INFORM PR, P3250
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005037
DA 2019-06-15
ER

PT S
AU Li, WY
   Mao, JW
   Zhang, Y
   Cui, SG
AF Li, Wenye
   Mao, Jingwei
   Zhang, Yin
   Cui, Shuguang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast Similarity Search via Optimal Sparse Lifting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID JOHNSON-LINDENSTRAUSS; NEAREST-NEIGHBOR
AB Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.
C1 [Li, Wenye; Mao, Jingwei; Zhang, Yin; Cui, Shuguang] Chinese Univ Hong Kong, Shenzhen, Peoples R China.
   [Li, Wenye; Cui, Shuguang] Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
RP Li, WY (reprint author), Chinese Univ Hong Kong, Shenzhen, Peoples R China.; Li, WY (reprint author), Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
EM wylishuguangcui@cuhk.edu.cn; 216019005@link.cuhk.edu.cn;
   yinzhangshuguangcui@cuhk.edu.cn; shuguangcui@cuhk.edu.cn
FU Shenzhen Fundamental Research Fund [JCYJ20170306141038939,
   KQJSCX20170728162302784, ZDSYS201707251409055]; Shenzhen Development and
   Reform Commission Fund; Guangdong Introducing Innovative and
   Entrepreneurial Teams Fund, China [2017ZT07X152]
FX This work was supported by Shenzhen Fundamental Research Fund
   (JCYJ20170306141038939, KQJSCX20170728162302784, ZDSYS201707251409055),
   Shenzhen Development and Reform Commission Fund, and Guangdong
   Introducing Innovative and Entrepreneurial Teams Fund (2017ZT07X152),
   China.
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Allen-Zhu Z, 2014, P NATL ACAD SCI USA, V111, P16872, DOI 10.1073/pnas.1419100111
   Andoni  A., 2015, P 47 ANN ACM S THEOR, P793
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463
   Caron SJC, 2013, NATURE, V497, P113, DOI 10.1038/nature12063
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868
   Duda R.O., 2012, PATTERN CLASSIFICATI
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   Jaiyam S., EFFICIENT NEAREST NE
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Jortner RA, 2007, J NEUROSCI, V27, P1659, DOI 10.1523/JNEUROSCI.4171-06.2007
   Kleinberg J. M., 1997, STOC 97, P599, DOI DOI 10.1145/258533.258653
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin AC, 2014, NAT NEUROSCI, V17, P559, DOI 10.1038/nn.3660
   Lin Y, 2013, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2013.64
   Manning C. D., 2008, INTRO INFORM RETRIEV
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papadopoulou M, 2011, SCIENCE, V332, P721, DOI 10.1126/science.1201835
   Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/neco_a_01018, 10.1162/NECO_a_01018]
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Schrijver A., 1998, THEORY LINEAR INTEGE
   Turner GC, 2008, J NEUROPHYSIOL, V99, P734, DOI 10.1152/jn.01283.2007
   Zheng ZH, 2018, CELL, V174, P730, DOI 10.1016/j.cell.2018.06.019
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300017
DA 2019-06-15
ER

PT S
AU Li, YY
   Bu, R
   Sun, MC
   Wu, W
   Di, XH
   Chen, BQ
AF Li, Yangyan
   Bu, Rui
   Sun, Mingchao
   Wu, Wei
   Di, Xinhan
   Chen, Baoquan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI PointCNN: Convolution On X -Transformed Points
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NET
AB We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.
C1 [Li, Yangyan; Sun, Mingchao; Wu, Wei] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Di, Xinhan] Huawei Inc, Shenzhen, Peoples R China.
   [Chen, Baoquan] Peking Univ, Beijing, Peoples R China.
RP Li, YY (reprint author), Shandong Univ, Jinan, Shandong, Peoples R China.
FU National Key Research and Development Program of China [2017YFB1002603];
   National Basic Research grant (973) [2015CB352501]; National Science
   Foundation of China [61772317]; "Qilu" Young Talent Program of Shandong
   University
FX Yangyan would like to thank Leonidas Guibas from Stanford University and
   Mike Haley from Autodesk Research for insightful discussions, and Noa
   Fish from Tel Aviv University and Thomas Schattschneider from Technical
   University of Hamburg for proof reading. The work is supported in part
   by National Key Research and Development Program of China grant No.
   2017YFB1002603, the National Basic Research grant (973) No.
   2015CB352501, National Science Foundation of China General Program grant
   No. 61772317, and "Qilu" Young Talent Program of Shandong University.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   ARMENI I, 2016, PROC CVPR IEEE, P1534, DOI DOI 10.1109/CVPR.2016.170
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Ben-Shabat  Yizhak, 2018, ARXIV171108241
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chollet F, 2016, ARXIV161002357
   Clevert DA, 2016, ICLR
   Cruz Rodrigo Santa, 2017, CVPR
   Dai A., 2017, CVPR
   Dieleman S., 2016, P 33 INT C MACH LEAR, V48, P1889
   EITZ M, 2012, TOG, V31
   Graham B., 2017, ARXIV171110275
   Graham  Benjamin, 2017, ARXIV170601307
   Groh  Fabian, 2018, ARXIV180307289
   Ha D., 2017, ARXIV170403477
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hua  Binh-Son, 2018, CVPR
   Huang  Qiangui, 2018, CVPR
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Kingma Diederik P, 2014, ICLR
   Klokov  Roman, 2017, ICCV
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Landrieu  Loic, 2017, ABS171109869 CORR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li  Jiaxin, 2018, CVPR
   Li Y., 2016, ADV NEURAL INFORM PR, P307
   Lin M., 2014, ICLR
   Maron H., 2017, ACM T GRAPHIC, V36
   Monti  Federico, 2017, CVPR
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5105
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Ravanbakhsh  S., 2016, ARXIV161104500
   Riegler G., 2017, CVPR
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P1
   Sabour S., 2017, ADV NEURAL INFORM PR, P3859
   Shao  Tianjia, 2018, ARXIV180311385
   Shen Y., 2018, CVPR
   Su  Hang, 2018, CVPR
   Tatarchenko  Maxim, 2018, CVPR
   Tchapmi Lyne P., 2017, 3DV
   Wang  Chu, 2018, ARXIV180305827
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang  Shenlong, 2018, CVPR
   Wang  Weiyue, 2018, CVPR
   Wang Y., 2018, ARXIV180107829
   Wu  Shihao, 2015, TOG, V34
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801
   Xu  Yifan, 2018, ARXIV180311527
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Yi  Li, 2016, TOG, V35
   Yi  Li, 2017, ARXIV171006104
   Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3
   Zaheer M, 2017, ADV NEURAL INFORM PR, P3394
NR 58
TC 0
Z9 0
U1 5
U2 5
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300076
DA 2019-06-15
ER

PT S
AU Li, YJ
   Bresler, Y
AF Li, Yanjun
   Bresler, Yoram
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SUBSPACE METHODS; RECONSTRUCTION; IDENTIFICATION; REPRESENTATION;
   ALGORITHM
AB Multichannel blind deconvolution is the problem of recovering an unknown signal f and multiple unknown channels x(i) from convolutional measurements y(i) = x(i)circle star f (i = 1, 2, .. . , N). We consider the case where the x(i)'s are sparse, and convolution with f is invertible. Our nonconvex optimization formulation solves for a filter h on the unit sphere that produces sparse output y(i)circle star h. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of f up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of f and x(i) using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods.
C1 [Li, Yanjun; Bresler, Yoram] Univ Illinois, CSL, Champaign, IL 61820 USA.
   [Li, Yanjun; Bresler, Yoram] Univ Illinois, Dept ECE, Champaign, IL 61820 USA.
RP Li, YJ (reprint author), Univ Illinois, CSL, Champaign, IL 61820 USA.; Li, YJ (reprint author), Univ Illinois, Dept ECE, Champaign, IL 61820 USA.
EM yli145@illinois.edu; ybresler@illinois.edu
FU National Science Foundation (NSF) [IIS 14-47879]
FX This work was supported in part by the National Science Foundation (NSF)
   under Grant IIS 14-47879. The authors would like to thank Ju Sun for
   helpful discussions about this paper. The manuscript benefited from
   constructive comments by the anonymous reviewers.
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464
   Ahmed A, 2014, IEEE T INFORM THEORY, V60, P1711, DOI 10.1109/TIT.2013.2294644
   Allen-Zhu  Z., 2017, ARXIV170200763
   Allen-Zhu  Zeyuan, 2017, ARXIV170808694
   Balzano L, 2007, PROCEEDINGS OF THE SIXTH INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P79, DOI 10.1109/IPSN.2007.4379667
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Berger CR, 2010, IEEE T SIGNAL PROCES, V58, P1708, DOI 10.1109/TSP.2009.2038424
   Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344
   Bilen C, 2014, IEEE T SIGNAL PROCES, V62, P4847, DOI 10.1109/TSP.2014.2342651
   Boumal  N., 2016, ARXIV160508101
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Chi YJ, 2016, IEEE J-STSP, V10, P782, DOI 10.1109/JSTSP.2016.2543462
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Eldar Y. C., 2017, ARXIV170703378
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Gitelman DR, 2003, NEUROIMAGE, V19, P200, DOI 10.1016/S1053-8119(03)00058-2
   Goldfarb D, 2017, COMPUT OPTIM APPL, V68, P479, DOI 10.1007/s10589-017-9925-6
   GURELLI MI, 1995, IEEE T SIGNAL PROCES, V43, P134, DOI 10.1109/78.365293
   Huang  W., 2017, ARXIV171003309
   Jin C., 2017, INT C MACH LEARN, V70, P1724
   Kaaresen KF, 1998, GEOPHYSICS, V63, P2093, DOI 10.1190/1.1444503
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   LEE J. D, 2017, ARXIV171007406
   Lee K., 2017, ARXIV170804343
   Lee K, 2017, IEEE T INFORM THEORY, V63, P802, DOI 10.1109/TIT.2016.2636204
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Li X., 2016, ARXIV160604933
   Li YJ, 2017, IEEE T INFORM THEORY, V63, P4619, DOI 10.1109/TIT.2017.2689779
   Li YJ, 2017, IEEE T INFORM THEORY, V63, P822, DOI 10.1109/TIT.2016.2637933
   Li YJ, 2016, IEEE T SIGNAL PROCES, V64, P5549, DOI 10.1109/TSP.2016.2598311
   Li YJ, 2016, IEEE T INFORM THEORY, V62, P4266, DOI 10.1109/TIT.2016.2569578
   Ling  S., 2016, SELF CALIBRATION VIA
   Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002
   Mei Song, 2016, ARXIV160706534
   MOULINES E, 1995, IEEE T SIGNAL PROCES, V43, P516, DOI 10.1109/78.348133
   Mukamel EA, 2012, BIOPHYS J, V102, P2391, DOI 10.1016/j.bpj.2012.03.070
   Panageas  I., 2016, ARXIV160500405
   Paulraj A., 1985, ICASSP 85. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No. 85CH2118-8), P640
   Rust MJ, 2006, NAT METHODS, V3, P793, DOI 10.1038/nmeth929
   Sabra KG, 2004, J ACOUST SOC AM, V116, P262, DOI 10.1121/1.1751151
   Samanta A, 2018, IEEE CONF COMPUT
   Sarder P, 2006, IEEE SIGNAL PROC MAG, V23, P32, DOI 10.1109/MSP.2006.1628876
   She HJ, 2015, MAGN RESON IMAGING, V33, P1106, DOI 10.1016/j.mri.2015.06.008
   Strohmer T, 2002, LINEAR ALGEBRA APPL, V343, P321, DOI 10.1016/S0024-3795(01)00243-9
   Sun J, 2018, FOUND COMPUT MATH, V18, P1131, DOI 10.1007/s10208-017-9365-9
   Sun J, 2017, IEEE T INFORM THEORY, V63, P885, DOI 10.1109/TIT.2016.2632149
   Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162
   Tian N, 2017, J ACOUST SOC AM, V141, P3337, DOI 10.1121/1.4983311
   Tong L, 1998, P IEEE, V86, P1951, DOI 10.1109/5.720247
   TONG L, 1991, CONFERENCE RECORD OF THE TWENTY-FIFTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P856, DOI 10.1109/ACSSC.1991.186568
   Wang LM, 2016, IEEE SIGNAL PROC LET, V23, P1384, DOI 10.1109/LSP.2016.2599104
   Wylie M. P., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P281, DOI 10.1109/ICASSP.1993.319110
   Xu GH, 1995, IEEE T SIGNAL PROCES, V43, P2982, DOI 10.1109/78.476442
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Zhang HC, 2013, PROC CVPR IEEE, P1051, DOI 10.1109/CVPR.2013.140
   Zhang  Y., 2017, P 10 NIPS WORKSH OPT
   Zhang  Y., 2017, P IEEE C COMP VIS PA, P4894
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301015
DA 2019-06-15
ER

PT S
AU Li, Y
   Cheng, MH
   Fujii, K
   Hsieh, FS
   Hsieh, CJ
AF Li, Yao
   Cheng, Minhao
   Fujii, Kevin
   Hsieh, Fushing
   Hsieh, Cho Jui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning from Group Comparisons: Exploiting Higher Order Interactions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of learning from group comparisons, with applications in predicting outcomes of sports and online games. Most of the previous works in this area focus on learning individual effects-they assume each player has an underlying score, and the "ability" of the team is modeled by the sum of team members' scores. Therefore, current approaches cannot model deeper interaction between team members: some players perform much better if they play together, while some players perform poorly together. In this paper, we propose a new model that takes the player-interaction effects into consideration. However, under certain circumstances, the total number of individuals can be very large, and number of player interactions grows quadratically, which makes learning intractable. In this case, we propose a latent factor model, and show that the sample complexity of our model is bounded under mild assumptions. Finally, we show that our proposed models have much better prediction power on several E-sports datasets, and furthermore can be used to reveal interesting patterns that cannot be discovered by previous methods.
C1 [Li, Yao; Fujii, Kevin; Hsieh, Fushing] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.
   [Cheng, Minhao; Hsieh, Cho Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
RP Li, Y (reprint author), Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.
EM yaoli@ucdavis.edu; mhcheng@ucla.edu; kmfujii@ucdavis.edu;
   fhsieh@ucdavis.edu
FU NSF [IIS-1719097]; Intel Faculty Award; Google Cloud and Nvidia
FX The paper is partially supported by the support of NSF via IIS-1719097,
   Intel Faculty Award, Google Cloud and Nvidia.
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Chen  S., 2016, PREDICTING MATCHUPS
   Chen S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P227, DOI 10.1145/2835776.2835787
   Chiang  K.-Y., 2017, ARTIF INTELL, P748
   Fujii  K., 2017, TECHNICAL REPORT
   Fushing H, 2011, P ROY SOC A-MATH PHY, V467, P3590, DOI 10.1098/rspa.2011.0268
   Fushing H, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.061110
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Gleich D.F., 2011, P 17 ACM SIGKDD INT, P60
   Glickman M. E., 1995, AM CHESS J
   Herbrich  R., 2006, NIPS
   Huang  T.-K., 2008, JMLR
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Purushotham  S., 2014, ACM SIGKDD INT C KNO, P552
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Shamir O, 2014, J MACH LEARN RES, V15, P3401
   Wauthier F. L., 2013, INT C MACH LEARN, P109
   Xiao  C., 2016, AAAI
   Zhang L, 2010, PROC INT C TOOLS ART, P249, DOI 10.1109/ICTAI.2010.108
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305003
DA 2019-06-15
ER

PT S
AU Li, Y
   Gupta, A
AF Li, Yin
   Gupta, Abhinav
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Beyond Grids: Learning Graph Representations for Visual Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose learning graph representations from 2D feature maps for visual recognition. Our method draws inspiration from region based recognition, and learns to transform a 2D image into a graph structure. The vertices of the graph define clusters of pixels ("regions"), and the edges measure the similarity between these clusters in a feature space. Our method further learns to propagate information across all vertices on the graph, and is able to project the learned graph representation back into 2D grids. Our graph representation facilitates reasoning beyond regular grids and can capture long range dependencies among regions. We demonstrate that our model can be trained from end-to-end, and is easily integrated into existing networks. Finally, we evaluate our method on three challenging recognition tasks: semantic segmentation, object detection and object instance segmentation. For all tasks, our method outperforms state-of-the-art methods.
C1 [Li, Yin] Univ Wisconsin, Dept Comp Sci, Dept Biostat & Med Informat, 1210 W Dayton St, Madison, WI 53706 USA.
   [Gupta, Abhinav] Carnegie Mellon Univ, Sch Comp Sci, Robot Inst, Pittsburgh, PA 15213 USA.
   [Li, Yin] CMU, Pittsburgh, PA 15213 USA.
RP Li, Y (reprint author), Univ Wisconsin, Dept Comp Sci, Dept Biostat & Med Informat, 1210 W Dayton St, Madison, WI 53706 USA.; Li, Y (reprint author), CMU, Pittsburgh, PA 15213 USA.
EM yin.li@wisc.edu; abhinavg@cs.cmu.edu
FU ONR MURI [N000141612007]; Sloan Fellowship; ONR Young Investigator
   Award; Okawa Fellowship
FX This work was supported by ONR MURI N000141612007, Sloan Fellowship,
   Okawa Fellowship and ONR Young Investigator Award to AG. The authors
   thank Xiaolong Wang for many helpful discussions, and Jianping Shi for
   sharing implementation details of PSPNet.
CR Arandjelovic R., 2013, CVPR
   Arbelaez  Pablo, 2012, CVPR
   Badrinarayanan  Vijay, 2017, TPAMI
   Carreira  Joao, 2012, IJCV
   Chen  L.-C., 2017, TPAMI
   Duvenaud David K, 2015, NIPS
   Farabet  Clement, 2013, TPAMI
   Fidler S., 2013, CVPR
   Fulkerson  Brian, 2009, CVPR
   Gadde  Raghudeep, 2016, ECCV
   Girshick  R., 2018, DETECTRON, P5
   Girshick R., 2014, CVPR
   Gould  Stephen, 2009, NIPS
   Gu C., 2009, CVPR
   He K., 2016, CVPR
   He Kaiming, 2017, ICCV
   Hoiem  Derek, 2005, ICCV
   Ioffe S., 2015, ICML
   Jegou H., 2010, CVPR
   Kalayeh Mahdi M, 2018, ARXIV180602892
   Kipf T. N., 2017, ICLR
   Kohli  Pushmeet, 2009, IJCV
   Krizhevsky A., 2012, NIPS
   Ladicky L., 2009, ICCV
   Lin  Guosheng, 2017, CVPR
   Lin T.-Y., 2014, ECCV
   Lin T.-Y, 2017, CVPR
   Liu  W., 2015, ARXIV150604579
   Long  J., 2015, CVPR
   Luo W., 2016, NIPS
   Malisiewicz  Tomasz, 2009, NIPS
   Munoz  Daniel, 2010, ECCV
   Perronnin F., 2010, ECCV
   Ren S., 2015, NIPS
   Russakovsky Olga, 2015, IJCV
   Scarselli  Franco, 2009, IEEE TNN
   Simonyan  Karen, 2013, NIPS
   Simonyan Karen, 2015, ICLR
   Wang X., 2018, CVPR
   Yan  Junjie, 2015, CVPR
   Yu  Fisher, 2015, ICLR
   Zhang  Hang, 2017, CVPR
   Zhang  Hang, 2018, CVPR
   Zhao H., 2017, CVPR
   Zhou  Bolei, 2017, CVPR
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003075
DA 2019-06-15
ER

PT S
AU Li, YT
   Murias, M
   Major, S
   Dawson, G
   Carlson, DE
AF Li, Yitong
   Murias, Michael
   Major, Samantha
   Dawson, Geraldine
   Carlson, David E.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Extracting Relationships by Multi-Domain Matching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In many biological and medical contexts, we construct a large labeled corpus by aggregating many sources to use in target prediction tasks. Unfortunately, many of the sources may be irrelevant to our target task, so ignoring the structure of the dataset is detrimental. This work proposes a novel approach, the Multiple Domain Matching Network (MDMN), to exploit this structure. MDMN embeds all data into a shared feature space while learning which domains share strong statistical relationships. These relationships are often insightful in their own right, and they allow domains to share strength without interference from irrelevant data. This methodology builds on existing distribution-matching approaches by assuming that source domains are varied and outcomes multi-factorial. Therefore, each domain should only match a relevant subset. Theoretical analysis shows that the proposed approach can have a tighter generalization bound than existing multiple-domain adaptation approaches. Empirically, we show that the proposed methodology handles higher numbers of source domains (up to 21 empirically), and provides state-of-the-art performance on image, text, and multi-channel time series classification, including clinical outcome data in an open label trial evaluating a novel treatment for Autism Spectrum Disorder.
C1 [Li, Yitong; Carlson, David E.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
   [Murias, Michael] Duke Univ, Duke Inst Brain Sci, Durham, NC 27706 USA.
   [Major, Samantha; Dawson, Geraldine] Duke Univ, Dept Psychiat, Durham, NC 27706 USA.
   [Major, Samantha; Dawson, Geraldine] Duke Univ, Dept Behav Sci, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA.
RP Li, YT (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM yitong.li@duke.edu; michael.murias@duke.edu; samantha.major@duke.edu;
   geraldine.dawson@duke.edu; david.carlson@duke.edu
FU Stylli Translational Neuroscience Award; Marcus Foundation; NIMH
   [3R01MH099192-05S2]; NICHD [P50-HD093074]
FX Funding was provided by the Stylli Translational Neuroscience Award,
   Marcus Foundation, NICHD P50-HD093074, and NIMH 3R01MH099192-05S2.
CR Ao S., 2017, AAAI
   Arjovsky M, 2017, ARXIV170107875
   Ash J. T., 2016, ARXIV160204889
   Ben- David S., 2010, MACHINE LEARNING
   Ben- David S., 2014, ANN MATH ARTIFICIAL
   Chen M., 2012, P LEARN WORKSH UT UT, V36
   Courty N., 2017, NIPS
   Courty N., 2017, IEEE PAMI
   Crammer K., 2008, JMLR
   Daume H., 2009, ARXIV09071815
   Dawson G., 2017, STEM CELLS TRANSLATI
   Duan L., 2009, ICML
   Fernando B., 2013, ICCV
   Ganin Y., 2016, JMLR
   Gebru T., 2017, ICCV
   Glorot X., 2011, ICML
   Goodfellow I., 2014, NIPS
   Gulrajani Ishaan, 2017, ARXIV170400028
   Hinton G., 2008, JMLR
   Hou C. - A., 2016, IEEE T IMAGE PROCESS
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, NIPS
   Li C., 2017, ARXIV170609549
   Li Y., 2017, NIPS
   Lin Y. - P., 2017, FRONTIERS HUMAN NEUR
   Liu H., 2016, ICDM
   Liu M. -Y., 2017, NIPS
   Liu Ming-Yu, 2016, NIPS
   Long M., 2016, ICML
   Long M., 2016, NIPS
   Long M., 2017, ICML
   Mansour Y., 2009, NIPS
   Motiian S., 2017, NIPS
   Pan S. J., 2011, IEEE T NEURAL NETWOR
   Russo P., 2017, ARXIV170508824
   Shi Y., 2012, ICML
   Sun Q., 2011, NIPS
   Tu W., 2012, NEUROCOMPUTING
   Tzeng E., 2017, ADVERSARIAL DISCRIMI
   Venkateswara H., 2017, CVPR
   Villani C., 2008, OPTIMAL TRANSPORT OL
   Vu M. - A. T., 2018, J NEUROSCIENCE
   Xie Q., 2017, NIPS
   Xu H., 2012, STAT SIGN PROC WORKS
   Zhao H., 2017, ARXIV170509684
   Zheng W. - L., 2015, IEEE T AUTONOMOUS ME
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001035
DA 2019-06-15
ER

PT S
AU Li, YJ
   Yu, MC
   Li, SZ
   Avestimehr, S
   Kim, NS
   Schwing, A
AF Li, Youjie
   Yu, Mingchao
   Li, Songze
   Avestimehr, Salman
   Kim, Nam Sung
   Schwing, Alexander
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep
   Net Training
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Distributed training of deep nets is an important technique to address some of the present day computing challenges like memory consumption and computational demands. Classical distributed approaches, synchronous or asynchronous, are based on the parameter server architecture, i.e., worker nodes compute gradients which are communicated to the parameter server while updated parameters are returned. Recently, distributed training with AllReduce operations gained popularity as well. While many of those operations seem appealing, little is reported about wall-clock training time improvements. In this paper, we carefully analyze the AllReduce based setup, propose timing models which include network latency, bandwidth, cluster size and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training Specifically, for a setup consisting of a four-node GPU cluster we show wall-clock time training improvements of up to 5.4 x compared to conventional approaches.
C1 [Li, Youjie; Kim, Nam Sung; Schwing, Alexander] Univ Illinois, Urbana, IL 61801 USA.
   [Yu, Mingchao; Li, Songze; Avestimehr, Salman] Univ Southern Calif, Los Angeles, CA USA.
RP Li, YJ (reprint author), Univ Illinois, Urbana, IL 61801 USA.
FU NSF [IIS 17-18221, CNS 17-05047, CNS 15-57244, CCF-1763673,
   CCF-1703575]; 3M; IBM-ILLINOIS Center for Cognitive Computing Systems
   Research (C3SR); Defense Advanced Research Projects Agency (DARPA)
   [HR001117C0053]
FX This work is supported in part by grants from NSF (IIS 17-18221, CNS
   17-05047, CNS 15-57244, CCF-1763673 and CCF-1703575). This work is also
   supported by 3M and the IBM-ILLINOIS Center for Cognitive Computing
   Systems Research (C3SR). Besides, this material is based in part upon
   work supported by Defense Advanced Research Projects Agency (DARPA)
   under Contract No. HR001117C0053. The views, opinions, and/or findings
   expressed are those of the author(s) and should not be interpreted as
   representing the official views or policies of the Department of Defense
   or the U.S. Government.
CR Abadi M., 2016, OSDI
   Alistarh D., 2017, NIPS
   Bengio Y., 2013, PAMI
   Chen C.-Y., 2018, AAAI
   Chen L. C., 2015, ICLR
   Chilimbi T., 2014, OSDI
   Cui H., 2016, EUROSYS
   Dean J., 2012, NIPS
   Dean  J., 2008, COMMUNICATIONS ACM
   Dryden N., 2016, MLHPC
   Dryden N., 2018, MLHPC
   Goyal P., 2017, CVPR
   Harlap A., 2018, ARXIV180603377V1
   He K., 2016, CVPR
   Ho Q., 2013, NIPS
   Iandola F. N., 2016, CVPR
   Intel Corporation, 2018, INT MATH KERN LIB
   Isard M., 2007, ACM SIGOP
   Kim H., 2016, ARXIV160208191CS
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Langford J., 2009, NIPS
   Larsson  G., 2016, FRACTALNET ULTRADEEP
   Lecun Y., 2015, NATURE
   LeCun Y., 1998, GRADIENT BASED LEARN
   Li M., 2014, OSDI
   Li M., 2014, NIPS
   Li Y., 2018, MICRO
   Lian X., 2017, NIPS
   Lian X., 2018, ARXIV171006952V3
   Liao R., 2016, NIPS
   Lin Y., 2018, ICLR
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Mnih V., 2015, NATURE
   Moritz P., 2016, ICLR
   Murray D. G., 2013, SOSP
   Nvidia, 2015, WHIT
   NVIDIA Corporation, 2017, TITAN XP
   NVIDIA Corporation, 2010, NVIDIA CUDA C PROGR
   OpenMPI Community, 2017, OPENMPI HIGH PERF ME
   Russakovsky Olga, 2015, IJCV
   Seide F, 2014, INT CONF ACOUST SPEE
   Sharma H., 2016, MICRO
   Silver D., 2016, NATURE
   Strom N., 2015, INTERSPEECH
   Thakur R., 2005, IJHPCA
   Wang Q., 2016, ISCAS
   Wang Q., 2017, NEUROCOMPUTING
   Wen W., 2017, NIPS
   Zaharia M., 2010, HOTCLOUD
   Zhang C., 2015, FPGA
   Zhou  B., 2014, NIPS
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002058
DA 2019-06-15
ER

PT S
AU Li, YZ
   Liang, YY
AF Li, Yuanzhi
   Liang, Yingyu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning Overparameterized Neural Networks via Stochastic Gradient
   Descent on Structured Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.
C1 [Li, Yuanzhi] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Liang, Yingyu] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
RP Li, YZ (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM yuanzhil@stanford.edu; yliang@cs.wisc.edu
FU NSF [CCF-1527371, DMS-1317308]; Simons Investigator Award; Simons
   Collaboration Grant; Office of the Vice Chancellor for Research and
   Graduate Education at the University of Wisconsin Madison; Wisconsin
   Alumni Research Foundation;  [FA9550-18-1-0166];  [ONR-N00014-16-1-2329]
FX We would like to thank the anonymous reviewers of NIPS' 18 and Jason Lee
   for helpful comments. This work was supported in part by
   FA9550-18-1-0166, NSF grants CCF-1527371, DMS-1317308, Simons
   Investigator Award, Simons Collaboration Grant, and
   ONR-N00014-16-1-2329. Yingyu Liang would also like to acknowledge that
   support for this research was provided by the Office of the Vice
   Chancellor for Research and Graduate Education at the University of
   Wisconsin Madison with funding from the Wisconsin Alumni Research
   Foundation.
CR Arora Sanjeev, 2018, ARXIV180205296
   Arpit Devansh, 2017, ARXIV170605394
   Bartlett P. L., 2017, ADV NEURAL INFORM PR, P6241
   Baykal Cenk, 2018, ARXIV180405345
   Boob Digvijay, 2017, ARXIV171011241
   Brutzkus Alon, 2017, ARXIV171010174
   Brutzkus Alon, 2017, ARXIV170207966
   Dinh L., 2017, ARXIV170304933
   Dziugaite Gintare Karolina, 2017, ARXIV170311008
   Ge Rong, 2017, ARXIV171100501
   Gunasekar Suriya, 2017, ADV NEURAL INFORM PR, P6152
   Hardt M., 2016, ARXIV161104231
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Keskar N. S., 2016, ARXIV160904836
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Yuanzhi, 2017, ARXIV171209203
   Li Yuanzhi, 2017, ADV NEURAL INFORM PR, P597
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Ma Cong, 2017, ARXIV171110467
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Moustapha Cisse, 2017, ARXIV170408847
   Nagarajan Vaishnavh, 2017, NIPS WORKSH DEEP LEA
   Neyshabur B., 2014, ARXIV14126614
   Neyshabur Behnam, 2017, ARXIV170709564
   Soltanolkotabi Mahdi, 2017, ARXIV170704926
   Soudry D, 2016, ARXIV160508361
   Soudry Daniel, 2017, ARXIV171010345
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Tian Yuandong, 2017, ARXIV170300560
   Xie Bo, 2016, ARXIV161103131
   Zhang C, 2016, ARXIV161103530
   Zhong Kai, 2017, ARXIV170603175
   Zhou Wenda, 2018, ARXIV180405862
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002068
DA 2019-06-15
ER

PT S
AU Li, ZZ
   Li, J
AF Li, Zhize
   Li, Jian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth finite-sum problems. In particular, the objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a possibly non-differentiable but convex component. We propose a proximal stochastic gradient algorithm based on variance reduction, called ProxSVRG+. Our main contribution lies in the analysis of ProxSVRG+. It recovers several existing convergence results and improves/generalizes them (in terms of the number of stochastic gradient oracle calls and proximal oracle calls). In particular, ProxSVRG+ generalizes the best results given by the SCSG algorithm, recently proposed by [Lei et al., 2017] for the smooth nonconvex case. ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis. Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes, which partially solves an open problem proposed in [Reddi et al., 2016]. Also, ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al., 2016]. Moreover, for nonconvex functions satisfied Polyak-Lojasiewicz condition, we prove that ProxSVRG+ achieves a global linear convergence rate without restart unlike ProxSVRG. Thus, it can automatically switch to the faster linear convergence in some regions as long as the objective function satisfies the PL condition locally in these regions. Finally, we conduct several experiments and the experimental results are consistent with the theoretical results.
C1 [Li, Zhize; Li, Jian] Tsinghua Univ, IIIS, Beijing, Peoples R China.
RP Li, ZZ (reprint author), Tsinghua Univ, IIIS, Beijing, Peoples R China.
EM zz-li14@mails.tsinghua.edu.cn; lijian83@mail.tsinghua.edu.cn
FU National Basic Research Program of China [2015CB358700]; National
   Natural Science Foundation of China [61772297, 61632016, 61761146003];
   Microsoft Research Asia
FX This research is supported in part by the National Basic Research
   Program of China Grant 2015CB358700, the National Natural Science
   Foundation of China Grant 61772297, 61632016, 61761146003, and a grant
   from Microsoft Research Asia. The authors would like to thank Rong Ge
   (Duke), Xiangliang Zhang (KAUST) and the anonymous reviewers for their
   useful suggestions.
CR Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Allen-Zhu  Zeyuan, 2017, ARXIV170808694
   Anitescu M, 2000, SIAM J OPTIMIZ, V10, P1116, DOI 10.1137/S1052623499359178
   Aravkin  Aleksandr, 2016, ARXIV161001101
   Csiba D., 2017, ARXIV170903014
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Fu  Haoyu, 2018, ARXIV180206463
   Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Goyal Priya, 2017, ARXIV170602677
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Keskar N. S., 2016, ARXIV160904836
   Lan GH, 2018, SIAM J OPTIMIZ, V28, P2753, DOI 10.1137/17M1157891
   Lan  Guanghui, 2015, ARXIV150702000
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Li  Qunwei, 2017, P 34 INT C MACH LEAR, P2111
   Necoara  Ion, 2015, ARXIV150406298
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Polyak Boris Teodorovich, 1963, ZH VYCH MAT MAT FIZ, V3, P643
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Reddi S. J., 2016, ADV NEURAL INFORM PR, V29, P1145
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P699
   Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157
   Zhong Kai, 2017, ARXIV170603175
   Zhou  Dongruo, 2018, ARXIV180607811
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000010
DA 2019-06-15
ER

PT S
AU Li, ZW
   Chen, QF
   Koltun, V
AF Li, Zhuwen
   Chen, Qifeng
   Koltun, Vladlen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Combinatorial Optimization with Graph Convolutional Networks and Guided
   Tree Search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GAME; GO
AB We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some NP-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.
C1 [Li, Zhuwen; Koltun, Vladlen] Intel Labs, Santa Clara, CA 95054 USA.
   [Chen, Qifeng] HKUST, Hong Kong, Peoples R China.
RP Li, ZW (reprint author), Intel Labs, Santa Clara, CA 95054 USA.
CR Akiba Takuya, 2015, ALENEX
   Andrade DV, 2012, J HEURISTICS, V18, P525, DOI 10.1007/s10732-012-9196-4
   Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090
   Balyo Tomas, SAT COMPETITION 2017
   Battiti R, 2001, ALGORITHMICA, V29, P610, DOI 10.1007/s004530010074
   Bello I., 2016, ARXIV161109940
   Bronstein Michael M., 2017, IEEE SIGNAL PROCESSI, V34
   Chen Qifeng, 2017, ICCV
   Christofides N, 1976, TECHNICAL REPORT
   Dai H., 2017, NIPS
   de Moura Leonardo Mendonca, 2008, TACAS
   Defferrard M., 2016, NIPS
   Feo Thomas A., 1994, OPERATIONS RES, V42
   Gilmer J., 2017, ICML
   Gonzalez Teofilo F., 2007, HDB APPROXIMATION AL
   Grosso A, 2008, J HEURISTICS, V14, P587, DOI 10.1007/s10732-007-9055-x
   Gurobi Optimization Inc, 2018, GUR OPT REF MAN VERS
   Guzman-Rivera A., 2012, NIPS
   He He, 2014, NIPS
   Hochbaum D. S., 1997, APPROXIMATION ALGORI
   Hoos Holger H., 2000, SAT
   Karp R., 1972, COMPLEXITY COMPUTER
   Ke Xu, 2007, ARTIFICIAL INTELLIGE, V171
   Kingma D. P., 2015, ICLR
   Kipf T. N., 2017, ICLR
   Kool Wouter, 2018, ARXIV180308475
   Lamm S, 2017, J HEURISTICS, V23, P207, DOI 10.1007/s10732-017-9337-x
   Leskovec J., 2014, SNAP DATASETS STANFO
   Li Zhuwen, 2018, CVPR
   Nair V., 2010, ICML
   Nowak A., 2017, ARXIV170607450
   Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI
   Selsam Daniel, 2018, ARXIV180203685
   Sen Prithviraj, 2008, AI MAGAZINE, V29
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Vazirani Vijay V., 2004, APPROXIMATION ALGORI
   Vinyals O., 2015, NIPS
   Williamson D. P., 2011, DESIGN APPROXIMATION
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300050
DA 2019-06-15
ER

PT S
AU Liang, C
   Norouzi, M
   Berant, J
   Le, Q
   Lao, N
AF Liang, Chen
   Norouzi, Mohammad
   Berant, Jonathan
   Quoc Le
   Lao, Ni
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Memory Augmented Policy Optimization for Program Synthesis and Semantic
   Parsing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimates. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization. Our key idea is to express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside a memory buffer, and a separate expectation over trajectories outside of the buffer. To design an efficient algorithm based on this idea, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to speed up training MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WIKITABEQUESTIONS benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WIKISQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at goo.gl/TXBp4e.
C1 [Liang, Chen; Norouzi, Mohammad; Quoc Le] Google Brain, Mountain View, CA 94039 USA.
   [Berant, Jonathan] Tel Aviv Univ, AI2, Tel Aviv, Israel.
   [Lao, Ni] SayMosaic Inc, Palo Alto, CA USA.
RP Liang, C (reprint author), Google Brain, Mountain View, CA 94039 USA.
EM crazydonkey200@gmail.com; mnorouzi@google.com; joberant@cs.tau.ac.il;
   qvl@google.com; ni.lao@mosaix.ai
FU Israel Science Foundation [942/16]
FX We would like to thank Dan Abolafia, Ankur Taly, Thanapon Noraset,
   Arvind Neelakantan, Wenyun Zuo, Chenchen Pan and Mia Liang for helpful
   discussions. Jonathan Berant was partially supported by The Israel
   Science Foundation grant 942/16.
CR Abadi M., 2016, ARXIV160304467
   Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Abolafia Daniel A., 2018, ARXIV180103526
   Andrychowicz Marcin, 2017, NIPS
   Balog M., 2017, ICLR
   Bellemare Marc G, 2013, JMLR
   Bello I., 2016, ARXIV161109940
   Berant J., 2013, EMNLP, V2, P6
   Brockman G, 2016, ARXIV160601540
   Bunel Rudy, 2018, INT C LEARN REPR
   Chen Liang, 2017, ACL
   Das Abhishek, 2017, ARXIV170306585
   Degris Thomas, 2012, ICML
   Espeholt Lasse, 2018, ARXIV180201561
   Grathwohl Will, 2017, ARXIV171100123
   Guu Kelvin, 2017, ACL
   Haug Till, 2018, ECIR
   Hester Todd, 2018, AAAI
   Hochreiter S., 1997, NEURAL COMPUT
   Huang Po-Sen, 2018, CORR
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Krishnamurthy Jayant, 2017, EMNLP
   Le Roux Nicolas, 2017, ICLR
   Li Dong, 2018, CORR
   Li J., 2016, ARXIV160601541
   Liang P., 2011, ACL
   LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1023/A:1022628806385
   Liu Hao, 2017, ARXIV171011198
   Mania H, 2018, ARXIV180307055
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mudrakarta Pramod Kaushik, 2018, ARXIV180304579
   Nachum Ofir, 2017, ADV NEURAL INFORM PR, P2775
   Nan Ding, 2017, ADV NEURAL INFORM PR, P2817
   Neelakantan Arvind, 2016, ARXIV161108945
   Nichol Alex, 2018, ARXIV180403720
   Norouzi Mohammad, 2016, ADV NEURAL INFORM PR, P1723
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Oh Junhyuk, 2018, ICML
   Pasupat P., 2016, P 54 ANN M ASS COMP, P23
   Pasupat Panupong, 2015, ACL
   Pasupat Panupong, 2016, ACL
   Pathak D, 2017, ICML
   Pennington Jeffrey, 2014, EMNLP
   Peters Jan, 2006, IROS
   Rajeswaran Aravind, 2017, NIPS
   Ranzato Marc Aurelio, 2016, ICLR
   Ross S., 2011, J MACHINE LEARNING R, P627
   Schaul T., 2016, ICLR
   Schulman  J., 2017, ARXIV170706347
   Schulman John, 2015, ICML
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Silver David, 2017, NATURE
   Sun Yibo, 2018, ARXIV180408338
   Tang Haoran, 2017, ADV NEURAL INFORM PR, V30, P2753
   Wang Chenglong, 2018, ICLR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu Cathy, 2018, ICLR
   Wu Y., 2016, ARXIV160908144
   Xu Xiaojun, 2018, ICLR
   Yih Wen-tau, 2016, ACL
   Yu Tao, 2018, ARXIV180409769
   Zaremba Wojciech, 2015, ARXIV150500521
   Zelle JM, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1050
   Zettlemoyer L. S., 2005, P 21 C UNC ART INT U, P658
   Zhang Yuchen, 2017, ACL
   Zhong V., 2017, ARXIV170900103
   Ziyu Wang, 2017, ICLR
   Zoph B., 2016, ARXIV161101578
   Zoph B., 2017, ARXIV170707012
   Zoph Barret, 2016, ICLR
NR 70
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004054
DA 2019-06-15
ER

PT S
AU Liang, SY
   Sun, RY
   Lee, JD
   Srikant, R
AF Liang, Shiyu
   Sun, Ruoyu
   Lee, Jason D.
   Srikant, R.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adding One Neuron Can Eliminate All Bad Local Minima
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum.
C1 [Liang, Shiyu; Srikant, R.] Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Champaign, IL 61820 USA.
   [Sun, Ruoyu] Univ Illinois, Dept ISE, Coordinated Sci Lab, Champaign, IL USA.
   [Lee, Jason D.] Univ Southern Calif, Marshall Sch Business, Los Angeles, CA 90089 USA.
RP Liang, SY (reprint author), Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Champaign, IL 61820 USA.
EM sliang26@illinois.edu; ruoyus@illinois.edu; jasonlee@marshall.usc.edu;
   rsrikant@illinois.edu
FU USDA/NSF CPS grant [AG 2018-67007-2837]; NSF [NeTS 1718203, CPS ECCS
   1739189, CCF 1755847]; DTRA Grant [HDTRA1-15-1-0003]; Dept. of ISE,
   University of Illinois Urbana-Champaign
FX Research is supported by the following grants: USDA/NSF CPS grant AG
   2018-67007-2837, NSF NeTS 1718203, NSF CPS ECCS 1739189, DTRA Grant DTRA
   grant HDTRA1-15-1-0003, NSF CCF 1755847 and a start-up grant from Dept.
   of ISE, University of Illinois Urbana-Champaign.
CR Andoni A., 2014, ICML
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Brutzkus Alon, 2017, ARXIV170207966
   Choromanska Anna, 2015, AISTATS
   Cortes C., 1995, MACHINE LEARNING
   Du S. S, 2018, ARXIV180301206
   Du S. S., 2017, ARXIV170906129
   Freeman C Daniel, 2016, ARXIV161101540
   Gautier A., 2016, ADV NEURAL INFORM PR, P1687
   Ge R., 2018, ICLR
   Goel S., 2017, ARXIV170906010
   Goodfellow I.J., 2013, ARXIV13024389
   Haeffele B., 2014, ICML
   HAEFFELE B. D., 2015, ARXIV150607540
   Hardt Moritz, 2017, ICLR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, CVPR
   Janzamin Majid, 2015, ARXIV150608473
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Krizhevsky A., 2012, NIPS
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Yuanzhi, 2017, ADV NEURAL INFORM PR, P597
   Liang S., 2018, UNDERSTANDING LOSS S
   Livni R., 2014, NIPS
   Mei Song, 2018, ARXIV180406561
   Nguyen Q., 2017, ARXIV171010928
   Safran Itay, 2018, ICML
   Sedghi H., 2014, ARXIV14122693
   Shamir O., 2018, ARXIV180406739
   Soltanolkotabi Mahdi, 2017, NIPS, V30, P2004
   Soudry  D., 2017, ARXIV170205777
   Soudry D, 2016, ARXIV160508361
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Yun Chulhee, 2017, ARXIV170702444
   Zhang C., 2016, ICLR
   Zhang X., 2012, SIAM J MATRIX ANAL A
   Zhong Kai, 2017, ARXIV170603175
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304037
DA 2019-06-15
ER

PT S
AU Liang, XD
   Hu, ZT
   Zhang, H
   Lin, L
   Xing, EP
AF Liang, Xiaodan
   Hu, Zhiting
   Zhang, Hao
   Lin, Liang
   Xing, Eric P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Symbolic Graph Reasoning Meets Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.
C1 [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
   [Hu, Zhiting; Zhang, Hao] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA.
RP Liang, XD (reprint author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.
EM xdliang328@gmail.com; zhitingh@cs.cmu.edu; hao@cs.cmu.edu;
   linliang@ieee.org; epxing@cs.cmu.edu
FU National Key Research and Development Program of China [2018YFC0830103];
   National High Level Talents Special Support Plan (Ten Thousand Talents
   Program); National Natural Science Foundation of China (NSFC) [61622214,
   61836012]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant No. 2018YFC0830103, in part by
   National High Level Talents Special Support Plan (Ten Thousand Talents
   Program), and in part by National Natural Science Foundation of China
   (NSFC) under Grant No. 61622214, and 61836012.
CR Arnab A, 2016, LECT NOTES COMPUT SC, V9906, P524, DOI 10.1007/978-3-319-46475-6_33
   Badrinarayanan V., 2015, CVPR
   BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X
   Caesar H., 2016, ARXIV161203716
   Chandra S., 2017, ICCV
   Chen LC, 2016, IEEE T PATTERN ANAL, P1, DOI DOI 10.1109/INFOCOM.2016.7524570
   Chen X., 2018, CVPR
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Gilmer J., 2017, ARXIV170401212
   He K., 2016, ECCV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton Geoffrey, 2018, ICLR
   Hu H., 2017, ARXIV170309891
   Huang G, 2017, CVPR
   Joulin Armand, 2016, ARXIV161203651
   Kipf T. N., 2017, ICLR
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, V24, P109
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lafferty J., 2001, CONDITIONAL RANDOM F
   Lao  N., 2011, P C EMP METH NAT LAN, P529
   Liang X., 2018, CVPR
   Liang  X., 2017, CVPR
   Liang X., 2016, ECCV
   LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348
   Lin  Guosheng, 2017, CVPR
   Liu  W., 2015, ARXIV150604579
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Marino K., 2016, ARXIV161204844
   Mitchell Tom M., 2015, P 29 AAAI C ART INT, P2302
   Mottaghi R., 2014, CVPR
   NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1207/s15516709cog0402_2
   Niepert M., 2016, INT C MACH LEARN, P2014
   Ordonez V, 2013, IEEE I CONF COMP VIS, P2768, DOI 10.1109/ICCV.2013.344
   Redmon J, 2017, CVPR
   Sabour S., 2017, NIPS
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schwing A. G., 2015, ARXIV150302351
   Shuai B., 2017, TPAMI
   Wang X., 2018, CVPR
   Wu Z., 2016, CORR
   Wu Z., 2016, ARXIV160506885
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yu F., 2015, ARXIV151107122
   Zagoruyko S, 2016, ARXIV160507146
   Zhao H., 2017, ICCV
   Zhao H., 2017, CVPR
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou B., 2016, ARXIV160805442
   Zhu Y., 2015, ARXIV150705670
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301081
DA 2019-06-15
ER

PT S
AU Lim, B
   Alaa, A
   van der Schaar, M
AF Lim, Bryan
   Alaa, Ahmed
   van der Schaar, Mihaela
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Forecasting Treatment Responses Over Time Using Recurrent Marginal
   Structural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MODELS
AB Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth [12], we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data - even under changes in the policy of treatment assignments - and performance gains over benchmarks.
C1 [Lim, Bryan] Univ Oxford, Dept Engn Sci, Oxford, England.
   [Alaa, Ahmed] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
   [van der Schaar, Mihaela] Alan Turing Inst, London, England.
RP Lim, B (reprint author), Univ Oxford, Dept Engn Sci, Oxford, England.
EM bryan.lim@eng.ox.ac.uk; ahmedmalaa@ucla.edu; mschaar@turing.ac.uk
FU Oxford-Man Institute of Quantitative Finance; US Office of Naval
   Research (ONR); Alan Turing Institute
FX This research was supported by the Oxford-Man Institute of Quantitative
   Finance, the US Office of Naval Research (ONR), and the Alan Turing
   Institute.
CR Alaa Ahmed M., 2017, P 31 C NEUR INF PROC
   Alaa Ahmed M., 2017, INT C MACH LEARN ICM
   Atan Mihaela van der Schaar Onur, 2018, MACHINE LEARNING
   Atan Mihaela van der Schaar Onur, 2018, AAAI
   Barbolosi D, 2001, COMPUT BIOL MED, V31, P157, DOI 10.1016/S0010-4825(00)00032-9
   Bartsch Helmut, 2007, V174, P19
   Carrara L, 2017, EXPERT OPIN DRUG DIS, V12, P785, DOI 10.1080/17460441.2017.1340271
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Clevert Djork-Arne, 2015, CORR
   Daniel RM, 2013, STAT MED, V32, P1584, DOI 10.1002/sim.5686
   Egger PH, 2013, ECON LETT, V119, P32, DOI 10.1016/j.econlet.2013.01.006
   Eigenmann MJ, 2017, J PHARMACOKINET PHAR, V44, P617, DOI 10.1007/s10928-017-9553-x
   Geng CR, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13646-z
   Hartford Jason, 2017, INT C MACH LEARN ICM
   Hernan MA, 2000, EPIDEMIOLOGY, V11, P561, DOI 10.1097/00001648-200009000-00012
   Hernan MA, 2001, J AM STAT ASSOC, V96, P440, DOI 10.1198/016214501753168154
   Hernan MA, 2018, CAUSAL INFERENCE
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hoiles William, 2016, NEURAL INFORM PROCES
   Howe CJ, 2012, EPIDEMIOLOGY, V23, P574, DOI 10.1097/EDE.0b013e31824d1ccb
   Kingma Diederik P., 2015, INT C LEAR REPR ICLR
   Lusivika-Nzinga C, 2017, BMC MED RES METHODOL, V17, DOI 10.1186/s12874-017-0434-1
   Mansournia MA, 2012, EPIDEMIOLOGY, V23, P631, DOI 10.1097/EDE.0b013e31824cc1c3
   Mansournia Mohammad Ali, 2017, BMJ, V359
   McCulloch C. E., 2001, GEN LINEAR MIXED MOD
   Mortimer KM, 2005, AM J EPIDEMIOL, V162, P382, DOI 10.1093/aje/kwi208
   Mould DR, CPT PHARMACOMETRICS, V4, P12
   Park K, 2017, YONSEI MED J, V58, P1, DOI 10.3349/ymj.2017.58.1.1
   Richardson TS, 2014, STAT SCI, V29, P459, DOI 10.1214/14-STS505
   Robins JM, 2000, EPIDEMIOLOGY, V11, P550, DOI 10.1097/00001648-200009000-00011
   Roy J, 2017, BIOSTATISTICS, V18, P32, DOI 10.1093/biostatistics/kxw029
   Schulam Peter, 2017, NEURAL INFORM PROCES
   Silva Ricardo, 2016, NEURAL INFORM PROCES
   Soleimani Hossein, 2017, UNCERTAINTY ARTIFICI
   Sutskever Ilya, 2014, NEURAL INFORM PROCES
   Swaminathan A, 2015, J MACH LEARN RES, V16, P1731
   Swaminathan Adith, 2015, CORR
   Thoemmes F, 2016, EMERG ADULTHOOD, V4, P40, DOI 10.1177/2167696815621645
   Titsias M. K., 2009, P 12 INT C ART INT S
   Vlachostergios Panagiotis J., 2018, NATURE REV CLIN ONCO
   Wager  S., 2017, J AM STAT ASS
   Xiao YL, 2010, INT J BIOSTAT, V6, DOI 10.2202/1557-4679.1208
   Xu Yanbo, 2016, P 1 MACH LEARN HEALT
   Yoon Jinsung, 2018, INT C LEARN REPR ICL
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002007
DA 2019-06-15
ER

PT S
AU Lim, CH
AF Lim, Cong Han
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI An Efficient Pruning Algorithm for Robust Isotonic Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONVEX-FUNCTIONS SUBJECT
AB We study a generalization of the classic isotonic regression problem where we allow separable nonconvex objective functions, focusing on the case where the functions are estimators used in robust regression. One can solve this problem to within epsilon-accuracy (of the global minimum) in O(n/epsilon) using a simple dynamic program, and the complexity of this approach is independent of the underlying functions. We introduce an algorithm that combines techniques from the convex case with branch-and-bound ideas that is able to exploit the shape of the functions. Our algorithm achieves the best known bounds for both the convex case (O(n log( 1/epsilon ))) and the general nonconvex case. Experiments show that this algorithm can perform much faster than the dynamic programming approach on robust estimators, especially as the desired accuracy increases.
C1 [Lim, Cong Han] Georgia Tech, Sch Ind Syst & Engn, Atlanta, GA 30332 USA.
RP Lim, CH (reprint author), Georgia Tech, Sch Ind Syst & Engn, Atlanta, GA 30332 USA.
EM clim31@gatech.edu
FU NSF [CMMI-1634597]; DIMACS/Simons Collaboration on Bridging Continuous
   and Discrete Optimization through NSF [CCF-1740425]; NSF Award at
   UW-Madison [IIS-1447449]
FX The author would like to thank Alberto Del Pia and Silvia Di Gregorio
   for initial discussion that lead to this work. The author was partially
   supported by NSF Award CMMI-1634597, NSF Award IIS-1447449 at
   UW-Madison. Part of the work was completed while visiting the Simons
   Institute for the Theory of Computing (partially supported by the
   DIMACS/Simons Collaboration on Bridging Continuous and Discrete
   Optimization through NSF Award CCF-1740425).
CR Ahuja RK, 2001, OPER RES, V49, P784, DOI 10.1287/opre.49.5.784.10601
   AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Bach F, 2018, ADV NEURAL INFORM PR, V31, P1
   Bach F., 2015, ARXIV151100394CSMATH
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Best MJ, 2000, SIAM J OPTIMIZ, V10, P658, DOI 10.1137/S1052623497314970
   Bogdan  M., 2013, ARXIV13101969
   BRUNK HD, 1955, ANN MATH STAT, V26, P607, DOI 10.1214/aoms/1177728420
   Burdakov O, 2017, J OPTIMIZ THEORY APP, V172, P929, DOI 10.1007/s10957-017-1060-0
   Felzenszwalb P.F., 2012, THEORY COMPUT, V8, P415, DOI DOI 10.4086/TOC.2012.V008A019
   Gunasekar S, 2016, ADV NEURAL INFORM PR, V29, P1370
   Hampel F. R, 2011, ROBUST STAT APPROACH, V196
   Hochbaum DS, 2017, SIAM J OPTIMIZ, V27, P2563, DOI 10.1137/15M1024081
   Hochbaum DS, 2001, J ACM, V48, P686, DOI 10.1145/502090.502093
   Huber P, 2004, WILEY SERIES PROBABI
   Kalai A, 2009, C LEARN THEOR
   Kolmogorov V, 2016, SIAM J IMAGING SCI, V9, P605, DOI 10.1137/15M1010257
   Lim C. H, 2016, P 19 INT C ART INT S, P1205
   Stout Q. F, 2014, FASTEST ISOTONIC REG
   Suehiro Daiki, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P260, DOI 10.1007/978-3-642-34106-9_22
   Sysoev O, 2016, SMOOTHED MONOTONIC R
   Tibshirani RJ, 2011, TECHNOMETRICS, V53, P54, DOI 10.1198/TECH.2010.10111
   Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55
   Zeng X., 2014, ARXIV14094271
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300021
DA 2019-06-15
ER

PT S
AU Lin, C
   Zhong, Z
   Wu, W
   Yan, JJ
AF Lin, Chen
   Zhong, Zhao
   Wu, Wei
   Yan, Junjie
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Synaptic Strength For Convolutional Neural Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a connection based on the amount of information it transports. Experiment results show the effectiveness of our approach. On CIFAR-10, we prune connections for various CNN models with up to 96%, which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which is competitive to state-of-the-art compact CNNs such as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as following: (1) We introduce Synaptic Strength, a new class of parameters for CNNs to indicate the importance of each connections. (2) Our approach can prune various CNNs with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis.
C1 [Lin, Chen; Zhong, Zhao; Wu, Wei; Yan, Junjie] SenseTime Res, Beijing, Peoples R China.
   [Zhong, Zhao] Univ Chinese Acad Sci, CASIA, NLPR, Beijing, Peoples R China.
RP Lin, C (reprint author), SenseTime Res, Beijing, Peoples R China.
EM linchen@sensetime.com; zhao.zhong@nlpr.ia.ac.cn; wuwei@sensetime.com;
   yanjunjie@sensetime.com
CR Anwar Sajid, 2016, ARXIV161009639
   Bruna J., 2014, ADV NEURAL INFORM PR
   Chechik G, 1998, NEURAL COMPUT, V10, P1759, DOI 10.1162/089976698300017124
   Chetlur S., 2014, ARXIV14100759
   Courbariaux M, 2016, ARXIV160202830
   Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, P3123
   Craik FIM, 2006, TRENDS COGN SCI, V10, P131, DOI 10.1016/j.tics.2006.01.007
   Denker John S, 1990, OPTIMAL BRAIN DAMAGE, P598
   Gray S., 2017, GPU KERNELS BLOCK SP
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   He Yihui, 2017, ARXIV170706168
   Hinton G., 2015, ARXIV150302531
   Howard Andrew G., 2017, ARXIV170404861
   Huang Gao, 2017, ARXIV171109224
   Huang Zehao, 2017, ARXIV170701213
   Ioffe S., 2015, ARXIV150203167
   Lavin Andrew, 2016, FAST ALGORITHMS CONV
   Li Hao, 2016, ARXIV160808710
   Li Sheng, 2017, ARXIV170208597
   Liu X, 2018, ARXIV180206367
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Louizos Christos, 2018, INT C LEARN REPR
   Mao Huizi, 2017, EXPLORING GRANULARIT
   Molchanov P., 2016, ARXIV161106440
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Romero Adriana, 2014, ARXIV14126550
   Sandler M., 2018, ARXIV180104381
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Vanderhaeghen P, 2010, CSH PERSPECT BIOL, V2, DOI 10.1101/cshperspect.a001859
   Vedaldi A., 2014, BMVC
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Wu Jianxin, 2017, ARXIV170706342
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhang T., 2017, CORR
   Zhang X, 2017, ARXIV170701083
   Zhong Z., 2017, CORR
   Zhou A., 2017, ARXIV170203044
   Zoph B., 2016, ARXIV161101578
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004068
DA 2019-06-15
ER

PT S
AU Lin, HZ
   Jegelka, S
AF Lin, Hongzhou
   Jegelka, Stefanie
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI ResNet with one-neuron hidden layers is a Universal Approximator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NETWORKS; BOUNDS
AB We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. l(1)(R-d). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.
C1 [Lin, Hongzhou; Jegelka, Stefanie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Lin, HZ (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM hongzhou@mit.edu; stefje@mit.edu
FU Defense Advanced Research Projects Agency [YFA17 N66001-17-1-4039]
FX We would like to thank Jeffery Z. HaoChen for useful feedback and
   suggestions for this paper. This research was supported by The Defense
   Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039).
   The views, opinions, and/or findings contained in this article are those
   of the author and should not be interpreted as representing the official
   views or policies, either expressed or implied, of the Defense Advanced
   Research Projects Agency or the Department of Defense.
CR Arora S., 2018, ARXIV180206509
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   Beise H., 2018, ARXIV180701194
   Brutzkus A., 2018, INT C LEARN REPR ICL
   Choromanska A., 2015, INT C ART INT STAT A
   Cohen N., 2016, C LEARN THEOR COLT
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Du Simon S., 2017, ARXIV171200779
   Eldan R., 2016, C LEARN THEOR COLT
   FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8
   Hanin Boris, 2017, ARXIV171011278
   Hardt M., 2017, INT C LEARN REPR ICL
   He Kaiming, 2016, EUR C COMP VIS ECCV
   He Kaiming, 2016, IEEE C COMP VIS PATT
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   KURKOVA V, 1992, NEURAL NETWORKS, V5, P501, DOI 10.1016/0893-6080(92)90012-8
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang S., 2017, INT C LEARN REPR ICL
   Lu Z., 2017, ADV NEURAL INFORM PR
   Mhaskar HN, 2016, ANAL APPL, V14, P829, DOI 10.1142/S0219530516400042
   Mhaskar HN, 1996, NEURAL COMPUT, V8, P164, DOI 10.1162/neco.1996.8.1.164
   Nguyen Q., 2017, P INT C MACH LEARN I
   Rolnick D., 2018, INT C LEARN REPR ICL
   Shalev-Shwartz S., 2017, ARXIV170600687
   Shamir O., 2018, ARXIV180406739
   Simonyan K, 2015, INT C LEARN REPR ICL
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2015, IEEE C COMP VIS PATT
   Szymanski L, 2014, IEEE T NEUR NET LEAR, V25, P1816, DOI 10.1109/TNNLS.2013.2296046
   Telgarsky M., 2016, C LEARN THEOR COLT
   Yarotsky D., 2018, ARXIV180203620
   Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002
   Yun C., 2018, INT C LEARN REPR ICL
   Zhang C., 2016, INT C LEARN REPR ICL
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000065
DA 2019-06-15
ER

PT S
AU Lin, ZA
   Khetan, A
   Fanti, G
   Oh, S
AF Lin, Zinan
   Khetan, Ashish
   Fanti, Giulia
   Oh, Sewoong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI PacGAN: The power of two samples in generative adversarial networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing-in particular the seminal result of Blackwell [4]-to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements.
C1 [Lin, Zinan; Fanti, Giulia] Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.
   [Khetan, Ashish; Oh, Sewoong] Univ Illinois, IESE Dept, Champaign, IL USA.
RP Lin, ZA (reprint author), Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.
EM zinanl@andrew.cmu.edu; ashish.khetan09@gmail.com; gfanti@andrew.cmu.edu;
   swoh@illinois.edu
FU NSF [CNS-1527754, CCF-1553452, CCF-1705007, RI-1815535, ACI-1445606];
   Google Faculty Research Award; National Science Foundation [OCI-1053575]
FX This work is supported by NSF awards CNS-1527754, CCF-1553452,
   CCF-1705007, and RI-1815535 and Google Faculty Research Award. This work
   used the Extreme Science and Engineering Discovery Environment (XSEDE),
   which is supported by National Science Foundation grant number
   OCI-1053575. Specifically, it used the Bridges system, which is
   supported by NSF award number ACI-1445606, at the Pittsburgh
   Supercomputing Center (PSC). This work is partially supported by the
   generous research credits on AWS cloud computing resources from Amazon.
CR Arjovsky M., 2017, ARXIV170107875
   Arora S., 2017, ARXIV170608224
   Arora S., 2017, ARXIV170300573
   BLACKWELL D, 1953, ANN MATH STAT, V24, P265, DOI 10.1214/aoms/1177729032
   Che T., 2016, ARXIV161202136
   Defferrard M., 2016, ADV NEURAL INFORM PR, P3844
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian, 2016, ARXIV170100160
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505
   Karras Tero, 2017, ARXIV171010196
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kipf T. N., 2016, ARXIV160902907
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li J., 2017, ARXIV170609884
   Liu S., 2017, ARXIV170508991
   Liu  Z., 2015, P INT C COMP VIS ICC
   Metz L., 2016, ARXIV161102163
   Nguyen T., 2017, ADV NEURAL INFORM PR, P2667
   Radford A., 2015, ARXIV151106434
   Reed S., 2016, ARXIV160505396
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Srivastava A., 2017, ARXIV170507761
   Thekumparampil K. K., 2018, ARXIV180303735
   Tolstikhin I., 2017, ARXIV170102386
   Zaheer Manzil, 2017, ADV NEURAL INFORM PR, P3391
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301048
DA 2019-06-15
ER

PT S
AU Lindenbaum, O
   Stanley, JS
   Wolf, G
   Krishnaswamy, S
AF Lindenbaum, Ofir
   Stanley, Jay S., III
   Wolf, Guy
   Krishnaswamy, Smita
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Geometry Based Data Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DENSITY ESTIMATORS; DIFFUSION
AB We propose a new type of generative model for high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, thus improves several downstream data analysis tasks, such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations, and it is able to reveal intrinsic patterns and mutual-information relationships between genes on a single-cell RNA sequencing dataset of hematopoiesis.
C1 [Lindenbaum, Ofir; Wolf, Guy] Yale Univ, Appl Math Program, New Haven, CT 06511 USA.
   [Stanley, Jay S., III] Yale Univ, Computat Biol & Bioinformat Program, New Haven, CT 06510 USA.
   [Krishnaswamy, Smita] Yale Univ, Dept Genet & Comp Sci, New Haven, CT 06510 USA.
RP Krishnaswamy, S (reprint author), Yale Univ, Dept Genet & Comp Sci, New Haven, CT 06510 USA.
EM ofir.lindenbaum@yale.edu; jay.stanley@yale.edu; guy.wolf@yale.edu;
   smita.krishnawamy@yale.edu
FU Chan-Zuckerberg Initiative [182702]
FX This research was partially funded by grant from the Chan-Zuckerberg
   Initiative (ID: 182702).
CR Alcala-Fdez J, 2009, SOFT COMPUT, V13, P307, DOI 10.1007/s00500-008-0323-y
   Beal M.J., 2003, BAYESIAN STAT, V7
   Bengio Yoshua, 2006, ADV NEURAL INFORM PR, V19, P115
   Bengio Yoshua, 2005, ADV NEURAL INFORM PR, V18, P129
   Bermanis A, 2016, APPL COMPUT HARMON A, V41, P190, DOI 10.1016/j.acha.2015.07.005
   Bermanis A, 2016, APPL COMPUT HARMON A, V40, P207, DOI 10.1016/j.acha.2015.02.001
   Brubaker Marcus, 2012, P 15 INT C ART INT S, P161
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Doersch C., 2016, ARXIV160605908
   Gine E, 2002, ANN I H POINCARE-PR, V38, P907, DOI 10.1016/S0246-0203(02)01128-7
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gorodkin J, 2004, COMPUT BIOL CHEM, V28, P367, DOI 10.1016/j.compbiolchem.2004.09.006
   Grun D, 2015, NATURE, V525, P251, DOI 10.1038/nature14966
   Gulrajani I., 2017, P ADV NEUR INF PROC, P5767
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Hensman P, 2015, THESIS
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Keller Y, 2010, IEEE T SIGNAL PROCES, V58, P403, DOI 10.1109/TSP.2009.2030861
   Kim JK, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9687
   Kingma D, 2014, INT C LEARN REPR ICL
   Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689
   Li X, 2013, PROCEEDINGS OF THE 2013 8TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE & EDUCATION (ICCSE 2013), P89
   Li YQ, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms10836
   Lindenbaum Ofir, 2017, ARXIV170701093
   Lopez V, 2013, INFORM SCIENCES, V250, P113, DOI 10.1016/j.ins.2013.07.007
   Moon K. R., 2017, BIORXIV, DOI [10.1101/120378, DOI 10.1101/120378]
   Moon Kevin R., 2017, CURRENT OPINION SYST
   Oztireli AC, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866190
   Rasmussen CE, 2000, ADV NEUR IN, V12, P554
   Scott DW, 2015, WILEY SER PROBAB ST, P125
   SCOTT DW, 1985, ANN STAT, V13, P1024, DOI 10.1214/aos/1176349654
   Seaman DE, 1996, ECOLOGY, V77, P2075, DOI 10.2307/2265701
   Seiffert C, 2010, IEEE T SYST MAN CY A, V40, P185, DOI 10.1109/TSMCA.2009.2029559
   Singer A, 2009, P NATL ACAD SCI USA, V106, P16090, DOI 10.1073/pnas.0905547106
   VARANASI MK, 1989, J ACOUST SOC AM, V86, P1404, DOI 10.1121/1.398700
   Velten L, 2017, NAT CELL BIOL, V19, P271, DOI 10.1038/ncb3493
   Vincent Pascal, 2003, ADV NEURAL INFORM PR, V16, P849
   Weiss G. M., 2004, ACM SIGKDD EXPLORATI, V6, P7, DOI DOI 10.1145/1007730.1007734
   Wu Junjie, 2012, ADV K MEANS CLUSTERI, P17, DOI DOI 10.1007/978-3-642-29807-3_2
   Zelnik-Manor L., 2005, ADV NEURAL INFORM PR, P1601
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301039
DA 2019-06-15
ER

PT S
AU Lindgren, EM
   Kocaoglu, M
   Dimakis, AG
   Vishwanath, S
AF Lindgren, Erik M.
   Kocaoglu, Murat
   Dimakis, Alexandros G.
   Vishwanath, Sriram
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Experimental Design for Cost-Aware Learning of Causal Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NETWORKS
AB We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem with a greedy algorithm. We then constrain the sparsity of each intervention. We develop an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices.
C1 [Lindgren, Erik M.; Dimakis, Alexandros G.; Vishwanath, Sriram] Univ Texas Austin, Austin, TX 78712 USA.
   [Kocaoglu, Murat] MIT IBM Watson AI Lab, Cambridge, MA USA.
RP Lindgren, EM (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM erikml@utexas.edu; murat@ibm.com; dimakis@austin.utexas.edu;
   sriram@ece.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU National Science Foundation Graduate Research Fellowship [DGE-1110007];
   NSF [CCF 1422549, 1618689, DMS 1723052, CCF 1763702]; ARO YIP
   [W911NF-14-1-0258]
FX This material is based upon work supported by the National Science
   Foundation Graduate Research Fellowship under Grant No. DGE-1110007.
   This research has been supported by NSF Grants CCF 1422549, 1618689, DMS
   1723052, CCF 1763702, ARO YIP W911NF-14-1-0258 and research gifts by
   Google, Western Digital, and NVIDIA.
CR CAI MC, 1984, DISCRETE MATH, V49, P15
   Delle Donne D, 2016, DISCRETE OPTIM, V21, P1, DOI 10.1016/j.disopt.2016.05.001
   Eberhardt Frederich, 2005, UNCERTAINTY ARTIFICI
   Eberhardt Frederick, 2007, THESIS
   Frank Andras, 1975, BRIT COMB C
   Garey M. R, 1979, COMPUTERS INTRACTABI
   Geiger Dan, 1994, UNCERTAINTY ARTIFICI
   Ghassami AmirEmad, 2018, INT C MACH LEARN
   Gurobi Optimization LLC, 2018, GUROBI OPTIMIZER REF
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   Hauser Alain, 2012, EUR WORKSH PROB GRAP
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503
   Hu Huining, 2014, NEURAL INFORM PROCES
   Hyttinen A, 2013, J MACH LEARN RES, V14, P3041
   IBARRA OH, 1975, J ACM, V22, P463, DOI 10.1145/321906.321909
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Jansen Klaus, 1997, IT C ALG COMPL
   Katona Gyula, 1966, J COMB THEORY, V1, P174
   King RD, 2004, NATURE, V427, P247, DOI 10.1038/nature02236
   Kocaoglu Murat, 2017, INT C MACH LEARN
   Krause A., 2014, SUBMODULAR FUNCTION
   Kroon Leo G, 1996, INT WORKSH GRAPH THE
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Pearl J, 2009, CAUSALITY MODELS REA
   Ramsey JD, 2010, NEUROIMAGE, V49, P1545, DOI 10.1016/j.neuroimage.2009.08.065
   Ramsey Joseph, 2017, Int J Data Sci Anal, V3, P121, DOI 10.1007/s41060-016-0032-z
   Rotmensch M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05778-z
   Rubin DB, 2006, STAT SCI, V21, P206, DOI 10.1214/088342306000000259
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Sachs Karen, 2017, INT C RES COMP MOL B
   Shanmugam Karthikeyan, 2015, NEURAL INFORM PROCES
   Spirtes P., 2001, CAUSATION PREDICTION
   Sverchkov Y, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005466
   Williamson D. P., 2011, DESIGN APPROXIMATION
   WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305031
DA 2019-06-15
ER

PT S
AU Lindsten, F
   Helske, J
   Vihola, M
AF Lindsten, Fredrik
   Helske, Jouni
   Vihola, Matti
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Graphical model inference: Sequential Monte Carlo meets deterministic
   approximations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over "plain" SMC.
C1 [Lindsten, Fredrik] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.
   [Helske, Jouni] Linkoping Univ, Dept Sci & Technol, Norrkoping, Sweden.
   [Vihola, Matti] Univ Jyvaskyla, Dept Math & Stat, Jyvaskyla, Finland.
RP Lindsten, F (reprint author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.
EM fredrik.lindsten@it.uu.se; jouni.helske@liu.se; matti.s.vihola@jyu.fi
FU Swedish Foundation for Strategic Research (SSF) via the project
   Probabilistic Modeling and Inference for Machine Learning [ICA16-0015];
   Swedish Research Council (VR) via the projects Learning of Large-Scale
   Probabilistic Dynamical Models [2016-04278]; NewLEADS - New Directions
   in Learning Dynamical Systems [621-2016-06079]; Academy of Finland
   [274740, 284513, 312605]
FX FL has received support from the Swedish Foundation for Strategic
   Research (SSF) via the project Probabilistic Modeling and Inference for
   Machine Learning (contract number: ICA16-0015) and from the Swedish
   Research Council (VR) via the projects Learning of Large-Scale
   Probabilistic Dynamical Models (contract number: 2016-04278) and
   NewLEADS - New Directions in Learning Dynamical Systems (contract
   number: 621-2016-06079). JH and MV have received support from the
   Academy of Finland (grants 274740, 284513 and 312605).
CR Andrieu C, 2015, ANN APPL PROBAB, V25, P1030, DOI 10.1214/14-AAP1022
   Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574
   Bivand R, 2015, J STAT SOFTW, V63, P1
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Buntine W., 2009, P 1 AS C MACH LEARN
   Carbonetto Peter, 2006, ADV NEURAL INFORM PR, V19, P201
   Cuthill E., 1969, P 1969 24 NAT C
   de Freitas N., 2001, UNCERTAINTY ARTIFICI, P120
   Del Moral P, 2004, PROB APPL S
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Doucet A., 2011, OXFORD HDB NONLINEAR, V12, P656
   Durbin J, 1997, BIOMETRIKA, V84, P669, DOI 10.1093/biomet/84.3.669
   Ghahramani Z, 1999, ADV NEUR IN, V11, P431
   Guarniero P, 2017, J AM STAT ASSOC, V112, P1636, DOI 10.1080/01621459.2016.1222291
   Hamze F., 2005, ADV NEURAL INF PROC, V18, P491
   Heng J., 2018, ARXIV170808396
   Jacob PE, 2015, STAT COMPUT, V25, P487, DOI 10.1007/s11222-013-9445-x
   Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026
   KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Minka T., 2002, P 18 C UNC ART INT U
   Minka T. P., 2001, P 17 C UNC ART INT U
   Naesseth C. A., 2015, NIPS WORKSH BLACK BO
   Naesseth C. A., 2014, ADV NEURAL INFORM PR, P1862
   Pearl J, 1988, PROBABILISTIC REASON
   Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179
   Robert C. P., 2004, MONTE CARLO STAT MET
   Rue H., 2005, MONOGRAPHS STAT APPL
   Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x
   Ruiz HC, 2017, IEEE T SIGNAL PROCES, V65, P3191, DOI 10.1109/TSP.2017.2686340
   Scott G. S., 2009, P 16 INT C ART INT S
   Shephard N, 1997, BIOMETRIKA, V84, P653, DOI 10.1093/biomet/84.3.653
   Vihola M., 2018, ARXIV160902541
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
   Wallach H. M, 2009, P 26 INT C MACH LEAR
   Whiteley N, 2016, BERNOULLI, V22, P494, DOI 10.3150/14-BEJ666
NR 38
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002071
DA 2019-06-15
ER

PT S
AU Linsley, D
   Kim, J
   Veerabadran, V
   Windolf, C
   Serre, T
AF Linsley, Drew
   Kim, Junkyung
   Veerabadran, Vijay
   Windolf, Charlie
   Serre, Thomas
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning long-range spatial dependencies with horizontal gated recurrent
   units
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SURROUND SUPPRESSION; CONTOUR SALIENCY; IMAGE ELEMENTS; BOUNDARIES;
   COMPUTATIONS; INTEGRATION; CONNECTIONS; NETWORKS
AB Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching - and sometimes even surpassing - human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathfinder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections - both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.
C1 [Linsley, Drew; Kim, Junkyung; Veerabadran, Vijay; Windolf, Charlie; Serre, Thomas] Brown Univ, Dept Cognit Linguist & Psychol Sci, Carney Inst Brain Sci, Providence, RI 02912 USA.
RP Linsley, D (reprint author), Brown Univ, Dept Cognit Linguist & Psychol Sci, Carney Inst Brain Sci, Providence, RI 02912 USA.
EM drew_linsley@brown.edu; junkyung_kim@brown.edu;
   vijay_veerabadran@brown.edu; thomas_serre@brown.edu
FU NSF early career award [IIS-1252951]; DARPA young faculty award [YFA
   N66001-14-1-4037]; Carney Institute for Brain Science; Center for
   Computation and Visualization (CCV) at Brown University
FX This research was supported by NSF early career award [grant number
   IIS-1252951] and DARPA young faculty award [grant number YFA
   N66001-14-1-4037]. Additional support was provided by the Carney
   Institute for Brain Science and the Center for Computation and
   Visualization (CCV) at Brown University.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Ben-Shahar O, 2004, NEURAL COMPUT, V16, P445, DOI 10.1162/089976604772744866
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Cooijmans T., 2017, P INT C LEARN REPR
   Ellis K., 2015, ADV NEURAL INFORM PR, P973
   FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q
   George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Graves A., 2009, ADV NEURAL INFORM PR, P545
   Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549
   GROSSBERG S, 1985, PERCEPT PSYCHOPHYS, V38, P141, DOI 10.3758/BF03198851
   Hamaguchi R., 2017, CORR
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Houtkamp R, 2010, J EXP PSYCHOL HUMAN, V36, P1443, DOI 10.1037/a0020248
   Ioffe S., 2015, ARXIV150203167
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2016.181
   Kim J, 2018, INTERFACE FOCUS, V8, DOI 10.1098/rsfs.2018.0011
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kokkinos I., 2016, P INT C LEARN REPR
   Lee C. Y., 2015, ARTIF INTELL, P562
   LESHER GW, 1993, VISION RES, V33, P2253, DOI 10.1016/0042-6989(93)90104-5
   Li W, 2002, J NEUROPHYSIOL, V88, P2846, DOI 10.1152/jn.00289.2002
   Li W, 2008, NEURON, V57, P442, DOI 10.1016/j.neuron.2007.12.011
   Li W, 2006, NEURON, V50, P951, DOI 10.1016/j.neuron.2006.05.035
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao Qianli, 2016, ARXIV160403640
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lotter W., 2017, P INT C LEARN REPR
   Maninis K., 2017, IEEE T PATTERN ANAL
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mely DA, 2018, PSYCHOL REV, V125, P769, DOI 10.1037/rev0000109
   Nayebi A., 2018, ARXIV180700053
   Papandreou G, 2015, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2015.7298636
   Peirce JW, 2007, J NEUROSCI METH, V162, P8, DOI 10.1016/j.jneumeth.2006.11.017
   ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307
   Roelfsema PR, 2011, ATTEN PERCEPT PSYCHO, V73, P2542, DOI 10.3758/s13414-011-0200-0
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026
   Series P, 2003, J PHYSIOL-PARIS, V97, P453, DOI 10.1016/j.jphysparis.2004.01.023
   Shushruth S, 2013, J NEUROSCI, V33, P106, DOI 10.1523/JNEUROSCI.2518-12.2013
   Shushruth S, 2012, J NEUROSCI, V32, P308, DOI 10.1523/JNEUROSCI.3789-11.2012
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551
   Srivastava R. K., 2015, ARXIV150500387
   Stettler DD, 2002, NEURON, V36, P739, DOI 10.1016/S0896-6273(02)01029-2
   Szegedy C., 2014, P INT C LEARN REPR
   Tallec C., 2018, ARXIV180411188
   Tanaka H, 2009, J NEUROPHYSIOL, V101, P1444, DOI 10.1152/jn.90749.2008
   Theis L., 2015, ADV NEURAL INFORM PR, V2, P1927
   van den Oord A., 2016, P 33 INT C MACH LEAR, P1747
   Volokitin A., 2017, ADV NEURAL INFORM PR, V30, P5628
   Wang TY, 2017, PROC INT C TOOLS ART, P1272, DOI 10.1109/ICTAI.2017.00192
   Wang X., 2018, P IEEE C COMP VIS PA
   Wang Y., 2017, P CVPR, P3892
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Yu F., 2016, P INT C LEARN REPR
   Zamir A. R., 2016, ARXIV161209508
   Zhaoping L, 2011, CURR OPIN NEUROBIOL, V21, P808, DOI 10.1016/j.conb.2011.07.005
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zucker SW, 2014, P IEEE, V102, P812, DOI 10.1109/JPROC.2014.2314723
NR 70
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300015
DA 2019-06-15
ER

PT S
AU Linzner, D
   Koeppl, H
AF Linzner, Dominik
   Koeppl, Heinz
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Cluster Variational Approximations for Structure Learning of
   Continuous-Time Bayesian Networks from Incomplete Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GENE-EXPRESSION; INFERENCE
AB Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However, if the available data is incomplete, one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques, such as sampling and low-order variational methods, either scale unfavorably in system size, or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics, we present a new approximation scheme based on cluster variational methods that significantly improves upon existing variational approximations. We can analytically marginalize the parameters of the approximate CTBN, as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.
C1 [Linzner, Dominik; Koeppl, Heinz] Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany.
   [Koeppl, Heinz] Tech Univ Darmstadt, Dept Biol, Darmstadt, Germany.
RP Linzner, D (reprint author), Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany.
EM dominik.linzner@bcs.tu-darmstadt.de; heinz.koeppl@bcs.tu-darmstadt.de
FU European Union's Horizon 2020 research and innovation programme [668858]
FX We thank the anonymous reviewers for helpful comments on the previous
   version of this manuscript. Dominik Linzner is funded by the European
   Union's Horizon 2020 research and innovation programme under grant
   agreement 668858.
CR Acerbi E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/s12859-014-0387-x
   Bansal M, 2006, BIOINFORMATICS, V22, P815, DOI 10.1093/bioinformatics/btl003
   Bansal M, 2007, MOL SYST BIOL, V3, DOI 10.1038/msb4100120
   Cantone I, 2009, CELL, V137, P172, DOI 10.1016/j.cell.2009.01.055
   Cohn I, 2010, J MACH LEARN RES, V11, P2745
   Vazquez ED, 2017, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/aa5d22
   El-Hay Tal, 2010, P 27 INT C MACH LEAR, P343
   El-Hay Tal, 2011, P 22 C UNC ART INT
   Fan Yu, 2008, AI AND MATH
   Friedman Nir, 1999, P 16 INT JOINT C ART
   GLAUBER RJ, 1963, J MATH PHYS, V4, P294, DOI 10.1063/1.1703954
   KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988
   Klann M, 2012, INT J MOL SCI, V13, P7798, DOI 10.3390/ijms13067798
   Nodelman U, 2003, P 19 INT C UNC ART I, P451
   Nodelman Uri, 2005, P 21 INT C UNC ART I, P421
   Nodelman Uri, 1995, P 18 C UNC ART INT, P378
   Opper M., 2008, ADV NEURAL INFORM PR, P1105
   Pelizzola Alessandro, 2017, J STAT MECH-THEORY E, V2017, P1
   Penfold CA, 2011, INTERFACE FOCUS, V1, P857, DOI 10.1098/rsfs.2011.0053
   Rao Vinayak, 2012, J MACHINE LEARNING R, V14, P3295
   Schadt EE, 2005, NAT GENET, V37, P710, DOI 10.1038/ng1589
   Studer Lukas, 2016, P AAAI C ART INT PHO, P2051
   Yedidia Jonathan S, 2000, ADV NEURAL INFORM, V13, P657
   Yu J, 2004, BIOINFORMATICS, V20, P3594, DOI 10.1093/bioinformatics/bth448
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002043
DA 2019-06-15
ER

PT S
AU Lipton, ZC
   Chouldechova, A
   McAuley, J
AF Lipton, Zachary C.
   Chouldechova, Alexandra
   McAuley, Julian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Does mitigating ML's impact disparity require treatment disparity?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.
C1 [Lipton, Zachary C.; Chouldechova, Alexandra] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [McAuley, Julian] Univ Calif San Diego, San Diego, CA 92103 USA.
RP Lipton, ZC (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM zlipton@cmu.edu; achould@cmu.edu; jmcauley@cs.ucsd.edu
CR Adler Philip, 2016, ARXIV160207043
   Bechavod Yahav, 2017, ARXIV170700044
   Berk R., 2017, ARXIV170309207
   Chouldechova Alexandra, 2017, BIG DATA
   Corbett-Davies Sam, 2017, ARXIV170108230
   Datta Anupam, 2017, ARXIV170708120
   Dwork Cynthia, 2012, INNOVATIONS THEORETI
   Dwork Cynthia, 2017, ARXIV170706613
   Feldman Michael, 2015, KDD
   Hardt M., 2016, NIPS
   Hartocollis Anemona, 2017, AFFIRMATIVE ACTION B
   Johndrow James E, 2017, ARXIV170304957
   Joseph Matthew, 2016, ARXIV161009559
   Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8
   Kamiran Faisal, 2009, COMPUTER CONTROL COM
   Kamiran Faisal, 2010, ICDM
   Kamishima Toshihiro, 2011, ICDM WORKSH
   Kilbertus Niki, 2017, ARXIV170602744
   Kim Pauline, 2017, AUDITING ALGORITHMS
   Kim Pauline T, 2017, WILLIAM MARY LAW REV, V58
   Kleinberg Jon, 2016, ARXIV160905807
   Kohavi Ron, 1996, KDD
   Kusner Matt J, 2017, ARXIV170306856
   Menon Aditya, 2018, FAIRNESS ACCOUNTABIL
   Moro S., 2014, DECISION SUPPORT SYS
   Nabi Razieh, 2017, ARXIV170510378
   Pedreshi Dino, 2008, KDD
   Ritov Yaacov, 2017, ARXIV170608519
   Yeh I. C., 2009, EXPERT SYSTEMS APPL
   Zafar Muhammad Bilal, 2017, ARXIV170700010
   Zafar Muhammad Bilal, 2017, AISTATS
   Zemel R., 2013, ICML
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002065
DA 2019-06-15
ER

PT S
AU Littwin, E
   Wolf, L
AF Littwin, Etai
   Wolf, Lior
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Regularizing by the Variance of the Activations' Sample-Variances
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks.
C1 [Littwin, Etai; Wolf, Lior] Tel Aviv Univ, Tel Aviv, Israel.
   [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel.
RP Littwin, E (reprint author), Tel Aviv Univ, Tel Aviv, Israel.
FU European Research Council (ERC) under the European Union [ERC CoG
   725974]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant ERC CoG 725974).
CR Ba J. L., 2016, ARXIV160706450
   Clevert D.A., 2015, ARXIV151107289
   Courbariaux M, 2016, ARXIV160202830
   Estep  Donald, 2006, ERROR ESTIMATION ADA
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang G., 2017, ARXIV170400109
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Ioffe Sergey, 2017, ADV NEURAL INFORM PR, P1942
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kingma D.P., 2013, ARXIV13126114
   Klambauer G., 2017, ADV NEURAL INFORM PR, P972
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lyu Siwei, 2008, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2008, P1
   Rebuffi Sylvestre-Alvise, 2017, ADV NEURAL INFORM PR, P506
   Rolfe J. T., 2016, ARXIV160902200
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Scardapane S, 2017, NEUROCOMPUTING, V241, P81, DOI 10.1016/j.neucom.2017.02.029
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Ulyanov D., 2016, ARXIV160708022
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Wu Y.X., 2018, ARXIV180308494
   Yoon  J., 2017, INT C MACH LEARN SYD, P3958
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302015
DA 2019-06-15
ER

PT S
AU Liu, AH
   Liu, YC
   Yeh, YY
   Wang, YCF
AF Liu, Alexander H.
   Liu, Yen-Cheng
   Yeh, Yu-Ying
   Wang, Yu-Chiang Frank
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Unified Feature Disentangler for Multi-Domain Image Translation and
   Manipulation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.
C1 [Liu, Alexander H.; Wang, Yu-Chiang Frank] Natl Taiwan Univ, Taipei, Taiwan.
   [Liu, Yen-Cheng] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Yeh, Yu-Ying] Univ Calif San Diego, La Jolla, CA 92093 USA.
   [Wang, Yu-Chiang Frank] MOST Joint Res Ctr AI Technol & All Vista Healthc, Taipei, Taiwan.
RP Liu, AH (reprint author), Natl Taiwan Univ, Taipei, Taiwan.
EM b03902034@ntu.edu.tw; ycliu@gatech.edu; yuyeh@eng.ucsd.edu;
   ycwang@ntu.edu.tw
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bousmalis  K., 2016, ADV NEURAL INFORM PR, P343
   Chen X., 2016, ADV NEURAL INFORM PR
   Choi  Y., 2018, P IEEE C COMP VIS PA
   Ganin  Y., 2015, P INT C MACH LEARN I
   Ghifary  M., 2016, P EUR C COMP VIS ECC
   Goodfellow  I., 2014, ADV NEURAL INFORM PR
   Higgins  I., 2017, P INT C LEARN REPR I
   Isola P., 2017, P IEEE C COMP VIS PA
   Kim  T., 2017, P INT C MACH LEARN I
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P, 2014, ADV NEURAL INFORM PR
   Kulkarni  T.D., 2015, ADV NEURAL INFORM PR
   Lample G., 2017, ADV NEURAL INFORM PR
   Liu M.-Y., 2016, ADV NEURAL INFORM PR
   LIU MY, 2017, ADV NEURAL INFORM PR
   Odena  A., 2017, P INT C MACH LEARN I
   Radford A., 2015, ARXIV151106434
   Rezende D. J., 2014, P INT C MACH LEARN I
   Sankaranarayanan  S., 2018, P IEEE C COMP VIS PA
   Taigman  Y., 2017, P INT C LEARN REPR I
   Tran Luan, 2017, P IEEE C COMP VIS PA
   Tzeng  E., 2015, P IEEE INT C COMP VI
   Tzeng  E., 2017, P IEEE C COMP VIS PA
   Xie XF, 2015, IEEE INT VAC ELECT C
   Yi Z, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND DIGITAL IMAGE PROCESSING (CGDIP 2017), DOI 10.1145/3110224.3110230
   Yuan HY, 2013, IEEE INT CON MULTI
   Zhao  S., 2017, P INT C MACH LEARN I
   Zhou WR, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277978
   Zhu J.Y., 2017, P IEEE INT C COMP VI
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302059
DA 2019-06-15
ER

PT S
AU Liu, D
   Wen, BH
   Fan, YC
   Loy, CC
   Huang, TS
AF Liu, Ding
   Wen, Bihan
   Fan, Yuchen
   Loy, Chen Change
   Huang, Thomas S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Non-Local Recurrent Network for Image Restoration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice [41] that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters. The code is available at https://github.com/Ding-Liu/NLRN.
C1 [Liu, Ding; Wen, Bihan; Fan, Yuchen; Huang, Thomas S.] Univ Illinois, Champaign, IL 61820 USA.
   [Loy, Chen Change] Nanyang Technol Univ, Singapore, Singapore.
RP Liu, D (reprint author), Univ Illinois, Champaign, IL 61820 USA.
EM dingliu2@illinois.edu; bwen3@illinois.edu; yuchenf4@illinois.edu;
   ccloy@ntu.edu.sg; t-huang1@illinois.edu
CR BEVILACQUA M., 2012, LOW COMPLEXITY SINGL
   Buades  A., 2005, CVPR
   Burger H. C., 2012, CVPR
   Chandra S., 2017, ICCV
   Chang H., 2004, CVPR
   Chen F., 2015, ICCV
   Chen Y., 2017, IEEE TPAMI
   Dabov K., 2007, IEEE TIP
   Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954
   Dong C., 2014, ECCV
   Dong Chao, 2016, ECCV
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Glasner  D., 2009, ICCV
   Grangier D., 2017, ICML
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Han W., 2018, CVPR
   Harley A. W., 2017, ICCV
   He K., 2016, ECCV
   Huang J. - B., 2015, CVPR
   Kim  J., 2016, CVPR
   Lai W. - S., 2017, CVPR
   Lefkimmiatis S., 2017, CVPR
   Lim B., 2017, CVPR WORKSH
   Liu D., 2018, IJCAI
   Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643
   Mairal J., 2009, ICCV
   Mao X., 2016, NIPS
   Martin  D., 2001, ICCV
   Qiao P., 2017, ACM MULT C
   Rudin L. I., 1994, ICIP
   Santoro A, 2017, NIPS
   Schulter S., 2015, CVPR
   Shi Wenzhe, 2016, CVPR
   Tai Y., 2017, CVPR
   Tai Y., 2017, ICCV
   Timofte R., 2013, ICCV
   Tomasi C., 1998, ICCV
   Tong T., 2017, ICCV
   Vaswani A., 2017, NIPS
   Wang X., 2017, ARXIV171107971
   Wang Z., 2004, IEEE TIP
   Wang Z., 2015, ICCV
   Wen B., 2015, IJCV
   Xu J., 2015, ICCV
   Yang J., 2010, IEEE TIP
   Yin RJ, 2017, SIAM J IMAGING SCI, V10, P711, DOI 10.1137/16M1091447
   Zeyde R., 2010, INT C CURV SURF
   Zhang K., 2017, CVPR
   Zhang K., 2017, IEEE TIP
   Zheng S., 2015, ICCV
   Zoran D., 2011, ICCV
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301064
DA 2019-06-15
ER

PT S
AU Liu, H
   Jin, S
   Zhang, CS
AF Liu, Hu
   Jin, Sheng
   Zhang, Changshui
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Connectionist Temporal Classification with Maximum Entropy
   Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Connectionist Temporal Classification (CTC) is an objective function for end-to-end sequence learning, which adopts dynamic programming algorithms to directly learn the mapping between sequences. CTC has shown promising results in many sequence learning applications including speech recognition and scene text recognition. However, CTC tends to produce highly peaky and overconfident distributions, which is a symptom of overfitting. To remedy this, we propose a regularization method based on maximum conditional entropy which penalizes peaky distributions and encourages exploration. We also introduce an entropy-based pruning method to dramatically reduce the number of CTC feasible paths by ruling out unreasonable alignments. Experiments on scene text recognition show that our proposed methods consistently improve over the CTC baseline without the need to adjust training settings. Code has been made publicly available at: https://github.com/liuhu-bigeye/enctc.crnn.
C1 [Liu, Hu; Jin, Sheng; Zhang, Changshui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,Dept Automat,Tsinghu, Beijing, Peoples R China.
RP Liu, H (reprint author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,Dept Automat,Tsinghu, Beijing, Peoples R China.
EM liuhu15@mails.tsinghua.edu.cn; js17@mails.tsinghua.edu.cn;
   zcs@mail.tsinghua.edu.cn
FU NSFC [61876095, 61751308, 61473167]; Beijing Natural Science Foundation
   [L172037]
FX This work is supported by NSFC (Grant No. 61876095, No. 61751308 and No.
   61473167) and Beijing Natural Science Foundation (Grant No. L172037).
CR Amodei D., 2016, INT C MACH LEARN, P173
   Battenberg E, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P206, DOI 10.1109/ASRU.2017.8268937
   Chorowski J., 2016, ARXIV161202695
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Hannun A., 2014, ARXIV14125567
   Hinton G. E, 2012, ARXIV12070580
   Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Jaderberg  Max, 2014, ARXIV14062227
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kim S., 2017, ARXIV171102212
   Krogh A., 1992, NEURAL INFORM PROCES, P950
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Lin  Mengxi, 2017, P THEM WORKSH ACM C, P393
   Liu WB, 2016, J RESIDUALS SCI TECH, V13, P1
   Lucas S. M., 2005, International Journal on Document Analysis and Recognition, V7, P105, DOI 10.1007/s10032-004-0134-3
   Maron O, 1998, ADV NEUR IN, V10, P570
   Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790
   Miller D, 1996, IEEE T SIGNAL PROCES, V44, P3108, DOI 10.1109/78.553484
   Mishra  Anand, 2012, BRIT MACH VIS C BMVC
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pereyra G., 2017, ARXIV170106548
   Senior A, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P604, DOI 10.1109/ASRU.2015.7404851
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Variani E, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4959, DOI 10.1109/ICASSP.2018.8461929
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Williams R. J., 1991, Connection Science, V3, P241, DOI 10.1080/09540099108946587
   Zeiler MD, 2013, ARXIV201313013557
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300077
DA 2019-06-15
ER

PT S
AU Liu, MM
   Cheng, G
AF Liu, Meimei
   Cheng, Guang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Early Stopping for Nonparametric Testing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REGRESSION; INFERENCE
AB Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a "sharp" stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes.
C1 [Liu, Meimei] Duke Univ, Dept Stat Sci, Durham, NC 27705 USA.
   [Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.
RP Liu, MM (reprint author), Duke Univ, Dept Stat Sci, Durham, NC 27705 USA.
EM meimei.liu@duke.edu; chengg@purdue.edu
CR Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Fan  Jianqing, 2007, TEST, V16, P409
   Fan JQ, 2001, ANN STAT, V29, P153, DOI 10.1214/aos/996986505
   GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751
   Guo WS, 2002, J ROY STAT SOC B, V64, P887, DOI 10.1111/1467-9868.00367
   Ingster Yu.I., 1993, MATH METHODS STAT, V2, P85
   Liu  Meimei, 2018, ARXIV180206308
   Lu J., 2016, ARXIV160106212
   Ma  Siyuan, 2017, ADV NEURAL INFORM PR, P3781
   Raskutti G, 2014, J MACH LEARN RES, V15, P335
   Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865
   Scholkopf B, 1999, ADV KERNEL METHODS S
   Shang ZF, 2013, ANN STAT, V41, P2608, DOI 10.1214/13-AOS1164
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Stewart GW, 2002, SIAM J MATRIX ANAL A, V23, P601, DOI 10.1137/S0895479800371529
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wei  Yuting, 2017, ARXIV171200711
   Wei Yuting, 2017, ADV NEURAL INFORM PR, P6067
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304003
DA 2019-06-15
ER

PT S
AU Liu, MR
   Zhang, XX
   Zhang, LJ
   Jin, R
   Yang, TB
AF Liu, Mingrui
   Zhang, Xiaoxuan
   Zhang, Lijun
   Jin, Rong
   Yang, Tianbao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound
   Conditions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have recently received increasing attention for developing optimization algorithms with fast convergence. However, the studies of EBC in statistical learning are hitherto still limited. The main contributions of this paper are two-fold. First, we develop fast and intermediate rates of empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and smooth convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization with Lipschitz continuous random functions, which requires only one pass of n samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between (O) over tilde (1/root n ) and (O) over tilde (1/n) depending on the power constant in EBC, and could be even faster than O (1/n) in special cases for ERM. Moreover, these convergence rates are automatically adaptive without using any knowledge of EBC.
C1 [Liu, Mingrui; Zhang, Xiaoxuan; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Zhang, Lijun] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Jin, Rong] Alibaba Grp, Machine Intelligence Technol, Bellevue, WA 98004 USA.
RP Liu, MR (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM mingrui-liu@uiowa.edu; zljzju@gmail.com; tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1545995, 2017QNRC001]
FX The authors thank the anonymous reviewers for their helpful comments. M.
   Liu and T. Yang are partially supported by National Science Foundation
   (IIS-1545995). L. Zhang is partially supported by YESS (2017QNRC001). We
   thank Nishant A. Mehta for pointing out the work [12] for the proof of
   Theorem 1.
CR Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Bartlett Peter L., 2006, PROBABILITY THEORY R
   Bartlett Peter L., 2005, ANN STAT
   Bolte Jerome, 2015, CORR
   BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063
   Drusvyatskiy D., 2016, ARXIV160206661
   Duchi J. C., 2010, P 23 ANN C LEARN THE, P14
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Feldman Vitaly, 2016, NIPS
   Garber Dan, 2016, ICML
   Gonen Alon, 2017, J MACH LEARN RES, V18, P8245
   Grunwald Peter D., 2016, CORR
   Hazan Elad, 2011, COLT
   Hazan Elad, 2007, MACHINE LEARNING
   Juditsky Anatoli, 2014, STOCH SYST
   Kakade Sham M., 2008, NIPS
   Karimi Hamed, 2016, ECML PKDD
   Kim Sujin, 2015, GUIDE SAMPLE AVERAGE, P207
   Koltchinskii Vladimir, 2006, ANN STAT
   Koolen Wouter M., 2016, NIPS
   Koren Tomer, 2015, NIPS
   Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577
   Li Guoyin, 2013, MATH PROGRAM
   Li Guoyin, 2016, CORR
   Mammen E, 1999, ANN STAT, V27, P1808
   Mehta Nishant A., 2017, AISTATS
   Mehta Nishant A., 2014, NIPS
   Necoara I., 2015, CORR
   Nemirovski Arkadi, 2009, SIAM J OPTIMIZATION
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Pang Jong-Shi, 1997, MATH PROGRAM
   Rakhlin Alexander, 2012, ICML
   Ramdas Aaditya, 2013, ICML
   Rockafellar R T, 1970, CONVEX ANAL
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Shalev-Shwartz Shai, 2009, COLT
   Shalev-Shwartz Shai, 2007, ICML
   Shamir Ohad, 2013, ICML
   Shapiro  A., 2014, LECT STOCHASTIC PROG
   Smale S., 2007, CONSTRUCTIVE APPROXI
   Srebro Nathan, 2010, NIPS
   Srebro Nathan, 2010, ARXIV E PRINTS
   Sridharan Karthik, 2008, NIPS
   Tsybakov AB, 2004, ANN STAT, V32, P135
   van Erven Tim, 2016, NIPS
   van Erven Tim, 2015, JMLR
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Xu Yi, 2017, P 34 INT C MACH LEAR, P3821
   Xu Yi, 2016, CORR
   Yang Tianbao, 2018, AISTATS, P445
   Yang Tianbao, 2016, CORR
   Yang W. H., 2009, SIAM J OPTIMIZATION
   Zhang Hui, 2016, CORR
   Zhang Lijun, 2017, CORR
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304067
DA 2019-06-15
ER

PT S
AU Liu, MR
   Li, Z
   Wang, XY
   Yi, JF
   Yang, TB
AF Liu, Mingrui
   Li, Zhe
   Wang, Xiaoyu
   Yi, Jinfeng
   Yang, Tianbao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adaptive Negative Curvature Descent with Applications in Non-convex
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g., epsilon(2) << 1) in order to achieve a sufficiently accurate second-order stationary solution (i.e., lambda(min)(del(2)f(x)) >= -epsilon(2)). One issue with this approach is that the target precision epsilon(2) is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity.
C1 [Liu, Mingrui; Li, Zhe; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Wang, Xiaoyu] Intellifusion, Parlin, NJ USA.
   [Yi, Jinfeng] JD AI Res, Stanford, CA USA.
RP Liu, MR (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1545995]
FX We thank the anonymous reviewers for their helpful comments. M. Liu, T.
   Yang are partially supported by National Science Foundation
   (IIS-1545995).
CR Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464
   Carmon Yair, 2016, CORR
   Cartis C, 2011, MATH PROGRAM, V130, P295, DOI 10.1007/s10107-009-0337-y
   Cartis C, 2011, MATH PROGRAM, V127, P245, DOI 10.1007/s10107-009-0286-5
   Curtis Frank E., 2017, CORR
   Fan R. E., 2011, LIBSVM DATA CLASSIFI
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Reddi SJ, 2016, IEEE DECIS CONTR P, P1971, DOI 10.1109/CDC.2016.7798553
   Reddi Sashank J., 2017, CORR
   Royer Clement W., 2017, CORR
   Xu Peng, 2017, CORR
   Xu Yi, 2017, CORR
   Zeyuan Allen-Zhu, 2017, CORR
   Zhang  Y., 2017, COLT, P1980
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304083
DA 2019-06-15
ER

PT S
AU Liu, MR
   Zhang, XX
   Zhou, X
   Yang, TB
AF Liu, Mingrui
   Zhang, Xiaoxuan
   Zhou, Xun
   Yang, Tianbao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Faster Online Learning of Optimal Threshold for Consistent F-measure
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GRADIENT; BOUNDS
AB In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack statistical consistency guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is a novel stochastic algorithm with low memory and computational costs, which can enjoy a convergence rate of (O) over tilde (1/root n) for learning the optimal threshold under a mild condition on the convergence of the posterior probability, where n is the number of processed examples. It is provably faster than its predecessor based on a heuristic for updating the threshold. The experiments verify the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms.
C1 [Liu, Mingrui; Zhang, Xiaoxuan; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Zhou, Xun] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA.
RP Liu, MR (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1545995]
FX The authors thank the anonymous reviewers for their helpful comments. M.
   Liu, X. Zhang and T. Yang are partially supported by National Science
   Foundation (IIS-1545995).
CR Agarwal Alekh, 2012, NIPS, V25, P1547
   Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Busa-Fekete Robert, 2015, P NEUR INF PROC SYST, P595
   CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Juditski A., 2014, STOCH SYST, V4, P44, DOI DOI 10.1287/10-SSY010
   Kar P, 2014, ADV NEURAL INFORM PR, P694
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Liu Mingrui, 2018, ADV NEURAL INFORM PR
   Liu Mingrui, 2018, P INT C MACH LEARN, P3195
   Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
   Narasimhan Harikrishna, 2015, JMLR P, P199
   Natarajan Nagarajan, 2014, NEURAL INFORM PROCES
   Parambath S. Puthiya, 2014, ADV NEURAL INFORM PR, V27, P2123
   Rakhlin Alexander, 2012, P INT C MACH LEARN I
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699
   Tang Yuchun, 2006, COLLABORATIVE COMPUT, P1
   van Erven Tim, 2015, J MACHINE LEARNING R
   Xu Yi, 2017, P 34 INT C MACH LEAR, P3821
   Yan Yan, 2017, P 31 AAAI C ART INT, P2817
   Ye Nan, 2012, P 29 INT C MACH LEAR
   Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704
   Zhao MJ, 2013, J MACH LEARN RES, V14, P1033
   Zhao Peilin, 2013, P 19 ACM SIGKDD INT, P919
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303085
DA 2019-06-15
ER

PT S
AU Liu, Q
   Allamanis, M
   Brockschmidt, M
   Gaunt, AL
AF Liu, Qi
   Allamanis, Miltiadis
   Brockschmidt, Marc
   Gaunt, Alexander L.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Constrained Graph Variational Autoencoders for Molecule Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.
C1 [Liu, Qi] Singapore Univ Technol & Design, Singapore, Singapore.
   [Allamanis, Miltiadis; Brockschmidt, Marc; Gaunt, Alexander L.] Microsoft Res, Cambridge, England.
RP Liu, Q (reprint author), Singapore Univ Technol & Design, Singapore, Singapore.
EM qiliu@u.nus.ed; miallama@microsoft.com; mabrocks@microsoft.com;
   algaunt@microsoft.com
CR Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Allamanis M., 2018, ICLR
   Bredt J, 1902, BER DTSCH CHEM GES, V35, P1286, DOI 10.1002/cber.19020350215
   Defferrard M., 2016, NIPS
   Erdos P, 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.1234/12345678
   Gilmer J., 2017, ARXIV170401212
   Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572
   Gomez-Bombarelli R, 2016, NAT MATER, V15, P1120, DOI [10.1038/NMAT4717, 10.1038/nmat4717]
   Gori M., 2005, IJCNN
   Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s
   Irwin JJ, 2012, J CHEM INF MODEL, V52, P1757, DOI 10.1021/ci3001277
   Jin W., 2018, P 36 INT C MACH LEAR
   Johnson D. D., 2017, ICLR
   Kingma D. P., 2013, ARXIV13126114
   Kipf T., 2018, ICML
   Kipf T. N., 2017, ICLR
   Kusner M. J., 2017, CORR
   Landrum G., 2014, RDKIT OPEN SOURCE CH
   Leskovec J, 2010, J MACH LEARN RES, V11, P985
   Li Y., 2018, CORR
   Li Y., 2016, ICLR
   Neil D., 2018, ICLR WORKSH
   Olivecrona M, 2017, J CHEMINFORMATICS, V9, DOI 10.1186/s13321-017-0235-x
   Qi X., 2017, P IEEE C COMP VIS PA, P5199
   Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22
   Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d
   Samanta B., 2018, CORR
   Segler M. H., 2017, ACS CENTRAL SCI
   Simonovsky M., 2018, ICLR WORKSH TRACK
   Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004
   Vinyals O., 2016, ICLR
   WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005
   Yeung S., 2017, ARXIV170603643
   You J., 2018, ARXIV180208773
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002035
DA 2019-06-15
ER

PT S
AU Liu, Q
   Wang, DL
AF Liu, Qiang
   Wang, Dilin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Stein Variational Gradient Descent as Moment Matching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving the Stein equation, which may motivate more efficient algorithms.
C1 [Liu, Qiang; Wang, Dilin] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Liu, Q (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM lqiang@cs.utexas.edu; dilin@cs.utexas.edu
CR Anderes Ethan, 2002, ARXIV12055314
   Barbour AD, 2005, LECT NOTES SER INST, V4, pIX
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Chen Liqun, 2018, ARXIV180511659
   Feng Yihao, 2017, C UNC ART INT UAI
   Gorham Jackson, 2017, INT C MACH LEARN ICM
   Gretton Arthur, 2016, INT C MACH LEARN ICM
   Haarnoja T., 2017, INT C MACH LEARN, P1352
   Kim Taesup, 2018, ADV NEURAL INFORM PR
   Koller D., 2009, PROBABILISTIC GRAPHI
   Liu Qiang, 2016, INT C MACH LEARN, P276
   Liu Qiang, 2016, ADV NEURAL INFORM PR, V29, P2378
   Liu Yang, 2017, C UNC ART INT UAI
   Lu Jianfeng, 2018, ARXIV180504035
   Oates Chris J, 2017, J ROYAL STAT SOC B
   Ollivier Yann, 2014, OPTIMAL TRANSPORT TH, V413
   Pu Yuchen, 2017, ADV NEURAL INFORM PR, P4239
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
   Stein C., 1972, P 6 BERK S MATH STAT, V2, P583
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang D., 2016, ARXIV161101722
   Zhu YH, 2018, J COMPUT PHYS, V366, P415, DOI 10.1016/j.jcp.2018.04.018
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003041
DA 2019-06-15
ER

PT S
AU Liu, Q
   Li, LH
   Tang, ZY
   Zhou, DY
AF Liu, Qiang
   Li, Lihong
   Tang, Ziyang
   Zhou, Dengyong
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider off-policy estimation of the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique for deriving (nearly) unbiased estimators, but is known to suffer from an excessively high variance in long-horizon problems. In the extreme case of infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimator that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance faced by existing methods. Our key contribution is a novel approach to estimating the density ratio of two stationary state distributions, with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical and empirical analyses.
C1 [Liu, Qiang; Tang, Ziyang] Univ Texas Austin, Austin, TX 78712 USA.
   [Li, Lihong; Zhou, Dengyong] Google Brain, Kirkland, WA 98033 USA.
RP Liu, Q (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM lqiang@cs.utexas.edu; lihong@google.com; ztang@cs.utexas.edu;
   dennyzhou@google.com
FU NSF [CRII 1830161]; Google Cloud
FX This work is supported in part by NSF CRII 1830161. We would like to
   acknowledge Google Cloud for their support.
CR Asmussen  Soren, 2007, PROBABILITY THEORY S
   BELLMAN R, 1957, DYNAMIC PROGRAMMING
   Berlinet A., 2011, REPRODUCING KERNEL H
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Chapelle  Olivier, 2014, ACM T INTEL SYST TEC, V5
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   Dudik M., 2011, P 28 INT C MACH LEAR, P1097
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guo  Zhaohan, 2017, ADV NEURAL INFORM PR, P2489
   Hallak  Assaf, 2017, P 34 INT C MACH LEAR, P1372
   Hallak  Assaf, 2016, P 30 AAAI C ART INT, P1631
   Hirano K, 2003, ECONOMETRICA, V71, P1161, DOI 10.1111/1468-0262.00442
   Jiang N., 2016, P 33 INT C MACH LEAR, P652
   Krajzewicz  D., 2012, INT J ADV SYSTEMS ME, V5
   Lagoudakis M., 2003, J MACHINE LEARNING R, V4, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107
   Levin David A, 2017, MARKOV CHAINS MIXING, V107
   Li L., 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Li  Lihong, 2015, P INT C ART INT STAT, P608
   Li Lihong, 2015, P 24 INT C WORLD WID, P929
   Liu H., 2018, P 6 INT C LEARN REPR
   Liu J., 2001, SPRINGER SERIES STAT
   Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077
   Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060
   Munos Remi, 2016, ADV NEURAL INFORM PR, P1046
   Murphy SA, 2001, J AM STAT ASSOC, V96, P1410, DOI 10.1198/016214501753382327
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Owen AB, 2013, MONTE CARLO THEORY M
   Precup D., 2000, INT C MACH LEARN, P759
   Precup Doina, 2001, P 18 INT C MACH LEAR, P417
   Puterman M. L., 1994, MARKOV DECISION PROC
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Serfling RJ, 2009, APPROXIMATION THEORE, V162
   Strehl A., 2010, ADV NEURAL INFORM PR, P2217
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 2016, J MACH LEARN RES, V17
   Tang L, 2013, P 22 ACM INT C C INF, P1587
   Thomas Philip, 2016, INT C MACH LEARN, P2139
   Thomas Philip S., 2017, P 31 AAAI C ART INT, P4740
   van der Pol Elise, 2016, NIPS WORKSH LEARN IN
   Wang  Yu-Xiang, 2017, P 34 INT C MACH LEAR, P3589
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305038
DA 2019-06-15
ER

PT S
AU Liu, RS
   Cheng, SC
   Liu, XK
   Ma, L
   Fan, X
   Luo, ZX
AF Liu, Risheng
   Cheng, Shichao
   Liu, Xiaokun
   Ma, Long
   Fan, Xin
   Luo, Zhongxuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Bridging Framework for Model Optimization and Deep Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches.
C1 [Liu, Risheng; Liu, Xiaokun; Ma, Long; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.
   [Liu, Risheng; Fan, Xin; Luo, Zhongxuan] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Liaoning, Peoples R China.
   [Cheng, Shichao; Luo, Zhongxuan] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
RP Liu, RS (reprint author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Liu, RS (reprint author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Liaoning, Peoples R China.
EM rsliu@dlut.edu.cn
FU National Natural Science Foundation of China [61672125, 61733002,
   61572096, 61632019]; Fundamental Research Funds for the Central
   Universities
FX This work is partially supported by the National Natural Science
   Foundation of China (Nos. 61672125, 61733002, 61572096 and 61632019),
   and Fundamental Research Funds for the Central Universities.
CR Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BEVILACQUA M, 2012, BRIT MACH VIS C, DOI DOI 10.5244/C.26.135
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Clevert D.A., 2015, ARXIV151107289
   Diamond Steven, 2017, ARXIV170508041
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Krishnan D., 2009, P ADV NEUR INF PROC, P1033
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kruse J, 2017, P IEEE INT C COMP VI, P4586
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li Ke, 2016, ARXIV160601885
   Liu R., 2018, IEEE T NEUR NET LEAR, P1
   Liu RS, 2014, NEURAL NETWORKS, V59, P1, DOI 10.1016/j.neunet.2014.06.005
   Liu Risheng, 2018, ARXIV181004012V1
   Liu Risheng, 2018, ARXIV180805331
   Liu Risheng, 2018, AAAI
   Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306
   Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779
   Sun LH, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL WORKSHOP ON COMPUTER SCIENCE IN SPORTS, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Ulyanov D., 2017, ARXIV171110925
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Zeyde R, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304034
DA 2019-06-15
ER

PT S
AU Liu, R
   Lehman, J
   Molino, P
   Such, FP
   Frank, E
   Sergeev, A
   Yosinski, J
AF Liu, Rosanne
   Lehman, Joel
   Molino, Piero
   Such, Felipe Petroski
   Frank, Eric
   Sergeev, Alex
   Yosinski, Jason
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI An intriguing failing of convolutional neural networks and the CoordConv
   solution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x, y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10-100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.
C1 [Liu, Rosanne; Lehman, Joel; Molino, Piero; Such, Felipe Petroski; Frank, Eric; Yosinski, Jason] Uber AI Labs, San Francisco, CA 94107 USA.
   [Sergeev, Alex] Uber Technol, Seattle, WA USA.
RP Liu, R (reprint author), Uber AI Labs, San Francisco, CA 94107 USA.
EM rosanne@uber.com; joel.lehman@uber.com; piero@uber.com;
   felipe.such@uber.com; mysterefrank@uber.com; asergeev@uber.com;
   yosinski@uber.com
CR Banino Andrea, 2018, NATURE, P1
   Brust C.-A., 2015, INT C COMP VIS THEOR
   Cueva C. J., 2018, ARXIV E PRINTS
   Dan Horgan, 2018, ARXIV180300933
   Fahlman S. E., 1990, ADV NEURAL INFORMATI, P524
   Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173
   Franzius M, 2007, PLOS COMPUT BIOL, V3, P1605, DOI 10.1371/journal.pcbi.0030166
   Gehring Jonas, 2017, CORR
   Gregor K., 2015, ARXIV150204623
   He K., 2015, CORR
   Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306
   Hoover AK, 2009, CONNECT SCI, V21, P227, DOI 10.1080/09540090902733871
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Karras Tero, 2018, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2
   Levine S, 2016, J MACH LEARN RES, V17
   Li Chunyuan, 2018, INT C LEARN REPR APR
   Lyu Yecheng, 2018, ARXIV180405164
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V., 2013, ARXIV E PRINTS
   Nguyen A., 2016, ARXIV E PRINTS
   Parmar N., 2018, ARXIV180205751
   Radford  A., 2015, ARXIV151106434
   Reed S. E., 2016, ADV NEURAL INFORM PR, P217
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   Santoro A, 2017, ADV NEURAL INFORM PR, P4974
   Sergeev A., 2018, ARXIV E PRINTS
   Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8
   Ulyanov D., 2017, ARXIV171110925
   Van Den Oord A., 2016, ARXIV160903499
   Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P6000
   Yu F., 2015, ARXIV150603365
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004019
DA 2019-06-15
ER

PT S
AU Liu, SC
   Hu, Y
   Zeng, YM
   Tang, QK
   Jin, BB
   Han, YH
   Li, XW
AF Liu, Shice
   Hu, Yu
   Zeng, Yiming
   Tang, Qiankun
   Jin, Beibei
   Han, Yinhe
   Li, Xiaowei
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI See and Think: Disentangling Semantic Scene Completion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID OBJECT DETECTION
AB Semantic scene completion predicts volumetric occupancy and object category of a 3D scene, which helps intelligent agents to understand and interact with the surroundings. In this work, we propose a disentangled framework, sequentially carrying out 2D semantic segmentation, 2D-3D reprojection and 3D semantic scene completion. This three-stage framework has three advantages: (1) explicit semantic segmentation significantly boosts performance; (2) flexible fusion ways of sensor data bring good extensibility; (3) progress in any subtask will promote the holistic performance. Experimental results show that regardless of inputing a single depth or RGB-D, our framework can generate high-quality semantic scene completion, and outperforms state-of-the-art approaches on both synthetic and real datasets.
C1 [Liu, Shice; Hu, Yu; Zeng, Yiming; Tang, Qiankun; Jin, Beibei; Han, Yinhe; Li, Xiaowei] Chinese Acad Sci, Inst Comp Technol, State Key Lab Comp Architecture, Beijing, Peoples R China.
   [Liu, Shice; Hu, Yu; Zeng, Yiming; Tang, Qiankun; Jin, Beibei; Han, Yinhe; Li, Xiaowei] Univ Chinese Acad Sci, Beijing, Peoples R China.
RP Liu, SC (reprint author), Chinese Acad Sci, Inst Comp Technol, State Key Lab Comp Architecture, Beijing, Peoples R China.; Liu, SC (reprint author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM liushice@ict.ac.cn; huyu@ict.ac.cn; zengyiming@ict.ac.cn;
   tangqiankun@ict.ac.cn; jinbeibei@ict.ac.cn; yinhes@ict.ac.cn;
   lxw@ict.ac.cn
FU National Natural Science Foundation of China [61274030]; Innovation
   Project of Institute of Computing Technology, Chinese Academy of
   Sciences [20186090]
FX We thank Shuran Song for sharing the SSCNet results and the SUNCG
   development kits. This work is supported in part by National Natural
   Science Foundation of China under grant No. 61274030, and in part by
   Innovation Project of Institute of Computing Technology, Chinese Academy
   of Sciences under grant No. 20186090. Yu Hu is the corresponding author.
CR Blaha M, 2016, PROC CVPR IEEE, P3176, DOI 10.1109/CVPR.2016.346
   CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813
   Chang A.X., 2015, ARXIV151203012
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   de Campos Teofilo Emidio, 2018, ARXIV180204735
   Nguyen DT, 2016, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2016.612
   Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586
   Garbade  Martin, 2018, ARXIV180403550
   Geiger A, 2015, LECT NOTES COMPUT SC, V9358, P183, DOI 10.1007/978-3-319-24947-6_15
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Goldstein E. B., 2008, BLACKWELL HDB SENSAT
   Guo  Ruiqi, 2015, ARXIV150402437
   Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hane C, 2013, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2013.20
   Handa A., 2015, ARXIV151107041
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Jiang H, 2013, PROC CVPR IEEE, P2171, DOI 10.1109/CVPR.2013.282
   Kar Abhishek, 2017, ADV NEURAL INFORM PR, P364
   Kim BS, 2013, IEEE I CONF COMP VIS, P1425, DOI 10.1109/ICCV.2013.180
   Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Mattausch O, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12286
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999
   Rezende Danilo Jimenez, 2016, ADV NEURAL INFORM PR, P4996
   SEKULER AB, 1992, J EXP PSYCHOL GEN, V121, P95, DOI 10.1037//0096-3445.121.1.95
   Silberman N, 2014, LECT NOTES COMPUT SC, V8691, P488, DOI 10.1007/978-3-319-10578-9_32
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Tateno K, 2016, IEEE INT CONF ROBOT, P2295, DOI 10.1109/ICRA.2016.7487378
   Wang P., 2017, ARXIV170208502
   Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22
   Wu Jiajun, 2017, ADV NEURAL INFORM PR, P540
   Yu F., 2015, ARXIV151107122
   Zhao H., 2017, IEEE C COMP VIS PATT, P2881
   Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300025
DA 2019-06-15
ER

PT S
AU Liu, SC
   Long, MS
   Wang, JM
   Jordan, MI
AF Liu, Shichen
   Long, Mingsheng
   Wang, Jianmin
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Generalized Zero-Shot Learning with Deep Calibration Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ATTRIBUTES
AB A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.
C1 [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.
   [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, BNRist, Beijing, Peoples R China.
   [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China.
   [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Long, MS (reprint author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.; Long, MS (reprint author), Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.; Long, MS (reprint author), Tsinghua Univ, BNRist, Beijing, Peoples R China.; Long, MS (reprint author), Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China.
EM liushichen95@gmail.com; mingsheng@tsinghua.edu.cn;
   jimwang@tsinghua.edu.cn; jordan@berkeley.edu
FU National Key R&D Program of China [2016YFB1000701]; Natural Science
   Foundation of China [61772299, 71690231, 61502265]; DARPA Program on
   Lifelong Learning Machines
FX This work was supported by the National Key R&D Program of China
   (2016YFB1000701), the Natural Science Foundation of China (61772299,
   71690231, 61502265) and the DARPA Program on Lifelong Learning Machines.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Al-Halah Z, 2015, IEEE WINT CONF APPL, P837, DOI 10.1109/WACV.2015.116
   Atzmon Y., 2018, UAI
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Changpinyo S., 2017, ICCV
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   FU ZY, 2015, PROC CVPR IEEE, P2635
   Gavves E, 2015, IEEE I CONF COMP VIS, P2731, DOI 10.1109/ICCV.2015.313
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Guo  Chuan, 2017, ICML
   He K., 2016, CVPR
   Hinton G., 2015, ARXIV150302531
   Kingma D.P., 2013, ARXIV13126114
   Kodirov E., 2017, CVPR
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Krizhevsky A., 2012, NIPS
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li ZY, 2014, LECT NOTES COMPUT SC, V8694, P350, DOI 10.1007/978-3-319-10599-4_23
   Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Norouzi M., 2013, ARXIV13125650
   Palatucci M, 2009, ADV NEURAL INFORM PR, P1410
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Romera- Paredes B., 2015, ICML, P2152
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED, VI
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Simonyan Karen, 2015, ICLR
   Socher R., 2013, ADV NEURAL INFORM PR, P935
   Szegedy C., 2015, CVPR
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Verma V. Kumar, 2018, CVPR
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang XY, 2013, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2013.264
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Xian Y., 2018, CVPR
   Xian Yongqin, 2017, CVPR
   Yang Y., 2015, ICLR
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Zhang  H., 2018, CVPR
   Zhang L., 2017, CVPR
   Zhang Z., 2015, ARXIV151104512
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhou B., 2014, ADV NEURAL INFORM PR, V27, P487, DOI DOI 10.1162/153244303322533223
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302005
DA 2019-06-15
ER

PT S
AU Liu, SJ
   Kailkhura, B
   Chen, PY
   Ting, PS
   Chang, SY
   Amini, L
AF Liu, Sijia
   Kailkhura, Bhavya
   Chen, Pin-Yu
   Ting, Paishun
   Chang, Shiyu
   Amini, Lisa
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order O(1/b), where b is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity.
C1 [Liu, Sijia; Chen, Pin-Yu; Chang, Shiyu; Amini, Lisa] MIT, IBM AI Watson Lab, IBM Res, Cambridge, MA 02139 USA.
   [Kailkhura, Bhavya] Lawrence Livermore Natl Lab, Livermore, CA USA.
   [Ting, Paishun] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Liu, SJ (reprint author), MIT, IBM AI Watson Lab, IBM Res, Cambridge, MA 02139 USA.
FU MIT-IBM Watson AI Lab; U.S. Department of Energy by Lawrence Livermore
   National Laboratory [DE-AC52-07NA27344 (LLNL-CONF-751658)]
FX This work was fully supported by the MIT-IBM Watson AI Lab. Bhavya
   Kailkhura was supported under the auspices of the U.S. Department of
   Energy by Lawrence Livermore National Laboratory under Contract
   DE-AC52-07NA27344 (LLNL-CONF-751658). The authors are also grateful to
   the anonymous reviewers for their helpful comments,
CR Agarwal A, 2010, P 23 ANN C LEARN THE, P28
   Brent R. P., 2013, ALGORITHMS MINIMIZAT
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chatterji N. S., 2018, ARXIV180205431
   Chen P. - Y., 2017, P 10 ACM WORKSH ART, P15
   Chen T., 2017, ARXIV170709060
   Choromanski K. M., 2017, ADV NEURAL INFORM PR, P6524
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Dvurechensky P., 2018, ARXIV180209022
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Fu MC, 2002, INFORMS J COMPUT, V14, P192, DOI 10.1287/ijoc.14.3.192.113
   Gao  X., 2014, OPTIMIZATION ONLINE, V12
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Grathwohl Will, 2017, ARXIV171100123
   Gu B., 2016, ARXIV161201425
   Hajinezhad  D., 2017, ARXIV171009997
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kirklin S, 2015, NPJ COMPUT MATER, V1, DOI 10.1038/npjcompumats.2015.10
   Kresse G, 1996, COMP MATER SCI, V6, P15, DOI 10.1016/0927-0256(96)00008-0
   Lei L., 2017, ADV NEURAL INFORM PR, V30, P2345
   Lian  X., 2016, ADV NEURAL INFORM PR, P3054
   Liu S., 2018, P 21 INT C ART INT S, V84, P288
   Madry Aleksander, 2017, ARXIV170606083
   Nesterov Y., 2015, FUNDATIONS COMPUTATI, V2, P527
   Nitanda A., 2016, ARTIF INTELL, P195
   Papernot N., 2017, P 2017 ACM AS C COMP, P506, DOI DOI 10.1145/3052973.3053009
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shamir O., 2013, COLT 2013, P3
   Shamir O, 2017, J MACH LEARN RES, V18
   Spall J. C., 2005, INTRO STOCHASTIC SEA, V65
   Tucker G., 2017, ADV NEURAL INFORM PR, P2624
   Wang H., 2014, ARXIV14070107
   Wang Y., 2018, P 21 INT C ART INT S, V84, P1356
   Ward L, 2016, NPJ COMPUT MATER, V2, DOI 10.1038/npjcompumats.2016.28
   Xu P., 2017, ARXIV170807827
   Yang W., 2003, COMPUTATIONAL MED CH, P103
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P1080
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303070
DA 2019-06-15
ER

PT S
AU Liu, TY
   Li, SY
   Shi, JP
   Zhou, EL
   Zhao, T
AF Liu, Tianyi
   Li, Shiyang
   Shi, Jianping
   Zhou, Enlu
   Zhao, Tuo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Towards Understanding Acceleration Tradeoff between Momentum and
   Asynchrony in Nonconvex Stochastic Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) have been widely used in distributed machine learning, e.g., training large collaborative filtering systems and deep neural networks. Due to current technical limit, however, establishing convergence properties of Async-MSGD for these highly complicated nonoconvex problems is generally infeasible. Therefore, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problems - streaming PCA. This allows us to make progress toward understanding Aync-MSGD and gaining new insights for more general problems. Specifically, by exploiting the diffusion approximation of stochastic optimization, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.
C1 [Liu, Tianyi; Zhou, Enlu; Zhao, Tuo] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
   [Li, Shiyang] Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China.
   [Shi, Jianping] Sensetime Grp Ltd, Hong Kong, Peoples R China.
RP Liu, TY (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
EM tliu341@gatech.edu; lsydevin@gmail.com; shijianping@sensetime.com;
   enlu.zhou@isye.gatech.edu; tuo.zhao@isye.gatech.edu
CR Chen J., 2016, ARXIV160400981
   CHEN Z., 2017, ARXIV170208134
   Ge R, 2016, ADV NEURAL INFORM PR
   GE R., 2015, C LEARN THEOR
   He K., 2016, P IEEE C COMP VIS PA
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35
   LI M., 2014, OSDI, V14
   LI X, 2016, ARXIV161209296
   LIAN X., 2015, ADV NEURAL INFORM PR
   LIAN X., 2016, ADV NEURAL INFORM PR
   LIU T., 2018, ARXIV180205155
   LIU W., 2017, ADV NEURAL INFORM PR
   MITLIAGKAS I., 2016, COMM CONTR COMP ALL
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov Ruslan, 2007, P 24 INT C MACH LEAR
   SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0
   SUN J., 2016, INF THEOR ISIT 2016
   ZHANG J., 2018, TRAINING, V1, P2
   Zhang W., 2015, ARXIV151105950
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303066
DA 2019-06-15
ER

PT S
AU Liu, WY
   Lin, RM
   Liu, Z
   Liu, LX
   Yu, ZD
   Dai, B
   Song, L
AF Liu, Weiyang
   Lin, Rongmei
   Liu, Zhen
   Liu, Lixin
   Yu, Zhiding
   Dai, Bo
   Song, Le
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning towards Minimum Hyperspherical Energy
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics - Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.
C1 [Liu, Weiyang; Liu, Zhen; Dai, Bo] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Lin, Rongmei] Emory Univ, Atlanta, GA 30322 USA.
   [Liu, Lixin] South China Univ Technol, Guangzhou, Guangdong, Peoples R China.
   [Yu, Zhiding] NVIDIA, Santa Clara, CA USA.
   [Dai, Bo] Google Brain, Mountain View, CA USA.
   [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China.
RP Liu, WY (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM wyliu@gatech.edu
FU NSF [IIS-1218749, IIS-1639792 EAGER, IIS-1841351 EAGER, CCF-1836822,
   CNS-1704701]; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR
   [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; Siemens
FX This project was supported in part by NSF IIS-1218749, NIH BIGDATA
   1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF
   IIS-1841351 EAGER, NSF CCF-1836822, NSF CNS-1704701, ONR
   N00014-15-1-2340, Intel ISTC, NVIDIA, Amazon AWS and Siemens. We would
   like to thank NVIDIA corporation for donating Titan Xp GPUs to support
   our research. We also thank Tuo Zhao for the valuable discussions and
   suggestions.
CR Aghasi Alireza, 2017, NIPS
   Ba J. L., 2016, ARXIV160706450
   Bilyk D, 2017, SB MATH+, V208, P744, DOI 10.1070/SM8656
   Brock A., 2017, ICLR
   Cogswell Michael, 2016, ICLR
   Deng J., 2018, ARXIV180107698
   Goodfellow I., 2014, NIPS
   Gotz M, 2001, INT S NUM M, V137, P159
   Gulrajani  Ishaan, 2017, NIPS
   Han S, 2016, ICLR
   Hardin D.P., 2004, NOT AM MATH SOC, V51, P1186
   Hardin DP, 2003, MATHPH0311024 ARXIV
   He K., 2016, ECCV
   He K., 2016, CVPR
   He K., 2015, ICCV
   Howard Andrew G., 2017, ARXIV170404861
   Huang G, 2017, CVPR
   Huang G.B., 2007, TECHNICAL REPORT
   Iandola Forrest N., 2016, ARXIV160207360
   Ioffe S., 2015, ICML
   Kemelmacher-Shlizerman Ira, 2016, CVPR
   Kuijlaars ABJ, 1998, T AM MATH SOC, V350, P523, DOI 10.1090/S0002-9947-98-02119-9
   Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006
   Landkof Naum Samouilovich, 1972, FDN MODERN POTENTIAL, V180
   Li N, 2012, JOINT EUR C MACH LEA
   Liu W., 2017, CVPR
   Liu Weiyang, 2016, ICML
   Liu Weiyang, 2018, CVPR
   Liu Weiyang, 2017, NIPS
   Liu Y., 2017, ARXIV171000870
   Mairal J., 2009, ICML
   Meng Deyu, 2014, NIPS
   Mishkin Dmytro, 2016, ICLR
   Miyato Takeru, 2018, ICLR
   Ramirez I., 2010, CVPR
   Rodriguez Pau, 2017, ICLR
   Saff EB, 1997, MATH INTELL, V19, P5, DOI 10.1007/BF03024331
   Salimans T., 2016, NIPS
   Schroff F., 2015, CVPR
   Shang Wenling, 2016, ICML
   Sharma Prakhar, 2017, NIPS WORKSH DEEP LEA
   Simonyan K, 2014, ARXIV14091556
   Smale S, 1998, MATH INTELL, V20, P7, DOI 10.1007/BF03025291
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Y., 2014, CVPR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tammes P. M. L., 1930, RECL TRAV BOT NEERL, V27, P1
   THOMSON JJ, 1904, LONDON EDINBURGH DUB, V7, P237, DOI DOI 10.1080/14786440409463107
   Wang F., 2018, ARXIV180105599
   Wang H., 2018, ARXIV180109414
   Warde-Farley David, 2017, ICLR
   Wen Y., 2016, ECCV
   Xiang Xiang, 2017, ARXIV170406369
   Xie Bo, 2016, ARXIV161103131
   Xie Di, 2017, ARXIV170301827
   Xie Pengtao, 2018, ICML
   Xie Pengtao, 2017, ICML
   Xie Pengtao, 2016, ICML
   Yi D., 2014, ARXIV14117923
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang X, 2017, ARXIV170701083
NR 61
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000070
DA 2019-06-15
ER

PT S
AU Liu, Y
   Gottesman, O
   Raghu, A
   Komorowski, M
   Faisal, A
   Doshi-Velez, F
   Brunskill, E
AF Liu, Yao
   Gottesman, Omer
   Raghu, Aniruddh
   Komorowski, Matthieu
   Faisal, Aldo
   Doshi-Velez, Finale
   Brunskill, Emma
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Representation Balancing MDPs for Off-Policy Policy Evaluation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.
C1 [Liu, Yao; Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA.
   [Gottesman, Omer; Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA.
   [Raghu, Aniruddh] Univ Cambridge, Cambridge, England.
   [Komorowski, Matthieu; Faisal, Aldo] Imperial Coll London, London, England.
RP Liu, Y (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM yaoliu@stanford.edu; gottesman@fas.harvard.edu; aniruddhraghu@gmail.com;
   matthieu.komorowski@gmail.com; a.faisal@imperial.ac.uk;
   finale@seas.harvard.edu; ebrun@cs.stanford.edu
FU Harvard Data Science Initiative; Siemens; NSF CAREER grant
FX This work was supported in part by the Harvard Data Science Initiative,
   Siemens, and a NSF CAREER grant.
CR Alaa A. M., 2017, ADV NEURAL INFORM PR, P3424
   Atan O., 2018, ARXIV180208679
   Brockman G, 2016, ARXIV160601540
   Cortes C, 2010, ADV NEURAL INFORM PR, P442
   Dudik M., 2011, P 28 INT C MACH LEAR, P1097
   Ernst D, 2006, IEEE DECIS CONTR P, P669
   Farajtabar  M., 2018, P 35 INT C MACH LEAR, P1447
   Guo  Z., 2017, ADV NEURAL INFORM PR, P2492
   Hanna JP, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P538
   Jiang N., 2016, P 33 INT C MACH LEAR, P652
   Johansson F. D., 2018, ARXIV180208598
   Johansson F. D., 2016, INT C MACH LEARN, V48, P3020
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   Kunzel  S., 2017, ARXIV170603461
   Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077
   Precup D., 2000, INT C MACH LEARN, P759
   ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910
   Schulam  P., 2017, ADV NEURAL INFORM PR, P1697
   Shalit U., 2017, P MACHINE LEARNING R, V70, P3076
   Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722
   Sriperumbudur Bharath K, 2009, ARXIV09012698
   Thomas P. S., 2015, AAAI
   Thomas Philip, 2016, INT C MACH LEARN, P2139
   Wager  S., 2017, J AM STAT ASS
   Yoon  J., 2018, ICLR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302064
DA 2019-06-15
ER

PT S
AU Liu, Y
   De Brabanter, K
AF Liu, Yu
   De Brabanter, Kris
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Derivative Estimation in Random Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONFIDENCE BANDS; REGRESSION; SIZER; CHOICE
AB We propose a nonparametric derivative estimation method for random design without having to estimate the regression function. The method is based on a variance-reducing linear combination of symmetric difference quotients. First, we discuss the special case of uniform random design and establish the estimator's asymptotic properties. Secondly, we generalize these results for any distribution of the dependent variable and compare the proposed estimator with popular estimators for derivative estimation such as local polynomial regression and smoothing splines.
C1 [Liu, Yu; De Brabanter, Kris] Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.
   [De Brabanter, Kris] Iowa State Univ, Dept Stat, Ames, IA 50011 USA.
RP Liu, Y; De Brabanter, K (reprint author), Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.; De Brabanter, K (reprint author), Iowa State Univ, Dept Stat, Ames, IA 50011 USA.
EM yuliu@iastate.edu; kbrabant@iastate.edu
CR Cabrera J. L. Ojeda, 2012, LOCPOL KERNEL LOCAL
   Charnigo R, 2007, J OPT SOC AM A, V24, P2578, DOI 10.1364/JOSAA.24.002578
   Charnigo R, 2011, TECHNOMETRICS, V53, P238, DOI 10.1198/TECH.2011.09147
   Chaudhuri P, 1999, J AM STAT ASSOC, V94, P807, DOI 10.2307/2669996
   Dai WL, 2016, J MACH LEARN RES, V17
   David H. A., 2003, ORDER STAT
   De Brabanter K., 2018, BIOMETRIKA
   De Brabanter K, 2013, J MACH LEARN RES, V14, P281
   Duong T., 2018, KS KERNEL SMOOTHING
   EUBANK RL, 1993, J AM STAT ASSOC, V88, P1287, DOI 10.2307/2291269
   Fan J., 1996, LOCAL POLYNOMIAL MOD
   Gijbels I, 2004, COMMUN STAT-THEOR M, V33, P851, DOI 10.1081/STA-120028730
   Gyorfi L, 2006, DISTRIBUTION FREE TH
   HALL P, 1990, BIOMETRIKA, V77, P521, DOI 10.2307/2336990
   Hardle W, 1990, APPL NONPARAMETRIC R
   Hassibi B., 1993, ADV NEURAL INFORMATI, V5, P164
   Heckman NE, 2000, CAN J STAT, V28, P241, DOI 10.2307/3315976
   Muller H- G., 2012, NONPARAMETRIC REGRES, V46
   MULLER HG, 1987, BIOMETRIKA, V74, P743, DOI 10.2307/2336468
   Park C, 2008, COMPUT STAT DATA AN, V52, P3954, DOI 10.1016/j.csda.2008.01.006
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Ramsay JO, 2007, APPL FUNCTIONAL DATA
   Ripley B., 2017, PSPLINE PENALIZED SM
   Rondonotti V, 2007, ELECTRON J STAT, V1, P268, DOI 10.1214/07-EJS006
   ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190
   RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632
   STONE CJ, 1985, ANN STAT, V13, P689, DOI 10.1214/aos/1176349548
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   WAHBA G, 1990, COMMUN STAT THEORY, V19, P1685, DOI 10.1080/03610929008830285
   Wang WW, 2015, J MACH LEARN RES, V16, P2617
   Xia YC, 1998, J ROY STAT SOC B, V60, P797, DOI 10.1111/1467-9868.00155
   Zhou SG, 2000, STAT SINICA, V10, P93
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303044
DA 2019-06-15
ER

PT S
AU Liu, ZH
   Xu, JZ
   Peng, XL
   Xiong, RQ
AF Liu, Zhenhua
   Xu, Jizheng
   Peng, Xiulian
   Xiong, Ruiqin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Frequency-Domain Dynamic Pruning for Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2 x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-10.
C1 [Liu, Zhenhua; Xiong, Ruiqin] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing, Peoples R China.
   [Xu, Jizheng; Peng, Xiulian] Microsoft Res Asia, Beijing, Peoples R China.
RP Liu, ZH (reprint author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing, Peoples R China.
EM liu-zh@pku.edu.cn; jzxu@microsoft.com; xipe@microsoft.com;
   rqxiong@pku.edu.cn
FU National Key Research and Development Program of China [2017YFB1002203];
   National Natural Science Foundation of China [61772041]; Beijing Natural
   Science Foundation [4172027]; Cooperative Medianet Innovation Center
FX This work was part supported by the National Key Research and
   Development Program of China (2017YFB1002203), the National Natural
   Science Foundation of China (61772041), the Beijing Natural Science
   Foundation (4172027), and also by the Cooperative Medianet Innovation
   Center. This work was done when Z. Liu was with Microsoft Research Asia.
CR Chen W., 2016, P 22 ACM SIGKDD INT, P1475
   Courbariaux M, 2016, ARXIV160202830
   Guo Y., 2016, ADV NEURAL INFORM PR, P1379
   Han  S., 2016, INT C LEARN REPR
   Han Song, 2015, P ADV NEUR INF PROC, V2015, P1135
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   He Y., 2017, P IEEE INT C COMP VI, V2, P6
   Jaderberg M., 2014, BRIT MACH VIS C
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Kim Yongdeok, 2016, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282
   Louizos C., 2017, ADV NEURAL INFORM PR, P3288
   Louizos Christos, 2017, ARXIV171201312
   Mellempudi  N., 2017, ARXIV170501462
   Molchanov Dmitry, 2017, ARXIV170105369
   Neklyudov  K., 2017, ADV NEURAL INFORM PR, P6775
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Tai  C., 2016, INT C LEARN REPR
   Ullrich K., 2017, ARXIV170204008
   Wang  P., 2016, COMPUTER VISION PATT, P4012
   Wang Y., 2016, P ADV NEUR INF PROC, V29, P253
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809
   Zhou A., 2017, ARXIV170203044
   Zhu  C., 2016, INT C LEARN REPR
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301007
DA 2019-06-15
ER

PT S
AU Locatello, F
   Dresdner, G
   Khanna, R
   Valera, I
   Ratsch, G
AF Locatello, Francesco
   Dresdner, Gideon
   Khanna, Rajiv
   Valera, Isabel
   Ratsch, Gunnar
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Boosting Black Box Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational approximation. Borrowing ideas from the classic boosting framework, recent approaches attempt to boost VI by replacing the selection of a single density with an iteratively constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.
C1 [Locatello, Francesco; Valera, Isabel] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
   [Locatello, Francesco; Dresdner, Gideon; Ratsch, Gunnar] Swiss Fed Inst Technol, Dept Comp Sci, Univ Str 6, CH-8092 Zurich, Switzerland.
   [Khanna, Rajiv] Univ Texas Austin, Austin, TX 78712 USA.
RP Locatello, F (reprint author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.; Locatello, F (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Univ Str 6, CH-8092 Zurich, Switzerland.
FU Max-Planck ETH Center for Learning Systems; ETH core grant; NSF [IIS
   1421729]
FX FL is partially supported by the Max-Planck ETH Center for Learning
   Systems. FL, GD are partially supported by an ETH core grant (to GR). RK
   is supported by NSF Grant IIS 1421729. We thank Matthias Huser for
   providing the preprocessed eICU dataset. We also thank David Blei and
   Anant Raj for helpful discussions.
CR Blei D. M., 2016, ARXIV160100670
   Christopher M B, 2016, PATTERN RECOGNITION
   Fortuin Vincent, 2018, ARXIV180602199
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Guo Fangjian, 2016, ARXIV161105559
   Jaakkola Tommi S., 1998, IMPROVING MEAN FIELD
   Jaggi Martin, 2013, ICML 2013 P 30 INT C
   Jaggi Martin, 2011, CONVEX OPTIMIZATION
   Jerfel Ghassen, 2017, BOOSTED STOCHASTIC B
   Krishnan Rahul G., 2015, BARRIER FRANK WOLFE, P532
   Lacoste-Julien Simon, 2015, ADV NEURAL INFORM PR, V28, P496
   Lacoste-Julien Simon, 2016, ARXIV160700345
   Locatello Francesco, 2018, P 21 INT C ART INT S, V84, P464
   Locatello Francesco, 2017, P INT C ART INT STAT
   Meir R., 2003, Advanced Lectures on Machine Learning. Machine Learning Summer School 2002. Revised Lectures. (Lecture Notes in Artificial Intelligence Vol.2600), P118
   Miller Andrew C, 2016, ARXIV161106585
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Ranganath  R., 2014, AISTATS, P814
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Saeedi A, 2017, J MACH LEARN RES, V18, P1
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.1145/1390156.1390267
   Salimans T., 2015, ICML
   Saxena Siddhartha, 2017, ABS170702510 CORR
   Tolstikhin I., 2017, ARXIV170102386
   Tran D., 2016, ARXIV161009787
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303040
DA 2019-06-15
ER

PT S
AU Logeswaran, L
   Lee, H
   Bengio, S
AF Logeswaran, Lajanugen
   Lee, Honglak
   Bengio, Samy
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Content preserving text generation with attribute controls
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this work, we address the problem of modifying textual attributes of sentences. Given an input sentence and a set of attribute labels, we attempt to generate sentences that are compatible with the conditioning information. To ensure that the model generates content compatible sentences, we introduce a reconstruction loss which interpolates between auto-encoding and back-translation loss components. We propose an adversarial loss to enforce generated samples to be attribute compatible and realistic. Through quantitative, qualitative and human evaluations we demonstrate that our model is capable of generating fluent sentences that better reflect the conditioning information compared to prior methods. We further demonstrate that the model is capable of simultaneously controlling multiple attributes.
C1 [Logeswaran, Lajanugen] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Lee, Honglak; Bengio, Samy] Google Brain, Mountain View, CA USA.
RP Logeswaran, L (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM llajan@umich.edu; honglak@google.com; bengio@google.com
CR Artetxe M., 2017, ARXIV171011041
   Bowman S. R., 2015, ARXIV151106349
   Chen X., 2016, ARXIV161102731
   Chung J., 2015, INT C MACH LEARN, p2067 , DOI DOI 10.1145/2661829.2661935
   Diao Q., 2014, P 20 ACM SIGKDD INT, P193, DOI DOI 10.1145/2623330.2623758
   Fu Z., 2017, ARXIV171106861
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hu Zhiting, 2017, ARXIV170300955
   Isola  P., 2016, ARXIV161107004
   Jozefowicz R., 2016, ARXIV160202410
   Kikuchi Yuta, 2016, ARXIV160909552
   Kingma D. P., 2013, ARXIV13126114
   Kiros R., 2014, ADV NEURAL INFORM PR, P2348
   Lample G., 2017, ARXIV171100043
   Li J., 2018, ARXIV180406437
   Li  J., 2016, ARXIV160306155
   Maas A. L., 2011, P 49 ANN M ASS COMP, V1, P142
   Miyato Takeru, 2018, ARXIV180205637
   Oord  A.v.d., 2016, ARXIV160106759
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Prabhumoye S., 2018, ARXIV180409000
   Radford  A., 2017, ARXIV170401444
   Ramm Anita, 2017, P ACL 2017 SYST DEM, P1
   Reed S., 2016, ARXIV160505396
   Sennrich Rico, 2016, P 2016 C N AM CHAPT, P35, DOI [DOI 10.18653/V1/N16-1005, 10.18653/v1/n16-1005]
   Shen T., 2017, ARXIV170509655
   Shetty Rakshith, 2017, ARXIV171101921
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Xia  Y., 2016, ADV NEURAL INFORM PR, P820
   Xu Wei, 2012, 24 INT C COMP LING C
   Yamagishi Hayahide, 2016, P 3 WORKSH AS TRANSL, P203
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305014
DA 2019-06-15
ER

PT S
AU Long, MS
   Cao, ZJ
   Wang, JM
   Jordan, MI
AF Long, Mingsheng
   Cao, Zhangjie
   Wang, Jianmin
   Jordan, Michael I.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Conditional Adversarial Domain Adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. With theoretical guarantees and a few lines of codes, the approach has exceeded state-of-the-art results on five datasets.
C1 [Long, Mingsheng] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, KLiss, MOE, Beijing, Peoples R China.
   [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, BNRist, Beijing, Peoples R China.
   [Cao, Zhangjie; Wang, Jianmin] Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China.
   [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Long, MS (reprint author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM mingsheng@tsinghua.edu.cn; caozhangjie14@gmail.com;
   jimwang@tsinghua.edu.cn; jordan@berkeley.edu
FU National Key R&D Program of China [2016YFB1000701]; Natural Science
   Foundation of China [61772299, 71690231, 61502265]; DARPA Program on
   Lifelong Learning Machines
FX We thank Yuchen Zhang at Tsinghua University for insightful discussions.
   This work was supported by the National Key R&D Program of China
   (2016YFB1000701), the Natural Science Foundation of China (61772299,
   71690231, 61502265) and the DARPA Program on Lifelong Learning Machines.
CR Arjovsky M., 2017, INT C LEARN REPR ITL
   Arjovsky Martin, 2017, INT C MACH LEARN ICM
   Arora S., 2017, INT C MACH LEARN ICM
   Ben-David S., 2007, ADV NEURAL INFORM PR
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Che T., 2017, INT C LEARN REPR ICL
   Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Courty N., 2017, ADV NEURAL INFORM PR, P3730
   Donahue  J., 2014, INT C MACH LEARN ICM
   Ganin Y., 2015, INT C MACH LEARN ICM
   Ganin Y, 2016, J MACH LEARN RES, V17
   Glorot X., 2011, INT C MACH LEARN ICM
   Gong B., 2012, IEEE C COMP VIS PATT
   Gong B., 2013, INT C MACH LEARN ICM
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gopalan R, 2011, PROC CVPR IEEE
   Grandvalet Yves, 2005, ADV NEURAL INFORM PR
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hoffman J., 2016, CORR
   Hoffman J., 2014, ADV NEURAL INFORM PR
   Hoffman J., 2018, P 35 INT C MACH LEAR, P1989
   Huang J., 2006, ADV NEURAL INFORM PR
   Isola P., 2017, IEEE C COMP VIS PATT
   Kar P., 2012, P 15 INT C ART INT S, V22, P583
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Liu  M., 2017, ADV NEURAL INFORM PR, P700
   Long M., 2015, INT C MACH LEARN ICM
   Long M, 2016, ADV NEURAL INFORM PR, V2016, P136
   Long M., 2017, INT C MACH LEARN ICM
   Mansour Y., 2009, C COMP LEARN THEOR C
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Odena A., 2017, INT C MACH LEARN ICM
   Oquab M., 2013, IEEE C COMP VIS PATT
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Quionero-Candela Joaquin, 2009, DATASET SHIFT MACHIN
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Saenko K., 2010, EUR C COMP VIS ECCV
   Sankaranarayanan S., 2018, IEEE C COMP VIS PATT
   Song L., 2013, ADV NEURAL INFORM PR, P3228
   Song L., 2010, INT C MACH LEARN ICM
   Song L., 2009, INT C MACH LEARN ICM
   Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713
   Sugiyama M., 2008, ADV NEURAL INFORM PR
   Sugiyama M, 2007, J MACH LEARN RES, V8, P985
   Tsai Y., 2018, IEEE C COMP VIS PATT
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng  Eric, 2017, IEEE C COMP VIS PATT
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H., 2017, IEEE C COMP VIS PATT
   Yosinski J., 2014, ADV NEURAL INFORM PR
   Zhang K., 2013, INT C MACH LEARN ICM
   Zhu J-Y, 2017, IEEE ICC
NR 56
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301061
DA 2019-06-15
ER

PT S
AU Lopez, R
   Regier, J
   Jordan, MI
   Yosef, N
AF Lopez, Romain
   Regier, Jeffrey
   Jordan, Michael I.
   Yosef, Nir
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Information Constraints on Auto-Encoding Variational Bayes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on auto-encoding variational Bayes, in which the search space is constrained via kernel-based measures of independence. In particular, our method employs the d-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors. We show how this method can be applied to a range of problems, including problems that involve learning invariant and conditionally independent representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal is mixed in complex ways with sequencing errors and sampling effects. We show that our method outperforms the state-of-the-art approach in this domain.
C1 [Lopez, Romain; Regier, Jeffrey; Jordan, Michael I.; Yosef, Nir] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Jordan, Michael I.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
   [Yosef, Nir] Ragon Inst MGH MIT & Harvard, Cambridge, MA USA.
   [Yosef, Nir] Chan Zuckerberg Biohub, San Francisco, CA USA.
RP Lopez, R (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM romain_lopez@berkeley.edu; regier@berkeley.edu; jordan@cs.berkeley.edu;
   niryosef@berkeley.edu
FU NIH-NIAID [U19 AI090023]
FX NY and RL were supported by grant U19 AI090023 from NIH-NIAID.
CR Buettner F, 2015, NAT BIOTECHNOL, V33, P155, DOI 10.1038/nbt.3102
   Burda Y., 2016, INT C LEARN REPR
   Burgess Christopher P, 2017, LEARNING DISENTANGLE
   Chen  Jianbo, 2018, P 35 INT C MACH LEAR, V80, P882
   Chen Tian Qi, 2018, INT C LEARN REPR WOR
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Cole Michael B, 2017, BIORXIV
   DURRIEU JL, 2012, IEEE INT C AC SPEECH, P4833
   Finak G, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0844-5
   Flaxman  Seth, 2016, P 32 C UNC ART INT
   Fukumizu K, 2008, ADV NEURAL INFORM PR, P489
   Gelman A., 2007, DATA ANAL USING REGR
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Gretton A., 2008, ADV NEURAL INFORM PR, V20, P585
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Grun D, 2014, NAT METHODS, V11, P637, DOI [10.1038/nmeth.2930, 10.1038/NMETH.2930]
   Higgins I., 2017, INT C LEARN REPR
   Hoffman Matthew D, 2016, ADV APPROXIMATE BAYE
   Jitkrittum  Wittawat, 2017, INT C MACH LEARN ICM, P1742
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Kim  Hyunjik, 2017, LEARNING DISENTANGLE
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma Diederik, 2014, INT C LEARN REPR
   Klein AM, 2015, CELL, V161, P1187, DOI 10.1016/j.cell.2015.04.044
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li QH, 2011, ANN APPL STAT, V5, P1752, DOI 10.1214/11-AOAS466
   Lopez  Romain, 2018, BIORXIV
   Louizos  Christos, 2016, INT C LEARN REPR
   Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8
   Macosko  Evan, 2017, CELL, V161, P1202
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Makhzani  Alireza, 2016, INT C LEARN REPR WOR
   Nakaya HI, 2011, NAT IMMUNOL, V12, P786, DOI 10.1038/ni.2067
   Perez-Suay  Adrian, 2018, APPL SOFT COMPUTING
   Pfister N, 2018, J R STAT SOC B, V80, P5, DOI 10.1111/rssb.12235
   Risso D, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02554-5
   Salakhutdinov R., 2010, INT C ART INT STAT, P693
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Szabo  Zoltan, 2018, J MACHINE LEARNING R, V18, P1
   Tanay A, 2017, NATURE, V541, P331, DOI 10.1038/nature21350
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wagner A, 2016, NAT BIOTECHNOL, V34, P1145, DOI 10.1038/nbt.3711
   Wang B, 2017, NAT METHODS, V14, P414, DOI 10.1038/nmeth.4207
   WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066
   Zheng GXY, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14049
NR 47
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000060
DA 2019-06-15
ER

PT S
AU Lu, T
   Schuurmans, D
   Boutilier, C
AF Lu, Tyler
   Schuurmans, Dale
   Boutilier, Craig
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Non-delusional Q-learning and Value Iteration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to the expressible policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation, instability and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets-sets that record constraints on policies consistent with backed-up Q-values. We prove that both the model-based and model-free algorithms using this backup remove delusional bias, yielding the first known algorithms that guarantee optimal results under general conditions. These algorithms furthermore only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias.
C1 [Lu, Tyler; Schuurmans, Dale; Boutilier, Craig] Google AI, Mountain View, CA 94043 USA.
RP Lu, T (reprint author), Google AI, Mountain View, CA 94043 USA.
EM tylerlu@google.com; schuurmans@google.com; cboutilier@google.com
CR Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30
   Bartlett Peter L, 2017, ARXIV170302930
   Bellemare M. G., 2016, AAAI, P1476
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bellemare Marc G., 2017, P INT C MACH LEARN I
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Blum Avrim, 1988, P 1988 WORKSH COMP L, P9
   Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369
   De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925
   Durrett R, 2013, PROBABILITY THEORY E
   Geist Matthieu, 2017, ADV NEURAL INFORM PR, P3208
   Gordon G. J., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P261
   Gordon Geoffrey J., 1999, THESIS
   Hasselt H. V., 2010, ADV NEURAL INFORM PR, P2613
   Hessel M., 2017, ARXIV171002298
   Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181
   Lehnert Lucas, 2018, P 32 AAAI C ART INT
   Maei Hamid Reza, 2010, INT C MACH LEARN, P719
   Melo FS, 2007, LECT NOTES COMPUT SC, V4539, P308, DOI 10.1007/978-3-540-72927-3_23
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384
   Munos Remi, 2016, ADV NEURAL INFORM PR, P1046
   Nachum Ofir, 2017, ADV NEURAL INFORM PR, P1476
   Petrik Marek, 2010, THESIS
   Sauer N., 1972, Journal of Combinatorial Theory, Series A, V13, P145, DOI 10.1016/0097-3165(72)90019-2
   SHELAH S, 1972, PAC J MATH, V41, P247, DOI 10.2140/pjm.1972.41.247
   Sutton R.S., 2018, REINFORCEMENT LEARNI
   Szepesvari Csaba, 2004, P INT C MACH LEARN I
   TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1007/BF00992697
   Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wang Ziyu, 2016, P INT C MACH LEARN I
   Watkins C. J. C. H., 1989, THESIS
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004050
DA 2019-06-15
ER

PT S
AU Lu, YY
   Fan, YY
   Lv, JC
   Noble, WS
AF Lu, Yang Young
   Fan, Yingying
   Lv, Jinchi
   Noble, William Stafford
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI DeepPINK: reproducible feature selection in deep neural networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FALSE DISCOVERY RATE; TESTS; MASS; FAT
AB Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.(2)
C1 [Lu, Yang Young; Noble, William Stafford] Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA.
   [Fan, Yingying; Lv, Jinchi] Univ Southern Calif, Marshall Sch Business, Data Sci & Operat Dept, Los Angeles, CA 90089 USA.
   [Noble, William Stafford] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
RP Lu, YY (reprint author), Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA.
EM ylu465@uw.edu; fanyingy@marshall.usc.edu; jinchilv@marshall.usc.edu;
   william-noble@uw.edu
FU NIH [1R01GM131407-01, R01GM121818]; Simons Foundation; Adobe Data
   Science Research Award
FX This work was supported by NIH awards 1R01GM131407-01 and R01GM121818, a
   grant from the Simons Foundation, and an Adobe Data Science Research
   Award.
CR Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074
   Barber R. F., 2016, ARXIV160203574
   Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337
   Benjamini Y, 2001, ANN STAT, V29, P1165
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8
   Blankson H, 2000, J NUTR, V130, P2943
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Candes Emmanuel J, 2018, J ROYAL STAT SOC B
   Chang Yin-Wen, 2008, JMLR WORKSHOP C P, V3, P53
   Chen  Jun, 2013, ANN APPL STAT, V7
   Chiu CM, 2014, BIOMED RES INT, DOI 10.1155/2014/906168
   Clarke S, 2009, ANN STAT, V37, P332, DOI 10.1214/07-AOS557
   Efron B, 2007, J AM STAT ASSOC, V102, P93, DOI 10.1198/016214506000001211
   Fan JQ, 2007, J AM STAT ASSOC, V102, P1282, DOI 10.1198/016214507000000969
   Fan JQ, 2012, J AM STAT ASSOC, V107, P1019, DOI 10.1080/01621459.2012.720478
   Fan Y., 2017, ARXIV170900092
   Fan Y., 2017, ARXIV170503604
   Fan YY, 2016, ANN STAT, V44, P2098, DOI 10.1214/15-AOS1416
   Fan YY, 2011, J ECONOMETRICS, V164, P331, DOI 10.1016/j.jeconom.2011.06.014
   Ghorbani Amirata, 2017, ARXIV171010547
   Hall P, 2010, BERNOULLI, V16, P418, DOI 10.3150/09-BEJ220
   Kingma D. P., 2014, ARXIV14126980
   Kuang YS, 2017, GIGASCIENCE, V6, DOI 10.1093/gigascience/gix058
   Lin W, 2014, BIOMETRIKA, V101, P785, DOI 10.1093/biomet/asu031
   Lipton Zachary C, 2016, ARXIV160603490
   Obermeyer Z, 2016, NEW ENGL J MED, V375, P1216, DOI 10.1056/NEJMp1606181
   Pimpin L, 2016, AM J CLIN NUTR, V103, P389, DOI 10.3945/ajcn.115.118612
   Rabot S, 2016, SCI REP-UK, V6, DOI 10.1038/srep32484
   Reeds DN, 2013, AESTHET SURG J, V33, P400, DOI 10.1177/1090820X13478630
   Rhee SY, 2006, P NATL ACAD SCI USA, V103, P17355, DOI 10.1073/pnas.0607274103
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Shrikumar A., 2017, ARXIV170402685
   Simonyan K, 2013, ARXIV13126034
   Storey JD, 2004, J ROY STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x
   Sundararajan M, 2017, ARXIV170301365
   Turner  Ryan, 2016, 2016 IEEE 26 INT WOR, P1
   Vanhala M, 2012, AM J EPIDEMIOL, V176, P253, DOI 10.1093/aje/kwr504
   Wu WB, 2008, ANN STAT, V36, P364, DOI 10.1214/009053607000000730
   Yang Qing, 2010, Yale Journal of Biology and Medicine, V83, P101
   Yang YJ, 2012, NUTR RES PRACT, V6, P68, DOI 10.4162/nrp.2012.6.1.68
   Yun Y, 2017, BMC MICROBIOL, V17, DOI 10.1186/s12866-017-1052-0
   Zhang Y, 2011, J AM STAT ASSOC, V106, P846, DOI 10.1198/jasa.2011.ap10657
   Zheng  Zemin, 2017, ARXIV171002704
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003025
DA 2019-06-15
ER

PT S
AU Lucic, M
   Kurach, K
   Michalski, M
   Bousquet, O
   Gelly, S
AF Lucic, Mario
   Kurach, Karol
   Michalski, Marcin
   Bousquet, Olivier
   Gelly, Sylvain
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Are GANs Created Equal? A Large-Scale Study
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].
C1 [Lucic, Mario; Kurach, Karol; Michalski, Marcin; Bousquet, Olivier; Gelly, Sylvain] Google Brain, Mountain View, CA 94043 USA.
RP Lucic, M; Kurach, K (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM lucic@google.com; kkurach@google.com
CR Arjovsky Martin, 2017, INT C MACH LEARN ICM
   Arora S., 2017, INT C MACH LEARN ICM
   Arora Sanjeev, 2018, INT C LEARN REPR ICL
   Bachman Philip, 2015, INT C MACH LEARN ICM
   Berthelot D., 2017, ARXIV170310717
   Chen X., 2016, ADV NEURAL INFORM PR
   Fedus William, 2018, INT C LEARN REPR ICL
   Gerhard HE, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002873
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gulrajani Ishaan, 2017, ADV NEURAL INFORM PR
   Heusel Martin, 2017, ADV NEURAL INFORM PR
   Huszar Ferenc, 2015, ARXIV151105101
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kodali N., 2017, ARXIV170507215
   Kurach K, 2018, ARXIV180704720
   Mahendran Aravindh, 2015, C COMP VIS PATT REC
   Mao X., 2017, INT C COMP VIS ICCV
   Mescheder L., 2017, ADV NEURAL INFORM PR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Odena A., 2017, INT C MACH LEARN ICM
   Radford A., 2015, ARXIV151106434
   Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR
   Salimans  T., 2016, ADV NEURAL INFORM PR
   Theis L., 2015, ARXIV151101844
   Wu Yuhuai, 2017, INT C LEARN REPR ICL
   Zhang Y, 2017, INT CONF IMAG VIS
NR 27
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300065
DA 2019-06-15
ER

PT S
AU Lui, KYC
   Ding, GW
   Huang, RT
   McCann, RJ
AF Lui, Kry Yik Chau
   Ding, Gavin Weiguang
   Huang, Ruitong
   McCann, Robert J.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Dimensionality Reduction has Quantifiable Imperfections: Two Geometric
   Bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID VISUAL ANALYSIS; WAIST
AB In this paper, we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view. In particular, we show that no DR maps can achieve perfect precision and perfect recall simultaneously. Thus a continuous DR map must have imperfect precision. We further prove an upper bound on the precision of Lipschitz continuous DR maps. While precision is a natural measure in an information retrieval setting, it does not measure "how" wrong the retrieved data is. We therefore propose a new measure based on Wasserstein distance that comes with similar theoretical guarantee. A key technical step in our proofs is a particular optimization problem of the L-2-Wasserstein distance over a constrained set of distributions. We provide a complete solution to this optimization problem, which can be of independent interest on the technical side.
C1 [Lui, Kry Yik Chau; Ding, Gavin Weiguang; Huang, Ruitong] Borealis AI, Toronto, ON, Canada.
   [McCann, Robert J.] Univ Toronto, Dept Math, Toronto, ON, Canada.
RP Lui, KYC (reprint author), Borealis AI, Toronto, ON, Canada.
EM yikchau.y.lui@borealisai.com; gavin.ding@borealisai.com;
   ruitong.huang@borealisai.com; mccann@math.toronto.edu
CR Akopyan A, 2017, B LOND MATH SOC, V49, P690, DOI 10.1112/blms.12062
   Akopyan  Arseniy, 2018, INT MATH RES NOTICES, DOI 10.1093/imrn/rny037
   Alpert H, 2015, J TOPOL ANAL, V7, P73, DOI 10.1142/S1793525315500053
   Altschuler J., 2017, ADV NEURAL INFORM PR, P1961
   Arora  Sanjeev, 2018, P 31 C LEARN THEOR P, V75, P1455
   Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192
   BOURGAIN J, 1985, ISRAEL J MATH, V52, P46, DOI 10.1007/BF02776078
   Boutsidis C., 2010, ADV NEURAL INFORM PR, V23, P298
   Caffarelli LA, 2010, ANN MATH, V171, P673, DOI 10.4007/annals.2010.171.673
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Figalli A, 2010, ARCH RATION MECH AN, V195, P533, DOI 10.1007/s00205-008-0212-7
   Flamary  Remi, 2017, POT PYTHON OPTIMAL T
   Granata D, 2016, SCI REP-UK, V6, DOI 10.1038/srep31377
   Gromov M, 2003, GEOM FUNCT ANAL, V13, P178, DOI 10.1007/s000390300004
   Guillemin V., 2010, DIFFERENTIAL TOPOLOG, V370
   Guth  LARRY, 2012, THE ABEL PRIZE 2008, P181
   Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989
   Klartag B, 2017, GEOM FUNCT ANAL, V27, P130, DOI 10.1007/s00039-017-0397-8
   Korman J, 2013, P NATL ACAD SCI USA, V110, P10064, DOI 10.1073/pnas.1221333110
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lespinats S, 2011, COMPUT GRAPH FORUM, V30, P113, DOI 10.1111/j.1467-8659.2010.01835.x
   Martins RM, 2014, COMPUT GRAPH-UK, V41, P26, DOI 10.1016/j.cag.2014.01.006
   McCann RJ, 2004, NONLINEARITY, V17, P1891, DOI 10.1088/0951-7715/17/5/017
   McQueen  James, 2016, ADV NEURAL INFORM PR, P2631
   Muger M., 2015, MATH SEMESTERBERICHT, V62, P59
   Narayanan  Hariharan, 2010, ADV NEURAL INFORM PR, P1786
   PAYNE LE, 1967, SIAM REV, V9, P453, DOI 10.1137/1009070
   Roeer  Malte, 2013, ARXIV13051529
   Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583
   Schreck T, 2010, INFORM VISUAL, V9, P181, DOI 10.1057/ivs.2010.2
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Van Der Maaten L, 2009, J MACHINE LEARNING R, V5, P384
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Verma N, 2013, J MACH LEARN RES, V14, P2415
   Wang X., 2005, MATH MAG, P390, DOI [10.2307/30044198, DOI 10.2307/30044198]
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003005
DA 2019-06-15
ER

PT S
AU Luise, G
   Rudi, A
   Pontil, M
   Ciliberto, C
AF Luise, Giulia
   Rudi, Alessandro
   Pontil, Massimiliano
   Ciliberto, Carlo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differential Properties of Sinkhorn Approximation for Learning with
   Wasserstein Distance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness of its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula is used to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.
C1 [Luise, Giulia; Pontil, Massimiliano; Ciliberto, Carlo] UCL, Dept Comp Sci, London, England.
   [Rudi, Alessandro] PSL Res Univ, Ecole Normale Super, Dept Informat, INRIA, Paris, France.
   [Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy.
   [Ciliberto, Carlo] Imperial Coll, Dept Elect & Elect Engn, London, England.
RP Luise, G (reprint author), UCL, Dept Comp Sci, London, England.
EM g.luise.16@ucl.ac.uk; alessandro.rudi@inria.fr; m.pontil@ucl.ac.uk;
   c.ciliberto@imperial.ac.uk
FU EPSRC [EP/P009069/1]; European Research Council [SEQUOIA 724063];
   Engineering and Physical Research Council (EPSRC) [EP/P009069/1]; UK
   Defence Science and Technology Laboratory (Dstl)
FX This work was supported in part by EPSRC Grant N. EP/P009069/1, by the
   European Research Council (grant SEQUOIA 724063), UK Defence Science and
   Technology Laboratory (Dstl) and Engineering and Physical Research
   Council (EPSRC) under grant EP/P009069/1. This is part of the
   collaboration between US DOD, UK MOD and UK EPSRC under the
   Multidisciplinary University Research Initiative.
CR Altschuler J., 2017, ADV NEURAL INFORM PR, P1961
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Bakir G., 2007, PREDICTING STRUCTURE
   Benamou Jean-David, 2015, SIAM J SCI COMPUTING, V37
   Bengio Yoshua, 2000, NEURAL COMPUTATION, V12
   Berlinet A., 2011, REPRODUCING KERNEL H
   Bertsimas D, 1997, INTRO LINEAR OPTIMIZ
   Bonneel N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925918
   Brezis H, 2011, FUNCTIONAL ANAL SOBO
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387
   Ciliberto Carlo, 2017, ADV NEURAL INFORM PR
   Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412
   COMINETTI R, 1994, MATH PROGRAM, V67, P169, DOI 10.1007/BF01582220
   Courty N., 2014, LNCS, P1
   Cuturi M., 2014, INT C MACH LEARN, P685
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600
   Dempe S., 2002, FDN BILEVEL PROGRAMM
   Edwards C. H., 2012, DOVER BOOKS MATH
   Feydy J., 2018, ARXIV E PRINTS
   Fiedler Miroslav, 1972, LINEAR ALGEBRA APPL, V5, P299
   Flamary Remi, 2018, MACHINE LEARNING
   Frogner C., 2015, ADV NEURAL INFORM PR, P2053
   Genevay A., 2018, INT C ART INT STAT, P1608
   Inc Google, QUICKDRAW DAT
   Kollo T., 2006, MATH ITS APPL
   Korba Anna, 2018, ARXIV180702374
   Lin Junhong, 2018, APPL COMPUTATIONAL H
   Moretti V., 2013, UNITEXT
   Osokin Anton, 2017, ADV NEURAL INFORM PR, P302
   Pedregosa Fabian, 2016, ARXIV160202355
   Peyre Gabriel, 2017, TECHNICAL REPORT
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Rolet Antoine, 2016, ARTIF INTELL, P630
   Rudi  Alessandro, 2017, ADV NEURAL INFORM PR, V30, P3888
   Rudi Alessandro, 2018, ADV NEURAL INFORM PR, P5615
   Rudi Alessandro, 2017, ADV NEURAL INFORM PR, V30, P3215
   Schmitz MA, 2018, SIAM J IMAGING SCI, V11, P643, DOI 10.1137/17M1140431
   SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343
   Smola A. J., 2000, SPARSE GREEDY MATRIX
   Steinwart I, 2008, INFORM SCI STAT, P1
   Villani C., 2008, GRUNDLEHREN MATH WIS
   Weston J., 2003, ADV NEURAL INFORM PR, V15, P873
   Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000037
DA 2019-06-15
ER

PT S
AU Lunz, S
   Oktem, O
   Schonlieb, CB
AF Lunz, Sebastian
   Oktem, Ozan
   Schonlieb, Carola-Bibiane
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Adversarial Regularizers in Inverse Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computed tomography reconstruction on the LIDC dataset.
C1 [Lunz, Sebastian; Schonlieb, Carola-Bibiane] Univ Cambridge, DAMTP, Cambridge CB3 0WA, England.
   [Oktem, Ozan] KTH Royal Inst Technol, Dept Math, S-10044 Stockholm, Sweden.
RP Lunz, S (reprint author), Univ Cambridge, DAMTP, Cambridge CB3 0WA, England.
EM lunz@math.cam.ac.uk; ozan@kth.se; cbs31@cam.ac.uk
FU EPSRC [EP/L016516/1]; Cantab Capital Institute for the Mathematics of
   Information; Swedish Foundation for Strategic Research grant
   [AM13-0049]; Leverhulme Trust project on 'Breaking the non-convexity
   barrier', EPSRC grant [EP/M00483X/1]; EPSRC Centre [EP/N014588/1]; RISE
   project CHiPS; RISE project NoMADS; Alan Turing Institute
FX The authors acknowledge the National Cancer Institute and the Foundation
   for the National Institutes of Health, and their critical role in the
   creation of the free publicly available LIDC/IDRI Database used in this
   study. The work by Sebastian Lunz was supported by the EPSRC grant
   EP/L016516/1 for the University of Cambridge Centre for Doctoral
   Training, the Cambridge Centre for Analysis and by the Cantab Capital
   Institute for the Mathematics of Information. The work by Ozan Oktem was
   supported by the Swedish Foundation for Strategic Research grant
   AM13-0049. Carola-Bibiane Schonlieb acknowledges support from the
   Leverhulme Trust project on 'Breaking the non-convexity barrier', EPSRC
   grant Nr. EP/M00483X/1, the EPSRC Centre Nr. EP/N014588/1, the RISE
   projects CHiPS and NoMADS, the Cantab Capital Institute for the
   Mathematics of Information and the Alan Turing Institute.
CR Adler J, 2018, IEEE T MED IMAGING, V37, P1322, DOI 10.1109/TMI.2018.2799231
   Adler J, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa9581
   Arbelaez Pablo, IEEE T PATTERN ANAL, V33
   Argyrou M, 2012, IEEE NUCL SCI CONF R, P3324
   Arjovsky Martin, 2017, INT C MACH LEARN ICM
   Armato Samuel, 2011, MED PHYS, V38
   Benning Martin, 2017, INT C SCAL SPAC VAR
   Bora A., 2017, ARXIV170303208
   Calatroni Luca, 2012, RADON BOOK SERIES, V8
   Chambolle Antonin, 2016, ACTA NUMERICA, V25
   Engl H. W., 1996, REGULARIZATION INVER, V375
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gulrajani Ishaan, 2017, ADV NEURAL INFORM PR
   Hammernik Kerstin, 2018, MAGNETIC RESONANCE M, V79
   Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099
   Knoll Florian, 2011, MAGNETIC RESONANCE M, V65
   Kunisch K, 2013, SIAM J IMAGING SCI, V6, P938, DOI 10.1137/120882706
   Leary R, 2013, ULTRAMICROSCOPY, V131, P70, DOI 10.1016/j.ultramic.2013.03.019
   Meinhardt Tim, 2017, INT C COMP VIS ICCV
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884
   Ronneberger O., 2015, INT C MED IM COMP CO
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Scherzer O, 2009, APPL MATH SCI, V167, P3
   Schlemper Jo, 2017, INT C INF PROC MED I
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie J, 2012, ADV NEURAL INFORM PR
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003010
DA 2019-06-15
ER

PT S
AU Luo, HP
   Wei, CY
   Zheng, K
AF Luo, Haipeng
   Wei, Chen-Yu
   Zheng, Kai
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient Online Portfolio with Logarithmic Regret
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHMS
AB We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret O(N In T) for N financial instruments over T rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of O(N-2 (ln T)(4)), and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time O (TN2.5) per round. The regret of all other existing works is either polynomial in T or has a potentially unbounded factor such as the inverse of the smallest price relative.
C1 [Luo, Haipeng; Wei, Chen-Yu] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Zheng, Kai] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Key Lab Machine Percept,MOE,Sch EECS, Beijing, Peoples R China.
RP Luo, HP (reprint author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM haipeng1@usc.edu; chenyu.wei@usc.edu; zhengk92@pku.edu.cn
FU China Scholarship Council; NSF [1755781]
FX The authors would like to thank Tim van Erven for introducing the
   problem and to thank Tim van Erven, Dirk van der Hoeven, and Wouter
   Koolen for helpful discussions throughout the projects, especially on
   the FTRL approach. The work was done while KZ visited the University of
   Southern California. KZ gratefully acknowledges financial support from
   China Scholarship Council. HL and CYW are grateful for the support of
   NSF Grant #1755781.
CR Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096
   Agarwal  A., 2006, P 23 INT C MACH LEAR, P9
   Agarwal Alekh, 2017, C LEARN THEOR, P12
   Agarwal Amit, 2005, EL C COMP COMPL
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bubeck Sebastien, 2015, ARXIV150702564
   Bubeck Sebastien, 2018, INT C ALG LEARN THEO
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Cover TM, 1996, AN S FDN CO, P534, DOI 10.1109/SFCS.1996.548512
   Foster Dylan J, 2016, ADV NEURAL INFORM PR, P4734
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Helmbold DP, 1998, MATH FINANC, V8, P325, DOI 10.1111/1467-9965.00058
   Kalai A., 2002, J MACHINE LEARNING R, V3, P423
   Lovasz L, 2006, ANN IEEE SYMP FOUND, P57
   Narayanan Hariharan, 2010, ADV NEURAL INFORM PR, P1777
   Nesterov Y., 1994, INTERIOR POINT POLYN, V13
   Orseau Laurent, 2017, P MACHINE LEARNING R, P372
   van Erven Tim, 2018, COMMUNICATION
   Wei Chen-Yu, 2018, C LEARN THEOR
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002075
DA 2019-06-15
ER

PT S
AU Luo, RQ
   Tian, F
   Qin, T
   Chen, EH
   Liu, TY
AF Luo, Renqian
   Tian, Fei
   Qin, Tao
   Chen, Enhong
   Liu, Tie-Yan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Architecture Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 2.11% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 and WildText-2. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 3.53%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.
C1 [Luo, Renqian; Chen, Enhong] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Tian, Fei; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China.
RP Luo, RQ (reprint author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
EM lrq@mail.ustc.edu.cn; fetia@microsoft.com; taoqin@microsoft.com;
   cheneh@ustc.edu.cn; tie-yan.liu@microsoft.com
CR Artetxe  M., 2018, INT C LEARN REPR
   Bahdanau D., 2014, ARXIV14090473
   Baker Bowen, 2018, INT C LEARN REPR
   Baker Bowen, 2017, INT C LEARN REPR
   Bender G., 2018, INT C MACH LEARN, V80, P549
   Brock Andrew, 2018, INT C LEARN REPR
   Cai Han, 2017, ARXIV170704873
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Deng Boyang, 2017, ARXIV171203351
   DeVries T, 2017, ARXIV170804552
   Fahlman S. E., 1990, ADV NEURAL INFORMATI, P524
   Gal Y., 2016, ADV NEURAL INFORM PR, P1019
   Gastaldi Xavier, 2017, CORR
   Grave Edouard, 2016, CORR
   Grosse R., 2012, UNCERTAINTY ARTIFICI, P306
   Han Cai, 2018, ARXIV180602639
   Huang Furong, 2017, ARXIV170604964
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Inan  Hakan, 2016, ARXIV161101462
   Kandasamy  Kirthevasan, 2018, ARXIV180207191
   Kitano H., 1990, Complex Systems, V4, P461
   Krueger D., 2016, ARXIV160601305
   Lample  G., 2018, INT C LEARN REPR
   Le Q., 2014, P 31 INT C MACH LEAR, P1188, DOI DOI 10.1007/978-1-4614-3223-4
   Liu C., 2017, ARXIV171200559
   Liu  H., 2018, ARXIV180609055
   Liu Hanxiao, 2018, INT C LEARN REPR
   Melis Gabor, 2018, INT C LEARN REPR
   Merity S., 2016, ARXIV160907843
   Merity S., 2017, CORR
   Miikkulainen R, 2017, ARXIV170300548
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Pham H., 2018, ARXIV180203268
   Real E., 2017, P 34 INT C MACH LEAR, P2902
   Real E., 2018, ARXIV180201548
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Wu Lijun, 2018, ADV NEURAL INFORM PR, P6465
   Xie LX, 2017, IEEE I CONF COMP VIS, P1388, DOI 10.1109/ICCV.2017.154
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang Fan, 2018, INT C LEARN REPR
   Yang Z., 2018, INT C LEARN REPR
   Zaremba W, 2014, ARXIV14092329
   Zilly J. G., 2017, P 34 INT C MACH LEAR, P4189
   Zoph B., 2016, ARXIV161101578
   Zoph Barret, 2018, P IEEE C COMP VIS PA
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002037
DA 2019-06-15
ER

PT S
AU Luo, R
   Wang, JH
   Yang, YD
   Zhu, ZX
   Wang, J
AF Luo, Rui
   Wang, Jianhong
   Yang, Yaodong
   Zhu, Zhanxing
   Wang, Jun
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for
   Bayesian learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MOLECULAR-DYNAMICS
AB We propose a new sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for Bayesian learning on large datasets and multimodal distributions. It simulates the Nose-Hoover dynamics of a continuously-tempered Hamiltonian system built on the distribution of interest. A significant advantage of this method is that it is not only able to efficiently draw representative i.i.d. samples when the distribution contains multiple isolated modes, but capable of adaptively neutralising the noise arising from mini-batches and maintaining accurate sampling. While the properties of this method have been studied using synthetic distributions, experiments on three real datasets also demonstrated the gain of performance over several strong baselines with various types of neural networks plunged in.
C1 [Luo, Rui; Wang, Jianhong; Yang, Yaodong; Wang, Jun] UCL, London, England.
   [Zhu, Zhanxing] Peking Univ, Beijing, Peoples R China.
RP Wang, J (reprint author), UCL, London, England.
EM j.wang@cs.ucl.ac.uk
CR Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Comer J, 2015, J PHYS CHEM B, V119, P1129, DOI 10.1021/jp506633n
   Darve E, 2001, J CHEM PHYS, V115, P9169, DOI 10.1063/1.1410978
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Feroz F, 2008, MON NOT R ASTRON SOC, V384, P449, DOI 10.1111/j.1365-2966.2007.12353.x
   Gobbo G, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.061301
   Graham Matthew M., 2017, P 33 C UNC ART INT U
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HOOVER WG, 1985, PHYS REV A, V31, P1695, DOI 10.1103/PhysRevA.31.1695
   Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941
   Kingma D. P., 2014, ABS14126980 CORR
   Lelievre T, 2008, NONLINEARITY, V21, P1155, DOI 10.1088/0951-7715/21/6/001
   Lenner N, 2016, J CHEM THEORY COMPUT, V12, P486, DOI 10.1021/acs.jctc.5b00751
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   NOSE S, 1984, J CHEM PHYS, V81, P511, DOI 10.1063/1.447334
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Risken H, 1989, FOKKER PLANCK EQUATI
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Ye  Nanyang, 2017, ADV NEURAL INFORM PR, P618
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005027
DA 2019-06-15
ER

PT S
AU Luo, YH
   Cai, XR
   Zhang, Y
   Xu, J
   Yuan, XJ
AF Luo, Yonghong
   Cai, Xiangrui
   Zhang, Ying
   Xu, Jun
   Yuan, Xiaojie
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multivariate Time Series Imputation with Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MISSING DATA
AB Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.
C1 [Luo, Yonghong; Cai, Xiangrui; Zhang, Ying; Yuan, Xiaojie] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
   [Xu, Jun] Renmin Univ China, Sch Informat, Beijing, Peoples R China.
RP Zhang, Y (reprint author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
EM luoyonghong@dbis.nankai.edu.cn; caixiangrui@dbis.nankai.edu.cn;
   yingzhang@nankai.edu.cn; junxu@ruc.edu.cn; yuanxj@nankai.edu.cn
FU National Natural Science Foundation of China [61772289, 61872338]
FX We thank the reviewers for their constructive comments. We also thank
   Zhicheng Dou for his helpful suggestions. This research is supported by
   National Natural Science Foundation of China (No. 61772289 and No.
   61872338).
CR Acuna E, 2004, ST CLASS DAT ANAL, P639
   Amiri M, 2016, NEUROCOMPUTING, V205, P152, DOI 10.1016/j.neucom.2016.04.015
   [Anonymous], 2013, MATLAB REL 2013A, P488
   Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Batista GEAPA, 2003, APPL ARTIF INTELL, V17, P519, DOI 10.1080/08839510390219309
   Bora Ashish, 2018, INT C LEARN REPR ICL
   Che T., 2017, ARXIV170207983
   Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9
   Cheema JR, 2014, REV EDUC RES, V84, P487, DOI 10.3102/0034654314532697
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Donders ART, 2006, J CLIN EPIDEMIOL, V59, P1087, DOI 10.1016/j.jclinepi.2006.01.014
   Fedus W., 2018, ARXIV180107736
   Garcia-Laencina PJ, 2015, COMPUT BIOL MED, V59, P125, DOI 10.1016/j.compbiomed.2015.02.006
   Garcia-Laencina PJ, 2010, NEURAL COMPUT APPL, V19, P263, DOI 10.1007/s00521-009-0295-6
   Gheyas IA, 2010, NEUROCOMPUTING, V73, P3039, DOI 10.1016/j.neucom.2010.06.021
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graham JW, 2009, ANNU REV PSYCHOL, V60, P549, DOI 10.1146/annurev.psych.58.110405.085530
   Hastie T, 2015, J MACH LEARN RES, V16, P3367
   Heusel M., 2017, ARXIV170608500
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Horvitz Eric, 2017, AAAI, P4953
   Hsieh TJ, 2011, APPL SOFT COMPUT, V11, P2510, DOI 10.1016/j.asoc.2010.09.007
   Huang X., 2017, IEEE C COMP VIS PATT, V2, P4
   Ioffe S., 2015, ARXIV150203167
   Johnson AEW, 2014, COMPUT CARDIOL, V41, P157
   Kaiser J, 2014, J SYSTEMS INTEGRATIO, V5, P42, DOI DOI 10.20470/JSI.V5I1.178
   Kantardzic  M., 2011, DATA MINING CONCEPTS
   Ledig Christian, 2016, PHOTOREALISTIC SINGL
   Li Y., 2017, P IEEE C COMP VIS PA, V1, P6
   Liu Pengpeng, 2017, ARXIV171109345
   Liu Shuang, 2017, ADV NEURAL INFORM PR, P5551
   Lu Z., 2017, ARXIV170604717
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   MCKNIGHT P.E., 2007, MISSING DATA GENTLE
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Nagarajan Vaishnavh, 2017, ADV NEURAL INFORM PR, P5591
   Nelwamondo FV, 2007, CURR SCI INDIA, V93, P1514
   Rajeswar Sai, 2017, ARXIV170510929
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Silva I, 2012, COMPUT CARDIOL, V39, P245
   Silva LO, 2014, INTELL DATA ANAL, V18, P1177, DOI 10.3233/IDA-140690
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wothke Werner, 2000, LONGITUDINAL MULTIGR
   Yoon Jinsung, 2018, ARXIV180602920
   Yu L., 2017, AAAI, P2852
   Zheng KP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2171, DOI 10.1145/3097983.3098149
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301057
DA 2019-06-15
ER

PT S
AU Luo, YC
   Tian, T
   Shi, JX
   Zhu, J
   Zhang, B
AF Luo, Yucen
   Tian, Tian
   Shi, Jiaxin
   Zhu, Jun
   Zhang, Bo
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Semi-crowdsourced Clustering with Deep Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the semi-supervised clustering problem where crowdsourcing provides noisy information about the pairwise comparisons on a small subset of data, i.e., whether a sample pair is in the same cluster. We propose a new approach that includes a deep generative model (DGM) to characterize low-level features of the data, and a statistical relational model for noisy pairwise annotations on its subset. The two parts share the latent variables. To make the model automatically trade-off between its complexity and fitting data, we also develop its fully Bayesian variant. The challenge of inference is addressed by fast (natural-gradient) stochastic variational inference algorithms, where we effectively combine variational message passing for the relational part and amortized learning of the DGM under a unified framework. Empirical results on synthetic and real-world datasets show that our model outperforms previous crowdsourced clustering methods.
C1 [Luo, Yucen; Tian, Tian; Shi, Jiaxin; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Tech, Inst AI, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, Dept Comp Sci & Tech, Inst AI, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China.
EM luoyc15@mails.tsinghua.edu.cn; rossowhite@163.com;
   shijx15@mails.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn;
   dcszb@mail.tsinghua.edu.cn
FU National Key Research and Development Program of China [2017YFA0700904];
   NSFC [61620106010, 61621136008, 61332007]; Beijing NSF Project
   [L172037]; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL
   Program; Siemens; NEC; Intel
FX Yucen Luo would like to thank Matthew Johnson for helpful discussions on
   the SVAE algorithm [13], and Yale Chang for sharing the code of the UCI
   benchmark experiments. We thank the anonymous reviewers for feedbacks
   that greatly improved the paper. This work was supported by the National
   Key Research and Development Program of China (No. 2017YFA0700904), NSFC
   Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project
   (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA
   NVAIL Program, and the projects from Siemens, NEC and Intel.
CR Bilenko M., 2004, P 21 INT C MACH LEAR, P11
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chang Yale, 2017, INT C MACH LEARN, P674
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Dheeru D., 2017, UCI MACHINE LEARNING
   Gomes R. G., 2011, NIPS, P558
   Gopalan P., 2012, ADV NEURAL INFORM PR, P2249
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Howe J, 2006, WIRED MAGAZINE, V14, P1, DOI DOI 10.1086/599595
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Luo Yucen, 2018, IEEE C COMP VIS PATT
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Ren Y., 2016, P NIPS, P2928
   Rezende D. J., 2016, P MACHINE LEARNING R, V48, P1521
   Shi J., 2017, ARXIV170905870
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735
   Tian T., 2015, ADV NEURAL INFORM PR, P1621
   Vinayak Ramya Korlakai, 2016, NEURAL INFORM PROCES
   Welinder P., 2010, ADV NEURAL INFORM PR, P2424
   Winn J, 2005, J MACH LEARN RES, V6, P661
   Wiwie C, 2015, NAT METHODS, V12, P1033, DOI [10.1038/NMETH.3583, 10.1038/nmeth.3583]
   Wu Lin, 2018, INT C LEARN REPR
   Xie Junyuan, 2016, INT C MACH LEARN, P478
   Xing Eric P, 2003, ADV NEURAL INFORM PR, P521
   Yi J., 2012, ADV NEURAL INFORM PR, P1772
   Zhou D., 2014, ICML, P262
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303023
DA 2019-06-15
ER

PT S
AU Lyddon, S
   Walker, S
   Holmes, C
AF Lyddon, Simon
   Walker, Stephen
   Holmes, Chris
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Nonparametric learning from Bayesian models with randomized objective
   functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID INFERENCE; LIKELIHOOD
AB Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.
C1 [Lyddon, Simon; Holmes, Chris] Univ Oxford, Dept Stat, Oxford, England.
   [Walker, Stephen] Univ Texas Austin, Dept Math, Austin, TX 78712 USA.
RP Lyddon, S (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM lyddon@stats.ox.ac.uk; s.g.walker@math.utexas.edu;
   cholmes@stats.ox.ac.uk
FU EPSRC OxWaSP CDT [EP/L016710/1]; MRC; Alan Turing Institute; Li Ka Shing
   foundation
FX SL is funded by the EPSRC OxWaSP CDT, through EP/L016710/1. CH
   gratefully acknowledges support for this research from the MRC, The Alan
   Turing Institute, and the Li Ka Shing foundation.
CR AKAIKE H, 1981, J ECONOMETRICS, V16, P3, DOI 10.1016/0304-4076(81)90071-3
   ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871
   Bernardo Jose M., 2006, BAYESIAN THEORY
   Bishop C. M., 2006, INFORM SCI STAT
   Bissiri PG, 2016, J R STAT SOC B, V78, P1103, DOI 10.1111/rssb.12158
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Burnham KP, 2003, MODEL SELECTION MULT
   Chamberlain G, 2003, J BUS ECON STAT, V21, P12, DOI 10.1198/073500102288618711
   Dheeru D., 2017, UCI MACHINE LEARNING
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   Fushiki T, 2005, BERNOULLI, V11, P747, DOI 10.3150/bj/1126126768
   Fushiki T, 2010, J STAT PLAN INFER, V140, P65, DOI 10.1016/j.jspi.2009.06.007
   Hjort N., 2010, CAMBRIDGE SERIES STA
   Huber P. J., 1967, P 5 BERK S MATH STAT, V1, P221
   Jaakkola Tommi S., 1997, INT WORKSH ART INT S
   Kucukelbir  Alp, 2015, ADV NEURAL INFORM PR, V1, P1
   LO AY, 1984, ANN STAT, V12, P351, DOI 10.1214/aos/1176346412
   Lyddon S. P., 2018, 170907616 ARXIV, P1
   Muller UK, 2013, ECONOMETRICA, V81, P1805, DOI 10.3982/ECTA9097
   NEWTON MA, 1994, J R STAT SOC B, V56, P3
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Robert C. P., 2005, SPRINGER TEXTS STAT
   Robert Christian P., 2007, SPRINGER TEXTS STAT
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Walker SG, 2013, J STAT PLAN INFER, V143, P1621, DOI 10.1016/j.jspi.2013.05.013
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302011
DA 2019-06-15
ER

PT S
AU Ma, FC
   Ayaz, U
   Karaman, S
AF Ma, Fangchang
   Ayaz, Ulas
   Karaman, Sertac
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Invertibility of Convolutional Generative Networks from Partial
   Measurements
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The problem of inverting generative neural networks (i.e., to recover the input latent code given partial network output), motivated by image inpainting, has recently been studied by a prior work that focused on fully-connected networks. In this work, we present new theoretical results on convolutional networks, which are more widely used in practice. The network inversion problem is highly non-convex, and hence is typically computationally intractable and without optimality guarantees. However, we rigorously prove that, for a 2-layer convolutional generative network with ReLU and Gaussian-distributed random weights, the input latent code can be deduced from the network output efficiently using simple gradient descent. This new theoretical finding implies that the mapping from the low-dimensional latent space to the high-dimensional image space is one-to-one, under our assumptions. In addition, the same conclusion holds even when the network output is only partially observed (i.e., with missing pixels). We further demonstrate, empirically, that the same conclusion extends to networks with multiple layers, other activation functions (leaky ReLU, sigmoid and tanh), and weights trained on real datasets.
C1 [Ma, Fangchang; Ayaz, Ulas; Karaman, Sertac] MIT, Cambridge, MA 02139 USA.
RP Ma, FC (reprint author), MIT, Cambridge, MA 02139 USA.
EM fcma@mit.edu; uayaz@mit.edu; sertac@mit.edu
CR Arora Sanjeev, ICLR WORKSH
   Bora A., 2017, ARXIV170303208
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Du Simon S., 2017, ARXIV171200779
   Gilbert A. C., 2017, ARXIV170508664
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hand Paul, 2017, ARXIV170507576
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kingma D. P., 2013, ARXIV13126114
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu  Z., 2015, P INT C COMP VIS ICC
   Ma Fangchang, 2018, NIPS
   Ma Fangchang, 2017, ARXIV170301398
   Ma Fangchang, 2017, ARXIV170907492
   Mirza M., 2014, ARXIV14111784
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Radford  A., 2015, ARXIV151106434
   Soltanolkotabi Mahdi, 2017, NIPS, V30, P2004
   Srivastava A., 2017, ADV NEURAL INFORM PR, P3310
   Yeh Raymond A, 2017, P IEEE C COMP VIS PA, P5485
   Zhu  J.-Y., 2016, P EUR C COMP VIS ECC
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004021
DA 2019-06-15
ER

PT S
AU Ma, TF
   Chen, J
   Xiao, C
AF Ma, Tengfei
   Chen, Jie
   Xiao, Cao
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Constrained Generation of Semantically Valid Graphs via Regularizing
   Variational Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For example, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.
C1 [Ma, Tengfei; Chen, Jie; Xiao, Cao] IBM Res, New York, NY 10598 USA.
RP Ma, TF (reprint author), IBM Res, New York, NY 10598 USA.
EM Tengfei.Ma1@ibm.com; chenjie@us.ibm.com; cxiao@us.ibm.com
CR Arjovsky Martin, 2017, ICML 17
   Arjovsky Martin, 2017, ICLR 17
   Arora Sanjeev, 2017, ICML 17
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bjerrum Esben Jannik, 2017, CORR
   Bojchevski Aleksandar, 2018, ICML
   Burda Yuri, 2015, CORR
   Dai Hanjun, 2018, INT C LEARN REPR
   Defferrard Michael, 2016, CORR
   Dieleman S., 2016, CORR
   Duvenaud David K, 2015, NIPS 15
   Erdos P., 1960, PUBL MATH I HUNG, V5, P17, DOI DOI 10.2307/1999405
   Gaunt Alexander L., 2016, CORR
   Gilmer J., 2017, ICML
   Gomez-Bombarelli R, 2016, CORR
   Goodfellow Ian, 2014, NIPS 14
   Guo Xiaojie, 2018, ARXIV180509980
   Hu Zhiting, 2017, P 34 INT C MACH LEAR
   Hu Zhiting, 2017, CORR
   Ioffe S., 2015, ARXIV150203167
   Irwin John J., 2012, J CHEM INFORM MODELI, V52
   Jaques Natasha, 2016, CORR
   Jin Wengong, 2018, CORR
   Johnson D. D., 2017, ICLR
   Kalchbrenner N., 2016, NIPS
   Kingma D. P., 2013, CORR
   Kipf T. N., 2016, CORR
   Kusner M. J., 2017, P 34 INT C MACH LEAR, P1945
   Li Yujia, 2015, ICML 15
   Li Yujia, 2018, LEARNING DEEP GENERA
   Nowozin Sebastian, 2016, NIPS
   Radford A., 2015, CORR
   Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22
   Salimans T., 2016, NIPS
   Scarselli F., 2009, IEEE T NEURAL NETWOR, V20
   Segler M. H. S., 2017, CORR
   Simonovsky Martin, 2018, GRAPHVAE GENERATION
   Tavakoli Sahar, 2017, INT C SOC COMP BEH C
   Vondrick Carl, 2016, CORR
   WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005
   Xi Chen, 2016, CORR
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001064
DA 2019-06-15
ER

PT S
AU MacKay, M
   Vicol, P
   Ba, J
   Grosse, R
AF MacKay, Matthew
   Vicol, Paul
   Ba, Jimmy
   Grosse, Roger
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Reversible Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs-RNNs for which the hidden-to-hidden transition can be reversed-offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We first show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. Our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10-15. We extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation memory cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder.
C1 [MacKay, Matthew; Vicol, Paul; Ba, Jimmy; Grosse, Roger] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP MacKay, M (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM mmackay@cs.toronto.edu; pvicol@cs.toronto.edu; jba@cs.toronto.edu;
   rgrosse@cs.toronto.edu
FU NSERC CGS-M award; NSERC PGS-D award
FX We thank Kyunghyun Cho for experimental advice and discussion. We also
   thank Aidan Gomez, Mengye Ren, Gennady Pekhimenko, and David Duvenaud
   for helpful discussion. MM is supported by an NSERC CGS-M award, and PV
   is supported by an NSERC PGS-D award.
CR Abadi M., 2016, ARXIV160304467
   Al-Rfou R., 2016, ARXIV160502688
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Bahdanau D., 2014, ARXIV14090473
   Cettolo Mauro, 2016, P 13 INT WORKSH SPOK
   Chen Tianqi, 2016, ARXIV160406174
   Cho K, 2014, ARXIV14061078
   Courbariaux M., 2014, ARXIV14127024
   Dinh L., 2016, ARXIV160508803
   Elliott Desmond, 2016, ARXIV160500459
   Gers F. A, 1999, LEARNING FORGET CONT
   Gomez A. N., 2017, ADV NEURAL INFORM PR, P2211
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Gruslys Audrunas, 2016, ADV NEURAL INFORM PR, P4125
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Helcl Jindrich, 2017, ARXIV170707631
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jaderberg Max, 2017, INT C MACH LEARN ICM
   Kingma Diederik P, 2016, ADV NEURAL INFORM PR, P4743
   Li Jing, 2016, ARXIV161205231
   Luong M.T., 2015, ARXIV150804025
   Maclaurin Dougal, 2015, P 32 INT C MACH LEAR
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Marian Czarnecki Wojciech, 2017, ARXIV170300522
   Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27
   Melis G., 2017, ARXIV170705589
   Merity S., 2017, ARXIV170802182
   Merity S., 2016, ARXIV160907843
   Papamakarios G., 2017, ADV NEURAL INFORM PR, P2335
   Pascanu R., 2013, ARXIV13126026
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wisdom S., 2016, ADV NEURAL INFORM PR, P4880
   Wu Y., 2016, ARXIV160908144
   Zaremba W., 2014, ARXIV14104615
   Zilly J. G., 2016, ARXIV160703474
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003057
DA 2019-06-15
ER

PT S
AU Madras, D
   Pitassi, T
   Zemel, R
AF Madras, David
   Pitassi, Toniann
   Zemel, Richard
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Predict Responsibly: Improving Fairness and Accuracy by Learning to
   Defer
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing learning to defer, which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.
C1 [Madras, David; Pitassi, Toniann; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada.
RP Madras, D (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM madras@cs.toronto.edu; toni@cs.toronto.edu; zemel@cs.toronto.edu
CR Attenberg Josh, 2011, HUMAN COMPUTATION, V11
   Bechavod Yahav, 2017, ARXIV170700044CSSTAT
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Bower Amanda, 2017, ARXIV170700391CSSTAT
   Burrell J, 2016, BIG DATA SOC, V3, P1, DOI 10.1177/2053951715622512
   Busenitz LW, 1997, J BUS VENTURING, V12, P9, DOI 10.1016/S0883-9026(96)00003-1
   Chouldechova A, 2017, BIG DATA-US, V5, P153, DOI 10.1089/big.2016.0047
   Chow C., 1957, IEEE T C
   Chow C., 1970, IEEE T C
   Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5
   Danziger S, 2011, P NATL ACAD SCI USA, V108, P6889, DOI 10.1073/pnas.1018033108
   DAWES RM, 1989, SCIENCE, V243, P1668, DOI 10.1126/science.2648573
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fischer Lydia, 2016, PROBABILISTIC CLASSI
   Grgic-Hlaca Nina, 2017, ARXIV170610208CSSTAT
   Guo C., 2017, P 34 INT C MACH LEAR, P1321
   Hadfield-Menell Dylan, 2016, ADV NEURAL INFORM PR, P3909
   Hannah-Moffat K, 2013, JUSTICE Q, V30, P270, DOI 10.1080/07418825.2012.682603
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Joseph Matthew, 2016, ADV NEURAL INFORM PR, V29, P325
   Kamiran Faisal, 2009, INT C COMP CONTR COM, P1, DOI DOI 10.1109/IC4.2009.4909197
   Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3
   Kingma D. P., 2014, ARXIV14126980
   Kirchner Lauren, 2016, WE ANAL COMPAS RECID
   Kleinberg Jon, 2016, ARXIV160905807CSSTAT
   Maddison Chris J, 2016, ARXIV161100712
   Pleiss Geoff, 2017, P C ADV NEUR INF PRO, P5680
   Selbst Andrew D, 2018, ACM C FAIRN ACC TRAN
   Varshney KR, 2017, BIG DATA-US, V5, P246, DOI 10.1089/big.2016.0051
   Williamson Robert C, 2018, C FAIRN ACC TRANSP, P107
   Xin Wang, 2017, ARXIV170600885CS
   Zafar MB, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1171, DOI 10.1145/3038912.3052660
   Zemel R., 2013, JMLR P, P325
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000063
DA 2019-06-15
ER

EF
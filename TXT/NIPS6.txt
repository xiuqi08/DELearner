FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Sukhbaatar, S
   Szlam, A
   Fergus, R
AF Sukhbaatar, Sainbayar
   Szlam, Arthur
   Fergus, Rob
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Multiagent Communication with Backpropagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.
C1 [Sukhbaatar, Sainbayar] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
   [Szlam, Arthur; Fergus, Rob] Facebook AI Res, New York, NY USA.
RP Sukhbaatar, S (reprint author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
EM sainbar@cs.nyu.edu; aszlam@fb.com; robfergus@fb.com
FU CIFAR
FX The authors wish to thank Daniel Lee and Y-Lan Boureau for their advice
   and guidance. Rob Fergus is grateful for the support of CIFAR.
CR Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061
   Crites RH, 1998, MACH LEARN, V33, P235, DOI 10.1023/A:1007518724497
   Foerster J. N., 2016, ABS160202672 ARXIV
   Fox D, 2000, AUTON ROBOT, V8, P325, DOI 10.1023/A:1008937911390
   Giles CL, 2002, LECT NOTES ARTIF INT, V2564, P377
   Guestrin C., 2001, NIPS
   Guo  Xiaoxiao, 2014, NIPS
   Kaiser Lukasz, 2016, ICLR
   Kasai Tatsuya, 2008, 2008 IEEE Conference on Soft Computing in Industrial Applications. SMCia/08, P1, DOI 10.1109/SMCIA.2008.5045926
   Kingma D. P., 2015, ICLR
   Lauer M., 2000, ICML
   Levine S, 2016, J MACH LEARN RES, V17
   Li Y., 2015, ICLR
   Littman M. L., 2001, Cognitive Systems Research, V2, P55, DOI 10.1016/S1389-0417(01)00015-8
   Maddison C. J., 2015, ICLR
   Maravall D, 2013, ROBOT AUTON SYST, V61, P661, DOI 10.1016/j.robot.2012.09.016
   Mataric MJ, 1997, AUTON ROBOT, V4, P73, DOI 10.1023/A:1008819414322
   Melo F. S., 2011, MULTIAGENT SYSTEMS, P189
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293
   Pearl J., 1982, AAAI
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stone P., 1998, INT J HUMAN COMPUTER
   Sukhbaatar S., 2015, NIPS
   Sukhbaatar Sainbayar, 2015, CORR
   Sutton R., 1998, INTRO REINFORCEMENT
   Tampuu Ardi, 2015, ARXIV151108779
   Tan M., 1993, ICML
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Varshavskaya P, 2009, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS 8, P367, DOI 10.1007/978-3-642-00644-9_33
   Wang X., 2002, ADV NEURAL INFORM PR, P1571
   Weston J., 2016, ICLR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xiong C, 2016, ICML
   Zhang Y, 2013, 2013 INTERNATIONAL CONFERENCE ON MECHANICAL AND AUTOMATION ENGINEERING (MAEE 2013), P110, DOI 10.1109/MAEE.2013.37
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703034
DA 2019-06-15
ER

PT S
AU Sumbul, U
   Roossien, D
   Chen, F
   Barry, N
   Boyden, ES
   Cai, DW
   Cunningham, JP
   Paninski, L
AF Sumbul, Uygar
   Roossien, Douglas, Jr.
   Chen, Fei
   Barry, Nicholas
   Boyden, Edward S.
   Cai, Dawen
   Cunningham, John P.
   Paninski, Liam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Automated scalable segmentation of neurons from multispectral images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RECONSTRUCTION; VISUALIZATION
AB Reconstruction of neuroanatomy is a fundamental problem in neuroscience. Stochastic expression of colors in individual cells is a promising tool, although its use in the nervous system has been limited due to various sources of variability in expression. Moreover, the intermingled anatomy of neuronal trees is challenging for existing segmentation algorithms. Here, we propose a method to automate the segmentation of neurons in such (potentially pseudo-colored) images. The method uses spatio-color relations between the voxels, generates supervoxels to reduce the problem size by four orders of magnitude before the final segmentation, and is parallelizable over the supervoxels. To quantify performance and gain insight, we generate simulated images, where the noise level and characteristics, the density of expression, and the number of fluorophore types are variable. We also present segmentations of real Brainbow images of the mouse hippocampus, which reveal many of the dendritic segments.
C1 [Sumbul, Uygar; Cunningham, John P.; Paninski, Liam] Columbia Univ, Grossman Ctr Stat Mind, New York, NY 10027 USA.
   [Sumbul, Uygar; Cunningham, John P.; Paninski, Liam] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Roossien, Douglas, Jr.; Cai, Dawen] Univ Michigan, Sch Med, Ann Arbor, MI 48109 USA.
   [Chen, Fei; Barry, Nicholas; Boyden, Edward S.] MIT, Media Lab, Cambridge, MA 02139 USA.
   [Chen, Fei; Barry, Nicholas; Boyden, Edward S.] MIT, McGovern Inst, Cambridge, MA 02139 USA.
RP Sumbul, U (reprint author), Columbia Univ, Grossman Ctr Stat Mind, New York, NY 10027 USA.; Sumbul, U (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.
FU ARO MURI [W911NF-12-1-0594]; DARPA [N66001-15-C-4032]; Google Faculty
   Research award; Intelligence Advanced Research Projects Activity (IARPA)
   via Department of Interior/ Interior Business Center (DoI/IBC)
   [D16PC00008]
FX Funding for this research was provided by ARO MURI W911NF-12-1-0594,
   DARPA N66001-15-C-4032 (SIMPLEX), and a Google Faculty Research award;
   in addition, this work was supported by the Intelligence Advanced
   Research Projects Activity (IARPA) via Department of Interior/ Interior
   Business Center (DoI/IBC) contract number D16PC00008. The U.S.
   Government is authorized to reproduce and distribute reprints for
   Governmental purposes notwithstanding any copyright annotation thereon.
   Disclaimer: The views and conclusions contained herein are those of the
   authors and should not be interpreted as necessarily representing the
   official policies or endorsements, either expressed or implied, of
   IARPA, DoI/IBC, or the U.S. Government.
CR BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   BERTRAND G, 1994, PATTERN RECOGN LETT, V15, P169, DOI 10.1016/0167-8655(94)90046-9
   Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344
   Cai D, 2013, NAT METHODS, V10, P540, DOI [10.1038/NMETH.2450, 10.1038/nmeth.2450]
   Chen F, 2015, SCIENCE, V347, P543, DOI 10.1126/science.1260088
   Chen KH, 2015, SCIENCE, V348, DOI 10.1126/science.aaa6090
   Chung K, 2013, NAT METHODS, V10, P508, DOI [10.1038/NMETH.2481, 10.1038/nmeth.2481]
   Cousty J, 2009, IEEE T PATTERN ANAL, V31, P1362, DOI 10.1109/TPAMI.2008.173
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   JAIN V, 2010, CVPR, P2488
   Kim JS, 2014, NATURE, V509, P331, DOI 10.1038/nature13240
   Kurihara K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2796
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee JH, 2014, SCIENCE, V343, P1360, DOI 10.1126/science.1250212
   Livet J, 2007, NATURE, V450, P56, DOI 10.1038/nature06293
   Longair MH, 2011, BIOINFORMATICS, V27, P2453, DOI 10.1093/bioinformatics/btr390
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Marblestone A H, 2014, ARXIV14045103
   MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4
   Meyer F, 1994, COMP IMAG VIS, V2, P77
   Miller J. W., 2013, ADV NEURAL INFORM PR, P199
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Peng HC, 2010, NAT BIOTECHNOL, V28, P348, DOI 10.1038/nbt.1612
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Rust MJ, 2006, NAT METHODS, V3, P793, DOI 10.1038/nmeth929
   Sumbul  U, 2014, FRONTIERS NEUROSCIEN
   Sumbul  U, 2014, NATURE COMMUNICATION, V5
   Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673
   Zador AM, 2012, PLOS BIOL, V10, DOI 10.1371/journal.pbio.1001411
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704073
DA 2019-06-15
ER

PT S
AU Suzuki, T
   Kanagawa, H
   Kobayash, H
   Shimizu, N
   Tagami, Y
AF Suzuki, Taiji
   Kanagawa, Heishiro
   Kobayash, Hayato
   Shimizu, Nobuyuki
   Tagami, Yukihiro
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Minimax Optimal Alternating Minimization for Kernel Nonparametric Tensor
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DECOMPOSITIONS
AB We investigate the statistical performance and computational efficiency of the alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between multimodal data sources. In addition to a linear model, a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider an alternating minimization procedure for a general nonlinear model where the true function consists of components in a reproducing kernel Hilbert space (RKHS). In this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.
C1 [Suzuki, Taiji; Kanagawa, Heishiro] Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo, Japan.
   [Suzuki, Taiji] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.
   [Suzuki, Taiji] RIKEN, Ctr Adv Integrated Intelligence Res, Wako, Saitama, Japan.
   [Kobayash, Hayato; Shimizu, Nobuyuki; Tagami, Yukihiro] Yahoo Japan Corp, Tokyo, Japan.
RP Suzuki, T (reprint author), Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo, Japan.; Suzuki, T (reprint author), Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.; Suzuki, T (reprint author), RIKEN, Ctr Adv Integrated Intelligence Res, Wako, Saitama, Japan.
EM s-taiji@is.titech.ac.jp; kanagawa.h.ab@m.titech.ac.jp
FU MEXT kakenhi [25730013, 25120012, 26280009, 15H01678, 15H05707];
   JST-PRESTO; JST-CREST
FX This work was partially supported by MEXT kakenhi (25730013, 25120012,
   26280009, 15H01678 and 15H05707), JST-PRESTO and JST-CREST.
CR Aswani A., 2014, ARXIV14120620
   Bahadori M. T., ADV NEURAL INFORM PR, V27
   Barak B., 2015, ARXIV150106521
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Bennett C., 1988, INTERPOLATION OPERAT
   Bhojanapalli S., 2015, ARXIV150205023
   Blanca V.-G., 2011, P 3 WORKSH CONT AW R
   Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Hitchcock F. L., 1927, J MATH PHYS, V6, P164, DOI DOI 10.1002/SAPM192761164
   Hitchcock FL, 1927, J MATH PHYS, V7, P39, DOI DOI 10.1002/SAPML9287139
   Imaizumi M., 2016, INT C MACH LEARN ICM
   Jain P., 2014, ADV NEURAL INFORM PR, P1431
   Kanagawa H., 2016, INT C MACH LEARN ICM, P1632
   Karatzoglou A, 2010, P 4 ACM C REC SYST, P79, DOI DOI 10.1145/1864708.1864727
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Ledoux M., 2005, MATH SURVEYS MONOGRA, V89
   Liu J, 2009, IEEE I CONF COMP VIS, P2114
   Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1
   Romera-Paredes B., 2013, P 30 INT C MACH LEAR, V28, P1444
   Shah  P., 2015, ARXIV150504085
   Shen WN, 2016, BERNOULLI, V22, P396, DOI 10.3150/14-BEJ663
   Signoretto M., 2010, 10186 ESATSISTA KU L
   Signoretto M., 2013, ABS13104977 CORR
   Steinwart I., 2009, C LEARNING THEORY CO, P79
   Steinwart I, 2008, INFORM SCI STAT, P1
   Sun Wei, 2015, Adv Neural Inf Process Syst, V28, P1081
   Suzuki T., 2015, P 32 INT C MACH LEAR, P1273
   Tomioka R., 2013, P ADV NEUR INF PROC, P1331
   Tomioka R, 2011, ADV NEURAL INFORM PR, V24, P972
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   VAN DE GEER S. A., 2000, EMPIRICAL PROCESSES
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   Wimalawarne K., 2014, ADV NEURAL INFORM PR, V27, P2825
   Xu Z., 2011, ABS11086296 CORR
   Zhang Z., 2015, ARXIV150204689
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703055
DA 2019-06-15
ER

PT S
AU Syrgkanis, V
   Luo, HP
   Krishnamurthy, A
   Schapire, RE
AF Syrgkanis, Vasilis
   Luo, Haipeng
   Krishnamurthy, Akshay
   Schapire, Robert E.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a new oracle-based algorithm, BISTRO+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order O((KT)(2/3) (log N)(1/3)), where K is the number of actions, T is the number of iterations, and N is the number of baseline policies. Our result is the first to break the O(T-3/4) barrier achieved by recent algorithms, which was left as a major open problem. Our analysis employs the recent relaxation framework of Rakhlin and Sridharan [7].
C1 [Syrgkanis, Vasilis; Luo, Haipeng; Schapire, Robert E.] Microsoft Res, Cambridge, MA 02142 USA.
   [Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA.
RP Syrgkanis, V (reprint author), Microsoft Res, Cambridge, MA 02142 USA.
EM vasy@microsoft.com; haipeng@microsoft.com; akshay@cs.umass.edu;
   schapire@microsoft.com
CR Agarwal Alekh, 2014, INT C MACH LEARN ICM
   Auer Peter, 1995, FDN COMPUTER SCI FOC
   Cesa-Bianchi Nicolo, 1997, J ACM JACM
   Dudik Miroslav, 2011, UNCERTAINTY ARTIFICI
   Freund Yoav, 1997, J COMPUTER SYSTEM SC
   Langford  John, 2008, ADV NEURAL INFORM PR
   Rakhlin Alexander, 2016, INT C MACH LEARN ICM
   Syrgkanis Vasilis, 2016, INT C MACH LEARN ICM
NR 8
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703036
DA 2019-06-15
ER

PT S
AU Tamar, A
   Wu, Y
   Thomas, G
   Levine, S
   Abbeel, P
AF Tamar, Aviv
   Wu, Yi
   Thomas, Garrett
   Levine, Sergey
   Abbeel, Pieter
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Value Iteration Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.
C1 [Tamar, Aviv; Wu, Yi; Thomas, Garrett; Levine, Sergey; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Tamar, A (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
FU Siemens; ONR; Army Research Office; NSF CAREER; Viterbi Scholarship,
   Technion; DARPA PPAML program [FA8750-14-C-0011]
FX This research was funded in part by Siemens, by ONR through a PECASE
   award, by the Army Research Office through the MAST program, and by an
   NSF CAREER grant. A. T. was partially funded by the Viterbi Scholarship,
   Technion. Y. W. was partially funded by a DARPA PPAML program, contract
   FA8750-14-C-0011.
CR BELLMAN R, 1957, DYNAMIC PROGRAMMING
   Bertsekas D. P., 2012, DYNAMIC PROGRAMMING, V2
   Ciregan D., 2012, PROC CVPR IEEE, P3642, DOI [10.1109/CVPR.2012.6248110, DOI 10.1109/CVPR.2012.6248110]
   Deisenroth M., 2011, ICML
   Duan Y., 2016, ARXIV160406778
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Finn  C., 2016, GUIDED POLICY SEARCH
   Fukushima Kunihiko, 1979, T IECE JAPAN A, V62, P658
   Giusti A., 2016, IEEE ROBOTICS AUTOMA
   Guo  Xiaoxiao, 2014, NIPS
   Guo Xiaoxiao, 2016, ARXIV160407095
   Ilin R., 2007, ADPRL
   Joseph J., 2013, ICRA
   Kaelbling LP, 2011, IEEE INT CONF ROBOT, P1470
   Krizhevsky A., 2012, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levine S., 2014, NIPS
   Levine  S., 2016, JMLR, V17
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mnih V., 2016, ARXIV160201783
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Neu G., 2007, UAI
   Nogueira R., 2016, ARXIV160202261
   Ross Stephane, 2011, AISTATS
   Schmidhuber J., 1990, INT JOINT C NEUR NET
   Schulman John, 2015, ICML
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Theano Development Team, 2016, ARXIV E PRINTS
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Watter M, 2015, NIPS
   Xu K, 2015, ICML
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700011
DA 2019-06-15
ER

PT S
AU Tan, CH
   Ma, SQ
   Dai, YH
   Qian, YQ
AF Tan, Conghui
   Ma, Shiqian
   Dai, Yu-Hong
   Qian, Yuqiu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Barzilai-Borwein Step Size for Stochastic Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization methods, the common practice in SGD is either to use a diminishing step size, or to tune a step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.
C1 [Tan, Conghui; Ma, Shiqian] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Dai, Yu-Hong] Chinese Acad Sci, Beijing, Peoples R China.
   [Qian, Yuqiu] Univ Hong Kong, Hong Kong, Peoples R China.
RP Tan, CH (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM chtan@se.cuhk.edu.hk; sqma@se.cuhk.edu.hk; dyh@lsec.cc.ac.cn;
   qyq790@cannect.hku.hk
FU Hong Kong Research Grants Council General Research Fund [14205314];
   Chinese NSF [11631013, 11331012]; National 973 Program of China
   [2015CB856000]
FX Research of Shiqian Ma was supported in part by the Hong Kong Research
   Grants Council General Research Fund (Grant 14205314). Research of
   Yu-Hong Dai was supported by the Chinese NSF (Nos. 11631013 and
   11331012) and the National 973 Program of China (No. 2015CB856000).
CR Babanezhad R., 2015, ADV NEURAL INFORM PR, P2242
   BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141
   Dai YH, 2005, NUMER MATH, V100, P21, DOI 10.1007/s00211-004-0569-y
   Dai YH, 2002, IMA J NUMER ANAL, V22, P1, DOI 10.1093/imanum/22.1.1
   Dai YH, 2006, IMA J NUMER ANAL, V26, P604, DOI [10.1093/imanum/dri006, 10.1093/imanum/dr1006]
   Dai YH, 2013, J OPER RES SOC CHINA, V1, P187, DOI 10.1007/s40305-013-0007-x
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fletcher R, 2005, APPL OPTIMIZAT, V96, P235
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   KESTEN H, 1958, ANN MATH STAT, V29, P41, DOI 10.1214/aoms/1177706705
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Mahsereci M., 2015, ARXIV150202846
   Masse P. Y., 2015, ARXIV151102540
   Raydan M, 1997, SIAM J OPTIMIZ, V7, P26, DOI 10.1137/S1052623494266365
   RAYDAN M, 1993, IMA J NUMER ANAL, V13, P321, DOI 10.1093/imanum/13.3.321
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Sopyla K, 2015, INFORM SCIENCES, V316, P218, DOI 10.1016/j.ins.2015.03.073
   Wang YF, 2007, INVERSE PROBL SCI EN, V15, P559, DOI 10.1080/17415970600881897
   Wen ZW, 2010, SIAM J SCI COMPUT, V32, P1832, DOI 10.1137/090747695
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702033
DA 2019-06-15
ER

PT S
AU Tandon, P
   Malviya, YH
   Rajendran, B
AF Tandon, Pulkit
   Malviya, Yash H.
   Rajendran, Bipin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient and Robust Spiking Neural Circuit for Navigation Inspired by
   Echolocating Bats
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID NETWORKS
AB We demonstrate a spiking neural circuit for azimuth angle detection inspired by the echolocation circuits of the Horseshoe bat Rhinolophus ferrumequinum and utilize it to devise a model for navigation and target tracking, capturing several key aspects of information transmission in biology. Our network, using only a simple local-information based sensor implementing the cardioid angular gain function, operates at biological spike rate of approximately 10 Hz. The network tracks large angular targets (60 degrees) within 1 sec with a 10% RMS error. We study the navigational ability of our model for foraging and target localization tasks in a forest of obstacles and show that it requires less than 200 X spike-triggered decisions, while suffering less than 1% loss in performance compared to a proportional-integral-derivative controller, in the presence of 50% additive noise. Superior performance can be obtained at a higher average spike rate of 100 Hz and 1000 Hz, but even the accelerated networks require 20 X and 10 X lesser decisions respectively, demonstrating the superior computational efficiency of bio-inspired information processing systems.
C1 [Tandon, Pulkit; Malviya, Yash H.] Indian Inst Technol, Bombay, Maharashtra, India.
   [Rajendran, Bipin] New Jersey Inst Technol, Newark, NJ 07102 USA.
RP Tandon, P (reprint author), Indian Inst Technol, Bombay, Maharashtra, India.
EM pulkit1495@gmail.com; yashmalviya94@gmail.com; bipin@njit.edu
FU CISCO Systems Inc.
FX This research was supported in part by the CAMPUSENSE project grant from
   CISCO Systems Inc.
CR Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Burger RM, 2001, J NEUROSCI, V21, P4830, DOI 10.1523/JNEUROSCI.21-13-04830.2001
   Hayward B., 1964, FLIGHT SPEEDS W BATS, V45, P236
   Hinton G., 2015, DEEP LEARNING, V521, P436
   Kandel E. R., 2000, NOBEL LECT PHISIOLOG
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Moss C. F., 2010, PROBING NATURAL SCEN
   Moss CF, 2003, CURR OPIN NEUROBIOL, V13, P751, DOI 10.1016/j.conb.2003.10.016
   Schuller G, 1988, TARGET DISCRIMINATIO, P413
   Shi R. Z., 2007, NEUROMORPHIC VLSI MO, P74
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simmons J. A, 1996, EPTESICUS FUSCUS, V100, P1764
   Sjostrom P. J, 2012, SPIKE TIMING DEPENDE, V4, P2
   Suga N., 1990, BIOSONAR NEURAL COMP, V262, P60
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704082
DA 2019-06-15
ER

PT S
AU Tang, PF
   Phillips, JM
AF Tang, Pingfan
   Phillips, Jeff M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Robustness of Estimator Composition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.
C1 [Tang, Pingfan; Phillips, Jeff M.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
RP Tang, PF (reprint author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM tang1984@cs.utah.edu; jeffp@cs.utah.edu
CR Aloupis G., 2006, DATA DEPTH ROBUST MU
   Cormode G., 2008, PODS
   Das Sarma A, 2009, VLDB J, V18, P989, DOI 10.1007/s00778-009-0147-0
   Davies PL, 2007, REVSTAT-STAT J, V5, P1
   Hampel F. R., 1986, ROBUST STAT APPROACH
   HAMPEL FR, 1971, ANN MATH STAT, V42, P1887, DOI 10.1214/aoms/1177693054
   HE XM, 1990, J AM STAT ASSOC, V85, P446, DOI 10.2307/2289782
   Huber P. J., 1981, ROBUST STAT
   Huber P. J., 2009, ROBUST STAT, P8
   Jorgensen A. G., 2011, WADS
   SIEGEL AF, 1982, BIOMETRIKA, V69, P242, DOI 10.2307/2335877
   Tang P., 2016, ARXIV160901226
   Weiszfeld E, 2009, ANN OPER RES, V167, P7, DOI 10.1007/s10479-008-0352-z
   Welsh A. H., 1996, ASPECTS STAT INFEREN, P245
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700020
DA 2019-06-15
ER

PT S
AU Tenzer, Y
   Schwing, A
   Gimpel, K
   Hazan, T
AF Tenzer, Yaniv
   Schwing, Alexander
   Gimpel, Kevin
   Hazan, Tamir
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Constraints Based Convex Belief Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Inference in Markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications. In order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances. Unfortunately this comes at the price of a tremendous computational burden. In this paper we suggest to tackle consistency by incorporating constraints on beliefs. This permits derivation of a closed-form message-passing algorithm which we refer to as the Constraints Based Convex Belief Propagation (CBCBP). Experiments show that CBCBP outperforms the conventional consistency potential based approach, while being at least an order of magnitude faster.
C1 [Tenzer, Yaniv] Hebrew Univ Jerusalem, Dept Stat, Jerusalem, Israel.
   [Schwing, Alexander] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA.
   [Gimpel, Kevin] Toyota Technol Inst, Chicago, IL USA.
   [Hazan, Tamir] Technion Israel Inst Technol, Fac Ind Engn & Management, Haifa, Israel.
RP Tenzer, Y (reprint author), Hebrew Univ Jerusalem, Dept Stat, Jerusalem, Israel.
CR Bach S. H., 2012, NIPS, V25, P2654
   Callison- Burch C., 2011, P WMT
   Chen L. - C., 2015, P ICML EQ CONTR
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Hazan T., 2012, P UAI
   Heskes T, 2004, NEURAL COMPUT, V16, P2379, DOI 10.1162/0899766041941943
   Koehn P., 2007, P ACL
   Koehn Philipp, 2003, P 2003 C N AM CHAPT, P48, DOI DOI 10.3115/1073445.1073462
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Koller D., 2009, PROBABILISTIC GRAPHI
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18
   Martins A. F., 2011, AUGMENTED LAGRANGIAN
   Martins A. R. Q., 2012, THESIS
   Meltzer T., 2009, UAI
   Meshi O., 2010, P ICML
   Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567
   Papineni  K., 2002, P ACL
   Schwing A. G., 2012, P NIPS
   Schwing A. G., 2011, P CVPR
   Schwing A. G., 2013, P ICCV
   Shotton J., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587503
   SMITH D. AND EISNER, 2008, P C EMP METH NAT LAN
   Tarlow D., 2011, ICML 2011 P 28 INT C, P113
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701076
DA 2019-06-15
ER

PT S
AU Nguyen, TB
   Olivetti, E
   Avesani, P
AF Thien Bao Nguyen
   Olivetti, Emanuele
   Avesani, Paolo
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Mapping Tractography Across Subjects
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID SEGMENTATION; REGISTRATION; IMAGES
AB Diffusion magnetic resonance imaging (dMRI) and tractography provide means to study the anatomical structures within the white matter of the brain. When studying tractography data across subjects, it is usually necessary to align, i.e. to register, tractographies together. This registration step is most often performed by applying the transformation resulting from the registration of other volumetric images (T1, FA). In contrast with registration methods that transform tractographies, in this work, we try to find which streamline in one tractography correspond to which streamline in the other tractography, without any transformation. In other words, we try to find a mapping between the tractographies. We propose a graph-based solution for the tractography mapping problem and we explain similarities and differences with the related well-known graph matching problem. Specifically, we define a loss function based on the pairwise streamline distance and reformulate the mapping problem as combinatorial optimization of that loss function. We show preliminary promising results where we compare the proposed method, implemented with simulated annealing, against a standard registration techniques in a task of segmentation of the corticospinal tract.
C1 [Thien Bao Nguyen] Univ Technol & Educ, Fac Informat Technol, Hochicinh City, Hochicinh, Vietnam.
   [Thien Bao Nguyen; Olivetti, Emanuele; Avesani, Paolo] Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.
   [Olivetti, Emanuele; Avesani, Paolo] Univ Trento, Ctr Mind & Brain Sci CIMeC, Trento, Italy.
RP Nguyen, TB (reprint author), Univ Technol & Educ, Fac Informat Technol, Hochicinh City, Hochicinh, Vietnam.; Nguyen, TB (reprint author), Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.
EM baont@fit.hcmute.edu.vn
CR BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1
   Bazin PL, 2011, NEUROIMAGE, V58, P458, DOI 10.1016/j.neuroimage.2011.06.020
   Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228
   Garyfallidis E., 2012, THESIS
   Golding D., 2011, P 22 ANN S PATT REC, P55
   Goodlett CB, 2009, NEUROIMAGE, V45, pS133, DOI 10.1016/j.neuroimage.2008.10.060
   Jenkinson M, 2001, MED IMAGE ANAL, V5, P143, DOI 10.1016/S1361-8415(01)00036-6
   Laarhoven PJ, 1987, SIMULATED ANNEALING
   Mori S, 2002, NMR BIOMED, V15, P468, DOI 10.1002/nbm.781
   O'Donnell LJ, 2012, LECT NOTES COMPUT SC, V7512, P123, DOI 10.1007/978-3-642-33454-2_16
   Olivetti E, 2013, INT WORKSHOP PATTERN, P42, DOI 10.1109/PRNI.2013.20
   Wang Y, 2011, NEUROIMAGE, V55, P1577, DOI 10.1016/j.neuroimage.2011.01.038
   Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245
   Zhang S, 2008, IEEE T VIS COMPUT GR, V14, P1044, DOI 10.1109/TVCG.2008.52
NR 14
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 21
EP 28
DI 10.1007/978-3-319-45174-9_3
PG 8
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400003
DA 2019-06-15
ER

PT S
AU Titsias, MK
AF Titsias, Michalis K.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI One-vs-Each Approximation to Softmax for Scalable Estimation of
   Probabilities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems. However, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant. Here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability. This bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization. It allows us to perform doubly stochastic estimation by subsampling both training instances and class labels. We show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems.
C1 [Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
RP Titsias, MK (reprint author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.
EM mtitsias@aueb.gr
CR Bengio Y., 2003, P C ART INT STAT AIS
   Bhatia K., 2015, ADV NEURAL INFORM PR, P730
   Bishop C. M., 2006, PATTERN RECOGNITION
   BOHNING D, 1992, ANN I STAT MATH, V44, P197, DOI 10.1007/BF00048682
   Bouchard Guillaume, 2007, TECHNICAL REPORT
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Devlin J., 2014, LONG PAPERS ASS COMP, P1370, DOI DOI 10.3115/V1/P14-1129
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gopal Siddharth, 2013, JMLR WORKSHOP C P, P289
   Huang TK, 2006, J MACH LEARN RES, V7, P85
   Ji Shihao, 2015, BLACKOUT SPEEDING RE
   Katakis Ioannis, 2008, P ECML PKDD 08 WORKS
   Khan Mohammad Emtiyaz, 2012, AISTATS, P610
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mnih A., 2012, P 29 INT C MACH LEAR, P1751
   Morin F., 2005, P AISTATS, P246
   Paquet Ulrich, 2012, ABS14092824 CORR
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Vijayanarasimhan Sudheendra, 2014, ABS14127479 CORR
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703104
DA 2019-06-15
ER

PT S
AU Tolstikhin, I
   Sriperumbudur, BK
   Scholkopf, B
AF Tolstikhin, Ilya
   Sriperumbudur, Bharath K.
   Schoelkopf, Bernhard
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID METRICS
AB Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on R-d and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its U-statistic variant, which are usually employed in applications.
C1 [Tolstikhin, Ilya; Schoelkopf, Bernhard] MPI Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany.
   [Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
RP Tolstikhin, I (reprint author), MPI Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany.
EM ilya@tuebingen.mpg.de; bks18@psu.edu; bs@tuebingen.mpg.de
CR Berlinet A, 2004, REPRODUCING KERNEL H
   Boucheron S., 2013, CONCENTRATION INEQUA
   Fukumizu K, 2008, ADV NEURAL INFORM PR, P489
   Fukumizu K, 2013, J MACH LEARN RES, V14, P3753
   Gretton A., 2007, ADV NEURAL INFORM PR, V19, P513
   Gretton A., 2008, ADV NEURAL INFORM PR, V20, P585
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Lehmann E. L., 2008, THEORY POINT ESTIMAT
   Lopez-Paz  D., 2015, P 32 INT C MACH LEAR
   Muandet  K., 2016, J MACHINE LEARNING R
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Song L., 2008, P 25 INT C MACH LEAR, P992
   Song L, 2012, J MACH LEARN RES, V13, P1393
   Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I, 2008, INFORM SCI STAT, P1
   Szabo  Z., 2015, INT C ART INT STAT A, V38, P948
   Tolstikhin I., 2016, ARXIV160204361MATHST
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704008
DA 2019-06-15
ER

PT S
AU Torrecilla, JL
   Suarez, A
AF Torrecilla, Jose L.
   Suarez, Alberto
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Feature selection in functional data classification with recursive
   maxima hunting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID VARIABLE SELECTION; LOGISTIC-REGRESSION
AB Dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction. In this work, we introduce recursive maxima hunting (RMH) for variable selection in classification problems with functional data. In this context, variable selection techniques are especially attractive because they reduce the dimensionality, facilitate the interpretation and can improve the accuracy of the predictive models. The method, which is a recursive extension of maxima hunting (MH), performs variable selection by identifying the maxima of a relevance function, which measures the strength of the correlation of the predictor functional variable with the class label. At each stage, the information associated with the selected variable is removed by subtracting the conditional expectation of the process. The results of an extensive empirical evaluation are used to illustrate that, in the problems investigated, RMH has comparable or higher predictive accuracy than standard dimensionality reduction techniques, such as PCA and PLS, and state-of-the-art feature selection methods for functional data, such as maxima hunting.
C1 [Torrecilla, Jose L.; Suarez, Alberto] Univ Autonoma Madrid, Comp Sci Dept, E-28049 Madrid, Spain.
RP Torrecilla, JL (reprint author), Univ Autonoma Madrid, Comp Sci Dept, E-28049 Madrid, Spain.
EM joseluis.torrecilla@uam.es; alberto.suarez@uam.es
RI Torrecilla, Jose Luis/P-6199-2019
OI Torrecilla, Jose Luis/0000-0003-3719-5190
FU Spanish Ministry of Economy and Competitiveness [TIN2013-42351-P];
   Regional Government of Madrid, CASI-CAM-CM project [S2013/ICE-2845]
FX The authors thank Dr. Jose R. Berrendero for his insightful suggestions.
   We also acknowledge financial support from the Spanish Ministry of
   Economy and Competitiveness, project TIN2013-42351-P and from the
   Regional Government of Madrid, CASI-CAM-CM project (S2013/ICE-2845).
CR Aneiros G, 2014, STAT PROBABIL LETT, V94, P12, DOI 10.1016/j.spl.2014.06.025
   Baillo A., 2011, CLASSIFICATION METHO, P259
   Berrendero JR, 2016, J STAT COMPUT SIM, V86, P891, DOI 10.1080/00949655.2015.1042378
   Berrendero J. R., 2015, ARXIV150704398, P1
   Berrendero JR, 2016, STAT SINICA, V26, P619, DOI 10.5705/ss.202014.0014
   Delaigle A, 2012, BIOMETRIKA, V99, P299, DOI 10.1093/biomet/ass003
   Delaigle A, 2012, ANN STAT, V40, P322, DOI 10.1214/11-AOS958
   Delaigle A, 2012, J R STAT SOC B, V74, P267, DOI 10.1111/j.1467-9868.2011.01003.x
   Fernandez-Lozano C, 2015, SOFT COMPUT, V19, P2469, DOI 10.1007/s00500-014-1573-5
   Ferraty F, 2010, BIOMETRIKA, V97, P807, DOI 10.1093/biomet/asq058
   Ferraty F, 2006, SPR S STAT
   Fraiman R, 2016, J MULTIVARIATE ANAL, V146, P191, DOI 10.1016/j.jmva.2015.09.006
   Galeano P, 2015, TECHNOMETRICS, V57, P281, DOI 10.1080/00401706.2014.902774
   GENTLEMAN R, 2005, J BIOINF COMPUT BIOL, P189
   Gomez-Verdejo V, 2009, NEUROCOMPUTING, V72, P3580, DOI 10.1016/j.neucom.2008.12.035
   Grosenick L, 2008, IEEE T NEUR SYS REH, V16, P539, DOI 10.1109/TNSRE.2008.926701
   Guyon I., 2006, FEATURE EXTRACTION F
   Kneip A, 2011, ANN STAT, V39, P2410, DOI 10.1214/11-AOS905
   Lange T, 2014, STAT PAP, V55, P49, DOI 10.1007/s00362-012-0488-4
   Li B, 2008, COMPUT STAT DATA AN, V52, P4790, DOI 10.1016/j.csda.2008.03.024
   Lindquist MA, 2009, J AM STAT ASSOC, V104, P1575, DOI 10.1198/jasa.2009.tm08496
   McKeague IW, 2010, ANN STAT, V38, P2559, DOI 10.1214/10-AOS791
   Morters  P., 2010, BROWNIAN MOTION
   Preda C, 2007, COMPUTATION STAT, V22, P223, DOI 10.1007/s00180-007-0041-4
   Ramsay JO, 2005, FUNCTIONAL DATA ANAL
   Ryali S, 2010, NEUROIMAGE, V51, P752, DOI 10.1016/j.neuroimage.2010.02.040
   Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505
   Szekely GJ, 2012, STAT PROBABIL LETT, V82, P2278, DOI 10.1016/j.spl.2012.08.007
   Tian TS, 2013, COMPUT STAT DATA AN, V57, P282, DOI 10.1016/j.csda.2012.06.017
   Yu L, 2004, J MACH LEARN RES, V5, P1205
   Zhou JH, 2013, STAT SINICA, V23, P25, DOI 10.5705/ss.2010.237
   Zou XB, 2010, ANAL CHIM ACTA, V667, P14, DOI 10.1016/j.aca.2010.03.048
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703028
DA 2019-06-15
ER

PT S
AU Toulis, P
   Parkes, DC
AF Toulis, Panagiotis (Panos)
   Parkes, David C.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Long-term causal effects via behavioral game theory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIFFERENCE-IN-DIFFERENCES
AB Planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy. One critical shortcoming of classical experimental methods, however, is that they typically do not take into account the dynamic nature of response to policy changes. For instance, in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue, agents may adapt their bidding in response to the experimental pricing changes. Thus, causal effects of the new pricing policy after such adaptation period, the long-term causal effects, are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy. Here, we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies. Central to our approach is behavioral game theory, which we leverage to formulate the ignorability assumptions that are necessary for causal inference. Under such assumptions we estimate long-term causal effects through a latent space approach, where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time.
C1 [Toulis, Panagiotis (Panos)] Univ Chicago, Booth Sch, Econometr & Stat, Chicago, IL 60637 USA.
   [Parkes, David C.] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA.
RP Toulis, P (reprint author), Univ Chicago, Booth Sch, Econometr & Stat, Chicago, IL 60637 USA.
EM panos.toulis@chicagobooth.edu; parkes@eecs.harvard.edu
FU Google US/Canada Fellowship in Statistics; NSF [CCF-1301976]; SEAS
   TomKat fund
FX The authors wish to thank Leon Bottou, the organizers and participants
   of CODE@MIT' 15, GAMES' 16, the Workshop on Algorithmic Game Theory and
   Data Science (EC' 15), and the anonymous NIPS reviewers for their
   valuable feedback. Panos Toulis has been supported in part by the 2012
   Google US/Canada Fellowship in Statistics. David C. Parkes was supported
   in part by NSF grant CCF-1301976 and the SEAS TomKat fund.
CR Abadie A, 2005, REV ECON STUD, V72, P1, DOI 10.1111/0034-6527.00321
   Aitchison J., 1986, STAT ANAL COMPOSITIO
   Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1
   Athey S, 2011, Q J ECON, V126, P207, DOI 10.1093/qje/qjq001
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   CARD D, 1994, AM ECON REV, V84, P772
   Donald SG, 2007, REV ECON STAT, V89, P221, DOI 10.1162/rest.89.2.221
   Fisher R. A., 1935, DESIGN EXPT
   GRUNWALD GK, 1993, J ROY STAT SOC B MET, V55, P103
   Hahn PR, 2015, ANN APPL STAT, V9, P1459, DOI 10.1214/15-AOAS830
   Heckman James J, 1998, AM ECON REV, V88
   Heckman JJ, 2005, ECONOMETRICA, V73, P669, DOI 10.1111/j.1468-0262.2005.00594.x
   HOLLAND JH, 1991, AM ECON REV, V81, P365
   HOLLAND PW, 1986, J AM STAT ASSOC, V81, P945, DOI 10.2307/2289064
   MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023
   Ostrovsky Michael, 2011, P 12 ACM C EL COMM, P59, DOI DOI 10.1145/1993574.1993585
   Pearl J., 2000, CAUSALITY MODELS REA
   RAPOPORT A, 1992, GAME ECON BEHAV, V4, P261, DOI 10.1016/0899-8256(92)90019-O
   Rubin Donald B, 2011, J AM STAT ASS
   STAHL DO, 1994, J ECON BEHAV ORGAN, V25, P309, DOI 10.1016/0167-2681(94)90103-1
   Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559
   Wright JR, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P901
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700023
DA 2019-06-15
ER

PT S
AU Le, T
   Nguyen, TD
   Nguyen, V
   Phung, D
AF Trung Le
   Tu Dinh Nguyen
   Vu Nguyen
   Dinh Phung
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dual Space Gradient Descent for Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PERCEPTRON
AB One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.
C1 [Trung Le; Tu Dinh Nguyen; Vu Nguyen; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
RP Le, T (reprint author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM trung.l@deakin.edu.au; tu.nguyen@deakin.edu.au; v.nguyen@deakin.edu.au;
   dinh.phung@deakin.edu.au
FU Australian Research Council [DP160109394]
FX This work is partially supported by the Australian Research Council
   under the Discovery Project DP160109394.
CR Cavallanti G, 2007, MACH LEARN, V69, P143, DOI 10.1007/s10994-007-5003-0
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dekel O., 2005, ADV NEURAL INFORM PR, P259
   Dredze Mark, 2008, P 25 INT C MACH LEAR, P264
   Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Le T., 2016, 32 C UNC ART INT JUN
   Le T., 2016, 19 INT C ART INT STA
   Lu J., 2015, J MACH LEARN RES
   Ming L., 2014, ACM T KNOWL DISCOV D, V8
   Orabona F, 2009, J MACH LEARN RES, V10, P2643
   Rahimi A., 2007, ADV NEURAL INFOMRATI
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Wang Z., 2010, INT C ART INT STAT, P908
   Wang Z, 2012, J MACH LEARN RES, V13, P3103
   Zhao P., 2012, CORR
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704084
DA 2019-06-15
ER

PT S
AU Tsai, CY
   Saxe, A
   Cox, D
AF Tsai, Chuan-Yung
   Saxe, Andrew
   Cox, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tensor Switching Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.
C1 [Tsai, Chuan-Yung; Saxe, Andrew; Cox, David] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
RP Tsai, CY (reprint author), Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
EM chuanyungtsai@fas.harvard.edu; asaxe@fas.harvard.edu;
   davidcox@fas.harvard.edu
OI Saxe, Andrew/0000-0002-9831-8812
FU NSF [IIS 1409097]; IARPA [D16PC00002]; Swartz Foundation
FX We would like to thank James Fitzgerald, Mien "Brabeeba" Wang, Scott
   Linderman, and Yu Hu for fruitful discussions. We also thank the
   anonymous reviewers for their valuable comments. This work was supported
   by NSF (IIS 1409097), IARPA (contract D16PC00002), and the Swartz
   Foundation.
CR Amos  B., 2015, ICLR WORKSH
   Anden  J., 2014, IEEE T SP
   Aslan  O., 2014, NIPS
   Bai  Q., 2016, ICML
   Cho  Y., 2010, NEURAL COMPUTATION
   Courville  A., 2011, AISTATS
   Deng  L., 2011, INTERSPEECH
   Duvenaud  D., 2014, AISTATS
   Goodfellow I, 2013, ICML
   Hahnloser  R., 2000, NATURE
   He K., 2016, ECCV
   Hochreiter S., 2001, FIELD GUIDE DYNAMICA
   Huang G., 2016, DENSELY CONNECTED CO
   Janzamin  M., 2015, BEATING PERILS NONCO
   Konda  K., 2015, ICLR
   Lecun Y., 2015, NATURE
   Miyato T., 2016, ICLR
   Nair V., 2010, ICML
   Saxe  A., 2015, COSYNE
   Saxe  A., 2014, ICLR
   Schmidhuber  J., 2015, NEURAL NETWORKS
   Simonyan Karen, 2015, ICLR
   Sonoda  S., 2015, APPL COMPUTATIONAL H
   Springenberg J. T., 2015, ICLR WORKSH
   Srivastava R. K., 2015, NIPS
   Szegedy C., 2015, CVPR
   Wang  S., 2016, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704041
DA 2019-06-15
ER

PT S
AU Turchetta, M
   Berkenkamp, F
   Krause, A
AF Turchetta, Matteo
   Berkenkamp, Felix
   Krause, Andreas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Safe Exploration in Finite Markov Decision Processes with Gaussian
   Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REINFORCEMENT; ROBOTICS; STATE
AB In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.
C1 [Turchetta, Matteo; Berkenkamp, Felix; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Turchetta, M (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM matteotu@ethz.ch; befelix@ethz.ch; krausea@ethz.ch
FU Max Planck ETH Center for Learning Systems; SNSF grant [200020_159557]
FX This research was partially supported by the Max Planck ETH Center for
   Learning Systems and SNSF grant 200020_159557.
CR Akametalu AK, 2014, IEEE DECIS CONTR P, P1424, DOI 10.1109/CDC.2014.7039601
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Berkenkamp F., 2015, EUR CONTR C, P2501
   Berkenkamp Felix, 2016, P IEEE INT C ROB AUT
   Coraluppi SP, 1999, AUTOMATICA, V35, P301, DOI 10.1016/S0005-1098(98)00153-8
   Garcia J, 2012, J ARTIF INTELL RES, V45, P515, DOI 10.1613/jair.3761
   Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666
   Ghosal S, 2006, ANN STAT, V34, P2413, DOI 10.1214/009053606000000795
   Hans  A., 2008, P EUR S ART NEUR NET, P143
   Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721
   Lockwood MK, 2006, J SPACECRAFT ROCKETS, V43, P257, DOI 10.2514/1.20678
   MCEWEN AS, 2007, J GEOPHYS RES-PLANET, V112, DOI DOI 10.1029/2005JE002605
   Mockus Jonas, 1989, MATH ITS APPL, V37
   Moldovan T. M., 2012, P INT C MACH LEARN I, P1711
   MSL, 2007, MSL LAND SIT SEL US
   Pecka M, 2014, LECT NOTES COMPUT SC, V8906, P357
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schaal S, 2010, IEEE ROBOT AUTOM MAG, V17, P20, DOI 10.1109/MRA.2010.936957
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Schreiter J, 2015, LECT NOTES ARTIF INT, V9286, P133, DOI 10.1007/978-3-319-23461-8_9
   Srinivas Niranjan, 2010, P INT C MACH LEARN I
   Sui Y, 2015, P 32 INT C MACH LEAR, P997
   Sutton R. S., 1998, REINFORCEMENT LEARNI
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702105
DA 2019-06-15
ER

PT S
AU Ustinova, E
   Lempitsky, V
AF Ustinova, Evgeniya
   Lempitsky, Victor
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Deep Embeddings with Histogram Loss
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIMILARITY
AB We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.
C1 [Ustinova, Evgeniya; Lempitsky, Victor] Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.
RP Ustinova, E (reprint author), Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.
FU Russian Ministry of Science and Education [RFMEFI57914X0071]
FX This research is supported by the Russian Ministry of Science and
   Education grant RFMEFI57914X0071.
CR Arandjelovic O, 2015, IEEE IJCNN
   Bowman AW, 1997, OXFORD STAT SCI SERI, V18
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Law MT, 2013, IEEE I CONF COMP VIS, P249, DOI 10.1109/ICCV.2013.38
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin J., 2015, ABS150104711 CORR
   Parkhi O. M., 2015, P BRIT MACHINE VISIO, V1, P6, DOI DOI 10.5244/C.29.41
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schultz M, 2004, ADV NEUR IN, V16, P41
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Song H. O., 2016, COMPUTER VISION PATT
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tadmor O., 2016, NIPS
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wah C., 2011, CNSTR2011001
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yi D., 2014, ARXIV14074979
   Zbontar J., 2015, ARXIV151005970
   Zheng L., 2015, COMP VIS IEEE INT C
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703100
DA 2019-06-15
ER

PT S
AU van den Oord, A
   Kalchbrenner, N
   Vinyals, O
   Espeholt, L
   Graves, A
   Kavukcuoglu, K
AF van den Oord, Aaron
   Kalchbrenner, Nal
   Vinyals, Oriol
   Espeholt, Lasse
   Graves, Alex
   Kavukcuoglu, Koray
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Conditional Image Generation with PixelCNN Decoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.
C1 [van den Oord, Aaron; Kalchbrenner, Nal; Vinyals, Oriol; Espeholt, Lasse; Graves, Alex; Kavukcuoglu, Koray] Google DeepMind, London, England.
RP van den Oord, A (reprint author), Google DeepMind, London, England.
EM avdnoord@google.com; nalk@google.com; vinyals@google.com;
   espeholt@google.com; gravesa@google.com; korayk@google.com
CR Abadi M., 2016, ARXIV160304467
   Bellemare Marc G, 2016, ARXIV160601868
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Dinh L., 2014, ARXIV14108516
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves  Alex, 2009, ADV NEURAL INFORM PR
   Gregor Karol, 2015, P 32 INT C MACH LEAR
   Gregor Karol, 2014, P 31 INT C MACH LEAR
   Gregor  Karol, 2016, ARXIV160106759
   He  Kaiming, 2015, ARXIV1501203385
   Jimenez Rezende D., 2014, P 31 INT C MACH LEAR
   Kaiser  Lukasz, 2015, ARXIV151108228
   Kalchbrenner  N., 2015, ARXIV150701526
   Larochelle  Hugo, 2011, J MACHINE LEARNING R
   Mansimov E., 2015, ARXIV151102793
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Oh J., 2015, NIPS, V28, P2845
   Olah  Christopher, 2015, INCEPTIONISM GOING D
   Oord  A.v.d., 2016, ARXIV160106759
   Reed S., 2016, ARXIV160505396
   Rezende D.J., 2016, ARXIV160305106
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sohl-Dickstein  Jascha, 2015, P 32 INT C MACH LEAR
   Srivastava R. K., 2015, ADV NEURAL INFORM PR, P2368
   Theis L., 2015, ARXIV151101844
   Theis  Lucas, 2015, ADV NEURAL INFORM PR
   Uria  Benigno, 2016, ARXIV160502226
   van den Oord  Aaron, 2015, INT C MACH LEARN ICM, P1, DOI DOI 10.1002/9781118541555.WBIEPC058
   van den Oord  Aaron, 2014, J MACHINE LEARNING R
   van den Oord  Aaron, 2014, ADV NEURAL INFORM PR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704051
DA 2019-06-15
ER

PT S
AU van Erven, T
   Koolen, WM
AF van Erven, Tim
   Koolen, Wouter M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI MetaGrad: Multiple Learning Rates in Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PREDICTION
AB In online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions. We are interested in designing adaptive methods that can automatically get fast rates in as many such subclasses as possible, without any manual tuning. Previous adaptive methods are able to interpolate between strongly convex and general convex functions. We present a new method, MetaGrad, that adapts to a much broader class of functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature. For instance, MetaGrad can achieve logarithmic regret on the unregularized hinge loss, even though it has no curvature, if the data come from a favourable probability distribution. MetaGrad's main feature is that it simultaneously considers multiple learning rates. Unlike previous methods with provable regret guarantees, however, its learning rates are not monotonically decreasing over time and are not tuned based on a theoretically derived bound on the regret. Instead, they are weighted directly proportional to their empirical performance on the data using a tilted exponential weights master algorithm.
C1 [van Erven, Tim] Leiden Univ, Leiden, Netherlands.
   [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
RP van Erven, T (reprint author), Leiden Univ, Leiden, Netherlands.
EM tim@timvanerven.nl; wmkoolen@cwi.nl
FU Netherlands Organization for Scientific Research (NWO) [639.021.439]
FX We would like to thank Haipeng Luo and the anonymous reviewers (in
   particular Reviewer 6) for valuable comments. Koolen acknowledges
   support by the Netherlands Organization for Scientific Research (NWO,
   Veni grant 639.021.439).
CR Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Chernov A. V., 2010, P 26 C UNC ART INT, P117
   Chernov A, 2010, THEOR COMPUT SCI, V411, P2647, DOI 10.1016/j.tcs.2010.04.003
   Chiang C. - K., 2012, P 25 ANN C LEARN THE
   Crammer K., 2009, ADV NEURAL INFORM PR, P414
   Do C. B., 2009, P 26 ACM INT C MACH, P257
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gaillard  Pierre, 2014, JMLR WORKSHOP C P, P176
   Hazan E., 2007, P 21 ANN C ADV NEUR, V20, P65
   Hazan E., 2016, INTRO ONLINE OPTIMIZ
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Ihara S., 1993, INFORM THEORY CONTIN
   Koolen W. M., 2015, COLT, V40, P1155
   Koolen W. M., 2016, NIPS, V29
   Koolen W. M., 2016, METAGRAD OPEN SOURCE
   Koolen Wouter M., 2014, P ADV NEUR INF PROC, V27, P2294
   Luo H., 2016, NIPS, V29
   McMahan H. B, 2010, P 23 C LEARN THEOR, P244
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Orabona F., 2016, NIPS 29
   Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
   Steinhardt Jacob, 2014, P 31 INT C MACH LEAR, P1593
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Zinkevich M., 2004, THESIS
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702015
DA 2019-06-15
ER

PT S
AU van Hasselt, H
   Guez, A
   Hessel, M
   Mnih, V
   Silver, D
AF van Hasselt, Hado
   Guez, Arthur
   Hessel, Matteo
   Mnih, Volodymyr
   Silver, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning values across many orders of magnitude
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Most learning algorithms are not invariant to the scale of the signal that is being approximated. We propose to adaptively normalize the targets used in the learning updates. This is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.
C1 [van Hasselt, Hado; Guez, Arthur; Hessel, Matteo; Mnih, Volodymyr; Silver, David] Google DeepMind, London, England.
RP van Hasselt, H (reprint author), Google DeepMind, London, England.
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bellemare M. G., 2016, AAAI
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Desjardins G., 2015, ADV NEURAL INFORM PR, P2062
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hasselt H. V., 2010, ADV NEURAL INFORM PR, P2613
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2015, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liang Y., 2016, INT C AUT AG MULT SY
   Martens J., 2015, INT C MACH LEARN, P2408
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mnih  V., 2016, INT C MACH LEARN
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Osband I., 2016, ABS160204621 CORR
   Rosenblatt F., 1962, PRINCIPLES NEURODYNA
   Ross S., 2013, P 29 C UNC ART INT
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Schaul T., 2016, INT C LEARN REPR PUE
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   van Hasselt H., 2016, AAAI
   Wang Z, 2016, INT C ELECTR MACH SY
   Watkins C.J.C.H., 1989, THESIS, P9
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700040
DA 2019-06-15
ER

PT S
AU Vezhnevets, A
   Mnih, V
   Agapiou, J
   Osindero, S
   Graves, A
   Vinyals, O
   Kavukcuoglu, K
AF Vezhnevets, Alexander (Sasha)
   Mnih, Volodymyr
   Agapiou, John
   Osindero, Simon
   Graves, Alex
   Vinyals, Oriol
   Kavukcuoglu, Koray
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Strategic Attentive Writer for Learning Macro-Actions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to - i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.
C1 [Vezhnevets, Alexander (Sasha); Mnih, Volodymyr; Agapiou, John; Osindero, Simon; Graves, Alex; Vinyals, Oriol; Kavukcuoglu, Koray] Google DeepMind, London, England.
RP Vezhnevets, A (reprint author), Google DeepMind, London, England.
EM vezhnick@google.com; vmnih@google.com; jagapiou@google.com;
   osindero@google.com; gravesa@google.com; vinyals@google.com;
   korayk@google.com
CR Bacon P., 2015, NIPS DEEP RL WORKSH
   Bellemare Marc G, 2012, J ARTIFICIAL INTELLI
   Bengio Y., 2013, ARXIV13083432
   Botvinick Matthew M, 2009, COGNITION
   Boutilier Craig, 1997, IJCAI
   Dayan Peter, 1993, NIPS
   Dayan Peter, 1993, NEURAL COMPUTATION
   Dietterich Thomas G, 2000, J ARTIF INTELL RES J
   Gers Felix A, 2000, NEURAL COMPUTATION
   Graves A, 2013, ARXIV13080850
   Gregor K., 2015, ICML
   Kaelbling Leslie Pack, 2014, ICML
   Kingma Diederik P, 2014, ICLR
   Kulkarni T., 2016, ARXIV160406057
   Levine S., 2015, ARXIV150400702
   Marcus Mitchell P, 1993, COMPUTATIONAL LINGUI
   Mnih V., 2016, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mozer Michael C, 1989, COMPLEX SYSTEMS
   Parr Ronald, 1998, NIPS
   Precup Doina, 2000, THESIS
   Precup Doina, 1998, EUR C MACH LEARN ECM
   Precup Doina, 1997, TECHNICAL REPORT
   Rezende D. J., 2014, ICML
   Schmidhuber Jurgen, 1991, TECHNICAL REPORT
   Schulman John, 2015, ICML
   Sutton R. S., 1999, ARTIFICIAL INTELLIGE
   Sutton Richard S, 1995, ICML
   Wiering Marco, 1997, ADAPTIVE BEHAV
   Zahavy T., 2016, ARXIV160407255
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703050
DA 2019-06-15
ER

PT S
AU Vinayak, RK
   Hassibi, B
AF Vinayak, Ramya Korlakai
   Hassibi, Babak
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Crowdsourced Clustering: Querying Edges vs Triangles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.
C1 [Vinayak, Ramya Korlakai; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
RP Vinayak, RK (reprint author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
EM ramya@caltech.edu; hassibi@systems.caltech.edu
CR Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Gomes R. G., 2011, NIPS, P558
   Heikinheimo Hannes, 2013, HCOMP
   Heim Eric, MACH LEARN KNOWL DIS, P563
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Karger David R., 2011, NEUR INF PROC SYST C
   Khosla A., 2011, 1 WORKSH FIN GRAIN V
   Lintott Chris, 2012, AJ UNPUB
   Liu Qiang, 2012, NEUR INF PROC SYST C
   Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Simpson Robert, 2014, P 23 INT C WORLD WID
   Snow R., 2008, P C EMP METH NAT LAN, P254, DOI DOI 10.3115/1613715.1613751
   Sorokin A., 2008, P 1 IEEE WORKSH INT, V08, P1
   Tamuz Omer, 2011, CORR
   van der Maaten L. J. P., 2012, MACH LEARN SIGN PROC, P1
   Vempaty Aditya, 2013, CORR
   Vinayak Ramya Korlakai, 2014, NEUR INF PROC SYST C
   von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379
   Wah C., 2011, CNSTR2011001 CALTECH
   Wah C, 2014, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2014.115
   Welinder Peter, 2010, NEUR INF PROC SYST C
   Wilber Michael, 2014, HUMAN COMPUTATION CR
   Yi Jinfeng, 2012, NEUR INF PROC SYST C
   Zhang Yuchen, 2014, NEUR INF PROC SYST C
   Zhou D., 2012, ADV NEURAL INFORM PR, P2195
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704024
DA 2019-06-15
ER

PT S
AU Vinyals, O
   Blundell, C
   Lillicrap, T
   Kavukcuoglu, K
   Wierstra, D
AF Vinyals, Oriol
   Blundell, Charles
   Lillicrap, Timothy
   Kavukcuoglu, Koray
   Wierstra, Daan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Matching Networks for One Shot Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.
C1 [Vinyals, Oriol; Blundell, Charles; Lillicrap, Timothy; Kavukcuoglu, Koray; Wierstra, Daan] Google DeepMind, London, England.
RP Vinyals, O (reprint author), Google DeepMind, London, England.
EM vinyals@google.com; cblundell@google.com; countzero@google.com;
   korayk@google.com; wierstra@google.com
CR Atkeson C. G., 1997, ARTIFICIAL INTELLIGE
   Bahdanau D, 2014, ICLR
   Donahue J., 2014, ICML
   Graves A, 2014, ARXIV14105401
   Hermann K, 2015, NIPS
   Hill F., 2015, ARXIV151102301
   Hinton Geoffrey, 2012, SIGNAL PROCESSING MA
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hoffer E, 2015, SIMILARITY BASED PAT
   Ioffe S., 2015, ARXIV150203167
   Koch  G., 2015, ICML DEEP LEARN WORK
   Krizhevsky A., 2012, NIPS
   Krizhevsky Alex, 2010, CONVOLUTIONAL UNPUB
   Lake BM, 2011, COGSCI
   Marcus Mitchell P, 1993, COMPUTATIONAL LINGUI
   Mikolov Tomas, 2010, INTERSPEECH
   Norouzi M., 2013, ARXIV13125650
   Roweis S, 2004, NIPS
   Russakovsky Olga, 2015, IJCV
   Salakhutdinov R, 2007, AISTATS
   Santoro A., 2016, ICML
   Simonyan K, 2014, ARXIV14091556
   Sutskever  I., 2014, NIPS
   Szegedy C., 2015, CVPR
   Szegedy C., 2015, ARXIV151200567
   Vinyals O., 2015, NIPS
   Vinyals O., 2015, ARXIV151106391
   Weinberger K, 2009, JMLR
   Weston J, 2014, ICLR
   Zaremba W, 2014, ARXIV14092329
NR 30
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703021
DA 2019-06-15
ER

PT S
AU Vondrick, C
   Pirsiavash, H
   Torralba, A
AF Vondrick, Carl
   Pirsiavash, Hamed
   Torralba, Antonio
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generating Videos with Scene Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.
C1 [Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA.
   [Pirsiavash, Hamed] UMBC, Baltimore, MD USA.
RP Vondrick, C (reprint author), MIT, Cambridge, MA 02139 USA.
EM vondrick@mit.edu; hpirsiav@umbc.edu; torralba@mit.edu
FU NSF [1524817]; START program at UMBC; Google PhD fellowship
FX We thank Yusuf Aytar for dataset discussions. We thank MIT TIG,
   especially Garrett Wollman, for troubleshooting issues on storing the 26
   TB of video. We are grateful for the Torch7 community for answering many
   questions. NVidia donated GPUs used for this research. This work was
   supported by NSF grant #1524817 to AT, START program at UMBC to HP, and
   the Google PhD fellowship to CV.
CR Aytar Yusuf, 2016, NIPS
   Basha Tali, 2012, ECCV
   Chen Chao- Yeh, 2013, CVPR
   Denton E. L., 2015, NIPS
   Doersch C, 2015, ICCV
   Finn Chelsea, 2016, IAN GOODFELLOW SERGE
   Fiser Jozsef, 2002, JEP
   Fragkiadaki Katerina, 2015, ICCV
   Goodfellow I., 2014, NIPS
   Hadsell Raia, 2006, CVPR
   Ioffe S., 2015, BATCH NORMALIZATION
   Isola Phillip, 2015, CVPR
   Jayaraman Dinesh, 2015, ICCV
   Ji Shuiwang, 2013, PAMI
   Kalchbrenner Nal, 2016, VIDEO PIXEL NETWORKS
   Kingma D., 2014, ADAM METHOD STOCHAST
   Kitani Kris M., 2012, ECCV
   Le Quoc V, 2013, CASSP
   Li Yin, 2015, UNSUPERVISED LEARNIN
   Lotter W, 2016, DEEP PREDICTIVE CODI
   Lowe David G, 1999, ICCV
   Mathieu M., 2015, DEEP MULTISCALE VIDE
   Mirza M., 2014, CONDITIONAL GENERATI
   Misra Ishan, 2016, ECCV
   Mobahi H., 2009, ICML
   Nguyen Phuc Xuan, 2016, OPEN WORLD MICROVIDE
   Owens Andrew, 2016, AMBIENT SOUND PROVID
   Pathak D., 2016, CONTEXT ENCODERS FEA
   Petrovic Nikola, 2006, CVPR
   Pickup Lyndsey, 2014, CVPR
   Radford A., 2015, UNSUPERVISED REPRESE
   Ramanathan Vignesh, 2015, CVPR
   Ranzato Marc Aurelio, 2014, VIDEO LANGUAGE MODEL
   Simonyan K., 2014, NIPS
   Soomro K., 2012, UCF101 DATASET 101 H
   Srivastava N., 2014, JMLR
   Srivastava N, 2015, UNSUPERVISED LEARNIN
   Theis L., 2015, NOTE EVALUATION GENE
   Thomee Bart, 2016, ACM
   Tran D, 2014, LEARNING SPATIOTEMPO
   Vondrick Carl, 2013, IJCV
   Vondrick Carl, 2015, CVPR
   Walker  Jacob, 2014, CVPR, P2
   Wang H, 2013, ICCV
   Wang  L., 2015, GOOD PRACTICES VERY
   Wang X., 2016, GENERATIVE IMAGE MOD
   Wang X., 2015, ICCV
   Xu B., 2015, EMPIRICAL EVALUATION
   Xue Tianfan, 2016, VISUAL DYNAMICS PROB
   Yuen J, 2010, ECCV
   Zeiler M. D., 2010, CVPR
   Zhou  B., 2014, NIPS
   Zhou Bolei, 2014, OBJECT DETECTORS EME
   Zhou Yipin, 2016, ECCV
   Zhou Yipin, 2015, ICCV
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701051
DA 2019-06-15
ER

PT S
AU Dinh, V
   Ho, LST
   Nguyen, D
   Nguyen, BT
AF Vu Dinh
   Ho, Lam Si Tung
   Duy Nguyen
   Nguyen, Binh T.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast learning rates with heavy-tailed losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INEQUALITIES; MINIMIZATION; CONVERGENCE
AB We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function sup(f) (is an element of) (F) vertical bar l o f vertical bar, where l is the loss function and F is the hypothesis class, exists and is L-r -integrable, and (ii) l satisfies the multi-scale Bernstein's condition on F. Under these assumptions, we prove that learning rate faster than O (n(1/2)) can be obtained and, depending on r and the multi-scale Bernstein's powers, can be arbitrarily close to O (n(-1)). We then verify these assumptions and derive fast learning rates for the problem of vector quantization by k -means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.
C1 [Vu Dinh] Fred Hutchinson Canc Res Ctr, Program Computat Biol, 1124 Columbia St, Seattle, WA 98104 USA.
   [Ho, Lam Si Tung] Univ Calif Los Angeles, Dept Biostat, Los Angeles, CA 90024 USA.
   [Duy Nguyen] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.
   [Nguyen, Binh T.] Univ Sci, Dept Comp Sci, Ho Chi Minh City, Vietnam.
RP Dinh, V (reprint author), Fred Hutchinson Canc Res Ctr, Program Computat Biol, 1124 Columbia St, Seattle, WA 98104 USA.
FU National Science Foundation [DMS-1223057, CISE-1564137]; National
   Institutes of Health [U54GM111274]; NSF [IIS 1251151]
FX Vu Dinh was supported by DMS-1223057 and CISE-1564137 from the National
   Science Foundation and U54GM111274 from the National Institutes of
   Health. Lam Si Tung Ho was supported by NSF grant IIS 1251151.
CR Antos A, 2005, IEEE T INFORM THEORY, V51, P4013, DOI 10.1109/TIT.2005.856976
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P1802, DOI 10.1109/18.705560
   Ben-David S, 2007, MACH LEARN, V66, P243, DOI 10.1007/s10994-006-0587-3
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169
   Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350
   Cortes Corinna, 2013, ARXIV13105796
   Grunwald Peter, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P169, DOI 10.1007/978-3-642-34106-9_16
   Hang H, 2014, J MULTIVARIATE ANAL, V127, P184, DOI 10.1016/j.jmva.2014.02.012
   Hsu D, 2016, J MACH LEARN RES, V17
   Lecue G, 2012, ANN STAT, V40, P832, DOI 10.1214/11-AOS965
   Lecue Guillaume, 2013, ARXIV13054825
   Lederer J, 2014, BERNOULLI, V20, P2020, DOI 10.3150/13-BEJ549
   Levrard C, 2013, ELECTRON J STAT, V7, P1716, DOI 10.1214/13-EJS822
   LINDER T, 1994, IEEE T INFORM THEORY, V40, P1728, DOI 10.1109/18.340451
   Mehta Nishant A., 2014, ADV NEURAL INFORMATI, P1197
   Mendelson S, 2008, J COMPLEXITY, V24, P380, DOI 10.1016/j.jco.2007.09.001
   POLLARD D, 1982, ANN PROBAB, V10, P919, DOI 10.1214/aop/1176993713
   Steinwart Ingo, 2009, ADV NEURAL INF PROCE, p[1768, 6]
   Telgarsky Matus J, 2013, ADV NEURAL INFORM PR, P2940
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
   Dinh V, 2015, LECT NOTES COMPUT SC, V9076, P375, DOI 10.1007/978-3-319-17142-5_32
   Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439
   Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700068
DA 2019-06-15
ER

PT S
AU Vuffray, M
   Misra, S
   Lokhov, AY
   Chertkov, M
AF Vuffray, Marc
   Misra, Sidhant
   Lokhov, Andrey Y.
   Chertkov, Michael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Interaction Screening: Efficient and Sample-Optimal Learning of Ising
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID FRAMEWORK; FIELDS
AB We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of "interaction screening". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.
C1 [Vuffray, Marc; Lokhov, Andrey Y.; Chertkov, Michael] Los Alamos Natl Lab, Theoret Div T 4, Los Alamos, NM 87545 USA.
   [Misra, Sidhant] Los Alamos Natl Lab, Theoret Div T 5, Los Alamos, NM 87545 USA.
   [Lokhov, Andrey Y.; Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA.
   [Chertkov, Michael] Skolkovo Inst Sci & Technol, Moscow 143026, Russia.
RP Vuffray, M (reprint author), Los Alamos Natl Lab, Theoret Div T 4, Los Alamos, NM 87545 USA.
EM vuffray@lanl.gov; sidhant@lanl.gov; lokhov@lanl.gov; chertkov@lanl.gov
FU U.S. Department of Energy's Office of Electricity as part of the DOE
   Grid Modernization Initiative
FX We are thankful to Guy Bresler and Andrea Montanari for valuable
   discussions, comments and insights. The work was supported by funding
   from the U.S. Department of Energy's Office of Electricity as part of
   the DOE Grid Modernization Initiative.
CR Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032
   Bresler G., 2015, P 47 ANN ACM S THEOR, P771
   Bresler G., 2014, P NIPS, V27, P1062
   Bresler G, 2013, SIAM J COMPUT, V42, P563, DOI 10.1137/100796029
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   D'Aspremont A, 2008, SIAM J MATRIX ANAL A, V30, P56, DOI 10.1137/060670985
   Deka D., 2015, IEEE CONTROL N UNPUB
   Eagle N, 2009, P NATL ACAD SCI USA, V106, P15274, DOI 10.1073/pnas.0900282106
   He M, 2011, IEEE T SMART GRID, V2, P342, DOI 10.1109/TSG.2011.2129544
   Johnson J. K., 2015, J MACHINE LEARNING
   Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386
   Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/nmeth.2016, 10.1038/NMETH.2016]
   Montanari A., 2009, ADV NEURAL INFORM PR, P1303
   Montanari A, 2015, ELECTRON J STAT, V9, P2370, DOI 10.1214/15-EJS1059
   Morcos F, 2011, P NATL ACAD SCI USA, V108, pE1293, DOI 10.1073/pnas.1111471108
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015
   Roth S, 2005, PROC CVPR IEEE, P860
   Roudi Y, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051915
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703011
DA 2019-06-15
ER

PT S
AU Wan, YL
   Meila, M
AF Wan, Yali
   Meila, Marina
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Graph Clustering: Block-models and model free results
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain "correctness" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.
C1 [Wan, Yali; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
RP Wan, YL (reprint author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.
EM yaliwan@washington.edu; mmp@stat.washington.edu
CR Abbe Emmanuel, 2015, ARXIV150300609
   Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Airoldi Edoardo M., 2011, ARXIV11056245
   Awasthi Pranjal, 2016, ENCY ALGORITHMS, P331
   Bach FR, 2006, J MACH LEARN RES, V7, P1963
   Balcan MF, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P767
   Ben-David Shai, 2015, CORR
   Bhatia  R., 2013, MATRIX ANAL, V169
   Bilu Yonatan, 2009, CORR
   Chung F. R., 1997, SPECTRAL GRAPH THEOR, V92
   Karrer B, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.046119
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Meila M, 2012, MACH LEARN, V86, P369, DOI 10.1007/s10994-011-5267-2
   Meila Marina, 2005, P ART INT STAT WORKS
   Ng Andrew Y., 2002, ADV NEURAL INFORM PR
   Peng Richard, 2015, P 28 C LEARN THEOR C, P1423
   ROHE K., 2013, ADV NEURAL INFORM PR, V26, P3120
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Stewart Gilbert W, 1990, MATRIX PERTURBATION, V175
   Wan Yali, 2015, ADV NEURAL INFORM PR
NR 20
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700104
DA 2019-06-15
ER

PT S
AU Wang, B
   Zhu, JJ
   Ursu, O
   Pourshafeie, A
   Batzoglou, S
   Kundaje, A
AF Wang, Bo
   Zhu, Junjie
   Ursu, Oana
   Pourshafeie, Armin
   Batzoglou, Serafim
   Kundaje, Anshul
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Learning from Noisy Networks with Applications to Hi-C Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CHROMATIN; PRINCIPLES; DOMAINS; GENOME
AB Complex networks play an important role in a plethora of disciplines in natural sciences. Cleaning up noisy observed networks poses an important challenge in network analysis. Existing methods utilize labeled data to alleviate the noise the noise levels. However, labeled data is usually expensive to collect while unlabeled data can be gathered cheaply. In this paper, we propose an optimization framework to mine useful structures from noisy networks in an unsupervised manner. The key feature of our optimization framework is its ability to utilize local structures as well as global patterns in the network. We extend our method to incorporate multi-resolution networks in order to add further resistance in the presence of high-levels of noise. The framework is generalized to utilize partial labels in order to further enhance the performance. We empirically test the effectiveness of our method in denoising a network by demonstrating an improvement in community detection results on multi-resolution Hi-C data both with and without Capture-C-generated partial labels.
C1 [Wang, Bo; Batzoglou, Serafim; Kundaje, Anshul] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Zhu, Junjie] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Ursu, Oana; Kundaje, Anshul] Stanford Univ, Dept Genet, Stanford, CA 94305 USA.
   [Pourshafeie, Armin] Stanford Univ, Dept Phys, Stanford, CA 94305 USA.
RP Wang, B (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM bowang87@stanford.edu
FU Stanford Graduate Fellowship; Stanford Genome Training Program [NIH
   5T32HG000044-17]; Alfred Sloan Foundation Fellowship; HHMI International
   Students Research Fellowship; NIH Sidow grant [1R01CA183904-01A1]
FX We would also like to thank Nasa Sinnott-Armstrong for initial advice on
   this project. JZ acknowledges support from Stanford Graduate Fellowship.
   AP was partially supported by Stanford Genome Training Program: NIH
   5T32HG000044-17. AK was supported by the Alfred Sloan Foundation
   Fellowship. OU is supported by the HHMI International Students Research
   Fellowship. BW and SB were supported by NIH Sidow grant
   (1R01CA183904-01A1).
CR Alexander S., 2003, CLUSTER ENSEMBLES KN
   Blondel V.D., 2008, J STAT MECH-THEORY E, V10, DOI DOI 10.1088/1742-5468/2008/10/P10008
   Cabreros I., 2015, 150905121 ARXIV
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chiquet J., 2011, INFERRING MULTIPLE G
   de Laat W, 2013, NATURE, V502, P499, DOI 10.1038/nature12753
   Dekker J, 2002, SCIENCE, V295, P1306, DOI 10.1126/science.1067799
   Dekker J, 2008, SCIENCE, V319, P1793, DOI 10.1126/science.1152850
   Dixon JR, 2012, NATURE, V485, P376, DOI 10.1038/nature11082
   Dostie J, 2006, GENOME RES, V16, P1299, DOI 10.1101/gr.5571506
   ENCODE Project Consortium, 2012, NATURE
   Ernst J, 2012, NAT METHODS, V9, P215, DOI 10.1038/nmeth.1906
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Grubert F, 2015, CELL, V162, P1051, DOI 10.1016/j.cell.2015.07.048
   Huang J, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-83
   Lettice LA, 2003, HUM MOL GENET, V12, P1725, DOI 10.1093/hmg/ddg180
   Lieberman-Aiden E, 2009, SCIENCE, V326, P289, DOI 10.1126/science.1181369
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Liu G., 2013, ROBUST RECOVERY SUBS
   Marcotte EM, 1999, SCIENCE, V285, P751, DOI 10.1126/science.285.5428.751
   Mifsud B, 2015, NAT GENET, V47, P598, DOI 10.1038/ng.3286
   Pritchard JK, 2000, GENETICS
   Qin YJ, 2004, HUM MOL GENET, V13, P1213, DOI 10.1093/hmg/ddh141
   Rao SSP, 2014, CELL, V159, P1665, DOI 10.1016/j.cell.2014.11.021
   Shaw Peter J, 2010, F1000 Biol Rep, V2, DOI 10.3410/B2-18
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonis M, 2006, NAT GENET, V38, P1348, DOI 10.1038/ng1896
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang B., 2012, COMPUTER VISION PATT
   Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/NMETH.2810, 10.1038/nmeth.2810]
   Wang Bo, 2013, SPARSE SUBSPACE DENO
   Yang J, 2013, IEEE DATA MINING, P1151, DOI 10.1109/ICDM.2013.167
   Zelnik-Manor L., 2005, SELF TUNING SPECTRAL, P1601
   Zhao Z, 2006, NAT GENET, V38, P1341, DOI 10.1038/ng1891
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702038
DA 2019-06-15
ER

PT S
AU Wang, H
   Shi, XJ
   Yeung, DY
AF Wang, Hao
   Shi, Xingjian
   Yeung, Dit-Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Natural-Parameter Networks: A Class of Probabilistic Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.
C1 [Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
RP Wang, H (reprint author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM hwangaz@cse.ust.hk; xshiab@cse.ust.hk; dyyeung@cse.ust.hk
CR Balan A. K., 2015, NIPS
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bishop C. M., 2006, PATTERN RECOGNITION
   Blundell C., 2015, ICML
   Chen  Tianqi, 2015, ABS151201274 CORR
   CLARK CE, 1961, OPER RES, V9, P145, DOI 10.1287/opre.9.2.145
   David J. MacKay, 1992, NEURAL COMPUTATION
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A., 2011, NIPS
   Henao R., 2015, NIPS
   Hernandez-Lobato J. M., 2015, ICML
   Hinton Geoffrey E, 1993, COLT, P8
   Karpathy A., 2015, CVPR
   Kingma D.P., 2013, ARXIV13126114
   Lee D. D., 2001, NIPS
   Leskovec J., 2014, SNAP DATASETS STANFO
   Neal R M, 1995, THESIS
   Neal Radford M., 1990, LEARNING STOCHASTIC
   Ranganath R., 2015, AISTATS
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang C., 2011, KDD
   Wang  H., 2013, IJCAI
   Wang H., 2016, TKDE
   Wang H., 2015, KDD
   Zhou M., 2012, AISTATS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702026
DA 2019-06-15
ER

PT S
AU Wang, H
   Shi, XJ
   Yeung, DY
AF Wang, Hao
   Shi, Xingjian
   Yeung, Dit-Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in
   the Blanks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.
C1 [Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
RP Wang, H (reprint author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM hwangaz@cse.ust.hk; xshiab@cse.ust.hk; dyyeung@cse.ust.hk
CR Bengio Y., 2015, DEEP LEARNING UNPUB
   Chen TQ, 2012, J MACH LEARN RES, V13, P3619
   Cho  K., 2014, EMNLP
   Chung J, 2015, NIPS
   Fabius Otto, 2014, ARXIV14126581
   Georgiev K., 2013, ICML
   Hidasi B., 2015, ARXIV151106939
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hu Y., 2008, ICDM
   Oord A. V. D., 2013, NIPS
   Papineni K., 2002, ACL
   Rendle S., 2009, UAI
   Ricci F., 2011, INTRO RECOMMENDER SY
   Sainath Tara N., 2013, ICASSP
   Salakhutdinov R., 2007, ICML
   Salakhutdinov R., 2007, NIPS
   Singh A. P., 2008, KDD
   Sutskever  I., 2014, NIPS
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang C., 2011, KDD
   Wang  H., 2013, IJCAI
   Wang H., 2015, AAAI
   Wang H., 2016, TKDE
   Wang H., 2015, KDD
   Wang X., 2014, ACM MM
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701020
DA 2019-06-15
ER

PT S
AU Wang, JZ
   Wang, WM
   Chen, XT
   Wang, RG
   Gao, W
AF Wang, Jinzhuo
   Wang, Wenmin
   Chen, Xiongtao
   Wang, Ronggang
   Gao, Wen
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Alternative Neural Network: Exploring Contexts as Early as Possible
   for Action Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RECEPTIVE FIELDS
AB Contexts are crucial for action recognition in video. Current methods often mine contexts after extracting hierarchical local features and focus on their high-order encoding s. This paper instead explores contexts as early as possible and leverages their evolutions for action recognition. In particular, we introduce a novel architecture called deep alternative neural network (DANN) stacking alternative layers. Each alternative layer consists of a volumetric convolutional layer followed by a recurrent layer. The former acts as local feature learner while the latter is used to collect contexts. Compared with feed-forward neural networks, DANN learns contexts of local features from the very beginning. This setting helps to preserve hierarchical context evolutions which we show are essential to recognize similar actions. Besides, we present an adaptive method to determine the temporal size for network input based on optical flow energy, and develop a volumetric pyramid pooling layer to deal with input clips of arbitrary sizes. We demonstrate the advantages of DANN on two benchmarks HMDB51 and UCF101 and report competitive or superior results to the state-of-the-art.
C1 [Wang, Jinzhuo; Wang, Wenmin; Chen, Xiongtao; Wang, Ronggang] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing, Peoples R China.
RP Wang, JZ (reprint author), Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.
EM jzwang@pku.edu.cn; wangwm@ece.pku.edu.cn; cxt@pku.edu.cn;
   rgwang@ece.pku.edu.cn; wgao@pku.edu.cn
FU Shenzhen Peacock Plan [20130408-183003656]
FX The work was supported by Shenzhen Peacock Plan (20130408-183003656).
CR Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25
   Collobert R, 2011, NIPS WORKSH
   Dayan P., 2001, THEORETICAL NEUROSCI, V806
   Deco G, 2004, EUR J NEUROSCI, V20, P1089, DOI 10.1111/j.1460-9568.2004.03528.x
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kantorov V, 2014, PROC CVPR IEEE, P2593, DOI 10.1109/CVPR.2014.332
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Liang M., 2015, ADV NEURAL INFORM PR, V28, P937
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   NG JYH, 2015, CVPR, P4694
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Peng Xiaojiang, 2014, ARXIV14054506
   Pinheiro P., 2014, P 31 INT C MACH LEAR, P82, DOI DOI 10.1016/J.VETPAR.2009.02.011
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Soomro K., 2012, ARXIV12120402
   Srivastava N., 2015, INT C MACH LEARN, P843
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Varol G., 2016, ARXIV160404494
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L, 2015, ARXIV150702159
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   WATERS RL, 1972, J ANAT, V111, P191
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214
   Zeiler MD, 2013, ARXIV201313013557
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702082
DA 2019-06-15
ER

PT S
AU Wang, MD
   Liu, J
   Fang, EX
AF Wang, Mengdi
   Liu, Ji
   Fang, Ethan X.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Accelerating Stochastic Composition Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.
C1 [Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA.
   Univ Rochester, Rochester, NY 14627 USA.
   Penn State Univ, University Pk, PA 16802 USA.
RP Wang, MD (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM mengdiw@princeton.edu; ji.liu.uwisc@gmail.com; xxf13@psu.edu
FU NSF [CNS-1548078, DMS-10009141]
FX This project is in part supported by NSF grants CNS-1548078 and
   DMS-10009141.
CR Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas DP, 2011, MATH PROGRAM, V129, P163, DOI 10.1007/s10107-011-0472-0
   Dai B., 2016, ARXIV160704579
   Dann C, 2014, J MACH LEARN RES, V15, P809
   Dentcheva D., 2015, ARXIV150402658
   Ermoliev Y. M., 1976, MONOGRAPHS OPTIMIZAT
   Ghadimi  Saeed, 2015, MATH PROGRAM, P1
   Gurbuzbalaban M., 2015, ARXIV150602081
   Liu B., 2015, 31 C UNC ART INT AMS
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Nedic A, 2001, SIAM J OPTIMIZ, V12, P109, DOI 10.1137/S1052623499362111
   Nedic A, 2011, MATH PROGRAM, V129, P225, DOI 10.1007/s10107-011-0468-9
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Rakhlin A., 2012, P 29 INT C MACH LEAR, P449
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Wang M., 2016, P WINT SIM C
   Wang M., 2015, ARXIV151103760
   Wang M., 2016, MATH PROGRAMMING A
   Wang MD, 2016, SIAM J OPTIMIZ, V26, P681, DOI 10.1137/130931278
   White A., 2016, ARXIV160208771
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703074
DA 2019-06-15
ER

PT S
AU Wang, P
   Shen, XH
   Russell, B
   Cohen, S
   Price, B
   Yuille, A
AF Wang, Peng
   Shen, Xiaohui
   Russell, Bryan
   Cohen, Scott
   Price, Brian
   Yuille, Alan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI SURGE: Surface Regularized Geometry Estimation from a Single Image
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a four-stream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30 % relative improvement in planar consistency on the NYU v2 dataset [24].
C1 [Wang, Peng] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
   [Shen, Xiaohui; Russell, Bryan; Cohen, Scott; Price, Brian] Adobe Res, San Jose, CA USA.
   [Yuille, Alan] Johns Hopkins Univ, Baltimore, MD 21218 USA.
RP Wang, P (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
FU NSF Expedition for Visual Cortex on Silicon NSF [CCF-1317376]; Army
   Research Office ARO [62250-CS]
FX This work is supported by the NSF Expedition for Visual Cortex on
   Silicon NSF award CCF-1317376 and the Army Research Office ARO 62250-CS.
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Bansal Aayush, 2016, CVPR
   Barron J. T., 2015, FAST BILATERAL SOLVE
   Barron Jonathan T, 2015, CVPR
   Chen L. C., 2015, ICLR
   Eigen D., 2015, ICCV
   Eigen David, 2014, NIPS
   Guo  R., 2013, ICCV
   Gupta S., 2014, ECCV
   Hoiem  D., 2007, ICCV
   Honauer  K., 2015, ICCV
   Ikehata  S., 2015, ICCV
   Jia Y., 2014, ARXIV14085093
   Kokkinos  I., 2016, ICLR
   Krahenbuhl  P., 2013, ICML
   Krahenbuhl  P., 2012, ECCV
   Krahenbuhl P., 2012, NIPS
   Ladicky  L., 2014, NON TRADITIONAL REF
   Li B., 2015, CVPR
   Liu  F., 2015, CVPR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song Shuran, 2015, CVPR
   Srajer  F., 2014, 2 INT C 3D VIS 3DV 2, V1
   Srivastava N., 2014, J MACH LEARN RES, V15
   Wang  P., 2015, CVPR
   Wang Xiaolong, 2015, CVPR
   Xiao  J., 2012, ECCV
   Xie  S., 2015, ICCV
   Zheng S, 2015, INT C COMP VIS ICCV
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704027
DA 2019-06-15
ER

PT S
AU Wang, SL
   Fidler, S
   Urtasun, R
AF Wang, Shenlong
   Fidler, Sanja
   Urtasun, Raquel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Proximal Deep Structured Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related. In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the dependencies between continuous output variables. We show that inference in our model using proximal methods can be efficiently solved as a feed-foward pass of a special type of deep recurrent neural network. We demonstrate the effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.
C1 [Wang, Shenlong; Fidler, Sanja; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada.
RP Wang, SL (reprint author), Univ Toronto, Toronto, ON, Canada.
EM slwang@cs.toronto.edu; fidler@cs.toronto.edu; urtasun@cs.toronto.edu
CR Belanger  D., 2016, ICML
   Chambolle A., 2011, JMIV
   Chen L.-C., 2015, ICML
   Chen T., 2015, MXNET FLEXIBLE EFFIC
   Chen Y.-T., 2015, CVPR
   Dabov K., 2007, TIP
   Deng J., 2014, ECCV
   Domke J., 2012, AISTATS
   Eigen D., 2015, ICCV
   Fanello S., 2014, CVPR
   Fischer P., 2015, CVPR
   Gabay D., 1976, COMPUTERS MATH APPL
   Geman D., 1995, TIP
   Gregor K., 2010, ICML
   He K, 2015, DEEP RESIDUAL LEARNI
   He K., 2015, ICCV
   Hinton G, 2012, DEEP NEURAL NETWORKS
   Ihler A., 2009, AISTATS
   Kingma D., 2014, ADAM METHOD STOCHAST
   Krishnan  D., 2009, NIPS
   Krizhevsky A., 2012, NIPS
   Mairal J., 2009, ICCV
   Martin  D., 2001, ICCV
   Mayer N., 2015, LARGE DATASET TRAIN
   Newcombe R., 2011, ISMAR
   Parikh Neal, 2014, FDN TRENDS OPTIMIZAT
   Ross S., 2011, CVPR
   Roth S., 2005, CVPR
   Schmidt  U., 2014, CVPR
   Schmidt U., 2013, PAMI
   Schwing A.G., 2015, FULLY CONNECTED DEEP
   Sudderth E., 2010, COMMUNICATIONS ACM
   Sutskever  I., 2014, NIPS
   Tsochantaridis I, 2004, ICML
   Wang S., 2014, NIPS
   Weiss Y., 2001, NEURAL COMPUTATION
   Zach C., 2012, ECCV
   Zbontar  J., 2015, CVPR
   Zheng S., 2015, ICCV
   Zoran D., 2011, ICCV
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700038
DA 2019-06-15
ER

PT S
AU Wang, TY
   Berthet, Q
   Plan, Y
AF Wang, Tengyao
   Berthet, Quentin
   Plan, Yaniv
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Average-case hardness of RIP certification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY; SPARSE; CLIQUES
AB The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models. It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it. These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime. Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.
C1 [Wang, Tengyao; Berthet, Quentin] Ctr Math Sci, Cambridge CB3 0WB, England.
   [Plan, Yaniv] 1986 Math Rd, Vancouver, BC V6T 1Z2, Canada.
RP Wang, TY (reprint author), Ctr Math Sci, Cambridge CB3 0WB, England.
EM t.wang@statslab.cam.ac.uk; q.berthet@statslab.cam.ac.uk;
   yaniv@math.ubc.ca
CR Alon N, 2007, ACM S THEORY COMPUT, P496, DOI 10.1145/1250790.1250863
   Arias-Castro E, 2014, ANN STAT, V42, P940, DOI 10.1214/14-AOS1208
   Awasthi P., 2015, J MACH LEARN RES COL, V40
   Bandeira A. S., 2014, INT MATH RES NOTICES
   Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Berthet Q., 2013, C LEARN THEOR, V30, P1046
   Berthet Q., 2015, DETECTION PLANTED SO
   Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127
   Bhaskara A, 2010, ACM S THEORY COMPUT, P201
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Blum A, 2003, J ACM, V50, P506, DOI 10.1145/792538.792543
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Bourgain J, 2011, DUKE MATH J, V159, P145, DOI 10.1215/00127094-1384809
   Candes E. J., 2006, COMMUNICATIONS PURE, V59, P2006
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   CHEN Y., 2014, ARXIV14021267
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   d'Aspremont A, 2011, MATH PROGRAM, V127, P123, DOI 10.1007/s10107-010-0416-0
   Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Feige U, 2003, SIAM J COMPUT, V32, P345, DOI 10.1137/S009753970240118X
   Feige U., P 34 ANN ACM S THEOR, P534
   FELDMAN V., 2013, P 45 ANN ACM S THEOR, P655
   Gao C., 2014, ARXIV14098565
   Hajek B. E., 2015, P 28 C LEARN THEOR C, P899
   Hazan E, 2011, SIAM J COMPUT, V40, P79, DOI 10.1137/090766991
   JERRUM M, 1992, RANDOM STRUCT ALGOR, V3, P347, DOI 10.1002/rsa.3240030402
   Juditsky A, 2011, MATH PROGRAM, V127, P57, DOI 10.1007/s10107-010-0417-z
   Juels A, 2000, DESIGN CODE CRYPTOGR, V20, P269, DOI 10.1023/A:1008374125234
   Koiran P., 2012, ARXIV12110665
   Lee K, 2008, INT CONF ACOUST SPEE, P5129
   Ma Z., 2013, COMPUTATIONAL BARRIE
   Mallat S., 1999, WAVELET TOUR SIGNAL
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Rauhut H., 2013, MATH INTRO COMPRESSI
   Tillmann Andreas M., 2014, IEEE Transactions on Information Theory, V60, P1248, DOI 10.1109/TIT.2013.2290112
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369
   Zhang Y., 2014, COLT, V35, P921
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700096
DA 2019-06-15
ER

PT S
AU Wang, WR
   Wang, JL
   Garber, D
   Srebro, N
AF Wang, Weiran
   Wang, Jialei
   Garber, Dan
   Srebro, Nathan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient Globally Convergent Stochastic Optimization for Canonical
   Correlation Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA, and the shift-and-invert preconditioning method for PCA, we propose two globally convergent meta-algorithms for CCA, both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance.
C1 [Wang, Weiran; Garber, Dan; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Wang, Jialei] Univ Chicago, Chicago, IL 60637 USA.
RP Wang, WR (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
EM weiranwang@ttic.edu; jialei@uchicago.edu; dgarber@ttic.edu;
   nati@ttic.edu
FU NSF BIGDATA [1546500]
FX Research partially supported by NSF BIGDATA grant 1546500.
CR Arora R., 2012, ALLERTON
   Balsubramani A., 2013, NIPS
   Frostig  Roy, 2015, ICML
   Garber D., 2015, FAST SIMPLE PCA VIA
   Ge R., 2016, EFFICIENT ALGORITHMS
   Golub G. H., 1996, MATRIX COMPUTATIONS
   Golub G.H., 1995, LINEAR ALGEBRA SIGNA, V69, P27
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Jin C., 2015, ROBUST SHIFT AND INV
   Johnson R., 2013, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin H., 2015, NIPS
   Lu Y., 2014, NIPS
   Ma Z., 2015, ICML
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Schmidt M., 2013, 00860051 HAL
   Shalev-Shwartz S., 2013, J MACHINE LEARNING R
   Shamir O., 2015, ICML
   Snoek C., 2006, MULTIMEDIA
   Vinod H. D., 1976, J ECONOMETRICS
   Wang W., 2015, ALLERTON
   Warmuth M., 2008, J MACHINE LEARNING R
   Westbury John, 1994, XRAY MICROBEAM SPEEC
   Witten D., 2009, BIOSTATISTICS
   Xie B., 2015, NIPS
   Yger F., 2012, ICML
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703095
DA 2019-06-15
ER

PT S
AU Wang, XY
   Dunson, D
   Leng, CL
AF Wang, Xiangyu
   Dunson, David
   Leng, Chenlei
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI DECOrrelated feature space partitioning for distributed sparse
   regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODEL SELECTION; REGULARIZATION
AB Fitting statistical models is computationally challenging when the sample size or the dimension of the dataset is huge. An attractive approach for down-scaling the problem size is to first partition the dataset into subsets and then fit using distributed algorithms. The dataset can be partitioned either horizontally (in the sample space) or vertically (in the feature space). While the majority of the literature focuses on sample space partitioning, feature space partitioning is more effective when p >> n. Existing methods for partitioning features, however, are either vulnerable to high correlations or inefficient in reducing the model dimension. In this paper, we solve these problems through a new embarrassingly parallel framework named DECO for distributed variable selection and parameter estimation. In DECO, variables are first partitioned and allocated to m distributed workers. The decorrelated subset data within each worker are then fitted via any algorithm designed for high-dimensional problems. We show that by incorporating the decorrelation step, DECO can achieve consistent variable selection and parameter estimation on each subset with (almost) no assumptions. In addition, the convergence rate is nearly minimax optimal for both sparse and weakly sparse models and does NOT depend on the partition number m. Extensive numerical experiments are provided to illustrate the performance of the new framework.
C1 [Wang, Xiangyu; Dunson, David] Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.
   [Leng, Chenlei] Univ Warwick, Dept Stat, Coventry, W Midlands, England.
RP Wang, XY (reprint author), Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.
EM wwrechard@gmail.com; dunson@stat.duke.edu; C.Leng@warwick.ac.uk
CR Chen JH, 2008, BIOMETRIKA, V95, P759, DOI 10.1093/biomet/asn034
   Cortez P., 2008, USING DATA MINING PR
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Jia Jinzhu, 2012, ARXIV12085584
   Mairal Julien, 2012, P 29 INT C MACH LEAR, P353
   Mcdonald Ryan, 2009, ADV NEURAL INFORM PR, P1231
   Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x
   Minsker S, 2015, BERNOULLI, V21, P2308, DOI 10.3150/14-BEJ645
   Raskutti Garvesh, 2009, COMM CONTR COMP 2009, P251
   Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370
   Scheetz TE, 2006, P NATL ACAD SCI USA, V103, P14429, DOI 10.1073/pnas.0602562103
   Scott Steven L, 2013, EFABBAYES 250 C, V16
   Song Qifan, 2014, J ROYAL STAT SOC B
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Trindade Artur, 2014, UCI MACHINE LEARNING
   Wang X., 2014, ADV NEURAL INFORM PR, P2195
   Wang Xiangyu, 2015, ADV NEURAL INFORM PR, P451
   Ye F, 2010, J MACH LEARN RES, V11, P3519
   Zhang Y., 2012, ADV NEURAL INFORM PR, P1502, DOI DOI 10.1109/CDC.2012.642669
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhou Yingbo, 2014, ADV NEURAL INFORM PR, V27, P3554
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702096
DA 2019-06-15
ER

PT S
AU Wang, XA
   Dasgupta, S
AF Wang, Xinan
   Dasgupta, Sanjoy
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An algorithm for l(1) nearest neighbor search via monotonic embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID OPTIMAL HASHING ALGORITHMS; RECOGNITION; JOHNSON
AB Fast algorithms for nearest neighbor (NN) search have in large part focused on l(2) distance. Here we develop an approach for l(1) distance that begins with an explicit and exactly distance-preserving embedding of the points into l(2)(2). We show how this can efficiently be combined with random-projection based methods for l(2) NN search, such as locality-sensitive hashing (LSH) or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives.
C1 [Wang, Xinan; Dasgupta, Sanjoy] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Wang, XA (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM xinan@ucsd.edu; dasgupta@cs.ucsd.edu
FU National Science Foundation [IIS-1162581]
FX The authors are grateful to the National Science Foundation for support
   under grant IIS-1162581.
CR Achlioptas D, 2001, P 20 ACM SIGMOD SIGA, P274, DOI [DOI 10.1145/375551.375608, 10.1145/375551.375608]
   Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096
   Andoni A., 2005, TECHNICAL REPORT
   Andoni A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1203, DOI 10.1145/1109557.1109690
   Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Arriaga R. I., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P616, DOI 10.1109/SFFCS.1999.814637
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Borg I., 1997, MODERN MULTIDIMENSIO
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 2000, J COMPUT SYST SCI, V60, P630, DOI 10.1006/jcss.1999.1690
   Cardoso-Cachopo A, 2007, THESIS
   Charikar M, 2002, ANN IEEE SYMP FOUND, P551, DOI 10.1109/SFCS.2002.1181979
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Clment K., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dasgupta S, 2015, ALGORITHMICA, V72, P237, DOI 10.1007/s00453-014-9885-5
   Datar M., 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Indyk P, 2006, J ACM, V53, P307, DOI 10.1145/1147954.1147955
   Jia Y., 2014, ARXIV14085093
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kleinberg J. M., 1997, STOC 97, P599, DOI DOI 10.1145/258533.258653
   Li P., 2013, ADV NEURAL INFORM PR, V26, P2571
   Linial N., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P577, DOI 10.1109/SFCS.1994.365733
   Liu T., 2004, ADV NEURAL INFORM PR, V17, P825
   Omohundro S. M., 1991, NIPS, V40, P175
   Rudelson M, 2007, J ACM, V54, DOI 10.1145/1255443.1255449
   Shrivastava A, 2014, NIPS, V27, P2321
   Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Uhlmann J. K., 1991, IMPLEMENTING M UNPUB
   UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wells J. H., 1975, EMBEDDINGS EXTENSION, V84
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701084
DA 2019-06-15
ER

PT S
AU Wang, YC
   Du, N
   Trivedi, R
   Song, L
AF Wang, Yichen
   Du, Nan
   Trivedi, Rakshit
   Song, Le
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Coevolutionary Latent Feature Processes for Continuous-Time User-Item
   Interactions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time, users' and items' feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. To learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.
C1 [Du, Nan] Google Res, Mountain View, CA USA.
   [Wang, Yichen; Trivedi, Rakshit; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
RP Wang, YC (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM yichen.wang@gatech.edu; dunan@google.com; rstrivedi@gatech.edu;
   lsong@cc.gatech.edu
FU NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF
   [IIS-1218749, CAREER IIS-1350983]
FX This project was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR
   N00014-15-1-2340, NSF IIS-1218749, and NSF CAREER IIS-1350983.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Agarwal D., 2009, KDD
   Baltrunas L., 2009, TIME DEPENDANT RECOM
   Bhargava J. Z. J. L. Preeti, 2015, WWW
   Charlin L., 2015, RECSYS
   Chen Y., 2009, KDD
   Chi EC, 2012, SIAM J MATRIX ANAL A, V33, P1272, DOI 10.1137/110859063
   Cox D., 2006, STAT METHODS APPL, V1, P159
   Cullum J. K., 2002, LANCZOS ALGORITHMS L, V41
   Du  N., 2015, NIPS
   Ekstrand Michael D., 2010, Foundations and Trends in Human-Computer Interaction, V4, P81, DOI 10.1561/1100000009
   Farajtabar  M., 2015, NIPS
   Gopalan P., 2015, UAI
   Gultekin S, 2014, IEEE DATA MINING, P140, DOI 10.1109/ICDM.2014.61
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Hidasi B., 2015, DATA MIN KNOWL DISC, P1
   Kapoor Komal, 2015, WSDM
   Karatzoglou A., 2010, RECSYS
   Koren  Y., 2009, KDD
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Salakhutdinov R., 2008, ICML
   Sastry S., 1990, HONORS PROJECTS
   Wang X., 2016, AAAI
   Wang Y., 2015, KDD
   Wang Y., 2016, ICML
   Wang  Y., 2015, IJCAI
   Wang Yichen, 2016, ARXIV160309021
   Xiong L., 2010, SDM
   Yang S. - H., 2011, WWW
   Yi X., 2014, RECSYS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704005
DA 2019-06-15
ER

PT S
AU Wang, YN
   Anandkumar, A
AF Wang, Yining
   Anandkumar, Animashree
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Online and Differentially-Private Tensor Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
DE Tensor decomposition; tensor power method; online methods; streaming;
   differential privacy; perturbation analysis
AB Tensor decomposition is an important tool for big data analysis. In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.
C1 [Wang, Yining] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Anandkumar, Animashree] Univ Calif Irvine, Dept EECS, Irvine, CA USA.
RP Wang, YN (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM yiningwa@cs.cmu.edu; a.anandkumar@uci.edu
FU Microsoft Faculty Fellowship; NSF [CCF-1254106]; ONR [N00014-14-1-0665];
   ARO YIP [W911NF-13-1-0084]; AFOSR YIP [FA9550-15-1-0221]
FX A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF
   Career award CCF-1254106, ONR Award N00014-14-1-0665, ARO YIP Award
   W911NF-13-1-0084 and AFOSR YIP FA9550-15-1-0221.
CR Anandkumar  A., 2015, P COLT
   Anandkumar  A., 2012, NIPS
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Azizzadenesheli K., 2016, COLT
   Bader BW, 2006, ACM T MATH SOFTWARE, V32, P635, DOI 10.1145/1186785.1186794
   Balcan M. - F., 2016, COLT
   Birge L, 2001, INST MATH S, V36, P113, DOI 10.1214/lnms/1215090065
   Cirel'son B. S., 1976, Proceedings of the 3rd Japan-USSR Symposium on Probability Theory, P20
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork Cynthia, 2014, STOC
   Ge  R., 2015, COLT
   Hardt  M., 2014, NIPS
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Hopkins S. B., 2015, COLT
   Hsu D., 2012, ELECTRON COMMUN PROB, V17, P6
   Huang  F., 2014, ARXIV14064566
   Huang FR, 2015, J MACH LEARN RES, V16, P2797
   Janzamin Majid, 2015, ARXIV150608473
   Kamath G., BOUNDS EXPECTATION M
   Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482
   Kuleshov  V., 2015, AISTATS
   Laurent B, 2000, ANN STAT, V28, P1302
   Massart Pascal, 2007, CONCENTRATION INEQUA, V6
   Montanari  A., 2014, NIPS
   Mu C, 2015, SIAM J MATRIX ANAL A, V36, P1638, DOI 10.1137/15M1010890
   Stewart GW, 1990, MATRIX PERTURBATION
   Tomioka  R., 2014, ARXIV14071870
   Wang  Y., 2014, NIPS
   Wang  Y., 2015, NIPS
   Zemel R., 2013, ICML
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704023
DA 2019-06-15
ER

PT S
AU Wang, YZ
   Miller, DJ
   Poskanzer, K
   Wang, Y
   Tian, L
   Yu, GQ
AF Wang, Yizhi
   Miller, David J.
   Poskanzer, Kira
   Wang, Yue
   Tian, Lin
   Yu, Guoqiang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Graphical Time Warping for Joint Alignment of Multiple Curves
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SERIES DATA
AB Dynamic time warping (DTW) is a fundamental technique in time series analysis for comparing one curve to another using a flexible time-warping function. However, it was designed to compare a single pair of curves. In many applications, such as in metabolomics and image series analysis, alignment is simultaneously needed for multiple pairs. Because the underlying warping functions are often related, independent application of DTW to each pair is a sub-optimal solution. Yet, it is largely unknown how to efficiently conduct a joint alignment with all warping functions simultaneously considered, since any given warping function is constrained by the others and dynamic programming cannot be applied. In this paper, we show that the joint alignment problem can be transformed into a network flow problem and thus can be exactly and efficiently solved by the max flow algorithm, with a guarantee of global optimality. We name the proposed approach graphical time warping (GTW), emphasizing the graphical nature of the solution and that the dependency structure of the warping functions can be represented by a graph. Modifications of DTW, such as windowing and weighting, are readily derivable within GTW. We also discuss optimal tuning of parameters and hyperparameters in GTW. We illustrate the power of GTW using both synthetic data and a real case study of an astrocyte calcium movie.
C1 [Wang, Yizhi; Wang, Yue; Yu, Guoqiang] Virginia Tech, Blacksburg, VA 24061 USA.
   [Miller, David J.] Penn State Univ, University Pk, PA 16802 USA.
   [Poskanzer, Kira] Univ Calif San Francisco, San Francisco, CA 94143 USA.
   [Tian, Lin] Univ Calif Davis, Davis, CA 95616 USA.
RP Wang, YZ (reprint author), Virginia Tech, Blacksburg, VA 24061 USA.
EM yzwang@vt.edu; djmiller@engr.psu.edu; Kira.Poskanzer@ucsf.edu;
   yuewang@vt.edu; lintian@ucdavis.edu; yug@vt.edu
CR Ahuja R. T., 1993, NETWORK FLOWS THEORY, P217
   Berndt D. J., 1994, P KDD WORKSH SEATTL, V10, P359
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chudova D., 2002, P 19 C UNC ART INT M, P134
   Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135
   Friedman J., 2001, SPRINGER SERIES STAT, V1
   Fu TC, 2011, ENG APPL ARTIF INTEL, V24, P164, DOI 10.1016/j.engappai.2010.09.007
   HOGEWEG P, 1984, J MOL EVOL, V20, P175, DOI 10.1007/BF02257378
   Ishikawa H., 1998, LECT NOTES COMPUTER, P232, DOI [10.1007/BFb0055670, DOI 10.1007/BFB0055670]
   Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022
   Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9
   Keogh EJ, 2001, P 2001 SIAM INT C DA, P1, DOI DOI 10.1137/1.9781611972719.1
   Korte B., 2012, COMBINATORIAL OPTIMI, V2
   Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025
   Rakthanmanon T., 2012, P 18 ACM SIGKDD INT, P262, DOI DOI 10.1145/2339530.2339576
   Schmidt FR, 2007, LECT NOTES COMPUT SC, V4679, P39
   Shokoohi-Yekta M., 2015, P SIAM INT C DAT MIN, P289, DOI [10.1137/1.9781611974010.33, DOI 10.1137/1.9781611974010.33]
   Sievers F, 2011, MOL SYST BIOL, V7, DOI 10.1038/msb.2011.75
   Tsai TH, 2013, BIOINFORMATICS, V29, P2774, DOI 10.1093/bioinformatics/btt461
   Uchida S, 2012, INT C PATT RECOG, P2294
   Volterra A, 2014, NAT REV NEUROSCI, V15, P327, DOI 10.1038/nrn3725
   Wang YX, 2016, I S BIOMED IMAGING, P351, DOI 10.1109/ISBI.2016.7493281
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702016
DA 2019-06-15
ER

PT S
AU Wang, YX
   Hebert, M
AF Wang, Yu-Xiong
   Hebert, Martial
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning from Small Sample Sets by Combining Unsupervised Meta-Training
   with CNNs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.
C1 [Wang, Yu-Xiong; Hebert, Martial] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Wang, YX (reprint author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
EM yuxiongw@cs.cmu.edu; hebert@cs.cmu.edu
FU ONR MURI [N000141612007]; U.S. Army Research Laboratory (ARL) under the
   Collaborative Technology Alliance Program [W911NF-10-2-0016]
FX We thank Liangyan Gui, Carl Doersch, and Deva Ramanan for valuable and
   insightful discussions. This work was supported in part by ONR MURI
   N000141612007 and U.S. Army Research Laboratory (ARL) under the
   Collaborative Technology Alliance Program, Cooperative Agreement
   W911NF-10-2-0016. We also thank NVIDIA for donating GPUs and AWS Cloud
   Credits for Research program.
CR Ahmed A., 2008, ECCV
   Azizpour H., 2015, TPAMI
   Ben-david S., 2009, AISTATS
   Bergamo A, 2014, IEEE T PATTERN ANAL, V36, P1988, DOI 10.1109/TPAMI.2014.2313111
   Bertinetto L., 2016, NIPS
   Chapelle  Olivier, 2005, AISTATS
   Choi J., 2013, CVPR
   Dai D., 2013, ICCV
   Dosovitskiy A., 2014, NIPS
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hariharan B., 2016, ARXIV160602819
   Held D., 2016, ICLR
   Hoffman J., 2012, ECCV
   Jia Y., 2014, ACM MM
   Joulin A., 2016, ECCV
   Koch G., 2015, ICML WORKSH
   Krizhevsky A., 2012, NIPS
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Li Z., 2016, ECCV
   Nilsback M.-E., 2008, ICVGIP
   Oquab M., 2014, CVPR
   Rastegari M., 2012, ECCV
   Razavian A. S., 2014, CVPR WORKSH
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan Karen, 2015, ICLR
   Singh A, 2016, AAAI
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Torralba A., 2009, CVPR
   Vinyals O., 2016, NIPS
   Wang Y.-X., 2015, CVPR
   Wang Yu- Xiong, 2016, ECCV
   Weston J., 2008, ICML
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Yang S., 2015, ICCV
   Yao B., 2011, ICCV
   Yosinski J., 2014, NIPS
   Zhou  B., 2014, NIPS
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703044
DA 2019-06-15
ER

PT S
AU Wang, YH
   Xu, C
   You, S
   Tao, DC
   Xu, C
AF Wang, Yunhe
   Xu, Chang
   You, Shan
   Tao, Dacheng
   Xu, Chao
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI CNNpack: Packing Convolutional Neural Networks in the Frequency Domain
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.
C1 [Wang, Yunhe; You, Shan; Xu, Chao] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.
   [Xu, Chang; Tao, Dacheng] Univ Technol Sydney, Sch Software, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia.
   [Wang, Yunhe; You, Shan; Xu, Chao] Peking Univ, Cooperat Medianet Innovat Ctr, Beijing, Peoples R China.
RP Wang, YH (reprint author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.; Wang, YH (reprint author), Peking Univ, Cooperat Medianet Innovat Ctr, Beijing, Peoples R China.
EM wangyunhe@pku.edu.cn; Chang.Xu@uts.edu.au; youshan@pku.edu.cn;
   Dacheng.Tao@uts.edu.au; xuchao@cis.pku.edu.cn
FU National Natural Science Foundation of China [NSFC 61375026,
   2015BAF15B00]; Australian Research Council [FT-130101457, DP-140102164,
   LE-140100061]
FX This work was supported by the National Natural Science Foundation of
   China under Grant NSFC 61375026 and 2015BAF15B00, and Australian
   Research Council Projects: FT-130101457, DP-140102164 and LE-140100061.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Arora Sanjeev, 2014, ICML
   Chen W., 2015, ARXIV150604449
   Chen Wenlin, 2015, COMPRESSING NEURAL N
   Courbariaux M, 2016, ARXIV160202830
   Denton E., 2014, NIPS
   Girshick R., 2014, CVPR
   Gong Y., 2014, ARXIV14126115
   Han S, 2016, DEEP COMPRESSION COM
   Han S., 2015, NIPS
   He  K., 2015, ARXIV151203385
   Jia Y., 2014, P ACM INT C MULT
   Krizhevsky A., 2012, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu B., 2015, CVPR
   Rastegari M., 2016, ARXIV160305279
   Ren S., 2015, NIPS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan Karen, 2015, ICLR
   Sun Yi, 2014, NIPS
   Vedaldi Andrea, 2015, P 23 ANN ACM C MULT
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703026
DA 2019-06-15
ER

PT S
AU Wang, Z
   Wei, XX
   Stocker, AA
   Lee, DD
AF Wang, Zhuo
   Wei, Xue-Xin
   Stocker, Alan A.
   Lee, Daniel D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient Neural Codes under Metabolic Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID FISHER INFORMATION; NEURONS; VARIABILITY; POPULATIONS
AB Neural codes are inevitably shaped by various kinds of biological constraints, e.g. noise and metabolic cost. Here we formulate a coding framework which explicitly deals with noise and the metabolic costs associated with the neural representation of information, and analytically derive the optimal neural code for monotonic response functions and arbitrary stimulus distributions. For a single neuron, the theory predicts a family of optimal response functions depending on the metabolic budget and noise characteristics. Interestingly, the well-known histogram equalization solution can be viewed as a special case when metabolic resources are unlimited. For a pair of neurons, our theory suggests that under more severe metabolic constraints, ON-OFF coding is an increasingly more efficient coding scheme compared to ON-ON or OFF-OFF. The advantage could be as large as one-fold, substantially larger than the previous estimation. Some of these predictions could be generalized to the case of large neural populations. In particular, these analytical results may provide a theoretical basis for the predominant segregation into ON- and OFF-cells in early visual processing areas. Overall, we provide a unified framework for optimal neural codes with monotonic tuning curves in the brain, and makes predictions that can be directly tested with physiology experiments.
C1 [Wang, Zhuo] Univ Penn, Dept Math, Philadelphia, PA 19104 USA.
   [Wei, Xue-Xin; Stocker, Alan A.] Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA.
   [Lee, Daniel D.] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.
   [Wang, Zhuo] NYU, Ctr Neural Sci, New York, NY 10003 USA.
   [Wei, Xue-Xin] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Wei, Xue-Xin] Columbia Univ, Ctr Theoret Neurosci, New York, NY 10027 USA.
RP Wang, Z (reprint author), Univ Penn, Dept Math, Philadelphia, PA 19104 USA.; Wang, Z (reprint author), NYU, Ctr Neural Sci, New York, NY 10003 USA.
EM wangzhuo@nyu.edu; weixxpku@gmail.com; astocker@sas.upenn.edu;
   ddlee@seas.upenn.edu
CR ATICK JJ, 1992, NETWORK-COMP NEURAL, V3, P213, DOI 10.1088/0954-898X/3/2/009
   Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308
   ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Balasubramanian V, 2001, NEURAL COMPUT, V13, P799, DOI 10.1162/089976601300014358
   Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301
   Barlow H. B., 1961, SENS COMMUN, P217, DOI DOI 10.7551/MITPRESS/9780262518420.003.0013
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bethge M, 2002, NEURAL COMPUT, V14, P2317, DOI 10.1162/08997660260293247
   Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115
   Carandini M, 2004, PLOS BIOL, V2, P1483, DOI 10.1371/journal.pbio.0020264
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638
   Gjorgjieva J, 2014, J NEUROSCI, V34, P12127, DOI 10.1523/JNEUROSCI.1032-14.2014
   Gottschalk A, 2002, NEURAL COMPUT, V14, P527, DOI 10.1162/089976602317250889
   Gur M, 2006, CEREB CORTEX, V16, P888, DOI 10.1093/cercor/bhj032
   Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768
   Johnson DH, 2004, J COMPUT NEUROSCI, V16, P129, DOI 10.1023/B:JCNS.0000014106.09948.83
   Karklin Yan, 2011, Adv Neural Inf Process Syst, V24, P999
   Kastner DB, 2015, P NATL ACAD SCI USA, V112, P2533, DOI 10.1073/pnas.1418092112
   LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531
   LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36
   LIU Z, 2016, NEUROL SCI, P1
   McDonnell MD, 2008, PHYS REV LETT, V101, DOI 10.1103/PhysRevLett.101.058103
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ratliff CP, 2010, P NATL ACAD SCI USA, V107, P17368, DOI 10.1073/pnas.1005846107
   Rieke F, 1995, P ROY SOC B-BIOL SCI, V262, P259, DOI 10.1098/rspb.1995.0204
   SCHILLER PH, 1992, TRENDS NEUROSCI, V15, P86, DOI 10.1016/0166-2236(92)90017-3
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   TOLHURST DJ, 1981, EXP BRAIN RES, V41, P414
   TOMKO GJ, 1974, BRAIN RES, V79, P405, DOI 10.1016/0006-8993(74)90438-7
   von der Twer T, 2001, NETWORK-COMP NEURAL, V12, P395, DOI 10.1088/0954-898X/12/3/309
   Wang Z., 2012, ADV NEURAL INF PROCE, V25, P2177
   Wassle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497
   Wei Xue-Xin, 2016, NEURAL COMPUTATION
   Wei Xue-Xin, 2015, NATURE NEUROSCIENCE
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703106
DA 2019-06-15
ER

PT S
AU Wei, CY
   Hong, YT
   Lu, CJ
AF Wei, Chen-Yu
   Hong, Yi-Te
   Lu, Chi-Jen
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tracking the Best Expert in Non-stationary Stochastic Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. We introduce a new parameter Lambda, which measures the total statistical variance of the loss distributions over T rounds of the process, and study how this amount affects the regret. We investigate the interaction between Lambda and Gamma, which counts the number of times the distributions change, as well as Lambda and V, which measures how far the distributions deviates over time. One striking result we find is that even when Gamma, V, and Lambda are all restricted to constant, the regret lower bound in the bandit setting still grows with T. The other highlight is that in the full-information setting, a constant regret becomes achievable with constant Gamma and Lambda, as it can be made independent of T, while with constant V and Lambda, the regret still has a T-1/3 dependency. We not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well.
C1 [Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
RP Wei, CY (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM bahh723@iis.sinica.edu.tw; ted0504@iis.sinica.edu.tw;
   cjlu@iis.sinica.edu.tw
CR Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Besbes O, 2014, ADV NEUR IN, V27
   Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chiang Chao-Kai, 2012, 25 C LEARN THEOR COL
   Gaillard Pierre, 2014, 27 C LEARN THEOR COL
   Garivier Aurelien, 2011, 22 INT C ALG LEARN T
   Jadbabaie Ali, 2015, P 18 INT C ART INT S
   Luo Haipeng, 2015, 28 C LEARN THEOR COL
   Maurer Andreas, 2009, 22 C LEARN THEOR COL
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704060
DA 2019-06-15
ER

PT S
AU Wei, D
AF Wei, Dennis
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Constant-Factor Bi-Criteria Approximation Guarantee for k-means plus
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID FACILITY LOCATION; ALGORITHM
AB This paper studies the k-means++ algorithm for clustering as well as the class of D-l sampling algorithms to which k-means++ belongs. It is shown that for any constant factor beta > 1, selecting beta k cluster centers by D-l sampling yields a constant-factor approximation to the optimal clustering with k centers, in expectation and without conditions on the dataset. This result extends the previously known O(log k) guarantee for the case beta = 1 to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.
C1 [Wei, Dennis] IBM Res, Yorktown Hts, NY 10598 USA.
RP Wei, D (reprint author), IBM Res, Yorktown Hts, NY 10598 USA.
EM dwei@us.ibm.com
CR Aggarwal A, 2009, LECT NOTES COMPUT SC, V5687, P15, DOI 10.1007/978-3-642-03685-9_2
   Ailon N., 2009, NIPS, P10
   Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0
   Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Arya V, 2004, SIAM J COMPUT, V33, P544, DOI [10.1137/S0097539702416402, 10.1137/S00097539702416402]
   Awasthi P., 2015, LEIBNIZ INT P INFORM, P754
   Bandyapadhyay S., 2015, TECHNICAL REPORT
   Charikar M, 2002, J COMPUT SYST SCI, V65, P129, DOI 10.1006/jcss.2002.1882
   Chen K, 2009, SIAM J COMPUT, V39, P923, DOI 10.1137/070699007
   Cohen-Addad V., 2016, TECHNICAL REPORT
   Dasgupta S., 2008, CS20080916 U CAL DEP
   Feldman Dan, 2007, P 23 ACM S COMP GEOM, P11
   Friggstad Z., 2016, TECHNICAL REPORT
   Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Jain K, 2001, J ACM, V48, P274, DOI 10.1145/375827.375845
   Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003
   Kumar A, 2010, J ACM, V57, DOI 10.1145/1667053.1667054
   Lloyd S, 1957, TECHNICAL REPORT
   Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274
   Makarychev K., 2015, TECHNICAL REPORT
   Matousek J, 2000, DISCRETE COMPUT GEOM, V24, P61, DOI 10.1007/s004540010019
   Mettu RR, 2004, MACH LEARN, V56, P35, DOI 10.1023/B:MACH.0000033114.18632.e0
   Ostrovsky R, 2012, J ACM, V59, DOI 10.1145/2395116.2395117
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702056
DA 2019-06-15
ER

PT S
AU Wei, ZJ
   Adeli, H
   Zelinsky, G
   Hoai, M
   Samaras, D
AF Wei, Zijun
   Adeli, Hossein
   Zelinsky, Gregory
   Hoai, Minh
   Samaras, Dimitris
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learned Region Sparsity and Diversity Also Predict Visual Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SEARCH
AB Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms, and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.
C1 [Wei, Zijun; Zelinsky, Gregory; Hoai, Minh; Samaras, Dimitris] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Adeli, Hossein; Zelinsky, Gregory] SUNY Stony Brook, Dept Psychol, Stony Brook, NY 11794 USA.
RP Wei, ZJ (reprint author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM zijwei@cs.stonybrook.edu; hossein.adelijelodar@stonybrook.edu;
   gregory.zelinsky@stonybrook.edu; minhhoai@cs.stonybrook.edu;
   samaras@cs.stonybrook.edu
FU National Science Foundation [IIS-1161876, IIS-1566248]; Subsample
   project from the Digiteo Institute, France
FX This project was partially supported by the National Science Foundation
   Awards IIS-1161876 and IIS-1566248 and the Subsample project from the
   Digiteo Institute, France.
CR Ba  Jimmy, 2015, ICLR
   Borji A., 2013, ICCV
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Bylinskii Z., MIT SALIENCY BENCHMA
   Bylinskii Zoya, 2016, ARXIV160403605
   Dario P., 2012, NATO ADV WORKSH
   Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011
   Gilani S. O., 2015, ICME
   Girshick R., 2014, CVPR
   Graves A, 2013, ARXIV13080850
   Hoai M., 2014, P BMVC
   Hoai M., 2014, P ACCV
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Judd T., 2009, P ICCV
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   Kannan A., 2007, NIPS
   Koch C, 1987, MATTERINTELLIGENCE, P115, DOI [10.1007/978-94-009-3833-5_5, DOI 10.1007/978-94-009-3833-5_5]
   Kokkinos I., 2007, RR6317 INRIA
   Krizhevsky A., 2012, NIPS
   Lazebnik S., 2006, CVPR
   Lee T. S., 1999, NIPS
   Mnih V., 2014, NIPS
   Papadopoulos D. P., 2014, ECCV
   Russakovsky Olga, 2015, IJCV
   Simonyan Karen, 2015, ICLR
   Torralba A., 2016, CVPR
   Wei Z., 2016, CVPR
   Xu K, 2015, ICML
   Zelinsky GJ, 2015, ANN NY ACAD SCI, V1339, P154, DOI 10.1111/nyas.12606
   Zelinsky GJ, 2013, PHILOS T R SOC B, V368, DOI 10.1098/rstb.2013.0058
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703087
DA 2019-06-15
ER

PT S
AU Wen, W
   Wu, CP
   Wang, YD
   Chen, YR
   Li, H
AF Wen, Wei
   Wu, Chunpeng
   Wang, Yandan
   Chen, Yiran
   Li, Hai
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Structured Sparsity in Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNN's evaluation. Experimental results show that SSL achieves on average 5.1 x and 3.1 x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth reduces a 20-layer Deep Residual Network (ResNet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original ResNet with 32 layers. For AlexNet, SSL reduces the error by similar to 1%.
C1 [Wen, Wei; Wu, Chunpeng; Wang, Yandan; Chen, Yiran; Li, Hai] Univ Pittsburgh, Pittsburgh, PA 15260 USA.
RP Wen, W (reprint author), Univ Pittsburgh, Pittsburgh, PA 15260 USA.
EM wew57@pitt.edu; chw127@pitt.edu; yaw46@pitt.edu; yic52@pitt.edu;
   hal66@pitt.edu
FU NSF [XPS-1337198, CCF-1615475]
FX This work was supported in part by NSF XPS-1337198 and NSF CCF-1615475.
   The authors thank Drs. Sheng Li and Jongsoo Park for valuable feedback
   on this work.
CR Chetlur S., 2014, ARXIV14100759
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Feng T, 2015, IEEE IC COMP COM NET
   Girshick R., 2014, IEEE C COMP VIS PATT
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   He  K., 2015, ARXIV151203385
   Ioannou Y., 2015, ARXIV151106744
   Ioffe S., 2015, ARXIV150203167
   Jaderberg M., 2014, ARXIV14053866
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebedev Vadim, 2016, IEEE C COMP VIS PATT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J, 2010, PROCEEDINGS OF THE 17TH INTERNATIONAL CONGRESS ON SOUND AND VIBRATION
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava R. K., 2015, ARXIV150500387
   Susanto H, 2015, IEEE IC COMP COM NET
   Szegedy C., 2015, ARXIV14094842
   Tai Cheng, 2015, ARXIV151106067
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704029
DA 2019-06-15
ER

PT S
AU Weston, J
AF Weston, Jason
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dialog-based Language Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of [23] and large-scale question answering from [3]. We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.
C1 [Weston, Jason] Facebook AI Res, New York, NY 10003 USA.
RP Weston, J (reprint author), Facebook AI Res, New York, NY 10003 USA.
EM jase@fb.com
CR Bassiri M. A., 2011, ENGLISH LANGUAGE LIT, V1, P61
   Clarke J., 2010, P COMP NAT LANG LEAR
   Dodge J, 2015, ARXIV151106931
   Higgins R, 2002, STUD HIGH EDUC, V27, P53, DOI 10.1080/03075070120099368
   Hixon B., 2015, ACL
   Kuhl PK, 2004, NAT REV NEUROSCI, V5, P831, DOI 10.1038/nrn1533
   Kuhlmann G., 2004, AAAI 2004 WORKSH SUP
   Latham AS, 1997, EDUC LEADERSHIP, V54, P86
   Lenz  I., 2015, ROBOTICS SCI SYSTEMS
   MARCUS GF, 1993, COGNITION, V46, P53, DOI 10.1016/0010-0277(93)90022-N
   Mikolov T., 2015, ARXIV151108130
   Narasimhan  K., 2015, ARXIV150608941
   Oh J., 2015, NIPS, V28, P2845
   Pappu Aasish, 2013, P SIGDIAL, P242
   Ranzato M., 2015, ARXIV151106732
   Rieser V, 2011, THEOR APPL NAT LANG, P1, DOI 10.1007/978-3-642-24942-6
   Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X
   Sordoni  A., 2015, NAACL
   Su P. - H., 2015, ARXIV150803391
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P2431
   Wayne G, 2014, NEURAL COMPUTATION
   Werts M. G., 1995, J BEHAV EDUC, V5, P55, DOI [DOI 10.1007/BF02110214, 10.1007/BF02110214]
   Weston J, 2015, ARXIV150205698
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702011
DA 2019-06-15
ER

PT S
AU Wiebe, N
   Kapoor, A
   Svore, KM
AF Wiebe, Nathan
   Kapoor, Ashish
   Svore, Krysta M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Quantum Perceptron Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N, namely O(root N). The second algorithm illustrates how the classical mistake bound of O(1/gamma(2)) can be further improved to O(1/root gamma) through quantum means, where gamma denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.
C1 [Wiebe, Nathan; Kapoor, Ashish; Svore, Krysta M.] Microsoft Res, Redmond, WA 98052 USA.
RP Wiebe, N (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM nawiebe@microsoft.com; akapoor@microsoft.com; ksvore@microsoft.com
CR Aimeur E, 2006, LECT NOTES ARTIF INT, V4013, P431
   Amin MH, 2016, ARXIV160102036
   Boyer Michel, 1996, QUANTPH9605034 ARXIV
   Brassard G., 2002, CONT MATH QUANTUM CO, V305, P53
   Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062
   Garnerone S, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.230506
   Gentile C, 2002, J MACH LEARN RES, V2, P213, DOI 10.1162/15324430260185600
   Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866
   Herbrich Ralf, 1999, IJCAI WORKSH SVMS, P23
   LEWENSTEIN M, 1994, J MOD OPTIC, V41, P2491, DOI 10.1080/09500349414552331
   Li Y, 2002, P 9 INT C MACH LEARN, P379
   Lloyd S., 2013, ARXIV13070411
   Lloyd S, 2014, NAT PHYS, V10, P631, DOI [10.1038/NPHYS3029, 10.1038/nphys3029]
   Minka T. P., 2001, THESIS
   Nielsen M. A., 2010, QUANTUM COMPUTATION
   Novikoff Albert BJ, 1963, TECHNICAL REPORT
   Rebentrost P, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.130503
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shalev-Shwartz S, 2005, LECT NOTES COMPUT SC, V3559, P264, DOI 10.1007/11503415_18
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Wiebe N., 2014, ARXIV14123489
   Wiebe N, 2015, QUANTUM INF COMPUT, V15, P316
   Wiebe Nathan, 2015, ARXIV151203145
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703037
DA 2019-06-15
ER

PT S
AU Wilson, AG
   Hu, ZT
   Salakhutdinov, R
   Xing, EP
AF Wilson, Andrew Gordon
   Hu, Zhiting
   Salakhutdinov, Ruslan
   Xing, Eric P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Variational Deep Kernel Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.
C1 [Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA.
   [Hu, Zhiting; Salakhutdinov, Ruslan; Xing, Eric P.] CMU, Pittsburgh, PA USA.
RP Wilson, AG (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
FU NSF [IIS-1563887]; ONR [N000141410684, N000141310721, N000141512791];
   ADeLAIDE [FA8750-16C-0130-001]
FX We thank NSF IIS-1563887, ONR N000141410684, N000141310721,
   N000141512791, and ADeLAIDE FA8750-16C-0130-001 grants.
CR Aggarwal C. C., 2001, SURPRISING BEHAV DIS
   Bui Thang D, 2015, BLACK BOX LEARN INF
   Calandra R., 2014, ARXIV14025876
   Dai Z., 2015, ARXIV151106455
   Damianou A., 2013, ARTIFICIAL INTELLIGE
   Dezfouli A., 2015, ADV NEURAL INFORM PR, P1414
   Durrande N., 2011, ARXIV11034023
   Gal Y., 2014, ADV NEURAL INFORM PR, P3257
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Hensman J, 2015, P 18 INT C ART INT S, P351
   Hensman J, 2015, ADV NEURAL INFORM PR, V28, P1648
   Hensman James, 2013, UNCERTAINTY ARTIFICI
   Jia Y., 2014, ARXIV14085093
   Kingma D.P., 2013, ARXIV13126114
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Lloyd J. R., 2014, ASS ADV ARTIFICIAL I
   Nickson  T., 2015, ARXIV151007965
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rahimi A, 2007, NEURAL INFORM PROCES
   Rasmussen C. E., 2001, NEURAL INFORM PROCES
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   SILVERMAN BW, 1985, J R STAT SOC B, V47, P1
   Titsias M, 2009, ARTIF INTELL, P567
   Wilson A. G., 2013, INT C MACH LEARN ICM
   Wilson A. G., 2015, ARXIV151101870
   Wilson A. G., 2015, INT C MACH LEARN ICM
   Wilson A. G., 2012, INT C MACH LEARN ICM
   Wilson Andrew Gordon, 2014, THESIS
   Wilson Andrew Gordon, 2016, ARTIFICIAL INTELLIGE
   Yang  Z., 2015, ARTIFICIAL INTELLIGE
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703062
DA 2019-06-15
ER

PT S
AU Winner, K
   Sheldon, D
AF Winner, Kevin
   Sheldon, Daniel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Probabilistic Inference with Generating Functions for Poisson Latent
   Variable Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ESTIMATING ABUNDANCE; COUNTS
AB Graphical models with latent count variables arise in a number of fields. Standard exact inference techniques such as variable elimination and belief propagation do not apply to these models because the latent variables have countably infinite support. As a result, approximations such as truncation or MCMC are employed. We present the first exact inference algorithms for a class of models with latent count variables by developing a novel representation of countably infinite factors as probability generating functions, and then performing variable elimination with generating functions. Our approach is exact, runs in pseudo-polynomial time, and is much faster than existing approximate techniques. It leads to better parameter estimates for problems in population ecology by avoiding error introduced by approximate likelihood computations.
C1 [Winner, Kevin; Sheldon, Daniel] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA.
RP Winner, K (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM kwinner@cs.umass.edu; sheldon@cs.umass.edu
FU National Science Foundation [1617533]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 1617533.
CR Al-Osh M., 1987, J TIME SER ANAL, V8, P261, DOI DOI 10.1111/J.1467-9892.1987.TB00438.X
   Bickson D., 2010, ADV NEURAL INFORM PR
   Bogdan P., 2016, P INT C HARDW SOFTW, P1
   Casella G., 2002, DUXBURY ADV SERIES S
   Chandler RB, 2011, ECOLOGY, V92, P1429, DOI 10.1890/10-2433.1
   Couturier T, 2013, J WILDLIFE MANAGE, V77, P454, DOI 10.1002/jwmg.499
   Dail D, 2011, BIOMETRICS, V67, P577, DOI 10.1111/j.1541-0420.2010.01465.x
   Dennis EB, 2015, BIOMETRICS, V71, P237, DOI 10.1111/biom.12246
   EICK SG, 1993, OPER RES, V41, P731, DOI 10.1287/opre.41.4.731
   Feller W, 1968, INTRO PROBABILITY TH
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Gross K, 2007, POPUL ECOL, V49, P191, DOI 10.1007/s10144-007-0034-8
   Jensen F. V., 1990, COMPUTATIONAL STAT Q
   Jha Abhay, 2010, P 24 ANN C NEUR INF, P973
   Kery M, 2009, J APPL ECOL, V46, P1163, DOI 10.1111/j.1365-2664.2009.01724.x
   LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157
   Mao YY, 2005, IEEE T INFORM THEORY, V51, P1635, DOI 10.1109/TIT.2005.846404
   McKenzie E, 2003, HANDB STAT, V21, P573, DOI 10.1016/S0169-7161(03)21018-X
   PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Resnick S. I, 2013, ADVENTURES STOCHASTI
   Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x
   Shenoy P. P., 1990, UNCERTAINTY ARTIFICI
   Weiss CH, 2008, ASTA-ADV STAT ANAL, V92, P319, DOI 10.1007/s10182-008-0072-3
   Winner K., 2015, P 32 INT C MACH LEAR, V37, P2512
   Zhang N. L., 1994, P 10 CAN C ART INT
   Zipkin EF, 2014, ECOLOGY, V95, P22, DOI 10.1890/13-1131.1
   ZONNEVELD C, 1991, ECOL ENTOMOL, V16, P115, DOI 10.1111/j.1365-2311.1991.tb00198.x
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705001
DA 2019-06-15
ER

PT S
AU Wisdom, S
   Powers, T
   Hershey, JR
   Le Roux, J
   Atlas, L
AF Wisdom, Scott
   Powers, Thomas
   Hershey, John R.
   Le Roux, Jonathan
   Atlas, Les
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Full-Capacity Unitary Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.
C1 [Wisdom, Scott; Powers, Thomas; Atlas, Les] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Hershey, John R.; Le Roux, Jonathan] Mitsubishi Elect Res Labs, Cambridge, MA USA.
RP Wisdom, S (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM swisdom@uw.edu; tcpowers@uw.edu; hershey@merl.com; leroux@merl.com;
   atlas@uw.edu
FU U.S. ONR [N00014-12-G-0078]; U.S. ARO [W911NF-15-1-0450]
FX We thank an anonymous reviewer for suggesting improvements to our proof
   in Section 3 and Vamsi Potluru for helpful discussions. Scott Wisdom and
   Thomas Powers were funded by U.S. ONR contract number N00014-12-G-0078,
   delivery orders 13 and 24. Les Atlas was funded by U.S. ARO grant
   W911NF-15-1-0450.
CR [Anonymous], 2000, PERCEPTUAL EVALUATIO
   Arjovsky M., 2016, INT C MACH LEARN ICM
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Brookes M., 2002, VOICEBOX SPEECH PROC
   Cho K., 2014, ARXIV14091259
   Garofolo J. S., 1993, 4930 NISTIR
   Gilmore R, 2008, LIE GROUPS PHYS GEOM
   Halberstadt A. K., 1998, THESIS
   He  K., 2015, ARXIV151203385
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 2001, FIELD GUIDE DYNAMICA
   HOUSEHOLDER AS, 1958, J ACM, V5, P339, DOI 10.1145/320941.320947
   Le Q. V., 2015, ARXIV150400941
   Loizou P. C., 2007, SPEECH ENHANCEMENT T
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   Pascanu R, 2012, ARXIV E PRINTS, V1211, P5063
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Sard A, 1942, B AM MATH SOC, V48, P883, DOI 10.1090/S0002-9904-1942-07811-6
   Saxe A.M., 2013, ARXIV13126120
   Taal CH, 2011, IEEE T AUDIO SPEECH, V19, P2125, DOI 10.1109/TASL.2011.2114881
   Tagare H., 2011, TECHNICAL REPORT
   Theano Development Team, 2016, ARXIV160502688 THEAN
   TIELEMAN T, 2012, COURSERA NEURAL NETW
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702074
DA 2019-06-15
ER

PT S
AU Wu, H
   Noe, F
AF Wu, Hao
   Noe, Frank
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Spectral Learning of Dynamic Systems from Nonequilibrium Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID OBSERVABLE OPERATOR MODELS
AB Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.
C1 [Wu, Hao; Noe, Frank] Free Univ Berlin, Dept Math & Comp Sci, Arnimallee 6, D-14195 Berlin, Germany.
RP Wu, H (reprint author), Free Univ Berlin, Dept Math & Comp Sci, Arnimallee 6, D-14195 Berlin, Germany.
EM hao.wu@fu-berlin.de; frank.noe@fu-berlin.de
FU Deutsche Forschungsgemeinschaft [SFB 1114]; European Research Council
FX This work was funded by Deutsche Forschungsgemeinschaft (SFB 1114) and
   European Research Council (starting grant "pcCells").
CR Beimel A, 2000, J ACM, V47, P506, DOI 10.1145/337244.337257
   Boots B., 2012, THESIS
   Boots B., 2010, P 27 INT C MACH LEAR
   Bowman G. R., 2013, INTRO MARKOV STATE M
   Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsu D., 2005, P 22 C LEARN THEOR C, P964
   Huang T. - K., 2013, P 30 INT C MACH LEAR, P630
   Jaeger H, 2000, NEURAL COMPUT, V12, P1371, DOI 10.1162/089976600300015411
   Jaeger H., 2001, 102 GMD GERM NAT RES
   Jaeger H., 2005, ADV NEURAL INF PROCE, P555
   Jaeger H., 2012, TECH REP
   Jiang N., 2016, P 30 AAAI C ART INT
   Littman M. L., 2001, NIPS, P1555
   Noe F, 2013, J CHEM PHYS, V139, DOI 10.1063/1.4828816
   Perez-Hernandez G, 2013, J CHEM PHYS, V139, DOI 10.1063/1.4811489
   Prinz JH, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3565032
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rosencrantz Matthew, 2004, P 21 INT C MACH LEAR, P88
   Ruttor A., 2013, ADV NEURAL INFORM PR, P2040
   Schaudinnus N, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.050602
   Shirts M, 2000, SCIENCE, V290, P1903, DOI 10.1126/science.290.5498.1903
   Siddiqi Sajid M., 2010, P 13 INT C ART INT S, P741
   Singh S., 2004, P 20 C UNC ART INT, P512
   Svensson A., 2016, 19 INT C ART INT STA, P213
   Thon M, 2015, J MACH LEARN RES, V16, P103
   Turner R., 2010, W CP, P868
   Wiewiora E., 2005, P 22 INT C MACH LEAR, P961
   Wu HR, 2015, J APPL PHYS, V117, DOI 10.1063/1.4917218
   Zhao MJ, 2009, NEURAL COMPUT, V21, P2687, DOI 10.1162/neco.2009.01-08-687
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701048
DA 2019-06-15
ER

PT S
AU Wu, HS
   Liu, X
AF Wu, Huasen
   Liu, Xin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Double Thompson Sampling for Dueling Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As its name suggests, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison according to two sets of samples independently drawn from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. For general Copeland dueling bandits, we show that D-TS achieves O (K-2 log T) regret. Moreover, using a back substitution argument, we refine the regret to O (K log T + K-2 log log T) in Condorcet dueling bandits and most practical Copeland dueling bandits. In addition, we propose an enhancement of D-TS, referred to as D-TS+, to reduce the regret in practice by carefully breaking ties. Experiments based on both synthetic and real-world data demonstrate that D-TS and D-TS+ significantly improve the overall performance, in terms of regret and robustness.
C1 [Wu, Huasen; Liu, Xin] Univ Calif Davis, Davis, CA 95616 USA.
RP Wu, HS (reprint author), Univ Calif Davis, Davis, CA 95616 USA.
EM hswu@ucdavis.edu; xinliu@ucdavis.edu
FU NSF [CCF-1423542, CNS-1457060, CNS-1547461]
FX This research was supported in part by NSF Grants CCF-1423542,
   CNS-1457060, and CNS-1547461. The authors would like to thank Prof. R.
   Srikant (UIUC), Prof. Shipra Agrawal (Columbia University), Masrour
   Zoghi (University of Amsterdam), and Dr. Junpei Komiyama (University of
   Tokyo) for their helpful discussions and suggestions.
CR Agrawal S, 2013, P 16 INT C ART INT S, P99
   Agrawal S., 2012, C LEARN THEOR COLT
   Ailon N., 2014, P 31 INT C MACH LEAR, V32, P856
   Bubeck S, 2010, THESIS
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Gopalan A., 2014, P 31 INT C MACH LEAR, P100
   Gopalan Aditya, 2015, P 28 C LEARN THEOR C, V40, P861
   Jamieson K., 2015, C LEARN THEOR COLT
   Komiyama J., 2016, INT C MACH LEARN ICM
   Komiyama J., 2015, INT C MACH LEARN ICM
   Komiyama J., 2015, P C LEARN THEOR
   Microsoft Research, 2010, MICR LEARN RANK DAT
   Russo D., 2014, ARXIV14035341
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Urvoy T., 2013, P 30 INT C MACH LEAR, V28, P91
   Welsh N., 2012, LARGE SCALE ONLINE L
   Xia Y., 2015, INT JOINT C ART INT
   Yue Y., 2009, P 26 ANN INT C MACH, P1201, DOI DOI 10.1145/1553374.1553527
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Zoghi M, 2014, WSDM 2014, P73
   Zoghi M, 2014, P INT C MACH LEARN I, P10
   Zoghi M, 2015, ADV NEURAL INFORM PR, P307
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701014
DA 2019-06-15
ER

PT S
AU Wu, JJ
   Zhang, CK
   Xue, TF
   Freeman, WT
   Tenenbaum, JB
AF Wu, Jiajun
   Zhang, Chengkai
   Xue, Tianfan
   Freeman, William T.
   Tenenbaum, Joshua B.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning a Probabilistic Latent Space of Object Shapes via 3D
   Generative-Adversarial Modeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.
C1 [Wu, Jiajun; Zhang, Chengkai; Xue, Tianfan; Tenenbaum, Joshua B.] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Freeman, William T.] MIT, CSAIL, Google Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Wu, JJ (reprint author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM jiajunwu@mit.edu; ckzhang@mit.edu; tfxue@mit.edu; billf@mit.edu;
   jbt@mit.edu
FU NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain,
   Minds and Machines (NSF STC) [CCF-1231216]; Toyota Research Institute;
   Adobe; Shell; IARPA MICrONS
FX This work is supported by NSF grants #1212849 and #1447476, ONR MURI
   N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF STC
   award CCF-1231216), Toyota Research Institute, Adobe, Shell, IARPA
   MICrONS, and a hardware donation from Nvidia.
CR Bansal Aayush, 2016, CVPR
   Blanz Volker, 1999, SIGGRAPH
   Carlson Wayne E, 1982, SIGGRAPH
   Chang A.X., 2015, ARXIV151203012
   Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930
   Chen Ding-Yun, 2003, CGF
   Choy C. B., 2016, ECCV
   Denton E. L., 2015, NIPS
   Dosovitskiy Alexey, 2015, CVPR
   Girdhar Rohit, 2016, ECCV
   Goodfellow I., 2014, NIPS
   Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694
   Im D. J., 2016, ARXIV160205110
   Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]
   Kar Abhishek, 2015, CVPR
   Kazhdan Michael, 2003, SGP
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Larsen Anders Boesen Lindbo, 2016, ICML
   Li C., 2016, ARXIV160404382
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Lim J., 2013, ICCV
   Maas A. L., 2013, ICML
   Maturana D., 2015, IROS
   Qi C. R., 2016, CVPR
   Radford A., 2016, ICLR
   Rezende D., 2016, NIPS
   Sedaghat N., 2016, ARXIV160403351
   Sharma Abhishek, 2016, ARXIV160403755
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Springenberg J. T., 2015, ICLR WORKSH
   Su Hao, 2015, ICCV
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Van Kaick Oliver, 2011, CGF
   Wang X., 2016, ECCV
   Wu Jiajun, 2016, ECCV
   Wu Z., 2015, CVPR
   Xiao J., 2010, CVPR
   Xue Tianfan, 2012, CVPR
   Yan X., 2016, NIPS
   Yu Xiang, 2015, CVPR
   Zhu Jun-Yan, 2016, ECCV
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700060
DA 2019-06-15
ER

PT S
AU Wu, SS
   Bhojanapalli, S
   Sanghavi, S
   Dimakis, AG
AF Wu, Shanshan
   Bhojanapalli, Srinadh
   Sanghavi, Sujay
   Dimakis, Alexandros G.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Single Pass PCA of Matrix Products
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION; ALGORITHMS
AB In this paper we present a new algorithm for computing a low rank approximation of the product A(T) B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to (a) first sketch A and B individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about A, B (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation(1) that shows better computational and statistical performance on real-world and synthetic evaluation datasets.
C1 [Wu, Shanshan; Sanghavi, Sujay; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA.
   [Bhojanapalli, Srinadh] Toyota Technol Inst, Chicago, IL USA.
RP Wu, SS (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM shanshan@utexas.edu; srinadh@ttic.edu; sanghavi@mail.utexas.edu;
   dimakis@austin.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000]; ARO YIP
   [W911NF-14-1-0258]
FX We thank the anonymous reviewers for their valuable comments. This
   research has been supported by NSF Grants CCF 1344179, 1344364, 1407278,
   1422549, 1302435, 1564000, and ARO YIP W911NF-14-1-0258.
CR Bhojanapalli S., 2015, P 26 ANN ACM SIAM S, P902
   Boufounos P. T., 2013, SPIE OPTICAL ENG APP
   Chen X., 2012, P INT C ART INT STAT, P199
   Chen Y., 2013, ARXIV13062979
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Cohen Michael B, 2015, ARXIV150702268
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Gittens Alex, 2016, ARXIV160701335
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Karnin Z., 2015, P 28 C LEARN THEOR C, V40, P1129
   Ma Justin, 2009, P 26 ANN INT C MACH, P681, DOI DOI 10.1145/1553374.1553462
   Ma Z., 2015, ARXIV150608170
   Magen A, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1422
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
   Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787
   Woodruff David P., 2014, ARXIV14114357
   Wu S., 2016, GITHUB REPOSITORY SI
   Zaharia M., 2012, P 9 USENIX C NETW SY
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700039
DA 2019-06-15
ER

PT S
AU Wu, T
   Benson, AR
   Gleich, DF
AF Wu, Tao
   Benson, Austin R.
   Gleich, David F.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI General Tensor Spectral Co-clustering for Higher-Order Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Spectral clustering and co-clustering are well-known techniques in data analysis, and recent work has extended spectral clustering to square, symmetric tensors and hypermatrices derived from a network. We develop a new tensor spectral co-clustering method that simultaneously clusters the rows, columns, and slices of a nonnegative three-mode tensor and generalizes to tensors with any number of modes. The algorithm is based on a new random walk model which we call the super-spacey random surfer. We show that our method out-performs state-of-the-art co-clustering methods on several synthetic datasets with ground truth clusters and then use the algorithm to analyze several real-world datasets.
C1 [Wu, Tao; Gleich, David F.] Purdue Univ, W Lafayette, IN 47907 USA.
   [Benson, Austin R.] Stanford Univ, Stanford, CA 94305 USA.
RP Wu, T (reprint author), Purdue Univ, W Lafayette, IN 47907 USA.
EM wu577@purdue.edu; arbenson@stanford.edu; dgleich@purdue.edu
FU NSF [IIS-1422918]; DARPA SIMPLEX; Stanford Graduate Fellowship
FX TW and DFG are supported by NSF IIS-1422918 and DARPA SIMPLEX. ARB is
   supported by a Stanford Graduate Fellowship.
CR Bader B. W., 2015, MATLAB TENSOR TOOLBO
   Bao Bing-Kun, 2013, P 3 ACM C INT C MULT, P135
   Benaim M, 1997, ANN PROBAB, V25, P361
   Benson A. R., 2016, CSNA160202102 ARXIV
   Benson Austin R, 2015, Proc SIAM Int Conf Data Min, V2015, P118
   Boley D, 1998, DATA MIN KNOWL DISC, V2, P325, DOI 10.1023/A:1009740529316
   Chung F., 1992, SPECTRAL GRAPH THEOR
   Chung F., 2007, ICCM
   Chung F, 2005, ANN COMB, V9, P1, DOI 10.1007/s00026-005-0237-z
   Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269
   Gao B., 2005, P 11 ACM SIGKDD INT, P41, DOI DOI 10.1145/1081870.1081879
   Ghoshdastidar D, 2015, P 29 AAAI C ART INT, P2610
   Gleich DF, 2015, SIAM J MATRIX ANAL A, V36, P1507, DOI 10.1137/140985160
   Hein M., 2013, ADV NEURAL INFORM PR, P2427
   Huang H., 2008, KDD, P327
   Jegelka S, 2009, LECT NOTES ARTIF INT, V5809, P368
   Kivela M, 2014, J COMPLEX NETW, V2, P203, DOI 10.1093/comnet/cnu016
   Meila M., 2001, AISTATS
   MIHAIL M, 1989, ANN IEEE SYMP FOUND, P526, DOI 10.1109/SFCS.1989.63529
   Ni J., 2015, P 21 ACM SIGKDD INT, P835
   Papalexakis EE, 2011, INT CONF ACOUST SPEE, P2064
   Pechenick EA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137041
   Ragnarsson S, 2013, LINEAR ALGEBRA APPL, V438, P853, DOI 10.1016/j.laa.2011.04.014
   Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001
   Stewart W. J, 1994, INTRO NUMERICAL SOLU
   Wagner D., 1993, Mathematical Foundations of Computer Science 1993. 18th International Symposium, MFCS '93 Proceedings, P744
   Zhou D., 2007, P 24 INT C MACH LEAR, P1159, DOI DOI 10.1145/1273496.1273642
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703012
DA 2019-06-15
ER

PT S
AU Wu, YH
   Zhang, SZ
   Zhang, Y
   Bengio, Y
   Salakhutdinov, R
AF Wu, Yuhuai
   Zhang, Saizheng
   Zhang, Ying
   Bengio, Yoshua
   Salakhutdinov, Ruslan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Multiplicative Integration with Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a general and simple structural design called "Multiplicative Integration" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.
C1 [Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada.
   [Zhang, Saizheng; Zhang, Ying; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Bengio, Yoshua; Salakhutdinov, Ruslan] CIFAR, Toronto, ON, Canada.
RP Wu, YH (reprint author), Univ Toronto, Toronto, ON, Canada.
EM ywu@cs.toronto.edu; saizheng.zhang@umontreal.ca;
   ying.zhang@umontreal.ca; yoshua.bengio@umontreal.ca; rsalakhu@cs.cmu.edu
FU NSERC; Canada Research Chairs; CIFAR; Calcul Quebec; Compute Canada;
   Disney research; ONR [N000141310721]
FX The authors acknowledge the following agencies for funding and support:
   NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada,
   Disney research and ONR Grant N000141310721. The authors thank the
   developers of Theano [29] and Keras [30], and also thank Jimmy Ba for
   many thought-provoking discussions.
CR Al-Rfou R., 2016, THEANO PYTHON FRAMEW
   Bahdanau  Dzmitry, 2015, ARXIV150804395
   BAUM LE, 1967, B AM MATH SOC, V73, P360, DOI 10.1090/S0002-9904-1967-11751-8
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chollet F., 2015, KERAS
   Chung J., 2015, ARXIV150202367
   Cooijmans T., 2016, RECURRENT BATCH NORM
   GOUDREAU MW, 1994, IEEE T NEURAL NETWOR, V5, P511, DOI 10.1109/72.286928
   Graves A, 2013, ARXIV13080850
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Hannun A. Y., 2014, ARXIV14082873
   He  K., 2015, ARXIV151203385
   Hermann K. M., 2015, ADV NEURAL INFORM PR, P1684
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kalchbrenner  N., 2015, ARXIV150701526
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kiros R, 2015, ADV NEURAL INFORM PR, P3276
   Krueger D., 2015, ARXIV151108400
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Miao Yajie, 2015, ARXIV150708240
   Mikolov Tomas, 2012, PREPRINT
   Mohri M, 2002, COMPUT SPEECH LANG, V16, P69, DOI 10.1006/csla.2001.0184
   Pachitariu M., 2013, ARXIV13015650
   Pezeshki M, 2015, ARXIV151106430
   Rasmus A., 2015, ARXIV150702672
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   Wessels T, 2000, IEEE IJCNN, P271, DOI 10.1109/IJCNN.2000.857908
   Yang  B., 2014, ARXIV14126575
   Zhang Saizheng, 2016, ARXIV160208210
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701072
DA 2019-06-15
ER

PT S
AU Xu, J
   Hsu, D
   Maleki, A
AF Xu, Ji
   Hsu, Daniel
   Maleki, Arian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Global Analysis of Expectation Maximization for Mixtures of Two
   Gaussians
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MAXIMUM-LIKELIHOOD; CONVERGENCE PROPERTIES; LEARNING MIXTURES; EM
   ALGORITHM
AB E xpectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.
C1 [Xu, Ji; Hsu, Daniel; Maleki, Arian] Columbia Univ, New York, NY 10027 USA.
RP Xu, J (reprint author), Columbia Univ, New York, NY 10027 USA.
EM jixu@cs.columbia.edu; djhsu@cs.columbia.edu; arian@stat.columbia.edu
FU NSF [CCF-1420328, DMREF-1534910]; Sloan Fellowship
FX The second named author thanks Yash Deshpande and Sham Kakade for many
   helpful initial discussions. JX and AM were partially supported by NSF
   grant CCF-1420328. DH was partially supported by NSF grant DMREF-1534910
   and a Sloan Fellowship.
CR Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31
   Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/10505160404000000512
   Balakrishnan S., 2014, ARXIV14082156
   BARKAI N, 1994, PHYS REV E, V50, P1766, DOI 10.1103/PhysRevE.50.1766
   Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16
   Brubaker S. C., 2008, 49 ANN IEEE S FDN CO
   Chaudhuri K., 2008, COLT, P9
   Chaudhuri K., 2009, ICML
   Chaudhuri K., 2009, ABS09120086 CORR
   CONNIFFE D., 1987, J ROY STAT SOC D-STA, V36, P317
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   Daskalakis C., 2016, ARXIV160900368
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fisher R. A., 1922, PHILOS T R SOC A, V222, P309, DOI DOI 10.1098/RSTA.1922.0009
   HARDT M., 2015, P 47 ANN ACM S THEOR, P753
   Hero A., 2008, ESAIM-PROBAB STAT, V12, P308
   Hsu D., 2013, 4 INNOVATIONS THEORE
   Jin C., 2016, ARXIV160900978
   Kalai AT, 2010, ACM S THEORY COMPUT, P553
   Kannan R, 2008, SIAM J COMPUT, V38, P1141, DOI 10.1137/S0097539704445925
   Klusowski J. M., 2016, ARXIV160802280
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15
   Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Srebro N, 2007, LECT NOTES COMPUT SC, V4539, P628, DOI 10.1007/978-3-540-72927-3_47
   Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073
   Vaida F, 2005, STAT SINICA, V15, P831
   Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jess.2003.11.008
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Xu J., 2016, ARXIV160807630
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700012
DA 2019-06-15
ER

PT S
AU Xu, LB
   Davenport, MA
AF Xu, Liangbei
   Davenport, Mark A.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dynamic matrix recovery from incomplete observations under an exact
   low-rank constraint
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID COMPLETION
AB Low-rank matrix factorizations arise in a wide variety of applications-including recommendation systems, topic models, and source separation, to name just a few. In these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice. However, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First, we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the matrix sensing and matrix completion observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.
C1 [Xu, Liangbei; Davenport, Mark A.] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA.
RP Xu, LB (reprint author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA.
EM lxu66@gatech.edu; mdav@gatech.edu
FU NRL [N00173-14-2-C001]; AFOSR [FA9550-14-1-0342]; NSF [CCF-1409406,
   CCF-1350616, CMMI-1537261]
FX This work was supported by grants NRL N00173-14-2-C001, AFOSR
   FA9550-14-1-0342, NSF CCF-1409406, CCF-1350616, and CMMI-1537261.
CR Agarwal A, 2012, ANN STAT, V40, P1171, DOI 10.1214/12-AOS1000
   Bakshi K, 2013, PROCEEDINGS OF THE 3RD ACM SYMPOSIUM ON COMPUTING FOR DEVELOPMENT (ACM DEV 2013)
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Dror G., 2011, P ACM SIGKDD INT C K
   Hardt M., 2014, P IEEE S FDN COMP SC
   Hardt M., 2014, P C LEARN THEOR BARC
   Jain P., 2015, P C LEARN THEOR PAR
   Keshavan R., 2009, P ADV NEUR PROC SYST
   Klopp O, 2014, BERNOULLI, V20, P282, DOI 10.3150/12-BEJ486
   Koren Y., 2009, BELLKOR SOLUTION NET
   Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677
   Mohammadiha N, 2015, IEEE T SIGNAL PROCES, V63, P949, DOI 10.1109/TSP.2014.2385655
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Recht B., 2008, P IEEE C DEC CONTR C
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Sun R., 2015, P IEEE S FDN COMP SC
   Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048
   Zhao T., 2015, P ADV NEURAL PROCESS
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703096
DA 2019-06-15
ER

PT S
AU Xu, P
   Gu, QQ
AF Xu, Pan
   Gu, Quanquan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Semiparametric Differential Graph Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INVERSE COVARIANCE ESTIMATION; VARIABLE SELECTION; NETWORKS
AB In many cases of network analysis, it is more attractive to study how a network varies under different conditions than an individual static network. We propose a novel graphical model, namely Latent Differential Graph Model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices. We propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization. We show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys the oracle property under mild conditions. Thorough experiments on both synthetic and real world data support our theory.
C1 [Xu, Pan; Gu, Quanquan] Univ Virginia, Charlottesville, VA 22903 USA.
RP Xu, P (reprint author), Univ Virginia, Charlottesville, VA 22903 USA.
EM px3ds@virginia.edu; qg5w@virginia.edu
FU NSF [III-1618948]
FX We would like to thank the anonymous reviewers for their helpful
   comments. Research was supported by NSF grant III-1618948.
CR Bandyopadhyay S, 2010, SCIENCE, V330, P1385, DOI 10.1126/science.1195618
   Barber R. F., 2015, ARXIV150207641
   Basso K, 2005, NAT GENET, V37, P382, DOI 10.1038/ng1532
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bellail AC, 2009, REV RECENT CLIN TRIA, V4, P34, DOI 10.2174/157488709787047530
   Carter SL, 2004, BIOINFORMATICS, V20, P2242, DOI 10.1093/bioinformatics/bth234
   Chiquet J, 2011, STAT COMPUT, V21, P537, DOI 10.1007/s11222-010-9191-2
   Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033
   de la Fuente A, 2010, TRENDS GENET, V26, P326, DOI 10.1016/j.tig.2010.05.001
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   FAZAYELI F, 2016, ARXIV160605302
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Golub G. H., 1996, MATRIX COMPUTATIONS
   GUO J, 2011, BIOMETRIKA
   He D, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/1471-2105-16-S1-S10
   Hudson NJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000382
   Kanehisa M, 2000, NUCLEIC ACIDS RES, V28, P27, DOI 10.1093/nar/28.1.27
   Kanehisa M, 2011, NUCLEIC ACIDS RES
   Lauritzen SL, 1996, GRAPHICAL MODELS
   LIU H, 2012, NIPS
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   LIU S, 2014, ARXIV14070581
   LOH P. -L., 2013, NIPS
   Marchini S, 2013, EUR J CANCER, V49, P520, DOI 10.1016/j.ejca.2012.06.026
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Oshlack A, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-12-220
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Tian DC, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw581
   Tothill RW, 2008, CLIN CANCER RES, V14, P5198, DOI 10.1158/1078-0432.CCR-08-0196
   Van der Vaart AW, 1998, ASYMPTOTIC STAT
   Vershynin  Roman, 2010, ARXIV10113027
   Vucic D, 2007, CLIN CANCER RES, V13, P5995, DOI 10.1158/1078-0432.CCR-07-0729
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   WEGKAMP M, 2013, ARXIV13056526
   YUAN H, 2015, ARXIV151109188
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   ZHANG T, 2014, BIOMETRIKA
   Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704053
DA 2019-06-15
ER

PT S
AU Xu, P
   Yang, JY
   Roosta-Khorasani, F
   Ree, C
   Mahoney, MW
AF Xu, Peng
   Yang, Jiyan
   Roosta-Khorasani, Farbod
   Re, Christopher
   Mahoney, Michael W.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sub-sampled Newton Methods with Non-uniform Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION
AB We consider the problem of finding the minimizer of a convex function F : R-d -> R of the form F (w) := Sigma(n)(i=1) f(i) (w) + R (w) where a low-rank factorization of del(2) f(i) (w) is readily available. We consider the regime where n >> d. We propose randomized Newton-type algorithms that exploit non-uniform sub-sampling of {del(2) f(i) (w)}(i=1)(n), as well as inexact updates, as means to reduce the computational complexity, and are applicable to a wide range of problems in machine learning. Two non-uniform sampling distributions based on block norm squares and block partial leverage scores are considered. Under certain assumptions, we show that our algorithms inherit a linear-quadratic convergence rate in w and achieve a lower computational complexity compared to similar existing methods. In addition, we show that our algorithms exhibit more robustness and better dependence on problem specific quantities, such as the condition number. We empirically demonstrate that our methods are at least twice as fast as Newton's methods on several real datasets.
C1 [Xu, Peng; Yang, Jiyan; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA.
   [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Xu, P (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM pengxu@stanford.edu; jiyan@stanford.edu; farbod@icsi.berkeley.edu;
   chrismre@cs.stanford.edu; mmahoney@stat.berkeley.edu
FU Army Research Office; Defense Advanced Research Projects Agency; Intel;
   Toshiba; Moore Foundation; DARPA [FA8750-14-2-0240, N66001-15-C-4043,
   FA8750-12-2-0335]; Office of Naval Research [N000141410102,
   N000141210041, N000141310129]
FX We would like to thank the Army Research Office and the Defense Advanced
   Research Projects Agency as well as Intel, Toshiba and the Moore
   Foundation for support along with DARPA through MEMEX
   (FA8750-14-2-0240), SIMPLEX (N66001-15-C-4043), and XDATA
   (FA8750-12-2-0335) programs, and the Office of Naval Research
   (N000141410102, N000141210041 and N000141310129). Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and do not necessarily reflect the views of
   DARPA, ONR, or the U.S. government.
CR Agarwal N., 2016, ARXIV160203943
   Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0
   Bubeck Sebastien, 2014, ARXIV14054980
   Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X
   DEMBO RS, 1982, SIAM J NUMER ANAL, V19, P400, DOI 10.1137/0719025
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Erdogdu Murat A, 2015, ADV NEURAL INFORM PR, V28, P3034
   Friedman J., 2001, SPRINGER SERIES STAT, V1
   Graf F, 2011, LECT NOTES COMPUT SC, V6892, P607, DOI 10.1007/978-3-642-23629-7_74
   Holodnak JT, 2015, SIAM J MATRIX ANAL A, V36, P110, DOI 10.1137/130940116
   Kawala  F., 2013, 4 C MOD AN RES APPR, P16
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Lichman M., 2013, UCI MACHINE LEARNING
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Mahoney Michael W, 2011, RANDOMIZED ALGORITHM
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Pilanci M., 2015, ARXIV150502250
   Roosta- Khorasani F., 2016, ARXIV160104738
   Roosta- Khorasani F., 2016, ARXIV160104737
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vinyals Oriol, 2011, ARXIV11114259
   Xu Peng, 2016, ARXIV160700559
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700002
DA 2019-06-15
ER

PT S
AU Xu, Y
   Yan, Y
   Lin, QH
   Yang, TB
AF Xu, Yi
   Yan, Yan
   Lin, Qihang
   Yang, Tianbao
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than
   O(1/epsilon)
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GRADIENT; MINIMIZATION
AB In this paper, we develop a novel homotopy smoothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is O(1/epsilon) without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of (O) over tilde (1/epsilon(1-theta)) with theta is an element of (0, 1] capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and l(1) norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods.
C1 [Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Yan, Yan] Univ Technol Sydney, QCIS, Sydney, NSW 2007, Australia.
   [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA.
RP Yang, TB (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM yi-xu@uiowa.edu; yan.yan-3@student.uts.edu.au; qihang-lin@uiowa.edu;
   tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1463988, IIS-1545995]
FX We thank the anonymous reviewers for their helpful comments. Y. Xu and
   T. Yang are partially supported by National Science Foundation
   (IIS-1463988, IIS-1545995).
CR Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Becker S, 2011, SIAM J IMAGING SCI, V4, P1, DOI 10.1137/090756855
   Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641
   Bolte Jerome, 2015, CORR
   Burke JV, 1996, SIAM J OPTIMIZ, V6, P265, DOI 10.1137/0806015
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514
   Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2
   Goebel R, 2008, J CONVEX ANAL, V15, P263
   Lan GH, 2011, MATH PROGRAM, V126, P1, DOI 10.1007/s10107-008-0261-6
   Lojasiewicz S, 1965, ENSEMBLES SEMIANALYT
   Necoara I., 2015, CORR
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Pang JS, 1997, MATH PROGRAM, V79, P299, DOI 10.1007/BF02614322
   Rockafellar RT, 1970, PRINCETON MATH SERIE
   TSENG P., 2008, SIAM J OPTIM
   Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997
   Yang T., 2014, MACHINE LEARNING
   Yang Tianbao, 2016, CORR
   Zhang Hui, 2016, ARXIV160600269
   Zhang XH, 2012, J MACH LEARN RES, V13, P3623
   Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157
   Zhou Z., 2015, CORR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703043
DA 2019-06-15
ER

PT S
AU Xu, Z
   Dong, W
   Srihari, S
AF Xu, Zhen
   Dong, Wen
   Srihari, Sargur
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Using Social Dynamics to Make Individual Predictions: Variational
   Inference with a Stochastic Kinetic Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly-rather than exponentially-with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.
C1 [Xu, Zhen; Dong, Wen; Srihari, Sargur] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
RP Xu, Z (reprint author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM zxu8@buffalo.edu; wendong@buffalo.edu; srihari@buffalo.edu
RI Srihari, Sargur N/E-8100-2011
CR Arkin A, 1998, GENETICS, V149, P1633
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Castellano C, 2009, REV MOD PHYS, V81, P591, DOI 10.1103/RevModPhys.81.591
   Cohn I, 2010, J MACH LEARN RES, V11, P2745
   Dong Wen, 2011, P 10 INT C MOB UB MU, P134, DOI DOI 10.1145/2107596.2107613
   Dong Wen, 2012, P C UNC ART INT CAT, P227
   Doucet A., 2009, HDB NONLINEAR FILTER, V12, P3
   Durlauf Steven N, 2004, SOCIAL DYNAMICS, V4
   Eubank S, 2004, NATURE, V429, P180, DOI 10.1038/nature02541
   Gillespie DT, 2007, ANNU REV PHYS CHEM, V58, P35, DOI 10.1146/annurev.physchem.58.032806.104637
   Golightly Andrew, 2011, INTERFACE FOCUS
   Heaukulani C., 2013, P 30 INT C MACH LEAR, P275
   Heskes T, 2002, P 18 C UNC ART INT, P216
   Keeling M., 2008, MODELING INFECT DIS
   Murphy K, 2001, STAT ENG IN, P499
   Nodelman  U., 2002, P 18 C UNC ART INT, P378
   Opper M., 2008, ADV NEURAL INFORM PR, P1105
   Rao V., 2011, P UAI
   Robinson JW, 2010, J MACH LEARN RES, V11, P3647
   Wen Dong, 2012, Social Computing, Behavioral-Cultural Modeling and Prediction. Proceedings of the 5th International Conference, SBP 2012, P172, DOI 10.1007/978-3-642-29047-3_21
   Wilkinson Darren J, 2011, STOCHASTIC MODELING
   Yedidia J. S., 2003, EXPLORING ARTIF INTE, V8, P236
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703089
DA 2019-06-15
ER

PT S
AU Xue, TF
   Wu, JJ
   Bouman, KL
   Freeman, WT
AF Xue, Tianfan
   Wu, Jiajun
   Bouman, Katherine L.
   Freeman, William T.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Visual Dynamics: Probabilistic Future Frame Synthesis via Cross
   Convolutional Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-world video frames. We also show that our model can be applied to visual analogy-making, and present an analysis of the learned network representations.
C1 [Xue, Tianfan; Wu, Jiajun; Bouman, Katherine L.; Freeman, William T.] MIT, Cambridge, MA 02139 USA.
   [Freeman, William T.] Google Res, Cambridge, MA USA.
RP Xue, TF (reprint author), MIT, Cambridge, MA 02139 USA.
EM tfxue@mit.edu; jiajunwu@mit.edu; klbouman@mit.edu; billf@mit.edu
FU NSF Robust Intelligence [1212849]; NSF Big Data [1447476]; ONR MURI
   [6923196]; Adobe; Shell Research
FX The authors thank Yining Wang for helpful discussions. This work is
   supported by NSF Robust Intelligence 1212849, NSF Big Data 1447476, ONR
   MURI 6923196, Adobe, and Shell Research. The authors would also like to
   thank Nvidia for GPU donations.
CR Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268
   De Brabandere  B., 2016, NIPS
   Denton E. L., 2015, NIPS
   Finn Chelsea, 2016, NIPS
   Fleet DJ, 2000, INT J COMPUT VISION, V36, P171, DOI 10.1023/A:1008156202475
   Goodfellow I., 2014, NIPS
   Gregor K., 2015, ICML
   Higgins I, 2016, ARXIV160605579
   Hinton Geoffrey E, 1993, COLT, P8
   Joshi  Neel, 2012, UIST, P2
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Liao Z, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461950
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu  Ce, 2009, THESIS, P6
   Mathieu M., 2016, ICLR
   Oh  J., 2015, NIPS
   Pintea Silvia L, 2014, ECCV, P2
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Radford A., 2016, ICLR
   Reed S. E., 2015, NIPS
   Roth  Stefan, 2005, ICCV, P2
   Schodl A, 2000, COMP GRAPH, P489
   Srivastava N, 2015, ICML
   Vondrick C., 2016, NIPS
   Vondrick  Carl, 2016, CVPR, P2
   Walker  J., 2016, ECCV
   Walker  Jacob, 2015, ICCV, P2
   Walker  Jacob, 2014, CVPR, P2
   Wang John YA, 1993, CVPR, P4
   Weiss  Yair, 1998, CTR BIOL COMPUTATION, V158, P1
   Wexler  Yonatan, 2004, CVPR, P2
   Wu  Jiajun, 2015, NIPS, P2
   Xie  Jianwen, 2016, ARXIV160600972, P2
   Xie  Junyuan, 2016, ARXIV160403650, P2
   Xue  Tianfan, 2014, ECCV, P2
   Yan XH, 2016, EARTHS FUTURE, V4, P472, DOI 10.1002/2016EF000417
   Zhou  T., 2016, ECCV
NR 38
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704076
DA 2019-06-15
ER

PT S
AU Xue, YX
   Li, ZY
   Ermon, S
   Gomes, CP
   Selman, B
AF Xue, Yexiang
   Li, Zhiyuan
   Ermon, Stefano
   Gomes, Carla P.
   Selman, Bart
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Solving Marginal MAP Problems with NP Oracles and Parity Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Arising from many applications at the intersection of decision-making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP problem, by encoding it as a single optimization in a polynomial size of the original problem. We evaluate our approach in several machine learning and decision-making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers.
C1 [Xue, Yexiang; Gomes, Carla P.; Selman, Bart] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   [Li, Zhiyuan] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.
   [Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Li, Zhiyuan] Cornell Univ, Ithaca, NY USA.
RP Xue, YX (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
EM yexiang@cs.cornell.edu; lizhiyuan13@mails.tsinghua.edu.cn;
   ermon@cs.stanford.edu; gomes@cs.cornell.edu; selman@cs.cornell.edu
FU National Science Foundation [0832782, 1522054, 1059284, 1649208]; Future
   of Life Institute [2015-143902]
FX This research was supported by National Science Foundation (Awards
   #0832782, 1522054, 1059284, 1649208) and Future of Life Institute (Grant
   2015-143902).
CR Achlioptas Dimitris, 2015, P UNC ART INT
   Belle Vaishak, 2015, P 31 UAI C
   Bengio Yoshua, 2006, ADV NEURAL INFORM PR, V19
   Chavira Mark, 2006, INT J APPROX REASONI
   Ermon S., 2013, ADV NEURAL INFORM PR, P2085
   Ermon S., 2013, P 30 INT C MACH LEAR
   Ermon Stefano, 2013, P 29 C UNC ART INT U
   Ermon Stefano, 2014, P 31 INT C MACH LEAR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jiang Jiarong, 2011, ADV NEURAL INFORM PR, V24
   Lee Junkyu, 2016, P 30 AAAI C ART INT
   Liu Qiang, 2013, J MACHINE LEARNING R, V14
   Marinescu Radu, 2015, P 24 INT C ART INT I
   Marinescu Radu, 2014, P 30 C UNC ART INT U
   Maua Denis Deratani, 2012, P 29 INT C MACH LEAR
   Pan D, 2015, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE OF THE ASSOCIATION OF PSYCHOLOGY AND PSYCHIATRY FOR ADULTS AND CHILDREN (A.P.P.A.C 2013)
   Park James D., 2003, P 19 C UNC ART INT U
   Park James D., 2004, J ARTIF INT RES
   Ping W, 2015, ADV NEUR IN, V28
   Sheldon Daniel, 2010, UAI
   Xue Shan, 2015, J ARTIF INTELL RES J
   Xue Yexiang, 2016, P 15 INT C AUT AG MU
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703098
DA 2019-06-15
ER

PT S
AU Yan, BW
   Sarkar, P
AF Yan, Bowei
   Sarkar, Purnamrita
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Robustness of Kernel Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONSISTENCY
AB Clustering is an important unsupervised learning problem in machine learning and statistics. Among many existing algorithms, kernel k-means has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. Also the error bounds suggest that SDP is more resilient towards outliers, which we also demonstrate with experiments.
C1 [Yan, Bowei; Sarkar, Purnamrita] Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA.
RP Yan, BW (reprint author), Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA.
CR Amini Arash A, 2014, ARXIV14065647
   Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4
   Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290
   Christmann A, 2007, BERNOULLI, V13, P799, DOI 10.3150/07-BEJ5102
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   De Brabanter K, 2009, LECT NOTES COMPUT SC, V5768, P100
   Debruyne M., 2008, J MACHINE LEARNING R, V9
   Debruyne M, 2010, COMPUT STAT DATA AN, V54, P3007, DOI 10.1016/j.csda.2009.08.018
   Dhillon IS, 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118
   Duan L, 2009, ANN OPER RES, V168, P151, DOI 10.1007/s10479-008-0371-9
   El Karoui N, 2010, ANN STAT, V38, P3191, DOI 10.1214/10-AOS801
   Kim DW, 2005, PATTERN RECOGN, V38, P607, DOI 10.1016/j.patchog.2004.09.006
   Kim J, 2012, J MACH LEARN RES, V13, P2529
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   Mixon D. G., 2016, ARXIV160206612
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Pamula R, 2011, Proceedings of the Second International Conference on Emerging Applications of Information Technology (EAIT 2011), P253, DOI 10.1109/EAIT.2011.25
   Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Steinhaus H., 1957, B ACAD POL SCI, V12, P801
   von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640
   Xu L., 2006, AAAI, V1, P536
   Yang MS, 2004, IEEE T PATTERN ANAL, V26, P434, DOI 10.1109/TPAMI.2004.1265860
   Yu Y, 2015, BIOMETRIKA, V102, P315, DOI 10.1093/biomet/asv008
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703025
DA 2019-06-15
ER

PT S
AU Yan, SB
   Chaudhuri, K
   Javidi, T
AF Yan, Songbai
   Chaudhuri, Kamalika
   Javidi, Tara
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Active Learning from Imperfect Labelers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.
C1 [Yan, Songbai; Chaudhuri, Kamalika; Javidi, Tara] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Yan, SB (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM yansongbai@eng.ucsd.edu; kamalika@cs.ucsd.edu; tjavidi@eng.ucsd.edu
FU NSF [IIS-1162581, CCF-1513883, CNS-1329819]
FX We thank NSF under IIS-1162581, CCF-1513883, and CNS-1329819 for
   research support.
CR Balcan M.-F., 2012, P 25 C LEARN THEOR
   Balcan M.-F., 2013, COLT
   Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Beygelzimer A., 2010, NIPS
   Beygelzimer Alina, 2016, ARXIV160207265
   Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189
   Chen  Y., 2015, P 28 C LEARN THEOR, P338
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Dasgupta S., 2007, NIPS
   Dasgupta S., 2005, NIPS
   Fang M, 2012, INT C PATT RECOG, P2238
   Hanneke S, 2007, LECT NOTES COMPUT SC, V4539, P66, DOI 10.1007/978-3-540-72927-3_7
   Hegedus T., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P108, DOI 10.1145/225298.225311
   Kaariainen M., 2006, ALT
   Kading C, 2015, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR.2015.7299063
   Li Y.-C., 2013, APPL MATH, V4, P1070
   Minsker S, 2012, J MACH LEARN RES, V13, P67
   Naghshvar M, 2015, IEEE T INFORM THEORY, V61, P4080, DOI 10.1109/TIT.2015.2432101
   Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298
   Raginsky Maxim, 2011, ADV NEURAL INFORM PR, P1026
   Ramdas Aaditya, 2016, P C UNC ART INT
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Urner R, 2012, 15 INT C ART INT STA, P1252
   Yan Songbai, 2015, COMM CONTR COMP ALL
   Zhang C, 2015, ADV NEURAL INFORM PR, V28, P703
   Zhang Chicheng, 2014, ADV NEURAL INFORM PR, P442
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701019
DA 2019-06-15
ER

PT S
AU Yan, XC
   Yang, JM
   Yumer, E
   Guo, YJ
   Lee, H
AF Yan, Xinchen
   Yang, Jimei
   Yumer, Ersin
   Guo, Yijie
   Lee, Honglak
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Perspective Transformer Nets: Learning Single-View 3D Object
   Reconstruction without 3D Supervision
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.
C1 [Yan, Xinchen; Guo, Yijie; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Yang, Jimei; Yumer, Ersin] Adobe Res, San Jose, CA USA.
   [Lee, Honglak] Google Brain, Mountain View, CA USA.
RP Yan, XC (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM xcyan@umich.edu; jimyang@adobe.com; yumer@adobe.com; guoyijie@umich.edu;
   honglak@umich.edu
FU NSF CAREER [IIS-1453651]; ONR [N00014-13-1-0762]; Sloan Research
   Fellowship
FX This work was supported in part by NSF CAREER IIS-1453651, ONR
   N00014-13-1-0762, Sloan Research Fellowship, and a gift from Adobe. We
   acknowledge NVIDIA for the donation of GPUs. We also thank Yuting Zhang,
   Scott Reed, Junhyuk Oh, Ruben Villegas, Seunghoon Hong, Wenling Shang,
   Kibok Lee, Lajanugen Logeswaran, Rui Zhang and Yi Zhang for helpful
   comments and discussions.
CR Chang A.X., 2015, ARXIV151203012
   Choy C. B., 2016, ECCV
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Girdhar R., 2016, ARXIV160308637
   Hinton G. E., 2011, ICANN
   Jaderberg M., 2015, NIPS
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kulkarni Tejas D, 2015, NIPS
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Lee H., 2009, ICML
   Memisevic R., 2007, CVPR
   Michalski V., 2014, NIPS
   Qi C. R., 2016, CVPR
   Reed S., 2014, ICML
   Rezende D., 2016, NIPS
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Su Hao, 2015, ICCV
   Szeliski R., 2010, COMPUTER VISION ALGO
   Tatarchenko Maxim, 2016, ECCV
   Wu Jiajun, 2016, ECCV
   Wu Z., 2015, CVPR
   Yang Jimei, 2015, NIPS
   Yumer E., 2016, ECCV
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701063
DA 2019-06-15
ER

PT S
AU Yang, F
   Barber, RF
   Jain, P
   Lafferty, J
AF Yang, Fan
   Barber, Rina Foygel
   Jain, Prateek
   Lafferty, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Selective inference for group-sparse linear models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REGRESSION
AB We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.
C1 [Yang, Fan; Barber, Rina Foygel] Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
   [Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India.
   [Lafferty, John] Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
   [Lafferty, John] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
RP Yang, F (reprint author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
EM fyang1@uchicago.edu; rina@uchicago.edu; prajain@microsoft.com;
   lafferty@galton.uchicago.edu
FU ONR [N00014-15-1-2379]; NSF [DMS-1513594, DMS-1547396]
FX Research supported in part by ONR grant N00014-15-1-2379, and NSF grants
   DMS-1513594 and DMS-1547396.
CR [Anonymous], 2015, ARXIV151108866
   Blumensath T, 2009, IEEE T INFORM THEORY, V55, P1872, DOI 10.1109/TIT.2009.2013003
   Hastie T., 2001, SPRINGER SERIES STAT
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Jain Prateek, 2016, CORR
   Lee J, 2013, ARXIV13116238
   Lee J. D., 2014, ADV NEURAL INFORM PR, V27, P136
   Loftus Joshua R, 2014, ARXIV14053920
   Loftus JR, 2015, ARXIV151101478
   Mosci S., 2010, ADV NEURAL INFORM PR, P2604
   R Core Team, 2016, R LANG ENV STAT COMP
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani Ryan J, 2014, ARXIV14013889
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703073
DA 2019-06-15
ER

PT S
AU Yang, JY
   Mahoney, MW
   Saunders, MA
   Sun, YK
AF Yang, Jiyan
   Mahoney, Michael W.
   Saunders, Michael A.
   Sun, Yuekai
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Feature-distributed sparse regression: a screen-and-clean approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Most existing approaches to distributed sparse regression assume the data is partitioned by samples. However, for high-dimensional data (D >> N), it is more natural to partition the data by features. We propose an algorithm to distributed sparse regression when the data is partitioned by features rather than samples. Our approach allows the user to tailor our general method to various distributed computing platforms by trading-off the total amount of data (in bits) sent over the communication network and the number of rounds of communication. We show that an implementation of our approach is capable of solving l(1)-regularized l(2) regression problems with millions of features in minutes.
C1 [Yang, Jiyan; Saunders, Michael A.] Stanford Univ, Stanford, CA 94305 USA.
   [Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Sun, Yuekai] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Yang, JY (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM jiyan@stanford.edu; mmahoney@stat.berkeley.edu; saunders@stanford.edu;
   yuekai@umich.edu
FU Army Research Office; Defense Advanced Research Projects Agency
FX We would like to thank the Army Research Office and the Defense Advanced
   Research Projects Agency for providing partial support for this work.
CR Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Clarkson K. L., 2013, S THEOR COMP STOC
   Demmel J, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P1942
   Fan JQ, 2008, J ROY STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x
   Gittens Alex, 2016, ARXIV160701335
   Hastie  T., 2015, STAT LEARNING SPARSI
   Lee J. D., 2015, ARXIV150304337
   Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722
   Pilanci Mert, 2014, ARXIV14110347
   Roosta- Khorasani F., 2016, ARXIV160104738
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787
   Wang Xiangyu, 2015, J ROYAL STAT SOC B
   Wang Xiangyu, 2016, ARXIV160202575
   Woodruff David P., 2014, ARXIV14114357
   Zhang YC, 2013, J MACH LEARN RES, V14, P3321
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701044
DA 2019-06-15
ER

PT S
AU Yang, Y
   Sun, J
   Li, HB
   Xu, ZB
AF Yang, Yan
   Sun, Jian
   Li, Huibin
   Xu, Zongben
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep ADMM-Net for Compressive Sensing MRI
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID IMAGE-RECONSTRUCTION
AB Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of under-sampled data in k-space, and accelerating the data acquisition in MRI. To improve the current MRI system in reconstruction accuracy and computational speed, in this paper, we propose a novel deep architecture, dubbed ADMM-Net. ADMM-Net is defined over a data flow graph, which is derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the training data for CS-based reconstruction task. Experiments on MRI image reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction accuracies with fast computational speed.
C1 [Yang, Yan; Sun, Jian; Li, Huibin; Xu, Zongben] Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.
RP Yang, Y (reprint author), Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.
EM yangyan92@stu.xjtu.edu.cn; jiansun@mail.xjtu.edu.cn;
   huibinli@mail.xjtu.edu.cn; zbxu@mail.xjtu.edu.cn
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bernstein MA, 2001, J MAGN RESON IMAGING, V14, P270, DOI 10.1002/jmri.1183
   Block KT, 2007, MAGN RESON MED, V57, P1086, DOI 10.1002/mrm.21236
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Chen C., 2012, ADV NEURAL INFORM PR, P1115
   Eksioglu Ender M, 2016, J MATH IMAGING VIS, P1
   Fang S, 2010, MAGN RESON MED, V64, P1414, DOI 10.1002/mrm.22392
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Hershey J. R., 2014, ARXIV14092574
   KAVI KM, 1986, IEEE T COMPUT, V35, P940
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Qu XB, 2014, MED IMAGE ANAL, V18, P843, DOI 10.1016/j.media.2013.09.007
   Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Sun J, 2015, IEEE T IMAGE PROCESS, V24, P4148, DOI 10.1109/TIP.2015.2448352
   Wang H., 2014, ADV NEURAL INFORM PR, P181
   Yang JF, 2010, IEEE J-STSP, V4, P288, DOI 10.1109/JSTSP.2010.2042333
   Zhan Zhifang, 2016, IEEE T BIOMEDICAL EN
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703042
DA 2019-06-15
ER

PT S
AU Yang, Y
   Aminoff, EM
   Tarr, MJ
   Kass, RE
AF Yang, Ying
   Aminoff, Elissa M.
   Tarr, Michael J.
   Kass, Robert E.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A state-space model of cross-region dynamic connectivity in MEG/EEG
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INVERSE PROBLEM; MEG; BRAIN; EEG
AB Cross-region dynamic connectivity, which describes the spatio-temporal dependence of neural activity among multiple brain regions of interest (ROIs), can provide important information for understanding cognition. For estimating such connectivity, magnetoencephalography (MEG) and electroencephalography (EEG) are well-suited tools because of their millisecond temporal resolution. However, localizing source activity in the brain requires solving an under-determined linear problem. In typical two-step approaches, researchers first solve the linear problem with generic priors assuming independence across ROIs, and secondly quantify cross-region connectivity. In this work, we propose a one-step state-space model to improve estimation of dynamic connectivity. The model treats the mean activity in individual ROIs as the state variable and describes non-stationary dynamic dependence across ROIs using time-varying auto-regression. Compared with a two-step method, which first obtains the commonly used minimum-norm estimates of source activity, and then fits the auto-regressive model, our state-space model yielded smaller estimation errors on simulated data where the model assumptions held. When applied on empirical MEG data from one participant in a scene-processing experiment, our state-space model also demonstrated intriguing preliminary results, indicating leading and lagged linear dependence between the early visual cortex and a higher-level scene-sensitive region, which could reflect feedforward and feedback information flow within the visual cortex during scene processing.
C1 [Yang, Ying; Tarr, Michael J.; Kass, Robert E.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Aminoff, Elissa M.] Fordham Univ, Bronx, NY 10458 USA.
RP Yang, Y (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ying.yang.cnbc.cmu@gmail.com; eaminoff@fordham.edu; michaeltarr@cmu.edu;
   kass@stat.cmu.edu
FU National Science Foundation [1439237]; National Institute of Mental
   Health [RO1 MH64537]; Henry L. Hillman Presidential Fellowship at
   Carnegie Mellon University
FX This work was supported in part by the National Science Foundation Grant
   1439237, the National Institute of Mental Health Grant RO1 MH64537, as
   well as the Henry L. Hillman Presidential Fellowship at Carnegie Mellon
   University.
CR Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cichy R. M., 2016, ARXIV160102970
   Dale AM, 2000, NEURON, V26, P55, DOI 10.1016/S0896-6273(00)81138-1
   David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045
   DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010
   Epstein R, 1999, NEURON, V23, P115, DOI 10.1016/S0896-6273(00)80758-8
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fukushima M, 2015, NEUROIMAGE, V105, P408, DOI 10.1016/j.neuroimage.2014.09.066
   Galka A, 2004, NEUROIMAGE, V23, P435, DOI 10.1016/j.neuroimage.2004.02.022
   Gramfort A, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00267
   Gramfort A, 2012, PHYS MED BIOL, V57, P1937, DOI 10.1088/0031-9155/57/7/1937
   HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413
   HAMALAINEN MS, 1994, MED BIOL ENG COMPUT, V32, P35, DOI 10.1007/BF02512476
   Lamus C, 2012, NEUROIMAGE, V63, P894, DOI 10.1016/j.neuroimage.2011.11.020
   Mattout J, 2006, NEUROIMAGE, V30, P753, DOI 10.1016/j.neuroimage.2005.10.037
   Mosher JC, 1999, IEEE T BIO-MED ENG, V46, P245, DOI 10.1109/10.748978
   PASCUALMARQUI RD, 1994, INT J PSYCHOPHYSIOL, V18, P49, DOI 10.1016/0167-8760(84)90014-X
   Sakkalis V, 2011, COMPUT BIOL MED, V41, P1110, DOI 10.1016/j.compbiomed.2011.06.020
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705007
DA 2019-06-15
ER

PT S
AU Yang, ZL
   Yuan, Y
   Wu, YX
   Salakhutdinov, R
   Cohen, WW
AF Yang, Zhilin
   Yuan, Ye
   Wu, Yuexin
   Salakhutdinov, Ruslan
   Cohen, William W.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Review Networks for Caption Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder-decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.(1)
C1 [Yang, Zhilin; Yuan, Ye; Wu, Yuexin; Salakhutdinov, Ruslan; Cohen, William W.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Yang, ZL (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM zhiliny@cs.cmu.edu; yey1@cs.cmu.edu; yuexinw@cs.cmu.edu;
   rsalakhu@cs.cmu.edu; wcohen@cs.cmu.edu
FU NSF [CCF-1414030, IIS-1250956]; Google; Disney Research; ONR
   [N000141512791]; ADeLAIDE grant [FA8750-16C-0130-001]
FX This work was funded by the NSF under grants CCF-1414030 and
   IIS-1250956, Google, Disney Research, the ONR grant N000141512791, and
   the ADeLAIDE grant FA8750-16C-0130-001.
CR Bandanau D., 2015, ICLR
   Chen X, 2015, CORR, V1504, P325
   Cho Kyunghyun, 2014, ACL
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Graves A., 2016, ARXIV160308983
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kumar A., 2016, ICML
   Luong M.-T., 2015, ACL
   Maddison Chris J, 2014, ICML
   Movshovitz-Attias Dana, 2013, ACL
   Rush A. M., 2015, EMNLP
   Simonyan Karen, 2015, ICLR
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P2431
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Szegedy C., 2015, ARXIV151200567
   Vinyals O., 2016, ICLR
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Weston Jason, 2015, ICLR
   Xu K, 2015, ICML
   You Q., 2016, CVPR
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701024
DA 2019-06-15
ER

PT S
AU Ye, HJ
   Zhan, DC
   Si, XM
   Jiang, Y
   Zhou, ZH
AF Ye, Han-Jia
   Zhan, De-Chuan
   Si, Xue-Min
   Jiang, Yuan
   Zhou, Zhi-Hua
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI What Makes Objects Similar: A Unified Multi-Metric Learning Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Uni fi ed Multi-Metric Learning ((UML)-L-2) framework to exploit multiple types of metrics. In UM2L, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce fl exibilities for representing and utilizing both spatial and semantic linkages. Besides, we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classi fi cation performance and comprehensibility of (UML)-L-2. Visualization results also validate its ability on physical meanings discovery.
C1 [Ye, Han-Jia; Zhan, De-Chuan; Si, Xue-Min; Jiang, Yuan; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
RP Ye, HJ (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM yehj@lamda.nju.edu.cn; zhandc@lamda.nju.edu.cn; sixm@lamda.nju.edu.cn;
   jiangy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn
FU NSFC [61273301, 61333014]; Collaborative Innovation Center of Novel
   Software Technology and Industrialization; Tencent Fund
FX This research was supported by NSFC (61273301, 61333014), Collaborative
   Innovation Center of Novel Software Technology and Industrialization,
   and Tencent Fund.
CR Amid E., 2015, P 32 INT C MACH LEAR, P1472
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Chakrabarti D., 2014, P 31 INT C MACH LEAR, P874
   Changpinyo S., 2013, ADV NEURAL INFORM PR, P1511
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Fetaya E., 2015, ICML, P162
   Frank M, 2012, J MACH LEARN RES, V13, P459
   Hu JH, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700405
   Huang KZ, 2009, IEEE DATA MINING, P189, DOI 10.1109/ICDM.2009.22
   Jun Wang, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P223, DOI 10.1007/978-3-642-33460-3_20
   Leskovec J., 2012, ADV NEURAL INFORM PR, P539
   Lim D., 2013, P 30 INT C MACH LEAR, V28, P615
   Noh YK, 2010, NIPS, P1822
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   Shi Y, 2014, P 28 AAAI C ART INT, P2078
   Wang B, 2012, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2012.6248029
   Wang J., 2012, ADV NEURAL INFORM PR, P1601
   Wang W., 2010, P 27 INT C MACH LEAR, P1135
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Ying YM, 2012, J MACH LEARN RES, V13, P1
   Zhan D.-C., 2009, ICML 09, P1225, DOI DOI 10.1145/1553374.1553530
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhou Z. - H., 2012, ENSEMBLE METHODS FDN
   Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3
NR 25
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701049
DA 2019-06-15
ER

PT S
AU Yen, IEH
   Huang, XR
   Zhong, K
   Zhang, RH
   Ravikumar, P
   Dhillon, IS
AF Yen, Ian E. H.
   Huang, Xiangru
   Zhong, Kai
   Zhang, Ruohan
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dual Decomposed Learning with Factorwise Oracles for Structural SVMs of
   Large Output Domain
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many applications of machine learning involve structured outputs with large domains, where learning of a structured predictor is prohibitive due to repetitive calls to an expensive inference oracle. In this work, we show that by decomposing training of a Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages, one can replace an expensive structured oracle with Factorwise Maximization Oracles (FMOs) that allow efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is then proposed to exploit the sparsity of messages while guarantees convergence to epsilon sub-optimality after O (log(1/epsilon)) passes of FMOs over every factor. We conduct experiments on chain-structured and fully-connected problems of large output domains, where the proposed approach is orders-of-magnitude faster than current state-of-the-art algorithms for training Structural SVMs.
C1 [Yen, Ian E. H.; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Huang, Xiangru; Zhong, Kai; Zhang, Ruohan; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
RP Yen, IEH (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
FU ARO [W911NF-12-1-0390]; NSF [CCF-1320746, CCF-1117055, IIS-1149803,
   IIS-1546452, IIS-1320894, IIS-1447574, IIS-1546459, CCF-1564000,
   DMS-1264033]; NIH via as part of the Joint DMS/NIGMS Initiative [R01
   GM117594-01]
FX We acknowledge the support of ARO via W911NF-12-1-0390, NSF via grants
   CCF-1320746, CCF-1117055, IIS-1149803, IIS-1546452, IIS-1320894,
   IIS-1447574, IIS-1546459, CCF-1564000, DMS-1264033, and NIH via R01
   GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
   Research at the Interface of the Biological and Mathematical Sciences.
CR Das D, 2014, COMPUT LINGUIST, V40, P9, DOI 10.1162/COLI_a_00163
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gimpel K, 2012, P 2012 C N AM CHAPT, P221
   Hoffman A., 1952, J RES NBS
   Hong M., 2012, ARXIV12083922
   JAGGI M., 2013, P INT C MACH LEARN, V28, P53
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/s10994-009-5108-8, 10.1007/S10994-009-5108-8]
   Kumar M. P., 2007, P ANN C NEUR INF PRO, V20, P1041
   Lacoste-Julien Simon, 2015, ADV NEURAL INFORM PR, V28, P496
   Meshi O., 2015, AISTAT
   Meshi O., 2010, LEARNING EFFICIENTLY
   Meshi O., 2015, ARXIV151101419
   Osokin A., 2016, ARXIV160509346
   Ravikumar P., 2006, ICML, V2, P4
   Samdani R., 2012, ICML
   Shalev-Shwartz S., 2011, MATH PROGRAMMING
   Su TH, 2007, INT J DOC ANAL RECOG, V10, P27, DOI 10.1007/s10032-006-0037-6
   Taskar B., 2003, ADV NEURAL INFORM PR, V16
   Taskar B., 2005, ICML
   Tsochantaridis I, 2004, ICML
   Woodland P., 2000, ASR2000 AUT SPEECH R
   Yen I. E., 2016, CONVEX ATOMIC NORM A
   Yen I. E., 2016, AISTAT
   Yen I. E.-H., 2015, NIPS
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703058
DA 2019-06-15
ER

PT S
AU Yi, XY
   Wang, ZR
   Yang, ZR
   Caramanis, C
   Liu, H
AF Yi, Xinyang
   Wang, Zhaoran
   Yang, Zhuoran
   Caramanis, Constantine
   Liu, Han
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI More Supervision, Less Computation: Statistical-Computational Tradeoffs
   in Weakly Supervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability 1 W-alpha. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by alpha. In this paper, we characterize the effect of a by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small alpha, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as alpha increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.
C1 [Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA.
   [Wang, Zhaoran; Yang, Zhuoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA.
RP Yi, XY (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM yixy@utexas.edu; zhaoran@princeton.edu; zy6@princeton.edu;
   constantine@utexas.edu; hanliu@princeton.edu
CR Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Berthet Q., 2013, C LEARN THEOR
   Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127
   Cai TT, 2014, J R STAT SOC B, V76, P349, DOI 10.1111/rssb.12034
   Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110
   CHEN Y., 2014, ARXIV14021267
   Deshpande Y., 2014, ADV NEURAL INFORM PR
   Fan J., 2016, CURSE HETEROGE UNPUB
   Fan JQ, 2012, J R STAT SOC B, V74, P745, DOI 10.1111/j.1467-9868.2012.01029.x
   Feldman V., 2013, ACM S THEOR COMP
   Feldman V., 2015, ACM S THEOR COMP
   Feldman V., 2015, ARXIV151209170
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Gao C., 2014, ARXIV14098565
   Garcia- Garcia D., 2011, ADV NEURAL INFORM PR
   Hajek B., 2014, ARXIV14066625
   JOHNSTONE IM, 1994, ANN STAT, V22, P271, DOI 10.1214/aos/1176325368
   Joulin A., 2012, INT C MACH LEARN
   Kearns M., 1993, ACM S THEOR COMP
   Ma ZM, 2015, ANN STAT, V43, P1089, DOI 10.1214/14-AOS1300
   Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z
   Patrini G., 2016, ARXIV160202450
   Ramdas A., 2016, ARXIV160202210
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Vershynin  Roman, 2010, ARXIV10113027
   Wang T., 2014, ARXIV14085369
   Wang Z., 2015, ARXIV151208861
   Zhang Y., 2014, C LEARN THEOR
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704043
DA 2019-06-15
ER

PT S
AU Ying, YM
   Wen, LY
   Lyu, SW
AF Ying, Yiming
   Wen, Longyin
   Lyu, Siwei
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Online AUC Maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Area under ROC (AUC) is a metric which is widely used for measuring the classification performance for imbalanced data. It is of theoretical and practical interest to develop online learning algorithms that maximizes AUC for large-scale data. A specific challenge in developing online AUC maximization algorithm is that the learning objective function is usually defined over a pair of training examples of opposite classes, and existing methods achieves on-line processing with higher space and time complexity. In this work, we propose a new stochastic online algorithm for AUC maximization. In particular, we show that AUC optimization can be equivalently formulated as a convex-concave saddle point problem. From this saddle representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datum. We establish theoretical convergence of SOLAM with high probability and demonstrate its effectiveness on standard benchmark datasets.
C1 [Ying, Yiming] SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA.
   [Wen, Longyin; Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
RP Ying, YM (reprint author), SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA.
CR Bach F. R., 2011, NIPS
   Bottou L., 2003, NIPS
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Cortes C., 2003, NIPS
   Gao W., 2015, INT JOINT C ART INT
   Gao W., 2013, ICML
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Joachims T., 2005, ICML
   Kar P., 2013, ICML
   Kotlowski W., 2011, ICML
   Lan GH, 2012, MATH PROGRAM, V133, P365, DOI 10.1007/s10107-010-0434-y
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI 10.1214/aop/1176988477
   Rakhlin Alexander, 2012, ICML
   Rakotomamonjy A., 2004, 1 INT WORKSH ROC AN
   Wang Y., 2012, COLT
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Ying YM, 2016, NEURAL COMPUT, V28, P743, DOI 10.1162/NECO_a_00817
   Zhao P., 2011, ICML
   Zinkevich Martin, 2003, ICML
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700029
DA 2019-06-15
ER

PT S
AU You, Y
   Lian, XR
   Liu, J
   Yu, HF
   Dhillon, IS
   Demmel, J
   Hsieh, CJ
AF You, Yang
   Lian, XiangRu
   Liu, Ji
   Yu, Hsiang-Fu
   Dhillon, Inderjit S.
   Demmel, James
   Hsieh, Cho-Jui
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Asynchronous Parallel Greedy Coordinate Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we propose and study an Asynchronous parallel Greedy Coordinate Descent (Asy-GCD) algorithm for minimizing a smooth function with bounded constraints. At each iteration, workers asynchronously conduct greedy coordinate descent updates on a block of variables. In the first part of the paper, we analyze the theoretical behavior of Asy-GCD and prove a linear convergence rate. In the second part, we develop an efficient kernel SVM solver based on Asy-GCD in the shared memory multi-core setting. Since our algorithm is fully asynchronous-each core does not need to idle and wait for the other cores-the resulting algorithm enjoys good speedup and outperforms existing multi-core kernel SVM solvers including asynchronous stochastic coordinate descent and multi-core LIBSVM.
C1 [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA.
   [Lian, XiangRu; Liu, Ji] Univ Rochester, Rochester, NY 14627 USA.
   [Yu, Hsiang-Fu; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
   [You, Yang; Demmel, James] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP You, Y (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM youyang@cs.berkeley.edu; xiangru@yandex.com; jliu@cs.rochester.edu;
   rofuyu@cs.utexas.edu; inderjit@cs.utexas.edu; demmel@eecs.berkeley.edu;
   chohsieh@cs.ucdavis.edu
FU NSF [CNS-1548078, CCF-1320746, IIS-1546459, CCF-1564000]; U.S.
   Department of Energy Office of Science, Office of Advanced Scientific
   Computing Research, Applied Mathematics program [DE-SC0010200]; U.S.
   Department of Energy Office of Science, Office of Advanced Scientific
   Computing Research [DE-SC0008700, AC02-05CH11231]; DARPA
   [HR0011-12-2-0016]; Intel; Google; HP; Huawei; LGE; Nokia; NVIDIA;
   Oracle; S Samsung; Mathworks; Cray; XSEDE
FX XL and JL are supported by the NSF grant CNS-1548078. HFY and ISD are
   supported by the NSF grants CCF-1320746, IIS-1546459 and CCF-1564000. YY
   and JD are supported by the U.S. Department of Energy Office of Science,
   Office of Advanced Scientific Computing Research, Applied Mathematics
   program under Award Number DE-SC0010200; by the U.S. Department of
   Energy Office of Science, Office of Advanced Scientific Computing
   Research under Award Numbers DE-SC0008700 and AC02-05CH11231; by DARPA
   Award Number HR0011-12-2-0016, Intel, Google, HP, Huawei, LGE, Nokia,
   NVIDIA, Oracle and S Samsung, Mathworks and Cray. CJH also thank the
   XSEDE and Nvidia support.
CR Avron H, 2014, INT PARALL DISTRIB P, DOI 10.1109/IPDPS.2014.31
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Boser B. E., 1992, COLT, P144
   Canutescu A., 2003, PROTEIN SCI
   Chang C. - C., 2000, LIBSVM INTRO BENCHMA
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dean J., 2012, NIPS
   Dhillon I. S., 2011, NIPS
   Duchi John C, 2015, ARXIV150800882
   Hsieh C. - J., 2011, KDD
   Hsieh C. - J., 2008, ICML
   Hsieh C. - J., 2015, INT C MACH LEARN ICM
   Hsieh C. J., 2014, ICML
   Joachims T., 1998, ADV KERNEL METHODS S
   Li M., 2014, OSDI
   Liu J., 2014, ICML
   Liu J., 2014, ASYNCHRONOUS STOCHAS
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Nutini Julie, 2015, ICML
   Platt J., 1998, ADV KERNEL METHODS S
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Scherrer C., 2012, ICML
   Scherrer C., 2012, NIPS
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Sridhar S., 2013, NIPS
   Wang PW, 2014, J MACH LEARN RES, V15, P1523
   Xing E. P., 2015, KDD
   Yen I., 2013, KDD
   Yu H. - F., 2013, KAIS
   Yun H., 2014, VLDB
   Zhang H., 2015, ARXIV E PRINTS
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700034
DA 2019-06-15
ER

PT S
AU Young, J
   Modat, M
   Cardoso, MJ
   Ashburner, J
   Ourselin, S
AF Young, Jonathan
   Modat, Marc
   Cardoso, Manuel J.
   Ashburner, John
   Ourselin, Sebastien
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI An Oblique Approach to Prediction of Conversion to Alzheimer's Disease
   with Multikernel Gaussian Processes
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
DE Gaussian processes; Regression; Atrophy; BSI; Multi-kernel learning;
   MRI; PET Alzheimer's disease; Mild cognitive impairment
ID MILD COGNITIVE IMPAIRMENT
AB Machine learning approaches have had some success in predicting conversion to Alzheimer's Disease (AD) in subjects with mild cognitive impairment (MCI), a less serious condition that nonetheless is a risk factor for AD. Predicting conversion is clinically important as because novel drugs currently being developed require administration early in the disease process to be effective. Traditionally training data are labelled with discrete disease states; which may explain the limited accuracies obtained as labels are noisy due to the difficulty in providing a definitive diagnosis of Alzheimer's without post-mortem confirmation, and ignore the existence of a continuous spectrum of disease severity. Here, we dispense with discrete training labels and instead predict the loss of brain volume over one year, a quantity that can be repeatably and objectively measured with the boundary shift integral and is strongly correlated with conversion. The method combines MRI and PET image data and cerebrospinal fluid biomarker levels in an Bayesian multi-kernel learning framework. The resulting predicted atrophy separates converting and non-converting MCI subjects with 74.6% accuracy, which compares well to state of the art methods despite a small training set size.
C1 [Young, Jonathan; Modat, Marc; Cardoso, Manuel J.; Ourselin, Sebastien] UCL, Ctr Med Image Comp, London, England.
   [Ashburner, John] UCL, Inst Neurol, Wellcome Trust Ctr Neuroimaging, London, England.
   [Ourselin, Sebastien] UCL, Inst Neurol, Dementia Res Ctr, London, England.
RP Young, J (reprint author), UCL, Ctr Med Image Comp, London, England.
EM jonathan.young@ucl.ac.uk
RI Cardoso, M. Jorge/N-5511-2018
OI Cardoso, M. Jorge/0000-0003-1284-2558
CR Aksu Y, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025074
   Beach TG, 2012, J NEUROPATH EXP NEUR, V71, P266, DOI 10.1097/NEN.0b013e31824b211b
   BRAAK H, 1995, NEUROBIOL AGING, V16, P271, DOI 10.1016/0197-4580(95)00021-6
   Cardoso M. J., 2012, 2012 IEEE Workshop on Mathematical Methods in Biomedical Image Analysis (MMBIA), P153, DOI 10.1109/MMBIA.2012.6164748
   Cardoso MJ, 2011, NEUROIMAGE, V56, P1386, DOI 10.1016/j.neuroimage.2011.02.013
   Gaser C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0067346
   Leung KK, 2012, NEUROIMAGE, V59, P3995, DOI 10.1016/j.neuroimage.2011.10.068
   Modat M, 2010, COMPUT METH PROG BIO, V98, P278, DOI 10.1016/j.cmpb.2009.09.002
   Petersen RC, 1999, ARCH NEUROL-CHICAGO, V56, P303, DOI 10.1001/archneur.56.3.303
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robert R, 2012, ARCH BIOCHEM BIOPHYS, V526, P132, DOI 10.1016/j.abb.2012.02.022
   Young J, 2013, NEUROIMAGE-CLIN, V2, P735, DOI 10.1016/j.nicl.2013.05.004
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
NR 13
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 122
EP 128
DI 10.1007/978-3-319-45174-9_13
PG 7
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400013
DA 2019-06-15
ER

PT S
AU Yu, FX
   Suresh, AT
   Choromanski, K
   Holtmann-Rice, D
   Kumar, S
AF Yu, Felix Xinnan
   Suresh, Ananda Theertha
   Choromanski, Krzysztof
   Holtmann-Rice, Daniel
   Kumar, Sanjiv
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Orthogonal Random Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from O (d(2)) to O (d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORE Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.
C1 [Yu, Felix Xinnan; Suresh, Ananda Theertha; Choromanski, Krzysztof; Holtmann-Rice, Daniel; Kumar, Sanjiv] Google Res, New York, NY 10011 USA.
RP Yu, FX (reprint author), Google Res, New York, NY 10011 USA.
EM felixyu@google.com; theertha@google.com; kchoro@google.com;
   dhr@google.com; sanjivk@google.com
CR Ailon Nir, 2006, STOC
   Andoni A., 2015, NIPS
   Bochner S., 1955, HARMONIC ANAL THEORY
   Charikar Moses S, 2002, STOC
   Cheng Yu, 2015, ICCV
   Choromanski  K., 2016, TRIPLESPIN A GENERIC
   Choromanski  K., 2015, ICML
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   FINO BJ, 1976, IEEE T COMPUT, V25, P1142
   Joachims  T., 2006, KDD
   Kar Purushottam, 2012, AISTATS
   Kennedy  C., 2016, FAST CROSS POLYTOPE
   Le  Q., 2013, ICML
   Li FX, 2010, LECT NOTES COMPUT SC, V6376, P262
   Maji  S., 2009, ICCV
   MUIRHEAD R. J., 2009, ASPECTS MULTIVARIATE, V197
   Niederreiter H., 2010, QUASIMONTE CARLO MET
   Pennington Jeffrey, 2015, NIPS
   Rahimi A., 2007, NIPS
   Rudi  A., 2016, ARXIV160204474
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Sreekanth  V., 2010, BMVC
   Sriperumbudur  B., 2015, NIPS
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Yang  J., 2014, ICML
   Yang  T., 2012, NIPS
   Yu F. X., 2014, ICML
   Yu Felix X., 2015, ARXIV150303893
   Zhang X., 2015, ICCV
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701103
DA 2019-06-15
ER

PT S
AU Yu, HF
   Rao, N
   Dhillon, IS
AF Yu, Hsiang-Fu
   Rao, Nikhil
   Dhillon, Inderjit S.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Temporal Regularized Matrix Factorization for High-dimensional Time
   Series Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting. For example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000. In addition, the data is generally noisy and full of missing values. Thus, modern applications require methods that are highly scalable, and can deal with noisy data in terms of corruptions or missing values. However, classical time series methods usually fall short of handling these issues. In this paper, we present a temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal learning and forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values. Our proposed TRMF is highly general, and subsumes many existing approaches for time series analysis. We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular, TRMF is two orders of magnitude faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.
C1 [Yu, Hsiang-Fu; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
   [Rao, Nikhil] Technicolor Res, Issy Les Moulineaux, France.
RP Yu, HF (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM rofuyu@cs.utexas.edu; nikhilrao86@gmail.com; inderjit@cs.utexas.edu
FU NSF [CCF-1320746, IIS-1546459, CCF-1564000]
FX This research was supported by NSF grants (CCF-1320746, IIS-1546459, and
   CCF-1564000) and gifts from Walmart Labs and Adobe. We thank Abhay Jha
   for the help on Walmart experiments.
CR Anava O., 2015, P 32 INT C MACH LEAR, P2191
   Chen Z., 2005, 68 RIKEN LAB ADV BRA
   Ghahramani  Z., 1996, CRGTR962 U TOTR DEP
   Graham R.L., 1994, CONCRETE MATH FDN CO
   Han F., 2013, P 30 INT C MACH LEAR, V28, P172
   JAIN P, 2013, ARXIV13060626
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Li L., 2011, P 28 INT C MACH LEAR, P185
   Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507
   Melnyk I., 2016, P 23 INT C MACH LEAR
   Nicholson WB, 2014, TECHNICAL REPORT
   Petris G, 2009, USE R, P1, DOI 10.1007/b135794_1
   Petris G, 2010, J STAT SOFTW, V36, P1
   Rallapalli S, 2010, MOBICOM 10 & MOBIHOC 10: PROCEEDINGS OF THE 16TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING AND THE 11TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P161
   Rao N., 2015, ADV NEURAL INFORM PR, V27
   Roughan M, 2012, IEEE ACM T NETWORK, V20, P662, DOI 10.1109/TNET.2011.2169424
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
   Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12
   Sun JZ, 2012, INT CONF ACOUST SPEE, P1897, DOI 10.1109/ICASSP.2012.6288274
   Wang HS, 2007, J ROY STAT SOC B, V69, P63
   West M., 2013, SPRINGER SERIES STAT
   Wilson KW, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P411
   Xiong L., 2010, SIAM INT C DATA MINI, P223
   Xu M., 2013, ADV NEURAL INFORM PR, P2301
   Yu H., 2014, P 31 INT C MACH LEAR, P593
   Zhang Y, 2009, SIGCOMM 2009, P267
   Zheng FH, 2012, PROCEEDINGS OF 2012 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (8TH), VOL III, P403
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701017
DA 2019-06-15
ER

PT S
AU Yu, M
   Gupta, V
   Kolar, M
AF Yu, Ming
   Gupta, Varun
   Kolar, Mladen
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Statistical Inference for Pairwise Graphical Models Using Score Matching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SELECTION
AB Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result, there is a large body of literature focused on consistent model selection. However, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form. We prove that the estimator is root n-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.
C1 [Yu, Ming; Gupta, Varun; Kolar, Mladen] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.
RP Yu, M (reprint author), Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.
EM mingyu@chicagobooth.edu; varun.gupta@chicagobooth.edu;
   mladen.kolar@chicagobooth.edu
FU IBM Corporation Faculty Research Fund at the University of Chicago Booth
   School of Business
FX This work is supported by an IBM Corporation Faculty Research Fund at
   the University of Chicago Booth School of Business. This work was
   completed in part with resources provided by the University of Chicago
   Research Computing Center.
CR [Anonymous], 2014, JMLR WORKSHOP C P, V33, P1042
   Arnold B. C., 1999, SPR S STAT
   Barber R. F., 2015, ARXIV150207641
   Belloni A, 2014, REV ECON STUD, V81, P608, DOI 10.1093/restud/rdt044
   Belloni A, 2013, BERNOULLI, V19, P521, DOI 10.3150/11-BEJ410
   Chen M., 2015, J AM STAT ASS
   Chen S., 2013, ARXIV13110085
   Drton M., 2016, ANN REV STAT ITS APP, V3
   Forbes PGM, 2015, LINEAR ALGEBRA APPL, V473, P261, DOI 10.1016/j.laa.2014.08.015
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Jankova J., 2014, ARXIV14036752
   Javanmard A, 2014, J MACH LEARN RES, V15, P2869
   Kolar M, 2015, ADV NEURAL INFORM PR, V28, P2287
   Lee JD, 2015, J COMPUT GRAPH STAT, V24, P230, DOI 10.1080/10618600.2014.900500
   Leeb H., 2007, ECON THEOR, V24, P338
   Lin L., 2015, ARXIV150700433
   Liu WD, 2015, J MULTIVARIATE ANAL, V135, P153, DOI 10.1016/j.jmva.2014.11.005
   Liu WD, 2013, ANN STAT, V41, P2948, DOI 10.1214/13-AOS1169
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   NEYMAN J, 1959, PROBABILITY STATISTI, P213
   Parry M, 2012, ANN STAT, V40, P561, DOI 10.1214/12-AOS971
   PORTNOY S, 1988, ANN STAT, V16, P356, DOI 10.1214/aos/1176350710
   POTSCHER B. M., 2009, SANKHYA, V71, P1
   Ren Z, 2015, ANN STAT, V43, P991, DOI 10.1214/14-AOS1286
   Rudelson M., 2011, RECONSTRUCTION ANISO
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Sriperumbudur B., 2013, ARXIV13123516
   Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221
   Wang J., 2016, P AISTATS, V51, P751
   Yang EH, 2015, J MACH LEARN RES, V16, P3813
   Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704054
DA 2019-06-15
ER

PT S
AU Yuan, XT
   Li, P
   Zhang, T
   Liu, QS
   Liu, GC
AF Yuan, Xiao-Tong
   Li, Ping
   Zhang, Tong
   Liu, Qingshan
   Liu, Guangcan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Additive Exponential Family Graphical Models via l(2,1)-norm
   Regularized M-Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID COVARIANCE ESTIMATION; SELECTION
AB We investigate a subclass of exponential family graphical models of which the sufficient statistics are de fined by arbitrary additive forms. We propose two l(2;1)-norm regularized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint MLE estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional MLE estimator which estimates the parameters for each node individually. For both estimators, statistical analysis shows that under mild conditions the extra flexibility gained by the additive exponential family models comes at almost no cost of statistical efficiency. A Monte-Carlo approximation method is developed to efficiently optimize the proposed estimators. The advantages of our estimators over Gaussian graphical models and Nonparanormal estimators are demonstrated on synthetic and real data sets.
C1 [Yuan, Xiao-Tong; Liu, Qingshan; Liu, Guangcan] Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.
   [Li, Ping; Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ USA.
   [Li, Ping] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ USA.
RP Yuan, XT (reprint author), Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.
EM xtyuan@nuist.edu.cn; pingli@stat.rutgers.edu; tzhang@stat.rutgers.edu;
   qsliu@nuist.edu.cn; gcliu@nuist.edu.cn
FU [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764]; 
   [AFOSR-FA9550-13-1-0137];  [NSFC-61402232];  [NSFC-61522308]; 
   [NSFJP-BK20141003];  [NSF-IIS-1407939];  [NSF-IIS-1250985]; 
   [NSFC-61532009];  [NSFC-61622305];  [NSFC-61502238];  [NSFJP-BK20160040]
FX Xiao-Tong Yuan and Ping Li were partially supported by
   NSF-Bigdata-1419210, NSF-III-1360971, ONR-N00014-13-1-0764, and
   AFOSR-FA9550-13-1-0137. Xiao-Tong Yuan is also partially supported by
   NSFC-61402232, NSFC-61522308, and NSFJP-BK20141003. Tong Zhang is
   supported by NSF-IIS-1407939 and NSF-IIS-1250985. Qingshan Liu is
   supported by NSFC-61532009. Guangcan Liu is supported by NSFC-61622305,
   NSFC-61502238 and NSFJP-BK20160040.
CR Bach F., 2002, P 16 ANN C NEUR INF
   Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Candes EJ, 2011, APPL COMPUT HARMON A, V31, P59, DOI 10.1016/j.acha.2010.10.002
   Dobra A, 2011, ANN APPL STAT, V5, P969, DOI 10.1214/10-AOAS397
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Gu C, 2013, STAT SINICA, V23, P1131, DOI 10.5705/ss.2011.319
   Lafferty J, 2012, STAT SCI, V27, P519, DOI 10.1214/12-STS391
   Lin LN, 2016, ELECTRON J STAT, V10, P806, DOI 10.1214/16-EJS1126
   Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Owen AB, 2013, MONTE CARLO THEORY M
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Schmidt  M., 2011, ADV NEURAL INFORM PR, P1458
   SPEED TP, 1986, ANN STAT, V14, P138, DOI 10.1214/aos/1176349846
   Sun S., 2015, P 29 ANN C NEUR INF
   Tansey W., 2015, P 32 INT C MACH LEAR, P684
   Tseng P., 2008, SIAM J OPTIMIZATION
   Vershynin R., 2011, ARXIVABS10113027 COR
   Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Xue LZ, 2012, ANN STAT, V40, P2541, DOI 10.1214/12-AOS1041
   Yang EH, 2015, J MACH LEARN RES, V16, P3813
   YANG Z, 2014, ARXIV14128697
   Yuan M, 2010, J MACH LEARN RES, V11, P2261
   Zhang C. -H., 2012, ARXIVABS12013302 COR
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700070
DA 2019-06-15
ER

PT S
AU Yuan, XT
   Li, P
   Zhang, T
AF Yuan, Xiao-Tong
   Li, Ping
   Zhang, Tong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Exact Recovery of Hard Thresholding Pursuit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHM; SPARSITY
AB The Hard Thresholding Pursuit (HTP) is a class of truncated gradient descent methods for finding sparse solutions of l(0)-constrained loss minimization problems. The HTP-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications. However, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency. It remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original NP-hard problem. In this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for HTP-style methods under restricted strong condition number bounding conditions. We further show that HTP-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number. Numerical results on simulated data confirms our theoretical predictions.
C1 [Yuan, Xiao-Tong] Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.
   [Li, Ping; Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.
   [Li, Ping] Rutgers State Univ, Dept CS, Piscataway, NJ 08854 USA.
RP Yuan, XT (reprint author), Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.
EM xtyuan@nuist.edu.cn; pingli@stat.rutgers.edu; tzhang@stat.rutgers.edu
FU [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764]; 
   [AFOSR-FA9550-13-1-0137];  [NSFC-61402232];  [NSFC-61522308]; 
   [NSFJP-BK20141003];  [NSF-IIS-1407939];  [NSF-IIS-1250985]
FX Xiao-Tong Yuan and Ping Li were partially supported by
   NSF-Bigdata-1419210, NSF-III-1360971, ONR-N00014-13-1-0764, and
   AFOSR-FA9550-13-1-0137. Xiao-Tong Yuan is also partially supported by
   NSFC-61402232, NSFC-61522308, and NSFJP-BK20141003. Tong Zhang is
   supported by NSF-IIS-1407939 and NSF-IIS-1250985.
CR Agarwal A., 2010, P 24 ANN C NEUR INF
   Bahmani S, 2013, J MACH LEARN RES, V14, P807
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Blumensath T, 2013, IEEE T INFORM THEORY, V59, P3466, DOI 10.1109/TIT.2013.2245716
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jain P., 2016, STRUCTURED SPARSE RE
   Jalali A., 2011, P 25 ANN C NEUR INF
   Jie Shen, 2016, TIGHT BOUND HARD THR
   Li Xingguo, 2016, P 33 INT C MACH LEAR
   Li Yen-Huan, 2015, P 18 INT C ART INT S
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574
   Yuan X. -T., 2014, P 31 INT C MACH LEAR
   Yuan XT, 2013, IEEE T PATTERN ANAL, V35, P3025, DOI 10.1109/TPAMI.2013.85
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703068
DA 2019-06-15
ER

PT S
AU Yun, SY
   Proutiere, A
AF Yun, Se-Young
   Proutiere, Alexandre
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Cluster Recovery in the Labeled Stochastic Block Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label 'appears with probability p (i, j, l) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(n polylog(n)) computations and without the a-priori knowledge of the model parameters.
C1 [Yun, Se-Young] Los Alamos Natl Lab, CNLS, Los Alamos, NM 87545 USA.
   [Proutiere, Alexandre] KTH, Automat Control Dept, S-10044 Stockholm, Sweden.
RP Yun, SY (reprint author), Los Alamos Natl Lab, CNLS, Los Alamos, NM 87545 USA.
EM syun@lanl.gov; alepro@kth.se
FU U.S. Department of Energy through the LANL/LDRD Program
FX We gratefully acknowledge the support of the U.S. Department of Energy
   through the LANL/LDRD Program for this work.
CR Abbe E., 2014, ABS14053267 CORR
   Abbe E., 2015, NIPS
   Abbe E., 2015, FOCS
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   Decelle A., 2011, PHYS REV LETT, V107
   Deshpande Y., 2015, ABS150708685 CORR
   Gao C., 2015, ABS150503772 CORR
   Hajek B., 2015, ABS150907859 CORR
   Hajek B., 2014, ABS14126156 CORR
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Heimlicher Simon, 2012, NIPS WORKSH ALG STAT
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jog V., 2015, ABS150906418 CORR
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Leskovec J., 2010, CHI
   Massoulie L., 2014, STOC
   Mossel E., 2015, STOC
   Mossel E., 2015, ABS150903281 CORR
   Mossel E, 2015, PROBAB THEORY REL, V162, P431, DOI 10.1007/s00440-014-0576-6
   Traag VA, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.036115
   Yun S., 2014, ABS14127335 CORR
   Yun S., 2014, COLT
   Zhang A., 2015, ABS150705313 CORR
   Zhang P., 2015, ABS150900107 CORR
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701053
DA 2019-06-15
ER

PT S
AU Yurochkin, M
   Nguyen, X
AF Yurochkin, Mikhail
   Nguyen, XuanLong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Geometric Dirichlet Means algorithm for topic inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.
C1 [Yurochkin, Mikhail; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
RP Yurochkin, M (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
EM moonfolk@umich.edu; xuanlong@umich.edu
FU  [NSF CAREER DMS-1351362];  [NSF CNS-1409303]
FX This research is supported in part by grants NSF CAREER DMS-1351362 and
   NSF CNS-1409303.
CR Anandkumar Animashree, 2012, ADV NEURAL INFORM PR
   Arora S., 2012, ARXIV12124777
   Blei  D., 2006, ADV NEURAL INFORM PR
   Blei D. M., 2006, ICML, P113, DOI DOI 10.1145/1143844.1143859
   Blei David M., 2008, ADV NEURAL INFORM PR, V7, P121
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Ding C., 2006, P NAT C ART INT, V21, P342
   Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836
   Golubitsky O., 2012, ACM COMMUN COMPUT AL, V46, P57
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Jiang K., 2012, ADV NEURAL INFORM PR, P3158
   Kulis B., 2012, P 29 INT C MACH LEAR
   Nguyen XL, 2015, BERNOULLI, V21, P618, DOI 10.3150/13-BEJ582
   Pritchard JK, 2000, GENETICS, V155, P945
   Tang J., 2014, P 31 INT C MACH LEAR, V32, P190
   Teh Yee Whye, 2006, J AM STAT ASS, V101
   Xu W., 2003, P 26 SIGIR TOR ON CA, P267, DOI DOI 10.1145/860435.860485
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702079
DA 2019-06-15
ER

PT S
AU Yurtsever, A
   Vu, BC
   Cevher, V
AF Yurtsever, Alp
   Bang Cong Vu
   Cevher, Volkan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Three-Composite Convex Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE
AB We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Numerical evidence supports the effectiveness of our method in real-world problems.
C1 [Yurtsever, Alp; Bang Cong Vu; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
RP Yurtsever, A (reprint author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
EM alp.yurtsever@epfl.ch; bang.vu@epfl.ch; cevher@epfl.ch
FU ERC; SNF [200021-146750, CRSII2-147633]; NCCR-Marvel
FX This work was supported in part by ERC Future Proof, SNF 200021-146750,
   SNF CRSII2-147633, and NCCR-Marvel.
CR Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032
   Atchade Y. F., 2014, ARXIV14022365V2
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Borodin A, 2004, ADV NEUR IN, V16, P345
   Briceno-Arias LM, 2015, OPTIMIZATION, V64, P1239, DOI 10.1080/02331934.2013.855210
   Brodie J, 2009, P NATL ACAD SCI USA, V106, P12267, DOI 10.1073/pnas.0904287106
   Cevher V., 2016, 215759 EPFL
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Combettes P. L., 2015, ARXIV150707095V1
   Combettes PL, 2014, OPTIMIZATION, V63, P1289, DOI 10.1080/02331934.2012.733883
   Davis D., 2015, ARXIV150401032V1
   Devolder O., 2011, TECHNICAL REPORT
   DUCHI J, 2009, MACH LEARN RES, V10, P2899
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Fama EF, 1996, J FINANC, V51, P55, DOI 10.2307/2329302
   Hu C., 2009, ADV NEURAL INF PROCE, P781
   Lan GH, 2012, MATH PROGRAM, V133, P365, DOI 10.1007/s10107-010-0434-y
   Ledoux M., 1991, PROBABILITY BANACH S
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin QH, 2014, OPTIM METHOD SOFTW, V29, P1281, DOI 10.1080/10556788.2014.891592
   Mosci S, 2010, LECT NOTES ARTIF INT, V6322, P418, DOI 10.1007/978-3-642-15883-4_27
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   Raguet H, 2013, SIAM J IMAGING SCI, V6, P1199, DOI 10.1137/120872802
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rosasco L., 2014, ARXIV14035074V3
   Vu BC, 2016, OPTIM LETT, V10, P781, DOI 10.1007/s11590-015-0904-5
   Wang M., 2015, ARXIV151103760V1
   Zhong L. W., 2014, J MACH LEARN RECH, P1086
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700091
DA 2019-06-15
ER

PT S
AU Zanella, G
   Betancourt, B
   Wallach, H
   Miller, J
   Zaidi, A
   Steorts, RC
AF Zanella, Giacomo
   Betancourt, Brenda
   Wallach, Hanna
   Miller, Jeffrey
   Zaidi, Abbas
   Steorts, Rebecca C.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Flexible Models for Microclustering with Application to Entity
   Resolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman-Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.
C1 [Zanella, Giacomo] Bocconi Univ, Dept Decis Sci, Milan, Italy.
   [Betancourt, Brenda; Zaidi, Abbas; Steorts, Rebecca C.] Duke Univ, Dept Stat Sci, Durham, NC USA.
   [Wallach, Hanna] Microsoft Res, New York, NY USA.
   [Miller, Jeffrey] Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA.
   [Steorts, Rebecca C.] Duke Univ, Dept Comp Sci, Durham, NC USA.
RP Zanella, G (reprint author), Bocconi Univ, Dept Decis Sci, Milan, Italy.
EM giacomo.zanella@unibocconi.it; bb222@stat.duke.edu; hanna@dirichlet.net;
   jwmiller@hsph.harvard.edu; amz19@stat.duke.edu; beka@stat.duke.edu
FU NSF [SBE-0965436, DMS-1045153, IIS-1320219]; NIH [5R01ES017436-05]; John
   Templeton Foundation; Foerster-Bernstein Postdoctoral Fellowship; UMass
   Amherst CIIR; EPSRC
FX We thank Tamara Broderick, David Dunson, Merlise Clyde, and Abel
   Rodriguez for conversations that helped form the ideas in this paper. In
   particular, Tamara Broderick played a key role in developing the idea of
   microclustering. We also thank the Human Rights Data Analysis Group for
   providing us with data. This work was supported in part by NSF grants
   SBE-0965436, DMS-1045153, and IIS-1320219; NIH grant 5R01ES017436-05;
   the John Templeton Foundation; the Foerster-Bernstein Postdoctoral
   Fellowship; the UMass Amherst CIIR; and an EPSRC Doctoral Prize
   Fellowship.
CR Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   Broderick T., 2014, NIPS 2014 WORKSH ADV
   Christen  P., 2012, DATA MATCHING CONCEP
   Christen P., 2012, IEEE T KNOWLEDGE DAT, V24
   HALL R., 2014, JMLR W CP, P922
   Ishwaran H, 2003, STAT SINICA, V13, P1211
   KINGMAN JFC, 1978, J LOND MATH SOC, V18, P374, DOI 10.1112/jlms/s2-18.2.374
   KOLCHIN VF, 1971, THEOR PROBAB APPL+, V16, P74, DOI 10.1137/1116005
   Miller J. W., 2015, ARXIV150206241
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   Pitman J., 2006, ECOLE ETE PROBABILIT
   Price M., 2013, UPDATED STAT ANAL DO
   Price M., 2014, UPDATED STAT ANAL DO
   Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Steorts R. C., J AM STAT SOC
   Steorts RC, 2015, BAYESIAN ANAL, V10, P849, DOI 10.1214/15-BA965SI
   Steorts RC, 2014, LECT NOTES COMPUT SC, V8744, P253, DOI 10.1007/978-3-319-11257-2_20
   Wallach H. M., 2010, P 13 INT C ART INT S
   Winkler W, 2006, TECHNICAL REPORT
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702081
DA 2019-06-15
ER

PT S
AU Zantedeschi, V
   Emonet, R
   Sebban, M
AF Zantedeschi, Valentina
   Emonet, Remi
   Sebban, Marc
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB During the past few years, the machine learning community has paid attention to developing new methods for learning from weakly labeled data. This field covers different settings like semi-supervised learning, learning with label proportions, multi-instance learning, noise-tolerant learning, etc. This paper presents a generic framework to deal with these weakly labeled scenarios. We introduce the beta-risk as a generalized formulation of the standard empirical risk based on surrogate margin-based loss functions. This risk allows us to express the reliability on the labels and to derive different kinds of learning algorithms. We specifically focus on SVMs and propose a soft margin beta-SVM algorithm which behaves better that the state of the art.
C1 [Zantedeschi, Valentina; Emonet, Remi; Sebban, Marc] Univ Lyon, UJM St Etienne, CNRS, Inst Opt,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France.
RP Zantedeschi, V (reprint author), Univ Lyon, UJM St Etienne, CNRS, Inst Opt,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France.
EM valentina.zantedeschi@univ-st-etienne.fr;
   remi.emonet@univ-st-etienne.fr; marc.sebban@univ-st-etienne.fr
FU ANR project SOLSTICE [ANR-13-BS02-01]; ANR project LIVES
   [ANR-15-CE230026-03]
FX We thank the reviewers for their valuable remarks. We also thank the ANR
   projects SOLSTICE (ANR-13-BS02-01) and LIVES (ANR-15-CE230026-03).
CR Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Ben-David S., 2012, P 29 INT C MACH LEAR
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bruzzone L, 2006, IEEE T GEOSCI REMOTE, V44, P3363, DOI 10.1109/TGRS.2006.877950
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Domahidi Alexander, 2013, 2013 European Control Conference (ECC), P3071
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Hastie T., 2009, UNSUPERVISED LEARNIN
   Joulin A., 2012, ARXIV12066413
   Kearns M., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P459, DOI 10.1145/237814.237994
   Li YF, 2013, J MACH LEARN RES, V14, P2151
   Lichman M., 2013, UCI MACHINE LEARNING
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225
   Patrini G, 2014, ADV NEURAL INFORM PR, V1, P190
   Patrini G., 2016, ARXIV160202450
   Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104
   Sheng V. S., 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965
   Zhu X., 2005, 1530 U WISC MAD COMP
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701013
DA 2019-06-15
ER

PT S
AU Zhai, SF
   Cheng, Y
   Lu, WN
   Zhang, ZF
AF Zhai, Shuangfei
   Cheng, Yu
   Lu, Weining
   Zhang, Zhongfei (Mark)
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Doubly Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.
C1 [Zhai, Shuangfei; Zhang, Zhongfei (Mark)] Binghamton Univ, Vestal, NY 13902 USA.
   [Cheng, Yu] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Lu, Weining] Tsinghua Univ, Beijing 10084, Peoples R China.
RP Zhai, SF (reprint author), Binghamton Univ, Vestal, NY 13902 USA.
EM szhai2@binghamton.edu; chengyu@us.ibm.com; luwn14@mails.tsinghua.edu.cn;
   zhongfei@cs.binghamton.edu
CR Agostinelli Forest, 2014, ABS14126830 CORR
   Cheng Yu, 2015, INT C COMP VIS ICCV
   Clevert D.A., 2015, ARXIV151107289
   Cohen T. S., 2016, ARXIV160207576
   Dieleman S., 2016, ARXIV160202660
   Gens Robert, 2014, ADV NEURAL INFORM PR, P2537
   Goodfellow I.J., 2013, ARXIV13024389
   He  K., 2015, ARXIV151203385
   Ioffe S., 2015, ARXIV150203167
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lee CY, 2016, ARXIV14095185
   Li Hongyang, 2016, ARXIV160400676
   Lin M., 2013, ARXIV13124400
   Novikov A, 2015, ADV NEUR IN, V28
   Shang  W., 2016, ARXIV160305201
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sindhwani V., 2015, ADV NEURAL INFORM PR, P3088
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yang Zichao, 2015, INT C COMP VIS ICCV
   Yu F., 2015, ARXIV151107122
   Zeiler M.D., 2012, ARXIV12125701
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702087
DA 2019-06-15
ER

PT S
AU Zhang, HY
   Reddi, SJ
   Sra, S
AF Zhang, Hongyi
   Reddi, Sashank J.
   Sra, Suvrit
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS; CONVEXITY
AB We study optimization of finite sums of geodesically smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sums have witnessed tremendous attention in the recent years, existing work is limited to vector space problems. We introduce Riemannian SVRG (RSVRG), a new variance reduced Riemannian optimization method. We analyze RSVRG for both geodesically convex and nonconvex (smooth) functions. Our analysis reveals that RSVRG inherits advantages of the usual SVRG method, but with factors depending on curvature of the manifold that influence its convergence. To our knowledge, RSVRG is the first provably fast stochastic Riemannian method. Moreover, our paper presents the first non-asymptotic complexity analysis (novel even for the batch setting) for nonconvex Riemannian optimization. Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis.
C1 [Zhang, Hongyi; Sra, Suvrit] MIT, Cambridge, MA 02139 USA.
   [Reddi, Sashank J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Zhang, HY (reprint author), MIT, Cambridge, MA 02139 USA.
EM hongyiz@mit.edu; sjakkamr@cs.cmu.edu; suvrit@mit.edu
FU NSF [IIS-1409802]; Leventhal Fellowship
FX SS acknowledges support of NSF grant: IIS-1409802. HZ acknowledges
   support from the Leventhal Fellowship.
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Agarwal  A., 2015, P 32 INT C MACH LEAR, P78
   Allen-Zhu Z., 2016, ARXIV160305643
   Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Bhatia R, 2007, PRINC SER APPL MATH, P1
   Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Cherian  A., 2015, ARXIV150702772
   Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Garber D., 2015, ARXIV150905647
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Gong Pinghua, 2014, ARXIV14061102
   Hosseini  R., 2015, NIPS
   Jeuris B, 2012, ELECTRON T NUMER ANA, V39, P379
   Jin C., 2015, ARXIV151008896
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kasai  H., 2016, ARXIV160507367
   Konecny J., 2013, ARXIV13121666
   Liu XW, 2004, IEEE T PATTERN ANAL, V26, P662, DOI 10.1109/TPAMI.2004.1273986
   Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877
   OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9
   Petersen P., 2006, RIEMANNIAN GEOMETRY, V171
   Reddi S. J., 2016, ARXIV160306160
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rubinstein R. Y., 2011, SIMULATION MONTE CAR, V707
   Schmidt  M., 2013, ARXIV13092388
   Shamir O., 2015, P 32 INT C MACH LEAR, P144
   Sra  S., 2013, ADV NEURAL INFORM PR, P2562
   Sun  J., 2015, ARXIV151104777
   Tan M., 2014, P 31 INT C MACH LEAR, V32, P1539
   Udriste C, 1994, CONVEX FUNCTIONS OPT, V297
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xinru Yuan, 2016, Procedia Computer Science, V80, P2147, DOI 10.1016/j.procs.2016.05.534
   Zhang  H., 2016, ARXIV160206053
   Zhang T, 2013, IEEE T SIGNAL PROCES, V61, P4141, DOI 10.1109/TSP.2013.2267740
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704040
DA 2019-06-15
ER

PT S
AU Zhang, HS
   Liang, YB
AF Zhang, Huishuai
   Liang, Yingbin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Reshaped Wirtinger Flow for Solving Quadratic System of Equations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GRADIENT SAMPLING ALGORITHM; PHASE RETRIEVAL; NONSMOOTH; RECOVERY
AB We study the problem of recovering a vector x is an element of R-n from its magnitude measurements y(i) = vertical bar < a(i),x >vertical bar, i = 1, ..., m. Our work is along the line of the Wirtinger flow (WF) approach Candes et al. [2015], which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization. In contrast to the smooth loss function used in WF, we adopt a nonsmooth but lower-order loss function, and design a gradient-like algorithm (referred to as reshaped-WF). We show that for random Gaussian measurements, reshaped-WF enjoys geometric convergence to a global optimal point as long as the number m of measurements is at the order of O(n), where n is the dimension of the unknown x. This improves the sample complexity of WF, and achieves the same sample complexity as truncated-WF Chen and Candes [2015] but without truncation at gradient step. Furthermore, reshaped-WF costs less computationally than WF, and runs faster numerically than both WF and truncated-WE Bypassing higher-order variables in the loss function and truncations in the gradient loop, analysis of reshaped-WF is simplified.
C1 [Zhang, Huishuai; Liang, Yingbin] Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.
RP Zhang, HS (reprint author), Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.
EM hzhan23@syr.edu; yliang06@syr.edu
FU  [AFOSR FA9550-16-1-0077];  [NSF ECCS 16-09916]
FX This work is supported in part by the grants AFOSR FA9550-16-1-0077 and
   NSF ECCS 16-09916.
CR Burke JV, 2005, SIAM J OPTIMIZ, V15, P751, DOI 10.1137/030601296
   Cai T. T., 2015, ARXIV150603382
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chai AW, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/1/015005
   Chen Y., 2015, ADV NEURAL INFORM PR
   Donahue James D., 1964, TECHNICAL REPORT
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   Fogel F., 2013, ARXIV13047735
   GERCHBERG RW, 1972, OPTIK, V35, P237
   Glorot X., 2011, INT C ART INT STAT A
   Gross D., 2015, APPL COMPUTATIONAL H
   Kiwiel KC, 2007, SIAM J OPTIMIZ, V18, P379, DOI 10.1137/050639673
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kruger AY, 2003, J MATH SCI, V116, P3325, DOI DOI 10.1023/A:1023673105317
   Netrapalli P., 2013, ADV NEURAL INFORM PR
   Ochs P, 2015, SIAM J IMAGING SCI, V8, P331, DOI 10.1137/140971518
   Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673
   Sun J., 2016, ARXIV160206664
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wei K, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/12/125008
   Zhang Huishuai, 2016, ARXIV160303805
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702066
DA 2019-06-15
ER

PT S
AU Zhang, P
AF Zhang, Pan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Robust Spectral Detection of Global Structures in the Data by Learning a
   Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.
C1 [Zhang, Pan] Chinese Acad Sci, Inst Theoret Phys, Beijing 100190, Peoples R China.
RP Zhang, P (reprint author), Chinese Acad Sci, Inst Theoret Phys, Beijing 100190, Peoples R China.
EM panzhang@itp.ac.cn
CR Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138
   Bell R. J., 1970, DISCUSS FARADAY SOC, V50, P55, DOI DOI 10.1039/DF9705000055
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Hashimoto K., 1989, ADV STUD PURE MATH, V15, P211
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113
   Johnson  S.G., 2014, NLOPT NONLINEAR OPTI
   Joseph A, 2013, ARXIV13121733
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   Keshavan RH, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1- 4, P324, DOI 10.1109/ISIT.2009.5205567
   Keshavan RH, 2009, ANN ALLERTON CONF, P1216, DOI 10.1109/ALLERTON.2009.5394534
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Le C. M., 2015, ARXIV150203049
   Le Can M., 2015, ARXIV150600669
   Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274
   Luxburg U. V., 2007, STAT COMPUT
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   Mossel E, 2012, ARXIV12021499
   Ng AY, 2002, ADV NEUR IN, V14, P849
   ROHE K., 2013, ADV NEURAL INFORM PR, V26, P3120
   Saade  A., 2014, ADV NEURAL INFORM PR, P406
   Saade A., 2015, ADV NEURAL INFORM PR, V28, P1261
   Saade A, 2016, IEEE INT SYMP INFO, P780, DOI 10.1109/ISIT.2016.7541405
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704016
DA 2019-06-15
ER

PT S
AU Zhang, SZ
   Wu, YH
   Che, T
   Lin, ZH
   Memisevic, R
   Salakhutdinov, R
   Bengio, Y
AF Zhang, Saizheng
   Wu, Yuhuai
   Che, Tong
   Lin, Zhouhan
   Memisevic, Roland
   Salakhutdinov, Ruslan
   Bengio, Yoshua
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Architectural Complexity Measures of Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID LONG-TERM DEPENDENCIES
AB In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.
C1 [Zhang, Saizheng; Lin, Zhouhan; Memisevic, Roland; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada.
   [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Che, Tong] Inst Hautes Etud Sci, Bures Sur Yvette, France.
   [Memisevic, Roland; Salakhutdinov, Ruslan; Bengio, Yoshua] CIFAR, Toronto, ON, Canada.
RP Zhang, SZ (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.
FU NSERC; Canada Research Chairs; CIFAR; Calcul Quebec; Compute Canada;
   Samsung; ONR Grant [N000141310721, N000141512791]; IARPA Raytheon BBN
   Contract [D11PC20071]
FX The authors acknowledge the following agencies for funding and support:
   NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada,
   Samsung, ONR Grant N000141310721, ONR Grant N000141512791 and IARPA
   Raytheon BBN Contract No. D11PC20071. The authors thank the developers
   of Theano [28] and Keras [29], and also thank Nicolas Ballas, Tim
   Cooijmans, Ryan Lowe, Mohammad Pezeshki, Roger Grosse and Alex Schwing
   for their insightful comments.
CR Al-Rfou R., 2016, ARXIV160502688
   Arjovsky Martin, 2015, ARXIV151106464
   Bahdanau D., 2014, ARXIV14090473
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chollet F., 2015, KERAS GITHUB REPOSIT
   ElHihi S, 1996, ADV NEUR IN, V8, P493
   Graves A, 2013, ARXIV13080850
   Greff K, 2015, ARXIV150304069
   Hermans M., 2013, ADV NEURAL INFORM PR, P190
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1991, THESIS
   Jozefowicz R., 2015, P 32 INT C MACH LEAR, V37, P2342, DOI DOI 10.1109/CVPR.2015.72987
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kiros Ryan, 2015, NIPS
   Koutnik J., 2014, P 31 INT C MACH LEAR, P1863
   Krueger D., 2015, ARXIV151108400
   Le Q. V., 2015, ARXIV150400941
   Lin TN, 1996, IEEE T NEURAL NETWOR, V7, P1329, DOI 10.1109/72.548162
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   Mikolov Tomas, 2012, PREPRINT
   Pascanu R., 2013, ARXIV13126026
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Raiko T., 2012, P INT C ART INT STAT, V22, P924
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Srivastava N, 2015, ICML
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Sutskever I, 2010, NEURAL NETWORKS, V23, P239, DOI 10.1016/j.neunet.2009.10.009
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702050
DA 2019-06-15
ER

PT S
AU Zhang, WH
   Wang, H
   Wong, KYM
   Wu, S
AF Zhang, Wen-Hao
   Wang, He
   Wong, K. Y. Michael
   Wu, Si
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI "Congruent" and "Opposite" Neurons: Sisters for Multisensory Integration
   and Segregation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BAYESIAN-INFERENCE; INFORMATION; REPRESENTATION; PERCEPTION; MOTION;
   CORTEX; AREA
AB Experiments reveal that in the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas, where visual and vestibular cues are integrated to infer heading direction, there are two types of neurons with roughly the same number. One is "congruent" cells, whose preferred heading directions are similar in response to visual and vestibular cues; and the other is "opposite" cells, whose preferred heading directions are nearly "opposite" (with an offset of 180 degrees) in response to visual vs. vestibular cues. Congruent neurons are known to be responsible for cue integration, but the computational role of opposite neurons remains largely unknown. Here, we propose that opposite neurons may serve to encode the disparity information between cues necessary for multisensory segregation. We build a computational model composed of two reciprocally coupled modules, MSTd and VIP, and each module consists of groups of congruent and opposite neurons. In the model, congruent neurons in two modules are reciprocally connected with each other in the congruent manner, whereas opposite neurons are reciprocally connected in the opposite manner. Mimicking the experimental protocol, our model reproduces the characteristics of congruent and opposite neurons, and demonstrates that in each module, the sisters of congruent and opposite neurons can jointly achieve optimal multisensory information integration and segregation. This study sheds light on our understanding of how the brain implements optimal multisensory integration and segregation concurrently in a distributed manner.
C1 [Zhang, Wen-Hao; Wang, He; Wong, K. Y. Michael] Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China.
   [Zhang, Wen-Hao; Wu, Si] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing, Peoples R China.
   [Zhang, Wen-Hao; Wu, Si] Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing, Peoples R China.
   [Zhang, Wen-Hao] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
RP Zhang, WH (reprint author), Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China.; Zhang, WH (reprint author), Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing, Peoples R China.; Zhang, WH (reprint author), Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing, Peoples R China.; Zhang, WH (reprint author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
EM wenhaoz@ust.hk; hwangaa@connect.ust.hk; phkywong@ust.hk; wusi@bnu.edu.cn
FU Research Grants Council of Hong Kong [N_HKUST606/12, 605813]; National
   Basic Research Program of China [2014CB846101]; Natural Science
   Foundation of China [31261160495]
FX This work is supported by the Research Grants Council of Hong Kong
   (N_HKUST606/12 and 605813) and National Basic Research Program of China
   (2014CB846101) and the Natural Science Foundation of China
   (31261160495).
CR Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029
   BAIZER JS, 1991, J NEUROSCI, V11, P168
   BOUSSAOUD D, 1990, J COMP NEUROL, V296, P462, DOI 10.1002/cne.902960311
   Bresciani JP, 2006, J VISION, V6, P554, DOI 10.1167/6.5.2
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Chen AH, 2013, J NEUROSCI, V33, P3567, DOI 10.1523/JNEUROSCI.4522-12.2013
   Chen AH, 2011, J NEUROSCI, V31, P12036, DOI 10.1523/JNEUROSCI.0395-11.2011
   Engel TA, 2011, J NEUROSCI, V31, P6982, DOI 10.1523/JNEUROSCI.6150-10.2011
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Girshick AR, 2009, J VISION, V9, DOI 10.1167/9.9.8
   Gu Y, 2006, J NEUROSCI, V26, P73, DOI 10.1523/JNEUROSCI.2356-05.2006
   Gu Y, 2008, NAT NEUROSCI, V11, P1201, DOI 10.1038/nn.2191
   Gu Y, 2012, J NEUROSCI, V32, P2299, DOI 10.1523/JNEUROSCI.5154-11.2012
   Jazayeri M, 2006, NAT NEUROSCI, V9, P690, DOI 10.1038/nn1691
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Morgan ML, 2008, NEURON, V59, P662, DOI 10.1016/j.neuron.2008.06.024
   Murray RF, 2010, J VISION, V10, DOI 10.1167/10.11.15
   Ohshiro T, 2011, NAT NEUROSCI, V14, P775, DOI 10.1038/nn.2815
   Roach NW, 2006, P ROY SOC B-BIOL SCI, V273, P2159, DOI 10.1098/rspb.2006.3578
   Sato Y, 2007, NEURAL COMPUT, V19, P3335, DOI 10.1162/neco.2007.19.12.3335
   Wallace MT, 2004, EXP BRAIN RES, V158, P252, DOI 10.1007/s00221-004-1899-9
   Zhang W., 2013, ADV NEURAL INFORM PR, P19
   Zhang WH, 2016, J NEUROSCI, V36, P532, DOI 10.1523/JNEUROSCI.0578-15.2016
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702064
DA 2019-06-15
ER

PT S
AU Zhang, YZ
   Wang, XY
   Chen, CY
   Henao, R
   Fan, K
   Carin, L
AF Zhang, Yizhe
   Wang, Xiangyu
   Chen, Changyou
   Henao, Ricardo
   Fan, Kai
   Carin, Lawrence
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Towards Unifying Hamiltonian Monte Carlo and Slice Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, demonstrating their connection via the Hamiltonian-Jacobi equation from Hamiltonian mechanics. This insight enables extension of HMC and slice sampling to a broader family of samplers, called Monomial Gamma Samplers (MGS). We provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the MGS draws decorrelated samples from the desired target distribution. We further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues. Our theoretical results are validated with synthetic data and real-world applications.
C1 [Zhang, Yizhe; Wang, Xiangyu; Chen, Changyou; Henao, Ricardo; Fan, Kai; Carin, Lawrence] Duke Univ, Durham, NC 27708 USA.
RP Zhang, YZ (reprint author), Duke Univ, Durham, NC 27708 USA.
EM yz196@duke.edu; xw56@duke.edu; changyou.chen@duke.edu;
   ricardo.henao@duke.edu; kf96@duke.edu; lcarin@duke.edu
CR Arnol'd V. I., 2013, MATH METHODS CLASSIC, V60
   BACHE K., 2013, UCI MACHINE LEARNING
   Betancourt Michael, 2014, OPTIMIZING INTEGRATO
   Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Cances Eric, 2007, ESAIM MATH MODELLING, V41
   Chao Wei- Lun, 2015, ICML
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Ekeland Ivar, 1980, ANN MATH
   Girolami Mark, 2011, J ROYAL STAT SOC B, V73
   Goldstein H., 1965, CLASSICAL MECH
   Homan Matthew D, 2014, J MACHINE LEARNING R, V15
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Isaac Richard, 1963, ANN MATH STAT
   Jiang Chengxiang, 2015, J APPL ANAL COMPUTAT, V5
   Johnson Alicia A, 2009, THESIS
   Korattikara Anoop, 2013, AUSTERITY MCMC LAND
   Landau L. D., 1976, MECHANICS
   Livingstone Samuel, 2016, GEOMETRIC ERGODICITY
   Murray I, 2009, ELLIPTICAL SLICE SAM
   Nadarajah S, 2005, J APPL STAT, V32, P685, DOI 10.1080/02664760500079464
   Neal Radford M, 2003, ANN STAT
   Neal RM, 2011, HDB MARKOV CHAIN MON, V2
   Pakman Ari, 2013, NIPS
   Robert C. P., 2004, MONTE CARLO STAT MET
   Roberts Gareth O, 1998, CANADIAN J STAT, V26
   Roberts Gareth O, 1996, BERNOULLI
   Rosenthal Jeffrey S, 1995, J AM STAT ASS, V90
   Salimans Tim, 2014, MARKOV CHAIN MONTE C
   Striebel Michael, 2011, ACCURACY SYMMETRIC P, V12
   Taylor J. R., 2005, CLASSICAL MECH
   Tierney L, 1999, STAT MED, V18, P2507, DOI 10.1002/(SICI)1097-0258(19990915/30)18:17/18<2507::AID-SIM272>3.0.CO;2-J
   Van Der Putten Peter, 2000, SENTIENT MACHINE RES, V9
   Vigario Ricardo, 1998, NIPS
   Wang Ziyu, 2013, ICML
   Zhang Yichuan, 2012, NIPS
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701004
DA 2019-06-15
ER

PT S
AU Zhao, H
   Poupart, P
   Gordon, G
AF Zhao, Han
   Poupart, Pascal
   Gordon, Geoff
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Unified Approach for Learning the Parameters of Sum-Product Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.
C1 [Zhao, Han; Gordon, Geoff] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Poupart, Pascal] Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada.
RP Zhao, H (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM han.zhao@cs.cmu.edu; ppoupart@uwaterloo.ca; ggordon@cs.cmu.edu
FU ONR [N000141512365]
FX HZ and GG gratefully acknowledge support from ONR contract
   N000141512365. HZ also thanks Ryan Tibshirani for the helpful discussion
   about CCCP.
CR Boyd S, 2007, OPTIM ENG, V8, P67, DOI 10.1007/s11081-007-9001-7
   Chan H., P 22 C UNC ART INT
   Chiang M., 2005, GEOMETRIC PROGRAMMIN
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Dennis A., 2015, INT JOINT C ART INT, V24
   Gens R., 2013, P 30 INT C MACH LEAR, V28, P873
   Gens R., 2012, NIPS, P3248
   Gunawardana A, 2005, J MACH LEARN RES, V6, P2049
   Hartman P., 1959, PAC J MATH, V9, P707, DOI 10.2140/pjm.1959.9.707
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Lanckriet G. R., 2009, CONVERGENCE CONCAVE, P1759
   Peharz R., 2015, THESIS
   Peharz R., 2015, AISTATS
   Poon H., 2011, P 12 C UNC ART INT, P2551
   Rooshenas A., 2014, ICML
   Salakhutdinov R., 2002, UAI
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Yuille AL, 2002, ADV NEUR IN, V14, P1033
   Zangwill W. I., 1969, NONLINEAR PROGRAMMIN, V196
   Zhao H., 2015, ICML
   Zhao Han, 2016, ICML
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703059
DA 2019-06-15
ER

PT S
AU Zhao, SJ
   Zhou, EZ
   Sabharwal, A
   Ermon, S
AF Zhao, Shengjia
   Zhou, Enze
   Sabharwal, Ashish
   Ermon, Stefano
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Concentration Inequalities for Sequential Decision Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees. We introduce Hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples. Our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems. In particular, we apply them to sequential hypothesis testing, best arm identification, and sorting. The resulting algorithms rival or exceed the state of the art both theoretically and empirically.
C1 [Zhao, Shengjia; Zhou, Enze] Tsinghua Univ, Beijing, Peoples R China.
   [Sabharwal, Ashish] Allen Inst AI, Seattle, WA USA.
   [Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA.
RP Zhao, SJ (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM zhaosj12@stanford.edu; zhouez_thu_12@126.com; AshishS@allenai.org;
   ermon@cs.stanford.edu
FU NSF [1649208]; Future of Life Institute [2016-158687]
FX This research was supported by NSF (#1649208) and Future of Life
   Institute (#2016-158687).
CR Auer  Peter, 2002, FINITE TIME ANAL MUL
   Balsubramani  A., 2014, SHARP FINITE TIME IT
   Balsubramani  A., 2015, SEQUENTIAL NONPARAME
   Breiman L., 1992, PROBABILITY
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chen  Lijie, 2015, ABS151103774 CORR
   Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255
   Even-Dar  Eyal, 2006, J MACHINE LEARNING R
   Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115
   FARRELL RH, 1964, ANN MATH STAT, V35, P36, DOI 10.1214/aoms/1177703731
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Hoeffding  Wassily, 1963, J AM STAT ASS
   Jamieson  Kevin, 2014, J MACHINE LEARNING R
   Jamieson  Kevin, 2013, FINDING LARGEST MEAN
   Jamieson  Kevin, 2014, BEST ARM IDENTIFICAT
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Karnin Z., 2013, P 30 INT C MACH LEAR, V28, P1238
   Karp RM, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P881
   Mnih V, 2008, P 25 INT C MACH LEAR, P672
   Rivasplata  Omar, 2012, SUBGAUSSIAN RANDOM V
   Sen Pranab K., 1993, LARGE SAMPLE METHODS
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704018
DA 2019-06-15
ER

PT S
AU Zhao, SN
   Rudzicz, F
AF Zhao, Shunan
   Rudzicz, Frank
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Combining Different Modalities in Classifying Phonological Categories
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
DE Phonological categories; Electroencephalography; Speech articulation;
   Deep-belief networks
ID SPEECH
AB This paper concerns a new dataset we are collecting combining 3 modalities (EEG, video of the face, and audio) during imagined and vocalized phonemic and single-word prompts. We pre-process the EEG data, compute features for all 3 modalities, and perform binary classification of phonological categories using a combination of these modalities. For example, a deep-belief network obtains accuracies over 90% on identifying consonants, which is significantly more accurate than two baseline support vector machines. These data may be used generally by the research community to learn multimodal relationships, and to develop silent-speech and brain-computer interfaces.
C1 [Zhao, Shunan; Rudzicz, Frank] Univ Toronto, Toronto, ON, Canada.
   [Rudzicz, Frank] Univ Toronto, Toronto Rehabil Inst, Toronto, ON, Canada.
RP Zhao, SN (reprint author), Univ Toronto, Toronto, ON, Canada.
EM frank@cs.toronto.edu
CR Bartels J, 2008, J NEUROSCI METH, V174, P168, DOI 10.1016/j.jneumeth.2008.06.030
   Blakely T, 2008, IEEE ENG MED BIO, P4964, DOI 10.1109/IEMBS.2008.4650328
   Brigham K., 2010, 2010 4 IEEE INT C BI, P1
   Callan DE, 2000, COGNITIVE BRAIN RES, V10, P173, DOI 10.1016/S0926-6410(00)00025-2
   D'Zmura M, 2009, LECT NOTES COMPUT SC, V5610, P40, DOI 10.1007/978-3-642-02574-7_5
   DaSalla CS, 2009, NEURAL NETWORKS, V22, P1334, DOI 10.1016/j.neunet.2009.05.008
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Fujimaki N., 1994, Brain Topography, V6, P259, DOI 10.1007/BF01211171
   Gomez- Herrero G., 2006, P 7 NORD SIGN PROC S, P130
   Kellis S, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/5/056007
   KENT RD, 1989, J SPEECH HEAR DISORD, V54, P482, DOI 10.1044/jshd.5404.482
   Pasley B.N., 2012, PLOS ONE, V10, P1
   Porbadnigk A, 2009, BIOSIGNALS 2009: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON BIO-INSPIRED SYSTEMS AND SIGNAL PROCESSING, P376
   SHARBROUGH F, 1991, J CLIN NEUROPHYSIOL, V8, P200, DOI DOI 10.1097/00004691-199104000-00007
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suppes P, 1997, P NATL ACAD SCI USA, V94, P14965, DOI 10.1073/pnas.94.26.14965
   Zhao S., 2015, P ICASSP 2015
NR 17
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 40
EP 48
DI 10.1007/978-3-319-45174-9_5
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400005
DA 2019-06-15
ER

PT S
AU Zhao, Y
   Park, IM
AF Zhao, Yuan
   Park, Il Memming
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Interpretable Nonlinear Dynamic Modeling of Neural Trajectories
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIMENSIONAL DYNAMICS; COMPUTATION; CHAOS
AB A central challenge in neuroscience is understanding how neural system implements computation through its dynamics. We propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories. Our model assumes low-dimensional continuous dynamics in a finite volume. It incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories. We show that our model can recover qualitative features of the phase portrait such as attractors, slow points, and bifurcations, while also producing reliable long-term future predictions in a variety of dynamical models and in real neural data.
C1 [Zhao, Yuan; Park, Il Memming] SUNY Stony Brook, Dept Appl Math & Stat, Inst Adv Computat Sci, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA.
RP Zhao, Y (reprint author), SUNY Stony Brook, Dept Appl Math & Stat, Inst Adv Computat Sci, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA.
EM yuan.zhao@stonybrook.edu; memming.park@stonybrook.edu
FU Thomas Hartman Foundation for Parkinson's Research
FX We thank the reviewers for their constructive feedback. This work was
   partially supported by the Thomas Hartman Foundation for Parkinson's
   Research.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Barak O, 2013, PROG NEUROBIOL, V103, P214, DOI 10.1016/j.pneurobio.2013.02.002
   CHEN S, 1990, INT J CONTROL, V52, P1327, DOI 10.1080/00207179008953599
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Curto C, 2009, J NEUROSCI, V29, P10600, DOI 10.1523/JNEUROSCI.2053-09.2009
   Daniels BC, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9133
   Eikenberry SE, 2013, J COMPUT NEUROSCI, V34, P163, DOI 10.1007/s10827-012-0412-x
   Gan M, 2010, INFORM SCIENCES, V180, P4370, DOI 10.1016/j.ins.2010.07.012
   Ganguli S, 2008, NEURON, V58, P15, DOI 10.1016/j.neuron.2008.01.038
   Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012
   HANSEL D, 1992, PHYS REV LETT, V68, P718, DOI 10.1103/PhysRevLett.68.718
   Izhikevich E.M., 2007, COMPUTATIONAL NEUROS
   Kantz  H., 2003, NONLINEAR TIME SERIE
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Machens CK, 2005, SCIENCE, V307, P1121, DOI 10.1126/science.1104171
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Mazurek ME, 2003, CEREB CORTEX, V13, P1257, DOI 10.1093/cercor/bhg097
   Ozaki T., 2012, TIME SERIES MODELING
   Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x
   Peyrache A, 2015, NAT NEUROSCI, V18, P569, DOI 10.1038/nn.3968
   Rabinovich MI, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000072
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Wong KF, 2006, J NEUROSCI, V26, P1314, DOI 10.1523/JNEUROSCI.3733-05.2006
   Zhao Y., 2016, ARXIV E PRINTS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704067
DA 2019-06-15
ER

PT S
AU Zhe, S
   Zhang, K
   Wang, PY
   Lee, KC
   Xu, ZL
   Qi, Y
   Gharamani, Z
AF Zhe, Shandian
   Zhang, Kai
   Wang, Pengyuan
   Lee, Kuang-chih
   Xu, Zenglin
   Qi, Yuan
   Gharamani, Zoubin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Distributed Flexible Nonlinear Tensor Factorization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Tensor factorization is a powerful tool to analyse multi-way data. Recently proposed nonlinear factorization methods, although capable of capturing complex relationships, are computationally quite expensive and may suffer a severe learning bias in case of extreme data sparsity. Therefore, we propose a distributed, flexible nonlinear tensor factorization model, which avoids the expensive computations and structural restrictions of the Kronecker-product in the existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected for training. Meanwhile, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed, key-value-free inference algorithm in the MAPREDUCE framework, which can fully exploit the memory cache mechanism in fast MAPREDUCE systems such as SPARK. Experiments demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency.
C1 [Zhe, Shandian] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Zhang, Kai] NEC Labs Amer, Princeton, NJ USA.
   [Wang, Pengyuan] Univ Georgia Athens, Dept Mkt, Athens, GA USA.
   [Lee, Kuang-chih] Yahoo Res, Sunnyvale, CA USA.
   [Xu, Zenglin] Univ Elect Sci & Tech China, Sch Comp Sci Engn, Big Data Res Ctr, Chengdu, Sichuan, Peoples R China.
   [Qi, Yuan] Alibaba, Ant Financial Serv Grp, Hangzhou, Zhejiang, Peoples R China.
   [Gharamani, Zoubin] Univ Cambridge, Cambridge, England.
RP Zhe, S (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM szhe@purdue.edu; kzhang@nec-labs.com; pengyuan@uga.edu;
   kclee@yahoo-inc.com; zlxu@uestc.edu.cn; alanqi0@outlook.com;
   zoubin@cam.ac.uk
FU NSF China [61572111]
FX Dr. Zenglin Xu was supported by a grant from NSF China under No.
   61572111. We thank IBM T.J. Watson Research Center for providing one
   dataset. We also thank Jiasen Yang for proofreading this paper.
CR Choi J. H., 2014, NIPS
   Chu W., 2009, AISTATS
   Davidson A., 2013, OPTIMIZING SHUFFLE P
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Gal Y., 2014, NIPS
   Harshman R. A., 1970, UCLA WORKING PAPERS, V16, P1
   Hoff P., 2011, COMPUTATIONAL STAT D
   Hu C., 2015, UAI
   Kang U., 2012, KDD
   Lawrence N. D., 2004, NIPS
   Lloyd J. R., 2012, NIPS
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rai P., 2015, IJCAI
   Rai P, 2014, ICML
   Shashua A., 2005, ICML
   Sutskever I., 2009, NIPS
   Titsias M., 2009, AISTATS
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Xu Z., 2012, ICML
   Yang Y, 2016, J AM STAT ASSOC, V111, P656, DOI 10.1080/01621459.2015.1029129
   Zaharia M., 2012, NSDI
   Zhe S., 2015, AISTATS
   Zhe S., 2016, AAAI
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704109
DA 2019-06-15
ER

PT S
AU Zheng, S
   Yue, YS
   Lucey, P
AF Zheng, Stephan
   Yue, Yisong
   Lucey, Patrick
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generating Long-term Trajectories Using Deep Hierarchical Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior. The key difficulty is that conventional models are "single-scale" and only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.
C1 [Zheng, Stephan; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA.
   [Lucey, Patrick] STATS, Chicago, IL USA.
RP Zheng, S (reprint author), CALTECH, Pasadena, CA 91125 USA.
EM stzheng@caltech.edu; yyue@caltech.edu; plucey@stats.com
FU NSF [1564330]
FX This research was supported in part by NSF Award #1564330, and a GPU
   donation (Tesla K40 and Titan X) by NVIDIA.
CR Bai AJ, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2717316
   Byrne RW, 1998, BEHAV BRAIN SCI, V21, P667, DOI 10.1017/S0140525X98001745
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Chung J., 2015, INT C MACH LEARN, p2067 , DOI DOI 10.1145/2661829.2661935
   Evans JST, 2013, PERSPECT PSYCHOL SCI, V8, P223, DOI 10.1177/1745691612460685
   Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000
   Hausknecht Matthew, P INT C LEARN REPR I
   He Ruijie, 2010, 24 AAAI C ART INT JU
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Konidaris G, 2012, INT J ROBOT RES, V31, P360, DOI 10.1177/0278364911428653
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Muelling K, 2014, BIOL CYBERN, V108, P603, DOI 10.1007/s00422-014-0599-1
   Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Xu K, 2015, ARXIV150203044
   Yue Yisong, IEEE INT C DAT MIN I
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 17
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704045
DA 2019-06-15
ER

PT S
AU Zhong, K
   Jain, P
   Dhillon, IS
AF Zhong, Kai
   Jain, Prateek
   Dhillon, Inderjit S.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Mixed Linear Regression with Multiple Components
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MIXTURES
AB In this paper, we study the mixed linear regression (MLR) problem, where the goal is to recover multiple underlying linear models from their unlabeled linear measurements. We propose a non-convex objective function which we show is locally strongly convex in the neighborhood of the ground truth. We use a tensor method for initialization so that the initial models are in the local strong convexity region. We then employ general convex optimization algorithms to minimize the objective function. To the best of our knowledge, our approach provides first exact recovery guarantees for the MLR problem with K >= 2 components. Moreover, our method has near-optimal computational complexity (O) over tilde (N d) as well as near-optimal sample complexity (O) over tilde (d) for constant K. Furthermore, we show that our non convex formulation can be extended to solving the subspace clustering problem as well. In particular, when initialized within a small constant distance to the true subspaces, our method converges to the global optima (and recovers true subspaces) in time linear in the number of points. Furthermore, our empirical results indicate that even with random initialization, our approach converges to the global optima in linear time, providing speed-up of up to two orders of magnitude.
C1 [Zhong, Kai; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
   [Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India.
RP Zhong, K (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM zhongkai@ices.utexas.edu; prajain@microsoft.com; inderjit@cs.utexas.edu
FU NSF [CCF-1320746, IIS-1546459, CCF-1564000]
FX This research was supported by NSF grants CCF-1320746, IIS-1546459 and
   CCF-1564000.
CR Adler A, 2015, IEEE T NEUR NET LEAR, V26, P2234, DOI 10.1109/TNNLS.2014.2374631
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arbenz Peter, LECT NOTES SOLVING L
   Balakrishnan Sivaraman, 2015, ANN STAT
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Carvalho AX, 2005, IEEE T NEURAL NETWOR, V16, P39, DOI 10.1109/TNN.2004.839356
   Chaganty A. T., 2013, ICML, V28, P1040
   Chai XJ, 2007, IEEE T IMAGE PROCESS, V16, P1716, DOI 10.1109/TIP.2007.899195
   Chen Yudong, 2014, COLT
   DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001
   Deb P, 2000, HEALTH ECON, V9, P475, DOI 10.1002/1099-1050(200009)9:6<475::AID-HEC544>3.0.CO;2-H
   Dyer EL, 2013, J MACH LEARN RES, V14, P2487
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Ferrari-Trecate G, 2002, LECT NOTES COMPUT SC, V2415, P444
   Gaffney Scott, 1999, KDD
   Hamm J., 2008, P 25 INT C MACH LEAR, P376
   Heckel R, 2013, INT CONF ACOUST SPEE, P3263, DOI 10.1109/ICASSP.2013.6638261
   Ho J, 2003, PROC CVPR IEEE, P11
   Hsu D., 2012, ELECTRON COMMUN PROB, V17, P6
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   Jie Shen, 2016, ICML
   Khalili Abbas, 2012, J AM STAT ASS
   Liu G., 2010, P INT C MACH LEARN, V27, P663
   Park  D., 2014, ADV NEURAL INFORM PR, P2753
   Sedghi Hanie, 2014, ARXIV14123046
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Viele K, 2002, STAT COMPUT, V12, P315, DOI 10.1023/A:1020779827503
   VIETH E, 1989, J APPL PHYSIOL, V67, P390
   Yi X., 2014, P 31 INT C MACH LEAR, P613
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701097
DA 2019-06-15
ER

PT S
AU Zhou, HH
   Ravi, SN
   Ithapu, VK
   Johnson, SC
   Wahba, G
   Singh, V
AF Zhou, Hao Henry
   Ravi, Sathya N.
   Ithapu, Vamsi K.
   Johnson, Sterling C.
   Wahba, Grace
   Singh, Vikas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hypothesis Testing in Unsupervised Domain Adaptation with Applications
   in Alzheimer's Disease
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Consider samples from two different data sources {X-s(i)} similar to P-source and {X-t(i)} similar to Ptarget. We only observe their transformed versions h (xi s) and g (X-t(i)), for some known function class h (center dot) and g (center dot). Our goal is to perform a statistical test checking if P-source = Ptarget while removing the distortions induced by the transformations. This problem is closely related to domain adaptation, and in our case, is motivated by the need to combine clinical and imaging based biomarkers from multiple sites and/or batches - a fairly common impediment in conducting analyses with much larger sample sizes. We address this problem using ideas from hypothesis testing on the transformed measurements, wherein the distortions need to be estimated in tandem with the testing. We derive a simple algorithm and study its convergence and consistency properties in detail, and provide lower-bound strategies based on recent work in continuous optimization. On a dataset of individuals at risk for Alzheimer's disease, our framework is competitive with alternative procedures that are twice as expensive and in some cases operationally infeasible to implement.
C1 [Johnson, Sterling C.] William S Middleton Mem VA Hosp, Shorewood Hills, WI USA.
   [Zhou, Hao Henry; Ravi, Sathya N.; Ithapu, Vamsi K.; Johnson, Sterling C.; Wahba, Grace; Singh, Vikas] Univ Wisconsin, Madison, WI 53706 USA.
RP Zhou, HH (reprint author), Univ Wisconsin, Madison, WI 53706 USA.
FU NIH [AG040396, U54AI117924]; NSF [DMS1308847]; NSF CAREER [1252725]; NSF
   CCF [1320755]; UW CPCP [AI117924]; UW ADRC [AG033514]; UW ICTR
   [1UL1RR025011]; Stay Sharp fund
FX This work is supported by NIH AG040396, NIH U54AI117924, NSF DMS1308847,
   NSF CAREER 1252725, NSF CCF 1320755 and UW CPCP AI117924. The authors
   are grateful for partial support from UW ADRC AG033514 and UW ICTR
   1UL1RR025011. We thank Marilyn S. Albert (Johns Hopkins) and Anne Fagan
   (Washington University at St. Louis) for discussions at a preclinical
   Alzheimer's disease meeting in 2015 (supported by Stay Sharp fund).
CR Baktashmotlagh M, 2013, P IEEE ICCV
   Ben- David S., 2010, MACHINE LEARNING
   Chalise B, 2011, IEEE CAMSAP
   Chandrasekaran V, 2014, ARXIV14097640
   Cortes C, 2011, ALGORITHMIC LEARNING
   Dall T, 2013, HLTH AFFAIRS
   Daume III H., 2010, P 2010 WORKSH DOM AD
   Dollar P, 2009, CVPR
   Fernando B, 2013, P IEEE ICCV
   Ganin Y., 2014, ARXIV14097495
   Glioma Meta-analysis Trialists GMT Group, 2002, LANCET
   Gong B., 2012, CVPR
   Gong B., 2013, ICML
   Gong Boqing, 2015, THESIS
   Gopalan R, 2011, P IEEE ICCV
   Gretton A., 2012, JMLR
   Gretton A, 2009, NIPS
   Haase M, 2009, AM J KIDNEY DIS
   Huang J., 2006, NIPS
   Klunk W, 2015, ALZHEIMERS DEMENTI S
   Kumar A, 2010, NIPS
   Lacoste- Julien S, 2012, ARXIV12074747
   Nguyen X, 2010, INFORM THEORY IEEE T
   Pan S, 2011, NEURAL NETWORKS IEEE
   Patel V, 2015, SIGNAL PROCESSING MA
   Plassman B, 2007, NEUROEPIDEMIOLOGY
   Qi Li, 2012, LIT SURVEY DOMAIN AD
   Saenko K., 2010, ECCV
   Sejdinovic D, 2013, ANN STAT
   Sriperumbudur B. K., 2009, NIPS
   Torralba A., 2011, CVPR
   Tuncel L, 2010, POLYHEDRAL SEMIDEFIN
   Vanderstichele H, 2012, ALZHEIMERS DEMENTI S
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701066
DA 2019-06-15
ER

PT S
AU Zhou, YX
   Spanos, CJ
AF Zhou, Yuxun
   Spanos, Costas J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Causal meets Submodular: Subset Selection with Directed Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study causal subset selection with Directed Information as the measure of prediction causality. Two typical tasks, causal sensor placement and covariate selection, are correspondingly formulated into cardinality constrained directed information maximizations. To attack the NP-hard problems, we show that the first problem is submodular while not necessarily monotonic. And the second one is "nearly" submodular. To substantiate the idea of approximate submodularity, we introduce a novel quantity, namely submodularity index (SmI), for general set functions. Moreover, we show that based on SmI, greedy algorithm has performance guarantee for the maximization of possibly non-monotonic and non-submodular functions, justifying its usage for a much broader class of problems. We evaluate the theoretical results with several case studies, and also illustrate the application of the subset selection to causal structure learning.
C1 [Zhou, Yuxun; Spanos, Costas J.] UC Berekely, Dept EECS, Berkeley, CA 94720 USA.
RP Zhou, YX (reprint author), UC Berekely, Dept EECS, Berkeley, CA 94720 USA.
EM yxzhou@berkeley.edu; spanos@berkeley.edu
FU Republic of Singapore's National Research Foundation
FX This research is funded by the Republic of Singapore's National Research
   Foundation through a grant to the Berkeley Education Alliance for
   Research in Singapore (BEARS) for the Singapore-Berkeley Building
   Efficiency and Sustainability in the Tropics (SinBerBEST) Program. BEARS
   has been established by the University of California, Berkeley as a
   center for intellectual excellence in research and education in
   Singapore. We also thank the reviews for their helpful suggestions.
CR Abrams Z, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P424
   Buchbinder J. S. N. Niv, 2014, ACM SIAM S DISCR ALG
   Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862
   Das D. K. Abhimanyu, 2011, P ICML 2011 SEATTL W
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46
   Jiao JT, 2013, IEEE T INFORM THEORY, V59, P6220, DOI 10.1109/TIT.2013.2267934
   Kawahara Y., 2011, P 25 ANN C NEUR INF, P2106
   KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684
   Krause A., 2005, UAI
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Lin  H., 2011, ACL HLT
   Lozano A. N.-M. Y. L. C. P. J. H. N. A. A., 2009, ACM SIGKDD C KNOWL D
   Mathai P., 2007, P INF THEOR APPL WOR, P274
   Murphy K, 2001, COMPUTING SCI STAT, V33, P1024
   Narasimhan M., 2012, ARXIV12071404
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Pearl J, 2009, CAUSALITY MODELS REA
   Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440
   Quinn CJ, 2011, J COMPUT NEUROSCI, V30, P17, DOI 10.1007/s10827-010-0247-2
   Sheehan D. V. B. P. R., 2008, PLOS MED
   Ver Steeg G., 2013, P 6 ACM INT C WEB SE, P3
   Yuxun Zhou, 2013, 2013 IEEE International Conference on Automation Science and Engineering (CASE), P593, DOI 10.1109/CoASE.2013.6654000
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703020
DA 2019-06-15
ER

PT S
AU Zhu, R
AF Zhu, Rong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Gradient-based Sampling: An Adaptive Importance Sampling for
   Least-squares
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MONTE-CARLO ALGORITHMS; APPROXIMATION
AB In modern data analysis, random sampling is an efficient and widely-used strategy to overcome the computational difficulties brought by large sample size. In previous studies, researchers conducted random sampling which is according to the input data but independent on the response variable, however the response variable may also be informative for sampling. In this paper we propose an adaptive sampling called the gradient-based sampling which is dependent on both the input data and the output for fast solving of least-square (LS) problems. We draw the data points by random sampling from the full data according to their gradient values. This sampling is computationally saving, since the running time of computing the sampling probabilities is reduced to O(nd) where n is the full sample size and d is the dimension of the input. Theoretically, we establish an error bound analysis of the general importance sampling with respect to LS solution from full data. The result establishes an improved performance of the use of our gradient-based sampling. Synthetic and real data sets are used to empirically argue that the gradient-based sampling has an obvious advantage over existing sampling methods from two aspects of statistical efficiency and computational saving.
C1 [Zhu, Rong] Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China.
RP Zhu, R (reprint author), Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China.
EM rongzhu@amss.ac.cn
FU National Natural Science Foundation of China [11301514, 71532013]
FX This research was supported by National Natural Science Foundation of
   China grants 11301514 and 71532013. We thank Xiuyuan Cheng for comments
   in a preliminary version.
CR Clarkson D. P., 2013, STOC
   Cohen M. B., 2014, ARXIV14085099
   Dhillon P., 2013, ADV NEURAL INFORM PR, P360
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Drineas P, 2006, SIAM J COMPUT, V36, P184, DOI 10.1137/S0097539704442702
   Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6
   Ma P., 2014, P 31 INT C MACH LEAR
   Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]
   Raskutti  G., 2015, P 32 ICML C
   Sarndal C.-E., 2003, MODEL ASSISTED SURVE
   Shender  D., 2013, P 30 INT C MACH LEAR
   Yang  T., 2015, P 32 ICML C
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704103
DA 2019-06-15
ER

PT S
AU Zhu, YC
   Chatterjee, S
   Duchi, J
   Lafferty, J
AF Zhu, Yuancheng
   Chatterjee, Sabyasachi
   Duchi, John
   Lafferty, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Local Minimax Complexity of Stochastic Convex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its "hardest local alternative" to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations.
C1 [Zhu, Yuancheng] Univ Penn, Wharton Stat Dept, Philadelphia, PA 19104 USA.
   [Chatterjee, Sabyasachi] Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
   [Duchi, John] Stanford Univ, Dept Elect Engn, Dept Stat, Stanford, CA 94305 USA.
   [Lafferty, John] Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA.
RP Zhu, YC (reprint author), Univ Penn, Wharton Stat Dept, Philadelphia, PA 19104 USA.
FU ONR [11896509]; NSF [DMS-1513594]
FX Research supported in part by ONR grant 11896509 and NSF grant
   DMS-1513594. The authors thank Tony Cai, Praneeth Netrapalli, Rob Nowak,
   Aaron Sidford, and Steve Wright for insightful discussions and valuable
   comments on this work.
CR Brown LD, 1996, ANN STAT, V24, P2524
   Cai TT, 2015, STAT SINICA, V25, P423, DOI 10.5705/ss.2013.279
   Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189
   Donoho David, 1987, TECHNICAL REPORT, V137
   DONOHO DL, 1991, ANN STAT, V19, P633, DOI 10.1214/aos/1176348114
   DONOHO DL, 1994, ANN STAT, V22, P238, DOI 10.1214/aos/1176325367
   Jean-Baptiste Hiriart-Urruty, 1993, CONVEX ANAL MINIMIZA
   Juditski A., 2014, STOCH SYST, V4, P44, DOI DOI 10.1287/10-SSY010
   Karp RM, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P881
   Moulines  E., 2011, ADV NEURAL INFORM PR, P451
   NAZIN AV, 1989, AUTOMAT REM CONTR+, V50, P531
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Ramdas Aaditya, 2015, ARXIV150504214
   Ramdas Aaditya, 2013, P 30 INT C MACH LEAR, P365
   Ruppert David, 1988, 781 CORN U OP RES IN, V781
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705015
DA 2019-06-15
ER

PT S
AU Zu, C
   Jie, B
   Chen, SC
   Zhang, DQ
AF Zu, Chen
   Jie, Biao
   Chen, Songcan
   Zhang, Daoqiang
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Label-Alignment-Based Multi-Task Feature Selection for Multimodal
   Classification of Brain Disease
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
DE Alzheimer's disease; Mild cognitive impairment; Label alignment;
   Multi-task learning; Multi-modality
ID MILD COGNITIVE IMPAIRMENT; ALZHEIMERS-DISEASE; PREDICTION; MODEL
AB Recently, multi-task feature selection methods have been applied to jointly identify the disease-related brain regions for fusing information from multiple modalities of neuroimaging data. However, most of those approaches ignore the complementary label information across modalities. To address this issue, in this paper, we present a novel label-alignment-based multi-task feature selection method to jointly select the most discriminative features from multi-modality data. Specifically, the feature selection procedure of each modality is treated as a task and a group sparsity regularizer (i.e., l(2),1 norm) is adopted to ensure that only a small number of features to be selected jointly. In addition, we introduce a new regularization term to preserve label relatedness. The function of the proposed regularization term is to align paired within-class subjects from multiple modalities, i.e., to minimize their distance in corresponding low-dimensional feature space. The experimental results on the magnetic resonance imaging (MRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) data of Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed method can achieve better performances over state-of-the-art methods on multimodal classification of Alzheimer's disease (AD) and mild cognitive impairment (MCI).
C1 [Zu, Chen; Jie, Biao; Chen, Songcan; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Engn, Nanjing 210016, Jiangsu, Peoples R China.
RP Zhang, DQ (reprint author), Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Engn, Nanjing 210016, Jiangsu, Peoples R China.
EM chenzu@nuaa.edu.cn; jbiao@nuaa.edu.cn; s.chen@nuaa.edu.cn;
   dqzhang@nuaa.edu.cn
CR Brookmeyer R, 2007, ALZHEIMERS DEMENT, V3, P186, DOI 10.1016/j.jalz.2007.04.381
   Gray KR, 2013, NEUROIMAGE, V65, P167, DOI 10.1016/j.neuroimage.2012.09.065
   Huang S., 2011, ADV NEURAL INFORM PR, P1431
   Jie B, 2013, LECT NOTES COMPUT SC, V8149, P275, DOI 10.1007/978-3-642-40811-3_35
   Liu F, 2013, LECT NOTES COMPUT SC, V8149, P308, DOI 10.1007/978-3-642-40811-3_39
   Liu Jun, 2009, TECHNICAL REPORT
   McEvoy LK, 2009, RADIOLOGY, V251, P195, DOI 10.1148/radiol.2511080924
   Mosconi L, 2010, J ALZHEIMERS DIS, V20, P843, DOI 10.3233/JAD-2010-091504
   Shen DG, 2002, IEEE T MED IMAGING, V21, P1421, DOI 10.1109/TMI.2002.803111
   Westman E, 2012, NEUROIMAGE, V62, P229, DOI 10.1016/j.neuroimage.2012.04.056
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhang DQ, 2012, NEUROIMAGE, V59, P895, DOI 10.1016/j.neuroimage.2011.09.069
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
NR 14
TC 0
Z9 0
U1 2
U2 7
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 51
EP 59
DI 10.1007/978-3-319-45174-9_6
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400006
DA 2019-06-15
ER

PT S
AU Abbe, E
   Sandon, C
AF Abbe, Emmanuel
   Sandon, Colin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Recovering Communities in the General Stochastic Block Model Without
   Knowing the Parameters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BLOCKMODELS
AB The stochastic block model (SBM) has recently gathered significant attention due to new threshold phenomena. However, most developments rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in Abbe-Sandon '15. In the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees. This lower-bound requirement is removed for the regime of diverging degrees. For the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that simultaneously learns the model parameters, achieves the optimal CH-limit for exact recovery, and runs in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the SBM.
C1 [Abbe, Emmanuel] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
   [Abbe, Emmanuel] Princeton Univ, PACM, Princeton, NJ 08540 USA.
   [Sandon, Colin] Princeton Univ, Dept Math, Princeton, NJ 08540 USA.
RP Abbe, E (reprint author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
EM eabbe@princeton.edu; sandon@princeton.edu
CR Abbe E., 2015, FOCS15
   Abbe E., 2014, IEEE T INFORM THEORY
   Abbe E., 2015, ARXIV150603729
   Alon N., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P346, DOI 10.1145/195058.195187
   Bandeira AS, 2015, ARXIV150403987
   Bhattacharyya S., 2014, ARXIV E PRINTS
   Bickel P. J., 2009, P NATL ACAD SCI
   Bordenave C, 2015, ARXIV150106087
   Borgs C., 2015, PRIVATE GRAPHO UNPUB
   BUI TN, 1987, COMBINATORICA, V7, P171, DOI 10.1007/BF02579448
   Chin  P., 2015, ARXIV150105021
   Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053
   Condon A., 1999, Randomization, Approximation, and Combinatorial Optimization. Algorithms and Techniques. Third International Workshop on Radomization and Approximation Techniques in Computer Science, and Second International Workshop on Approximation Algorithms for Combinatorial Optimization Problems RANDOM-APPROX'99. Proceedings (Lecture Notes in Computer Science Vol.1671), P221
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1
   Gopalan P. K., 2013, P NATL ACAD SCI
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jerrum M, 1998, DISCRETE APPL MATH, V82, P155, DOI 10.1016/S0166-218X(97)00133-9
   Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mossel E., 2014, ARXIV13114115MATHPR
   Mossel E., 2012, ARXIV12021499MATHPR
   Mossel E., 2014, STOC15
   Mossel E., 2013, ARXIVARXIV13091380
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004
   Vershynin R, 2014, ARXIV14114686
   Vu V., 2014, ARXIV14043918
   Xu J., 2014, ARXIV14021267
   Xu J., 2014, ARXIV14126156
   Yun Se-Young, 2014, ARXIV14127335
   Zhang A. Y., 2015, ARXIV150503772
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102055
DA 2019-06-15
ER

PT S
AU Abdolmaleki, A
   Lioutikov, R
   Lau, N
   Reis, LP
   Peters, J
   Neumann, G
AF Abdolmaleki, Abbas
   Lioutikov, Rudolf
   Lau, Nuno
   Reis, Luis Paulo
   Peters, Jan
   Neumann, Gerhard
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Model-Based Relative Entropy Stochastic Search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the 'distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task. The proposed method considerably outperforms the existing approaches.
C1 [Abdolmaleki, Abbas; Lau, Nuno] Univ Aveiro, IEETA, Aveiro, Portugal.
   [Abdolmaleki, Abbas; Reis, Luis Paulo] Univ Minho, DSI, Braga, Portugal.
   [Abdolmaleki, Abbas; Reis, Luis Paulo] Univ Porto, LIACC, Porto, Portugal.
   [Lioutikov, Rudolf; Peters, Jan] Tech Univ Darmstadt, IAS, Darmstadt, Germany.
   [Neumann, Gerhard] Tech Univ Darmstadt, CLAS, Darmstadt, Germany.
   [Peters, Jan] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
RP Abdolmaleki, A (reprint author), Univ Aveiro, IEETA, Aveiro, Portugal.
EM abbas.a@ua.pt; Lioutikov@ias.tu-darmstadt.de; nunolau@ua.pt;
   lpreis@dsi.uminho.pt; peters@ias.tu-darmstadt.de;
   neumann@ias.tu-darmstadt.de
FU European Unions Horizon 2020 research and innovation programme [645582];
   FCT [SFRH/BD/81155/2011]
FX This project has received funding from the European Unions Horizon 2020
   research and innovation programme under grant agreement No #645582
   (RoMaNS) and the first author is supported by FCT under grant
   SFRH/BD/81155/2011.
CR Amari Shun-Ichi, 1998, NEURAL COMPUTATION
   Boyd S., 2004, CONVEX OPTIMIZATION
   Furmston T., 2012, NEURAL INFORM PROCES
   Hansen N, 2003, REDUCING TIME COMPLE
   Ijspeert A., 2003, ADV NEURAL INFORM PR, V15
   Jolliffe I., 1986, PRINCIPAL COMPONENT
   Kober J., 2010, MACH LEARN, P1
   Kupcsik A., 2013, P NAT C ART INT AAAI
   Loshchilov I., 2013, GECCO
   Loshchilov I., 2013, CORR
   Mannor S., 2003, P 20 INT C MACH LEAR
   Mehmet G., 2013, IEEE T CYBERNETICS
   Molga M, 2005, TEST FUNCTIONS OPTIM
   Murray I., 2010, JMLR W CP, V9
   Peters J, 2010, P 24 NAT C ART INT A
   Powell M. J. D., 2004, 2004NA05 DAMTP U CAM
   Powell MJD, 2009, 2009NA06 DAMTP U CAM
   Ruckstiess T., 2008, P EUR C MACH LEARN E
   Stulp F., 2012, INT C MACH LEARN ICM
   Sun Y., 2009, P 11 ANN C GEN EV CO
   Theodorou E., 2010, J MACHINE LEARNING R
   Wierstra D., 2014, J MACHINE LEARNING R
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100040
DA 2019-06-15
ER

PT S
AU Abernethy, J
   Lee, C
   Tewari, A
AF Abernethy, Jacob
   Lee, Chansoo
   Tewari, Ambuj
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fighting Bandits with a New Kind of Smoothness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We provide a new analysis framework for the adversarial multi-armed bandit problem. Using the notion of convex smoothing, we define a novel family of algorithms with minimax optimal regret guarantees. First, we show that regularization via the Tsallis entropy, which includes EXP3 as a special case, matches the O(root NT) minimax regret with a smaller constant factor. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O(root NT logN), as long as the perturbation distribution has a bounded hazard function. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property and lead to near-optimal algorithms.
C1 [Abernethy, Jacob; Lee, Chansoo; Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Abernethy, J (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM jabernet@umich.edu; chansool@umich.edu; tewaria@umich.edu
FU NSF under CAREER [IIS-1453304, IIS-1452099]
FX J. Abernethy acknowledges the support of NSF under CAREER grant
   IIS-1453304. A. Tewari acknowledges the support of NSF under CAREER
   grant IIS-1452099.
CR Abernethy J., 2014, P 27 C LEARN THEOR C, V35, P807
   Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096
   Audibert J.-Y., 2011, COLT
   Audibert J.-Y., 2009, COLT, P217
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P., 1995, FOCS
   Bertsekas DP, 1973, J OPTIMIZ THEORY APP, V12, P218, DOI 10.1007/BF00934819
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Dani V., 2008, NIPS
   Dani V, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P937, DOI 10.1145/1109557.1109660
   Devroye L., 2013, P 25 ANN C LEARN THE, P460
   Elsayed E., 2012, WILEY SERIES SYSTEMS
   Embrechts P, 1997, APPL MATH
   FLAXMAN AD, 2005, SODA, P385
   GITTINS J, 1996, DRUG INF J, V30, P479
   Gittins J., 2011, MULTIARMED BANDIT AL
   Hannan J., 1957, ANN MATH STUD, V3, P97
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Kocak T., 2014, ADV NEURAL INFORM PR, P613
   Kujala J, 2005, LECT NOTES ARTIF INT, V3734, P371
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   McMahan HB, 2004, LECT NOTES COMPUT SC, V3120, P109, DOI 10.1007/978-3-540-27819-1_8
   Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234
   Pacula Maciej, 2012, Applications of Evolutionary Computation. Proceedings of EvoApplications 2012: EvoCOMNET, EvoCOMPLEX, EvoFIN, EvoGAMES, EvoHOT, EvoIASP, EvoNUM, EvoPAR, EvoRISK, EvoSTIM, and EvoSTOC, P73, DOI 10.1007/978-3-642-29178-4_8
   PENOT JP, 1994, NONLINEAR ANAL-THEOR, V23, P689, DOI 10.1016/0362-546X(94)90212-7
   Rakhlin A, 2012, ADV NEURAL INFORM PR, V2, P2141
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429
   Van den Broeck G, 2009, LECT NOTES ARTIF INT, V5828, P367, DOI 10.1007/978-3-642-05224-8_28
   Van Erven T., 2014, COLT
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103068
DA 2019-06-15
ER

PT S
AU Acharya, J
   Daskalakis, C
   Kamath, G
AF Acharya, Jayadev
   Daskalakis, Constantinos
   Kamath, Gautam
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Optimal Testing for Properties of Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DENSITY
AB Given samples from an unknown discrete distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, as well as in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of discrete distributions such as monotonicity, independence, log-concavity, unimodality, and monotone-hazard rate, the optimal sample complexity is unknown.
   We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in ! 2-distance, or far in total variation distance?
   The optimality of our testers is established by providing matching lower bounds, up to constant factors. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave, monotone hazard rate distributions.
C1 [Acharya, Jayadev; Daskalakis, Constantinos; Kamath, Gautam] MIT, EECS, Cambridge, MA 02139 USA.
RP Acharya, J (reprint author), MIT, EECS, Cambridge, MA 02139 USA.
EM jayadev@mit.edu; costis@mit.edu; g@mit.edu
CR Acharya J., 2014, ISIT
   Acharya J., 2012, COLT
   Acharya J., 2013, P AISTATS, P57
   Acharya Jayadev, 2015, P 26 ANN ACM SIAM S, P1829
   Adamaszek M, 2010, PROC APPL MATH, V135, P56
   Agresti A, 2011, CATEGORICAL DATA ANA
   Alon N., 2007, P STOC
   Balabdaoui F., 2011, MAXIMUM LIKELIHOOD E
   Balabdaoui F, 2010, STAT NEERL, V64, P45, DOI 10.1111/j.1467-9574.2009.00438.x
   Barlow R, 1972, STAT INFERENCE ORDER
   Batu T., 2001, P FOCS
   Batu T., 2004, P STOC
   Bhattacharya B. B., 2015, NIPS
   Bhattacharyya Arnab, 2011, P ITCS, P239
   BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488
   Canonne C., 2016, STACS
   Canonne C. L., 2015, ECCC
   Chan S. O., 2013, P SODA
   Chan Siu-On, 2014, P 25 ANN ACM SIAM S, P1193, DOI DOI 10.1137/1.9781611973402.88
   Cule M, 2010, ELECTRON J STAT, V4, P254, DOI 10.1214/09-EJS505
   Fischer E., 2001, SCIENCE
   Fisher R, 1925, STAT METHODS RES WOR
   Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865
   Hall P, 2005, ANN STAT, V33, P1109, DOI 10.1214/009053605000000039
   Huang DY, 2013, IEEE T INFORM THEORY, V59, P8157, DOI 10.1109/TIT.2013.2283266
   Jankowski HK, 2009, ELECTRON J STAT, V3, P1567, DOI 10.1214/09-EJS526
   Kamath S., 2015, COLT
   Lehmann EL, 2006, TESTING STAT HYPOTHE
   Levi R., 2013, THEORY COMPUT, V9, P295
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   Paninski L., 2008, IEEE T INFORM THEORY, V54
   RAO JNK, 1981, J AM STAT ASSOC, V76, P221, DOI 10.2307/2287815
   Rubinfeld R., 2006, INT C MATH
   Saumard A, 2014, STAT SURV, V8, P45, DOI 10.1214/14-SS107
   Valiant G., 2011, P STOC
   Valiant G., 2014, FOCS
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101098
DA 2019-06-15
ER

PT S
AU Adeli-Mosabbeb, E
   Thung, KH
   An, L
   Shi, F
   Shen, DG
AF Adeli-Mosabbeb, Ehsan
   Thung, Kim-Han
   An, Le
   Shi, Feng
   Shen, Dinggang
CA ADNI
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust Feature-Sample Linear Discriminant Analysis for Brain Disorders
   Diagnosis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SEGMENTATION; IMAGES
AB A wide spectrum of discriminative methods is increasingly used in diverse applications for classification or regression tasks. However, many existing discriminative methods assume that the input data is nearly noise-free, which limits their applications to solve real-world problems. Particularly for disease diagnosis, the data acquired by the neuroimaging devices are always prone to different sources of noise. Robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers. These methods focus on detecting either the sample-outliers or feature-noises. Moreover, they usually use unsupervised de-noising procedures, or separately de-noise the training and the testing data. All these factors may induce biases in the learning process, and thus limit its performance. In this paper, we propose a classification method based on the least-squares formulation of linear discriminant analysis, which simultaneously detects the sample-outliers and feature-noises. The proposed method operates under a semi-supervised setting, in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space. Therefore, the violating samples or feature values are identified as sample-outliers or feature-noises, respectively. We test our algorithm on one synthetic and two brain neurodegenerative databases (particularly for Parkinson's disease and Alzheimer's disease). The results demonstrate that our method outperforms all baseline and state-of-the-art methods, in terms of both accuracy and the area under the ROC curve.
C1 [Adeli-Mosabbeb, Ehsan] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.
   Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA.
RP Adeli-Mosabbeb, E (reprint author), Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.
EM eadeli@med.unc.edu; khthung@med.unc.edu; le_an@med.unc.edu;
   fengshi@med.unc.edu; dgshen@med.unc.edu
CR Adeli-Mosabbeb E, 2015, IMAGE VISION COMPUT, V39, P38, DOI 10.1016/j.imavis.2015.04.006
   Bissantz N, 2009, SIAM J OPTIMIZ, V19, P1828, DOI 10.1137/050639132
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Braak H, 2003, NEUROBIOL AGING, V24, P197, DOI 10.1016/S0197-4580(02)00065-9
   Cai D., 2007, CVPR
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Croux C, 2001, CAN J STAT, V29, P473, DOI 10.2307/3316042
   De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184
   Elhamifar E., 2011, CVPR
   Fidler S, 2006, IEEE T PATTERN ANAL, V28, P337, DOI 10.1109/TPAMI.2006.46
   Fritsch V, 2012, MED IMAGE ANAL, V16, P1359, DOI 10.1016/j.media.2012.05.002
   Goldberg A., 2010, ADV NEURAL INFORM PR, V23, P757
   Huang D, 2012, LECT NOTES COMPUT SC, V7575, P616, DOI 10.1007/978-3-642-33765-9_44
   Joulin A., 2012, ICML
   Kim S.J., 2005, ADV NEURAL INFORM PR, P659
   Li H., 2015, IEEE TIP, V24
   Li H., 2003, P ADV NEUR INF PROC, P97
   LIM KO, 1989, J COMPUT ASSIST TOMO, V13, P588, DOI 10.1097/00004728-198907000-00006
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CW, 2013, PROC CVPR IEEE, P415, DOI 10.1109/CVPR.2013.60
   Marek K, 2011, PROG NEUROBIOL, V95, P629, DOI 10.1016/j.pneurobio.2011.09.005
   PEARCE BR, 1984, NEUROCHEM PATHOL, V2, P221
   Shen DG, 2002, IEEE T MED IMAGING, V21, P1421, DOI 10.1109/TMI.2002.803111
   Thung KH, 2014, NEUROIMAGE, V91, P386, DOI 10.1016/j.neuroimage.2014.01.033
   Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P273, DOI 10.1006/nimg.2001.0978
   Wagner A, 2009, PROC CVPR IEEE, P597, DOI 10.1109/CVPRW.2009.5206654
   Wang YY, 2014, NANOSCALE RES LETT, V9, P1, DOI 10.1186/1556-276X-9-251
   Wang YP, 2011, LECT NOTES COMPUT SC, V6893, P635, DOI 10.1007/978-3-642-23626-6_78
   Worker A., 2014, PLOS ONE, V9
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
   Zhu X., 2005, 1530 U WISC MAD COMP
   Ziegler David A, 2013, Imaging Med, V5, P91
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100030
DA 2019-06-15
ER

PT S
AU Afshar, HM
   Domke, J
AF Afshar, Hadi Mohasel
   Domke, Justin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Reflection, Refraction, and Hamiltonian Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.
C1 [Afshar, Hadi Mohasel] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
   [Domke, Justin] NICTA, Canberra, ACT 0200, Australia.
   [Domke, Justin] Australian Natl Univ, Canberra, ACT 0200, Australia.
RP Afshar, HM (reprint author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
EM hadi.afshar@anu.edu.au; Justin.Domke@nicta.com.au
FU Australian Government through the Department of Communications;
   Australian Research Council through the ICT Centre of Excellence Program
FX NICTA is funded by the Australian Government through the Department of
   Communications and the Australian Research Council through the ICT
   Centre of Excellence Program.
CR Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Buchdahl Hans Adolph, 1993, INTRO HAMILTONIAN OP
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Glen AG, 2004, COMPUT STAT DATA AN, V44, P451, DOI 10.1016/S0167-9473(02)00234-7
   Greenwood D., 1988, PRINCIPLES DYNAMICS
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680
   Neal RM, 2011, HDB MARKOV CHAIN MON, V2
   Pakman A., 2013, ADV NEURAL INF PROCE, V26, P2490
   Pakman A, 2014, J COMPUT GRAPH STAT, V23, P518, DOI 10.1080/10618600.2013.788448
   Roberts GO, 1997, ANN APPL PROBAB, V7, P110
   Sanner Scott, 2015, ASS ADVANCEMENT ARTI, P665
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101061
DA 2019-06-15
ER

PT S
AU Ahn, S
   Park, S
   Chertkov, M
   Shin, J
AF Ahn, Sungsoo
   Park, Sejun
   Chertkov, Michael
   Shin, Jinwoo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Minimum Weight Perfect Matching via Blossom Belief Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Max-product Belief Propagation (BP) is a popular message-passing algorithm for computing a Maximum-A-Posteriori (MAP) assignment over a distribution represented by a Graphical Model (GM). It has been shown that BP can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective Linear Programming (LP) relaxation is tight, i.e., no integrality gap is present. However, when LP shows an integrality gap, no model has been known which can be solved systematically via sequential applications of BP. In this paper, we develop the first such algorithm, coined Blossom-BP, for solving the minimum weight matching problem over arbitrary graphs. Each step of the sequential algorithm requires applying BP over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices. Our scheme guarantees termination in O(n(2)) of BP runs, where n is the number of vertices in the original graph. In essence, the Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom algorithm by jumping at once over many sub-steps with a single BP. Moreover, our result provides an interpretation of the Edmonds' algorithm as a sequence of LPs.
C1 [Ahn, Sungsoo; Park, Sejun; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
   [Chertkov, Michael] Los Alamos Natl Lab, Theoret Div, Los Alamos, NM USA.
   [Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM USA.
RP Ahn, S (reprint author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
EM sungsoo.ahn@kaist.ac.kr; sejun.park@kaist.ac.kr; chertkov@lanl.gov;
   jinwoos@kaist.ac.kr
FU Institute for Information & communications Technology Promotion(IITP) -
   Korea government(MSIP) [R0132-15-1005]; National Nuclear Security
   Administration of the U.S. Department of Energy [DE-AC52-06NA25396]
FX This work was supported by Institute for Information & communications
   Technology Promotion(IITP) grant funded by the Korea government(MSIP)
   (No. R0132-15-1005), Content visual browsing technology in the online
   and offline environments. The work at LANL was carried out under the
   auspices of the National Nuclear Security Administration of the U.S.
   Department of Energy under Contract No. DE-AC52-06NA25396.
CR Bayati M, 2008, IEEE T INFORM THEORY, V54, P1241, DOI 10.1109/TIT.2007.915695
   Bayati M, 2011, SIAM J DISCRETE MATH, V25, P989, DOI 10.1137/090753115
   Chandra R., 2000, PARALLEL PROGRAMMING
   Chandrasekaran K., 2012, FDN COMPUTER SCI FOC
   EDMONDS J, 1965, CANADIAN J MATH, V17, P449, DOI 10.4153/CJM-1965-045-4
   Fischetti M, 2007, MATH PROGRAM, V110, P3, DOI 10.1007/s10107-006-0054-8
   GAMARNIK D, 2010, SODA, V135, P279
   Gonzalez J., 2009, INT C ART INT STAT
   GROTSCHEL M, 1985, MATH PROGRAM, V33, P243, DOI 10.1007/BF01584376
   Huang B., 2007, ARTIFICIAL INTELLIGE
   Kolmogorov V, 2009, MATH PROGRAM COMPUT, V1, P43, DOI 10.1007/s12532-009-0002-8
   Kyrola A., 2012, OPERATING SYSTEMS DE
   Low Y, 2010, C UNC ART INT UAI
   Malioutov DM, 2006, J MACH LEARN RES, V7, P2031
   Mezard M, 2009, OXFORD GRADUATE TEXT
   Moallemi C., 2008, 45 ALL C COMM CONTR
   PADBERG MW, 1982, MATH OPER RES, V7, P67, DOI 10.1287/moor.7.1.67
   Park S., 2015, C UNC ART INT UAI
   Richardson T., 2008, MODERN CODING THEORY
   Ruozzi N., 2008, 46 ANN ALL C COMM CO
   Sanghavi S., 2007, NEURAL INFORM PROCES
   Trick M., 1978, THESIS
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Weiss Y., 2007, C UNC ART INT UAI
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100104
DA 2019-06-15
ER

PT S
AU Alabdulmohsin, I
AF Alabdulmohsin, Ibrahim
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Algorithmic Stability and Uniform Generalization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LEARNABILITY; BOUNDS
AB One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result. For instance, a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.
C1 [Alabdulmohsin, Ibrahim] King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.
RP Alabdulmohsin, I (reprint author), King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.
EM ibrahim.alabdulmohsin@kaust.edu.sa
CR Audibert JY, 2007, J MACH LEARN RES, V8, P863
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Devroye L., 1996, PROBABILISTIC THEORY
   Diaconis P., 1991, STAT SCI, V6, P284
   Downs T, 2002, J MACH LEARN RES, V2, P293, DOI 10.1162/15324430260185637
   Elisseeff A., 2002, NATO ASI SERIES LE 3
   Kearns M, 1999, NEURAL COMPUT, V11, P1427, DOI 10.1162/089976699300016304
   Kutin S., 2002, P 18 C UNC ART INT U
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   Poggio T, 2004, NATURE, V428, P419, DOI 10.1038/nature02341
   Robbins H., 1955, AM MATH MONTHLY, P26, DOI [DOI 10.2307/2308012, 10.2307/2308012]
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Stigler SM, 1986, HIST STAT MEASUREMEN
   Talagrand M, 1996, ANN PROBAB, V24, P1049, DOI 10.1214/aop/1065725175
   Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042
   Vapnik V. N., 1999, NEURAL NETWORKS IEEE, V10
   Wager Stefan, 2013, ADV NEURAL INFORM PR, P351
   Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103057
DA 2019-06-15
ER

PT S
AU Alistarh, D
   Iglesias, J
   Vojnovic, M
AF Alistarh, Dan
   Iglesias, Jennifer
   Vojnovic, Milan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Streaming Min-Max Hypergraph Partitioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In many applications, the data is of rich structure that can be represented by a hypergraph, where the data items are represented by vertices and the associations among items are represented by hyperedges. Equivalently, we are given an input bipartite graph with two types of vertices: items, and associations (which we refer to as topics). We consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized. This is a clustering problem with various applications, e.g. partitioning of a set of information objects such as documents, images, and videos, and load balancing in the context of modern computation platforms.
   In this paper, we focus on the streaming computation model for this problem, in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time. Motivated by scalability requirements, we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components. We show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions. We also report results of an extensive empirical evaluation, which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches.
C1 [Alistarh, Dan; Vojnovic, Milan] Microsoft Res, Cambridge, England.
   [Iglesias, Jennifer] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Alistarh, D (reprint author), Microsoft Res, Cambridge, England.
EM dan.alistarh@microsoft.com; jiglesia@andrew.cmu.edu;
   milanv@microsoft.com
CR Bansal N, 2014, SIAM J COMPUT, V43, P872, DOI 10.1137/120873996
   Bourse F., 2014, P ACM KDD
   Chen Y., 2012, P NIPS
   Cheng Y, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P93
   CHUNG F, 2003, ANN COMB, V7, P141
   Dhillon I. S., 2003, P ACM KDD
   Dhillon I. S., 2001, P ACM KDD
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Hein M., 2013, P NIPS
   Karagiannis T., 2010, P ACM SOCC
   Karypis G., 2000, VLSI DESIGN, V11
   Krauthgamer R., 2009, PARTITIONING GRAPHS
   Li M., 2015, ARXIV150504636
   Massoulie L., 2014, P ACM STOC
   Mitliagkas I., 2013, P NIPS
   Mossel E., 2014, PROBABILITY THEORY R, V162, P1
   O'Connor L., 2014, P NIPS
   Pujol JM, 2012, IEEE ACM T NETWORK, V20, P1162, DOI 10.1109/TNET.2012.2188815
   Stanton I., 2014, P ACM SIAM SODA
   Stanton I., 2012, P ACM KDD
   Svitkina Z, 2004, LECT NOTES COMPUT SC, V3122, P207
   Tsourakakis C. E., 2014, P ACM WSDM
   Yun S.-Y., 2014, P NIPS
   Zong B., 2015, P ACM DEBS
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102046
DA 2019-06-15
ER

PT S
AU Anava, O
   Hazan, E
   Mannor, S
AF Anava, Oren
   Hazan, Elad
   Mannor, Shie
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Learning for Adversaries with Memory: Price of Past Mistakes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretical results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.
C1 [Anava, Oren; Mannor, Shie] Technion, Haifa, Israel.
   [Hazan, Elad] Princeton Univ, New York, NY USA.
RP Anava, O (reprint author), Technion, Haifa, Israel.
EM oanava@tx.technion.ac.il; ehazan@cs.princeton.edu;
   shie@ee.technion.ac.il
FU European Community [306638]
FX This work has been supported by the European Community's Seventh
   Framework Programme (FP7/2007-2013) under grant agreement 306638
   (SUPREL).
CR Anava Oren, 2013, ARXIV13026927
   Anava Oren, 2015, ICML
   Arora Raman, 2012, ONLINE BANDIT LEARNI
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi Nicolo, 2013, CORR
   Clements MP, 1996, OXFORD B ECON STAT, V58, P657
   Cuturi Marco, 2013, MEAN REVERSION VARIA, V28, P271
   D'Aspremont A, 2011, QUANT FINANC, V11, P351, DOI 10.1080/14697688.2010.481634
   Dekel O., 2013, ARXIV13102997
   Geulen S., 2010, P 23 ANN C LEARN THE, P132
   Gofer Eyal, 2014, P 27 C LEARN THEOR, P210
   Gyorgy A., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2218, DOI 10.1109/ISIT.2011.6033954
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287
   JOHANSEN S, 1991, ECONOMETRICA, V59, P1551, DOI 10.2307/2938278
   Jurek Jakub W, 2007, EFA 2006 M PAP
   LITTLESTONE N, 1989, ANN IEEE SYMP FOUND, P256, DOI 10.1109/SFCS.1989.63487
   LOVASZ L, 2003, FOCS, P640
   Maddala G.S., 1998, UNIT ROOTS COINTEGRA
   Marcellino M, 2006, J ECONOMETRICS, V135, P499, DOI 10.1016/j.jeconom.2005.07.020
   Merhav N, 2002, IEEE T INFORM THEORY, V48, P1947, DOI 10.1109/TIT.2002.1013135
   Narayanan Hariharan, 2010, ADV NEURAL INFORM PR, P1777
   Puterman M. L., 1994, WILEY SERIES PROBABI
   Schmidt Anatoly B., 2011, FINANCIAL MARKETS TR
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103063
DA 2019-06-15
ER

PT S
AU Andoni, A
   Indyk, P
   Laarhoven, T
   Razenshteyn, I
   Schmidt, L
AF Andoni, Alexandr
   Indyk, Piotr
   Laarhoven, Thijs
   Razenshteyn, Ilya
   Schmidt, Ludwig
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Practical and Optimal LSH for Angular Distance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.
   We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.
C1 [Andoni, Alexandr] Columbia Univ, New York, NY 10027 USA.
   [Indyk, Piotr; Razenshteyn, Ilya; Schmidt, Ludwig] MIT, Cambridge, MA 02139 USA.
   [Laarhoven, Thijs] TU Eindhoven, Eindhoven, Netherlands.
RP Andoni, A (reprint author), Columbia Univ, New York, NY 10027 USA.
FU NSF; Simons Foundation
FX We thank Michael Kapralov for many valuable discussions during various
   stages of this work. We also thank Stefanie Jegelka and Rasmus Pagh for
   helpful conversations. This work was supported in part by the NSF and
   the Simons Foundation. Work done in part while the first author was at
   the Simons Institute for the Theory of Computing.
CR Ailon N, 2014, DISCRETE COMPUT GEOM, V52, P780, DOI 10.1007/s00454-014-9632-3
   Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096
   Andoni A., 2015, STOC
   Andoni Alexandr, 2015, TIGHT LOWER BOUNDS D
   Andoni Alexandr, 2014, SODA
   Charikar Moses S, 2002, STOC
   Dasgupta Anirban, 2011, KDD
   DIACONIS P, 1987, ANN I H POINCARE-PR, V23, P397
   Dubiner M, 2010, IEEE T INFORM THEORY, V57, P4166, DOI 10.1109/TIT.2010.2050814
   Eshghi Kave, 2008, KDD
   Feige U, 2002, RANDOM STRUCT ALGOR, V20, P403, DOI 10.1002/rsa.10036
   Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI [DOI 10.4086/T0C.2012.V008A014, 10.4086/toc.2012.v008a014]
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Lichman M., 2013, UCI MACHINE LEARNING
   Lv Qin, 2007, VLDB
   Motwani R, 2007, SIAM J DISCRETE MATH, V21, P930, DOI 10.1137/050646858
   O'Donnell Ryan, 2014, ACM Transactions on Computation Theory, V6, DOI 10.1145/2578221
   Samet H., 2006, FDN MULTIDIMENSIONAL
   Schmidt L., 2014, ICASSP
   Shakhnarovich G., 2005, NEAREST NEIGHBOR MET
   Shrivastava Anshumali, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P474, DOI 10.1007/978-3-642-33460-3_36
   Shrivastava A., 2014, ICML
   Slaney M, 2012, P IEEE, V100, P2604, DOI 10.1109/JPROC.2012.2193849
   Sundaram Narayanan, 2013, VLDB
   Terasawa K, 2007, LECT NOTES COMPUT SC, V4619, P27
   Weinberger Kilian, 2009, ICML
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102042
DA 2019-06-15
ER

PT S
AU Andreas, J
   Rabinovich, M
   Jordan, MI
   Klein, D
AF Andreas, Jacob
   Rabinovich, Maxim
   Jordan, Michael I.
   Klein, Dan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the Accuracy of Self-Normalized Log-Linear Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as "self-normalization", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking.
   We prove upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.
C1 [Andreas, Jacob; Rabinovich, Maxim; Jordan, Michael I.; Klein, Dan] Univ Calif Berkeley, Comp Sci Div, Berkeley, CA 94720 USA.
RP Andreas, J (reprint author), Univ Calif Berkeley, Comp Sci Div, Berkeley, CA 94720 USA.
EM jda@cs.berkeley.edu; rabinovich@cs.berkeley.edu; jordan@cs.berkeley.edu;
   klein@cs.berkeley.edu
FU NSF Graduate Fellowships; Fannie and John Hertz Foundation Fellowship
FX The authors would like to thank Robert Nishihara for useful discussions.
   JA and MR are supported by NSF Graduate Fellowships, and MR is
   additionally supported by the Fannie and John Hertz Foundation
   Fellowship.
CR Andreas J., 2014, P ANN M N AM CHAPT A
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Chen YH, 2010, ADV INTELL SOFT COMP, V66, P109, DOI 10.1145/1866919.1866935
   Devlin J., 2014, P ANN M ASS COMP LIN
   Doucet A, 2001, INTRO SEQUENTIAL MON
   Gutmann Michael, 2010, P INT C ART INT STAT, P297
   Lafferty John D., 2001, CONDITIONAL RANDOM F, P282
   McCullagh P., 1989, GEN LINEAR MODELS
   Morin F, 2005, P INT WORKSH ART INT, P246
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   Vaswani A., 2013, P C EMP METH NAT LAN
   Yang E., 2012, ADV NEURAL INFORM PR, V25, P1358
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101066
DA 2019-06-15
ER

PT S
AU Arjevani, Y
   Shamir, O
AF Arjevani, Yossi
   Shamir, Ohad
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Communication Complexity of Distributed Convex Learning and Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.
C1 [Arjevani, Yossi; Shamir, Ohad] Weizmann Inst Sci, IL-7610001 Rehovot, Israel.
RP Arjevani, Y (reprint author), Weizmann Inst Sci, IL-7610001 Rehovot, Israel.
EM yossi.arjevani@weizmann.ac.il; ohad.shamir@weizmann.ac.il
FU FP7 Marie Curie CIG grant; Intel ICRI-CI Institute; Israel Science
   Foundation [425/13]
FX This research is supported in part by an FP7 Marie Curie CIG grant, the
   Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13. We
   thank Nati Srebro for several helpful discussions and insights.
CR Agarwal A., 2011, ABS11104198 CORR
   Balcan M., 2012, COLT
   Balcan M.-F., 2014, NIPS
   Bekkerman R., 2011, SCALING MACHINE LEAR
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Clarkson Kenneth L, 2009, STOC
   Cotter A., 2011, NIPS
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027
   Frostig R., 2014, ARXIV14126606
   Jaggi M., 2014, NIPS
   Lee J., 2015, 150707595 CORR
   Mahajan D., 2013, ABS13110636 CORR
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Recht B., 2011, NIPS
   Richtarik P., 2013, ABS13102059 CORR
   Shamir O., 2014, NIPS
   Shamir O., 2014, ICML
   Shamir O., 2014, ALL C COMM CONTR COM
   Tao T, 2012, TOPICS RANDOM MATRIX, V132
   Tsitsiklis J. N., 1987, Journal of Complexity, V3, P231, DOI 10.1016/0885-064X(87)90013-6
   Yang T., 2013, NIPS
   Yu Yaoliang, 2013, NIPS
   Zhang YC, 2013, J MACH LEARN RES, V14, P3321
   Zhang Yuchen, 2015, ICML
   Zinkevich M. A., 2010, NIPS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100099
DA 2019-06-15
ER

PT S
AU Asteris, M
   Papailiopoulos, D
   Dimakis, AG
AF Asteris, Megasthenis
   Papailiopoulos, Dimitris
   Dimakis, Alexandros G.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Orthogonal NMF through Subspace Exploration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NONNEGATIVE MATRIX FACTORIZATION
AB Orthogonal Nonnegative Matrix Factorization (ONMF) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors, one of which has orthonormal columns. It yields potentially useful data representations as superposition of disjoint parts, while it has been shown to work well for clustering tasks where traditional methods underperform. Existing algorithms rely mostly on heuristics, which despite their good empirical performance, lack provable performance guarantees.
   We present a new ONMF algorithm with provable approximation guarantees. For any constant dimension k, we obtain an additive EPTAS without any assumptions on the input. Our algorithm relies on a novel approximation to the related Nonnegative Principal Component Analysis (NNPCA) problem; given an arbitrary data matrix, NNPCA seeks k nonnegative components that jointly capture most of the variance. Our NNPCA algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component.
   We evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art.
C1 [Asteris, Megasthenis; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA.
   [Papailiopoulos, Dimitris] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Asteris, M (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM megas@utexas.edu; dimitrisp@berkeley.edu; dimakis@austin.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [CCF-1217058, CCF-1116404, CCF 1344179, 1344364, 1407278, 1422549];
   MURI AFOSR [556016]; ARO YIP [W911NF-14-1-0258]
FX DP is generously supported by NSF awards CCF-1217058 and CCF-1116404 and
   MURI AFOSR grant 556016. This research has been supported by NSF Grants
   CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258.
CR Arora S., 2012, P 44 ANN ACM S THEOR, P145
   Arora S., 2012, ARXIV12124777
   Asteris M., 2014, P 31 INT C MACH LEAR, P1728
   BACHE K., 2013, UCI MACHINE LEARNING
   Buchsbaum G, 2002, VISION RES, V42, P559, DOI 10.1016/S0042-6989(01)00303-0
   Cai Liming, 2003, J COMPUTER SYSTEM SC
   CAO B, 2007, IJCAI, P2689
   Cesati M, 1997, INFORM PROCESS LETT, V64, P165, DOI 10.1016/S0020-0190(97)00164-6
   Chen G, 2009, INFORM PROCESS MANAG, V45, P368, DOI 10.1016/j.ipm.2008.12.004
   Choi S, 2008, IEEE IJCNN, P1828, DOI 10.1109/IJCNN.2008.4634046
   Cichocki A, 2009, NONNEGATIVE MATRIX T
   Ding C., 2006, P 12 ACM SIGKDD INT, P126, DOI DOI 10.1145/1150402.1150420
   Gillis NA, 2013, IEEE T PATTERN ANAL
   Gillis Nicolas, 2012, ARXIV12081237
   Huang K, 2013, INT CONF ACOUST SPEE, P4524, DOI 10.1109/ICASSP.2013.6638516
   Kuang D, 2012, SDM, V12, P106, DOI DOI 10.1137/1.9781611972825.10
   Kumar A., 2013, INT C MACH LEARN ICM, P231
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li HL, 2007, J VLSI SIG PROC SYST, V48, P83, DOI 10.1007/s11265-006-0039-0
   Li T, 2006, IEEE DATA MINING, P362
   Li X., 2007, P 24 INT C MACH LEAR, P537
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756
   Pompili Filippo, 2012, ARXIV12010901
   Pompili Filippo, 2013, ESANN 2013
   Recht B., 2012, ADV NEURAL INFORM PR, P1223
   Shahnaz F, 2006, INFORM PROCESS MANAG, V42, P373, DOI 10.1016/j.ipm.2004.11.005
   Sigg C. D., 2008, P 25 INT C MACH LEAR, P960, DOI [10.1145/1390156.1390277, DOI 10.1145/1390156.1390277]
   Sung  K.-K., 1996, THESIS
   Yang Z., 2012, P ADV NEUR INF PROC, V25, P1088
   Yang ZR, 2010, IEEE T NEURAL NETWOR, V21, P734, DOI 10.1109/TNN.2010.2041361
   Yuan ZJ, 2005, LECT NOTES COMPUT SC, V3540, P333
   Zass R., 2007, ADV NEURAL INFORM PR, V19, P1561
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103036
DA 2019-06-15
ER

PT S
AU Asteris, M
   Papailiopoulos, D
   Kyrillidis, A
   Dimakis, AG
AF Asteris, Megasthenis
   Papailiopoulos, Dimitris
   Kyrillidis, Anastasios
   Dimakis, Alexandros G.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sparse PCA via Bipartite Matchings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PRINCIPAL COMPONENT; POWER METHOD; ROTATION
AB We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. Such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the input data. We evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.
C1 [Asteris, Megasthenis; Kyrillidis, Anastasios; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA.
   [Papailiopoulos, Dimitris] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Asteris, M (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM megas@utexas.edu; dimitrisp@berkeley.edu; anastasios@utexas.edu;
   dimakis@austin.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [CCF-1217058, CCF-1116404, CCF 1344179, 1344364, 1407278, 1422549];
   MURI AFOSR [556016]; ARO [YIP W911NF-14-1-0258]
FX DP is generously supported by NSF awards CCF-1217058 and CCF-1116404 and
   MURI AFOSR grant 556016. This research has been supported by NSF Grants
   CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258.
CR Asteris M., 2015, ARXIV150800625
   Asteris M, 2014, IEEE T INFORM THEORY, V60, P2281, DOI 10.1109/TIT.2014.2303975
   Boutsidis C., 2011, ADV NEURAL INFORM PR, P2285
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Jenatton R, 2010, P 13 INT C ART INT S, P366
   Jiang R., 2011, P 17 ACM SIGKDD INT, P886
   Johnstone I. M., 2009, J AM STAT ASS, V104
   JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395
   Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148
   Journee M, 2010, J MACH LEARN RES, V11, P517
   KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233
   Lichman M., 2013, UCI MACHINE LEARNING
   LIU H., 2014, ARXIV14085352
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   Mackey L., 2009, ADV NEURAL INFORM PR, V21, P1017
   Magdon-Ismail M., 2015, CORR
   Magdon-Ismail Malik, 2015, ARXIV150206626
   Majumdar A, 2009, SIGNAL IMAGE VIDEO P, V3, P27, DOI 10.1007/s11760-008-0056-5
   Moghaddam B., 2006, ADV NEURAL INFORM PR, V18, P915
   Papailiopoulos D. S., 2013, ICML, P747
   Ramshaw  L., 2012, HPL201240R1
   Richard E., 2014, ADV NEURAL INFORM PR, P3284
   Sigg C. D., 2008, P 25 INT C MACH LEAR, P960, DOI [10.1145/1390156.1390277, DOI 10.1145/1390156.1390277]
   Vu V. Q., 2012, P 15 INT C ART INT S, P1278
   Vu V.Q., 2013, ADV NEURAL INFORM PR, P2670
   Wang Z., 2013, P 16 INT C ART INT S, P48
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zhang YW, 2012, INT SER OPER RES MAN, V166, P915, DOI 10.1007/978-1-4614-0769-0_31
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102050
DA 2019-06-15
ER

PT S
AU Audiffren, J
   Ralaivola, L
AF Audiffren, Julien
   Ralaivola, Liva
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Cornering Stationary and Restless Mixing Bandits with Remix-UCB
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INEQUALITIES; CONVERGENCE; BOUNDS; RATES; SUMS
AB We study the restless bandit problem where arms are associated with stationary phi-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of carefully recovering some independence by 'ignoring' the values of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off, which we do by considering the idea of a waiting arm in the new Remix-UCB algorithm, a generalization of Improved-UCB for the problem at hand, that we introduce. We provide a regret analysis for this bandit strategy; two noticeable features of Remix-UCB are that i) it reduces to the regular Improved-UCB when the phi-mixing coefficients are all 0, i.e. when the i.i.d scenario is recovered, and ii) when phi(n) = O(n(-alpha)), it is able to ensure a controlled regret of order (circle dot) over tilde(Delta((alpha-2)/alpha)(*) log(1/alpha) T), where Delta(*) encodes the distance between the best arm and the best suboptimal arm, even in the case when alpha < 1, i.e. the case when the phi-mixing coefficients are not summable.
C1 [Audiffren, Julien] Paris Saclay Univ, ENS Cachan, CMLA, F-94235 Cachan, France.
   [Ralaivola, Liva] Aix Marseille Univ, QARMA, LIF, CNRS, F-13289 Marseille 9, France.
RP Audiffren, J (reprint author), Paris Saclay Univ, ENS Cachan, CMLA, F-94235 Cachan, France.
EM audiffren@cmla.ens-cachan.fr; liva.ralaivola@lif.univ-mrs.fr
FU ANR [ANR-12-BS02-004-01]; ND project
FX This work is partially supported by the ANR-funded projet GRETA -
   Greediness: theory and algorithms (ANR-12-BS02-004-01) and the ND
   project.
CR Audibert JY, 2009, ANN C LEARN THEOR
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6
   Bernstein S, 1927, MATH ANN, V97, P1, DOI 10.1007/BF01447859
   Bubeck Sebastien, 2012, FDN TRENDS MACHINE L, V5
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196
   Karandikar RL, 2002, STAT PROBABIL LETT, V58, P297, DOI 10.1016/S0167-7152(02)00124-4
   Kontorovich L, 2008, ANN PROBAB, V36, P2126, DOI 10.1214/07-AOP384
   Kulkarni S, 2005, ADV NEURAL INFORM PR, P819
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   McDonald D, 2011, ARXIV11030941
   Mohri M., 2009, ADV NEURAL INFORM PR, P1097
   Mohri M, 2010, J MACH LEARN RES, V11, P789
   Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19
   Pandey S., 2007, P 24 INT C MACH LEAR, P721, DOI DOI 10.1145/1273496.1273587
   Ralaivola L, 2010, J MACH LEARN RES, V11, P1927
   Seldin  Y., 2014, INT C MACH LEARN ICM, P1287
   Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001
   Steinwart Ingo, 2009, ADV NEURAL INF PROCE, p[1768, 6]
   Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613
   YU B, 1994, ANN PROBAB, V22, P94, DOI 10.1214/aop/1176988849
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103067
DA 2019-06-15
ER

PT S
AU Awasthi, P
   Risteski, A
AF Awasthi, Pranjal
   Risteski, Andrej
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On some provably correct cases of variational inference for topic models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID EM
AB Variational inference is an efficient, popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. Our initializations are natural, one of them being used in LDA-c, the most popular implementation of variational inference. In addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to use non-standard proof arguments, which we believe might be of general theoretical interest.
C1 [Awasthi, Pranjal] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.
   [Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.
RP Awasthi, P (reprint author), Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.
EM pranjal.awasthi@rutgers.edu; risteski@cs.princeton.edu
CR Agarwal A., 2013, P 27 C LEARN THEOR C
   Anandkumar A., 2012, TECHNICAL REPORT
   Anandkumar A., 2013, P 30 INT C MACH LEAR
   Arora S, 2013, P 30 INT C MACH LEAR
   Arora S., 2015, P 28 C LEARN THEOR C
   Arora S., 2014, P 27 C LEARN THEOR C
   Arora S., 2012, P 53 ANN IEEE S FDN
   Arora S., 2012, P 44 ANN ACM S THEOR, P145
   Balakrishnan S., 2014, ARXIV14082156
   Bansal T., 2014, ADV NEURAL INFORM PR
   Blei D. M., 2009, TEXT MINING CLASSIFI, V10, P71, DOI DOI 10.1145/1141844.1143859
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Dasgupta S., 2000, P UNC ART INT UAI
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Ding W., 2014, P 17 INT C ART INT S, P167
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kumar A., 2010, P FDN COMP SCI FOCS
   Lee D. D., 2000, ADV NEURAL INFORM PR
   Netrapalli P., 2013, ADV NEURAL INFORM PR
   Saligrama V., 2013, ARXIV13033664
   Sontag D., 2000, ADV NEURAL INFORM PR
   Sundberg R., 1974, SCAND J STAT, V1, P49
   Telgarsky M., 2013, DIRICHLET DRAW UNPUB
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101006
DA 2019-06-15
ER

PT S
AU Ba, J
   Grosse, R
   Salakhutdinov, R
   Frey, B
AF Ba, Jimmy
   Grosse, Roger
   Salakhutdinov, Ruslan
   Frey, Brendan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Wake-Sleep Recurrent Attention Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.
C1 [Ba, Jimmy; Grosse, Roger; Salakhutdinov, Ruslan; Frey, Brendan] Univ Toronto, Toronto, ON, Canada.
RP Ba, J (reprint author), Univ Toronto, Toronto, ON, Canada.
EM jimmy@psi.toronto.edu; rgrosse@cs.toronto.edu; rsalskhu@cs.toronto.edu;
   frey@psi.toronto.edu
FU Fields Institute; ONR [N00014-14-1-0232]; Samsung
FX This work was supported by the Fields Institute, Samsung, ONR Grant
   N00014-14-1-0232 and the hardware donation of NVIDIA Corporation.
CR Ba J., 2015, INT C LEARN REPR
   Bahdanau D., 2015, INT C LEARN REPR
   Bornschein Jorg, 2014, ARXIV14062751
   Burda Y., 2015, ARXIV150900519
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Graves A., 2014, ARXIV13080850
   Gregor K., 2015, ARXIV150204623
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Judd T, 2009, INT C COMP VIS
   Karpathy Andrej, 2014, ARXIV14122306
   Kingma D. P., 2014, ARXIV14126980
   Kingma Diederik, 2014, INT C LEARN REPR
   Krizhevsky Alex, 2012, NEURAL INFORM PROCES
   Larochelle H., 2010, NEURAL INFORM PROCES
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mnih Andriy, 2014, INT C MACH LEARN
   Mnih V., 2014, NEURAL INFORM PROCES
   Neal Radford M., 1992, ARTIFICIAL INTELLIGE
   Paisley  J., 2012, INT C MACH LEARN
   Rezende D. J., 2014, INT C MACH LEARN
   Tang Y., 2014, NEURAL INFORM PROCES
   Tang Y., 2013, NEURAL INFORM PROCES
   Weaver  L., 2001, P 17 C UNC ART INT, P538
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xu K., 2015, INT C MACH LEARN
   Zaremba Wojciech, 2015, ARXIV150500521
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102010
DA 2019-06-15
ER

PT S
AU Babanezhad, R
   Ahmed, MO
   Virani, A
   Schmidt, M
   Konecny, J
   Sallinen, S
AF Babanezhad, Reza
   Ahmed, Mohamed Osama
   Virani, Alim
   Schmidt, Mark
   Konecny, Jakub
   Sallinen, Scott
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Stop Wasting My Gradients: Practical SVRG
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly-used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method.
C1 [Babanezhad, Reza; Ahmed, Mohamed Osama; Virani, Alim; Schmidt, Mark] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
   [Konecny, Jakub] Univ Edinburgh, Sch Math, Edinburgh, Midlothian, Scotland.
   [Sallinen, Scott] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC, Canada.
RP Babanezhad, R (reprint author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
EM rezababa@cs.ubc.ca; moahmed@cs.ubc.ca; alim.virani@gmail.com;
   schmidtm@cs.ubc.ca; kubo.konecny@gmail.com; scotts@ece.ubc.ca
FU Natural Sciences and Engineering Research Council of Canada [RGPIN
   312176-2010, RGPIN 311661-08, RGPIN-06068-2015]; Google European
   Doctoral Fellowship
FX We would like to thank the reviewers for their helpful comments. This
   research was supported by the Natural Sciences and Engineering Research
   Council of Canada (RGPIN 312176-2010, RGPIN 311661-08,
   RGPIN-06068-2015). Jakub Konecny is supported by a Google European
   Doctoral Fellowship.
CR Aravkin A, 2012, MATH PROGRAM, V134, P101, DOI 10.1007/s10107-012-0571-6
   Bottou L., 2007, ADV NEURAL INFORM PR
   Byrd RH, 2012, MATH PROGRAM, V134, P127, DOI 10.1007/s10107-012-0572-5
   Defazio A., 2014, ADV NEURAL INFORM PR
   Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629
   Hu C., 2009, ADV NEURAL INFORM PR
   Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169
   Johnson R., 2013, ADV NEURAL INFORM PR
   Konecny J., 2014, MS2GD MINI BATCH SEM
   Konecny J., 2013, SEMISTOCHASTIC GRADI
   Le Roux N., 2012, ADV NEURAL INFORM PR
   Lohr S.L, 2009, SAMPLING DESIGN ANAL
   Mahdavi  M., 2013, ADV NEURAL INFORM PR
   Mairal J., 2013, INT C MACH LEARN ICM
   Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370
   Schmidt M., 2011, ADV NEURAL INFORM PR
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Usunier N., 2010, INT C ART INT STAT A
   van den Doel K, 2012, SIAM J SCI COMPUT, V34, pA185, DOI 10.1137/110826692
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang L., 2013, ADV NEURAL INFORM PR
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100079
DA 2019-06-15
ER

PT S
AU Bachman, P
   Precup, D
AF Bachman, Philip
   Precup, Doina
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Data Generation as Sequential Decision Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation - perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the models using neural networks and train them using a form of guided policy search [9]. Our models generate predictions through an iterative process of feedback and refinement. We show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.
C1 [Bachman, Philip; Precup, Doina] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
RP Bachman, P (reprint author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
EM phil.bachman@gmail.com; dprecup@cs.mcgill.ca
CR Denton E. L., 2015, ARXIV150605751CSCV
   Graves A., 2013, ARXIV13080850CSNE
   Gregor K., 2015, INT C MACH LEARN ICM
   Gregor K., 2014, INT C MACH LEARN ICM
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P, 2014, ADV NEURAL INFORM PR
   Larochelle Hugo, 2011, INT C MACH LEARN ICM
   Levine S, 2014, INT C MACH LEARN ICM
   Levine S., 2014, ADV NEURAL INFORM PR
   Levine  S., 2013, INT C MACH LEARN ICM
   Levine S., 2013, ADV NEURAL INFORM PR
   Mnih A., 2014, INT C MACH LEARN ICM
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Rezende Danilo J, 2015, INT C MACH LEARN ICM
   Rezende Danilo Jimenez, 2014, INT C MACH LEARN ICM
   Sohl-Dickstein J., 2015, INT C MACH LEARN ICM
   Susskind Joshua, 2010, TORONTO FACE DATABAS
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101019
DA 2019-06-15
ER

PT S
AU Bahmani, S
   Romberg, J
AF Bahmani, Sohail
   Romberg, Justin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Compressive Phase Retrieval with Constrained Sensing Vectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY
AB We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.
   In recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix. However, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.
   Deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace,then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is O(k log d/k), where k and d denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.
C1 [Bahmani, Sohail; Romberg, Justin] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Bahmani, S (reprint author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM sohail.bahmani@ece.gatech.edu; jrom@ece.gatech.edu
FU ONR [N00014-11-1-0459]; NSF [CCF-1415498, CCF-1422540]
FX This work was supported by ONR grant N00014-11-1-0459, and NSF grants
   CCF-1415498 and CCF-1422540.
CR Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5
   Bertolotti J, 2012, NATURE, V491, P232, DOI 10.1038/nature11578
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999
   Iwen Mark, 2015, APPL COMPUTATIONAL H
   Kueng R., 2015, APPL COMPUTATIONAL H
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Liutkus A, 2014, SCI REP-UK, V4, DOI 10.1038/srep05552
   Moravec ML, 2007, PROC SPIE, V6701, DOI 10.1117/12.736360
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Ohlsson H., 2012, ADV NEURAL INFORM PR, P1367
   Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574
   Pedarsani R, 2014, ANN ALLERTON CONF, P842, DOI 10.1109/ALLERTON.2014.7028542
   Schniter P, 2015, IEEE T SIGNAL PROCES, V63, P1043, DOI 10.1109/TSP.2014.2386294
   Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687
   Shechtman Y, 2011, OPT EXPRESS, V19, P14807, DOI 10.1364/OE.19.014807
   Tropp Joel A., 2014, ARXIV14051102CSIT
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103059
DA 2019-06-15
ER

PT S
AU Balsubramani, A
   Freund, Y
AF Balsubramani, Akshay
   Freund, Yoav
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Scalable Semi-Supervised Aggregation of Classifiers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.
C1 [Balsubramani, Akshay; Freund, Yoav] Univ Calif San Diego, San Diego, CA 92093 USA.
RP Balsubramani, A (reprint author), Univ Calif San Diego, San Diego, CA 92093 USA.
EM abalsubr@cs.ucsd.edu; yfreund@cs.ucsd.edu
FU National Science Foundation [IIS-1162581]
FX The authors acknowledge support from the National Science Foundation
   under grant IIS-1162581.
CR [Anonymous], 2012, PREDICTING BIOL RESP
   Balsubramani Akshay, 2015, C LEARN THEOR
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/BF00058655
   Caruana R., 2008, P INT C MACH LEARN, P96, DOI DOI 10.1145/1390156.1390169
   Chapelle Olivier, SEMISUPERVISED LEARN
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Freund Y, 1997, P 29 ANN ACM S THEOR, P334, DOI DOI 10.1145/258533.258616
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Joachims Thorsten, 1999, P 16 INT C MACH LEAR, P200
   Koren Y., 2009, BELLKOR SOLUTION NET
   Lin Y, 2006, J AM STAT ASSOC, V101, P578, DOI 10.1198/016214505000001230
   Mallapragada PK, 2009, IEEE T PATTERN ANAL, V31, P2000, DOI 10.1109/TPAMI.2008.235
   Moosmann F, 2007, 20 ANN C NEUR INF PR, V19, P985
   Parisi F, 2014, P NATL ACAD SCI USA, V111, P1253, DOI 10.1073/pnas.1219097111
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Schapire RE, 1998, ANN STAT, V26, P1651
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Zhu X, 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI DOI 10.2200/S00196ED1V01Y200906AIM006
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102090
DA 2019-06-15
ER

PT S
AU Banerjee, S
   Lofgren, P
AF Banerjee, Siddhartha
   Lofgren, Peter
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Bidirectional Probability Estimation in Markov Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We develop a new bidirectional algorithm for estimating Markov chain multi-step transition probabilities: given a Markov chain, we want to estimate the probability of hitting a given target state in ` steps after starting from a given source distribution. Given the target state t, we use a (reverse) local power iteration to construct an ` expanded target distribution', which has the same mean as the quantity we want to estimate, but a smaller variance -this can then be sampled efficiently by a Monte Carlo algorithm. Our method extends to any Markov chain on a discrete (finite or countable) state-space, and can be extended to compute functions of multi-step transition probabilities such as PageRank, graph diffusions, hitting/ return times, etc. Our main result is that in ` sparse' Markov Chains -wherein the number of transitions between states is comparable to the number of states the running time of our algorithm for a uniform-random target node is order-wise smaller than Monte Carlo and power iteration based algorithms; in particular, our method can estimate a probability p using only O(1/root p) running time.
C1 [Banerjee, Siddhartha] Cornell, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA.
   [Lofgren, Peter] Stanford, Comp Sci Dept, Stanford, CA 94305 USA.
RP Banerjee, S (reprint author), Cornell, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA.
EM sbanerjee@cornell.edu; plofgren@cs.stanford.edu
FU DARPA GRAPHS program [FA9550-12-1-0411]; NSF [1447697]; NPSC fellowship
FX Research supported by the DARPA GRAPHS program via grant
   FA9550-12-1-0411, and by NSF grant 1447697. Peter Lofgren was supported
   by an NPSC fellowship. Thanks to Ashish Goel and other members of the
   Social Algorithms Lab at Stanford for many helpful discussions.
CR Andersen Reid, 2006, IEEE FOCS 06
   Andersen Reid, 2007, ALGORITHMS MODELS WE
   Athreya Krishna B, 2003, SANKHYA, P763
   Banerjee Siddhartha, 2015, TECHNICAL REPORT
   Boldi Paolo, 2011, ACM WWW 11
   Chung F, 2007, P NATL ACAD SCI USA, V104, P19735, DOI 10.1073/pnas.0708838104
   Doeblin Wolfgang, 1940, ANN SCI ECOLE NORM S, V57, P61
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Goldreich Oded, 2011, STUDIES COMPLEXITY C
   Kale Satyen, 2008, IEEE FOCS 08
   Kloster Kyle, 2014, ACM SIGKDD 14
   Lee C. E., 2013, ADV NEURAL INFORM PR, P1376
   Lempel R, 2000, COMPUT NETW, V33, P387, DOI 10.1016/S1389-1286(00)00034-7
   Lofgren Peter, 2014, ACM SIGKDD 14
   Mislove A., 2007, P 5 ACM US INT MEAS
   Motwani R, 2007, LECT NOTES COMPUT SC, V4596, P53
   Negahban Sahand, 2012, ADV NEURAL INFORM PR, P2474
   Page L, 1999, PAGERANK CITATION RA
   Sarkar P., 2008, P 25 INT C MACH LEAR, P896
   Steinhardt Jacob, 2015, ICML 15
   Takac Lubos, 2012, INT SCI C WORKSH PRE
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101074
DA 2019-06-15
ER

PT S
AU Bardenet, R
   Titsias, MK
AF Bardenet, Remi
   Titsias, Michalis K.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Inference for determinantal point processes without spectral knowledge
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Determinantal point processes (DPPs) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel K. DPPs possess desirable properties, such as exact sampling or analyticity of the moments, but learning the parameters of kernel K through likelihood-based inference is not straightforward. First, the kernel that appears in the likelihood is not K, but another kernel L related to K through an often intractable spectral decomposition. This issue is typically bypassed in machine learning by directly parametrizing the kernel L, at the price of some interpretability of the model parameters. We follow this approach here. Second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a DPP over a finite set of objects, and the form of a Fredholm determinant in the case of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood of a DPP, both for finite and continuous domains. Unlike previous work, our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variational inference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.
C1 [Bardenet, Remi] Univ Lille, CNRS, Lille, France.
   [Bardenet, Remi] Univ Lille, CRIStAL, UMR 9189, Lille, France.
   [Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
RP Bardenet, R (reprint author), Univ Lille, CNRS, Lille, France.
EM remi.bardenet@gmail.com; mtitsias@aueb.gr
FU EPSRC [EP/I017909/1]; ANR [ANR-13-BS-03-0006-01]
FX We would like to thank Adrien Hardy for useful discussions and Emily Fox
   for kindly providing access to the diabetic neuropathy dataset. RB was
   funded by a research fellowship through the 2020 Science programme,
   funded by EPSRC grant number EP/I017909/1, and by ANR project
   ANR-13-BS-03-0006-01.
CR Affandi R. H., 2013, P C ART INT STAT AIS
   Affandi R. H., 2014, P INT C MACH LEARN I
   Bornemann F, 2010, MATH COMPUT, V79, P871, DOI 10.1090/S0025-5718-09-02280-7
   Cristianini N., 2004, KERNEL METHODS PATTE
   Daley D. J., 2003, INTRO THEORY POINT P
   Devroye L, 1986, NONUNIFORM RANDOM VA
   Fasshauer G. E., 2012, SIAM J SCI COMPUTING, V34
   Gillenwater Jennifer A, 2014, ADV NEURAL INFORM PR
   Gohberg I., 1990, CLASSES LINEAR OPERA, VI
   Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737
   Hansen N, 2006, NEW EVOLUTIONARY COM
   Hough J. B., 2006, PROBABILITY SURVEYS
   Kulesza A., 2012, FDN TRENDS MACHINE L
   Lavancier F., 2014, J ROYAL STAT SOC
   MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855
   Mariet Z., 2015, ADV NEURAL INFORM SY
   Michalis K., 2009, AISTATS, V5
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robert C. P., 2004, MONTE CARLO STAT MET
   Seiler E., 1975, P NATL ACAD SCI
   Simon B, 2005, TRACE IDEALTHEIR A
   Waller LA, 2011, STAT MED, V30, P2827, DOI 10.1002/sim.4315
   Zou J. Y., 2012, ADV NEURAL INFORM PR
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101070
DA 2019-06-15
ER

PT S
AU Bareinboim, E
   Forney, A
   Pearl, J
AF Bareinboim, Elias
   Forney, Andrew
   Pearl, Judea
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bandits with Unobserved Confounders: A Causal Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MULTIARMED BANDIT
AB The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.
C1 [Bareinboim, Elias] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Forney, Andrew; Pearl, Judea] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.
RP Bareinboim, E (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM eb@purdue.edu; forns@cs.ucla.edu; judea@cs.ucla.edu
CR Agrawal S., 2011, ABS11111797 CORR
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Bareinboim E., 2015, R460 UCLA COGN SYST
   Bubeck S., 2012, ABS12024473 CORR
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Busa-Fekete R., 2010, INT C MACH LEARN, P143
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Dudik Miroslav, 2011, ABS11062369 CORR
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Fisher RA, 1951, DESIGN EXPT
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Ortega PA, 2010, J ARTIF INTELL RES, V38, P475, DOI 10.1613/jair.3062
   Pearl J., 2000, CAUSALITY MODELS REA
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874
   Seldin Yevgeny, 2014, INT C MACH LEARN
   Shpitser Ilya, 2007, P 23 C UNC ART INT, P352
   Slivkins A, 2014, J MACH LEARN RES, V15, P2533
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100060
DA 2019-06-15
ER

PT S
AU Bekker, J
   Davis, J
   Choi, A
   Darwiche, A
   Van den Broeck, G
AF Bekker, Jessa
   Davis, Jesse
   Choi, Arthur
   Darwiche, Adnan
   Van den Broeck, Guy
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Tractable Learning for Complex Probability Queries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFERENCE; MODELS
AB Tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient. However, the particular class of queries that is tractable depends on the model and underlying representation. Usually this class is MPE or conditional probabilities Pr(x vertical bar y) for joint assignments x, y. We propose a tractable learner that guarantees efficient inference for a broader class of queries. It simultaneously learns a Markov network and its tractable circuit representation, in order to guarantee and measure tractability. Our approach differs from earlier work by using Sentential Decision Diagrams (SDD) as the tractable language instead of Arithmetic Circuits (AC). SDDs have desirable properties, which more general representations such as ACs lack, that enable basic primitives for Boolean circuit compilation. This allows us to support a broader class of complex probability queries, including counting, threshold, and parity, in polytime.
C1 [Bekker, Jessa; Davis, Jesse] Katholieke Univ Leuven, Leuven, Belgium.
   [Choi, Arthur; Darwiche, Adnan; Van den Broeck, Guy] Univ Calif Los Angeles, Los Angeles, CA USA.
RP Bekker, J (reprint author), Katholieke Univ Leuven, Leuven, Belgium.
EM jessa.bekker@cs.kuleuven.be; jesse.davis@cs.kuleuven.be;
   aychoi@cs.ucla.edu; darwiche@cs.ucla.edu; guyvdb@cs.ucla.edu
OI Bekker, Jessa/0000-0003-1928-7374; Van den Broeck,
   Guy/0000-0003-3434-2503
FU IWT [SB/141744]; Research Fund KU Leuven [OT/11/051, C22/15/015]; EU FP7
   Marie Curie CIG [294068]; FWO-Vlaanderen [G.0356.12]; NSF [IIS-1514253];
   ONR [N00014-12-1-0423]; IWT (SBO-HYMOP)
FX We thank Songbai Yan for prior collaborations on related projects. JB is
   supported by IWT (SB/141744). JD is partially supported by the Research
   Fund KU Leuven (OT/11/051, C22/15/015), EU FP7 Marie Curie CIG
   (#294068), IWT (SBO-HYMOP) and FWO-Vlaanderen (G.0356.12). AC and AD are
   partially supported by NSF (#IIS-1514253) and ONR (#N00014-12-1-0423).
CR Bach F. R., 2001, ADV NEURAL INFORM PR, P569
   Buchman D., 2015, P AAAI
   Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002
   Chechetka A., 2007, ADV NEURAL INFORM PR, P273
   Chen SM, 2014, J ARTIF INTELL RES, V49, P601, DOI 10.1613/jair.4218
   Choi A., 2015, P IJCAI
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Darwiche A., 2011, P INT JOINT C ART IN, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143
   DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021
   Domingos P., 2014, ICML WORKSH LEARN TR
   Gens R., 2013, P 30 INT C MACH LEAR, V28, P873
   Kisa D., 2014, KR
   Krause C., 2005, P IJCAI
   Kulesza A., 2012, FDN TRENDS MACHINE L
   Lowd D., 2013, J MACHINE LEARNING R, P406
   Lowd Daniel, 2014, J MACHINE LEARNING R, V15
   Maua DD, 2013, ARTIF INTELL, V205, P30, DOI 10.1016/j.artint.2013.10.002
   Narasimhan M., 2004, P UAI
   Niepert M., 2014, P ICML
   Niepert M., 2014, P AAAI
   PARBERRY I, 1989, NEURAL NETWORKS, V2, P59, DOI 10.1016/0893-6080(89)90015-4
   Park J. D., 2002, P UAI
   Rahman Tahrima, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P630, DOI 10.1007/978-3-662-44851-9_40
   Rooshenas A., 2014, P 31 INT C MACH LEAR, V32, P710
   van der Gaag L. C., 2004, 20 C UNC ART INT, P569
   Van Haaren J., 2015, MACHINE LEARNING
   Zhang NL, 2004, J MACH LEARN RES, V5, P697
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100044
DA 2019-06-15
ER

PT S
AU Bengio, S
   Vinyals, O
   Jaitly, N
   Shazeer, N
AF Bengio, Samy
   Vinyals, Oriol
   Jaitly, Navdeep
   Shazeer, Noam
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Scheduled Sampling for Sequence Prediction with Recurrent Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.
C1 [Bengio, Samy; Vinyals, Oriol; Jaitly, Navdeep; Shazeer, Noam] Google Res, Mountain View, CA 94043 USA.
RP Bengio, S (reprint author), Google Res, Mountain View, CA 94043 USA.
EM bengio@google.com; vinyals@google.com; ndjaitly@google.com;
   noam@google.com
CR Bahdanau  D., 2015, INT C LEARN REPR ICL
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2009, P INT C MACH LEARN I
   Chorowski J., 2014, ARXIV14121602
   Collins M., 2004, P ASS COMPUT LING AC
   Cui Y., 2015, MICROSOFT COCO CAPTI
   Donahue J., 2015, IEEE C COMP VIS PATT
   Goldberg Y., 2012, P COLING
   Hal Daume III, 2009, MACHINE LEARNING J
   Hochreiter  S, 1997, NEURAL COMPUTATION, V9
   Hovy E. H., 2006, P HUM LANG TECHN C N, P57
   Jaitly N., 2014, THESIS
   Karpathy A., 2015, IEEE C COMP VIS PATT
   Kiros R., 2015, TACL
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Lin T.-Y., 2014, ARXIV14050312
   loffe S., 2015, P INT C MACH LEARN I
   Mao  Junhua, 2015, INT C LEARN REPR ICL
   Povey Daniel, 2011, IEEE 2011 WORKSH AUT
   Ross S., 2011, P WORKSH ART INT STA
   Sutskever  I., 2014, ADV NEURAL INFORM PR
   Vedantam R., 2015, IEEE C COMP VIS PATT
   Venkatraman A., 2015, 29 AAAI C ART INT AA
   Vinyals O., 2015, IEEE C COMP VIS PATT
   Vinyals  Oriol, 2014, ARXIV14127449
   Zhou GY, 2015, IEEE COMPUT SOC CONF
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102104
DA 2019-06-15
ER

PT S
AU Berglund, M
   Raiko, T
   Honkala, M
   Karkkainen, L
   Vetek, A
   Karhunen, J
AF Berglund, Mathias
   Raiko, Tapani
   Honkala, Mikko
   Karkkainen, Leo
   Vetek, Akos
   Karhunen, Juha
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bidirectional Recurrent Neural Networks as Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult. Recently, two different frameworks, GSN and NADE, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible. As far as we know, neither GSN or NADE have been studied in the context of time series before. As an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods.
C1 [Berglund, Mathias; Raiko, Tapani; Karhunen, Juha] Aalto Univ, Helsinki, Finland.
   [Honkala, Mikko; Karkkainen, Leo; Vetek, Akos] Nokia Labs, Espoo, Finland.
RP Berglund, M (reprint author), Aalto Univ, Helsinki, Finland.
FU Nokia; Academy of Finland
FX We thank KyungHyun Cho and Yoshua Bengio for useful discussions. The
   software for the simulations for this paper was based on Theano [3, 7].
   Nokia has supported Mathias Berglund and the Academy of Finland has
   supported Tapani Raiko.
CR Bahdanau D., 2015, P INT C LEARN REPR I
   Baldi P, 1999, BIOINFORMATICS, V15, P937, DOI 10.1093/bioinformatics/15.11.937
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bayer Justin, 2014, ARXIV14117610
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Brakel P, 2013, J MACH LEARN RES, V14, P2771
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow I., 2013, ADV NEURAL INFORM PR, V26, P548
   Graves A., 2013, ARXIV13035778
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Haykin S. S., 2009, NEURAL NETWORKS LEAR, V3
   Hermans M., 2013, ADV NEURAL INFORM PR, P190
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Koutnik J., 2014, P 31 INT C MACH LEAR
   Lewandowski N. B., 2012, P 29 INT C MACH LEAR, P1159
   Maas A. L., 2014, ARXIV14082873
   Mikolov T., 2014, ARXIV14127753
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Raiko T, 2001, 8TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, VOLS 1-3, PROCEEDING, P822
   Raiko T., 2015, INT C LEARN REPR ICL
   Remes U, 2011, IEEE SIGNAL PROC LET, V18, P563, DOI 10.1109/LSP.2011.2163508
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   Uria Benigno, 2014, P 31 INT C MACH LEAR, V32, P467
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100019
DA 2019-06-15
ER

PT S
AU Beygelzimer, A
   Hazan, E
   Kale, S
   Luo, HP
AF Beygelzimer, Alina
   Hazan, Elad
   Kale, Satyen
   Luo, Haipeng
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Gradient Boosting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS
AB We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.
C1 [Beygelzimer, Alina; Kale, Satyen] Yahoo Labs, New York, NY 10036 USA.
   [Hazan, Elad; Luo, Haipeng] Princeton Univ, Princeton, NJ 08540 USA.
RP Beygelzimer, A (reprint author), Yahoo Labs, New York, NY 10036 USA.
EM beygel@yahoo-inc.com; ehazan@cs.princeton.edu; satyen@yahoo-inc.com;
   haipengl@cs.princeton.edu
CR Bartlett PL, 2007, J MACH LEARN RES, V8, P2347
   Beygelzimer Alina, 2015, ICML
   Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439
   Chen Shang-Tse, 2014, ICML
   Chen Shang-Tse, 2012, ICML
   Collins Michael, 2000, COLT
   Duffy N, 2002, MACH LEARN, V47, P153, DOI 10.1023/A:1013685603443
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Grabner H., 2006, IEEE C COMP VIS PATT, V1, P260, DOI DOI 10.1109/CVPR.2006.215
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   HASTIE T, 1990, GEN ADDITIVE MODELS
   Hastie T, 2001, ELEMENTS STAT LEARNI
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2014, J MACH LEARN RES, V15, P2489
   Liu X., 2007, P IEEE INT C COMP VI, P1
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mason  L., 2000, NIPS
   Oza N., 2001, ARTIF INTELL, P105
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Telgarsky Matus, 2013, COLT
   Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255
   Zinkevich Martin, 2003, ICML
NR 24
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100093
DA 2019-06-15
ER

PT S
AU Bhatia, K
   Jain, P
   Kar, P
AF Bhatia, Kush
   Jain, Prateek
   Kar, Purushottam
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust Regression via Hard Thresholding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X is an element of R-pxn and an underlying model w*, the response vector is generated as y = X(T)w* + b where b is an element of R-n is the corruption vector supported over at most C . n coordinates. Existing exact recovery results for RLSR focus solely on L-1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X.
   In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* is an element of R-p.
   Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery. and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L-1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20x faster than the best L-1 solver.
C1 [Bhatia, Kush; Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India.
   [Kar, Purushottam] Indian Inst Technol Kanpur, Kanpur, Uttar Pradesh, India.
RP Bhatia, K (reprint author), Microsoft Res, Bengaluru, Karnataka, India.
EM t-kushb@microsoft.com; prajain@microsoft.com; purushot@cse.iitk.ac.in
CR Blumensath T, 2011, IEEE T INFORM THEORY, V57, P4660, DOI 10.1109/TIT.2011.2146550
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Chen Yudong, 2013, 30 INT C MACH LEARN
   Garg Rahul, 2009, 26 INT C MACH LEARN
   Jain Prateek, 2014, 28 ANN C NEUR INF PR
   Laurent B, 2000, ANN STAT, V28, P1302
   Legendre Adrien-Marie, 1805, SOURCE BOOK MATH, P576
   McWilliams B., 2014, 28 ANN C NEUR INF PR
   Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347
   Rousseeuw P. J., 1987, ROBUST REGRESSION OU
   Rousseeuw PJ, 2006, DATA MIN KNOWL DISC, V12, P29, DOI 10.1007/s10618-005-0024-4
   ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718
   She Yiyuan, ARXIV10062592STATME
   Studer C, 2012, IEEE T INFORM THEORY, V58, P3115, DOI 10.1109/TIT.2011.2179701
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Wright J, 2010, IEEE T INFORM THEORY, V56, P3540, DOI 10.1109/TIT.2010.2048473
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang Allen Y., 2012, CORR
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103048
DA 2019-06-15
ER

PT S
AU Bhatia, K
   Jain, H
   Kar, P
   Varma, M
   Jain, P
AF Bhatia, Kush
   Jain, Himanshu
   Kar, Purushottam
   Varma, Manik
   Jain, Prateek
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sparse Local Embeddings for Extreme Multi-label Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies, or scale to large problems as the low rank assumption is violated in most real world applications.
   In this paper we develop the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring )tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors.
   We conducted extensive experiments on several real-world, as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based )by as much as 35%) as well as tree-based )by as much as 6%) methods. SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.
C1 [Bhatia, Kush; Varma, Manik; Jain, Prateek] Microsoft Res, Bengaluru, India.
   [Jain, Himanshu] Indian Inst Technol Delhi, Delhi, India.
   [Kar, Purushottam] Indian Inst Technol Kanpur, Kanpur, Uttar Pradesh, India.
   [Kar, Purushottam] Microsoft Res India, Bengaluru, India.
RP Bhatia, K (reprint author), Microsoft Res, Bengaluru, India.
EM t-kushb@microsoft.com; himanshu.j689@gmail.com; purushot@cse.iitk.ac.in;
   manik@microsoft.com; prajain@microsoft.com
FU Google India PhD Fellowship at IIT Delhi
FX We are grateful to Abhishek Kadian for helping with the experiments.
   Himanshu Jain is supported by a Google India PhD Fellowship at IIT Delhi
CR Agrawal R., 2013, P 22 INT C WORLD WID, P13
   [Anonymous], 2014, WIKIPEDIA DATASET 4
   Balasubramanian Krishnakumar, 2012, ICML
   Bi Wei, 2013, ICML
   Chen Y., 2012, NIPS, P1538
   Cisse M, 2013, ADV NEURAL INFORM PR, V26, P1851
   Feng C.-S., 2011, JMLR, V20
   Hariharan B., 2012, ML
   Hsu  D., 2009, NIPS
   Jain P., 2010, ADV NEURAL INFORM PR, V23, P937
   Ji S., 2008, KDD
   Kar P., 2013, ICML
   Katakis I., 2008, P ECML PKDD 2008 DIS
   Leskovec J., 2014, SNAP DATASETS STANFO
   Lin Z., 2014, ICML, V2014, P325
   Makadia  A., 2013, ICML
   Mencia E. L., 2008, ECML PKDD
   Ng Andrew Y., 2002, NIPS
   Prabhu Y., 2014, P 20 ACM SIGKDD INT, P263
   Shaw B., 2007, JMLR WORKSHOP C P, P460
   SNOEK CGM, 2006, ACM MULTIMEDIA
   Sprechmann P., 2013, NIPS
   Tai F., 2010, WORKSH P LEARN MULT
   Tsoumakas G., 2008, ECML PKDD
   Weinberger K. Q., 2006, AAAI, P1683
   Weston  J., 2011, IJCAI
   Wetzker R., 2008, MIN SOC DAT MSODA WO, P26
   Yu H. F., 2014, ICML
   Zhang Y, 2011, P 14 INT C ART INT S, P873
   Zubiaga A., 2009, ENHANCING NAVIGATION
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103007
DA 2019-06-15
ER

PT S
AU Bhattacharya, BB
   Valiant, G
AF Bhattacharya, Bhaswar B.
   Valiant, Gregory
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Testing Closeness With Unequal Sized Samples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ENTROPY
AB We consider the problem of testing whether two unequal-sized samples were drawn from identical distributions, versus distributions that differ significantly. Specifically, given a target error parameter epsilon > 0, m(1) independent draws from an unknown distribution p with discrete support, and m(2) draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case that parallel to p - q parallel to(1) >= epsilon. If p and q are supported on at most n elements, then our test is successful with high probability provided m(1) >= n(2/3)/epsilon(4/3) and m(2) = Omega (max{n/root m(1)epsilon(2), root n/epsilon(2)}). We show that this tradeoff is information theoretically optimal throughout this range in the dependencies on all parameters, n, m(1), and epsilon, to constant factors for worst-case distributions. As a consequence, we obtain an algorithm for estimating the mixing time of a Markov chain on n states up to a log n factor that uses (O) over tilde (n(3/2)T(mix)) queries to a "next node" oracle. The core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic and on natural language data. We believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems.
C1 [Bhattacharya, Bhaswar B.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Valiant, Gregory] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Bhattacharya, BB (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
EM bhaswar@stanford.edu; valiant@stanford.edu
FU NSF CAREER Award [CCF-1351108]
FX Supported in part by NSF CAREER Award CCF-1351108
CR Acharya J., 2012, COLT
   Acharya J., 2015, NIPS
   Acharya J., 2011, COLT
   Acharya J, 2014, IEEE INT SYMP INFO, P3200, DOI 10.1109/ISIT.2014.6875425
   Bar-Yossef Z., 2001, STOC
   Batu T., 2001, FOCS
   Batu T., 2000, FOCS
   Batu T., 2005, SIAM J COMPUTING
   Chan Siu-On, 2014, P 25 ANN ACM SIAM S, P1193, DOI DOI 10.1137/1.9781611973402.88
   Charikar M., 2000, S PRINC DAT SYST POD
   Czumaj A., 2007, FOCS
   Goldreich O., 2000, ECCC, VTR00-020
   Guha S., 2006, S DISCR ALG SODA
   Hsu D., 2015, NIPS
   Kale S, 2008, LECT NOTES COMPUT SC, V5125, P527, DOI 10.1007/978-3-540-70575-8_43
   Keinan A, 2012, SCIENCE, V336, P740, DOI 10.1126/science.1217283
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Nachmias A., 2007, ELECT C COMPUTATIONA, V14
   Nelson MR, 2012, SCIENCE, V337, P100, DOI 10.1126/science.1217876
   Paninski L, 2004, IEEE T INFORM THEORY, V50, P2200, DOI 10.1109/TIT.2004.833360
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987
   Raskhodnikova S, 2009, SIAM J COMPUT, V39, P813, DOI 10.1137/070701649
   Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052
   SINCLAIR A, 1989, INFORM COMPUT, V82, P93, DOI 10.1016/0890-5401(89)90067-9
   Tennessen JA, 2012, SCIENCE, V337, P64, DOI 10.1126/science.1219240
   Valiant G., 2011, STOC
   Valiant G., 2013, NIPS
   Valiant G, 2014, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2014.14
   Valiant P., 2008, STOC
   Valiant  P., 2008, THESIS
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102057
DA 2019-06-15
ER

PT S
AU Bitzer, S
   Kiebel, SJ
AF Bitzer, Sebastian
   Kiebel, Stefan J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Brain Uses Reliability of Stimulus Information when Making
   Perceptual Decisions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID AREA
AB In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made. However, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability. We here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability. We then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings. Our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.
C1 [Bitzer, Sebastian; Kiebel, Stefan J.] Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany.
RP Bitzer, S (reprint author), Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany.
EM sebastian.bitzer@tu-dresden.de; stefan.kiebel@tu-dresden.de
CR Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021
   Bitzer S, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00102
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Churchland AK, 2008, NAT NEUROSCI, V11, P693, DOI 10.1038/nn.2123
   Churchland AK, 2011, NEURON, V69, P818, DOI 10.1016/j.neuron.2010.12.037
   Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429
   Deneve S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00075
   Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038
   Hanks Timothy D., 2015, NATURE
   Huang Y, 2012, ADV NEURAL INFORM PR, V25, P1277
   JOHN ID, 1967, AUST J PSYCHOL, V19, P27, DOI 10.1080/00049536708255558
   Luce P. D, 1986, OXFORD PSYCHOL SERIE, V8
   NEWSOME WT, 1988, J NEUROSCI, V8, P2201
   Pilly PK, 2009, VISION RES, V49, P1599, DOI 10.1016/j.visres.2009.03.019
   Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976
   Roitman JD, 2002, J NEUROSCI, V22, P9475
   Shadlen MN, 2008, BETTER CONSCIOUS DEC
   Solway A, 2012, PSYCHOL REV, V119, P120, DOI 10.1037/a0026435
   Wald A., 1947, SEQUENTIAL ANAL
   Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9
   Yu AJ, 2005, ADV NEURAL INFORM PR, P1577
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101049
DA 2019-06-15
ER

PT S
AU Bourdoukan, R
   Deneve, S
AF Bourdoukan, Ralph
   Deneve, Sophie
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Enforcing balance allows local supervised learning in spiking recurrent
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NEURON
AB To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.
C1 [Bourdoukan, Ralph; Deneve, Sophie] ENS Paris, Grp Neural Theory, Rue Ulm 29, Paris, France.
RP Bourdoukan, R (reprint author), ENS Paris, Grp Neural Theory, Rue Ulm 29, Paris, France.
EM ralph.bourdoukan@gmail.com; sophie.deneve@ens.fr
FU ERC grant FP7-PREDISPIKE; James McDonnell Foundation Award - Human
   Cognition;  [ANR-10-LABX-0087 IEC];  [ANR-10-IDEX-0001-02 PSL]
FX This work was supported by ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02
   PSL, ERC grant FP7-PREDISPIKE and the James McDonnell Foundation Award -
   Human Cognition.
CR Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bourdoukan R, 2012, ADV NEURAL INFORM PR, V25, P2285
   Brunel N, 2000, J PHYSIOLOGY-PARIS, V94, P445, DOI 10.1016/S0928-4257(00)01084-6
   Chen C, 1995, CELL, V83, P1233, DOI 10.1016/0092-8674(95)90148-5
   ECCLES JC, 1966, J PHYSIOL-LONDON, V182, P268, DOI 10.1113/jphysiol.1966.sp007824
   Gutig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Jaeger H., 2001, BONN GER GER NATL RE, V148, P2001
   Kawato M, 1999, CURR OPIN NEUROBIOL, V9, P718, DOI 10.1016/S0959-4388(99)00028-8
   Knudsen E.I., 1994, J NEUROSCI, V14
   Lackner JR, 1998, J NEUROPHYSIOL, V80, P546
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rumelhart D. E., 1988, COGNITIVE MODELING, V5
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Thalmeier D, LEARNING UNIVERSAL C
   vanVreeswijk C, 1996, SCIENCE, V274, P1724
   Vertechi P., 2014, ADV NEURAL INFORM PR, P3653
   Watanabe M, 2011, EUR J NEUROSCI, V34, P1697, DOI 10.1111/j.1460-9568.2011.07894.x
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102096
DA 2019-06-15
ER

PT S
AU Briol, FX
   Oates, CJ
   Girolami, M
   Osborne, MA
AF Briol, Francois-Xavier
   Oates, Chris J.
   Girolami, Mark
   Osborne, Michael A.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with
   Theoretical Guarantees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB There is renewed interest in formulating integration as a statistical inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging Bayesian model choice problem in cellular biology.
C1 [Briol, Francois-Xavier; Girolami, Mark] Univ Warwick, Dept Stat, Coventry, W Midlands, England.
   [Oates, Chris J.] Univ Technol Sydney, Sch Math & Phys Sci, Sydney, NSW, Australia.
   [Osborne, Michael A.] Univ Oxford, Dept Engn Sci, Oxford, England.
RP Briol, FX (reprint author), Univ Warwick, Dept Stat, Coventry, W Midlands, England.
EM f-x.briol@warwick.ac.uk; christopher.oates@uts.edu.au;
   m.girolami@warwick.ac.uk; mosb@robots.ox.ac.uk
FU EPSRC [EP/L016710/1, EP/D002060/1, EP/J016934/1]; EU grant [EU/259348];
   Royal Society Wolfson Research Merit Award; EPSRC Established Career
   Fellowship
FX The authors are grateful for discussions with Simon Lacoste-Julien, Simo
   Sarkka, Arno Solin, Dino Sejdinovic, Tom Gunter and Mathias Cronjager.
   FXB was supported by EPSRC [EP/L016710/1]. CJO was supported by EPSRC
   [EP/D002060/1]. MG was supported by EPSRC [EP/J016934/1], an EPSRC
   Established Career Fellowship, the EU grant [EU/259348] and a Royal
   Society Wolfson Research Merit Award.
CR Bach F., 2012, P 29 INT C MACH LEAR, P1359
   Bach F. R., 2015, ARXIV150206800
   Chen Y., 2015, J MACHINE LEARNING R
   Chen YH, 2010, ADV INTELL SOFT COMP, V66, P109, DOI 10.1145/1866919.1866935
   Conrad P., 2015, ARXIV150604592
   Diaconis P., 1988, STATISTICAL DECISION, V1, P163
   Dick J., 2010, DIGITAL NETS SEQUENC
   DUNN JC, 1980, SIAM J CONTROL OPTIM, V18, P473, DOI 10.1137/0318035
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Garber D., 2015, P 32 INT C MACH LEAR, V37, P541
   Ghahramani Z, 2003, ADV NEURAL INFORM PR, P489
   Gunter T., 2014, ADV NEURAL INFORM PR
   Hamrick J.B., 2013, NIPS 2013 WORKSH BAY
   Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142
   Hennig P, 2015, SIAM J OPTIMIZ, V25, P234, DOI 10.1137/140955501
   HUSZAR F., 2012, P 28 C ANN C UNC ART, P377
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   Lacoste-Julien S., 2015, P 18 INT C ART INT S, V38, P544
   O'Hagan A., 1984, J ROYAL STAT SOC D, V36, P247
   Oates C.J., 2015, ARXIV14102392
   Oates CJ, 2014, BIOINFORMATICS, V30, pI468, DOI 10.1093/bioinformatics/btu452
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   Osborne M, 2012, P 15 INT C ART INT S, P832
   Owen A. B., 2015, NUMER MATH, P1
   Sarkka S., 2015, ARXIV150405994
   Schober M., 2014, ADV NEURAL INFORM PR, P739
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101009
DA 2019-06-15
ER

PT S
AU Brown, N
   Sandholm, T
AF Brown, Noam
   Sandholm, Tuomas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regret-Based Pruning in Extensive-Form Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Counterfactual Regret Minimization (CFR) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games. CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set. We introduce an improvement to CFR that prunes any path of play in the tree, and its descendants, that has negative regret. It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration. The new algorithm maintains CFR's convergence guarantees while making iterations significantly faster-even if previously known pruning techniques are used in the comparison. This improvement carries over to CFR+, a recent variant of CFR. Experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.
C1 [Brown, Noam] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.
RP Brown, N (reprint author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.
EM noamb@cmu.edu; sandholm@cs.cmu.edu
FU National Science Foundation [IIS-1320620, IIS-1546752]
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1320620 and IIS-1546752, as well as XSEDE
   computing resources provided by the Pittsburgh Supercomputing Center.
CR Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown Noam, 2015, P 2015 INT C AUT AG
   Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2
   Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324
   Hoda S, 2010, MATH OPER RES, V35, P494, DOI 10.1287/moor.1100.0452
   Jackson Eric, 2014, AAAI WORKSH COMP POK
   Johanson Michael, 2016, AAAI C ART INT AAAI
   Kroer C., 2015, P ACM C EC COMP EC
   Lanctot Marc, 2009, ADV NEURAL INFORM PR, P1078
   Pays  FranAgois, 2014, AAAI COMP POK WORKSH
   Sandholm T, 2010, AI MAG, V31, P13, DOI 10.1609/aimag.v31i4.2311
   Southey F., 2005, P 21 C UNC ART INT, P550
   Tammelin O., 2014, ARXIV14075042
   Tammelin Oskari, 2015, IJCAI, V2015
   Waugh Kevin, 2009, INT C AUT AG MULT SY
   Zinkevich  M., 2007, P ANN C NEUR INF PRO
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102059
DA 2019-06-15
ER

PT S
AU Bubeck, S
   Eldan, R
   Lehec, J
AF Bubeck, Sebastien
   Eldan, Ronen
   Lehec, Joseph
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Finite-Time Analysis of Projected Langevin Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We analyze the projected Langevin Monte Carlo (LMC) algorithm, a close cousin of projected Stochastic Gradient Descent (SGD). We show that LMC allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood. This gives the first Markov chain to sample from a log-concave distribution with a first-order oracle, as the existing chains with provable guarantees (lattice walk, ball walk and hit-and-run) require a zeroth-order oracle. Our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand SGD and its variants.
C1 [Bubeck, Sebastien] Microsoft Res, New York, NY 10011 USA.
   [Eldan, Ronen] Weizmann Inst Sci, Rehovot, Israel.
   [Lehec, Joseph] Univ Paris 09, Paris, France.
RP Bubeck, S (reprint author), Microsoft Res, New York, NY 10011 USA.
EM sebubeck@microsoft.com; roneneldan@gmail.com; lehec@ceremade.dauphine.fr
CR Ahn S., 2012, ICML 2012
   Bach F., ADV NEURAL INFORM PR, V26, P773
   Cousins B., 2014, ARXIV14096011
   Dalalyan A., 2014, ARXIV14127392
   DYER M, 1991, J ACM, V38, P1, DOI 10.1145/102782.102783
   Kannan R, 2012, MATH OPER RES, V37, P1, DOI 10.1287/moor.1110.0519
   Lovasz L, 2006, SIAM J COMPUT, V35, P985, DOI 10.1137/S009753970544727X
   Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   PFLUG GC, 1986, SIAM J CONTROL OPTIM, V24, P655, DOI 10.1137/0324039
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Skorokhod A. V., 1961, THEOR PROBAB APPL, V6, P264, DOI [DOI 10.1137/1106035, 10.1137/1106035]
   Tanaka  H., 1979, HIROSHIMA MATH J, V9, P163
   Welling M., 2011, ICML 2011
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102082
DA 2019-06-15
ER

PT S
AU Busa-Fekete, R
   Szorenyi, B
   Dembczynski, K
   Hullermeier, E
AF Busa-Fekete, Robert
   Szorenyi, Balazs
   Dembczynski, Krzysztof
   Huellermeier, Eyke
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online F-Measure Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.
C1 [Busa-Fekete, Robert; Huellermeier, Eyke] Univ Paderborn, Dept Comp Sci, Paderborn, Germany.
   [Szorenyi, Balazs] Technion, Haifa, Israel.
   [Szorenyi, Balazs] MTA SZTE Res Grp Artificial Intelligence, Szeged, Hungary.
   [Dembczynski, Krzysztof] Poznan Univ Tech, Inst Comp Sci, Poznan, Poland.
RP Busa-Fekete, R (reprint author), Univ Paderborn, Dept Comp Sci, Paderborn, Germany.
EM busarobi@upb.de; szorenyibalazs@gmail.com;
   kdembczynski@cs.put.poznan.pl; eyke@upb.de
FU Polish National Science Centre [2013/09/D/ST6/03917]
FX Krzysztof Dembczynski is supported by the Polish National Science Centre
   under grant no. 2013/09/D/ST6/03917.
CR Amigo E, 2013, LECT NOTES COMPUT SC, V8138, P333, DOI 10.1007/978-3-642-40802-1_31
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Busa-Fekete R, 2013, MACH LEARN, V93, P261, DOI 10.1007/s10994-013-5360-9
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Devroye L., 1985, NONPARAMETRIC DENSIT
   Devroye L., 1996, PROBABILISTIC THEORY
   Gao W., 2013, P 30 INT C MACH LEAR, P906
   Hangya V., 2013, WORK NOT CLEF 2013 E
   Kar P., 2014, NIPS
   Nagarajan N., 2014, ADV NEURAL INFORM PR, P2744
   Narasimhan H., 2014, NIPS
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Tsuruokas Y., 2009, P JOINT C 47 ANN M A, V1, P477
   VANRIJSBERGEN CJ, 1974, J DOC, V30, P365, DOI 10.1108/eb026584
   Varadhan S. R. S., 2000, PROBABILITY THEORY
   Waegeman W, 2014, J MACH LEARN RES, V15, P3333
   Ye N., 2012, ICML
   Zhao MJ, 2013, J MACH LEARN RES, V14, P1033
   Zhao Peilin, 2011, P 28 INT C MACH LEAR, P233
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103032
DA 2019-06-15
ER

PT S
AU Bzdok, D
   Eickenberg, M
   Grisel, O
   Thirion, B
   Varoquaux, G
AF Bzdok, Danilo
   Eickenberg, Michael
   Grisel, Olivier
   Thirion, Bertrand
   Varoquaux, Gael
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Semi-Supervised Factored Logistic Regression for High-Dimensional
   Neuroimaging Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
DE Brain Imaging; Cognitive Science; Semi-Supervised Learning; Systems
   Biology
ID FMRI
AB Imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets. Existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks. However, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations. We therefore propose to blend representation modelling and task classification into a unified statistical learning problem. A multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder. We show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.
C1 [Bzdok, Danilo] INRIA, Parietal Team, Saclay, France.
   CEA, Neurospin, Gif Sur Yvette, France.
RP Bzdok, D (reprint author), INRIA, Parietal Team, Saclay, France.
EM danilo.bzdok@inria.fr; michael.eickenberg@inria.fr;
   olivier.grisel@inria.fr; bertrand.thirion@inria.fr;
   gael.varoquaux@inria.fr
FU European Union [604102]; German National Academic Foundation; MetaMRI
   associated team
FX The research leading to these results has received funding from the
   European Union Seventh Framework Programme (FP7/2007-2013) under grant
   agreement no. 604102 (Human Brain Project). Data were provided by the
   Human Connectome Project. Further support was received from the German
   National Academic Foundation (D.B.) and the MetaMRI associated team
   (B.T., G.V.).
CR Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014
   Amunts K, 2013, SCIENCE, V340, P1472, DOI 10.1126/science.1235381
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Barch DM, 2013, NEUROIMAGE, V80, P169, DOI 10.1016/j.neuroimage.2013.05.033
   Bastien F, 2012, ARXIV12115590
   Beckmann CF, 2005, PHILOS T ROY SOC B, V360, P1001, DOI 10.1098/rstb.2005.1634
   Bergstra J, 2010, P PYTH SCI COMP C SC, P3
   Biswal BB, 2010, P NATL ACAD SCI USA, V107, P4734, DOI 10.1073/pnas.0911855107
   Cole M. W., 2014, NEURON C, V83
   Fox MD, 2007, NAT REV NEUROSCI, V8, P700, DOI 10.1038/nrn2201
   Frackowiak R, 2015, PHILOS T R SOC B, V370, P20, DOI 10.1098/rstb.2014.0171
   Friston K. J., 1995, HUMAN BRAIN MAPPING, V2, P189, DOI DOI 10.1002/HBM.460020402
   Friston KJ, 1997, NEUROIMAGE, V6, P218, DOI 10.1006/nimg.1997.0291
   Gorgolewski Krzysztof, 2011, Front Neuroinform, V5, P13, DOI 10.3389/fninf.2011.00013
   Hertz J., 1991, INTRO THEORY NEURAL, V1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hipp J. F., 2015, CURR BIOL
   Le Q. V., 2011, ICA RECONSTRUCTION C, P1017
   Need Anna C, 2010, Dialogues Clin Neurosci, V12, P37
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pinel P, 2007, BMC NEUROSCI, V8, DOI 10.1186/1471-2202-8-91
   Poldrack RA, 2014, NAT NEUROSCI, V17, P1510, DOI 10.1038/nn.3818
   Schwartz Y, 2013, ADV NEURAL INFORM PR
   Smith SM, 2013, NEUROIMAGE, V80, P144, DOI 10.1016/j.neuroimage.2013.05.039
   Smith SM, 2009, P NATL ACAD SCI USA, V106, P13040, DOI 10.1073/pnas.0905267106
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Varoquaux G, 2011, LECT NOTES COMPUT SC, V6801, P562, DOI 10.1007/978-3-642-22092-0_46
   2009, PSYCHOL SCI, V20, P1364
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100014
DA 2019-06-15
ER

PT S
AU Campbell, T
   Straub, J
   Fisher, JW
   How, JP
AF Campbell, Trevor
   Straub, Julian
   Fisher, John W., III
   How, Jonathan P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Streaming, Distributed Variational Inference for Bayesian Nonparametrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.
C1 [Campbell, Trevor; How, Jonathan P.] MIT, LIDS, Cambridge, MA 02139 USA.
   [Straub, Julian; Fisher, John W., III] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Campbell, T (reprint author), MIT, LIDS, Cambridge, MA 02139 USA.
EM tdjc@mit.edu; jstraub@csailmit.edu; fisher@csailmit.edu; jhow@mit.edu
FU Office of Naval Research under ONR MURI [N000141110688]
FX This work was supported by the Office of Naval Research under ONR MURI
   grant N000141110688.
CR Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Broderick Tamara, 2013, ADV NEURAL INFORM PR, V26
   Bryant Michael, 2009, ADV NEURAL INFORM PR, V23
   Carvalho CM, 2010, BAYESIAN ANAL, V5, P709, DOI 10.1214/10-BA525
   Chang Jason, 2013, ADV NEURAL INFORM PR, V26
   Doshi-Velez Finale, 2009, P INT C MACH LEARN
   Dubey Avinava, 2014, P 30 C UNC ART INT
   EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699
   Griffiths Thomas L., 2005, ADV NEURAL INFORM PR, V22
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hughes Michael, 2013, ADV NEURAL INFORM PR, V26
   Jasra A, 2005, STAT SCI, V20, P50, DOI 10.1214/08834230500000016
   LeCun Yann, MNIST DATABASE HANDW
   Lin Dahua, 2013, ADV NEURAL INFORM PR, V26
   Miller JW, 2013, ADV NEURAL INFORM PR, V26
   Neiswanger Willie, 2014, P 30 C UNC ART INT
   Nobile A, 1994, THESIS
   PITMAN J, 1995, PROBAB THEORY REL, V102, P145, DOI 10.1007/BF01213386
   Stephens M, 2000, J ROY STAT SOC B, V62, P795, DOI 10.1111/1467-9868.00265
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Teh Yee Whye, 2010, ENCY MACHINE LEARNIN
   Trevor Campbell, 2014, P 30 C UNC ART INT
   Wang Chong, 2011, P 11 INT C ART INT S
   Wang Chong, 2012, ADV NEURAL INFORM PR, V25
   Xiao Jianxiong, SUN 397 IMAGE DATABA
   Zhang XL, 2014, J COMPUT GRAPH STAT, V23, P1143, DOI 10.1080/10618600.2013.870906
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102025
DA 2019-06-15
ER

PT S
AU Cao, W
   Li, J
   Tao, YF
   Li, ZZ
AF Cao, Wei
   Li, Jian
   Tao, Yufei
   Li, Zhize
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric, up to a small relative error. We study the topic under two standard settings-multi-armed bandits and hidden bipartite graphs-which differ in the nature of the input distributions. In the former setting, each distribution can be sampled (in the i.i.d. manner) an arbitrary number of times, whereas in the latter, each distribution is defined on a population of a finite size m (and hence, is fully revealed after m samples). For both settings, we prove lower bounds on the total number of samples needed, and propose optimal algorithms whose sample complexities match those lower bounds.
C1 [Cao, Wei; Li, Jian; Li, Zhize] Tsinghua Univ, Beijing, Peoples R China.
   [Tao, Yufei] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
RP Cao, W (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM cao-w13@mails.tsinghua.edu.cn; lijian83@mail.tsinghua.edu.cn;
   taoyf@cse.cuhk.edu.hk; zz-li14@mails.tsinghua.edu.cn
FU National Basic Research Program of China [2015CB358700, 2011CBA00300,
   2011CBA00301]; National NSFC [61202009, 61033001, 61361136003]; HKRGC
   [GRF 4168/13, GRF 142072/14]
FX Jian Li, Wei Cao, Zhize Li were supported in part by the National Basic
   Research Program of China grants 2015CB358700, 2011CBA00300,
   2011CBA00301, and the National NSFC grants 61202009, 61033001,
   61361136003. Yufei Tao was supported in part by projects GRF 4168/13 and
   GRF 142072/14 from HKRGC.
CR Amsterdamer Y, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P589, DOI 10.1145/2588555.2610514
   Audibert  J.-Y., 2010, COLT
   Bar-Yossef Z., 2002, THESIS
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059
   Chen S., 2014, ADV NEURAL INFORM PR, V26, P379
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212
   Mannor S, 2004, J MACH LEARN RES, V5, P623
   Parameswaran A, 2014, PROC VLDB ENDOW, V7, P685, DOI 10.14778/2732939.2732942
   Sheng C., 2012, TODS, V37, P12
   Wang JG, 2013, IEEE T KNOWL DATA EN, V25, P2245, DOI 10.1109/TKDE.2012.178
   Zhou Y., 2014, P 31 INT C MACH LEAR, P217
   Zhu ML, 2005, IEEE T KNOWL DATA EN, V17, P567, DOI 10.1109/TKDE.2005.65
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103065
DA 2019-06-15
ER

PT S
AU Carlson, DE
   Collins, E
   Hsieh, YP
   Carin, L
   Cevher, V
AF Carlson, David E.
   Collins, Edo
   Hsieh, Ya-Ping
   Carin, Lawrence
   Cevher, Volkan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Preconditioned Spectral Descent for Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that exploits the so far unused "geometry" in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADAgrad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.
C1 [Carlson, David E.] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Collins, Edo; Hsieh, Ya-Ping; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
   [Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RP Carlson, DE (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.
FU ARO; DARPA; ONR; European Commission [MIRG-268398]; European Commission
   under ERC Future Proof; Swiss Science Foundation [SNF 200021-146750, SNF
   CRSII2-147633]; NCCR Marvel; DOE; NGA
FX The research reported here was funded in part by ARO, DARPA, DOE, NGA
   and ONR, and in part by the European Commission under grants MIRG-268398
   and ERC Future Proof, by the Swiss Science Foundation under grants SNF
   200021-146750, SNF CRSII2-147633, and the NCCR Marvel. We thank the
   reviewers for their helpful comments.
CR Carlson D., 2016, IEEE J SPECIAL TOPIC
   Carlson D., 2015, AISTATS
   Cho K., 2013, NEURAL COMPUTATION
   Choromanska Anna, 2015, AISTATS
   Dauphin Y. N., ARXIV150204390
   Dauphin Yann N., 2014, NIPS
   Duchi J., 2010, JMLR
   Erhan D., 2010, JMLR
   Halko  N., 2011, SIAM REV
   Hinton G., 2010, PRACTICAL GUIDE TRAI
   Hinton G. E., 2002, NEURAL COMPUTATION
   Hinton G. E., 2006, NEURAL COMPUTATION
   Kelner J. A., 2013, ALMOST LINEAR TIME A
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Le Quoc, 2011, ICML
   Marlin B., 2010, ICML
   Martens J., 2010, AISTATS
   Martens James, 2015, ARXIV150305671
   Nair V., 2010, ICML
   Neal R M, 1998, ANNEALED IMPORTANCE
   Rokhlin V., 2010, SIAM J MATRIX ANAL A
   Salakhutdinov R., 2008, ICML
   Salakhutdinov R., 2009, AISTATS
   Schaul T., 2012, ARXIV12061106
   Smolensky P, 1986, INFORM PROCESSING DY
   Snoek  Jasper, 2012, NIPS
   Tieleman T., 2009, ICML
   Wan L., 2013, ICML
   Zeiler M.D., 2012, ARXIV12125701
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101055
DA 2019-06-15
ER

PT S
AU Carreira-Perpinan, MA
   Vladymyrov, M
AF Carreira-Perpinan, Miguel A.
   Vladymyrov, Max
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Fast, Universal Algorithm to Learn Parametric Nonlinear Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DIMENSIONALITY REDUCTION; NEURAL-NETWORKS
AB Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.
C1 [Carreira-Perpinan, Miguel A.] Univ Calif Merced, EECS, Merced, CA 95343 USA.
   [Vladymyrov, Max] UC Merced, Merced, CA USA.
   [Vladymyrov, Max] Yahoo Labs, Sunnyvale, CA USA.
RP Carreira-Perpinan, MA (reprint author), Univ Calif Merced, EECS, Merced, CA 95343 USA.
EM maxv@yahoo-inc.com
FU NSF [IIS-1423515]
FX Work funded by NSF award IIS-1423515. We thank Weiran Wang for help with
   training the deep net in the MNIST experiment.
CR BARNES J, 1986, NATURE, V324, P446, DOI 10.1038/324446a0
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bengio Y, 2004, NEURAL COMPUT, V16, P2197, DOI 10.1162/0899766041732396
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Carreira-Perpinan M., 2012, ARXIV12125921CSLG
   Carreira-Perpinan M., 2007, AISTATS
   Carreira-Perpinan M., 2014, AISTATS
   Carreira-Perpinan M. A, 2010, ICML
   Globerson A., 2006, NIPS
   Goldberger J., 2005, NIPS
   Greengard L., 1987, J COMP PHYS, V73
   Griewank A, 2008, EVALUATING DERIVATIV
   Hadsell Raia, 2006, CVPR
   He X., 2004, NIPS
   Hinton G., 2003, NIPS
   Lowe D, 1996, NEURAL COMPUT APPL, V4, P83, DOI 10.1007/BF01413744
   MAO JC, 1995, IEEE T NEURAL NETWOR, V6, P296, DOI 10.1109/72.363467
   Min R., 2010, ICML
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Peltonen J, 2005, IEEE T NEURAL NETWOR, V16, P68, DOI 10.1109/TNN.2004.836194
   Raziperchikolaei R., 2015, ARXIV150105352CSLG
   Salakhutdinov R, 2007, AISTATS
   Sammon Jr J. W., 1969, IEEE T COMPUTERS, V18
   Teh Y. W., 2003, NIPS
   van der Maaten L., 2009, AISTATS
   van der Maaten L. J. P., 2013, INT C LEARN REPR ICL
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Vladymyrov M., 2014, AISTATS
   Vladymyrov M., 2013, ICML
   Vladymyrov M., 2012, ICML
   WEBB AR, 1995, PATTERN RECOGN, V28, P753, DOI 10.1016/0031-3203(94)00135-9
   Weston J., 2008, ICML
   Yang Z., 2013, ICML
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103010
DA 2019-06-15
ER

PT S
AU Chakrabarti, A
AF Chakrabarti, Ayan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Color Constancy by Learning to Predict Chromaticity from Luminance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes-without any spatial or semantic context-can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a "classifier" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminanceto- chromaticity classifier "end-to-end". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.
C1 [Chakrabarti, Ayan] Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA.
RP Chakrabarti, A (reprint author), Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA.
EM ayanc@ttic.edu
CR Bianco S., 2010, PATTERN RECOGNITION
   Bianco S., 2008, J ELECT IMAG
   Bianco S., 2015, ARXIV150404548CSCV
   Brainard DH, 2014, NEW VISUAL NEUROSCIENCES, P545
   Buchsbaum G., 1980, J FRANKLIN I
   Chakrabarti A., 2011, P CVPR
   Chakrabarti A., 2012, PAMI
   Chong H., 2007, P ICCV
   Drew M. S., 2014, PAMI
   Forsyth D., 1990, IJCV
   Gehler P. V., 2008, CVPR
   Gijsenij A., 2011, IEEE T IMAGE P
   Gijsenij A., 2010, IJCV
   Judd D. B., 1964, JOSA
   Land E. H., 1971, SCI AM
   Li B., 2014, IEEE T IMAG P
   Li B, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857898
   Lu R., 2009, ICCV
   Shi L., RE PROCESSED VERSION
   Srivastava N., 2014, JMLR
   van de Weijer J., 2007, IEEE T IMAGE P
   Xiong W., 2006, J IMAG SCI TECHNOL
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102013
DA 2019-06-15
ER

PT S
AU Chakraborty, M
   Das, S
AF Chakraborty, Mithun
   Das, Sanmay
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Market Scoring Rules Act As Opinion Pools For Risk-Averse Agents
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFORMATION AGGREGATION; PREDICTION
AB A market scoring rule (MSR) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents. In this paper, we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a MSR incorporates private information from agents who deviate from the assumption of risk-neutrality. We first establish that, for a myopic trading agent with a risk-averse utility function, a MSR satisfying mild regularity conditions elicits the agent's risk-neutral probability conditional on the latest market state rather than her true subjective probability. Hence, we show that a MSR under these conditions effectively behaves like a more traditional method of belief aggregation, namely an opinion pool, for agents' true probabilities. In particular, the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents, and as a linear pool for an atypical budget-constrained agent utility with decreasing absolute risk aversion. We also point out the interpretation of a market maker under these conditions as a Bayesian learner even when agent beliefs are static.
C1 [Chakraborty, Mithun; Das, Sanmay] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
RP Chakraborty, M (reprint author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM mithunchakraborty@wustl.edu; sanmay@wustl.edu
FU NSF IIS [1414452, 1527037]
FX We are grateful for support from NSF IIS awards 1414452 and 1527037.
CR Abernethy Jacob, 2014, P 15 ACM C EC COMP, P395
   Beygelzimer Alina, 2012, P 11 INT C AUT AG MU, P1317
   BORDLEY RF, 1982, MANAGE SCI, V28, P1137, DOI 10.1287/mnsc.28.10.1137
   Boucheron S, 2004, LECT NOTES ARTIF INT, V3176, P208
   Brahma A., 2012, P 13 ACM C EL COMM, P215
   Chen Y, 2010, P 11 ACM C EL COMM, P189
   Chen Yiling, 2007, P UAI 07
   Cliff Dave, 1997, TECHNICAL REPORT, P105
   Cowgill B, 2015, REV ECON STUD, V82, P1309, DOI 10.1093/restud/rdv014
   Farmer JD, 2005, P NATL ACAD SCI USA, V102, P2254, DOI 10.1073/pnas.0409157102
   Frongillo Rafael M., 2012, ADV NEURAL INFORM PR, P3266
   Garg Ashutosh, 2004, P 8 INT S ART INT MA
   Genest C., 1986, STAT SCI, V1, P114
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Hanson R, 2003, INFORM SYST FRONT, V5, P107, DOI 10.1023/A:1022058209073
   Hu Jinli, 2014, P ICML
   Iyer K, 2014, MANAGE SCI, V60, P2509, DOI 10.1287/mnsc.2014.1929
   Mas-Colell  A., 1995, MICROECONOMIC THEORY, V1
   Millin Jono, 2012, P 29 INT C MACH LEAR, P1815
   Ostrovsky M, 2012, ECONOMETRICA, V80, P2595, DOI 10.3982/ECTA8479
   Pennock David M., 1999, THESIS
   Sethi R, 2015, NAT CONF COMPUT VIS
   Storkey AJ, 2015, LECT NOTES ARTIF INT, V9284, P560, DOI 10.1007/978-3-319-23528-8_35
   Surowiecki J, 2005, WISDOM CROWDS
   Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321
   Xia L, 2011, C UNC ART INT, P581
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101099
DA 2019-06-15
ER

PT S
AU Chaudhuri, K
   Kakade, SM
   Netrapalli, P
   Sanghavi, S
AF Chaudhuri, Kamalika
   Kakade, Sham M.
   Netrapalli, Praneeth
   Sanghavi, Sujay
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convergence Rates of Active Learning for Maximum Likelihood Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well.
   Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting - maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields.
   We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in [12] and [13] (on active linear and logistic regression) shows the promise of this approach.
C1 [Chaudhuri, Kamalika] Univ Calif San Diego, Dept CS, La Jolla, CA 92093 USA.
   [Kakade, Sham M.] Univ Washington, Dept CS & Stat, Seattle, WA 98195 USA.
   [Netrapalli, Praneeth] Microsoft Res New England, Cambridge, MA USA.
   [Sanghavi, Sujay] Univ Texas Austin, Dept ECE, Austin, TX 78712 USA.
RP Chaudhuri, K (reprint author), Univ Calif San Diego, Dept CS, La Jolla, CA 92093 USA.
EM kamalika@cs.ucsd.edu; sham@cs.washington.edu; praneeth@microsoft.com;
   sanghavi@mail.utexas.edu
FU NSF [IIS 1162581]
FX KC thanks NSF under IIS 1162581 for research support.
CR Agarwal A., 2013, JMLR P, P1220
   Balcan M.-F., 2013, COLT
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Beygelzimer A., 2010, NIPS
   Casella George, 1998, THEORY POINT ESTIMAT, V31
   Cornell J. A., 2002, EXPT MIXTURES DESIGN
   Dasgupta S., 2008, ICML
   Dasgupta S., 2007, NIPS
   Dasgupta S., 2005, NIPS
   Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054
   Frostig R., 2014, ARXIV14126606
   Gu Q., 2012, P ADV NEUR INF PROC
   Gu Q., 2014, 30 C UNC ART INT UAI
   Hanneke S., 2007, ICML
   Kaariainen M., 2006, ALT
   Le Cam L, 1986, ASYMPTOTIC METHODS S
   Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298
   Sabato S., 2014, NIPS
   Urner R., 2013, COLT
   van der Vaart A. W., 2000, ASYMPTOTIC STAT
   Yang G. L., 2000, ASYMPTOTICS STAT SOM
   Zhang C., 2014, P NEUR INF PROC SYST
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102069
DA 2019-06-15
ER

PT S
AU Chen, CY
   Ding, N
   Carin, L
AF Chen, Changyou
   Ding, Nan
   Carin, Lawrence
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the Convergence of Stochastic Gradient MCMC Algorithms with
   High-Order Integrators
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of L-4/5 at L iterations, compared to L-2/3 for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.
C1 [Chen, Changyou; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Ding, Nan] Google Inc, Venice, CA USA.
RP Chen, CY (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM cchangyou@gmail.com; dingnan@google.com; lcarin@duke.edu
FU ARO; DARPA; DOE; NGA; ONR
FX Supported in part by ARO, DARPA, DOE, NGA and ONR. We acknowledge
   Jonathan C. Mattingly and Chunyuan Li for inspiring discussions; David
   Carlson for the AIS codes.
CR Abdulle A, 2015, SIAM J NUMER ANAL, V53, P1, DOI 10.1137/140962644
   Betancourt M., 2015, ICML
   Blei D. M., 2003, JMLR
   Chen T., 2014, ICML
   Debussche A, 2012, SIAM J NUMER ANAL, V50, P1735, DOI 10.1137/110831544
   Ding N., 2014, NIPS
   Gan Z., 2015, AISTATS
   Gan Zhe, 2015, ICML
   Giesl P, 2007, LECT NOTES MATH, V1904, P1, DOI 10.1007/978-3-540-69909-5
   Has'minskii R. Z., 2012, STOCHASTIC STABILITY
   Hoffman Matthew, 2010, NIPS
   Kopec M., 2014, IMA J NUMER ANAL
   Kryloff N, 1937, ANN MATH, V38, P65, DOI 10.2307/1968511
   Leimkuhler B., 2015, ARXIV150506889V1 U E
   Leimkuhler B, 2013, APPL MATH RES EXPRES, P34, DOI 10.1093/amrx/abs010
   Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527
   Mnih A., 2014, ICML
   Patterson S., 2013, NIPS
   Risken H, 1989, FOKKER PLANCK EQUATI
   Salakhutdinov R., 2008, ICML
   Sato I., 2014, ICML
   Teh Y. W., 2014, ARXIV14090578 U OXF
   Vollmer S. J., 2015, ARXIV150100438 U OXF
   Welling M., 2011, ICML
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100064
DA 2019-06-15
ER

PT S
AU Chen, JS
   He, J
   Shen, YL
   Xiao, L
   He, XD
   Gao, JF
   Song, XY
   Deng, L
AF Chen, Jianshu
   He, Ji
   Shen, Yelong
   Xiao, Lin
   He, Xiaodong
   Gao, Jianfeng
   Song, Xinying
   Deng, Li
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI End-to-end Learning of LDA by Mirror-Descent Back Propagation over a
   Deep Architecture
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SUBGRADIENT METHODS
AB We develop a fully discriminative learning approach for supervised Latent Dirich-let Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.
C1 [Chen, Jianshu; Shen, Yelong; Xiao, Lin; He, Xiaodong; Gao, Jianfeng; Song, Xinying; Deng, Li] Microsoft Res, Redmond, WA 98052 USA.
   [He, Ji] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
RP Chen, JS (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM jianshuc@microsoft.com; jvking@uw.edu; yeshen@microsoft.com;
   lin.xiao@microsoft.com; xiaohe@microsoft.com; jfgao@microsoft.com;
   xinson@microsoft.com; deng@microsoft.com
CR Asuncion Arthur, 2009, P 25 C UNC ART INT, P27
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Blei D., 2007, ADV NEURAL INFORM PR, P121
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blitzer J., 2007, ANN M ASS COMP LING, V7, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Bouchard G., 2004, P COMP STAT 16 S IAS, P721
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hershey J. R., 2014, ARXIV14092574
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Holub A, 2005, PROC CVPR IEEE, P664
   Huang P.-S., 2013, P 22 ACM INT C C INF, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   Kapadia S., 1998, THESIS
   Lacoste-Julien S., 2008, NIPS, V21, P897
   Lasserre J, 2007, BAYESIAN STAT, V8, P3
   McAuley JJ, 2013, P 22 INT C WORLD WID, P897, DOI DOI 10.1145/2488388.2488466
   McCallum Andrew Kachites, 2002, MALLET MACHINE LEARN
   Nemirovsky D. B., 1983, PROBLEM COMPLEXITY M
   Sontag D., 2011, NIPS, P1008
   Stoyanov Veselin, 2011, JMLR P, P725
   Tseng P., 2008, SIAM J OPTIMIZATION
   Wallach H. M., 2009, ADV NEURAL INFORM PR, P1973
   Wallach Hanna M., 2009, P 26 ANN INT C MACH, P1105, DOI DOI 10.1145/1553374.1553515
   Yakhnenko Oksana, 2005, P IEEE ICDM
   Zhu J., 2014, P NIPS, P1511
   Zhu J, 2014, J MACH LEARN RES, V15, P1073
   Zhu J, 2012, J MACH LEARN RES, V13, P2237
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103005
DA 2019-06-15
ER

PT S
AU Chen, S
   Banerjee, A
AF Chen, Sheng
   Banerjee, Arindam
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Structured Estimation with Atomic Norms: General Bounds and Applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGRESSION; SELECTION
AB For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular Gaussian width of the unit norm ball, Gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric measures can be difficult. In this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.
C1 [Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Chen, S (reprint author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM shengc@cs.umn.edu; banerjee@cs.umn.edu
FU NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711]; NASA [NNX12AQ39A]
FX The research was supported by NSF grants IIS-1447566, IIS-1422557,
   CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant
   NNX12AQ39A.
CR Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005
   Argyriou A., 2012, ADV NEURAL INFORM PR
   Banerjee A., 2014, ADV NEURAL INFORM PR
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Bogdan  M., 2013, ARXIV13101969
   Cai T. T., 2014, ARXIV14044408
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Candes E, 2013, MATH PROGRAM, V141, P577, DOI 10.1007/s10107-012-0540-0
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee S., 2014, ADV NEURAL INFORM PR
   Chen S., 2015, INT C ART INT STAT A
   Figueiredo Mario AT, 2014, ARXIV14094005
   GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761
   Jacob L., 2009, INT C MACH LEARN ICM
   Maurer A., 2014, C LEARN THEOR COLT
   McDonald A. M., 2014, ADV NEURAL INFORM PR
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Oymak Samet, 2013, ARXIV13110830
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Rao N., 2012, INT C ART INT STAT A
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp JA, 2015, APPL NUMER HARMON AN, P67, DOI 10.1007/978-3-319-19749-4_2
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zeng X., 2014, ARXIV14094271
   Zhang X., 2013, ADV NEURAL INFORM PR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101093
DA 2019-06-15
ER

PT S
AU Chen, XZ
   Kundu, K
   Zhu, YK
   Berneshawi, A
   Ma, HM
   Fidler, S
   Urtasun, R
AF Chen, Xiaozhi
   Kundu, Kaustav
   Zhu, Yukun
   Berneshawi, Andrew
   Ma, Huimin
   Fidler, Sanja
   Urtasun, Raquel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI 3D Object Proposals for Accurate Object Class Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.
C1 [Chen, Xiaozhi; Ma, Huimin] Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.
   [Kundu, Kaustav; Zhu, Yukun; Berneshawi, Andrew; Fidler, Sanja; Urtasun, Raquel] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
RP Chen, XZ (reprint author), Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.
EM chenxz12@mails.tsinghua.edu.cn; kkundu@cs.toronto.edu;
   yukun@cs.toronto.edu; andrew.berneshawi@mail.utoronto.ca;
   mhmpub@tsinghua.edu.cn; fidler@cs.toronto.edu; urtasun@cs.toronto.edu
FU NSFC [61171113]; NSERC; Toyota Motor Corporation
FX The work was partially supported by NSFC 61171113, NSERC and Toyota
   Motor Corporation.
CR Alexe B., 2012, PAMI, V2
   Arbelaez Pablo, 2014, CVPR
   Banica D., 2013, CORR
   Benenson  R., 2013, CVPR
   Carreira J., 2012, ECCV
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Cheng M. M., 2014, CVPR
   Dollar P., 2014, PAMI
   Everingham M., PASCAL VISUAL OBJECT
   Felzenszwalb Pedro F., 2010, PAMI
   Fidler S., 2013, CVPR
   Geiger A., 2011, NIPS
   Geiger A., 2012, CVPR
   Girshick R., 2015, ICCV
   Girshick  R.B., 2013, ARXIV13112524
   Gonzalez A., 2015, IV
   Gupta S., 2014, ECCV
   Hosang J., 2015, TAKING DEEPER LOOK P
   Hosang J., 2015, ARXIV150205082
   Karpathy  A., 2013, ICRA
   Krizhevsky A., 2012, NIPS
   Lee T., 2015, ICCV
   Li B., 2014, ECCV
   Lin D., 2013, ICCV
   Long C., 2014, ACCV
   Ohn-Bar E., 2015, IEEE T INTELLIGENT T
   Oneata Dan, 2014, ECCV
   Paisitkriangkrai S., 2014, ARXIV14095209
   Pepik B., 2013, CVPR
   Pepik B., 2015, PAMI
   Premebida C., 2014, IROS
   Schwing A. G., 2013, ICCV
   Simonyan K, 2014, ARXIV14091556
   Song S., 2014, ECCV
   Tsochantaridis I, 2004, ICML
   van de Sande K.E.A., 2011, ICCV
   Wang S., 2015, CVPR
   Xiang Y., 2015, CVPR
   Xu J., 2014, ARXIV14085400
   Yamaguchi K., 2014, ECCV
   Yebes J., 2014, IV
   Zhang S., 2015, ARXIV150105759
   Zhu Y., 2015, CVPR
   Zia M. Z., 2015, IJCV
   Zitnick L., 2014, ECCV
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100012
DA 2019-06-15
ER

PT S
AU Chen, YC
   Genovese, CR
   Ho, S
   Wasserman, L
AF Chen, Yen-Chi
   Genovese, Christopher R.
   Ho, Shirley
   Wasserman, Larry
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Optimal Ridge Detection using Coverage Risk
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BANDWIDTH SELECTION; MEAN-SHIFT
AB We introduce the concept of coverage risk as an error measure for density ridge estimation. The coverage risk generalizes the mean integrated square error to set estimation. We propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk. We study the rate of convergence for coverage risk and prove consistency of the risk estimators. We apply our method to three simulated datasets and to cosmology data. In all the examples, the proposed method successfully recover the underlying density structure.
C1 [Chen, Yen-Chi; Genovese, Christopher R.; Wasserman, Larry] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
   [Ho, Shirley] Carnegie Mellon Univ, Dept Phys, Pittsburgh, PA 15213 USA.
RP Chen, YC (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
EM yenchic@andrew.cmu.edu; genovese@stat.cmu.edu; shirleyh@andrew.cmu.edu;
   larry@stat.cmu.edu
CR Bas E, 2012, J VIS COMMUN IMAGE R, V23, P1260, DOI 10.1016/j.jvcir.2012.09.003
   Bas E, 2011, I S BIOMED IMAGING, P1358, DOI 10.1109/ISBI.2011.5872652
   Cadre B., 2006, J MULTIVARIATE ANAL
   Chen Y. - C., 2014, ARXIV14065663
   Chen Y. - C., 2015, ARXIV150105303
   Chen Y. - C., 2014, ARXIV14121716
   Chen Y.-C., 2014, ARXIV14061803
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cuevas A., 2006, AUST NZ J STAT
   Eberly D, 1996, RIDGES IMAGE DATA AN
   Einbeck J, 2011, J PATTERN RECOGNIT R, V6, P175, DOI 10.13176/11.288
   Einmahl U., 2005, ANN STAT
   Evans L. C., 1991, MEASURE THEORY FINE, V5
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Genovese CR, 2014, ANN STAT, V42, P1511, DOI 10.1214/14-AOS1218
   Gine E., 2002, ANNALES I H POINCARE
   HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
   Hastie T., 1984, TECHNICAL REPORT
   Jones MC, 1996, J AM STAT ASSOC, V91, P401, DOI 10.2307/2291420
   Mason DM, 2009, ANN APPL PROBAB, V19, P1108, DOI 10.1214/08-AAP569
   Miao Z., 2014, METHOD ACCURATE ROAD
   Ozertem U., 2011, J MACHINE LEARNING R
   Rinaldo A., 2010, ANN STAT
   Scott D. W., 2009, MULTIVARIATE DENSITY, V383
   Silverman B. W., 1986, DENSITY ESTIMATION S
   SILVERMAN BW, 1987, BIOMETRIKA, V74, P469, DOI 10.1093/biomet/74.3.469
   Tibshirani R., 1992, Statistics and Computing, V2, P183, DOI 10.1007/BF01889678
   Wasserman L, 2006, ALL NONPARAMETRIC ST
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103034
DA 2019-06-15
ER

PT S
AU Chen, YX
   Candes, EJ
AF Chen, Yuxin
   Candes, Emmanuel J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Solving Random Quadratic Systems of Equations Is Nearly as Easy as
   Solving Linear Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PHASE RETRIEVAL
AB This paper is concerned with finding a solution x to a quadratic system of equations y(i) = vertical bar < a(i); x >vertical bar(2), i = 1, . . . , m. We demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from O(n) equations in linear time, that is, in time proportional to reading the data {a(i)} and {y(i)}. This is accomplished by a novel procedure, which starting from an initial guess given by a spectral initialization procedure, attempts to minimize a nonconvex objective. The proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion, which discard terms bearing too much influence on the initial estimate or search directions. These careful selection rules-which effectively serve as a variance reduction scheme-provide a tighter initial guess, more robust descent directions, and thus enhanced practical performance. Further, this procedure also achieves a nearoptimal statistical accuracy in the presence of noise. Empirically, we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.
C1 [Chen, Yuxin; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA.
RP Chen, YX (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
EM yxchen@stanfor.edu; candes@stanford.edu
FU NSF [CCF-0963835]; Math + X Award from the Simons Foundation; NSF
FX E. C. is partially supported by NSF under grant CCF-0963835 and by the
   Math + X Award from the Simons Foundation. Y. C. is supported by the
   same NSF grant.
CR Arora S., 2015, C LEARN THEOR COLT
   Balakrishnan S., 2014, ARXIV14082156
   Ben-Tal  A., 2001, LECT MODERN CONVEX O, V2
   Cai T., ANN STATS
   Candes E. J., 2014, APPL COMPUTATIONAL H
   Candes E. J., 2013, COMMUNICATIONS PURE, V66, P1017
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z
   Chen Y., 2014, C LEARN THEOR COLT
   Chen YX, 2015, IEEE T INFORM THEORY, V61, P4034, DOI 10.1109/TIT.2015.2429594
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   Gross D, 2015, J FOURIER ANAL APPL, V21, P229, DOI 10.1007/s00041-014-9361-2
   Jaganathan Kishore, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1473, DOI 10.1109/ISIT.2012.6283508
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Netrapalli P., 2013, NIPS
   Ohlsson H., 2012, ADV NEURAL INFORM PR
   Repetti A, 2014, IEEE IMAGE PROC, P1753, DOI 10.1109/ICIP.2014.7025351
   Schniter P, 2015, IEEE T SIGNAL PROCES, V63, P1043, DOI 10.1109/TSP.2014.2386294
   Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687
   Shechtman Y, 2011, OPT EXPRESS, V19, P14807, DOI 10.1364/OE.19.014807
   Soltanolkotabi M., 2014, THESIS
   Sun J., 2015, ICML
   Sun R., 2015, FOCS
   Trefethen L. N., 1997, NUMERICAL LINEAR ALG, V50
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wei K., 2015, ARXIV150201822
   White C. D., 2015, ARXIV150607868
   Yi  X., 2014, INT C MACH LEARN
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101003
DA 2019-06-15
ER

PT S
AU Chiang, KY
   Hsieh, CJ
   Dhillon, IS
AF Chiang, Kai-Yang
   Hsieh, Cho-Jui
   Dhillon, Inderjit S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Matrix Completion with Noisy Side Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the matrix completion problem with side information. Side information has been considered in several matrix completion applications, and has been empirically shown to be useful in many cases. Recently, researchers studied the effect of side information for matrix completion from a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features. However, since in reality most given features are noisy or only weakly informative, the development of a model to handle a general feature set, and investigation of how much noisy features can help matrix recovery, remains an important issue. In this paper, we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise. Moreover, we study the effect of general features in theory and show that by using our model, the sample complexity can be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight into the usefulness of general side information. Finally, we consider synthetic data and two applications - relationship prediction and semi-supervised clustering - and show that our model outperforms other methods for matrix completion that use features both in theory and practice.
C1 [Chiang, Kai-Yang; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
   [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA.
RP Chiang, KY (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM kychiang@cs.utexas.edu; chohsieh@ucdavis.edu; inderjit@cs.utexas.edu
FU NSF [CCF-1320746, CCF-1117055]
FX We thank David Inouye and Hsiang-Fu Yu for helpful comments and
   discussions. This research was supported by NSF grants CCF-1320746 and
   CCF-1117055.
CR Abernethy J, 2009, J MACH LEARN RES, V10, P803
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chandrasekaran V., 2012, ANN STAT
   Chen TQ, 2012, J MACH LEARN RES, V13, P3619
   Chen Y., 2014, ICML
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Chiang KY, 2014, J MACH LEARN RES, V15, P1177
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Feige U, 2002, RANDOM STRUCT ALGOR, V20, P403, DOI 10.1002/rsa.10036
   Grippo L, 1999, OPTIM METHOD SOFTW, V10, P587, DOI 10.1080/10556789908805730
   Hsieh C.-J., 2012, KDD
   Hsieh C. J., 2014, ICML
   Jain P., 2013, ABS13060626 CORR
   Jalali A., 2010, NIPS
   Kakade S. M., 2008, NIPS, P793
   Keshavan R., 2010, JMLR
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Laurent B, 2000, ANN STAT, V28, P1302
   Leskovec J., 2010, WWW
   Li Z., 2009, ICCV
   Massa P., 2006, P ECAI WORKSH REC SY, P29
   Meir R., 2003, JMLR
   Menon A. K., 2011, P 17 ACM SIGKDD INT, P141, DOI DOI 10.1145/2020408.2020436
   Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294
   Shamir O, 2014, J MACH LEARN RES, V15, P3401
   Shin D., 2015, P 24 ACM INT C INF K, P203
   SREBRO N, 2005, COLT, V3559, P545
   Xu M., 2013, NIPS
   Yang E., 2013, NIPS
   Yi J., 2013, ICML
   Zhong Kai, 2015, INT C ALG LEARN THEO
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102088
DA 2019-06-15
ER

PT S
AU Cho, M
   Dhir, CS
   Lee, J
AF Cho, Minhyung
   Dhir, Chandra Shekhar
   Lee, Jaehyung
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Hessian-free Optimization for Learning Deep Multidimensional Recurrent
   Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.
C1 [Cho, Minhyung; Dhir, Chandra Shekhar; Lee, Jaehyung] Gracenote Inc, Appl Res Korea, Seoul, South Korea.
RP Cho, M (reprint author), Gracenote Inc, Appl Res Korea, Seoul, South Korea.
EM mhyung.cho@gmail.com; shekhardhir@gmail.com; jaehyung.lee@kaist.ac.kr
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bishop C., 2007, PATTERN RECOGNITION
   Boyd S., 2004, CONVEX OPTIMIZATION
   DARPA-ISTO, 1990, SPEECH DISC CD1 1 1
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Graves A., 2009, ADV NEURAL INFORM PR, P545
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Graves A., 2008, ADV NEURAL INFORM PR, P577
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves Alex, 2008, RNNLIB RECURRENT NEU
   Graves Alex, 2012, ICML REPR LEARN WORK
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4
   Pascanu Razvan, 2014, INT C LEARN REPR
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Pechwitz M., 2002, C INT FRANC ECR DOC, P129
   Romero A., 2014, CORR
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100032
DA 2019-06-15
ER

PT S
AU Choromanska, A
   Langford, J
AF Choromanska, Anna
   Langford, John
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Logarithmic Time Online Multiclass prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.
C1 [Choromanska, Anna] Courant Inst Math Sci, New York, NY 10012 USA.
   [Langford, John] Microsoft Res, New York, NY USA.
RP Choromanska, A (reprint author), Courant Inst Math Sci, New York, NY 10012 USA.
EM achoroma@cims.nyu.edu; jcl@microsoft.com
CR Agarwal A., 2014, ICML
   Agarwal R., 2013, WWW
   Beijbom O., 2014, ICML
   Bengio S., 2010, NIPS
   Bennett P. N., 2009, SIGIR
   Beygelzimer A., 2009, ALT
   Beygelzimer A., 2009, UAI
   Bishop C. M., 2006, PATTERN RECOGNITION
   Breiman L., 1984, CLASSIFICATION REGRE
   Carnap R., 1962, LOGICAL FDN PROBABIL, P468
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Deng J., 2011, NIPS
   Deng J., 2009, CVPR
   Hsu  D., 2009, NIPS
   Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543
   Liu T. -Y., 2005, SIGKDD EXPLORATIONS
   Madzarov G, 2009, INFORM-J COMPUT INFO, V33, P225
   Makadia  A., 2013, ICML
   Montillo A., 2013, DECISION FORESTCOM
   Nesterov Y., 2004, APPL OPTIMIZATION
   Prabhu Y., 2014, ACM SIGKDD
   Rifkin R, 2004, J MACH LEARN RES, V5, P101
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Tentori K, 2007, COGNITION, V103, P107, DOI 10.1016/j.cognition.2005.09.006
   Yu H. F., 2014, ICML
   Zhao B., 2013, CVPR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102085
DA 2019-06-15
ER

PT S
AU Chow, Y
   Tamar, A
   Mannor, S
   Pavone, M
AF Chow, Yinlam
   Tamar, Aviv
   Mannor, Shie
   Pavone, Marco
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID VARIANCE
AB In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.
C1 [Chow, Yinlam; Pavone, Marco] Stanford Univ, Stanford, CA 94305 USA.
   [Tamar, Aviv] Univ Calif Berkeley, Berkeley, CA USA.
   [Mannor, Shie] Technion, Haifa, Israel.
RP Chow, Y (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ychow@stanford.edu; avivt@berkeley.edu; shie@ee.technion.ac.il;
   pavone@stanford.edu
FU Croucher Foundation; Office of Naval Research, Science of Autonomy
   Program [N00014-15-1-2673]; European Community [306638]
FX The authors would like to thank Mohammad Ghavamzadeh for helpful
   comments on the technical details, and Daniel Vainsencher for practical
   optimization advice. Y-L. Chow and M. Pavone are partially supported by
   the Croucher Foundation doctoral scholarship and the Office of Naval
   Research, Science of Autonomy Program, under Contract N00014-15-1-2673.
   Funding for Shie Mannor and Aviv Tamar were partially provided by the
   European Community's Seventh Framework Programme (FP7/2007-2013) under
   grant agreement 306638 (SUPREL).
CR Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068
   Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0
   Bertsekas D. P., 2012, DYNAMIC PROGRAMMING, V2
   Borkar V, 2014, IEEE T AUTOMAT CONTR, V59, P2574, DOI 10.1109/TAC.2014.2309262
   Chow Yinlam, 2014, ADV NEURAL INFORM PR, P3509
   Dowd K., 2007, MEASURING MARKET RIS
   FILAR JA, 1995, IEEE T AUTOMAT CONTR, V40, P2, DOI 10.1109/9.362904
   Haskell W., 2014, SIAM J CONTROL OPTIM
   HOWARD RA, 1972, MANAGE SCI, V18, P356, DOI 10.1287/mnsc.18.7.356
   Iyengar G., 2005, MATH OPER RES, V30
   Iyengar G, 2013, ANN OPER RES, V205, P203, DOI 10.1007/s10479-012-1245-8
   Mannor S., 2012, INT C MACH LEARN, P385
   Mannor S, 2007, MANAGE SCI, V53, P308, DOI 10.1287/mnsc.1060.0614
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216
   Osogami T., 2012, P ADV NEUR INF PROC, P233
   Pflug G., 2015, OPTIMIZATION
   Phillips M., 2003, INTERPOLATION APPROX, V14
   Prashanth LA, 2014, LECT NOTES ARTIF INT, V8776, P155, DOI 10.1007/978-3-319-11662-4_12
   Rockafellar R, 2000, J RISK, V2, P21, DOI DOI 10.21314/JOR.2000.038
   Rockafellar RT, 2006, J BANK FINANC, V30, P743, DOI 10.1016/j.jbankfin.2005.04.004
   Serraino  G., 2013, ENCY OPERATIONS RES, P258
   Shapiro A, 2009, LECT STOCHASTIC PROG
   SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832
   Tamar A., 2015, AAAI
   Uryasev S., 2010, CARISMA C
   Xu H., 2006, ADV NEURAL INFORM PR, P1537
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103052
DA 2019-06-15
ER

PT S
AU Colin, I
   Salmon, J
   Clemencon, S
   Bellet, A
AF Colin, Igor
   Salmon, Joseph
   Clemencon, Stephan
   Bellet, Aurelien
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Extending Gossip Algorithms to Distributed Estimation of U-Statistics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.
C1 [Colin, Igor; Salmon, Joseph; Clemencon, Stephan] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
   [Bellet, Aurelien] INRIA Lille Nord Europe, Magnet Team, F-59650 Villeneuve Dascq, France.
RP Colin, I (reprint author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
EM igor.colin@telecom-paristech.fr; joseph.salmon@telecom-paristech.fr;
   stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr
FU chair Machine Learning for Big Data of Telecom ParisTech
FX This work was supported by the chair Machine Learning for Big Data of
   Telecom ParisTech, and was conducted when A. Bellet was affiliated with
   Telecom ParisTech.
CR Bollobas B, 1998, GRAD TEXT M, V184, P215
   Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516
   Chung F. R., 1997, SPECTRAL GRAPH THEOR, V92
   Clemencon S., 2011, ADV NEURAL INFORM PR, P37
   Dimakis ADG, 2008, IEEE T SIGNAL PROCES, V56, P1205, DOI 10.1109/TSP.2007.908946
   Dimakis AG, 2010, P IEEE, V98, P1847, DOI 10.1109/JPROC.2010.2052531
   Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Karp R, 2000, ANN IEEE SYMP FOUND, P565, DOI 10.1109/SFCS.2000.892324
   Kempe D, 2003, ANN IEEE SYMP FOUND, P482, DOI 10.1109/SFCS.2003.1238221
   Kowalczyk Wojtek, 2004, ADV NEURAL INFORM PR, P713
   Lee AJ, 1990, U STAT THEORY PRACTI
   Li WJ, 2010, IEEE T INFORM THEORY, V56, P6208, DOI 10.1109/TIT.2010.2081030
   MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491
   Mosk-Aoyama D, 2008, IEEE T INFORM THEORY, V54, P2997, DOI 10.1109/TIT.2008.924648
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Pelckmans K., 2009, IFAC WORKSH EST CONT, P48
   Shah D, 2008, FOUND TRENDS NETW, V3, P1, DOI 10.1561/1300000014
   Tsitsiklis J. N., 1984, THESIS
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101007
DA 2019-06-15
ER

PT S
AU Comanici, G
   Precup, D
   Panangaden, P
AF Comanici, Gheorghe
   Precup, Doina
   Panangaden, Prakash
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Basis Refinement Strategies for Linear Value Function Approximation in
   MDPs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We provide a theoretical framework for analyzing basis function construction for linear value function approximation in Markov Decision Processes (MDPs). We show that important existing methods, such as Krylov bases and Bellman-error-based methods are a special case of the general framework we develop. We provide a general algorithmic framework for computing basis function refinements which "respect" the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework. We also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding "prototypes" that are diverse enough to represent the given MDP.
C1 [Comanici, Gheorghe; Precup, Doina; Panangaden, Prakash] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
RP Comanici, G (reprint author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
EM gcoman@cs.mcgill.ca; dprecup@cs.mcgill.ca; prakash@cs.mcgill.ca
CR Barreto A. S., 2011, ADV NEURAL INFORM PR, P720
   Bertsekas D. P., 1989, IEEE T AUTOMATIC CON, V34
   Blackwell D., 1965, ANN MATH STAT, V36, P226, DOI DOI 10.1214/AOMS/1177700285
   Bradtke SJ, 1996, MACH LEARN, V22, P33, DOI 10.1023/A:1018056104778
   Comanici G., 2011, AAAI
   Desharnais J, 2004, THEOR COMPUT SCI, V318, P323, DOI 10.1016/j.tcs.2003.09.013
   Desharnais J., 1999, CONCUR
   Ferns N., 2004, P 20 C UNC ART INT, P162
   Geramifard Alborz, 2011, INT C MACH LEARN ICM, P881
   Givan R, 2003, ARTIF INTELL, V147, P163, DOI 10.1016/S0004-3702(02)00376-4
   Jong N., 2006, ICML WORKSH KERN MAC
   KELLER PW, 2006, P 23 INT C MACH LEAR, P449
   Konidaris G., 2011, P 25 C ART INT, P380
   LARSEN KG, 1991, INFORM COMPUT, V94, P1, DOI 10.1016/0890-5401(91)90030-6
   Mahadevan S., 2005, P 22 INT C MACH LEAR, P553, DOI DOI 10.1145/1102351.1102421
   Munos R, 2002, MACH LEARN, V49, P291, DOI 10.1023/A:1017992615625
   Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829
   Parr R., 2008, P 25 INT C MACH LEAR, P752
   Parr R., 2008, ICML, P737
   Ravindran B., 2002, Abstraction, Reformulation, and Approximation. 5th International Symposium, SARA 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2371), P196
   Ruan S., 2015, AAAI, P3578
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Villani C., 2003, TOPICS OPTIMAL TRANS
   Yu H., 2006, TECHNICAL REPORT
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100062
DA 2019-06-15
ER

PT S
AU Combes, R
   Talebi, MS
   Proutiere, A
   Lelarge, M
AF Combes, Richard
   Talebi, M. Sadegh
   Proutiere, Alexandre
   Lelarge, Marc
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Combinatorial Bandits Revisited
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID OPTIMIZATION
AB This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose COMBEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.
C1 [Combes, Richard] Cent Supelec, L2S, Gif Sur Yvette, France.
   [Talebi, M. Sadegh; Proutiere, Alexandre] KTH, Dept Automat Control, Stockholm, Sweden.
   [Lelarge, Marc] INRIA, Paris, France.
   [Lelarge, Marc] ENS, Paris, France.
RP Combes, R (reprint author), Cent Supelec, L2S, Gif Sur Yvette, France.
EM richard.combes@supelec.fr; mstms@kth.se; alepro@kth.se;
   marc.lelarge@ens.fr
FU ERC FSA grant; SSF ICT-Psi project
FX A. Proutiere's research is supported by the ERC FSA grant, and the SSF
   ICT-Psi project.
CR Ailon N, 2014, LECT NOTES ARTIF INT, V8776, P215, DOI 10.1007/978-3-319-11662-4_16
   ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck Sebastien, 2012, P COLT
   Cesa-Bianchi N., 2006, PREDICTION LEARNING, V1
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen Wei, 2013, P ICML
   Csiszar  I., 2004, INFORM THEORY STAT T
   Gai Y., 2010, P IEEE DYSPAN
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Garivier Aurelien, 2011, P COLD
   Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440
   Gyorgy A., 2007, J MACHINE LEARNING R, V8
   Helmbold DP, 2009, J MACH LEARN RES, V10, P1705
   Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054
   Kveton Branislav, 2014, P UAI
   Kveton Branislav, 2015, P AISTATS
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Magureanu Stefan, 2014, P COLT
   Neu Gergely, 2015, P COLT
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Sherali H. D., 1987, American Journal of Mathematical and Management Sciences, V7, P253
   Wen Zheng, 2015, P ICML
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101090
DA 2019-06-15
ER

PT S
AU Corneil, D
   Gerstner, W
AF Corneil, Dane
   Gerstner, Wulfram
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Attractor Network Dynamics Enable Preplay and Rapid Path Planning in
   Maze-like Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PLACE; INTEGRATION; SLOWNESS; MEMORY
AB Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial. While the mechanism for rapid path planning is unknown, the CA3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal "preplay" periods may trace out future goal-directed trajectories. Here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model, resulting in bump-like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.
C1 [Corneil, Dane; Gerstner, Wulfram] Ecole Polytech Fed Lausanne, Lab Computat Neurosci, CH-1015 Lausanne, Switzerland.
RP Corneil, D (reprint author), Ecole Polytech Fed Lausanne, Lab Computat Neurosci, CH-1015 Lausanne, Switzerland.
EM dane.corneil@epfl.ch; wulfram.gerstner@epfl.ch
FU Swiss National Science Foundation [200020_147200]
FX This research was supported by the Swiss National Science Foundation
   (grant agreement no. 200020_147200). We thank Laureline Logiaco and
   Johanni Brea for valuable discussions.
CR Bast T, 2009, PLOS BIOL, V7, P730, DOI 10.1371/journal.pbio.1000089
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Conklin J, 2005, J COMPUT NEUROSCI, V18, P183, DOI 10.1007/s10827-005-6558-z
   DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613
   Drew PJ, 2006, P NATL ACAD SCI USA, V103, P8876, DOI 10.1073/pnas.0600676103
   Eliasmith C., 2004, NEURAL ENG COMPUTATI
   Franzius M, 2007, PLOS COMPUT BIOL, V3, P1605, DOI 10.1371/journal.pcbi.0030166
   Gustafson NJ, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002235
   Howard Eichenbaum, 1993, MEMORY AMNESIA HIPPO
   Kjelstrup KB, 2008, SCIENCE, V321, P140, DOI 10.1126/science.1157086
   Larimer P, 2010, NAT NEUROSCI, V13, P213, DOI 10.1038/nn.2458
   Mahadevan Sridhar, 2009, LEARNING REPRESENTAT, V3
   Martinet LE, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002045
   Nakashiba T, 2008, SCIENCE, V319, P1260, DOI 10.1126/science.1151120
   Nakazawa K, 2003, NEURON, V38, P305, DOI 10.1016/S0896-6273(03)00165-X
   OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1
   Pfeiffer BE, 2013, NATURE, V497, P74, DOI 10.1038/nature12112
   Ponulak F, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00098
   Samsonovich A, 1997, J NEUROSCI, V17, P5900
   Schonfeld F, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00051
   SCOVILLE WB, 1957, J NEUROL NEUROSUR PS, V20, P11, DOI 10.1136/jnnp.20.1.11
   Sprekeler H, 2007, PLOS COMPUT BIOL, V3, P1136, DOI 10.1371/journal.pcbi.0030112
   Sprekeler H, 2011, NEURAL COMPUT, V23, P3287, DOI 10.1162/NECO_a_00214
   Stachenfeld K., 2014, ADV NEURAL INFORM PR, V27, P2528
   Sutton R., 1998, INTRO REINFORCEMENT
   Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030
   Wikenheiser Andrew M, 2015, NATURE NEUROSCIENCE
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102005
DA 2019-06-15
ER

PT S
AU Courbariaux, M
   Bengio, Y
   David, JP
AF Courbariaux, Matthieu
   Bengio, Yoshua
   David, Jean-Pierre
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI BinaryConnect: Training Deep Neural Networks with binary weights during
   propagations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
C1 [Courbariaux, Matthieu; David, Jean-Pierre] Ecole Polytech Montreal, Montreal, PQ, Canada.
   [Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
RP Courbariaux, M (reprint author), Ecole Polytech Montreal, Montreal, PQ, Canada.
EM matthieu.courbariaux@polymtl.ca; yoshua.bengio@gmail.com;
   jean-pierre.david@polymtl.ca
FU NSERC; Canada Research Chairs; CIFAR; Compute Canada
FX We thank the reviewers for their many constructive comments. We also
   thank Roland Memisevic for helpful discussions. We thank the developers
   of Theano [42, 43], a Python library which allowed us to easily develop
   a fast and optimized code for GPU. We also thank the developers of
   Pylearn2 [44] and Lasagne, two Deep Learning libraries built on the top
   of Theano. We are also grateful for funding from NSERC, the Canada
   Research Chairs, Compute Canada, and CIFAR.
CR Bahdanau D., 2015, ICLR 2015
   Bartol TM, 2015, BIORXIV
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   Cheng Z., 2015, ARXIV150303562
   Collobert R., 2004, THESIS
   Courbariaux Matthieu, 2015, ICLR 2015 WORKSH
   David JP, 2007, IEEE T COMPUT, V56, P1308, DOI [10.1109/TC.2007.1084, 10.1109/TC.2007.1084.]
   Dean J., 2012, NIPS 2012
   Devlin J., 2014, P ACL 2014
   Glorot X., 2011, AISTATS 2011
   Glorot X., 2010, AISTATS 2010
   Goodfellow I. J., 2013, ARXIV13084214
   Goodfellow Ian J., 2013, 13024389 U MONTR
   Graham B., 2014, ARXIV14096070
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Gupta Suyog, 2015, ICML 2015
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hwang Kyuyeon, 2014, SIGN PROC SYST SIPS, P1, DOI DOI 10.1109/SIPS.2014.6986082
   Ioffe S., 2015, BATCH NORMALIZATION
   Jonghong Kim, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7510, DOI 10.1109/ICASSP.2014.6855060
   Kim SK, 2009, I C FIELD PROG LOGIC, P367, DOI 10.1109/FPL.2009.5272262
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, NIPS 2012
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2016, ARXIV14095185
   Lin M., 2013, ARXIV13124400
   Minka Thomas P, 2001, UAI 2001
   Muller L. K., 2015, ARXIV150405767
   Nair V., 2010, ICML 2010
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Raina Rajat, 2009, ICML 2009
   Sainath Tara, 2013, ICASSP 2013
   Simonyan Karen, 2015, ICLR
   Soudry D., 2014, NIPS 2014
   Srivastava N, 2013, THESIS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2014, NIPS 2014
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Y., 2013, ICML
   Wan L., 2013, ICML 2013
NR 44
TC 0
Z9 0
U1 5
U2 5
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100015
DA 2019-06-15
ER

PT S
AU Dance, C
   Silander, T
AF Dance, Christopher
   Silander, Tomi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI When are Kalman-Filter Restless Bandits Indexable?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is indexable in the sense that the Whittle index is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about Schur-convexity and mechanical words, which are particular binary strings intimately related to palindromes.
C1 [Dance, Christopher; Silander, Tomi] Xerox Res Ctr Europe, 6 Chemin Maupertuis, Meylan, Isere, France.
RP Dance, C (reprint author), Xerox Res Ctr Europe, 6 Chemin Maupertuis, Meylan, Isere, France.
EM dance@xrce.xerox.com; silander@xrce.xerox.com
CR Altman E, 2000, MATH OPER RES, V25, P324, DOI 10.1287/moor.25.2.324.12230
   Altman E, 1995, QUEUEING SYST, V21, P267, DOI 10.1007/BF01149165
   Araya M., 2010, ADV NEURAL INFORM PR, P64
   Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Berstel Jean, 2008, CRM MONOGRAPH SERIES
   Bubeck Sebastien, 2012, FDN TRENDS MACHINE L, V5
   Chen Yuxin, 2014, P 31 INT C MACH LEAR, P55
   Gittins J., 2011, MULTIARMED BANDIT AL
   Graham R.L., 1994, CONCRETE MATH FDN CO
   Guha S, 2010, J ACM, V58, DOI 10.1145/1870103.1870106
   La Scala BF, 2006, DIGIT SIGNAL PROCESS, V16, P479, DOI 10.1016/j.dsp.2006.04.008
   Le Ny J, 2011, IEEE T AUTOMAT CONTR, V56, P1381, DOI 10.1109/TAC.2010.2095970
   Lothaire  M., 2002, ALGEBRAIC COMBINATOR
   Marshall AW, 2011, SPRINGER SER STAT, P3, DOI 10.1007/978-0-387-68276-1
   MEIER L, 1967, IEEE T AUTOMAT CONTR, VAC12, P528, DOI 10.1109/TAC.1967.1098668
   Nino-Mora J, 2009, IEEE DECIS CONTR P, P2905, DOI 10.1109/CDC.2009.5400949
   Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19
   Rajpathak B, 2012, CHAOS, V22, DOI 10.1063/1.4740061
   Thiele Thorvald, 1880, COMPENSATION QUELQUE
   Verloop I., 2014, HAL00743781 CNRS
   Villar Soffa, 2012, THESIS
   Vul E., 2009, ADV NEURAL INFORM PR, P1955
   WEBER RR, 1990, J APPL PROBAB, V27, P637, DOI 10.2307/3214547
   Whittle P., 1988, J APPL PROB A, P287, DOI [10.2307/3214163, DOI 10.2307/3214163]
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102070
DA 2019-06-15
ER

PT S
AU Dann, C
   Brunskill, E
AF Dann, Christoph
   Brunskill, Emma
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound (O) over tilde(vertical bar S vertical bar(2) vertical bar A vertical bar H-2/epsilon(2) In 1/delta) and a lower PAC bound (Omega) over tilde(O) over tilde(vertical bar S vertical bar(2) vertical bar A vertical bar H-2/epsilon(2) In 1/delta+c) that match up to log-terms and an additional linear dependency on the number of states vertical bar S vertical bar. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finitehorizon MDPs which have a time-horizon dependency of at least H-3.
C1 [Dann, Christoph] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Brunskill, Emma] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
RP Dann, C (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM cdann@cdann.net; ebrun@cs.cmu.edu
FU NSF CAREER award; ONR Young Investigator Program
FX We thank Tor Lattimore for the helpful suggestions and comments. This
   work was supported by an NSF CAREER award and the ONR Young Investigator
   Program.
CR Auer Peter, 2005, P 1 AUSTR COGN VIS W
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115
   Fiechter Claude-Nicolas, 1997, INT C MACH LEARN
   Fiechter Claude-Nicolas, 1994, C LEARN THEOR
   Gheshlaghi Azar Mohammad, 2012, INT C MACH LEARN
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Jaksch Thomas, 2010, ADV NEURAL INFORM PR
   Kakade S. M., 2003, THESIS
   Kearns Michael J, 1999, ADV NEURAL INFORM PR
   Lattimore T., 2012, INT C ALG LEARN THEO
   Mannor S, 2004, J MACH LEARN RES, V5, P623
   Maurer A, 2009, C LEARN THEOR
   Ng Andrew Y, 2009, INT C MACH LEARN
   Reveliotis S, 2007, DISCRETE EVENT DYN S, V17, P307, DOI 10.1007/s10626-007-0014-3
   SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832
   Strehl A. L., 2006, INT C MACH LEARN
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Strehl AL, 2009, J MACH LEARN RES, V10, P2413
   Strehl Alexander L, 2006, C UNC ART INT
   Szita I., 2010, INT C MACH LEARN
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101086
DA 2019-06-15
ER

PT S
AU Dauphin, YN
   de Vries, H
   Bengio, Y
AF Dauphin, Yann N.
   de Vries, Harm
   Bengio, Yoshua
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Equilibrated adaptive learning rates for non-convex optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.
C1 [Dauphin, Yann N.; de Vries, Harm; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
RP Dauphin, YN (reprint author), Univ Montreal, Montreal, PQ, Canada.
EM dauphiya@iro.umontreal.ca; devries@iro.umontreal.ca;
   yoshua.bengio@umontreal.ca
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003
   Bottou L, 1998, LECT NOTES COMPUTER, V1524
   Bradley Andrew M, 2011, ARXIV11102805
   Choromanska A, 2014, LOSS SURFACE MULTILA
   Datta B. N., 2010, NUMERICAL LINEAR ALG
   Dauphin Yann, 2014, NIPS 2014
   Duchi  J., 2011, J MACHINE LEARNING R
   Guggenheimer Heinrich W, 1995, COLL MATH J, V26, P2
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Martens James, 2012, ARXIV12066464
   Pascanu R., 2014, INT C LEARN REPR 201
   Schaul T, 2013, ARXIV13126055
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Sutskever  I., 2013, ICML
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   VANDERSLUIS A, 1969, NUMER MATH, V14, P14, DOI 10.1007/BF02165096
   Vinyals Oriol, 2011, ARXIV11114259
   Zeiler M.D., 2012, ARXIV12125701
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102019
DA 2019-06-15
ER

PT S
AU De Sa, C
   Zhang, C
   Olukotun, K
   Re, C
AF De Sa, Christopher
   Zhang, Ce
   Olukotun, Kunle
   Re, Christopher
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Taming the Wild: A Unified Analysis of HOGWILD!-Style Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (HOGWILD!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called BUCKWILD!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.
C1 [De Sa, Christopher] Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.
   Stanford Univ, Dept Comp Sci, Stanford, CA 94309 USA.
RP De Sa, C (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.
EM cdesa@stanford.edu; czhang@cs.wisc.edu; kunle@stanford.edu;
   chrismre@stanford.edu
FU DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; NSF [IIS-1247701,
   CCF-1111943, CCF-1337375, IIS-1353606]; DOE [108845]; ONR
   [N000141210041, N000141310129]; NIH [U54EB020405]; Oracle; NVIDIA;
   Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American
   Family Insurance; Google; Toshiba
FX The BUCKWILD! name arose out of conversations with Benjamin Recht.
   Thanks also to Madeleine Udell for helpful conversations. The authors
   acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF
   CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF
   IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405;
   Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore
   Foundation; American Family Insurance; Google; and Toshiba.
CR Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   De Sa Christopher, 2015, ICML
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659
   Fercoq O., 2013, ARXIV13125799
   Fleming Thomas R, 1991, COUNTING PROCESSES S, V169, P56
   Gupta P., 2013, P 22 INT C WORLD WID, P505, DOI DOI 10.1145/2488388.2488433
   Gupta S., 2015, ICML
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X
   Konecny Jakub, 2014, NIPS OPT MACH LEARN
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Mitliagkas Ioannis, 2015, PVLDB
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Noel Cyprien, 2014, DOGWILD DISTRIBUTED
   Parambath Shameem Ahamed Puthiya, 2013, MATRIX FACTORIZATION
   Qing Tao, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P537, DOI 10.1007/978-3-642-33460-3_40
   Rakhlin Alexander, 2012, ICML
   Richtarik P., 2012, MATH PROGRAM, P1
   Tappenden Rachael, 2015, ARXIV150303033
   Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168
   Zhang Ce, 2014, PVLDB
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100085
DA 2019-06-15
ER

PT S
AU De Sa, C
   Zhang, C
   Olukotun, K
   Re, C
AF De Sa, Christopher
   Zhang, Ce
   Olukotun, Kunle
   Re, Christopher
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using
   Hierarchy Width
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width-regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.
C1 [De Sa, Christopher] Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.
   Stanford Univ, Dept Comp Sci, Stanford, CA 94309 USA.
RP De Sa, C (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.
EM cdesa@stanford.edu; czhang@cs.wisc.edu; kunle@stanford.edu;
   chrismre@stanford.edu
FU DARPA [FA8750-13-2-0039, FA8750-12-2-0335]; NSF [IIS-1353606,
   IIS-1247701, CCF-1111943, CCF-1337375]; NIH [U54EB020405]; NVIDIA; Moore
   Foundation; American Family Insurance; Google; Toshiba; DOE [108845];
   ONR [N000141210041, N000141310129]; Oracle; Huawei; SAP Labs; Sloan
   Research Fellowship
FX The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF
   IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA
   FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129;
   NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research
   Fellowship; Moore Foundation; American Family Insurance; Google; and
   Toshiba.
CR Chandrasekaran Venkat, 2012, ARXIV12063240
   Diaconis P, 2008, STAT SCI, V23, P151, DOI 10.1214/07-STS252
   Diaconis P, 2010, SANKHYA SER A, V72, P136, DOI 10.1007/s13171-010-0004-7
   Domingos Pedro M, 2012, AAAI
   Gonzalez J., 2011, AISTATS, P324
   Gottlob G, 2014, TRACTABILITY, P3
   Ihler AT, 2005, J MACH LEARN RES, V6, P905
   Koller D., 2009, PROBABILISTIC GRAPHI
   KWISTHOUT JHP, 2010, ECAI, V215, P237, DOI DOI 10.3233/978-1-60750-606-5-237
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Liu Xianghang, 2014, ADV NEURAL INFORM PR, V27, P1377
   Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680
   Marx D, 2013, J ACM, V60, DOI 10.1145/2535926
   McCallum A., 2009, ADV NEURAL INFORM PR, P1249
   Newman D., 2007, ADV NEURAL INFORM PR, P1081
   Ng KS, 2008, ANN MATH ARTIF INTEL, V54, P159, DOI 10.1007/s10472-009-9136-7
   Peters SE, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0113523
   Poole D., 2003, P 18 INT JOINT C ART, P985
   ROBERTSON N, 1986, J ALGORITHM, V7, P309, DOI 10.1016/0196-6774(86)90023-4
   Shin Jaeho, 2015, PVLDB
   Singla P., 2008, P 23 AAAI C ART INT, P1094
   Smola Alexander, 2010, PVLDB
   Suciu Dan, 2011, SYNTHESIS LECT DATA, DOI 10.2200/S00362ED1V01Y201105DTM016
   Surdeanu  M., OVERVIEW ENGLISH SLO
   Theis Lucas, 2012, NIPS, P1124
   Venugopal D., 2012, NIPS, V25, P1655
   Venugopal Deepak, 2015, AAAI C ART INT
   Zhang Ce, 2014, PVLDB
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101017
DA 2019-06-15
ER

PT S
AU Dehaene, G
   Barthelme, S
AF Dehaene, Guillaume
   Barthelme, Simon
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bounding errors of Expectation-Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of O(n(-2)) for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.
C1 [Dehaene, Guillaume] Univ Geneva, Geneva, Switzerland.
   [Barthelme, Simon] CNRS, Gipsa lab, Paris, France.
RP Dehaene, G (reprint author), Univ Geneva, Geneva, Switzerland.
EM guillaume.dehaene@gmail.com; simon.barthelme@gipsa-lab.fr
CR Bishop Christopher M., 2006, CORR
   BRASCAMP HJ, 1976, ADV MATH, V20, P151, DOI 10.1016/0001-8708(76)90184-5
   DasGupta A, 2008, SPRINGER TEXTS STAT, P1
   Dehaene Guillaume, 2015, TECHNICAL REPORT
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   Minka T. P, 2005, TECHNICAL REPORT
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Nickisch H, 2008, J MACH LEARN RES, V9, P2035
   Raymond Jack, 2014, EXPECTATION PROPAGAT
   Saumard A, 2014, STAT SURV, V8, P45, DOI 10.1214/14-SS107
   Seeger M., 2005, TECHNICAL REPORT
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102061
DA 2019-06-15
ER

PT S
AU Dekel, O
   Eldan, R
   Koren, T
AF Dekel, Ofer
   Eldan, Ronen
   Koren, Tomer
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of (O) over tilde (T-5/6), while the best known lower bound is Omega(T-1/2). Many attempts have been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of (O) over tilde (T-2/3). We present an efficient algorithm for the bandit smooth convex optimization problem that guarantees a regret of ($) over tilde (T-5/8). Our result rules out an Omega(T-2/3) lower bound and takes a significant step towards the resolution of this open problem.
C1 [Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA.
   [Eldan, Ronen] Weizmann Inst Sci, Rehovot, Israel.
   [Koren, Tomer] Technion, Haifa, Israel.
RP Dekel, O (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM oferd@microsoft.com; roneneldan@gmail.com; tomerk@technion.ac.il
CR Abernethy Jacob, 2009, 2009 Information Theory and Applications Workshop (ITA), P280, DOI 10.1109/ITA.2009.5044958
   Abernethy  J., 2008, P 21 ANN C LEARN THE
   Agarwal A., 2011, ADV NEURAL INFORM PR
   Agarwal Alekh, 2010, P 23 ANN C LEARN THE
   Bubeck S., 2015, ARXIV150706580
   Bubeck S., 2015, ARXIV14121587
   Bubeck S., 2015, P 28 ANN C LEARN THE
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Dani Varsha, 2008, ADV NEURAL INFORM PR
   Dekel O., 2014, P 46 ANN S THEOR COM
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Hazan E., 2014, ADV NEURAL INFORM PR
   Nesterov Y., 1994, INTERIOR POINT POLYN, V13
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Saha A., 2011, INT C ART INT STAT A, P636
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101101
DA 2019-06-15
ER

PT S
AU Desjardins, G
   Simonyan, K
   Pascanu, R
   Kavukcuoglu, K
AF Desjardins, Guillaume
   Simonyan, Karen
   Pascanu, Razvan
   Kavukcuoglu, Koray
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Natural Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.
C1 [Desjardins, Guillaume; Simonyan, Karen; Pascanu, Razvan; Kavukcuoglu, Koray] Google DeepMind, London, England.
RP Desjardins, G (reprint author), Google DeepMind, London, England.
EM gdesjardins@google.com; simonyan@google.com; razp@google.com;
   korayk@google.com
CR Amari Shun-Ichi, 1998, NEURAL COMPUTATION
   Ba Jimmy, 2014, NIPS
   Beck Amir, 2003, OPER RES LETT
   Bottou L, 1998, LECT NOTES COMPUTER, V1524
   Combettes P L., 2009, ARXIV E PRINTS
   Duchi J., 2011, JMLR
   Glorot X., 2010, AISTATS
   Ioffe S., 2015, ICML
   Krizhevsky A., 2009, THESIS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Martens J., 2010, ICML
   Martens Roger Grosse James, 2015, ICML
   Miiller K.-R., 2013, NEURAL NETWORKS TRIC
   Nicol N, 1998, IDSIA3398
   Ollivier Yann, 2013, ABS13030818 ARXIV
   Pascanu R., 2014, ICLR
   Povey Daniel, 2015, ICLR WORKSH
   Raiko T, 2012, AISTATS
   Raskutti G., 2013, INFORM GEOMETRY MIRR
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Salakhutdinov Ruslan, 2015, ICML
   Simonyan  K., 2015, INT C LEARN REPR
   Sohl-Dickstein Jascha, 2012, NATURAL GRADIENT ANA
   Srivastava N., 2014, J MACHINE LEARNING R
   Szegedy  C., 2014, GOING DEEPER CONVOLU
   Thomas Philip S, 2013, ADV NEURAL INFORM PR, V26
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Vatanen Tommi, 2013, ICONIP
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102101
DA 2019-06-15
ER

PT S
AU Dezfouli, A
   Bonilla, EV
AF Dezfouli, Amir
   Bonilla, Edwin V.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Scalable Inference for Gaussian Process Models with Black-Box
   Likelihoods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a sparse method for scalable automated variational inference (AVI) in a large class of models with Gaussian process (GP) priors, multiple latent functions, multiple outputs and non-linear likelihoods. Our approach maintains the statistical efficiency property of the original AVI method, requiring only expectations over univariate Gaussian distributions to approximate the posterior with a mixture of Gaussians. Experiments on small datasets for various problems including regression, classification, Log Gaussian Cox processes, and warped GPs show that our method can perform as well as the full method under high sparsity levels. On larger experiments using the MNIST and the SARCOS datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models.
C1 [Dezfouli, Amir; Bonilla, Edwin V.] Univ New South Wales, Sydney, NSW, Australia.
RP Dezfouli, A (reprint author), Univ New South Wales, Sydney, NSW, Australia.
EM akdezfuli@gmail.com; e.bonilla@unsw.edu.au
FU UNSW's Faculty of Engineering Research Grant Program project [PS37866];
   AWS in Education Research Grant award; Australian Research Council
   [DP150104878]
FX This work has been partially supported by UNSW's Faculty of Engineering
   Research Grant Program project #PS37866 and an AWS in Education Research
   Grant award. AD was also supported by a grant from the Australian
   Research Council #DP150104878.
CR Alvarez MA, 2011, J MACH LEARN RES, V12, P1459
   Alvarez Mauricio A., 2010, AISTATS
   BACHE K., 2013, UCI MACHINE LEARNING
   Damianou A., 2013, AISTATS
   Domingos Pedro, 2006, AAAI
   Gal Y., 2014, NIPS
   Goodman N. D., 2008, UAI
   Hensman J., 2015, AISTATS
   Hensman J., 2013, UAI
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   JARRETT RG, 1979, BIOMETRIKA, V66, P191, DOI 10.2307/2335266
   Lawrence Neil D, 2002, NIPS
   Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115
   Murray Iain, 2010, AISTATS
   Nguyen T. V., 2014, UAI
   Nguyen Trung V., 2014, ICML
   Nguyen Trung V., 2014, NIPS
   Nickisch Hannes, 2008, JMLR, V9
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Ranganath Rajesh, 2014, AISTATS
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Snelson E., 2003, NIPS
   Snelson E., 2006, NIPS
   Titsias M., 2009, AISTATS
   Vijayakumar Sethu, 2000, ICML
   Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807
   Wilson A. G., 2012, ICML
   Yang Z., 2015, AISTATS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100033
DA 2019-06-15
ER

PT S
AU Diakonikolas, I
   Hardt, M
   Schmidt, L
AF Diakonikolas, Ilias
   Hardt, Moritz
   Schmidt, Ludwig
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Differentially Private Learning of Structured Discrete Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DENSITY; ALGORITHM
AB We investigate the problem of learning an unknown probability distribution over a discrete population from random samples. Our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing Differential Privacy to the individuals of the population.
   We describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families. Our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient-both in terms of sample size and running time-as their non-private counterparts. We complement our theoretical guarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.
C1 [Diakonikolas, Ilias] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Hardt, Moritz] Google Res, Mountain View, CA USA.
   [Schmidt, Ludwig] MIT, Cambridge, MA 02139 USA.
RP Diakonikolas, I (reprint author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.
FU EPSRC [EP/L021749/1]; Marie Curie Career Integration grant; MADALGO;
   MIT-Shell Initiative
FX Ilias Diakonikolas was supported by EPSRC grant EP/L021749/1 and a Marie
   Curie Career Integration grant. Ludwig Schmidt was supported by MADALGO
   and a grant from the MIT-Shell Initiative.
CR Acharya J., 2015, SAMPLE OPTIMAL DENSI
   Balabdaoui F, 2007, ANN STAT, V35, P2536, DOI 10.1214/009053607000000262
   Baldi P., 2014, NATURE COMMUNICATION
   Beimel Amos, 2013, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Algorithms and Techniques. 16th International Workshop, APPROX 2013 and 17th International Workshop, RANDOM 2013. Proceedings: LNCS 8096, P363, DOI 10.1007/978-3-642-40328-6_26
   Birge L, 1997, ANN STAT, V25, P970, DOI 10.1214/aos/1069362733
   BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488
   Bun Mark, 2015, CORR
   CHAN S., 2014, NIPS, P1844
   Chan S., 2014, STOC, P604, DOI DOI 10.1145/2591796.2591848
   Chan S. O., 2013, SODA
   Daskalakis C., 2012, SODA, P1371
   Devroye L., 2001, SPRINGER SERIES STAT
   DUCHI J. C., 2013, ADV NEURAL INFORM PR, P1529
   Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53
   Dumbgen L, 2009, BERNOULLI, V15, P40, DOI 10.3150/08-BEJ141
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Dwork C, 2009, LECT NOTES COMPUT SC, V5444, P496
   Dwork Cynthia, 2010, FOCS
   Feldman J, 2005, ANN IEEE SYMP FOUND, P501
   Freund Y., 1999, COLT
   Grenander U, 1956, SKAND AKTUARIETIDSK, V39, P125
   GROENEBOOM P., 1985, P BERK C HON J NEYM, VII, P539
   Hardt Moritz, 2012, NIPS
   Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155
   Li C, 2014, PROC VLDB ENDOW, V7, P341
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   RAO BLSP, 1969, SANKHYA SER A, V31, P23
   ROTE G, 1992, COMPUTING, V48, P337, DOI 10.1007/BF02238642
   Walther G., 2009, STAT SCI
   Warner S., 1965, J AM STAT ASS, V60
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100081
DA 2019-06-15
ER

PT S
AU Domke, J
AF Domke, Justin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Maximum Likelihood LearningWith Arbitrary Treewidth via Fast-Mixing
   Parameter Sets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID POLYNOMIAL-TIME; MODELS
AB Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of "fast-mixing parameters" where Markov chainMonte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon,disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution epsilon-accurate in parameter distance with effort quadratic in 1/epsilon. Both of these provide of a fully-polynomial time randomized approximation scheme.
C1 [Domke, Justin] Australian Natl Univ, NICTA, Canberra, ACT, Australia.
RP Domke, J (reprint author), Australian Natl Univ, NICTA, Canberra, ACT, Australia.
EM justin.domke@nicta.com.au
FU Australian Government through the Dept. of Communications; Australian
   Research Council through the ICT Centre of Excellence Program
FX Thanks to Ivona Bezakova, Aaron Defazio, Nishant Mehta, Aditya Menon,
   Cheng Soon Ong and Christfried Webers. NICTA is funded by the Australian
   Government through the Dept. of Communications and the Australian
   Research Council through the ICT Centre of Excellence Program.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Asuncion A, 2010, AISTATS
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   Boucheron S., 2013, CONCENTRATION INEQUA
   Carreira-Peripinan M. A, 2005, AISTATS
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Descombes X, 1999, IEEE T IMAGE PROCESS, V8, P954, DOI 10.1109/83.772239
   Domke Justin, 2013, NIPS
   Dyer M, 2009, ANN APPL PROBAB, V19, P71, DOI 10.1214/08-AAP532
   Geyer C, 1991, S INT
   Gu MG, 2001, J ROY STAT SOC B, V63, P339, DOI 10.1111/1467-9868.00289
   Hayes T, 2006, FOCS
   Heinemann U., 2014, ICML
   Hinton Geoffrey E., 2010, TECHNICAL REPORT
   Huber M, 2011, J STAT THEORY PRACT, V5, P413, DOI 10.1080/15598608.2011.10412038
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   Koller D., 2009, PROBABILISTIC GRAPHI
   Levin D. A., 2006, MARKOV CHAINS MIXING
   Lindsay BG, 1988, CONT MATH, V80, P221, DOI DOI 10.1090/CONM/080/999014
   Liu X, 2014, NIPS
   Marlin B, 2011, UAI
   Mizrahi Y, 2014, ICML
   Papandreou G., 2011, ICCV
   Salakhutdinov R., 2009, NIPS
   Schmidt M, 2011, NIPS
   Schmidt Uwe, 2010, CVPR
   Steinhardt J, 2015, ICML
   Tieleman T., 2008, ICML
   Varin C, 2011, STAT SINICA, V21, P5
   Wainwright MJ, 2006, J MACH LEARN RES, V7, P1829
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102056
DA 2019-06-15
ER

PT S
AU Du, N
   Wang, YC
   He, N
   Song, L
AF Du, Nan
   Wang, Yichen
   He, Niao
   Song, Le
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Time-Sensitive Recommendation From Recurrent User Activities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item at the right moment, and how to predict the next returning time of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains O(1/epsilon) convergence rate, scales up to problems with millions of user-item pairs and hundreds of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation tasks. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.
C1 [Du, Nan; Wang, Yichen; Song, Le] Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.
   [He, Niao] Georgia Tech, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA USA.
RP Du, N (reprint author), Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.
EM dunan@gatech.edu; yichen.wang@gatech.edu; nhe6@gatech.edu;
   lsong@cc.gatech.edu
FU NSF [IIS-1116886]; NSF/NIH BIGDATA [1R01GM108341]; NSF CAREER
   [IIS-1350983]
FX The research was supported in part by NSF IIS-1116886, NSF/NIH BIGDATA
   1R01GM108341, NSF CAREER IIS-1350983.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Baltrunas L., 2009, TIME DEPENDANT RECOM
   Bhargava J. Z. J. L. Preeti, 2015, WWW
   Chi E. C., 2012, TENSORS SPARSITY NON
   Cox D., 1980, POINT PROCESSES, V12
   Cox D., 2006, STAT METHODS APPL, V1, P159
   Daley DJ, 2007, INTRO THEORY POINT P, VII
   Du N., 2013, ARTIFICIAL INTELLIGE
   Du N., 2012, COLLECTION ADV NEURA, P2789
   Du N., 2015, KDD 15
   Harchaoui A. N. Zaid, 2013, MATH PROGRAMMING
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Kapoor K., 2014, P 20 ACM SIGKDD INT, P1719, DOI DOI 10.1145/2623330.2623348
   Kapoor K., 2015, P 8 ACM INT C WEB SE, P233, DOI DOI 10.1145/2684822.2685306
   Karatzoglou A., 2010, PROC 4 ACM C REC SYS
   KINGMAN JFC, 1964, P CAMB PHILOS SOC, V60, P923
   Koenigstein N., 2011, P 5 ACM C REC SYST, P165, DOI [DOI 10.1145/2043932.2043964, 10.1145/2043932.2043964]
   Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447
   Lan G., 2012, MATH PROGRAMMING
   Lan G., 2014, ARXIV13095550V2
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Ouyang H., 2013, ICML
   Rendle S., 2011, STUDIES COMPUTATIONA, V330, P137
   Sastry S., 1990, SOME NP COMPLETE PRO
   Wang Y., 2015, KDD
   Xiong L, 2010, P SIAM INT C DAT MIN, P211, DOI DOI 10.1137/1.9781611972801.19
   Yi X, 2014, P 8 ACM C REC SYST, P113, DOI DOI 10.1145/2645710.2645724
   Yu A. W., 2014, NIPS
   Zhou K., 2013, INT C MACH LEARN ICM
   Zhou K., 2013, ARTIFICIAL INTELLIGE
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103017
DA 2019-06-15
ER

PT S
AU El Alaoui, A
   Mahoney, MW
AF El Alaoui, Ahmed
   Mahoney, Michael W.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Randomized Kernel Ridge Regression with Statistical Guarantees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MONTE-CARLO ALGORITHMS; APPROXIMATION
AB One approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of statistical leverage scores to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the effective dimensionality of the problem. This latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom. We give an empirical evidence supporting this fact. Our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples. More precisely, the running time of the algorithm is O (np(2)) with p only depending on the trace of the kernel matrix and the regularization parameter. This is obtained via a variant of squared length sampling that we adapt to the kernel setting. Lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.
C1 [El Alaoui, Ahmed] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Mahoney, Michael W.] Univ Calif Berkeley, Stat & Int Comp Sci Inst, Berkeley, CA 94720 USA.
RP El Alaoui, A (reprint author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM elalaoui@eecs.berkeley.edu; mmahoney@stat.berkeley.edu
CR Bach F., 2013, J MACH LEARN RES, V30, P185
   Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521
   Bach Francis, 2015, COMMUNICATION
   Chatterjee S, 1986, STAT SCI, V1, P379, DOI DOI 10.1214/SS/1177013622
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6
   El Alaoui Ahmed, 2014, ARXIV14110306
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3
   Kumar S., 2009, P INT C ART INT STAT, P304
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Zhang Y., 2013, C LEARN THEOR, P592
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100084
DA 2019-06-15
ER

PT S
AU Ellis, K
   Solar-Lezama, A
   Tenenbaum, JB
AF Ellis, Kevin
   Solar-Lezama, Armando
   Tenenbaum, Joshua B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Unsupervised Learning by Program Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our techniques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures, and a technique for applying program synthesis tools to noisy data.
C1 [Ellis, Kevin; Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.
   [Solar-Lezama, Armando] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Ellis, K (reprint author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.
EM ellisk@mit.edu; asolar@csail.mit.edu; jbt@mit.edu
FU NSF [SHF-1161775]; Center for Minds, Brains and Machines (CBMM) - NSF
   STC award [CCF-1231216]; ARO MURI contract [W911NF-08-1-0242]
FX We are grateful for discussions with Timothy O'Donnell on morphological
   rule learners, for advice from Brendan Lake and Tejas Kulkarni on the
   convolutional network baselines, and for the suggestions of our
   anonymous reviewers. This material is based upon work supported by
   funding from NSF award SHF-1161775, from the Center for Minds, Brains
   and Machines (CBMM) funded by NSF STC award CCF-1231216, and from ARO
   MURI contract W911NF-08-1-0242.
CR Albright A, 2003, COGNITION, V90, P119, DOI 10.1016/S0010-0277(03)00146-X
   Baayen R, 1995, CELEX2 LDC96L14
   Chan E., 2010, RES LANGUAGE COMPUTA, V8, P209, DOI [10. 1007/s11168-011-9077-2, DOI 10.1007/S11168-011-9077-2]
   de Moura L, 2008, LECT NOTES COMPUT SC, V4963, P337, DOI 10.1007/978-3-540-78800-3_24
   Dehaene S, 2006, SCIENCE, V311, P381, DOI 10.1126/science.1121739
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fleuret F, 2011, P NATL ACAD SCI USA, V108, P17621, DOI 10.1073/pnas.1109168108
   Goldsmith J, 2001, COMPUT LINGUIST, V27, P153, DOI 10.1162/089120101750300490
   Goodman N., 2008, P 24 C UNC ART INT, V8, P220
   Gregor Karol, 2015, CORR
   Gulwani S, 2011, PLDI 11: PROCEEDINGS OF THE 2011 ACM CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P62
   Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423
   Gulwani Sumit, 2015, COMMUN ACM
   Katz Yarden, 2008, COGSCI, P71
   Koza J. R., 1993, GENETIC PROGRAMMING
   Lake B. M, 2013, ADV NEURAL INFORM PR, V26, P2526
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lezama Armando Solar, 2008, THESIS
   Liang P., 2010, P 27 INT C MACH LEAR, P639
   O'Donnell TJ, 2015, PRODUCTIVITY REUSE L
   Rumelhart D., 1986, PARALLEL DISTRIBUTED, P216
   Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150
   Seidenberg MS, 2014, COGNITIVE SCI, V38, P1190, DOI 10.1111/cogs.12147
   SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2
   Thornburg David D., 1983, COMPUTE          MAR
   Torlak E., 2013, P 2013 ACM INT S NEW, P135, DOI DOI 10.1145/2509578.2509586
   Virpioja  S., 2013, TECHNICAL REPORT
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101045
DA 2019-06-15
ER

PT S
AU Erdogdu, MA
AF Erdogdu, Murat A.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (n >> p >> 1). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets.
C1 [Erdogdu, Murat A.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Erdogdu, MA (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
EM erdogdu@stanford.edu
CR Baik J, 2006, J MULTIVARIATE ANAL, V97, P1382, DOI 10.1016/j.jmva.2005.08.003
   Bishop C M, 1995, NEURAL NETWORKS PATT
   Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0
   Bottou Leon, 2010, LARGESCALE MACHINE L
   Boyd S., 2004, CONVEX OPTIMIZATION
   Byrd R. H., 2014, ARXIV14017020
   Byrd Richard H, 2011, SIAM J OPTIMIZATION
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen LHY, 2011, PROBAB APPL SER, P1, DOI 10.1007/978-3-642-15007-4
   Dicker Lee H, 2015, ARXIV150904388
   Donoho D L, 2013, ARXIV13110851
   Donoho David L, 2013, ARXIV13055870
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Erdogdu M. A., 2015, ARXIV150802810
   Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Gibbs Alison L, 2002, ISR, V70
   Graf Franz, 2011, 2D IMAGE REGISTRATIO
   Koller D., 2009, PROBABILISTIC GRAPHI
   Le Roux Nicolas, 2010, FAST NATURAL NEWTON
   Le Roux Nicolas, 2008, TOPMOUMOUTE ONLINE N
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin Chih-J, 2008, JMLR
   Martens James, 2010, DEEP LEARNING VIA HE, P735
   McCullagh P., 1989, GEN LINEAR MODELS, V2
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Shun-Ichi Amari, 1998, NEURAL COMPUTATION, V10
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Vershynin  Roman, 2010, ARXIV10113027
   Vinyals Oriol, 2012, KRYLOV SUBSPACE DESC
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101010
DA 2019-06-15
ER

PT S
AU Esser, SK
   Appuswamy, R
   Merolla, PA
   Arthur, JV
   Modha, DS
AF Esser, Steve K.
   Appuswamy, Rathinakumar
   Merolla, Paul A.
   Arthur, John V.
   Modha, Dharmendra S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Backpropagation for Energy-Efficient Neuromorphic Computing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NETWORK; SYSTEM
AB Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets. For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses. Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of 64), we achieve 99.42% accuracy at 108 mu J per image, and with a high efficiency network (ensemble of 1) we achieve 92.7% accuracy at 0.268 mu J per image.
C1 [Esser, Steve K.; Appuswamy, Rathinakumar; Merolla, Paul A.; Arthur, John V.; Modha, Dharmendra S.] IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.
RP Esser, SK (reprint author), IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.
EM sesser@us.ibm.com; rappusw@us.ibm.com; pameroll@us.ibm.com;
   arthurjo@us.ibm.com; dmodha@us.ibm.com
FU Defense Advanced Research Projects Agency [HR0011- 09-C-0002,
   FA9453-15-C-0055]
FX This research was sponsored by the Defense Advanced Research Projects
   Agency under contracts No. HR0011- 09-C-0002 and No. FA9453-15-C-0055.
   The views, opinions, and/or findings contained in this paper are those
   of the authors and should not be interpreted as representing the
   official views or policies of the Department of Defense or the U.S.
   Government.
CR BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cassidy AS, 2013, IEEE IJCNN
   Cheng Z., 2015, ARXIV150303562
   Diehl PU, 2015, IEEE IJCNN
   Fiesler E., 1990, HAG 90 12 16 APR, P164
   Hannun A., 2014, ARXIV14125567
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moerland P, 1997, HDB NEURAL COMPUTATI
   Muller L. K., 2015, ARXIV150405767
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Stromatias E., 2015, INT JOINT C NEUR NET
   Zhao JY, 1996, NEURAL NETWORKS, V9, P991, DOI 10.1016/0893-6080(96)00025-1
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102011
DA 2019-06-15
ER

PT S
AU Fan, K
   Wang, ZT
   Beck, J
   Kwok, JT
   Heller, K
AF Fan, Kai
   Wang, Ziteng
   Beck, Jeffrey
   Kwok, James T.
   Heller, Katherine
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Second-Order Stochastic Backpropagation for Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.
C1 [Fan, Kai; Beck, Jeffrey; Heller, Katherine] Duke Univ, Durham, NC 27706 USA.
   [Wang, Ziteng; Kwok, James T.] HKUST, Hong Kong, Hong Kong, Peoples R China.
RP Fan, K (reprint author), Duke Univ, Durham, NC 27706 USA.
EM kai.fan@stat.duke.edu; wangzt2012@gmail.com; jeff.beck@duke.edu;
   jamesk@cse.ust.hk; kheller@gmail.com
FU Research Grants Council of the Hong Kong Special Administrative Region
   [614513]
FX This research was supported in part by the Research Grants Council of
   the Hong Kong Special Administrative Region (Grant No. 614513).
CR Beal M.J., 2003, THESIS
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bonnans J. F., 2006, NUMERICAL OPTIMIZATI
   BONNET G, 1964, ANNALES TELECOMMUN, V19, P203
   Dahl G. E., 2013, ICASSP
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   FERGUSON TS, 1962, ANN MATH STAT, V33, P986, DOI 10.1214/aoms/1177704466
   Gan Z., 2015, NIPS
   Gregor K., 2015, ICML
   Hensman James, 2012, NIPS
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Khan Mohammad E, 2014, NIPS
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Martens J., 2010, ICML
   Mnih A., 2014, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ngiam  Jiquan, 2011, ICML
   Pascanu R., 2013, ARXIV13013584
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   PRICE R, 1958, IRE T INFORM THEOR, V4, P69, DOI 10.1109/TIT.1958.1057444
   Rezende D. J., 2014, ICML
   Salimans T., 2015, ICML
   Sutskever  I., 2014, NIPS
   Titsias M., 2014, ICML
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100038
DA 2019-06-15
ER

PT S
AU Feldman, V
   Perkins, W
   Vempala, S
AF Feldman, Vitaly
   Perkins, Will
   Vempala, Santosh
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Subsampled Power Iteration: a Unified Algorithm for Block Models and
   Planted CSP's
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ATTRIBUTE-EFFICIENT
AB We present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (CSP), via a common generalization in terms of random bipartite graphs. Our algorithm matches up to a constant factor the best-known bounds for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches.
   The main contribution of the algorithm is in the case of unequal sizes in the bi-partition that arises in our reduction from the planted CSP. Here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix.
   Other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.
C1 [Feldman, Vitaly] IBM Res Almaden, San Jose, CA 95120 USA.
   [Perkins, Will] Univ Birmingham, Birmingham, W Midlands, England.
   [Vempala, Santosh] Georgia Tech, Atlanta, GA 30332 USA.
RP Feldman, V (reprint author), IBM Res Almaden, San Jose, CA 95120 USA.
EM vitaly@post.harvard.edu; w.f.perkins@bham.ac.uk; vempala@cc.gatech.edu
FU NSF [CCF-1217793]
FX S. Vempala supported in part by NSF award CCF-1217793.
CR Abbe E, 2014, ARXIV14053267
   Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611
   Berthet Q., 2013, C LEARN THEOR, V30, P1046
   BLUM A, 1992, MACH LEARN, V9, P373, DOI 10.1023/A:1022653502461
   Bogdanov A, 2009, LECT NOTES COMPUT SC, V5687, P392, DOI 10.1007/978-3-642-03685-9_30
   Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22
   Coja-Oghlan A, 2003, LECT NOTES COMPUT SC, V2751, P15
   Coja-Oghlan A, 2010, SIAM J DISCRETE MATH, V23, P2000, DOI 10.1137/080730160
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   Daniely A., 2013, ADV NEURAL INFORM PR, V26, P145
   Daniely A., 2014, CORRABS14043378
   Decatur SE, 2000, SIAM J COMPUT, V29, P854, DOI 10.1137/S0097539797325648
   Feige U, 2005, RANDOM STRUCT ALGOR, V27, P251, DOI 10.1002/rsa.20089
   Feige U, 2004, LECT NOTES COMPUT SC, V3142, P519
   FELDMAN V., 2013, P 45 ANN ACM S THEOR, P655
   Feldman V., 2014, CORRABS14072774
   Feldman V., 2014, COLT, P1283
   Feldman V., 2015, P 47 ANN ACM S THEOR, P77
   Feldman V, 2007, J MACH LEARN RES, V8, P1431
   Florescu  Laura, 2015, ARXIV150606737
   FREDMAN ML, 1984, J ACM, V31, P538, DOI 10.1145/828.1884
   Friedman J, 2005, SIAM J COMPUT, V35, P408, DOI 10.1137/S009753970444096X
   Goerdt A., 2001, STACS 2001. 18th Annual Symposium on Theoretical Aspects of Computer Science. Proceedings (Lecture Notes in Computer Science Vol.2010), P294
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Korada S. B., 2011, P ACM SIGMETRICS JOI, P209
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mossel E, 2013, ARXIV13114115
   O'Donnell R., 2014, C COMP COMPL
   Servedio RA, 2000, J COMPUT SYST SCI, V60, P161, DOI 10.1006/jcss.1999.1666
   Shalev-Shwartz S., 2012, P 15 INT C ART INT S, P1019
   Vu V., 2014, ARXIV14043918
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101094
DA 2019-06-15
ER

PT S
AU Flach, PA
   Kull, M
AF Flach, Peter A.
   Kull, Meelis
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Precision-Recall-Gain Curves: PR Analysis Done Right
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CLASSIFICATION
AB Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracy-based performance assessment, many researchers have taken to report PrecisionRecall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions - e.g., the area under a PR curve takes the arithmetic mean of precision values whereas the F-beta score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system, and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular, the area under Precision-Recall-Gain curves conveys an expected F-1 score on a harmonic scale, and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of b values for which the point optimises F-beta. We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected F-1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection.
C1 [Flach, Peter A.; Kull, Meelis] Univ Bristol, Intelligent Syst Lab, Bristol, Avon, England.
RP Flach, PA (reprint author), Univ Bristol, Intelligent Syst Lab, Bristol, Avon, England.
EM Peter.Flach@bristol.ac.uk; Meelis.Kull@bristol.ac.uk
FU REFRAME project - European Coordinated Research on Long-Term Challenges
   in Information and Communication Sciences & Technologies ERA-Net
   (CHIST-ERA); Engineering and Physical Sciences Research Council in the
   UK [EP/K018728/1]
FX This work was supported by the REFRAME project granted by the European
   Coordinated Research on Long-Term Challenges in Information and
   Communication Sciences & Technologies ERA-Net (CHIST-ERA), and funded by
   the Engineering and Physical Sciences Research Council in the UK under
   grant EP/K018728/1. Discussions with Hendrik Blockeel helped to clarify
   the intuitions underlying this work.
CR Boyd Kendrick, 2012, Proc Int Conf Mach Learn, V2012, P349
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   Fawcett T, 2007, MACH LEARN, V68, P97, DOI 10.1007/s10994-007-5011-0
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Flach P., 2010, ENCY MACHINE LEARNIN, P869, DOI DOI 10.1007/978-0-387-30164-8_733
   Flach P.A., 2003, P 20 INT C MACH LEAR, P194
   Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831
   Hernandez-Orallo J, 2012, J MACH LEARN RES, V13, P2813
   Lipton Zachary C, 2014, Mach Learn Knowl Discov Databases, V8725, P225, DOI 10.1007/978-3-662-44851-9_15
   Nagarajan N., 2014, ADV NEURAL INFORM PR, P2744
   Narasimhan H., 2014, ADV NEURAL INF PROC, V27, P1493
   Parambath S. Puthiya, 2014, ADV NEURAL INFORM PR, V27, P2123
   Platt J, 1999, ADV LARGE MARGIN CLA, V10, P61
   Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854
   Sluban Borut, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P650, DOI 10.1007/978-3-642-40994-3_47
   Van Rijsbergen C.J., 1979, INFORM RETRIEVAL
   Vanschoren J., 2013, ACM SIGKDD EXPLORATI, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]
   Ye N., 2012, P 29 INT C MACH LEAR
   Zadrozny B., 2001, P 18 INT C MACH LEAR, P609
   Zhao MJ, 2013, J MACH LEARN RES, V14, P1033
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102016
DA 2019-06-15
ER

PT S
AU Foster, DJ
   Rakhlin, A
   Sridharan, K
AF Foster, Dylan J.
   Rakhlin, Alexander
   Sridharan, Karthik
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Adaptive Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BOUNDS
AB We propose a general framework for studying adaptive regret bounds in the online learning setting, subsuming model selection and data-dependent bounds. Given a data-or model-dependent bound we ask, "Does there exist some algorithm achieving this bound?" We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes.
   Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem.
C1 [Foster, Dylan J.; Sridharan, Karthik] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   [Rakhlin, Alexander] Univ Penn, Dept Stat, Philadelphia, PA 19104 USA.
RP Foster, DJ (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
CR Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812
   Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P297
   Chiang C., 2012, COLT
   Cover Thomas M., 1967, T 4 PRAG C INF THEOR, P263
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Even-Dar E, 2008, MACH LEARN, V72, P21, DOI 10.1007/s10994-008-5060-z
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Koolen W. M., 2015, COLT, V40, P1155
   Liang Tengyuan, 2015, P 28 C LEARN THEOR
   Lugosi G, 1999, ANN STAT, V27, P1830
   Luo Haipeng, 2015, CORR
   Massart Pascal, 2007, CONCENTRATION INEQUA, V10
   Mendelson Shahar, 2014, C LEARN THEOR
   Orabona Francesco, 2014, P 27 C LEARN THEOR
   Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI 10.1214/aop/1176988477
   Rakhlin Alexander, 2010, ARXIV10113168
   Rakhlin Alexander, 2014, P 27 C LEARN THEOR
   Rakhlin Alexander, 2010, ADV NEURAL INFORM PR, V23
   Rakhlin Alexander, 2014, PROBABILITY THEORY R
   Rakhlin Alexander, 2012, STAT LEARNING THEORY
   Rakhlin Alexander, 2013, P 26 ANN C LEARN THE
   Rakhlin S., 2012, ADV NEURAL INFORM PR, V25, P2150
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101103
DA 2019-06-15
ER

PT S
AU Frogner, C
   Zhang, CY
   Mobahi, H
   Araya-Polo, M
   Poggio, T
AF Frogner, Charlie
   Zhang, Chiyuan
   Mobahi, Hossein
   Araya-Polo, Mauricio
   Poggio, Tomaso
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning with a Wasserstein Loss
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DISTANCE
AB Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.
C1 [Frogner, Charlie; Zhang, Chiyuan; Poggio, Tomaso] MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.
   [Mobahi, Hossein] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Araya-Polo, Mauricio] Shell Int E&P Inc, The Hague, Netherlands.
RP Frogner, C (reprint author), MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.
EM frogner@mit.edu; chiyuan@mit.edu; hmobahi@csail.mit.edu;
   Mauricio.Araya@shell.com; tp@ai.mit.edu
CR Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001
   Bertsimas D, 1997, INTRO LINEAR OPTIMIZ
   Bogachev VI, 2012, RUSS MATH SURV+, V67, P785, DOI 10.1070/RM2012v067n05ABEH004808
   Chen L. C., 2015, ICLR
   Chizat Lenaic, 2015, UNBALANCED OPTIMAL T
   Coen M. H., 2010, ICML, P231
   Cuturi M., 2013, NIPS
   Cuturi M., 2014, ICML
   Cuturi Marco, 2015, SMOOTHED DUAL APPROA
   Edelsbrunner Herbert, 2012, P EUR C MATH
   GIVENS CR, 1984, MICH MATH J, V31, P231
   Grauman K., 2004, CVPR
   Knight Philip A, 2012, IMA J NUMER ANAL, V33
   Ledoux M, 2011, CLASS MATH, P1
   Long Jonathan, 2015, CVPR IN PRESS
   Mikolov T., 2013, NIPS
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Shirdhonkar S, 2008, CVPR
   Solomon J., 2014, P 31 INT C MACH LEAR, V32
   Thomee B., 2015, ARXIV150301817
   Vedaldi A., 2014, ABS14124564 CORR
   Villani C., 2008, OPTIMAL TRANSPORT OL
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100047
DA 2019-06-15
ER

PT S
AU Frongillo, R
   Kash, IA
AF Frongillo, Rafael
   Kash, Ian A.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On Elicitation Complexity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question-all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable. Specifically, what is the minimum number of regression parameters needed to compute the property? Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation. We establish several general results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.
C1 [Frongillo, Rafael] Univ Colorado, Boulder, CO 80309 USA.
   [Kash, Ian A.] Microsoft Res, New York, NY USA.
RP Frongillo, R (reprint author), Univ Colorado, Boulder, CO 80309 USA.
EM raf@colorado.edu; iankash@microsoft.com
CR Abernethy J., 2012, P 25 COLT, P1
   Agarwal A., 2015, COLT
   Banerjee A, 2005, IEEE T INFORM THEORY, V51, P2664, DOI 10.1109/TIT.2005.850145
   Bellini F., QUANTITATIV IN PRESS, DOI [10.1080/14697688.2014.946955, DOI 10.1080/14697688.2014.946955]
   Ben-Tal A, 2007, MATH FINANC, V17, P449, DOI 10.1111/j.1467-9965.2007.00311.x
   Emmer Susanne, 2013, ARXIV13121645QFIN
   Fissler Tobias, 2015, ARXIV150308123MATHQF
   Fissler Tobias, 2015, ARXIV150700244QFIN
   Fllmer Hans, 2015, ANN REV FINANCIAL EC, V7
   Frongillo R., 2015, J MACH LEARN RES WOR, P1
   Frongillo R, 2014, LECT NOTES COMPUT SC, V8877, P354, DOI 10.1007/978-3-319-13129-0_29
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Gneiting T, 2011, J AM STAT ASSOC, V106, P746, DOI 10.1198/jasa.2011.r10138
   Heinrich C., 2013, BIOMETRIKA
   Lambert N. S., 2011, PREPRINT
   Lambert N, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P109
   Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129
   NEWEY WK, 1987, ECONOMETRICA, V55, P819, DOI 10.2307/1911031
   Osband K. H., 1985, PROVIDING INCENTIVES
   Rockafellar RT, 2014, EUR J OPER RES, V234, P140, DOI 10.1016/j.ejor.2013.10.046
   Rockafellar R.T., 2013, SURV OPER RES MANAGE, V18, P33, DOI DOI 10.1016/J.S0RMS.2013.03.001)
   SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229
   Steinwart I., 2014, J MACHINE LEARNING R, P482
   Wang RD, 2015, STAT PROBABIL LETT, V100, P172, DOI 10.1016/j.spl.2015.02.004
   Ziegel Johanna F., 2014, MATH FINANCE
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101091
DA 2019-06-15
ER

PT S
AU Frongillo, R
   Reid, MD
AF Frongillo, Rafael
   Reid, Mark D.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convergence Analysis of Prediction Markets via Randomized Subspace
   Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID OPTIMIZATION; OLD
AB Prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders. The pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market. However, little is known about rates and guarantees for the convergence of these sequential mechanisms, and two recent papers cite this as an important open question.
   In this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent (RSD). We establish convergence rates for RSD and leverage them to prove rates for the two prediction market models above, answering the open questions. Our results extend beyond standard centralized markets to arbitrary trade networks.
C1 [Frongillo, Rafael] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
   [Reid, Mark D.] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.
   [Reid, Mark D.] NICTA, Sydney, NSW, Australia.
RP Frongillo, R (reprint author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
EM raf@colorado.edu; mark.reid@anu.edu.au
FU ARC Discovery Early Career Research Award [DE130101605]
FX We would like to thank Matus Telgarsky for his generous help, as well as
   the lively discussions with, and helpful comments of, Sebastien Lahaie,
   Miro Dudik, Jenn Wortman Vaughan, Yiling Chen, David Parkes, and Nageeb
   Ali. MDR is supported by an ARC Discovery Early Career Research Award
   (DE130101605). Part of this work was developed while he was visiting
   Microsoft Research.
CR Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12, DOI DOI 10.1145/2465769.2465777
   Abernethy J. D., 2011, ADV NEURAL INFORM PR, V24, P2600
   Abernethy Jacob, 2014, P 15 ACM C EC COMP, P395
   Ben-Tal A, 2007, MATH FINANC, V17, P449, DOI 10.1111/j.1467-9965.2007.00311.x
   Boyd S., 2004, CONVEX OPTIMIZATION
   Burgert C, 2006, STATIST RISK MODEL, V24, P153, DOI 10.1524/stnd.2006.24.1.153
   Chen Y, 2010, P 11 ACM C EL COMM, P189
   de Abreu NMM, 2007, LINEAR ALGEBRA APPL, V423, P53, DOI 10.1016/j.laa.2006.08.017
   Follmer Hans, 2004, GRUYTER STUDIES MATH, V27
   Frongillo Rafael M, 2012, P NEUR INF PROC SYST
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   Hiriart-Urruty JB, 1993, CONVEX ANAL MINIMIZA, VII, P306
   Hu Jinli, 2014, P 31 INT C MACH LEAR
   Millin Jono, 2012, P 29 INT C MACH LEAR, P1815
   Mohar Bojan, 1991, GRAPH THEORY COMBINA
   Necoara I, 2014, TECHNICAL REPORT
   Necoara I, 2013, IEEE T AUTOMAT CONTR, V58, P2001, DOI 10.1109/TAC.2013.2250071
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Premachandra Mindika, 2013, P 5 AS C MACH LEARN, P373
   Reddi S., 2014, ARXIV14092617
   Reid Mark D, 2015, P C LEARN THEOR COLT
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Rockafellar RT, 1997, CONVEX ANAL
   Storkey Amos J., 2011, P 14 INT C ART INT S, P716
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100095
DA 2019-06-15
ER

PT S
AU Gan, Z
   Li, CY
   Henao, R
   Carlson, D
   Carin, L
AF Gan, Zhe
   Li, Chunyuan
   Henao, Ricardo
   Carlson, David
   Carin, Lawrence
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Temporal Sigmoid Belief Networks for Sequence Modeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.
C1 [Gan, Zhe; Li, Chunyuan; Henao, Ricardo; Carlson, David; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
RP Gan, Z (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM zhe.gan@duke.edu; chunyuan.li@duke.edu; r.henao@duke.edu;
   david.carlson@duke.edu; lcarin@duke.edu
FU ARO; DARPA; ONR; NGA; DOE
FX This research was supported in part by ARO, DARPA, DOE, NGA and ONR.
CR Acharya  A., 2015, AISTATS
   Bayer Justin, 2014, ARXIV14117610
   Boulanger-Lewandowski Nicolas, 2012, ICML
   Chung J, 2015, NIPS
   Fabius Otto, 2014, ARXIV14126581
   Fan K., 2015, NIPS
   Gan Z., 2015, AISTATS
   Gan Zhe, 2015, ICML
   Graves A, 2013, ARXIV13080850
   Han S., 2014, NIPS
   Henderson J., 2010, JMLR
   Hermans Michiel, 2013, NIPS
   Hinton G., 1995, P ICANN
   Hinton G., 1995, SCIENCE
   Hinton G., 2009, NIPS
   Hinton G. E., 2002, NEURAL COMPUTATION
   Hinton G. E., 2006, NEURAL COMPUTATION
   Kalman R., 1963, J SOC IND APPL MAT A
   Kingma Diederik P, 2014, ICLR
   Martens J., 2011, ICML
   Mittelman R., 2014, ICML
   Mnih A., 2014, ICML
   Neal Radford M., 1992, ARTIFICIAL INTELLIGE
   Pascanu Razvan, 2013, ICML
   Rabiner Lawrence, 1986, ASSP MAGAZINE
   Rezende D. J., 2014, ICML
   Sutskever I., 2009, NIPS
   Sutskever  I., 2007, AISTATS
   Taylor G., 2009, ICML
   Taylor G. W., 2006, NIPS
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Werbos Paul J., 1990, P IEEE
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100023
DA 2019-06-15
ER

PT S
AU Ganti, R
   Balzano, L
   Willett, R
AF Ganti, Ravi
   Balzano, Laura
   Willett, Rebecca
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Matrix Completion Under Monotonic Single Index Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces. In real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation. This paper addresses the challenge of matrix completion in the face of such nonlinearities. Given a few observations of a matrix that are obtained by applying a Lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries. We propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements. Mean squared error bounds provide insight into how well the matrix can be estimated based on the size, rank of the matrix and properties of the nonlinear transformation. Empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.
C1 [Ganti, Ravi] UW Madison, Wisconsin Inst Discovery, Madison, WI 53706 USA.
   [Balzano, Laura] Univ Michigan, Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
   [Willett, Rebecca] UW Madison, Dept Elect & Comp Engn, Madison, WI 53706 USA.
RP Ganti, R (reprint author), UW Madison, Wisconsin Inst Discovery, Madison, WI 53706 USA.
EM gantimahapat@wisc.edu; girasole@umich.edu; rmwillett@wisc.edu
CR Agarwal A., 2014, INT C MACH LEARN ICM, V32, P541
   Becker S., 2011, LOW RANK MATR OPT S
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Cucuringu Mihai, 2012, THESIS
   Dattorro J., 2010, CONVEX OPTIMIZATION
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Elhamifar Ehsan, 2013, TPAMI
   Eriksson Brian, 2012, AISTATS
   Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999
   Horowitz JL, 1996, J AM STAT ASSOC, V91, P1632, DOI 10.2307/2291590
   ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K
   Kakade S. M., 2011, NIPS
   Kalai Adam Tauman, 2009, COLT
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Koyejo O., 2013, P 7 ACM C REC SYST, P49
   Melville Prem, 2010, ENCY MACHINE LEARNIN
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Singh A. A., 2012, SEX ROLES, P1, DOI DOI 10.1109/SCES.2012.6199064
   Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034
   Tan M., 2014, P 31 INT C MACH LEAR, V32, P1539
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Vershynin  Roman, 2010, ARXIV10113027
   Wang Z., 2014, P 31 INT C MACH LEAR, P91
   Wen  Z., 2012, MATH PROGRAMMING COM
   Yang Congyuan, 2015, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102065
DA 2019-06-15
ER

PT S
AU Gao, T
   Ji, Q
AF Gao, Tian
   Ji, Qiang
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Local Causal Discovery of Direct Causes and Effects
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs (CPDAG) in order to identify direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.
C1 [Gao, Tian; Ji, Qiang] Rensselaer Polytech Inst, Dept ECSE, Troy, NY 12180 USA.
RP Gao, T (reprint author), Rensselaer Polytech Inst, Dept ECSE, Troy, NY 12180 USA.
EM gaot@rpi.edu; jiq@rpi.edu
CR Aliferis Constantin, 2003, THESIS
   Chickering David Maxwell, 2002, J MACHINE LEARNING R
   Cooper GF, 1997, DATA MIN KNOWL DISC, V1, P203, DOI 10.1023/A:1009787925236
   Guyon Isabelle, 2007, CAUSAL FEATURE SELEC
   Koller D., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P284
   Mani S, 2004, ST HEAL T, V107, P731
   Mani S, 1999, J AM MED INFORM ASSN, P315
   Mani S., 2010, NIPS CAUSALITY OBJEC, P121
   MARGARITIS D., 1999, P NEUR INF PROC SYST, P505
   Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403
   Niinimaki  T., 2012, P 28 ANN C UNC ART I, P634
   Pearl J, 1988, PROBABILISTIC REASON
   Pearl J., 2000, CAUSALITY MODELS REA, V29
   Pellet Jean-Philippe, 2008, J MACHINE LEARNING
   Pellet JP, 2009, P NIPS, P1249
   Pena JM, 2007, INT J APPROX REASON, V45, P211, DOI 10.1016/j.ijar.2006.06.008
   Silverstein C, 2000, DATA MIN KNOWL DISC, V4, P163, DOI 10.1023/A:1009891813863
   Spirtes P., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P499
   Spirtes P., 2000, CAUSATION PREDICTION
   Spirtes Peter, 2000, CONSTRUCTING BAYESIA
   Statnikov Alexander, 2008, CAUSATION PREDICTION
   Tsamardinos I., 2003, P 9 ACM SIGKDD INT C, P673, DOI DOI 10.1145/956750.956838
   Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7
   Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103012
DA 2019-06-15
ER

PT S
AU Gao, YJ
   Buesing, L
   Shenoy, KV
   Cunningham, JP
AF Gao, Yuanjun
   Buesing, Lars
   Shenoy, Krishna V.
   Cunningham, John P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI High-dimensional neural spike train analysis with generalized count
   linear dynamical systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODELS; REGRESSION
AB Latent factor models have been widely used to analyze simultaneous recordings of spike trains from large, heterogeneous neural populations. These models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time, which is observed in high dimension via noisy point-process observations. These techniques have been well used to capture neural correlations across a population and to provide a smooth, denoised, and concise representation of high-dimensional spiking data. One limitation of many current models is that the observation model is assumed to be Poisson, which lacks the flexibility to capture under-and over-dispersion that is common in recorded neural data, thereby introducing bias into estimates of covariance. Here we develop the generalized count linear dynamical system, which relaxes the Poisson assumption by using a more general exponential family for count data. In addition to containing Poisson, Bernoulli, negative binomial, and other common count distributions as special cases, we show that this model can be tractably learned by extending recent advances in variational inference techniques. We apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods, both in capturing the variance structure of the data and in held-out prediction.
C1 [Gao, Yuanjun; Buesing, Lars; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Shenoy, Krishna V.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Gao, YJ (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.
EM yg2312@columbia.edu; lars@stat.columbia.edu; shenoy@stanford.edu;
   jpc2181@columbia.edu
FU Sloan Research Fellowship; Simons Foundation [325171, 325233]; Grossman
   Center at Columbia University; Gatsby Charitable Trust
FX JPC received funding from a Sloan Research Fellowship, the Simons
   Foundation (SCGB#325171 and SCGB#325233), the Grossman Center at
   Columbia University, and the Gatsby Charitable Trust. Thanks to Byron
   Yu, Gopal Santhanam and Stephen Ryu for providing the cortical data.
CR Adams R, 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376
   Ananth CV, 1997, INT J EPIDEMIOL, V26, P1323, DOI 10.1093/ije/26.6.1323
   Boyd S., 2009, CONVEX OPTIMIZATION
   Buesing L., 2014, ADV NEURAL INFORM PR, V27, P3500
   Buesing L., 2015, ADV STATE SPACE METH
   Buesing L, 2012, NETWORK-COMP NEURAL, V23, P24, DOI 10.3109/0954898X.2012.677095
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Cunningham John P, 2007, NIPS, P329
   del Castillo J, 2005, J STAT PLAN INFER, V134, P486, DOI 10.1016/j.jspi.2004.04.019
   Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711
   Khan M. E., 2013, INT C MACH LEARN, P951
   Koyama S., 2015, NEURAL COMPUTATION
   Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173
   LAMBERT D, 1992, TECHNOMETRICS, V34, P1, DOI 10.2307/1269547
   Lawhern V, 2010, J NEUROSCI METH, V189, P267, DOI 10.1016/j.jneumeth.2010.03.024
   Linderman S. W., 2015, COSYNE
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0
   Petreska B., 2011, ADV NEURAL INF PROCE, V24, P756
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Rao C. R., 1965, SANKHYA A, V27, P311
   Scott J., 2012, ADV NEURAL INFORM PR, P1898
   Sellers KF, 2010, ANN APPL STAT, V4, P943, DOI 10.1214/09-AOAS306
   SINGH J, 1978, SIAM J APPL MATH, V34, P545, DOI 10.1137/0134043
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Vidne M, 2012, J COMPUT NEUROSCI, V33, P97, DOI 10.1007/s10827-011-0376-2
   Yu B. M., 2009, ADV NEURAL INFORM PR, P1881, DOI [10.1152/jn.90941.2008, DOI 10.1152/JN.90941]
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101027
DA 2019-06-15
ER

PT S
AU Gardner, JR
   Malkomes, G
   Garnett, R
   Weinberger, KQ
   Barbour, D
   Cunningham, JP
AF Gardner, Jacob R.
   Malkomes, Gustavo
   Garnett, Roman
   Weinberger, Kilian Q.
   Barbour, Dennis
   Cunningham, John P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bayesian Active Model Selection with an Application to Automated
   Audiometry
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.
C1 [Gardner, Jacob R.; Weinberger, Kilian Q.] Cornell Univ, CS, Ithaca, NY 14850 USA.
   [Malkomes, Gustavo; Garnett, Roman] WUSTL, CSE, St Louis, MO 63130 USA.
   [Barbour, Dennis] WUSTL, BME, St Louis, MO 63130 USA.
   [Cunningham, John P.] Columbia Univ, Stat, New York, NY 10027 USA.
RP Gardner, JR (reprint author), Cornell Univ, CS, Ithaca, NY 14850 USA.
EM jrg365@cornell.edu; luizgustavo@wustl.edu; garnett@wustl.edu;
   kqw4@cornell.edu; dbarbour@wustl.edu; jpc2181@columbia.edu
FU National Science Foundation (NSF) [IIA-1355406]; NSF [IIS-1525919,
   IIS-1550179, EFMA-1137211]; CAPES/BR; NIH [R01-DC009215]; CIMIT
FX This material is based upon work supported by the National Science
   Foundation (NSF) under award number IIA-1355406. Additionally, JRG and
   KQW are supported by NSF grants IIS-1525919, IIS-1550179, and
   EFMA-1137211; GM is supported by CAPES/BR; DB acknowledges NIH grant
   R01-DC009215 as well as the CIMIT; JPC acknowledges the Sloan
   Foundation.
CR Ali A., 2014, AAAI
   Bailey TC, 2013, J HOSP MED, V8, P236, DOI 10.1002/jhm.2009
   CARHART R, 1959, J SPEECH HEAR DISORD, V24, P330, DOI 10.1044/jshd.2404.330
   DON M, 1979, ANN OTO RHINOL LARYN, V88, P1
   Duvenaud D, 2014, THESIS
   Duvenaud DK, 2013, P INT C MACH LEARN I, V3, P1166
   Gardner J. R., 2015, UAI
   Garnett R., 2014, P 30 C UNC ART INT, P230
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Houlsby N, 2014, P 31 INT C MACH LEAR, P766
   Houlsby N., 2012, ADV NEURAL INFORM PR, P2096
   Hughson W, 1944, T AM ACADEMY OPHTH S, V48, P1
   Kononenko I, 2001, ARTIF INTELL MED, V23, P89, DOI 10.1016/S0933-3657(01)00077-X
   Kuha J, 2004, SOCIOL METHOD RES, V33, P188, DOI 10.1177/0049124103262065
   Kulick J., 2014, ABS14097552 CORR
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   MacKay D. J, 2003, INFORM THEORY INFERE
   McBride DI, 2001, OCCUP ENVIRON MED, V58, P46, DOI 10.1136/oem.58.1.46
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Murphy KP, 2012, MACHINE LEARNING PRO
   Nelson DI, 2005, AM J IND MED, V48, P446, DOI 10.1002/aijm.20223
   Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Saria S, 2010, SCI TRANSL MED, V2, DOI 10.1126/scitranslmed.3001304
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Shargorodsky J, 2010, JAMA-J AM MED ASSOC, V304, P772, DOI 10.1001/jama.2010.1124
   Williams C., 1996, NIPS
   Wilson A. G., 2014, ADV NEURAL INF PROCE, P3626
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102020
DA 2019-06-15
ER

PT S
AU Gillenwater, J
   Iyer, R
   Lusch, B
   Kidambi, R
   Bilmes, J
AF Gillenwater, Jennifer
   Iyer, Rishabh
   Lusch, Bethany
   Kidambi, Rahul
   Bilmes, Jeff
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Submodular Hamming Metrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).
C1 [Gillenwater, Jennifer; Iyer, Rishabh; Kidambi, Rahul; Bilmes, Jeff] Univ Washington, Dept EE, Seattle, WA 98195 USA.
   [Lusch, Bethany] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.
RP Gillenwater, J (reprint author), Univ Washington, Dept EE, Seattle, WA 98195 USA.
EM jengi@uw.edu; rkiyer@uw.edu; herwaldt@uw.edu; rkidambi@uw.edu;
   bilmes@uw.edu
CR Arthur  David, 2007, SODA
   Bateni M., 2010, TECHNICAL REPORT
   Batra D., 2012, ECCV
   Buchbinder N., 2012, FOCS
   Buchbinder N., 2014, SODA
   CUNNINGHAM WH, 1985, COMBINATORICA, V5, P185, DOI 10.1007/BF02579361
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Goel G., 2009, FOCS
   Gusfield D., 1997, ALGORITHMS STRINGS T
   Halmos P, 1974, MEASURE THEORY
   Hazan T., 2013, NIPS
   Iyer R., 2013, ICML
   Iyer R., 2013, NIPS
   Iyer R., 2012, NIPS
   Jegelka S., 2011, ICML
   Jegelka S., 2011, CVPR
   Lin H., ACL
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mikolov T., 2013, NIPS
   Nemhauser G., 1978, ANAL APPROXIMATIONS, V14
   Osokin A., 2014, ECCV
   Svitkina Z., 2008, FOCS
   Szummer M., 2008, ECCV
   Tschiatschek S., 2014, NIPS
   Vondrak J., 2010, RIMS KOKYUROKU BESSA, V23
   Yu J., 2015, ICML
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101001
DA 2019-06-15
ER

PT S
AU Giordano, R
   Broderick, T
   Jordan, M
AF Giordano, Ryan
   Broderick, Tamara
   Jordan, Michael
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Linear Response Methods for Accurate Covariance Estimates from Mean
   Field Variational Bayes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFERENCE; MODELS
AB Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, a well known major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables-both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.
C1 [Giordano, Ryan; Jordan, Michael] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Broderick, Tamara] MIT, Cambridge, MA 02139 USA.
RP Giordano, R (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM rgiordano@berkeley.edu; tbroderick@csail.mit.edu; jordan@cs.berkeley.edu
FU Berkeley Fellowships
FX The authors thank Alex Blocker for helpful comments. R. Giordano and T.
   Broderick were funded by Berkeley Fellowships.
CR Bates D, 2013, J STAT SOFTW, V52, P1
   Bishop C. M., 2006, PATTERN RECOGN
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Hadfield JD, 2010, J STAT SOFTW, V33, P1
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hojen-Sorensen PADFR, 2002, NEURAL COMPUT, V14, P889, DOI 10.1162/089976602317319009
   Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lubin M, 2015, INFORMS J COMPUT, V27, P238, DOI 10.1287/ijoc.2014.0623
   MacKay D. J, 2003, INFORM THEORY INFERE
   MENG XL, 1991, J AM STAT ASSOC, V86, P899, DOI 10.2307/2290503
   Opper M., 2003, ADV NEURAL INFORM PR
   Opper  M., 2001, ADV MEAN FIELD METHO
   Parisi G., 1988, STAT FIELD THEORY, V4
   Plummer M., 2006, R NEWS, V6, P7, DOI DOI 10.1159/000323281
   Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x
   Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302
   Tanaka T, 2000, NEURAL COMPUT, V12, P1951, DOI 10.1162/089976600300015213
   Turner R., 2011, BAYESIAN TIME SERIES
   Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang B., 2004, WORKSH ART INT STAT, P373
   Welling M, 2004, NEURAL COMPUT, V16, P197, DOI 10.1162/08997660460734056
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101015
DA 2019-06-15
ER

PT S
AU Goldstein, T
   Li, M
   Yuan, XM
AF Goldstein, Thomas
   Li, Min
   Yuan, Xiaoming
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Adaptive Primal-Dual Splitting Methods for Statistical Learning and
   Image Processing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS; CONVERGENCE; PENALTY
AB The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler sub-steps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.
C1 [Goldstein, Thomas] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Li, Min] Southeast Univ, Sch Econ & Management, Nanjing, Jiangsu, Peoples R China.
   [Yuan, Xiaoming] Hong Kong Baptist Univ, Dept Math, Kowloon Tong, Hong Kong, Peoples R China.
RP Goldstein, T (reprint author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM tomg@cs.umd.edu; limin@seu.edu.cn; xmyuan@hkbu.edu.hk
FU National Science Foundation [1535902]; Office of Naval Research
   [N00014-15-1-2676]; Hong Kong Research Grants Council's General Research
   Fund [HKBU 12300515]; Program for New Century Excellent University
   Talents [NCET-12-0111]; Qing Lan Project
FX This work was supported by the National Science Foundation (#1535902),
   the Office of Naval Research (#N00014-15-1-2676), and the Hong Kong
   Research Grants Council's General Research Fund (HKBU 12300515). The
   second author was supported in part by the Program for New Century
   Excellent University Talents under Grant No. NCET-12-0111, and the Qing
   Lan Project.
CR Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043
   Bonettini S, 2012, J MATH IMAGING VIS, V44, P236, DOI 10.1007/s10851-011-0324-9
   Boyd S., 2010, FDN TRENDS MACHINE L
   Chambolle A., 2010, J MATH IMAGING VIS, V40, P1
   Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9
   Esser E, 2010, SIAM J IMAGING SCI, V3, P1015, DOI 10.1137/09076934X
   GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41
   Glowinski R, 1989, AUGMENTED LAGRANGIAN
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Goldstein Tom, 2013, PREPRINT
   He BS, 2012, SIAM J IMAGING SCI, V5, P119, DOI 10.1137/100814494
   He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Ouyang Yuyuan, 2014, ARXIV14016607
   Pock T, 2009, IEEE I CONF COMP VIS, P1133, DOI 10.1109/ICCV.2009.5459348
   POPOV LD, 1980, MATH NOTES+, V28, P845, DOI 10.1007/BF01141092
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sun TN, 2012, BIOMETRIKA, V99, P879, DOI 10.1093/biomet/ass043
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Zhang XQ, 2005, IEEE NUCL SCI CONF R, P2332
   Zhu M, 2008, 0834 UCLA CAM
NR 21
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100091
DA 2019-06-15
ER

PT S
AU Gong, PH
   Ye, JP
AF Gong, Pinghua
   Ye, Jieping
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI HONOR: Hybrid Optimization for NOn-convex Regularized problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; SPARSE; ALGORITHM
AB Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.
C1 [Gong, Pinghua; Ye, Jieping] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Gong, PH (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM gongp@umich.edu; jpye@umich.edu
FU NIH [R01 LM010730, U54 EB020403]; NSF [IIS-0953662, III-1539991,
   III-1539722]
FX This work is supported in part by research grants from NIH (R01
   LM010730, U54 EB020403) and NSF (IIS-0953662, III-1539991, III-1539722).
CR An LTH, 2005, ANN OPER RES, V133, P23, DOI 10.1007/s10479-004-5022-1
   Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI DOI 10.1145/1273496.1273501
   Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319
   Byrd R. H., 2012, TECHNICAL REPORT
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Byrd RH, 2012, MATH PROGRAM, V134, P127, DOI 10.1007/s10107-012-0572-5
   Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Clarke F. H., 1983, OPTIMIZATION NONSMOO
   Dutta J., 2005, TOP, V13, P185
   Fan JQ, 2014, ANN STAT, V42, P819, DOI 10.1214/13-AOS1198
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Gasso G, 2009, IEEE T SIGNAL PROCES, V57, P4686, DOI 10.1109/TSP.2009.2026004
   Gong P., 2015, ICML
   Gong Pinghua, 2013, Proc Int Conf Mach Learn, V28, P37
   Jorge N., 1999, NUMERICAL OPTIMIZATI
   Li G.-C., 2011, CIDU, P159
   Mazumder R., 2011, J AM STAT ASS, V106
   Olsen Peder A., 2012, ADV NEURAL INFORM PR, P764
   Rakotomamonjy A., 2014, DC PROXIMAL NEWTON N
   Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308
   Tan X, 2011, IEEE T SIGNAL PROCES, V59, P1088, DOI 10.1109/TSP.2010.2096218
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   Zhang T., 2012, MULTISTAGE CONVEX RE
   Zhang T, 2010, J MACH LEARN RES, V11, P1081
   Zou H, 2008, ANN STAT, V36, P1509, DOI 10.1214/009053607000000802
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101088
DA 2019-06-15
ER

PT S
AU Gorham, J
   Mackey, L
AF Gorham, Jackson
   Mackey, Lester
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Measuring Sample Quality with Stein's Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MULTIVARIATE NORMAL APPROXIMATION; EXCHANGEABLE PAIRS; METRICS
AB To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that bounds the discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyper-parameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.
C1 [Gorham, Jackson; Mackey, Lester] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Gorham, J (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
CR Ahn  Sungjin, 2012, P 29 INT C MACH LEAR
   Bach F., 2012, P 29 INT C MACH LEAR
   Barbour A. D., 1988, J APPL PROBAB A, V25, P175
   Bouts Q. W., 2014, P 30 ANN S COMP GEOM
   Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Caflisch R. E., 1998, Acta Numerica, V7, P1, DOI 10.1017/S0962492900002804
   Chatterjee S, 2011, ANN APPL PROBAB, V21, P464, DOI 10.1214/10-AAP712
   Chatterjee S, 2008, ALEA-LAT AM J PROBAB, V4, P257
   Chen LHY, 2011, PROBAB APPL SER, P1, DOI 10.1007/978-3-642-15007-4
   Chen Y., 2010, P 26 UNC ART INT UAI
   Del Barrio E, 1999, ANN PROBAB, V27, P1009, DOI 10.1214/aop/1022677394
   Dobler C., 2014, ARXIV14114477
   Fan Y., 2006, J COMPUTATIONAL GRAP, V15
   Geyer C. J., 1991, COMP SCI STAT, P156, DOI DOI 10.1080/01621459.1995.10476590
   Glaeser G., 1958, J ANAL MATH, V6, P1
   Gretton A., 2006, ADV NEURAL INFORM PR, P513
   Gurobi Optimization, 2015, GUROBI OPTIMIZER REF
   Har-Peled S, 2006, SIAM J COMPUT, V35, P1148, DOI 10.1137/S0097539704446281
   Korattikara A, 2014, P 31 INT C MACH LEAR
   Lubin M, 2015, INFORMS J COMPUT, V27, P238, DOI 10.1287/ijoc.2014.0623
   Meckes E., 2009, HIGH DIMENSIONAL PRO, P153
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Paul Chew L., 1986, P 2 ANN S COMP GEOM, P169, DOI DOI 10.1145/10515.10534
   PELEG D, 1989, J GRAPH THEOR, V13, P99, DOI 10.1002/jgt.3190130114
   Reinert G, 2009, ANN PROBAB, V37, P2150, DOI 10.1214/09-AOP467
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Shvartsman P, 2008, T AM MATH SOC, V360, P5529, DOI 10.1090/S0002-9947-08-04469-3
   Stein C., 1972, P 6 BERK S MATH STAT, V2, P583
   Vallander S. S., 1973, THEOR PROBAB APPL, V18, P784
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   ZELLNER A, 1995, J AM STAT ASSOC, V90, P921, DOI 10.2307/2291326
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101028
DA 2019-06-15
ER

PT S
AU Goroshin, R
   Mathieu, M
   LeCun, Y
AF Goroshin, Ross
   Mathieu, Michael
   LeCun, Yann
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning to Linearize Under Uncertainty
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Training deep feature hierarchies to solve supervised learning tasks has achieved state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabeled natural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing latent variables that are non-deterministic functions of the input into the network architecture.
C1 [Goroshin, Ross; Mathieu, Michael; LeCun, Yann] Courant Inst Math Sci, Dept Comp Sci, New York, NY 10012 USA.
   [LeCun, Yann] Facebook AI Res, New York, NY USA.
RP Goroshin, R (reprint author), Courant Inst Math Sci, Dept Comp Sci, New York, NY 10012 USA.
EM goroshin@cs.nyu.edu; mathieu@cs.nyu.edu; yann@cs.nyu.edu
CR Bengio Y, 2012, TECHNICAL REPORT
   Cadieu Charles F., 2012, NEURAL COMPUTATION
   Cohen T. S., 2014, ARXIV14127659
   Goroshin Ross, 2014, ARXIV14126056
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Kayser Christoph, 2001, ICANN 2001
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Mobahi H., 2009, ICML
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Ranzato M., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Ranzato M, 2014, ARXIV14126604
   Vondrick C., 2015, ARXIV150408023
   Wang X., 2015, ARXIV150500687
   Wiskott L., 2002, NEURAL COMPUTATION
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102099
DA 2019-06-15
ER

PT S
AU Gotovos, A
   Hassani, SH
   Krause, A
AF Gotovos, Alkis
   Hassani, S. Hamed
   Krause, Andreas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sampling from Probabilistic Submodular Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Submodular and supermodular functions have found wide applicability in machine learning, capturing notions such as diversity and regularity, respectively. These notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention. However, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference. Prominent, well-studied special cases include Ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied. In this paper, we investigate the use of Markov chain Monte Carlo sampling to perform approximate inference in general log-submodular and log-supermodular models. In particular, we consider a simple Gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (O(n log n)) mixing. We also evaluate the efficiency of the Gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.
C1 [Gotovos, Alkis; Hassani, S. Hamed; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Gotovos, A (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM alkisg@inf.ethz.ch; hamed@inf.ethz.ch; krausea@ethz.ch
FU ERC Starting Grant [307036]
FX This work was partially supported by ERC Starting Grant 307036.
CR Aldous David, 1983, SEMINAIRE PROBABILIT
   Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905
   Bubley Russ, 1998, S DISCR ALG
   Bubley Russ, 1997, S FDN COMP SCI
   Conforti Michele, 1984, DISC APP MATH
   Diaconis Persi, 1991, ANN APPL PROBABILITY
   Djolonga J., 2014, NEURAL INFORM PROCES
   Dyer Martin, 2009, ANN APPL PROBABILITY
   Feige Uriel, 2007, S FDN COMP SCI
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Golovin Daniel, 2011, J ARTIFICIAL INTELLI
   Greenhill Catherine, 2000, J ALGORITHMS
   Hui Lin, 2011, HUMAN LANGUAGE TECHN
   Iyer Rishabh, 2015, INT C ART INT STAT
   Jerrum M., 1993, SIAM J COMPUTING
   Jerrum Mark, 2004, J ACM
   Jerrum Mark, 1995, RANDOM STRUCTURES AL
   Jerrum Mark, 1989, SIAM J COMPUTING
   Jerrum MR, 2003, COUNTING SAMPLING IN
   Kempe David, 2003, C KNOWL DISC DAT MIN
   Koller D., 2009, PROBABILISTIC GRAPHI
   Krause A., 2008, J WATER RESOURCES PL
   Krause Andreas, 2006, INFORM PROCESSING SE
   Kulesza A., 2012, FDN TRENDS MACHINE L
   Levin D. A., 2008, MARKOV CHAINS MIXING
   Nemhauser G., 1978, MATH PROGRAMMING
   Rebeschini Patrick, 2015, C LEARN THEOR
   Sinclair Alistair, 1992, COMBINATORICS PROBAB
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101004
DA 2019-06-15
ER

PT S
AU Grefenstette, E
   Hermann, KM
   Suleyman, M
   Blunsom, P
AF Grefenstette, Edward
   Hermann, Karl Moritz
   Suleyman, Mustafa
   Blunsom, Phil
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning to Transduce with Unbounded Memory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.
C1 [Grefenstette, Edward; Hermann, Karl Moritz; Suleyman, Mustafa; Blunsom, Phil] Google DeepMind, London, England.
   [Blunsom, Phil] Univ Oxford, Oxford, England.
RP Grefenstette, E (reprint author), Google DeepMind, London, England.
EM etg@google.com; kmh@google.com; mustafasul@google.com;
   pblunsom@google.com
CR Aho Alfred V., 1972, THEORY PARSING TRANS, Vtwo
   Allauzen C, 2007, LECT NOTES COMPUT SC, V4783, P11
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Das S., 1992, P 14 ANN C COGN SCI
   Das Sreerupa, 1993, ADV NEURAL INFORM PR
   Dreyer Markus, 2008, EMNLP, P1080
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Graves A., 2014, ARXIV14105401
   Graves Alex, 2012, REPR LEARN WORKSH IC
   Joulin  Armand, 2015, ARXIV150301007
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pascanu R, 2012, ARXIV E PRINTS, V1211, P5063
   Sukhbaatar Sainbayar, 2015, ABS150308895 CORR
   Sun GZ, 1998, NEURAL NETWORK PUSHD
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Wu D, 1997, COMPUT LINGUIST, V23, P377
   Wu D., 1998, COLING ACL 98 36 ANN, P1408
   Zaremba Wojciech, 2015, ARXIV150500521
   Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100016
DA 2019-06-15
ER

PT S
AU Grill, JB
   Valko, M
   Munos, R
AF Grill, Jean-Bastien
   Valko, Michal
   Munos, Remi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Black-box optimization of noisy functions with unknown smoothness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of black-box optimization of a function f of any dimension, given function evaluations perturbed by noise. The function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown. Our contribution is an adaptive optimization algorithm, POO or parallel optimistic optimization, that is able to deal with this setting. POO performs almost as well as the best known algorithms requiring the knowledge of the smoothness. Furthermore, POO works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense. We provide a finite-time analysis of POO's performance, which shows that its error after n evaluations is at most a factor of root ln n away from the error of the best known optimization algorithms using the knowledge of the smoothness.
C1 [Grill, Jean-Bastien; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France.
   [Munos, Remi] Google DeepMind, London, England.
RP Grill, JB (reprint author), INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France.
EM jean-bastien.grill@inria.fr; michal.valko@inria.fr; munos@google.com
FU French Ministry of Higher Education and Research; Nord-Pas-de-Calais
   Regional Counci; Ecole Normale Superieure in Paris; Carnegie Mellon
   University associated-team project EduBand; French National Research
   Agency project ExTra-Learn [ANR-14-CE24-0010-01]; Inria
FX The research presented in this paper was supported by French Ministry of
   Higher Education and Research, Nord-Pas-de-Calais Regional Council, a
   doctoral grant of Ecole Normale Superieure in Paris, Inria and Carnegie
   Mellon University associated-team project EduBand, and French National
   Research Agency project ExTra-Learn (n. ANR-14-CE24-0010-01).
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Azar Mohammad Gheshlaghi, 2014, INT C MACH LEARN
   Bubeck S., 2011, J MACHINE LEARNING R, V12, P1587
   Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059
   Bubeck Sebastien, 2011, ALGORITHMIC LEARNING
   Bull AD, 2015, BERNOULLI, V21, P2289, DOI 10.3150/14-BEJ644
   Combes Richard, 2015, ARXIV E PRINTS
   Coquelin Pierre- Arnaud, 2007, UNCERTAINTY ARTIFICI
   Kleinberg Robert, 2008, S THEOR COMP
   Kocsis Levente, 2006, EUR C MACH LEARN
   Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038
   Munos Remi, 2011, NEURAL INFORM PROCES
   Preux Philippe, 2014, C EV COMP
   Slivkins Aleksandrs, 2011, NEURAL INFORM PROCES
   Valko Michal, 2013, INT C MACH LEARN
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100089
DA 2019-06-15
ER

PT S
AU Gu, SX
   Ghahramani, Z
   Turner, RE
AF Gu, Shixiang
   Ghahramani, Zoubin
   Turner, Richard E.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Neural Adaptive Sequential Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.
C1 [Gu, Shixiang; Ghahramani, Zoubin; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England.
   [Gu, Shixiang] MPI Intelligent Syst, Tubingen, Germany.
RP Gu, SX (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM sg717@cam.ac.uk; zoubin@eng.cam.ac.uk; ret26@cam.ac.uk
FU Cambridge-Tubingen Fellowship; ALTA Institute; Jesus College, Cambridge;
   EPSRC [EP/G050821/1, EP/L000776/1]
FX SG is generously supported by Cambridge-Tubingen Fellowship, the ALTA
   Institute, and Jesus College, Cambridge. RET thanks the EPSRC (grants
   EP/G050821/1 and EP/L000776/1). We thank Theano developers for their
   toolkit, the authors of [5] for releasing the source code, and Roger
   Frigola, Sumeet Singh, Fredrik Lindsten, and Thomas Schon for helpful
   suggestions on experiments.
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Bayer Justin, 2014, ARXIV14117610
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Bishop C. M., 1994, MIXTURE DENSITY NETW
   Bornschein J., 2015, INT C LEARN REPR ICL
   Boulanger-Lewandowski N., 2012, INT C MACH LEARN ICM
   Cornebise J., 2009, THESIS, P6
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Ghahramani Z, 2014, ADV NEURAL INFORM PR, P3680
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Graves A., 2013, CORR
   Gregor Karol, 2015, P 32 INT C MACH LEAR, V37, P1462
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   MacKay David JC, 2003, INFORM THEORY INFERE, V7
   McHutchon A., 2014, THESIS
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Mnih A., 2014, INT C MACH LEARN ICM
   Poyiadjis G, 2011, BIOMETRIKA, V98, P65, DOI 10.1093/biomet/asq062
   Rezende Danilo Jimenez, 2014, INT C MACH LEARN ICM
   Sutskever I., 2014, P 27 INT C NEUR INF, V3104, P3112, DOI DOI 10.1021/acs.analchem.7b05329
   Turner Richard E., 2011, BAYESIAN TIME SERIES, P109
   Van Der Merwe Rudolph, 2000, NIPS, P584
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102109
DA 2019-06-15
ER

PT S
AU Gunasekar, S
   Banerjee, A
   Ghosh, J
AF Gunasekar, Suriya
   Banerjee, Arindam
   Ghosh, Joydeep
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Unified View of Matrix Completion under General Structural Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral k-support norm.
C1 [Gunasekar, Suriya; Ghosh, Joydeep] UT Austin, Austin, TX 78712 USA.
   [Banerjee, Arindam] UMN Twin Cities, Minneapolis, MN USA.
RP Gunasekar, S (reprint author), UT Austin, Austin, TX 78712 USA.
EM suriya@utexas.edu; banerjee@cs.umn.edu; ghosh@ece.utexas.edu
FU NSF [IIS-1421729, IIS-1417697, IIS1116656, IIS-1447566, IIS-1422557,
   CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX We thank the anonymous reviewers for helpful comments and suggestions.
   S. Gunasekar and J. Ghosh acknowledge funding funding from NSF grants
   IIS-1421729, IIS-1417697, and IIS1116656. A. Banerjee acknowledges NSF
   grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711, and NASA grant NNX12AQ39A.
CR Amelunxen D., 2014, INFORM INFERENCE
   Argyriou A., 2012, NIPS
   Banerjee A., 2014, NIPS
   Banerjee A., 2005, JMLR
   Cai T., 2014, GEOMETRIZING LOCAL R
   Candes E. J., 2009, FOCM
   Candes E. J., 2011, ACM
   Candes E. J., 2010, P IEEE
   Candes Emmanuel J, 2005, INFORM THEORY IEEE T
   Chandrasekaran V., 2012, FDN COMPUTATIONAL MA
   Davenport M. A., 2014, INFORM INFERENCE
   Dudley R. M., 1967, J FUNCTIONAL ANAL
   Edelman A., 1988, J MATRIX ANAL APPL
   Fazel M., 2001, AM CONTR C
   Forster J., 2002, J COMPUTER SYSTEM SC
   Gunasekar S., 2014, ICML
   Jacob L., 2009, NIPS
   Keshavan R., 2010, JMLR
   Keshavan R. H., 2010, IEEE T IT
   Klopp O., 2014, BERNOULLI
   Klopp O., 2015, MATRIX COMPLETION SI
   Koltchinskii Vladimir, 2011, ANN STAT
   Ledoux M., 1991, PROBABILITY BANACH S
   Litvak A. E., 2005, ADV MATH
   McDonald A. M., 2014, NEW PERSPECTIVES K S
   Negahban S., 2012, JMLR
   Negahban S., 2009, NIPS
   Recht B., 2011, JMLR
   Richard E., 2014, ARXIV E PRINTS
   Srebro N., 2005, LEARNING THEORY
   Talagrand  M., 2014, UPPER LOWER BOUNDS S
   Talagrand M., 1996, ANN PROBABILITY
   Talagrand M., 2001, ANN PROBABILITY
   Tropp J. A., 2014, CONVEX RECOVERY STRU
   Tropp J. A., 2012, FDN COMPUTATIONAL MA
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Vershynin R., 2014, ARXIV E PRINTS
   Watson A. G., 1992, LINEAR ALGEBRA ITS A
   Yang E., 2013, NIPS
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103060
DA 2019-06-15
ER

PT S
AU Harel, Y
   Meir, R
   Opper, M
AF Harel, Yuval
   Meir, Ron
   Opper, Manfred
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Tractable Approximation to Optimal Point Process Filtering:
   Application to Neural Encoding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.
C1 [Harel, Yuval; Meir, Ron] Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.
   [Opper, Manfred] Tech Univ Berlin, Dept Artificial Intelligence, D-10587 Berlin, Germany.
RP Harel, Y (reprint author), Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.
EM yharel@tx.technion.ac.il; rmeir@ee.technion.ac.il;
   opperm@cs.tu-berlin.de
CR Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059
   Anderson B. D., 2005, OCEANS 2005
   Bethge M, 2002, NEURAL COMPUT, V14, P2317, DOI 10.1162/08997660260293247
   Bobrowski O, 2009, NEURAL COMPUT, V21, P1277, DOI 10.1162/neco.2008.01-08-692
   Brand A, 2002, NATURE, V417, P543, DOI 10.1038/417543a
   Bremaud P., 1981, POINT PROCESSES QUEU
   Brigo D, 1998, IEEE T AUTOMAT CONTR, V43, P247, DOI 10.1109/9.661075
   Daum F, 2005, IEEE AERO EL SYS MAG, V20, P57, DOI 10.1109/MAES.2005.1499276
   Dayan P, 2005, THEORETICAL NEUROSCI
   DOUCET A., 2009, HDB NONLINEAR FILTER, V12, P656
   Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638
   Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768
   Julier S, 2000, IEEE T AUTOMAT CONTR, V45, P477, DOI 10.1109/9.847726
   Kalman R. E., 1961, J BASIC ENG, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Maybeck P, 1979, STOCHASTIC MODELS ES
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Opper M., 1998, ON LINE LEARNING NEU, P363
   RHODES IB, 1977, IEEE T AUTOMAT CONTR, V22, P338, DOI 10.1109/TAC.1977.1101534
   SNYDER DL, 1972, IEEE T INFORM THEORY, V18, P91, DOI 10.1109/TIT.1972.1054756
   Snyder DL, 1991, RANDOM POINT PROCESS
   Susemihl A. K., 2014, ADV NEURAL INFORM PR, P1
   Susemihl A. K., 2011, ADV NEURAL INFORM PR, V24, P2303
   Susemihl A, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03009
   Yaeli S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00130
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102012
DA 2019-06-15
ER

PT S
AU Hashimoto, TB
   Sun, Y
   Jaakkola, TS
AF Hashimoto, Tatsunori B.
   Sun, Yi
   Jaakkola, Tommi S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI From random walks to distances on unweighted graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited.
   We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives.
C1 [Hashimoto, Tatsunori B.; Jaakkola, Tommi S.] MIT EECS, Cambridge, MA 02142 USA.
   [Sun, Yi] MIT Math, Cambridge, MA USA.
RP Hashimoto, TB (reprint author), MIT EECS, Cambridge, MA 02142 USA.
EM thashim@mit.edu; yisun@mit.edu; tommi@mit.edu
CR Alamgir M., 2011, ADV NEURAL INFORM PR, P379
   Alamgir M., 2012, P 29 INT C MACH LEAR, P1031
   Chebotarev P, 2011, DISCRETE APPL MATH, V159, P295, DOI 10.1016/j.dam.2010.11.017
   Croydon DA, 2008, POTENTIAL ANAL, V29, P351, DOI 10.1007/s11118-008-9101-9
   Gehrke J., 2003, SIGKDD EXPLORATIONS, V5, P149, DOI DOI 10.1145/980972.980992
   Hashimoto T. B., 2015, P 18 INT C ART INT S, P342
   Kiss G., 1973, COMPUTER LIT STUDIES, P153
   Kivimaki I, 2014, PHYSICA A, V393, P600, DOI 10.1016/j.physa.2013.09.016
   Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   Oksendal B., 2003, STOCHASTIC DIFFERENT
   Sarkar P., 2007, P UAI
   Sarkar P., 2011, IJCAI, P2722
   Shaw B, 2009, P 26 ANN INT C MACH, P937, DOI [10.1145/1553374.1553494, DOI 10.1145/1553374.1553494]
   Smith ST, 2014, IEEE T SIGNAL PROCES, V62, P5324, DOI 10.1109/TSP.2014.2336613
   Stroock D. W., 1979, MULTIDIMENSIONAL DIF, V233
   Tahbaz-Salehi A, 2006, IEEE DECIS CONTR P, P4664, DOI 10.1109/CDC.2006.377308
   Ting D., 2010, P 27 INT C MACH LEAR, P1079
   von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640
   von Luxburg U, 2014, J MACH LEARN RES, V15, P1751
   Yazdani M., 2013, THESIS
   Yen  L., 2008, P 14 SIGKDD INT C KN, P785
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103047
DA 2019-06-15
ER

PT S
AU Hazan, E
   Levy, KY
   Shalev-Shwartz, S
AF Hazan, Elad
   Levy, Kfir Y.
   Shalev-Shwartz, Shai
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Beyond Convexity: Stochastic Quasi-Convex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONVERGENCE
AB Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD).
   The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.
C1 [Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA.
   [Levy, Kfir Y.] Technion, Haifa, Israel.
   [Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Jerusalem, Israel.
RP Hazan, E (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM ehazan@cs.princeton.edu; kfiryl@tx.technion.ac.il; shais@cs.huji.ac.il
FU European Union [336078 - ERC-SUBLRN]; ISF [1673/14]; Intel's ICRI-CI
FX The research leading to these results has received funding from the
   European Union's Seventh Framework Programme (FP7/2007-2013) under grant
   agreement no 336078 - ERC-SUBLRN. Shai S-Shwartz is supported by ISF no
   1673/14 and by Intel's ICRI-CI.
CR Auer P, 1996, ADV NEUR IN, V8, P316
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boyd S., 2004, CONVEX OPTIMIZATION
   Doya K, 1993, IEEE T NEURAL NETWOR, V1, P75
   Goffin JL, 1996, SIAM J OPTIMIZ, V6, P638, DOI 10.1137/S1052623493258635
   Kalai Adam Tauman, 2009, COLT
   Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083
   Khabibullin Rustem F, 1977, ISSLED PRIKL MAT, V4, P15
   Kiwiel KC, 2001, MATH PROGRAM, V90, P1, DOI 10.1007/PL00011414
   Konnov IV, 2003, OPTIM METHOD SOFTW, V18, P53, DOI 10.1080/1055678031000111236
   Laffont J.-J., 2009, THEORY INCENTIVES PR
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   McCullagh P., 1989, GEN LINEAR MODELS
   Nesterov Yurii, 1984, MATEKON, V29, P519
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   POLYAK BT, 1967, DOKL AKAD NAUK SSSR+, V174, P33
   Sikorski Jaroslaw, 1986, ANAL ALGORITHMS OPTI, P203
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   VARIAN HR, 1985, AM ECON REV, V75, P870
   Wolfstetter E., 1999, TOPICS MICROECONOMIC
   Zabotin Yaroslav Ivanovich, 1972, IZV VUZ MATEM, P27
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100086
DA 2019-06-15
ER

PT S
AU He, B
   Yue, YS
AF He, Bryan
   Yue, Yisong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Smooth Interactive Submodular Set Cover
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few (cost-weighted) actions as possible. It models settings where there is uncertainty regarding which submodular function to optimize. In this paper, we propose a new extension, which we call smooth interactive submodular set cover, that allows the target threshold to vary depending on the plausibility of each hypothesis. We present the first algorithm for this more general setting with theoretical guarantees on optimality. We further show how to extend our approach to deal with realvalued functions, which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings.
C1 [He, Bryan] Stanford Univ, Stanford, CA 94305 USA.
   [Yue, Yisong] CALTECH, Pasadena, CA 91125 USA.
RP He, B (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM bryanhe@stanford.edu; yyue@caltech.edu
CR Batra D., 2012, EUR C COMP VIS ECCV
   Chen Yuxin, 2013, INT C MACH LEARN ICM
   Dey Debadeepta, 2012, ROB SCI SYST C RSS
   El-Arini Khalid, 2009, ACM C KNOWL DISC DAT
   El-Arini Khalid, 2011, ACM C KNOWL DISC DAT
   Gabillon Victor, 2013, NEURAL INFORM PROCES
   Golovin Daniel, 2010, C LEARN THEOR COLT
   Guillory Andrew, 2012, THESIS
   Guillory Andrew, 2011, INT C MACH LEARN ICM
   Guillory Andrew, 2010, INT C MACH LEARN ICM
   Hanneke Steve, 2007, THESIS
   Javdani Shervin, 2013, IEEE INT C ROB AUT I
   Javdani Shervin, 2014, ARTIFICIAL INTELLIGE
   Krause Andreas, 2005, INT C MACH LEARN ICM
   Leskovec Jure, 2007, ACM C KNOWL DISC DAT
   Lin H., 2012, UNCERTAINTY ARTIFICI
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Prasad Adarsh, 2014, NEURAL INFORM PROCES
   Radlinski Filip, 2008, INT C MACH LEARN ICM
   Raman Karthik, 2012, ACM C KNOWL DISC DAT
   Rodriguez Manuel Gomez, 2010, ACM C KNOWL DISC DAT
   Ross Stephane, 2013, INT C MACH LEARN ICM
   Tschiatschek S., 2014, NEURAL INFORM PROCES
   WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435
   Yue Y., 2011, NEURAL INFORM PROCES
   Yue Yisong, 2008, INT C MACH LEARN ICM
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103040
DA 2019-06-15
ER

PT S
AU He, N
   Harchaoui, Z
AF He, Niao
   Harchaoui, Zaid
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems. Typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty. The proposed algorithm, called Semi-Proximal Mirror-Prox, leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain. The algorithm stands in contrast with more classical proximal gradient algorithms with smoothing, which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal complexity bounds, i.e. O (1/epsilon(2)), for the number of calls to linear minimization oracle. We present promising experimental results showing the interest of the approach in comparison to competing methods.
C1 [He, Niao] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Harchaoui, Zaid] NYU, INRIA, New York, NY 10003 USA.
RP He, N (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM nhe6@gatech.edu; zaid.harchaoui@nyu.edu
FU NSF [CMMI-1232623]; LabEx Persyval-Lab [ANR-11-LABX-0025]; project
   "Titan" (CNRS-Mastodons); MSR-Inria joint centre; Moore-Sloan Data
   Science Environment at NYU; project "Macaron" [ANR-14-CE23-0003-01]
FX The authors would like to thank A. Juditsky and A. Nemirovski for
   fruitful discussions. This work was supported by NSF Grant CMMI-1232623,
   LabEx Persyval-Lab (ANR-11-LABX-0025), project "Titan" (CNRS-Mastodons),
   project "Macaron" (ANR-14-CE23-0003-01), the MSR-Inria joint centre, and
   the Moore-Sloan Data Science Environment at NYU.
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bach Francis, 2015, SIAM J OPTIMIZATION
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Bertsekas D. P., 2015, CONVEX OPTIMIZATION
   Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514
   Cox Bruce, 2013, MATH PROGRAM, P1
   Dudik M., 2012, P 15 INT C ART INT S
   Garber  Dan, 2013, ARXIV13014666
   Harchaoui Zaid, 2013, MATH PROGRAM, P1
   Hazan E., 2012, ICML
   He Niao, 2013, ARXIV13111098
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   Juditsky Anatoli, 2013, ARXIV1312107
   Lan Guanghui, 2013, ARXIV
   Lan Guanghui, 2014, CONDITIONAL GRADIENT
   Mu Cun, 2014, ARXIV14037588
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nemirovski A, 2010, MATH OPER RES, V35, P52, DOI 10.1287/moor.1090.0427
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y, 2007, MATH PROGRAM, V110, P245, DOI 10.1007/s10107-006-0001-8
   Nesterov Yurii, 2015, TECHNICAL REPORT
   Ouyang Y., 2014, ACCELERATED LINEARIZ
   Parikh N., 2013, FDN TRENDS OPTIM, P1
   Pierucci Federico, 2014, C APPR AUT ACT CAP 1
   Schmidt Mark, 2011, ADV NIPS
   Zhang X., 2012, NIPS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101084
DA 2019-06-15
ER

PT S
AU Heess, N
   Wayne, G
   Silver, D
   Lillicrap, T
   Tassa, Y
   Erez, T
AF Heess, Nicolas
   Wayne, Greg
   Silver, David
   Lillicrap, Timothy
   Tassa, Yuval
   Erez, Tom
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Continuous Control Policies by Stochastic Value Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment instead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.
C1 [Heess, Nicolas; Wayne, Greg; Silver, David; Lillicrap, Timothy; Tassa, Yuval; Erez, Tom] Google DeepMind, London, England.
RP Heess, N (reprint author), Google DeepMind, London, England.
EM heess@google.com; gregwayne@google.com; davidsilver@google.com;
   countzero@google.com; tassa@google.com; etom@google.com
CR Abbeel P., 2006, ICML
   Atkeson C. G., 2012, ACC
   Baird L., 1995, ICML
   Balduzzi D., 2015, ARXIV150903005
   Coulom R, 2002, THESIS
   Deisenroth M., 2011, ICML
   Fairbank M., 2012, IJCNN
   Fairbank M., 2014, THESIS
   Grondman I, 2015, THESIS
   Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Kingma D.P., 2013, ARXIV13126114
   Lillicrap T P, 2015, ARXIV150902971
   LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1023/A:1022628806385
   Munos R, 2006, J MACH LEARN RES, V7, P771
   Narendra K, 1990, IEEE T NEURAL NETWOR, V1, P4, DOI DOI 10.1109/72.80202
   Nguyen D. H., 1990, IEEE Control Systems Magazine, V10, P18, DOI 10.1109/37.55119
   Pascanu Razvan, 2013, ICML
   Rezende D. J., 2014, ICML
   Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317
   SCHULMAN J., 2015, CORR
   Silver D., 2014, ICML
   Singh S. P., 1994, ICML
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 1999, NIPS
   Tassa Y., 2008, NIPS
   Todorov Emanuel, 2012, IROS
   Wawrzynski P, 2009, LECT NOTES COMPUT SC, V5495, P380, DOI 10.1007/978-3-642-04921-7_39
   Wawrzynski P, 2009, NEURAL NETWORKS, V22, P1484, DOI 10.1016/j.neunet.2009.05.011
   Werbos PJ, 1990, NEURAL NETWORKS CONT, V3, P67
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101056
DA 2019-06-15
ER

PT S
AU Hefny, A
   Downey, C
   Gordon, GJ
AF Hefny, Ahmed
   Downey, Carlton
   Gordon, Geoffrey J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Supervised Learning for Dynamical System Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recently there has been substantial interest in spectral methods for learning dynamical systems. These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporate prior information such as sparsity or structure. To address this problem, we present a new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L-1 regularization. Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does; the correctness of these instances follows directly from our general analysis.
C1 [Hefny, Ahmed; Downey, Carlton; Gordon, Geoffrey J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Hefny, A (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ahefny@cs.cmu.edu; cmdowney@cs.cmu.edu; ggordon@cs.cmu.edu
FU Department of Defense [FA8721-05-C-0003]; Carnegie Mellon University for
   the operation of the Software Engineering Institute; PNC Center for
   Financial Services Innovation; NIH [R01 MH 064537]; ONR [N000141512365]
FX This material is based upon work funded and supported by the Department
   of Defense under Contract No. FA8721-05-C-0003 with Carnegie Mellon
   University for the operation of the Software Engineering Institute, a
   federally funded research and development center.; Supported by a grant
   from the PNC Center for Financial Services Innovation; Supported by NIH
   grant R01 MH 064537 and ONR contract N000141512365.
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Balle  B., 2014, ICML, P1386
   BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196
   Boots B., 2012, THESIS
   Boots  B., 2011, P 25 NAT C ART INT A
   Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092
   Boots Byron, 2013, P 29 INT C UNC ART I
   Boots Byron, 2012, P 29 INT C MACH LEAR
   CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253
   Fukumizu K, 2013, J MACH LEARN RES, V14, P3753
   Gilks W., 1996, MARKOV CHAIN MONTE C
   Hsu Daniel J., 2009, COLT
   Koedinger K. R., 2010, HDB ED DATA MINING, V43, P43
   Langford J, 2009, P 26 ANN INT C MACH, P593
   Pearl J., 2000, CAUSALITY MODELS REA
   Rosencrantz M., 2004, P 21 INT C MACH LEAR, P695
   Siddiqi Sajid, 2010, P 13 INT C ART INT S
   Song L., 2010, P 27 INT C MACH LEAR
   Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961
   Stock J. H., 2011, ADDISON WESLEY SERIE
   Van Overschee P, 1996, SUBSPACE IDENTIFICAT
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100041
DA 2019-06-15
ER

PT S
AU Henao, R
   Gan, Z
   Lu, J
   Carin, L
AF Henao, Ricardo
   Gan, Zhe
   Lu, James
   Carin, Lawrence
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Poisson Factor Modeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a new deep architecture for topic modeling, based on Poisson Factor Analysis (PFA) modules. The model is composed of a Poisson distribution to model observed vectors of counts, as well as a deep hierarchy of hidden binary units. Rather than using logistic functions to characterize the probability that a latent binary unit is on, we employ a Bernoulli-Poisson link, which allows PFA modules to be used repeatedly in the deep architecture. We also describe an approach to build discriminative topic models, by adapting PFA modules. We derive efficient inference via MCMC and stochastic variational methods, that scale with the number of non-zeros in the data and binary units, yielding significant efficiency, relative to models based on logistic links. Experiments on several corpora demonstrate the advantages of our model when compared to related deep models.
C1 [Henao, Ricardo; Gan, Zhe; Lu, James; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
RP Henao, R (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM r.henao@duke.edu; zhe.gan@duke.edu; james.lu@duke.edu; lcarin@duke.edu
FU ARO; DARPA; ONR; NGA; DOE
FX This research was supported in part by ARO, DARPA, DOE, NGA and ONR.
CR Blei D. M., 2004, NIPS
   Blei D. M., 2007, AOAS
   Blei D. M., 2003, JMLR
   Chen T., 2014, ICML
   Ding N., 2014, NIPS
   Gan Z., 2015, NIPS
   Gan Z., 2015, AISTATS
   Gan Zhe, 2015, ICML
   Guhaniyogi R., 2014, ARXIV14013632
   Hinton G., 2009, NIPS
   Hinton G. E., 2013, UAI
   Hinton G. E., 2002, NEURAL COMPUTATION
   Hinton G. E., 2006, NEURAL COMPUTATION
   Ho Q., 2013, NIPS
   Hoffman M. D., 2013, JMLR
   Hoffman Matthew, 2010, NIPS
   Lacoste-Julien S., 2009, NIPS
   Larochelle H., 2012, NIPS
   Lecun Y., 1998, P IEEE
   Li M., 2014, NIPS
   Maaloe L, 2015, ARXIV150104325
   Mcauliffe J. D., 2008, NIPS
   Neal Radford M., 1992, ARTIFICIAL INTELLIGE
   Paisley J., 2015, PAMI
   Ranganath Rajesh, 2014, AISTATS
   Salakhutdinov R., 2009, AISTATS
   Teh Y. W., 2006, JASA
   Welling M., 2011, ICML
   Williamson S., 2010, ICML
   Zhou M., 2015, AISTATS
   Zhou M., 2012, AISTATS
   Zhou M., 2015, PAMI
   Zhu J., 2012, JMLR
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101046
DA 2019-06-15
ER

PT S
AU Herbster, M
   Pasteris, S
   Ghosh, S
AF Herbster, Mark
   Pasteris, Stephen
   Ghosh, Shaona
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Prediction at the Limit of Zero Temperature
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID COMPLEXITY; CUTS
AB We design an online algorithm to classify the vertices of a graph. Underpinning the algorithm is the probability distribution of an Ising model isomorphic to the graph. Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far. Computing these classifications is unfortunately based on a #P complete problem. This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework. Our algorithm is optimal when the graph is a tree matching the prior results in [1]. For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient, as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.
C1 [Herbster, Mark; Pasteris, Stephen] UCL, Dept Comp Sci, London WC1E 6BT, England.
   [Ghosh, Shaona] Univ Southampton, ECS, Southampton SO17 1BJ, Hants, England.
RP Herbster, M (reprint author), UCL, Dept Comp Sci, London WC1E 6BT, England.
EM m.herbster@cs.ucl.ac.uk; s.pasteris@cs.ucl.ac.uk; ghosh.shaona@gmail.com
CR BALL MO, 1983, NETWORKS, V13, P253, DOI 10.1002/net.3230130210
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Blum A, 2001, P 18 INT C MACH LEAR, P19
   Cesa-Bianchi Nicolo, 2010, P INT C MACH LEARN, P175
   Cesa-Bianchi Nicolo, 2009, P 22 ANN C LEARN
   Chapelle O., 2003, ADV NEURAL INFORM PR, P601
   Ford L. R., 1956, CAN J MATH, V8, P399, DOI DOI 10.4153/CJM-1956-045-5
   Freivalds R. V., 1972, SOV MATH DOKL, V13, P1224
   Gartner Thomas, 2007, P 18 EUR C MACH LEAR
   Goldberg LA, 2007, COMB PROBAB COMPUT, V16, P43, DOI 10.1017/S096354830600767X
   Herbster M., 2005, P 22 INT C MACH LEAR, P305
   Herbster M., 2009, P 22 ANN C LEARN THE
   Herbster M, 2008, LECT NOTES ARTIF INT, V5254, P54, DOI 10.1007/978-3-540-87987-9_9
   Herbster Mark, 2009, ADV NEURAL INFORM PR, P649
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   PICARD JC, 1980, MATH PROGRAM STUD, V13, P8
   PROVAN JS, 1983, SIAM J COMPUT, V12, P777, DOI 10.1137/0212053
   Szummer M, 2001, NIPS, V14, P945
   Vitale Fabio, 2011, NIPS, P1584
   Zhou D., 2003, NIPS
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100058
DA 2019-06-15
ER

PT S
AU Hermann, KM
   Kocisky, T
   Grefenstette, E
   Espeholt, L
   Kay, W
   Suleyman, M
   Blunsom, P
AF Hermann, Karl Moritz
   Kocisky, Tomas
   Grefenstette, Edward
   Espeholt, Lasse
   Kay, Will
   Suleyman, Mustafa
   Blunsom, Phil
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Teaching Machines to Read and Comprehend
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.
C1 [Hermann, Karl Moritz; Kocisky, Tomas; Grefenstette, Edward; Espeholt, Lasse; Kay, Will; Suleyman, Mustafa; Blunsom, Phil] Google DeepMind, London, England.
   [Kocisky, Tomas; Blunsom, Phil] Univ Oxford, Oxford, England.
RP Hermann, KM (reprint author), Google DeepMind, London, England.
EM kmh@google.com; tkocisky@google.com; etg@google.com;
   lespeholt@google.com; wkay@google.com; mustafasul@google.com;
   pblunsom@google.com
CR Bahdanau  Dzmitry, 2014, ABS14090473 CORR
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Das Dipanjan, 2013, COMPUTATIONAL LINGUI, V40, P9
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Gregor  Karol, 2015, ABS150204623 CORR
   Hermann Karl Moritz, 2014, P ACL JUN
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kalchbrenner N., 2014, P ACL
   Mnih Volodymyr, ADV NEURAL INFORM PR, V27
   Poon Hoifung, P NAACL HLT 2010 1 I
   Richardson Matthew, P EMNLP
   Riloff Ellen, P ANLP NAACL WORKSH
   Sukhbaatar Sainbayar, 2015, ABS150308895 CORR
   Sutskever Ilya, ADV NEURAL INFORM PR, V27
   Svore Krysta, P EMNLP CONLL
   Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Weston J., 2014, ARXIV14103916
   Winograd T., 1972, UNDERSTANDING NATURA
   Woodsend Kristian, 2010, P ACL
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102093
DA 2019-06-15
ER

PT S
AU Hong, S
   Noh, H
   Han, B
AF Hong, Seunghoon
   Noh, Hyeonwoo
   Han, Bohyung
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in PASCAL VOC dataset.
C1 [Hong, Seunghoon; Noh, Hyeonwoo; Han, Bohyung] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
RP Hong, S (reprint author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
EM maga33@postech.ac.kr; hyeonwoonoh_@postech.ac.kr; bhhan@postech.ac.kr
FU ICT R&D program of MSIP/IITP [B0101-15-0307]; Samsung Electronics Co.,
   Ltd.; ICT R&D program of MSIP/IITP [ML Center] [B0101-15-0552]; ICT R&D
   program of MSIP/IITP [DeepView]
FX This work was partly supported by the ICT R&D program of MSIP/IITP
   [B0101-15-0307; ML Center, B0101-15-0552; DeepView] and Samsung
   Electronics Co., Ltd.
CR Chen L. C., 2015, ICLR
   Dai J., 2015, ICCV
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Hariharan B., 2015, CVPR
   Hariharan B., 2014, ECCV
   Hariharan B., 2011, ICCV
   Jia Deng, 2009, CVPR
   Jia Y., 2014, ARXIV14085093
   Long  J., 2015, CVPR
   Mostajabi  M., 2015, CVPR
   Noh H., 2015, ICCV
   Papandreou G., 2015, ICCV
   Pathak D., 2015, ICLR
   Pinheiro Pedro O, 2015, CVPR
   Simonyan K, 2014, ICLR WORKSH
   Simonyan Karen, 2015, ICLR
   Zheng S., 2015, ICCV
   Zitnick C. L., 2014, ECCV
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102007
DA 2019-06-15
ER

PT S
AU Hosseini, R
   Sra, S
AF Hosseini, Reshad
   Sra, Suvrit
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Matrix Manifold Optimization for Gaussian Mixtures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MAXIMUM-LIKELIHOOD; EM ALGORITHM
AB We take a new look at parameter estimation for Gaussian Mixture Model (GMMs). Specifically, we advance Riemannian manifold optimization (on the manifold of positive definite matrices) as a potential replacement for Expectation Maximization (EM), which has been the de facto standard for decades. An out-of-the-box invocation of Riemannian optimization, however, fails spectacularly: it obtains the same solution as EM, but vastly slower. Building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences: it makes Riemannian optimization not only match EM (a nontrivial result on its own, given the poor record nonlinear programming has had against EM), but also outperform it in many settings. To bring our ideas to fruition, we develop a welltuned Riemannian LBFGS method that proves superior to known competing methods (e.g., Riemannian conjugate gradient). We hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics.
C1 [Hosseini, Reshad] Univ Tehran, Coll Engn, Sch ECE, Tehran, Iran.
   [Sra, Suvrit] MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Hosseini, R (reprint author), Univ Tehran, Coll Engn, Sch ECE, Tehran, Iran.
EM reshad.hosseini@ut.ac.ir; suvrit@mit.edu
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bhatia R, 2007, PRINC SER APPL MATH, P1
   Bishop C., 2007, PATTERN RECOGNITION
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Burer S., 1999, TR9917 RIC U
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Ge R., 2015, ARXIV150300424
   Hosseini R., 2015, ARXIV150706065
   Hosseini R., 2015, ARXIV150607677
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Keener RW, 2010, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-93839-4
   Lee J., 2012, INTRO SMOOTH MANIFOL
   Ma JW, 2000, NEURAL COMPUT, V12, P2881, DOI 10.1162/089976600300014764
   McLachlan G.J., 2000, FINITE MIXTURE MODEL
   Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15
   Murphy KP, 2012, MACHINE LEARNING PRO
   Naim  I., 2012, P 29 INT C MACH LEAR, P1655
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X
   Salakhutdinov R., 2003, P INT C MACH LEARN, P672
   Sra  S., 2013, ADV NEURAL INFORM PR, P2562
   Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168
   Udriste  C., 1994, CONVEX FUNCTIONS OPT
   Vanderbei R. J., 2000, TECHNICAL REPORT
   Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
   Verbeek JJ, 2003, NEURAL COMPUT, V15, P469, DOI 10.1162/089976603762553004
   Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
   Zoran D., 2012, ADV NEURAL INFORM PR, P1736
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101072
DA 2019-06-15
ER

PT S
AU Hsu, D
   Kontorovich, A
   Szepesvari, C
AF Hsu, Daniel
   Kontorovich, Aryeh
   Szepesvari, Csaba
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Mixing Time Estimation in Reversible Markov Chains from a Single Sample
   Path
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DISTRIBUTIONS; CONVERGENCE; EXTENSION; THEOREM; FINITE; RATES
AB This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time t(mix) of a finite reversible ergodic Markov chain at a prescribed confidence level. The interval is computed from a single finite-length sample path from the Markov chain, and does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time t(relax), which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a root n rate, where n is the length of the sample path. Upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy. The lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before seeing each state at least Omega(t(relax)) times on the average. Finally, future directions of research are identified.
C1 [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA.
   [Kontorovich, Aryeh] Ben Gurion Univ Negev, Beer Sheva, Israel.
   [Szepesvari, Csaba] Univ Alberta, Edmonton, AB, Canada.
RP Hsu, D (reprint author), Columbia Univ, New York, NY 10027 USA.
EM djhsu@cs.columbia.edu; karyeh@cs.bgu.ac.il; szepesva@cs.ualberta.ca
CR Atchade Y., 2015, BERNOULLI
   Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113
   Batu T, 2013, J ACM, V60, DOI 10.1145/2432622.2432626
   Bernstein S, 1927, MATH ANN, V97, P1, DOI 10.1007/BF01447859
   Bhatnagar Nayantara, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P424, DOI 10.1007/978-3-642-22935-0_36
   Cho GE, 2001, LINEAR ALGEBRA APPL, V335, P137, DOI 10.1016/S0024-3795(01)00320-2
   Flegal JM, 2011, CH CRC HANDB MOD STA, P175
   Gamarnik D, 2003, IEEE T INFORM THEORY, V49, P338, DOI 10.1109/TIT.2002.806131
   Garren ST, 2000, BERNOULLI, V6, P215, DOI 10.2307/3318575
   Gillman D, 1998, SIAM J COMPUT, V27, P1203, DOI 10.1137/S0097539794268765
   Gyori B. M., 2014, ARXIV12122016
   Hsu D., 2015, CORR
   Jones GL, 2001, STAT SCI, V16, P312, DOI 10.1214/ss/1015346317
   Karandikar RL, 2002, STAT PROBABIL LETT, V58, P297, DOI 10.1016/S0167-7152(02)00124-4
   KIPNIS C, 1986, COMMUN MATH PHYS, V104, P1, DOI 10.1007/BF01210789
   Kontoyiannis I., 2006, VALUETOOLS, P45
   Leon CA, 2004, ANN APPL PROBAB, V14, P958
   Levin D. A., 2008, MARKOV CHAINS MIXING
   Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4
   Liu J., 2001, SPRINGER SERIES STAT
   Maurer Andreas, 2009, COLT
   McDonald Daniel J, 2011, JMLR Workshop Conf Proc, V15, P516
   MEYER CD, 1975, SIAM REV, V17, P443
   Meyn S. P., 1993, MARKOV CHAINS STOCHA
   Mnih V, 2008, P 25 INT C MACH LEAR, P672
   Mohri M., 2008, NIPS
   Mohri M., 2009, NIPS
   Paulin D, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-4039
   Steinwart I., 2009, NIPS
   Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001
   Stewart GW, 1990, MATRIX PERTURBATION
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Swaminathan A., 2015, ICML
   Tropp J., 2015, FDN TRENDS MACHINE L
   YU B, 1994, ANN PROBAB, V22, P94, DOI 10.1214/aop/1176988849
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103058
DA 2019-06-15
ER

PT S
AU Huang, JJ
   Qiu, Q
   Sapiro, G
   Calderbank, R
AF Huang, Jiaji
   Qiu, Qiang
   Sapiro, Guillermo
   Calderbank, Robert
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Discriminative Robust Transformation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, epsilon)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild, demonstrate the value of being able to balance discrimination and robustness.
C1 [Huang, Jiaji; Qiu, Qiang; Sapiro, Guillermo; Calderbank, Robert] Duke Univ, Dept Elect Engn, Durham, NC 27708 USA.
RP Huang, JJ (reprint author), Duke Univ, Dept Elect Engn, Durham, NC 27708 USA.
EM jiaji.huang@duke.edu; qiang.qiu@duke.edu; guillermo.sapiro@duke.edu;
   robert.calderbank@duke.edu
FU AFOSR [FA 9550-13-1-0076]; NGA [HM017713-1-0006]; NSF; DoD
FX The work of Huang and Calderbank was supported by AFOSR under FA
   9550-13-1-0076 and by NGA under HM017713-1-0006. The work of Qiu and
   Sapiro is partially supported by NSF and DoD.
CR Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044
   Chen D., 2013, IEEE C COMP VIS PATT
   Chen D, 2012, EUR C COMP VIS ECCV
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Fukunaga K., 1990, INTRO STAT PATTERN R
   Globerson A., 2005, ADV NEURAL INFORM PR
   Goldberger J., 2004, ADV NEURAL INFORM PR
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang G. B., 2007, 0749 U MASS
   Huang J, 2015, IEEE IC COMP COM NET
   Qiu Q, 2015, J MACH LEARN RES, V16, P187
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xing E. P., 2002, ADV NEURAL INFORM PR
   Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1
   Zha Z., 2009, INT JOINT C ART INT
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103013
DA 2019-06-15
ER

PT S
AU Huang, QQ
   Kakade, SM
AF Huang, Qingqing
   Kakade, Sham M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Super-Resolution Off the Grid
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DECOMPOSITION
AB Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly.
   Suppose we have k point sources in d dimensions, where the points are separated by at least Delta from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees:
   The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/Delta) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as Omega(root d/Delta).
   The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation Delta. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities.
   Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hypergrid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition).
C1 [Huang, Qingqing] MIT, EECS, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Kakade, Sham M.] Univ Washington, Dept Stat Comp Sci & Engn, Seattle, WA 98195 USA.
RP Huang, QQ (reprint author), MIT, EECS, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM qqh@mit.edu; sham@cs.washington.edu
FU Washington Research Foundation for innovation in Data-intensive
   Discovery
FX The authors thank Rong Ge and Ankur Moitra for very helpful discussions.
   Sham Kakade acknowledges funding from the Washington Research Foundation
   for innovation in Data-intensive Discovery.
CR Anandkumar A., 2012, ARXIV12030683
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora S., 2001, P 33 S THEOR COMP, P247
   Candes EJ, 2013, J FOURIER ANAL APPL, V19, P1229, DOI 10.1007/s00041-013-9292-3
   Candes EJ, 2014, COMMUN PUR APPL MATH, V67, P906, DOI 10.1002/cpa.21455
   Chen YX, 2014, IEEE T INFORM THEORY, V60, P6576, DOI 10.1109/TIT.2014.2343623
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   DASGUPTA S, 2000, P 16 C UNC ART INT, P152
   DONOHO DL, 1992, SIAM J MATH ANAL, V23, P1309, DOI 10.1137/0523074
   Fernandez-Granda C., 2014, THESIS
   Harshman RA, 1970, FDN PARAFAC PROCEDUR
   Komornik Vilmos, 2005, SPRINGER MG MATH
   LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071
   Liao W., 2014, APPL COMPUTATIONAL H
   Moitra A., 2014, ARXIV14081681
   MOSSEL E., 2005, P 37 ANN ACM S THEOR, P366
   Nandi S, 2013, COMPUT STAT DATA AN, V58, P147, DOI 10.1016/j.csda.2011.03.002
   Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003
   Potts D, 2013, LINEAR ALGEBRA APPL, V439, P1024, DOI 10.1016/j.laa.2012.10.036
   RUSSELL DL, 1978, SIAM REV, V20, P639, DOI 10.1137/1020095
   Schiebinger G., 2015, ARXIV150603144
   Tang GG, 2013, IEEE T INFORM THEORY, V59, P7465, DOI 10.1109/TIT.2013.2277451
   Vempala S. S., 2014, ARXIV14122954
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100105
DA 2019-06-15
ER

PT S
AU Huang, TK
   Agarwal, A
   Hsu, D
   Langford, J
   Schapire, RE
AF Huang, Tzu-Kuo
   Agarwal, Alekh
   Hsu, Daniel
   Langford, John
   Schapire, Robert E.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient and Parsimonious Agnostic Active Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We develop a new active learning algorithm for the streaming setting satisfying three important properties: 1) It provably works for any classifier representation and classification problem including those with severe noise. 2) It is efficiently implementable with an ERM oracle. 3) It is more aggressive than all previous approaches satisfying 1 and 2. To do this, we create an algorithm based on a newly defined optimization problem and analyze it. We also conduct the first experimental analysis of all efficient agnostic active learning algorithms, evaluating their strengths and weaknesses in different settings.
C1 [Huang, Tzu-Kuo; Agarwal, Alekh; Langford, John; Schapire, Robert E.] Microsoft Res, New York, NY 10011 USA.
   [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA.
RP Huang, TK (reprint author), Microsoft Res, New York, NY 10011 USA.
EM tkhuang@microsoft.com; alekha@microsoft.com; djhsu@cs.columbia.edu;
   jcl@microsoft.com; schapire@microsoft.com
CR Balcan M. - F., 2013, 26 ANN C LEARN THEOR, P288
   Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5
   Beygelzimer A., 2009, ICML
   Beygelzimer A., 2010, NIPS
   Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Dasgupta S., 2007, NIPS
   Dasgupta Sanjoy, 2005, ADV NEURAL INFORM PR, V18
   Hanneke S, 2009, THESIS
   Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037
   HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784
   Hsu Daniel, 2010, THESIS
   Huang Tzu-Kuo, 2015, ARXIV150608669
   Karampatziakis N., 2011, P C UNC ART INT, P392
   Koltchinskii V, 2010, J MACH LEARN RES, V11, P2457
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Zhang Chicheng, 2014, ADV NEURAL INFORM PR, P442
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102087
DA 2019-06-15
ER

PT S
AU Huang, Y
   Wang, W
   Wang, L
AF Huang, Yan
   Wang, Wei
   Wang, Liang
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bidirectional Recurrent Convolutional Networks for Multi-Frame
   Super-Resolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RESOLUTION
AB Super resolving a low-resolution video is usually handled by either single-image super-resolution (SR) or multi-frame SR. Single-Image SR deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution. Multi-Frame SR generally extracts motion information, e.g., optical flow, to model the temporal dependency, which often shows high computational cost. Considering that recurrent neural networks (RNNs) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame SR. Different from vanilla RNNs, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling. With the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance. Due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.
C1 [Huang, Yan; Wang, Wei; Wang, Liang] Chinese Acad Sci, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.
   [Wang, Liang] Chinese Acad Sci, Inst Automat, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.
RP Huang, Y (reprint author), Chinese Acad Sci, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.
EM yhuang@nlpr.ia.ac.cn; wangwei@nlpr.ia.ac.cn; wangliang@nlpr.ia.ac.cn
FU National Natural Science Foundation of China [61420106015, 61175003,
   61202328, 61572504]; National Basic Research Program of China
   [2012CB316300]
FX This work is jointly supported by National Natural Science Foundation of
   China (61420106015, 61175003, 61202328, 61572504) and National Basic
   Research Program of China (2012CB316300).
CR Baker S., 1999, TECHNICAL REPORT
   Bascle B., 1996, COMPUTER VISION ECCV, P571
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H., 2004, COMP VIS PATT REC 20, V1, pI
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jain V., 2008, P ADV NEUR INF PROC, V8, P769
   Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Mitzel D, 2009, LECT NOTES COMPUT SC, V5748, P432
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shahar O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3353, DOI 10.1109/CVPR.2011.5995360
   Sutskever  I., 2007, INT C ART INT STAT, P548
   Takeda H, 2009, IEEE T IMAGE PROCESS, V18, P1958, DOI 10.1109/TIP.2009.2023703
   Taylor G., 2006, P ADV NEUR INF PROC, P448
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Xu  L., 2014, ADV NEURAL INFORM PR, P1790
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101038
DA 2019-06-15
ER

PT S
AU Hughes, MC
   Stephenson, W
   Sudderth, EB
AF Hughes, Michael C.
   Stephenson, William
   Sudderth, Erik B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Scalable Adaptation of State Complexity for Nonparametric Hidden Markov
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DISCOVERY
AB Bayesian nonparametric hidden Markov models are typically learned via fixed truncations of the infinite state space or local Monte Carlo proposals that make small changes to the state space. We develop an inference algorithm for the sticky hierarchical Dirichlet process hidden Markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality. Unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space. Our birth proposals use observed data statistics to create useful new states that escape local optima. Merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations. Experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors. We have released an open-source Python implementation which can parallelize local inference steps across sequences.
C1 [Hughes, Michael C.; Stephenson, William; Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
RP Hughes, MC (reprint author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
EM mhughes@cs.brown.edu; wtstephe@gmail.com; sudderth@cs.brown.edu
FU NSF CAREER Award [IIS-1349774]; NSF Graduate Research Fellowship
   [DGE0228243]
FX This research supported in part by NSF CAREER Award No. IIS-1349774. M.
   Hughes supported in part by an NSF Graduate Research Fellowship under
   Grant No. DGE0228243.
CR Beal M.J., 2003, THESIS
   Beal Matthew J., 2001, NEURAL INFORM PROCES
   Broderick Tamara, 2013, NEURAL INFORM PROCES
   Bryant Michael, 2012, NEURAL INFORM PROCES
   Chang Jason, 2014, NEURAL INFORM PROCES
   Ernst J, 2010, NAT BIOTECHNOL, V28, P817, DOI 10.1038/nbt.1662
   Foti Nicholas, 2014, NEURAL INFORM PROCES
   Fox EB, 2014, ANN APPL STAT, V8, P1281, DOI 10.1214/14-AOAS742
   Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395
   Ghahramani Z, 2001, INT J PATTERN RECOGN, V15, P9, DOI 10.1142/S0218001401000836
   Hoffman Matt, 2013, J MACHINE LEARNING R, V14
   Hoffman MM, 2012, NAT METHODS, V9, P473, DOI [10.1038/NMETH.1937, 10.1038/nmeth.1937]
   Hughes Michael C., 2015, ARTIFICIAL INTELLIGE
   Hughes Michael C., 2013, NEURAL INFORM PROCES
   Johnson Matthew J., 2014, INT C MACH LEARN
   Liang Percy, 2007, EMPIRICAL METHODS NA
   NIST, 2007, RICH TRANSCR DAT
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Stolcke Andreas, 1993, NEURAL INFORM PROCES
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Teh Yee Whye, 2008, NEURAL INFORM PROCES
   Van Gael J., 2008, INT C MACH LEARN
   Wang Chong, 2012, NEURAL INFORM PROCES
   Wang Chong, 2012, ARXIV12011657
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100029
DA 2019-06-15
ER

PT S
AU Inouye, DI
   Ravikumar, P
   Dhillon, IS
AF Inouye, David I.
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a novel distribution that generalizes the Multinomial distribution to enable dependencies between dimensions. Our novel distribution is based on the parametric form of the Poisson MRF model [1] but is fundamentally different because of the domain restriction to a fixed-length vector like in a Multinomial where the number of trials is fixed or known. Thus, we propose the Fixed-Length Poisson MRF (LPMRF) distribution. We develop AIS sampling methods to estimate the likelihood and log partition function (i.e. the log normalizing constant), which was not developed for the Poisson MRF model. In addition, we propose novel mixture and topic models that use LPMRF as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed Admixture of Poisson MRFs [2]. We show the effectiveness of our LPMRF distribution over Multinomial models by evaluating the test set perplexity on a dataset of abstracts and Wikipedia. Qualitatively, we show that the positive dependencies discovered by LPMRF are interesting and intuitive. Finally, we show that our algorithms are fast and have good scaling (code available online).
C1 [Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Inouye, DI (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM dinouye@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu
FU NSF [DGE-1110007, IIS-1149803, IIS-1447574, DMS-1264033, CCF-1320746];
   ARO [W911NF-12-1-0390]
FX This work was supported by NSF (DGE-1110007, IIS-1149803, IIS-1447574,
   DMS-1264033, CCF-1320746) and ARO (W911NF-12-1-0390).
CR Aletras N., 2013, P 10 INT C COMP SEM, P13
   Altham P. M. E., 1978, APPLIED STATISTICS, V27, P162, DOI DOI 10.2307/2346943
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chang J., 2009, NIPS
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Inouye D., 2014, P 31 INT C MACH LEAR, P683
   Inouye D. I., 2014, NIPS, P3158
   Mimno D., 2011, P C EMP METH NAT LAN, P227
   Mimno D., 2011, P C EMP METH NAT LAN, P262
   Nallapati R., 2007, ICDM, P343
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Newman D., 2010, P 10 ANN JOINT C DIG, P215, DOI DOI 10.1145/1816123.1816156
   Steyvers Mark, 2007, HDB LATENT SEMANTIC, V427, P424
   Yang E, 2013, NIPS, P1718
   YANG E., 2012, ADV NEURAL INFO PROC, V25, P1367
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102026
DA 2019-06-15
ER

PT S
AU Jain, P
   Tewari, A
AF Jain, Prateek
   Tewari, Ambuj
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Alternating Minimization for Regression Problems with Vector-valued
   Outputs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RECOVERY
AB In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality.
C1 [Jain, Prateek] Microsoft Res, Bangalore, Karnataka, India.
   [Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Jain, P (reprint author), Microsoft Res, Bangalore, Karnataka, India.
EM prajain@microsoft.com; tewaria@umich.edu
CR Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Arora S., 2014, P 27 C LEARN THEOR, P779
   Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Fiebig Denzil G., 2001, COMPANION THEORETICA
   Greene William H., 2011, ECONOMETRIC ANAL
   Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1
   Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Kim S, 2009, BIOINFORMATICS, V25, pI204, DOI 10.1093/bioinformatics/btp218
   Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013
   Lounici K., 2009, COLT
   Moon H. R, 2008, NEW PALGRAVE DICT EC
   Negahban SN, 2011, IEEE T INFORM THEORY, V57, P3841, DOI 10.1109/TIT.2011.2144150
   OBERHOFER W, 1974, ECONOMETRICA, V42, P579, DOI 10.2307/1911792
   Obozinski G, 2011, ANN STAT, V39, P1, DOI 10.1214/09-AOS776
   POURAHMADI M., 2013, HIGH DIMENSIONAL COV
   Rai P., 2012, NIPS, P3185
   Reinsel Gregory, 2004, ENCY STAT SCI
   Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188
   Sohn Kyung-Ah, 2012, AISTATS
   Srivastava VK, 1987, SEEMINGLY UNRELATED
   VERSHYNIN R., 2010, CORR
   ZELLNER A, 1962, J AM STAT ASSOC, V57, P348, DOI 10.2307/2281644
   Zellner A, 2010, J ECONOMETRICS, V159, P33, DOI 10.1016/j.jeconom.2010.04.005
   Zhang Y., 2014, COLT, V35, P921
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101079
DA 2019-06-15
ER

PT S
AU Jain, P
   Natarajan, N
   Tewari, A
AF Jain, Prateek
   Natarajan, Nagarajan
   Tewari, Ambuj
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Predtron: A Family of Online Algorithms for General Prediction Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds. The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification. For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.
C1 [Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India.
   [Natarajan, Nagarajan] Univ Texas Austin, Austin, TX 78712 USA.
   [Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Jain, P (reprint author), Microsoft Res, Bengaluru, Karnataka, India.
EM prajain@microsoft.com; naga86@cs.utexas.edu; tewaria@umich.edu
FU NSF [IIS-1319810]
FX A. Tewari acknowledges the support of NSF under grant IIS-1319810.
CR Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1
   Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939
   Crammer K, 2002, ADV NEUR IN, V14, P641
   Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188
   Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Li Y, 2002, P 9 INT C MACH LEARN, P379
   Mencia EL, 2008, IEEE IJCNN, P2899, DOI 10.1109/IJCNN.2008.4634206
   Mohri M., 2012, FDN MACHINE LEARNING
   Mroueh Y., 2012, ADV NEURAL INFORM PR, P2789
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   Ramaswamy Harish G, 2012, ADV NEURAL INFORM PR, P2078
   Ratliff N. D., 2007, AISTATS, P380
   Ratsch Gunnar, NIPS 2002 WORKSH CLA
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Tewari Ambuj, 2015, JMLR WORKSHOP C P, V37
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103038
DA 2019-06-15
ER

PT S
AU Jamieson, K
   Jain, L
   Fernandez, C
   Glattard, N
   Nowak, R
AF Jamieson, Kevin
   Jain, Lalit
   Fernandez, Chris
   Glattard, Nick
   Nowak, Robert
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI NEXT: A System for Real-World Development, Evaluation, and Application
   of Active Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for real-world, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.
C1 [Jamieson, Kevin] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Jain, Lalit; Fernandez, Chris; Glattard, Nick; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA.
RP Jamieson, K (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM kjamieson@berkeley.edu; ljain@wisc.edu; crfernandez@wisc.edu;
   glattard@wisc.edu; rdnowak@wisc.edu
CR Agarwal A, 2014, J MACH LEARN RES, V15, P1111
   Agarwal Alekh, 2013, ARXIV13108243
   Agarwal Deepak, 2009, DAT MIN 2009 ICDM 9
   Agarwal S, 2007, J MACHINE LEARNING R, P11
   Ambert KH, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00038
   Barowy DW, 2012, ACM SIGPLAN NOTICES, V47, P639, DOI 10.1145/2398857.2384663
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Crankshaw D., 2015, CIDR
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Gureckis T. M., PSITURK OPEN SOURCE
   Jamieson K., 2011, ALL C COMM CONTR COM
   Jamieson K., 2014, P 27 C LEARN THEOR
   Jamieson K. G., 2015, AISTATS
   Li L., 2010, INT C WORLD WID WEB
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Meng X., 2015, ARXIV150506807
   Moon Seungwhan, 2014, DAT SCI ADV AN DSAA
   Recht  Benjamin, 2011, ADV NEURAL INFORM PR
   Settles B., U WISCONSIN MADISON, V52, P11
   Tamuz Omer, 2011, P 28 INT C MACH LEAR
   Urvoy Tanguy, 2013, P 30 INT C MACH LEAR
   van der Maaten L. J. P., 2012, MACH LEARN SIGN PROC, P1
   Wallace BC, 2012, P 2 ACM SIGHIT INT H, P819, DOI [DOI 10.1145/2110363.2110464, 10.1145/2110363.2110464]
   Yue Y., 2012, J COMPUTER SYSTEM SC, V78
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102017
DA 2019-06-15
ER

PT S
AU Jawanpuria, P
   Lapin, M
   Hein, M
   Schiele, B
AF Jawanpuria, Pratik
   Lapin, Maksim
   Hein, Matthias
   Schiele, Bernt
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Output Kernel Learning for Multiple Tasks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.
C1 [Jawanpuria, Pratik; Hein, Matthias] Saarland Univ, Saarbrucken, Germany.
   [Lapin, Maksim; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany.
RP Jawanpuria, P (reprint author), Saarland Univ, Saarbrucken, Germany.
FU Cluster of Excellence (MMCI)
FX P.J. and M.H. acknowledge the support by the Cluster of Excellence
   (MMCI).
CR Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   BENISRAEL A, 1986, J AUST MATH SOC B, V28, P1, DOI 10.1017/S0334270000005142
   Caponnetto A, 2008, J MACH LEARN RES, V9, P1615
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Ciliberto C., 2015, ICML
   Dinuzzo F., 2011, ICML
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Evgeniou T., 2004, KDD
   Hein M., 2004, TR127 M PLANCK I BIO
   Hiai F, 2009, LINEAR ALGEBRA APPL, V431, P1125, DOI 10.1016/j.laa.2009.04.001
   HORN RA, 1969, T AM MATH SOC, V136, P269, DOI 10.2307/1994714
   Jacob L., 2008, NIPS
   Jalali A., 2010, NIPS
   Jawanpuria P., 2012, ICML
   Jawanpuria P., 2011, SDM
   Jawanpuria P, 2015, J MACH LEARN RES, V16, P617
   Kang Z., 2011, ICML
   Koskela M., 2014, P ACM INT C MULT
   Lapin M., 2014, CVPR
   Lounici K., 2009, COLT
   Maurer A., 2013, ICML
   Micchelli C. A., 2005, NIPS
   Scholkopf B., 2002, LEARNING KERNELS
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Xiao J., 2010, CVPR
   Zhang Y., 2010, UAI
   Zhou  B., 2014, NIPS
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102047
DA 2019-06-15
ER

PT S
AU Johansson, FD
   Chattoraj, A
   Bhattacharyya, C
   Dubhashi, D
AF Johansson, Fredrik D.
   Chattoraj, Ankani
   Bhattacharyya, Chiranjib
   Dubhashi, Devdatt
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Weighted Theta Functions and Embeddings with Applications to Max-Cut,
   Clustering and Summarization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS; SUPPORT
AB We introduce a unifying generalization of the Lovasz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming, and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for Max-Cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.
C1 [Johansson, Fredrik D.; Dubhashi, Devdatt] Chalmers Univ Technol, Comp Sci & Engn, SE-41296 Gothenburg, Sweden.
   [Chattoraj, Ankani] Univ Rochester, Brain & Cognit Sci, Rochester, NY 14627 USA.
   [Bhattacharyya, Chiranjib] Indian Inst Sci, Comp Sci & Automat, Bangalore 560012, Karnataka, India.
RP Johansson, FD (reprint author), Chalmers Univ Technol, Comp Sci & Engn, SE-41296 Gothenburg, Sweden.
EM frejohk@chalmers.se; achattor@ur.rochester.edu; chiru@csa.iisc.ernet.in;
   dubhashi@chalmers.se
FU Swedish Foundation for Strategic Research (SSF)
FX This work is supported in part by the Swedish Foundation for Strategic
   Research (SSF).
CR Arora S, 2005, ANN IEEE SYMP FOUND, P339, DOI 10.1109/SFCS.2005.35
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95
   Bonchi F, 2013, KNOWL INF SYST, V35, P1, DOI 10.1007/s10115-012-0522-9
   Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021
   Burer S, 2001, OPTIM METHOD SOFTW, V15, P175, DOI 10.1080/10556780108805818
   Elsner M., 2009, P WORKSH INT LIN PRO, P19
   Goemans MX, 1997, MATH PROGRAM, V79, P143, DOI 10.1007/BF02614315
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Grotschel  Martin, 1988, ALGORITHMS COMBINATO, V2
   Helmberg C, 2000, SIAM J OPTIMIZ, V10, P673, DOI 10.1137/S1052623497328987
   Hush D, 2006, J MACH LEARN RES, V7, P733
   Iyengar G, 2011, SIAM J OPTIMIZ, V21, P231, DOI 10.1137/090762671
   Jethava V., 2013, INF THEOR WORKSH ITW, P1
   Jethava V, 2013, J MACH LEARN RES, V14, P3495
   Johanson F. D., 2015, SUPPLEMENTARY MAT
   Knuth D. E., 1994, ELECT J COMB, V1
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985
   Lovasz L., 1999, P ERDOS HIS MATH
   Marti R, 2009, INFORMS J COMPUT, V21, P26, DOI 10.1287/ijoc.1080.0275
   Pham DT, 2005, P I MECH ENG C-J MEC, V219, P103, DOI 10.1243/095440605X8298
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   SCHRIJVER A, 1979, IEEE T INFORM THEORY, V25, P425, DOI 10.1109/TIT.1979.1056072
   Wang J, 2013, J MACH LEARN RES, V14, P771
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102051
DA 2019-06-15
ER

PT S
AU Johnson, R
   Zhang, T
AF Johnson, Rie
   Zhang, Tong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Semi-supervised Convolutional Neural Networks for Text Categorization
   via Region Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.
C1 [Johnson, Rie] RJ Res Consulting, Tarrytown, NY USA.
   [Zhang, Tong] Baidu Inc, Beijing, Peoples R China.
   [Zhang, Tong] Rutgers State Univ, Piscataway, NJ USA.
RP Johnson, R (reprint author), RJ Res Consulting, Tarrytown, NY USA.
EM riejohnson@gmail.com; tzhang@stat.rutgers.edu
FU NSF [IIS-1407939, IIS-1250985]; NIH [R01AI116744]
FX Tong Zhang would like to acknowledge NSF IIS-1250985, NSF IIS-1407939,
   and NIH R01AI116744 for supporting his research.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Collobert Ronan, 2008, P ICML
   Dhillon Paramveer S., 2011, P NIPS
   Gao J., 2014, P EMNLP
   Glorot X., 2011, P ICML
   Hinton G. E, 2012, ARXIV12070580
   JOACHIMS T, 1999, P ICML
   Johnson Rie, 2015, P NAACL HLT
   Kalchbrenner N., 2014, P 52 ANN M ASS COMP, V1, P655, DOI DOI 10.3115/V1/P14-1062
   Kim Y, 2014, P 2014 C EMP METH NA, P1746, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]
   LeCun Yann, P IEEE
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Maas A L, 2011, P ACL
   Mesnil Gregoire, 2015, ARXIV14125335V5
   Mikolov T., 2013, P NIPS
   Mnih A., 2008, NIPS
   Quoc Le, 2014, P ICML
   Rie K, 2007, P ICML
   Shen Yelong, 2014, P CIKM
   Tang D., 2014, P 52 ANN M ASS COMP, V1, P1555, DOI DOI 10.3115/V1/P14-1146
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Weston Jason, 2014, P 2014 C EMP METH NA, P1822
   Xu L, 2014, P ACL, V1, P336
   Xu Puyang, 2013, ASRU
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101108
DA 2019-06-15
ER

PT S
AU Jun, KS
   Zhu, XJ
   Rogers, T
   Yang, ZR
   Yuan, M
AF Jun, Kwang-Sung
   Zhu, Xiaojin
   Rogers, Timothy
   Yang, Zhuoran
   Yuan, Ming
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Human Memory Search as Initial-Visit Emitting Random Walk
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID FLUENCY
AB Imagine a random walk that outputs a state only when visiting it for the first time. The observed output is therefore a repeat-censored version of the underlying walk, and consists of a permutation of the states or a prefix of it. We call this model initial-visit emitting random walk (INVITE). Prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks, which is of great interest in both the study of human cognition and various clinical applications. However, parameter estimation in INVITE is challenging, because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable. In this paper, we propose the first efficient maximum likelihood estimate (MLE) for INVITE by decomposing the censored output into a series of absorbing random walks. We also prove theoretical properties of the MLE including identifiability and consistency. We show that INVITE outperforms several existing methods on real-world human response data from memory search tasks.
C1 [Jun, Kwang-Sung] Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA.
   [Zhu, Xiaojin] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
   [Rogers, Timothy] Univ Wisconsin, Dept Psychol, Madison, WI 53706 USA.
   [Yang, Zhuoran] Tsinghua Univ, Dept Math Sci, Beijing, Peoples R China.
   [Yuan, Ming] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.
RP Jun, KS (reprint author), Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA.
EM kjun@discovery.wisc.edu; jerryzhu@cs.wisc.edu; ttrogers@wisc.edu;
   yzr11@mails.tsinghua.edu.cn; myuan@stat.wisc.edu
FU NSF [IIS-0953219, DGE-1545481, DMS-1265202]; NIH Big Data to Knowledge
   [1U54AI117924-01]; NIH [1U54AI117924-01]
FX The authors are thankful to the anonymous reviewers for their comments.
   This work is supported in part by NSF grants IIS-0953219 and
   DGE-1545481, NIH Big Data to Knowledge 1U54AI117924-01, NSF Grant
   DMS-1265202, and NIH Grant 1U54AI117924-01.
CR Abbott J., 2012, ADV NEURAL INFORM PR, V25, P3050
   Abrahao B. D., 2013, CORR
   Bottou L., 2012, NEURAL NETWORKS TRIC, V7700, P430
   BRODER A, 1989, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.1989.63516
   CHAN AS, 1993, J COGNITIVE NEUROSCI, V5, P254, DOI 10.1162/jocn.1993.5.2.254
   Cockrell J. R., 2002, PRINCIPLES PRACTICE, P140, DOI DOI 10.1002/0470846410.CH27(II)
   Doyle P. G., 1984, RANDOM WALKS ELECT N
   Durrett R., 2012, SPRINGER TEXTS STAT
   Flory PJ, 1953, PRINCIPLES POLYM CHE
   Glenberg A. M., 2009, ITALIAN J LINGUISTIC
   Goni J, 2011, COGN PROCESS, V12, P183, DOI 10.1007/s10339-010-0372-x
   Griffiths TL, 2007, PSYCHOL SCI, V18, P1069, DOI 10.1111/j.1467-9280.2007.02027.x
   HENLEY NM, 1969, J VERB LEARN VERB BE, V8, P176, DOI 10.1016/S0022-5371(69)80058-7
   Hills TT, 2012, PSYCHOL REV, V119, P431, DOI 10.1037/a0027373
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   PASQUIER F, 1995, J NEUROL NEUROSUR PS, V58, P81, DOI 10.1136/jnnp.58.1.81
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Rodriguez M Gomez, 2010, P 16 ACM SIGKDD INT, P1019, DOI [DOI 10.1145/1835804.1835933, 10.1145/1835804.1835933]
   Rogers TT, 2006, NEUROPSYCHOLOGY, V20, P319, DOI 10.1037/0894-4105.20.3.319
   Ruppert D, 1988, TECH REP
   Troyer A., 1998, NEUROPSYCHOLOGIA, V36
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102027
DA 2019-06-15
ER

PT S
AU Kairouz, P
   Oh, S
   Viswanath, P
AF Kairouz, Peter
   Oh, Sewoong
   Viswanath, Pramod
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Secure Multi-party Differential Privacy
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of interactive function computation by multiple parties, each possessing a bit, in a differential privacy setting (i.e., there remains an uncertainty in any party's bit even when given the transcript of interactions and all the other parties' bits). Each party wants to compute a function, which could differ from party to party, and there could be a central observer interested in computing a separate function. Performance at each party is measured via the accuracy of the function to be computed. We allow for an arbitrary cost metric to measure the distortion between the true and the computed function values. Our main result is the optimality of a simple non-interactive protocol: each party randomizes its bit (sufficiently) and shares the privatized version with the other parties. This optimality result is very general: it holds for all types of functions, heterogeneous privacy conditions on the parties, all types of cost metrics, and both average and worst-case (over the inputs) measures of accuracy.
C1 [Kairouz, Peter; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA.
RP Kairouz, P (reprint author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM kairouz2@illinois.edu; swoh@illinois.edu; pramodv@illinois.edu
FU NSF CISE [CCF-1422278]; NSF SaTC award [CNS-1527754]; NSF CMMI award
   [MES-1450848]; NSF ENG award [ECCS-1232257]
FX This research is supported in part by NSF CISE award CCF-1422278, NSF
   SaTC award CNS-1527754, NSF CMMI award MES-1450848 and NSF ENG award
   ECCS-1232257.
CR Abbe Emmanuel, 2011, AM ECON REV, V102, P65
   Beimel A, 2008, LECT NOTES COMPUT SC, V5157, P451, DOI 10.1007/978-3-540-85174-5_25
   Ben-Or M., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P1, DOI 10.1145/62212.62213
   BLACKWELL D, 1953, ANN MATH STAT, V24, P265, DOI 10.1214/aoms/1177729032
   Blum A, 2005, PODS, P128, DOI DOI 10.1145/1065167.1065184
   Brenner H, 2010, ANN IEEE SYMP FOUND, P71, DOI 10.1109/FOCS.2010.13
   Calandrino JA, 2011, P IEEE S SECUR PRIV, P231, DOI 10.1109/SP.2011.40
   Chaudhuri K., 2012, ADV NEURAL INFORM PR, V25, P989
   Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Chaum D., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P11, DOI 10.1145/62212.62214
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Geng Q., 2012, ARXIV12121186
   Geng Q., 2013, ARXIV13120655
   Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X
   Goldreich O., 1987, P 19 STOC, P218, DOI DOI 10.1145/28395.28420
   Goyal V, 2013, LECT NOTES COMPUT SC, V8042, P298, DOI 10.1007/978-3-642-40041-4_17
   Gupte M, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P135, DOI 10.1145/1807085.1807105
   Hardt Moritz, 2012, P 44 ANN ACM S THEOR, P1255, DOI DOI 10.1145/2213977.2214088
   Homer N, 2008, PLOS GENET, V4, DOI 10.1371/journal.pgen.1000167
   Kairouz P., 2014, ADV NEURAL INFORM PR
   Kapralov M, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1395
   Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090
   Kilian J., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P316, DOI 10.1145/335305.335342
   Kunzler R, 2009, LECT NOTES COMPUT SC, V5444, P238
   McGregor A, 2010, ANN IEEE SYMP FOUND, P81, DOI 10.1109/FOCS.2010.14
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Narayanan A, 2008, P IEEE S SECUR PRIV, P111, DOI 10.1109/SP.2008.33
   Oh S., 2013, ARXIV13110776
   Prabhakaran MM, 2012, 2012 IEEE INFORMATION THEORY WORKSHOP (ITW), P99, DOI 10.1109/ITW.2012.6404773
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Sweeney L., 2000, HEALTH, V671, P1
   WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137
   Yao Andrew C, 2013, 2013 IEEE 54 ANN S F, P160
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103042
DA 2019-06-15
ER

PT S
AU Kandasamy, K
   Krishnamurthy, A
   Poczos, B
   Wasserman, L
   Robins, JM
AF Kandasamy, Kirthevasan
   Krishnamurthy, Akshay
   Poczos, Barnabas
   Wasserman, Larry
   Robins, James M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Nonparametric von Mises Estimators for Entropies, Divergences and Mutual
   Informations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are derived from the von Mises expansion and are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.
C1 [Kandasamy, Kirthevasan; Poczos, Barnabas; Wasserman, Larry] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Krishnamurthy, Akshay] Microsoft Res, New York, NY USA.
   [Robins, James M.] Harvard Univ, Cambridge, MA 02138 USA.
RP Kandasamy, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kandasamy@cs.cmu.edu; akshaykr@cs.cmu.edu; bapoczos@cs.cmu.edu;
   larry@stat.cmu.edu; robins@hsph.harvard.edu
FU NSF [IIS-1247658]; DOE [DESC0011114]
FX This work is supported in part by NSF Big Data grant IIS-1247658 and DOE
   grant DESC0011114.
CR Beirlant Jan, 1997, INT J MATH STAT SCI
   Bickel Peter J., 1988, SANKHYA INDIAN J STA
   Birge Lucien, 1995, ANN OF STAT
   Carter Kevin M., 2010, IEEE T SIGNAL PROCES
   Dhillon IS, 2003, J MACH LEARN RES
   Emery M, 1998, LECT PROB THEORY STA
   Goria Mohammed Nawaz, 2005, NONPARAMETRIC STAT
   Hero Bing Ma, 2002, IEEE SIGNAL PROCESSI, V19
   Kallberg David, 2012, ESTIMATION ENTROPY T
   Kerkyacharian Gerard, 1996, ANN OF STAT
   Krishnamurthy Akshay, 2015, ARTIFICIAL INTELLIGE
   Krishnamurthy Akshay, 2014, ICML
   Laurent Beatrice, 1996, ANN OF STAT
   Learned-Miller Erik, 2003, MACH LEARN RES
   Leibe B., 2003, CVPR
   Leonenko Nikolai, 2010, J MULTIVARIATE ANAL
   Lewi Jeremy, 2006, NIPS
   LIU H, 2012, NIPS
   Luisa Fernholz, 1983, LECT NOTES STAT
   Miller Erik G, 2003, ICASSP
   Moon Kevin, 2014, NIPS
   Nguyen X., 2010, IEEE T INFORM THEORY
   Noughabi Havva Alizadeh, 2013, J STAT COMPUTATION S
   Pal David, 2010, NIPS
   Peng Hanchuan, 2005, IEEE PAMI
   Perez-Cruz Fernando, 2008, IEEE ISIT
   Poczos Barnabas, 2011, AISTATS
   Poczos Barnabas, 2011, UAI
   Ramirez David, 2009, EUSIPCO
   Robins James, 2009, METRIKA
   Schneidman Elad, 2002, NIPS
   Singh Shashank, 2014, NIPS
   Stowell Dan, 2009, IEEE SIGNAL PROCESS
   Szabo Zoltan, 2014, J MACH LEARN RES
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Van der Vaart AW, 1998, ASYMPTOTIC STAT
   Wang Qing, 2009, IEEE T INFORM THEORY
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102060
DA 2019-06-15
ER

PT S
AU Kappel, D
   Habenschuss, S
   Legenstein, R
   Maass, W
AF Kappel, David
   Habenschuss, Stefan
   Legenstein, Robert
   Maass, Wolfgang
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and
   Rewiring
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NEURONS; EMERGENCE
AB We reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks. We propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters. This view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience. In simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances. Furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks.
C1 [Kappel, David; Habenschuss, Stefan; Legenstein, Robert; Maass, Wolfgang] Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Kappel, D (reprint author), Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
EM kappel@igi.tugraz.at; habenschuss@igi.tugraz.at; legi@igi.tugraz.at;
   maass@igi.tugraz.at
FU European Union [604102]; CHIST-ERA ERA-Net [I753-N23]
FX Written under partial support of the European Union project #604102 The
   Human Brain Project (HBP) and CHIST-ERA ERA-Net (Project FWF #I753-N23,
   PNEUMA).
CR Bi GQ, 1998, J NEUROSCI, V18, P10464
   Bill J, 2014, FRONTIERS NEUROSCIEN, V8
   Bishop C. M., 2006, PATTERN RECOGNITION
   Brea J., 2011, ADV NEURAL INFORM PR, P1422
   Carandini M, 2012, NAT NEUROSCI, V15, P507, DOI 10.1038/nn.3043
   Gardiner C.W, 2004, HDB STOCHASTIC METHO
   Gerstner W., 2002, SPIKING NEURON MODEL
   Habenschuss S, 2012, ADV NEURAL INFORM PR, P782
   Habenschuss S, 2013, NEURAL COMPUT, V25, P1371, DOI 10.1162/NECO_a_00446
   Hanchen Xiong, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P419, DOI 10.1007/978-3-319-11179-7_53
   Hatfield G, 2002, PERCEPTION PHYS WORL, P115
   Holtmaat AJGD, 2005, NEURON, V45, P279, DOI 10.1016/j.neuron.2005.01.003
   Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5
   Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Kennedy AD, 1999, PARALLEL COMPUT, V25, P1311, DOI 10.1016/S0167-8191(99)00053-8
   Loewenstein Y, 2011, J NEUROSCI, V31, P9481, DOI 10.1523/JNEUROSCI.6130-10.2011
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415
   Marder E, 2011, P NATL ACAD SCI USA, V108, P15542, DOI 10.1073/pnas.1010674108
   Mensi S., 2011, ADV NEURAL INF PROCE, P1377
   Montgomery JM, 2001, NEURON, V29, P691, DOI 10.1016/S0896-6273(01)00244-6
   Nessler B., 2009, ADV NEURAL INFORM PR, V22, P1357
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Sato I., 2014, P 31 INT C MACH LEAR, V32, P982
   Schemmel J, 2006, IEEE IJCNN, P1
   Sjostrom PJ, 2001, NEURON, V32, P1149, DOI 10.1016/S0896-6273(01)00542-6
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Winkler I, 2012, PHILOS T R SOC B, V367, P1001, DOI 10.1098/rstb.2011.0359
NR 29
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102100
DA 2019-06-15
ER

PT S
AU Kawale, J
   Bui, H
   Kveton, B
   Thanh, LT
   Chawla, S
AF Kawale, Jaya
   Bui, Hung
   Kveton, Branislav
   Thanh, Long Tran
   Chawla, Sanjay
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Thompson Sampling for Online Matrix-Factorization
   Recommendation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DISTRIBUTIONS
AB Matrix factorization (MF) collaborative filtering is an effective and widely used method in recommendation systems. However, the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative filtering from cold-start, has not been previously addressed. In this paper, we present a novel algorithm for online MF recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efficient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle filter. Extensive experiments in collaborative filtering using several real-world datasets demonstrate that PTS significantly outperforms the current state-of-the-arts.
C1 [Kawale, Jaya; Bui, Hung; Kveton, Branislav] Adobe Res, San Jose, CA 95120 USA.
   [Thanh, Long Tran] Univ Southampton, Southampton, Hants, England.
   [Chawla, Sanjay] Qatar Comp Res Inst, Doha, Qatar.
   [Chawla, Sanjay] Univ Sydney, Sydney, NSW, Australia.
RP Kawale, J (reprint author), Adobe Res, San Jose, CA 95120 USA.
EM kawale@adobe.com; hubui@adobe.com; kveton@adobe.com;
   ltt08r@ecs.soton.ac.uk; sanjay.chawla@sydney.edu.au
CR Agrawal S., 2013, P 30 INT C MACH LEAR, P127
   Arnold BC, 2000, STAT PROBABIL LETT, V49, P355, DOI 10.1016/S0167-7152(00)00068-7
   Arnold BC, 2001, STAT SCI, V16, P249
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Chopin N, 2002, BIOMETRIKA, V89, P539, DOI 10.1093/biomet/89.3.539
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Doucet Arnaud, 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.
   GELMAN A, 1991, AM STAT, V45, P125, DOI 10.2307/2684374
   Gentile Claudio, 2014, ARXIV14018257
   Gopalan A., 2014, P 31 INT C MACH LEAR, P100
   Kocak Tomas, 2014, P 28 AAAI C ART INT
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Maillard  O.-A., 2014, P 31 INT C MACH LEAR
   Mnih A., 2007, NIPS, V1, P2
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Valko Michal, 2014, 31 INT C MACH LEARN
   Zhao X., 2013, P 22 ACM INT C C INF, P1411, DOI DOI 10.1145/2505515.2505690
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103023
DA 2019-06-15
ER

PT S
AU Khalvati, K
   Rao, RPN
AF Khalvati, Koosha
   Rao, Rajesh P. N.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Bayesian Framework for Modeling Confidence in Perceptual Decision
   Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID OBSERVABLE MARKOV-PROCESSES; PROBABILITY
AB The degree of confidence in one's choice or decision is a critical aspect of perceptual decision making. Attempts to quantify a decision maker's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal. In this paper, we introduce a Bayesian framework to model confidence in perceptual decision making. We show that this model, based on partially observable Markov decision processes (POMDPs), is able to predict confidence of a decision maker based only on the data available to the experimenter. We test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task. In both experiments, we show that our model's predictions closely match experimental data. Additionally, our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making.
C1 [Khalvati, Koosha; Rao, Rajesh P. N.] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
RP Khalvati, K (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
EM koosha@cs.washington.edu; rao@cs.washington.edu
FU NSF [EEC-1028725, 1318733]; ONR [N000141310817]
FX This research was supported by NSF grants EEC-1028725 and 1318733, and
   ONR grant N000141310817.
CR ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X
   Drugowitsch J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096511
   Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012
   Hanks TD, 2011, J NEUROSCI, V31, P6339, DOI 10.1523/JNEUROSCI.5613-10.2011
   Huang Y, 2012, ADV NEURAL INFORM PR, V25, P1277
   Huang YP, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068314
   Juslin P, 1997, J BEHAV DECIS MAKING, V10, P189, DOI 10.1002/(SICI)1099-0771(199709)10:3<189::AID-BDM258>3.0.CO;2-4
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   Kepecs A, 2008, NATURE, V455, P227, DOI 10.1038/nature07200
   Kepecs A, 2012, PHILOS T R SOC B, V367, P1322, DOI 10.1098/rstb.2012.0037
   Khalvati K., 2013, P 27 AAAI C ART INT, P187
   Kiani R, 2014, NEURON, V84, P1329, DOI 10.1016/j.neuron.2014.12.015
   Kiani R, 2009, SCIENCE, V324, P759, DOI 10.1126/science.1169405
   Kurniawati Hanna, 2008, P ROB SCI SYST 4
   Persaud N, 2007, NAT NEUROSCI, V10, P257, DOI 10.1038/nn1840
   Rao RPN, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00146
   Ross Stphane, 2008, J ARTIFICIAL INTELLI, V32
   Shadlen MN, 1996, P NATL ACAD SCI USA, V93, P628, DOI 10.1073/pnas.93.2.628
   SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071
   SONDIK EJ, 1978, OPER RES, V26, P282, DOI 10.1287/opre.26.2.282
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100027
DA 2019-06-15
ER

PT S
AU Khan, ME
   Baque, P
   Fleuret, F
   Fua, P
AF Khan, Mohammad Emtiyaz
   Baque, Pierre
   Fleuret, Francois
   Fua, Pascal
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Kullback-Leibler Proximal Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a new variational inference method based on a proximal framework that uses the Kullback-Leibler (KL) divergence as the proximal term. We make two contributions towards exploiting the geometry and structure of the variational bound. First, we propose a KL proximal-point algorithm and show its equivalence to variational inference with natural gradients (e.g., stochastic variational inference). Second, we use the proximal framework to derive efficient variational algorithms for non-conjugate models. We propose a splitting procedure to separate non-conjugate terms from conjugate ones. We linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution. Overall, our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models. We show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms. Applications to real-world datasets show comparable performances to existing methods.
C1 [Khan, Mohammad Emtiyaz; Baque, Pierre; Fua, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland.
RP Khan, ME (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM emtiyaz@gmail.com; pierre.baque@epfl.ch; francois.fleuret@idiap.ch;
   pascal.fua@epfl.ch
FU Swiss National Science Foundation [CRSII2-147693]
FX Mohammad Emtiyaz Khan would like to thank Masashi Sugiyama and Akiko
   Takeda from University of Tokyo, Matthias Grossglauser and Vincent Etter
   from EPFL, and Hannes Nickisch from Philips Research (Hamburg) for
   useful discussions and feedback. Pierre Baque was supported in part by
   the Swiss National Science Foundation, under the grant CRSII2-147693
   "Tracking in the Wild".
CR Babagholami-Mohamadabadi Behnam, 2015, ARXIV150700824
   Challis E., 2011, INT C ART INT STAT
   Chretien S, 2000, IEEE T INFORM THEORY, V46, P1800, DOI 10.1109/18.857792
   Dai Bo, 2015, COMPUTING RES REPOSI
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Honkela A., 2011, J MACHINE LEARNING R, V11, P3235
   Honkela Antti, 2004, ADV NEURAL INFORM PR, P593
   Khan Mohammad Emtiyaz, 2015, ARXIV151100146
   Khan Mohammad Emtiyaz, 2014, ADV NEURAL INFORM PR
   Lappalainen H, 2000, PERSP NEURAL COMP, P93
   Marlin B., 2011, INT C MACH LEARN
   Paquet Ulrich, 2014, NIPS WORKSH VAR INF
   Pascanu R., 2013, ARXIV13013584
   Polson Nicholas G, 2015, ARXIV150203175
   Ranganath R., 2013, ARXIV14010118
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ravikumar Pradeep, 2008, INT C MACH LEARN
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   Sato M, 2001, NEURAL COMPUT, V13, P1649, DOI 10.1162/089976601750265045
   Seeger MW, 2011, SIAM J IMAGING SCI, V4, P166, DOI 10.1137/090758775
   Teboulle M, 1997, SIAM J OPTIMIZ, V7, P1069, DOI 10.1137/S1052623495292130
   Theis Lucas, 2015, INT C MACH LEARN
   Titsias M. K., 2014, INT C MACH LEARN
   Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073
   Wang C, 2013, J MACH LEARN RES, V14, P1005
   Wang Huahua, 2014, ADV NEURAL INFORM PR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102044
DA 2019-06-15
ER

PT S
AU Kingma, DP
   Salimans, T
   Welling, M
AF Kingma, Diederik P.
   Salimans, Tim
   Welling, Max
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Variational Dropout and the Local Reparameterization Trick
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.
C1 [Kingma, Diederik P.; Welling, Max] Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.
   [Salimans, Tim] Algoritmica, Amsterdam, Netherlands.
   [Welling, Max] Univ Calif Irvine, Irvine, CA USA.
   [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada.
RP Kingma, DP (reprint author), Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.
EM D.P.Kingma@uva.nl; salimans.tim@gmail.com; M.Welling@uva.nl
FU Google European Fellowship in Deep Learning; NWO project in Natural AI
   [NAI.14.108]; Google; Facebook
FX We thank the reviewers and Yarin Gal for valuable feedback. Diederik
   Kingma is supported by the Google European Fellowship in Deep Learning,
   Max Welling is supported by research grants from Google and Facebook,
   and the NWO project in Natural AI (NAI.14.108).
CR Ahn Sungjin, 2012, ARXIV12066380
   Ba  J., 2013, ADV NEURAL INFORM PR, P3084
   Bayer J., 2015, ARXIV150705331
   Bengio Y., 2013, ARXIV13052982
   Bergstra J., 2010, P PYTH SCI COMP C SC, V4
   Blundell C, 2015, ARXIV150505424
   Gal Y, 2015, ARXIV150602142
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Hernandez-Lobato J. M., 2015, ARXIV150205336
   Hinton G. E, 2012, ARXIV12070580
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Kingma D., 2015, P INT C LEARN REPR 2
   Kingma D. P., 2013, ARXIV13060733
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Maeda S., 2014, ARXIV14127003
   Neal R M, 1995, THESIS
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang S., 2013, P 30 INT C MACH LEAR, p[118, 126]
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100034
DA 2019-06-15
ER

PT S
AU Kirillov, A
   Schlesinger, D
   Vetrov, D
   Rother, C
   Savchynskyy, B
AF Kirillov, Alexander
   Schlesinger, Dmitrij
   Vetrov, Dmitry
   Rother, Carsten
   Savchynskyy, Bogdan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI M-Best-Diverse Labelings for Submodular Energies and Beyond
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MINIMIZATION
AB We consider the problem of finding M best diverse solutions of energy minimization problems for graphical models. Contrary to the sequential method of Batra et al., which greedily finds one solution after another, we infer all M solutions jointly. It was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones. The only obstacle for using this new technique is the complexity of the corresponding inference problem, since it is considerably slower algorithm than the method of Batra et al. In this work we show that the joint inference of M best diverse solutions can be formulated as a submodular energy minimization if the original MAP-inference problem is submodular, hence fast inference techniques can be used. In addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case.
C1 [Kirillov, Alexander; Schlesinger, Dmitrij; Rother, Carsten; Savchynskyy, Bogdan] Tech Univ Dresden, Dresden, Germany.
   [Vetrov, Dmitry] Skoltech, Moscow, Russia.
RP Kirillov, A (reprint author), Tech Univ Dresden, Dresden, Germany.
EM alexander.kirillov@tu-dresden.de
FU European Research Council (ERC) under the European Unions Horizon 2020
   research and innovation programme [647769]; RFBR [15-31-20596];
   Microsoft [RPD 1053945]
FX This project has received funding from the European Research Council
   (ERC) under the European Unions Horizon 2020 research and innovation
   programme (grant agreement No 647769). D. Vetrov was supported by RFBR
   proj. (No. 15-31-20596) and by Microsoft (RPD 1053945).
CR Arora C., 2015, TPAMI
   Batra D., 2012, ECCV
   Batra  Dhruv, 2012, ARXIV12104841
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y., 2001, ICCV
   Chen C., 2013, AISTATS
   Elidan G., PROBABILISTIC INFERE
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Fix A., 2011, ICCV
   Franc V, 2008, J MACH LEARN RES, V9, P67
   Fromer M., 2009, NIPS, V22
   Guzman-Rivera A., 2014, AISTATS
   Guzman-Rivera A., 2013, AISTATS
   Guzman-Rivera A., 2012, NIPS, V25
   Ishikawa H., 2003, TPAMI
   Jegelka S., 2011, CVPR
   Kappes J., 2015, INT J COMPUT VISION, V115, P1, DOI DOI 10.1007/S11263-015-0809-X
   Kirillov A., 2015, ICCV
   Kolmogorov V., 2012, DISCRETE APPL MATH
   Kolmogorov V., 2004, TPAMI
   Kulesza A., 2010, NIPS, V23
   Lawler E. L., 1972, MANAGEMENT SCI, V18
   Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483
   Prasad A., 2014, NIPS, V27
   Premachandran V., 2014, CVPR
   Ramakrishna V., 2012, NIPS WORKSH PERT OPT
   Schlesinger D., 2006, TRANSFORMING ARBITRA
   Schlesinger M. I., 2002, COMP IMAG VIS, V24
   Tarlow D., 2010, AISTATS
   Werner T., 2007, TPAMI, V29
   Yadollahpour P., 2013, CVPR
   Yanover C., 2004, NIPS, V17
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103015
DA 2019-06-15
ER

PT S
AU Kiros, R
   Zhu, YK
   Salakhutdinov, R
   Zemel, RS
   Torralba, A
   Urtasun, R
   Fidler, S
AF Kiros, Ryan
   Zhu, Yukun
   Salakhutdinov, Ruslan
   Zemel, Richard S.
   Torralba, Antonio
   Urtasun, Raquel
   Fidler, Sanja
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Skip-Thought Vectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.
C1 [Kiros, Ryan; Zhu, Yukun; Salakhutdinov, Ruslan; Zemel, Richard S.; Urtasun, Raquel; Fidler, Sanja] Univ Toronto, Toronto, ON, Canada.
   [Salakhutdinov, Ruslan; Zemel, Richard S.] Canadian Inst Adv Res, Toronto, ON, Canada.
   [Torralba, Antonio] MIT, Cambridge, MA 02139 USA.
RP Kiros, R (reprint author), Univ Toronto, Toronto, ON, Canada.
FU NSERC; Samsung; CIFAR; Google; ONR [N00014-14-1-0232]
FX We thank Geoffrey Hinton for suggesting the name skip-thoughts. We also
   thank Felix Hill, Kelvin Xu, Kyunghyun Cho and Ilya Sutskever for
   valuable comments and discussion. This work was supported by NSERC,
   Samsung, CIFAR, Google and ONR Grant N00014-14-1-0232.
CR Bandanau D., 2015, ICLR
   Bjerva J., 2014, SEMEVAL 2014, P642
   Cho  K., 2014, EMNLP
   Cho Kyunghyun, 2014, SSST8
   Chung J., 2014, NIPS DEEP LEARN WORK
   Das Dipanjan, 2009, ACL
   DOLAN B., 2004, P 20 INT C COMP LING
   Finch Andrew, 2005, IWP
   Hinton G., 2008, JMLR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ji  Y., 2013, P 2013 C EMP METH NA, P891
   Jimenez Sergio, 2014, SEMEVAL 2014
   Kalchbrenner N., 2013, P 2013 C EMP METH NA, P1700
   Kalchbrenner N., 2014, ACL
   Karpathy A., 2015, CVPR
   Kingma D. P., 2015, ICLR
   Klein Benjamin, 2015, CVPR
   Lai Alice, 2014, SEMEVAL 2014
   Le Quoc V, 2014, ICML
   Madnani Nitin, 2012, NAACL
   Mao J, 2015, ICLR
   Marelli Marco, 2014, SEMEVAL 2014
   Mikolov T., 2013, ICLR
   MIKOLOV Tomas, 2013, ARXIV13094168
   Saxe  A., 2014, ICLR
   Simonyan Karen, 2015, ICLR
   Socher R., 2013, EMNLP
   Socher R., 2014, TACL
   Socher Richard, 2011, NIPS
   Sutskever  I., 2014, NIPS
   Tai Kai Sheng, 2015, ACL
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Wan Stephen, 2006, P AUSTR LANG TECHN W
   Wang Sida, 2012, ACL
   Yoon Kim, 2014, EMNLP
   Zhao Han, 2015, IJCAI
   Zhao Jiang, 2014, SEMEVAL 2014
   Zhu Y., 2015, ICCV
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102098
DA 2019-06-15
ER

PT S
AU Kishimoto, A
   Marinescu, R
   Botea, A
AF Kishimoto, Akihiro
   Marinescu, Radu
   Botea, Adi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Parallel Recursive Best-First AND/OR Search for Exact MAP Inference in
   Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The paper presents and evaluates the power of parallel search for exact MAP inference in graphical models. We introduce a new parallel shared-memory recursive best-first AND/OR search algorithm, called SPRBFAOO, that explores the search space in a best-first manner while operating with restricted memory. Our experiments show that SPRBFAOO is often superior to the current state-of-the-art sequential AND/OR search approaches, leading to considerable speed-ups (up to 7-fold with 12 threads), especially on hard problem instances.
C1 [Kishimoto, Akihiro; Marinescu, Radu; Botea, Adi] IBM Res, Dublin, Ireland.
RP Kishimoto, A (reprint author), IBM Res, Dublin, Ireland.
EM akihirok@ie.ibm.com; radu.marinescu@ie.ibm.com; adibotea@ie.ibm.com
CR ALLIS LV, 1994, ARTIF INTELL, V66, P91, DOI 10.1016/0004-3702(94)90004-3
   Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1
   Chrabakh W., 2003, TECHNICAL REPORT
   Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003
   Enzenberger M, 2010, IEEE T COMP INTEL AI, V2, P259, DOI 10.1109/TCIAIG.2010.2083662
   Fishelson M., 2002, BIOINFORMATICS, P189
   Grama A, 1999, IEEE T KNOWL DATA EN, V11, P28, DOI 10.1109/69.755612
   Hoki K, 2013, ICGA J, V36, P22, DOI 10.3233/ICG-2013-36103
   Kaneko T, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P95
   Kishimoto A, 2012, ICGA J, V35, P131, DOI 10.3233/ICG-2012-35302
   Kishimoto A, 2013, ARTIF INTELL, V195, P222, DOI 10.1016/j.artint.2012.10.007
   KORF RE, 1993, ARTIF INTELL, V62, P41, DOI 10.1016/0004-3702(93)90045-D
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Marinescu R., 2014, UAI, P400
   Marinescu R, 2009, ARTIF INTELL, V173, P1457, DOI 10.1016/j.artint.2009.07.003
   Marinescu R, 2009, ARTIF INTELL, V173, P1492, DOI 10.1016/j.artint.2009.07.004
   Nagai A., 2002, THESIS
   Otten L., 2012, C UNC ART INT, P665
   Pearl J, 1988, PROBABILISTIC REASON
   Pennock D. M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P431
   Saito JT, 2010, LECT NOTES COMPUT SC, V6048, P75
   Xia Y., 2008, IEEE INT S PAR DISTR
   Yanover C, 2008, J COMPUT BIOL, V15, P899, DOI 10.1089/cmb.2007.0158
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103018
DA 2019-06-15
ER

PT S
AU Kobilarov, M
AF Kobilarov, Marin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sample Complexity Bounds for Iterative Stochastic Policy Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.
C1 [Kobilarov, Marin] Johns Hopkins Univ, Dept Mech Engn, Baltimore, MD 21218 USA.
RP Kobilarov, M (reprint author), Johns Hopkins Univ, Dept Mech Engn, Baltimore, MD 21218 USA.
EM marin@jhu.edu
CR Bemporad A, 1999, LECT NOTES CONTR INF, V245, P207
   Boucheron S., 2013, CONCENTRATION INEQUA
   Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454
   Choset H.M., 2005, PRINCIPLES ROBOT MOT
   Cortes Corinna, 2010, ADV NEURAL INFORM PR, V23
   Deisenroth M. P., 2013, SURVEY POLICY SEARCH, P388
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Igel C, 2007, EVOL COMPUT, V15, P1, DOI 10.1162/evco.2007.15.1.1
   Kobilarov M, 2013, P AMER CONTR CONF, P1044
   Kobilarov M, 2012, INT J ROBOT RES, V31, P855, DOI 10.1177/0278364912444543
   Koltchinskii V, 2001, APPL MATH COMPUT, V120, P31, DOI 10.1016/S0096-3003(99)00283-0
   Langford J, 2005, J MACH LEARN RES, V6, P273
   Larraaga Pedro, 2002, ESTIMATION DISTRIBUT
   Levine Sergey, 2014, NEURAL INFORM PROCES
   Mahony R, 2004, INT J ROBUST NONLIN, V14, P1035, DOI 10.1002/rnc.931
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   Pelikan M, 2002, COMPUT OPTIM APPL, V21, P5, DOI 10.1023/A:1013500812258
   RAY LR, 1993, AUTOMATICA, V29, P229, DOI 10.1016/0005-1098(93)90187-X
   Rubinstein R., 2004, CROSS ENTROPY METHOD
   Schaal S, 2010, IEEE ROBOT AUTOM MAG, V17, P20, DOI 10.1109/MRA.2010.936957
   SUTTON R.S., 1999, NIPS, V99, P1057
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Tempo R., 2004, RANDOMIZED ALGORITHM
   Theodorou EA, 2010, J MACH LEARN RES, V11, P3137
   Vapnik VN, 1995, NATURE STAT LEARNING
   Vidyasagar M, 2008, J PROCESS CONTR, V18, P421, DOI 10.1016/j.jprocont.2007.10.009
   Vidyasagar M, 2001, AUTOMATICA, V37, P1515, DOI 10.1016/S0005-1098(01)00122-4
   Wang Q, 2006, PROBABILISTIC AND RANDOMIZED METHODS FOR DESIGN UNDER UNCERTAINTY, P381, DOI 10.1007/1-84628-095-8_15
   Zhigljavsky A., 2008, STOCHASTIC GLOBAL OP
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100061
DA 2019-06-15
ER

PT S
AU Komiyama, J
   Honda, J
   Nakagawa, H
AF Komiyama, Junpei
   Honda, Junya
   Nakagawa, Hiroshi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial
   Monitoring
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.
C1 [Komiyama, Junpei; Honda, Junya; Nakagawa, Hiroshi] Univ Tokyo, Tokyo, Japan.
RP Komiyama, J (reprint author), Univ Tokyo, Tokyo, Japan.
EM junpei@komiyama.info; honda@stat.t.u-tokyo.ac.jp;
   nakagawa@dl.itc.u-tokyo.ac.jp
FU JSPS KAKENHI [15J09850, 26106506]
FX The authors gratefully acknowledge the advice of Kentaro Minami and
   sincerely thank the anonymous reviewers for their useful comments. This
   work was supported in part by JSPS KAKENHI Grant Number 15J09850 and
   26106506.
CR Agarwal A., 2010, P 13 INT C ART INT S, P9
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bartok G., 2013, COLT, V30, P696
   Bartok G., 2011, COLT 2011 JMLR, P133
   Bartok Gabor, 2012, ICML
   Cesa-Bianchi N, 2005, IEEE T INFORM THEORY, V51, P2152, DOI 10.1109/TIT.2005.847729
   Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206
   Dani V., 2008, COLT, P355
   Dembo  A., 1998, APPL MATH
   Fiacco A. V., 1983, INTRO SENSITIVITY ST
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Honda J., 2010, P COLT 2010 HAIF ISR, P67
   Ito S, 2000, ANN OPER RES, V98, P189, DOI 10.1023/A:1019208524259
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   PICCOLBONI A, 2001, COMPUTATIONAL LEARNI, V2111, P208
   Vanchinathan H. P., 2014, NIPS, P1691
   Wachter Andreas, INTERIOR POINT OPTIM
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103062
DA 2019-06-15
ER

PT S
AU Koolen, WM
   Malek, A
   Bartlett, PL
   Abbasi-Yadkori, Y
AF Koolen, Wouter M.
   Malek, Alan
   Bartlett, Peter L.
   Abbasi-Yadkori, Yasin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Minimax Time Series Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID TRACKING
AB We consider an adversarial formulation of the problem of predicting a time series with square loss. The aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect. Our approach allows natural measures of smoothness such as the squared norm of increments. More generally, we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms. We derive the minimax strategy for all problems of this type and show that it can be implemented efficiently. The optimal predictions are linear in the previous observations. We obtain an explicit expression for the regret in terms of the parameters defining the problem. For typical, simple definitions of smoothness, the computation of the optimal predictions involves only sparse matrices. In the case of norm-constrained data, where the smoothness is defined in terms of the squared norm of the comparator's increments, we show that the regret grows as T/root lambda(T), where T is the length of the game and lambda(T) is an increasing limit on comparator smoothness.
C1 [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Malek, Alan; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA USA.
   [Bartlett, Peter L.; Abbasi-Yadkori, Yasin] QUT, Brisbane, Qld, Australia.
RP Koolen, WM (reprint author), Ctr Wiskunde & Informat, Amsterdam, Netherlands.
EM wmkoolen@cwi.nl; malek@berkeley.edu; bartlett@cs.berkeley.edu;
   yasin.abbasiyadkori@qut.edu.au
FU NSF [CCF-1115788]; Australian Research Council through an Australian
   Laureate Fellowship [FL110100281]; Australian Research Council through
   ARC Centre of Excellence for Mathematical and Statistical Frontiers
FX We gratefully acknowledge the support of the NSF through grant
   CCF-1115788, and of the Australian Research Council through an
   Australian Laureate Fellowship (FL110100281) and through the ARC Centre
   of Excellence for Mathematical and Statistical Frontiers. Thanks also to
   the Simons Institute for the Theory of Computing Spring 2015 Information
   Theory Program.
CR Abernethy J., 2008, P 21 ANN C LEARN THE, P415
   Bartlett Peter L., 2015, P 28 ANN C LEARN THE, P226
   Blum A, 2000, MACH LEARN, V39, P35, DOI 10.1023/A:1007621832648
   Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654
   Cesa-Bianchi Nicolo, 2012, ADV NEURAL INFORM PR, P980
   Chaudhuri Kamalika, 2010, P 26 C UNC ART INT U, P101
   Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Hu GY, 1996, J PHYS A-MATH GEN, V29, P1511, DOI 10.1088/0305-4470/29/7/020
   Koolen Wouter M., 2014, ADV NEURAL INFORM PR, P3230
   Monteleoni Claire, 2003, THESIS
   Moroshko Edward, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P245, DOI 10.1007/978-3-642-34106-9_21
   Moroshko Edward, 2013, J MACH LEARN RES, P451
   Takimoto E., 2000, 13 COLT, P100
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100098
DA 2019-06-15
ER

PT S
AU Kopp, T
   Singla, P
   Kautz, H
AF Kopp, Tim
   Singla, Parag
   Kautz, Henry
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Lifted Symmetry Detection and Breaking for MAP Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability. In this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes MAP inference in Markov Logic and similar statistical-relational languages. We introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory. We provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time. We show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain. We demonstrate the effectiveness of these techniques through experiments in two relational domains. We also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.
C1 [Kopp, Tim; Kautz, Henry] Univ Rochester, Rochester, NY 14627 USA.
   [Singla, Parag] IIT Delhi, New Delhi, India.
RP Kopp, T (reprint author), Univ Rochester, Rochester, NY 14627 USA.
EM tkopp@cs.rochester.edu; parags@cse.iitd.ac.in; kautz@cs.rochester.edu
CR Aloul FA, 2003, DES AUT CON, P836
   Apsel Udi, 2014, P AAAI 2014, P2403
   Audemard G, 2006, J AUTOM REASONING, V36, P177, DOI 10.1007/s10817-006-9040-3
   Berre D. L., 2010, J SATISFIABILITY BOO, V7, P59
   Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319
   Bui Hung B., 2012, P AAAI
   Bui Hung Hai, 2013, UAI, P132
   CHVATAL V, 1988, J ACM, V35, P759, DOI 10.1145/48014.48016
   Crawford J, 1996, MOR KAUF R, P148
   Domingos P.M., 2009, SYNTHESIS LECT ARTIF
   Flener P, 2009, CONSTRAINTS, V14, P506, DOI 10.1007/s10601-008-9059-7
   Gogate V., 2011, UAI, P256
   Heras F, 2008, J ARTIF INTELL RES, V31, P1
   Katebi H, 2010, LECT NOTES COMPUT SC, V6175, P113, DOI 10.1007/978-3-642-14186-7_11
   LUKS EM, 1982, J COMPUT SYST SCI, V25, P42, DOI 10.1016/0022-0000(82)90009-5
   Martins Ruben, 2014, LECT NOTES COMPUTER, V8561
   Meseguer P, 2001, ARTIF INTELL, V129, P133, DOI 10.1016/S0004-3702(01)00104-7
   Mittal H., 2014, P ADV NEUR INF PROC, P649
   Mladenov M., 2014, P UAI, P603
   Mladenov M., 2012, JMLR W CP, P788
   Niepert M., 2012, P 28 C UNC ART INT, P624
   Niepert Mathias, 2013, P AAAI
   Noessner Jan, 2013, P AAAI
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Sellmann M, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P298
   Singla P., 2008, P 23 AAAI C ART INT, P1094
   Singla P., 2014, AAAI, P2497
   Van Den Broeck G., 2011, IJCAI, V3, P2178
   Van den Broeck G., 2013, ADV NEURAL INFORM PR, P2868
   Van den Broeck Guy, 2012, P AAAI
   Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17
   Walsh Toby, 2012, P AAAI
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100059
DA 2019-06-15
ER

PT S
AU Korattikara, A
   Rathod, V
   Murphy, K
   Welling, M
AF Korattikara, Anoop
   Rathod, Vivek
   Murphy, Kevin
   Welling, Max
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bayesian Dark Knowledge
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/or where we need accurate posterior predictive densities p(yjx;D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).
   We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.
C1 [Korattikara, Anoop; Rathod, Vivek; Murphy, Kevin] Google Res, Mountain View, CA 94043 USA.
   [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands.
RP Korattikara, A (reprint author), Google Res, Mountain View, CA 94043 USA.
EM kbanoop@google.com; rathodv@google.com; kpmurphy@google.com;
   m.welling@uva.nl
CR Ahn S., 2012, ICML
   Ahn S., 2014, ICML
   Bickel J. E., 2007, DECIS ANAL, V4, P49, DOI DOI 10.1287/DECA.1070.0089
   Blundell C., 2015, ICML
   Bucila C., 2006, KDD
   Chen T., 2014, ICML
   Ding N., 2014, NIPS
   Gal Y, 2015, DROPOUT BAYESIAN APP
   Graves A., 2011, NIPS
   Hernandez-Lobato J. M., 2015, ICML
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Kingma Diederik P, 2014, ICLR
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Patterson S., 2013, NIPS
   Rezende D. J., 2014, ICML
   Romero Adriana, 2014, FITNETS HINTS THIN D, V19
   Snelson Edward, 2005, ICML
   Szegedy C., 2014, ICLR
   Welling M., 2011, ICML
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103003
DA 2019-06-15
ER

PT S
AU Koren, T
   Levy, KY
AF Koren, Tomer
   Levy, Kfir Y.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Rates for Exp-concave Empirical Risk Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ONLINE; STABILITY
AB We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses-a general optimization framework that captures several important learning problems including linear and logistic regression, learning SVMs with the squared hinge-loss, portfolio selection and more. In this setting, we establish the first evidence that ERM is able to attain fast generalization rates, and show that the expected loss of the ERM solution in d dimensions converges to the optimal expected loss in a rate of d=n. This rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.
C1 [Koren, Tomer; Levy, Kfir Y.] Technion, IL-32000 Haifa, Israel.
RP Koren, T (reprint author), Technion, IL-32000 Haifa, Israel.
EM tomerk@technion.ac.il; kfiryl@tx.technion.ac.il
CR Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Kivinen J, 1999, LECT NOTES ARTIF INT, V1572, P153
   Koren T., 2013, C LEARN THEOR, P1073
   Lecue G., 2014, ARXIV14025763
   Mahdavi M., 2015, P 28 C LEARN THEOR
   Shalev-Shwartz S., 2014, MATH PROGRAM, P1, DOI DOI 10.1007/S10107-014-0839-0
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Shamir O., 2014, ARXIV14065143
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
   Sridharan K., 2009, ADV NEURAL INFORM PR, V21, P1545
   Vovk V, 2001, INT STAT REV, V69, P213
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103072
DA 2019-06-15
ER

PT S
AU Koyejo, O
   Natarajan, N
   Ravikumar, P
   Dhillon, IS
AF Koyejo, Oluwasanmi
   Natarajan, Nagarajan
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Consistent Multilabel Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro- and macro-averages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance- conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Hamming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.
C1 [Koyejo, Oluwasanmi] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.
   [Natarajan, Nagarajan; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Koyejo, O (reprint author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.
EM sanmi@stanford.edu; naga86@cs.utexas.edu; pradeepr@cs.utexas.edu;
   inderjit@cs.utexas.edu
FU NSF [CCF-1117055, CCF-1320746, IIS-1320894]; NIH part of the Joint
   DMS/NIGMS Initiative to Support Research at the Interface of the
   Biological and Mathematical Sciences [R01 GM117594-01]
FX We acknowledge the support of NSF via CCF-1117055, CCF-1320746 and
   IIS-1320894, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS
   Initiative to Support Research at the Interface of the Biological and
   Mathematical Sciences.
CR Dembczynski K, 2013, P 30 INT C MACH LEAR, P1130
   Dembczynski K., 2012, P 29 INT C MACH LEAR, P1319
   Dembczynski K. J., 2011, ADV NEURAL INFORM PR, P1404
   Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8
   Dembczynski Krzysztof, 2010, P 27 INT C MACH LEAR, P279
   Devroye L., 1996, PROBABILISTIC THEORY, V31
   Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001
   Kapoor Ashish, 2012, ADV NEURAL INFORM PR, P2645
   Nagarajan N., 2014, ADV NEURAL INFORM PR, P2744
   Narasimhan H., 2014, ADV NEURAL INF PROC, V27, P1493
   Narasimhan Harikrishna, 2015, P 32 INT C MACH LEAR, P2398
   Petterson J., 2011, ADV NEURAL INFORM PR, V24, P1512
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Waegeman W, 2014, J MACH LEARN RES, V15, P3333
   Ye Nan, 2012, P INT C MACH LEARN
   Yu H., 2014, P 31 INT C MACH LEAR, P593
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102032
DA 2019-06-15
ER

PT S
AU Kozdoba, M
   Mannor, S
AF Kozdoba, Mark
   Mannor, Shie
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Community Detection via Measure Space Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID COMPLEX NETWORKS; ALGORITHMS
AB We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of k -means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks, and find its performance to be better or at least as good as previously known algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a p, q-stochastic block model with where p >= c . N (-1/2 + epsilon) and p - q >= c' root pN(-1/2+epsilon) logN.
C1 [Kozdoba, Mark; Mannor, Shie] The Technion, Haifa, Israel.
RP Kozdoba, M (reprint author), The Technion, Haifa, Israel.
EM markk@tx.technion.ac.il; shie@ee.technion.ac.il
CR Amini Arash A., 2013, PSEUDO LIKELIHOOD ME, V41
   Anandkumar Animashree, 2013, JMLR P, V30
   Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103
   Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008
   Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22
   Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205
   Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2
   Esquivel AV, 2011, PHYS REV X, V1, DOI 10.1103/PhysRevX.1.021025
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Fortunato Santo, 2009, 4 INT ICST C
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Glance Natalie, 2005, LINKKDD 05
   Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110
   Gregory S, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/10/103018
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jordan Michael I., 2001, ADV NEURAL INFORM PR, V14
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   Lancichinetti A, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016118
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Lancichinetti A, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/3/033015
   Newman MEJ, 2007, P NATL ACAD SCI USA, V104, P9564, DOI 10.1073/pnas.0610537104
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066133
   Pons P., 2004, J GRAPH ALGORITHMS A, V10, P284, DOI DOI 10.7155/JGAA.00124
   Ronhovde P, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016109
   Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105
   Shamir R, 2007, RANDOM STRUCT ALGOR, V31, P418, DOI 10.1002/rsa.20181
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101068
DA 2019-06-15
ER

PT S
AU Krichene, W
   Bayen, AM
   Bartlett, PL
AF Krichene, Walid
   Bayen, Alexandre M.
   Bartlett, Peter L.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Accelerated Mirror Descent in Continuous and Discrete Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SYSTEMS
AB We study accelerated mirror descent dynamics in continuous and discrete time. Combining the original continuous-time motivation of mirror descent with a recent ODE interpretation of Nesterov's accelerated method, we propose a family of continuous-time descent dynamics for convex functions with Lipschitz gradients, such that the solution trajectories converge to the optimum at a O (1/t(2)) rate. We then show that a large family of first-order accelerated methods can be obtained as a discretization of the ODE, and these methods converge at a O (1/k(2)) rate. This connection between accelerated mirror descent and the ODE provides an intuitive approach to the design and analysis of accelerated first-order algorithms.
C1 [Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Bartlett, Peter L.] QUT, Brisbane, Qld, Australia.
RP Krichene, W (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM walid@eecs.berkeley.edu; bayen@berkeley.edu; bartlett@berkeley.edu
FU NSF [CCF-1115788, CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166];
   ARC [FL110100281]; Simons Institute Fall 2014 Algorithmic Spectral Graph
   Theory Program; ARC (ACEMS)
FX We gratefully acknowledge the NSF (CCF-1115788, CNS-1238959,
   CNS-1238962, CNS-1239054, CNS-1239166), the ARC (FL110100281 and ACEMS),
   and the Simons Institute Fall 2014 Algorithmic Spectral Graph Theory
   Program.
CR Allen-Zhu Zeyuan, 2014, LINEAR COUPLING ULTI
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Ben-Tal A, 2001, SIAM J OPTIMIZ, V12, P79, DOI 10.1137/S1052623499354564
   Ben-Tal A., 2001, LECT MODERN CONVEX O
   Bloch A., 1994, HAMILTONIAN GRADIENT
   BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Butcher JC, 2008, NUMERICAL METHODS OR
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Dekel Ofer, 2011, P 28 INT C MACH LEAR
   Helmke U., 1994, COMMUNICATIONS CONTR
   Juditsky A., 2011, STOCHASTIC SYSTEMS, V1, P17, DOI DOI 10.1214/10-SSY011
   Juditsky Anatoli, 2013, LECT NOTES
   Khalil H. K., 1992, NONLINEAR SYSTEMS
   Krichene Walid, 2015, 54 IEEE C DEC CONTR
   Lyapunov A., 1992, CONTROL THEORY APPL
   Nemirovsky A. S., 1983, WILEY INTERSCIENCE S
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3
   Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639
   Rockafellar R T, 1970, CONVEX ANAL
   Schropp J, 2000, NUMER FUNC ANAL OPT, V21, P537, DOI 10.1080/01630560008816971
   Su Weijie, 2014, NIPS
   Teschl G., 2012, ORDINARY DIFFERENTIA, V140
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101102
DA 2019-06-15
ER

PT S
AU Krishnan, RG
   Lacoste-Julien, S
   Sontag, D
AF Krishnan, Rahul G.
   Lacoste-Julien, Simon
   Sontag, David
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Barrier Frank-Wolfe for Marginal Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.
C1 [Krishnan, Rahul G.; Sontag, David] NYU, Courant Inst, New York, NY 10003 USA.
   [Lacoste-Julien, Simon] Ecole Normale Super, Sierra Project Team, INRIA, Paris, France.
RP Krishnan, RG (reprint author), NYU, Courant Inst, New York, NY 10003 USA.
FU DARPA Probabilistic Programming for Advancing Machine Learning (PPAML)
   Program under AFRL prime [FA8750-14-C-0005]
FX RK and DS gratefully acknowledge the support of the DARPA Probabilistic
   Programming for Advancing Machine Learning (PPAML) Program under AFRL
   prime contract no. FA8750-14-C-0005.
CR Allouche D., 2010, TOULBAR2 OPEN SOURCE
   Andres B. T, 2012, OPENGM C PLUS PLUS L
   Belanger D., 2013, NIPS WORKSH GREED OP
   Besag J., 1986, J R STAT SOC B
   Boykov Y., 2004, TPAMI
   Domke J., 2013, TPAMI
   Ermon S., 2013, ICML
   Garber  Dan, 2013, ARXIV13014666
   Globerson Amir, 2007, UAI
   Hazan T., 2012, ICML
   I. Gurobi Optimization, 2015, GUROBI OPTIMIZER REF
   Jaggi  M., 2013, ICML
   Jancsary J., 2011, AISTATS
   Kappes Jorg Hendrik, 2013, CVPR
   Kolmogorov V., 2006, TPAMI
   Kolmogorov V., 2007, TPAMI
   Koo T., 2007, EMNLP CONLL
   Lacoste-Julien S., 2015, NIPS
   Lacoste-Julien S., 2013, ICML
   London B., 2015, ICML
   Mooij J. M., 2010, JMLR
   Nowozin S., 2011, ICCV, P2
   Papandreou G., 2011, ICCV
   Salakhutdinov R., 2008, TECHNICAL REPORT
   Shimony S. E., 1994, ARTIFICIAL INTELLIGE
   Sontag D., 2008, UAI
   Sontag D., 2007, NIPS
   Ullman S., 2002, ECCV
   Wainwright M.J., 2005, IEEE T INFORM THEORY
   Wang S., 2014, ICLR WORKSH
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102041
DA 2019-06-15
ER

PT S
AU Kuleshov, V
   Liang, P
AF Kuleshov, Volodymyr
   Liang, Percy
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Calibrated Structured Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In user-facing applications, displaying calibrated confidence measures-probabilities that correspond to true frequency-can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting, and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.
C1 [Kuleshov, Volodymyr; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Kuleshov, V (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
FU NSERC Canada Graduate Scholarship; Sloan Research Fellowship
FX This research is supported by an NSERC Canada Graduate Scholarship to
   the first author and a Sloan Research Fellowship to the second author.
CR Brier GW., 1950, MONTHLY WEATHER REVI, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
   Brocker J, 2009, Q J ROY METEOR SOC, V135, P1512, DOI 10.1002/qj.456
   Buja A, 2005, LOSS FUNCTIONS BINAR
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720
   Foster D. P., 1998, ASYMPTOTIC CALIBRATI
   Gneiting T, 2007, J ROY STAT SOC B, V69, P243, DOI 10.1111/j.1467-9868.2007.00587.x
   HECKERMAN DE, 1992, METHOD INFORM MED, V31, P106
   Jiang XQ, 2012, J AM MED INFORM ASSN, V19, P263, DOI 10.1136/amiajnl-2011-000291
   Kassel R. H., 1995, THESIS
   Krizhevsky A., 2009, TECHNICAL REPORT
   Liang P., 2006, INT C COMP LING ASS
   Lichtenstein S, 1982, JUDGEMENT UNCERTAINT
   Menon Aditya Krishna, 2012, INT C MACH LEARN ICM
   Mueller A., 2013, THESIS
   Murphy A. H., 1973, Journal of Applied Meteorology, V12, P595, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2
   Nguyen Khanh, 2015, EMPIRICAL METHODS NA, P1587
   Niculescu-Mizil A., 2005, P 22 INT C MACH LEAR, P625, DOI DOI 10.1145/1102351.1102430
   Platt J, 1999, ADV LARGE MARGIN CLA, V10, P61
   Seigel M., 2013, THESIS
   Stephenson DB, 2008, WEATHER FORECAST, V23, P752, DOI 10.1175/2007WAF2006116.1
   Yu D, 2011, IEEE T AUDIO SPEECH, V19, P2461, DOI 10.1109/TASL.2011.2141988
   Zadrozny B, 2002, P 8 ACM SIGKDD INT C, P694, DOI DOI 10.1007/S10994-013-5343-X
   Zhong W, 2013, P 23 INT JOINT C ART, P1939
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100026
DA 2019-06-15
ER

PT S
AU Kundu, A
   Drineas, P
   Magdon-Ismail, M
AF Kundu, Abhisek
   Drineas, Petros
   Magdon-Ismail, Malik
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Approximating Sparse PCA from Incomplete Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PRINCIPAL COMPONENT ANALYSIS
AB We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, O (e(-2)k max {m, n) elements gives an epsilon-additive approximation to the sparse PCA problem ((k) over tilde is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.
C1 [Kundu, Abhisek; Drineas, Petros; Magdon-Ismail, Malik] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.
RP Kundu, A (reprint author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.
EM kundua2@rpi.edu; drinep@cs.rpi.edu; magdon@cs.rpi.edu
FU NSF [IIS-1447283, IIS-1319280]
FX AK and PD are partially supported by NSF IIS-1447283 and IIS-1319280.
CR Asteris M., 2014, P ICML
   CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614
   Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Gabrilovich E., 2004, P INT C MACH LEARN
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148
   Kundu A., 2015, RECOVERING PCA HYBRI
   Lei J, 2015, ANN STAT, V43, P299, DOI 10.1214/14-AOS1273
   Lounici Karim, 2012, SPARSE PRINCIPAL COM
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   Magdon-Ismail M., 2015, NP HARDNESS INAPPROX
   Magdon-Ismail M., 2015, ARXIV REPORT
   Moghaddam B., 2006, P ICML
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007
   Sjstrand K., 2012, J STAT SOFTWARE
   Wang Z., 2014, NONCONVEX STAT OPTIM
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102054
DA 2019-06-15
ER

PT S
AU Kuznetsov, V
   Mohri, M
AF Kuznetsov, Vitaly
   Mohri, Mehryar
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Theory and Algorithms for Forecasting Non-Stationary Time
   Series
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONVERGENCE; PREDICTION
AB We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.
C1 [Kuznetsov, Vitaly; Mohri, Mehryar] Courant Inst, New York, NY 10011 USA.
   [Mohri, Mehryar] Google Res, New York, NY 10011 USA.
RP Kuznetsov, V (reprint author), Courant Inst, New York, NY 10011 USA.
EM vitaly@cims.nyu.edu; mohri@cims.nyu.edu
FU NSF [IIS-1117591, CCF-1535987]; NSERC PGS D3
FX This work was partly funded by NSF IIS-1117591 and CCF-1535987, and the
   NSERC PGS D3.
CR Adams TM, 2010, ANN PROBAB, V38, P1345, DOI 10.1214/09-AOP511
   Agarwal A, 2013, IEEE T INFORM THEORY, V59, P573, DOI 10.1109/TIT.2012.2212414
   Alquier P., 2014, DEPENDENCE MODELLING, V1, P65
   Alquier P., 2010, 201039 CTR RECH EC S
   Andrews  D., 1983, COWLES FDN DISCUSSIO, V664
   Baillie RT, 1996, J ECONOMETRICS, V73, P5, DOI 10.1016/0304-4076(95)01732-1
   Barve Rakesh D., 1996, COLT
   Berti P, 1997, STAT PROBABIL LETT, V32, P385, DOI 10.1016/S0167-7152(96)00098-3
   Bollerslev Tim, 1986, J ECONOMETRICS
   Box G. E. P., 1990, TIME SERIES ANAL FOR
   Brockwell P. J., 1986, TIME SERIES THEORY M
   de la Pena V. H., 1999, PROBABILITY ITS APPL
   Doukhan P., 1994, LECT NOTES STAT
   ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773
   Hamilton J, 1994, TIME SERIES ANAL
   Kuznetsov Vitaly, 2014, ALT
   Lozano A.C., 2006, ADV NEURAL INFORM PR, V18, P819
   Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810
   Modha DS, 1998, IEEE T INFORM THEORY, V44, P117, DOI 10.1109/18.650998
   Mohri M., 2012, ALT
   Mohri M, 2010, J MACH LEARN RES, V11, P789
   Pestov V., 2010, GRC
   Rakhlin Alexander, 2011, NIPS
   Rakhlin Alexander, 2015, PROBABILITY THEORY R
   Rakhlin Alexander, 2010, NIPS
   Shalizi C., 2013, NIPS
   Steinwart I., 2009, NIPS
   Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313
   Vidyasagar M., 1997, THEORY LEARNING GEN
   YU B, 1994, ANN PROBAB, V22, P94, DOI 10.1214/aop/1176988849
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101095
DA 2019-06-15
ER

PT S
AU Kveton, B
   Wen, Z
   Ashkan, A
   Szepesvari, C
AF Kveton, Branislav
   Wen, Zheng
   Ashkan, Azin
   Szepesvari, Csaba
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Combinatorial Cascading Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.
C1 [Kveton, Branislav] Adobe Res, San Jose, CA 95110 USA.
   [Wen, Zheng] Yahoo Labs, Sunnyvale, CA USA.
   [Ashkan, Azin] Technicolor Res, Los Altos, CA USA.
   [Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
RP Kveton, B (reprint author), Adobe Res, San Jose, CA 95110 USA.
EM kveton@adobe.com; zhengwen@yahoo-inc.com; azin.ashkan@technicolor.com;
   szepesva@cs.ualberta.ca
CR AGRAWAL R, 1989, IEEE T AUTOMAT CONTR, V34, P258, DOI 10.1109/9.16415
   Agrawal S., 2012, P 25 ANN C LEARN THE
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bartok Gabor, 2012, P 29 INT C MACH LEAR
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Choi Baek-Young, 2004, P 23 ANN JOINT C IEE
   Combes Richard, 2015, P 2015 ACM SIGMET RI
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Kveton B., 2015, P 18 INT C ART INT S
   Kveton B., 2015, P 32 INT C MACH LEAR
   Kveton B., 2014, P 30 C UNC ART INT J, P420
   Lam Shyong, 2015, MOVIELENS DATASET
   Lin  Tian, 2014, P 31 INT C MACH LEAR
   Papadimitriou C. H., 1998, COMBINATORIAL OPTIMI
   Spring N, 2004, IEEE ACM T NETWORK, V12, P2, DOI 10.1109/TNET.2003.822655
   Le T, 2014, IEEE T SIGNAL PROCES, V62, P5919, DOI 10.1109/TSP.2014.2357779
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Wen Zheng, 2015, P 32 INT C MACH LEAR
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100090
DA 2019-06-15
ER

PT S
AU Kwitt, R
   Huber, S
   Niethammer, M
   Lin, WL
   Bauer, U
AF Kwitt, Roland
   Huber, Stefan
   Niethammer, Marc
   Lin, Weili
   Bauer, Ulrich
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Statistical Topological Data Analysis - A Kernel Perspective
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the problem of statistical computations with persistence diagrams, a summary representation of topological features in data. These diagrams encode persistent homology, a widely used invariant in topological data analysis. While several avenues towards a statistical treatment of the diagrams have been explored recently, we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel Hilbert spaces. In fact, a positive definite kernel on persistence diagrams has recently been proposed, connecting persistent homology to popular kernel-based learning techniques such as support vector machines. However, important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored. Our contribution is to close this gap by proving universality of a variant of the original kernel, and to demonstrate its effective use in two-sample hypothesis testing on synthetic as well as real-world data.
C1 [Kwitt, Roland] Univ Salzburg, Dept Comp Sci, Salzburg, Austria.
   [Huber, Stefan] IST Austria, Klosterneuburg, Austria.
   [Niethammer, Marc] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
   [Niethammer, Marc; Lin, Weili] Univ N Carolina, BRIC, Chapel Hill, NC 27515 USA.
   [Lin, Weili] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27515 USA.
   [Bauer, Ulrich] Tech Univ Munich, Dept Math, Munich, Germany.
RP Kwitt, R (reprint author), Univ Salzburg, Dept Comp Sci, Salzburg, Austria.
EM rkwitt@gmx.at; stefan.huber@ist.ac.at; mn@cs.unc.edu;
   weili_lin@med.unc.edu; ulrich@bauer.org
FU Austrian Science Fund [KLI 00012]
FX This work has been partially supported by the Austrian Science Fund,
   project no. KLI 00012. We also thank the anonymous reviewers for their
   valuable comments/suggestions.
CR Adcock A., 2013, RING ALGEBRAIC FUNCT
   Bendich P., 2014, PERSISTENT HOMOLOGY
   Bompard L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108306
   Bubenik P, 2015, J MACH LEARN RES, V16, P77
   Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X
   Chazal F., 2014, SOCG
   Christmann A., 2010, NIPS
   Chung M.K., 2009, IPMI
   Dryden I., 1998, WILEY SERIES PROBABI
   Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO
   Fasy BT, 2014, ANN STAT, V42, P2301, DOI 10.1214/14-AOS1252
   Fukumizu K, 2013, J MACH LEARN RES, V14, P3753
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Ledoux M., 1991, CLASSICS MATH
   Lee H., 2014, MICCAI
   Li C, 2014, CVPR
   Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407
   Mileyko Y, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/12/124007
   Munch E., 2013, CORR
   Reininghaus R, 2015, CVPR
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Singh N., 2014, MLMI
   Smola A., 2007, ALT
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I, 2002, J MACH LEARN RES, V2, P67
   Steinwart I, 2008, INFORM SCI STAT, P1
   Sun Jian, 2009, SGP
   Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102036
DA 2019-06-15
ER

PT S
AU Kyng, R
   Rao, A
   Sachdeva, S
AF Kyng, Rasmus
   Rao, Anup
   Sachdeva, Sushant
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast, Provable Algorithms for Isotonic Regression in all l(p)-norms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Given a directed acyclic graph G; and a set of values y on the vertices, the Isotonic Regression of y is a vector x that respects the partial order described by G; and minimizes parallel to x - y parallel to; for a specified norm. This paper gives improved algorithms for computing the Isotonic Regression for all weighted l(p)-norms with rigorous performance guarantees. Our algorithms are quite practical, and variants of them can be implemented to run fast in practice.
C1 [Kyng, Rasmus; Sachdeva, Sushant] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
   [Rao, Anup] Georgia Tech, Sch Comp Sci, Atlanta, GA USA.
RP Kyng, R (reprint author), Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
EM rasmus.kyng@yale.edu; arao89@gatech.edu; sachdeva@cs.yale.edu
FU AFOSR Award [FA9550-12-1-0175]; NSF [CCF-1111257]; Simons Investigator
   Award
FX We thank Sabyasachi Chatterjee for introducing the problem to us, and
   Daniel Spielman for his advice and comments. We would also like to thank
   Quentin Stout and anonymous reviewers for their suggestions. This
   research was partially supported by AFOSR Award FA9550-12-1-0175, NSF
   grant CCF-1111257, and a Simons Investigator Award to Daniel Spielman.
CR Acton ST, 1998, IEEE T IMAGE PROCESS, V7, P979, DOI 10.1109/83.701153
   Angelov S., 2006, SODA
   AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Barlow D. J., 1972, STAT INFERENCE ORDER
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chatterjee S., ANN STAT
   Cohen M. B., 2014, STOC 14
   Daitch SI, 2008, ACM S THEORY COMPUT, P451
   DENHERTOG D, 1995, MATH PROGRAM, V69, P75, DOI 10.1007/BF01585553
   DYKSTRA RL, 1982, ANN STAT, V10, P708, DOI 10.1214/aos/1176345866
   GEBHARDT F, 1970, BIOMETRIKA, V57, P263, DOI 10.2307/2334835
   Hochbaum DS, 2003, SIAM J DISCRETE MATH, V16, P192, DOI 10.1137/S0895480100369584
   Kakade S. M., 2011, NIPS
   Kalai Adam Tauman, 2009, COLT
   KAUFMAN Y, 1993, DISCRETE APPL MATH, V47, P251, DOI 10.1016/0166-218X(93)90130-G
   Koutis I, 2011, ANN IEEE SYMP FOUND, P590, DOI 10.1109/FOCS.2011.85
   Kyng R., 2015, COLT, P1190
   LEE CIC, 1983, ANN STAT, V11, P467, DOI 10.1214/aos/1176346153
   Lee Y. T., 2014, FOCS
   MacShane E., 1934, B AM MATH SOC, V40, P837, DOI 10.1090/S0002-9904-1934-05978-0
   Madry A., 2013, FOCS
   MAXWELL WL, 1985, OPER RES, V33, P1316, DOI 10.1287/opre.33.6.1316
   Moon T., 2010, P 3 ACM INT C WEB SE, P151, DOI DOI 10.1145/1718487.1718507
   Narasimhan H., 2013, NIPS
   Nemirovski A., 2004, LECURE NOTES INTERIO
   Punera K., 2008, WWW
   Renegar J., 2001, MATH VIEW INTERIOR P
   ROUNDY R, 1986, MATH OPER RES, V11, P699, DOI 10.1287/moor.11.4.699
   Spielman D.A., 2004, P 36 ANN ACM S THEOR, P81, DOI DOI 10.1145/1007352.1007372
   Stout Q. F., 2011, WEIGHTED L ISO UNPUB
   Stout Q. F, FASTEST ISOTONIC REG
   Stout Q. F., 2015, CORR
   Stout QF, 2015, ALGORITHMICA, V71, P450, DOI 10.1007/s00453-013-9814-z
   Stout QF, 2013, ALGORITHMICA, V66, P93, DOI 10.1007/s00453-012-9628-4
   Stout QF, 2012, J OPTIMIZ THEORY APP, V152, P121, DOI 10.1007/s10957-011-9865-8
   Whitney H, 1934, T AM MATH SOC, V36, P63, DOI 10.2307/1989708
   Zadrozny B, 2002, P 8 ACM SIGKDD INT C, P694, DOI DOI 10.1007/S10994-013-5343-X
   Zheng Z., 2008, COMM CONTR COMP ALL
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101083
DA 2019-06-15
ER

PT S
AU Lacoste-Julien, S
   Jaggi, M
AF Lacoste-Julien, Simon
   Jaggi, Martin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the Global Linear Convergence of Frank-Wolfe Optimization Variants
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SIMPLICIAL DECOMPOSITION
AB The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.
C1 [Lacoste-Julien, Simon] INRIA, SIERRA Project Team, Ecole Normale Super, Paris, France.
   [Jaggi, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Lacoste-Julien, S (reprint author), INRIA, SIERRA Project Team, Ecole Normale Super, Paris, France.
FU MSR-Inria Joint Center; Google Research Award
FX We thank J.B. Alayrac, E. Hazan, A. Hubard, A. Osokin and P. Marcotte
   for helpful discussions. This work was partially supported by the
   MSR-Inria Joint Center and a Google Research Award.
CR Ahipasaoglu SD, 2008, OPTIM METHOD SOFTW, V23, P5, DOI 10.1080/10556780701589669
   Alexander R., 1977, GEOMETRIAE DEDICATA, V6, P87
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Beck A, 2004, MATH METHOD OPER RES, V59, P235, DOI 10.1007/s001860300327
   Beck A., 2015, ARXIV150405002V1
   CANON MD, 1968, SIAM J CONTROL, V6, P509, DOI 10.1137/0306032
   Chari V., 2015, CVPR
   DUNN JC, 1979, SIAM J CONTROL OPTIM, V17, P187, DOI 10.1137/0317015
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Garber D., 2013, ARXIV13014666V5
   Garber D., 2015, ICML
   Guelat J., 1986, MATH PROGRAMMING
   HEARN DW, 1987, MATH PROGRAM STUD, V31, P99
   Holloway C. A., 1974, Mathematical Programming, V6, P14, DOI 10.1007/BF01580219
   Jaggi M., 2013, ICML
   Joulin A., 2014, ECCV
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krishnan R. G., 2015, NIPS
   Kumar P., 2010, INFORMS J COMPUTING
   Lacoste-Julien S., 2013, ICML
   Lacoste-Julien S., 2013, ARXIV13127864V2
   Lan G., 2013, ARXIV13095550V2
   Levitin E., 1966, ZH VYCH MAT MAT FIZ, V6, P787
   Lin H., 2015, NIPS
   Mitchell B., 1974, SIAM J CONTROL, V12
   Nanculef R., 2014, INFORM SCI
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Pena J., 2015, ARXIV150704073V2
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Robinson S. M., 1982, GEN EQUATIONS THEI 2
   VONHOHENBALKEN B, 1977, MATH PROGRAM, V13, P49, DOI 10.1007/BF01584323
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang PW, 2014, J MACH LEARN RES, V15, P1523
   WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381
   Wolfe P., 1970, INTEGER NONLINEAR PR
   Ziegler G. M., 1999, ARXIVMATH9909177V1
NR 36
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102073
DA 2019-06-15
ER

PT S
AU Lapin, M
   Hein, M
   Schiele, B
AF Lapin, Maksim
   Hein, Matthias
   Schiele, Bernt
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Top-k Multiclass SVM
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.
C1 [Lapin, Maksim; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany.
   [Hein, Matthias] Saarland Univ, Saarbrucken, Germany.
RP Lapin, M (reprint author), Max Planck Inst Informat, Saarbrucken, Germany.
CR Bordes A., 2007, P 24 INT C MACH LEAR, P89
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bu S., 2013, MM, P681
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Doersch C., 2013, P ADV NEUR INF PROC, P494
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gong  Y., 2014, ECCV
   Gupta MR, 2014, J MACH LEARN RES, V15, P1461
   Jia Y., 2014, ARXIV14085093
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Juneja M., 2013, CVPR
   Kiwiel KC, 2008, J OPTIMIZ THEORY APP, V136, P445, DOI 10.1007/s10957-007-9317-7
   Koskela M, 2014, P 22 ACM INT C MULT, P1169
   Lapin M., 2014, CVPR
   Li N., 2014, ADV NEURAL INFORM PR, P1502
   Ogryczak W, 2003, INFORM PROCESS LETT, V85, P117, DOI 10.1016/S0020-0190(02)00370-8
   Patriksson M, 2008, EUR J OPER RES, V185, P1, DOI 10.1016/j.ejor.2006.12.006
   Patriksson M, 2015, EUR J OPER RES, V243, P703, DOI 10.1016/j.ejor.2015.01.029
   Quattoni A., 2009, CVPR
   Razavian A- S., 2014, CORR, V1403, P6382
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Sanchez Garcia J. A., 2013, Insecta Mundi, P1
   Shalev-Shwartz S., 2014, MATH PROGRAM, P1, DOI DOI 10.1007/S10107-014-0839-0
   Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422
   Swersky K., 2012, NIPS, P3050
   Usunier N., 2009, P 26 ANN INT C MACH, P1057
   Weston Jason, 2011, IJCAI, P2764, DOI [10.5591/978-1-57735-516-8/IJCAI11-460, DOI 10.5591/978-1-57735-516-8/IJCAI11-460]
   Xiao J., 2010, CVPR
   Zhou  B., 2014, NIPS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101002
DA 2019-06-15
ER

PT S
AU Lattimore, T
   Crammer, K
   Szepesvari, C
AF Lattimore, Tor
   Crammer, Koby
   Szepesvari, Csaba
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Linear Multi-Resource Allocation with Semi-Bandit Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study an idealised sequential resource allocation problem. In each time step the learner chooses an allocation of several resource types between a number of tasks. Assigning more resources to a task increases the probability that it is completed. The problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy. Our main contribution is the new setting and an algorithm with nearly-optimal regret analysis. Along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise. We also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work, especially in the sparse case.
C1 [Lattimore, Tor; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
   [Crammer, Koby] Technion, Dept Elect Engn, Haifa, Israel.
RP Lattimore, T (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM tor.lattimore@gmail.com; koby@ee.technion.ac.il; szepesva@ualberta.ca
CR Abbasi-Yadkori Y, 2012, JMLR WORKSHOP C P, V22, P1
   Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Agrawal Shipra, 2012, ARXIV12093352
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bennett K. P., 1993, Computational Optimization and Applications, V2, P207, DOI 10.1007/BF01299449
   Bubeck  S., 2012, FDN TRENDS MACHINE L
   Dani V., 2008, COLT, P355
   Krishnamurthy Akshay, 2015, ARXIV150205890
   Kveton Branislav, 2014, ARXIV14100949
   Lattimore Tor, 2014, P 30 C UNC ART INT U
   Petrik M, 2011, J MACH LEARN RES, V12, P3027
   Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446
   Sowell Thomas, 1993, IS REALITY OPTIONAL
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102079
DA 2019-06-15
ER

PT S
AU Lattimore, T
AF Lattimore, Tor
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Pareto Regret Frontier for Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Omega(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.
C1 [Lattimore, Tor] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
RP Lattimore, T (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM tor.lattimore@gmail.com
CR Agrawal Shipra, 2012, 4 INT C INT HUM
   Agrawal Shipra, 2012, P C LEARN THEOR COLT
   Audibert J.-Y., 2009, COLT, P217
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Bubeck  S., 2012, FDN TRENDS MACHINE L
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Even-Dar E, 2008, MACH LEARN, V72, P21, DOI 10.1007/s10994-008-5060-z
   Hutter M, 2005, J MACH LEARN RES, V6, P639
   Kapralov Michael, 2011, P 25 ANN C NEUR INF, P828
   Koolen Wouter M, 2013, ADV NEURAL INFORM PR, P863
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lattimore Tor, 2015, TECHNICAL REPORT
   Liu Che-Yu, 2015, ARXIV150603378
   Sani  Amir, 2014, ADV NEURAL INFORM PR, V27, P810
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 17
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103070
DA 2019-06-15
ER

PT S
AU Lee, JD
   Sun, YK
   Taylor, J
AF Lee, Jason D.
   Sun, Yuekai
   Taylor, Jonathan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Evaluating the statistical significance of biclusters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Biclustering (also known as submatrix localization) is a problem of high practical relevance in exploratory analysis of high-dimensional data. We develop a framework for performing statistical inference on biclusters found by score-based algorithms. Since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm, this is a form of selective inference. Our framework gives exact (non-asymptotic) confidence intervals and p-values for the significance of the selected biclusters.
C1 [Lee, Jason D.; Sun, Yuekai; Taylor, Jonathan] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.
RP Lee, JD (reprint author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.
EM jdl17@stanford.edu; yuekai@stanford.edu; jonathan.taylor@stanford.edu
CR Addario-Berry L, 2010, ANN STAT, V38, P3063, DOI 10.1214/10-AOS817
   Ames BPW, 2014, MATH PROGRAM, V143, P299, DOI 10.1007/s10107-013-0733-1
   Ames Brendan PW, 2012, MATH PROGRAM, P1
   Arias-Castro E, 2011, ANN STAT, V39, P278, DOI 10.1214/10-AOS839
   BALAKRISHNAN S., 2011, NIPS 2011 WORKSH COM
   Bhamidi Shankar, 2012, ARXIV12112284
   Chen Y., 2014, ARXIV14021267
   Cheng Y, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P93
   Lazzeroni L, 2002, STAT SINICA, V12, P61
   Lee J, 2013, ARXIV13116238
   Ma Z., 2013, ARXIV13095914
   Shabalin AA, 2009, ANN APPL STAT, V3, P985, DOI 10.1214/09-AOAS239
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101075
DA 2019-06-15
ER

PT S
AU Lee, J
   Choi, S
AF Lee, Juho
   Choi, Seungjin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and realworld datasets demonstrate the benefit of our method.
C1 [Lee, Juho; Choi, Seungjin] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, 77 Cheongam Ro, Pohang 37673, South Korea.
RP Lee, J (reprint author), Pohang Univ Sci & Technol, Dept Comp Sci & Engn, 77 Cheongam Ro, Pohang 37673, South Korea.
EM stonecold@postech.ac.kr; seungjin@postech.ac.kr
FU IT R&D Program of MSIP/IITP [B0101-15-0307]; National Research
   Foundation (NRF) of Korea [NRF-2013R1A2A2A01067464]; IITP-MSRA Creative
   ICT/SW Research Project
FX This work was supported by the IT R&D Program of MSIP/IITP
   (B0101-15-0307, Machine Learning Center), National Research Foundation
   (NRF) of Korea (NRF-2013R1A2A2A01067464), and IITP-MSRA Creative ICT/SW
   Research Project.
CR ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871
   Brix A, 1999, ADV APPL PROBAB, V31, P929, DOI 10.1017/S0001867800009538
   Chen C., 2012, P INT C MACH LEARN I
   Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Griffin JE, 2011, J COMPUT GRAPH STAT, V20, P241, DOI 10.1198/jcgs.2010.08176
   Heller K. A., 2005, P INT C MACH LEARN I
   Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001
   James LF, 2009, SCAND J STAT, V36, P76, DOI 10.1111/j.1467-9469.2008.00609.x
   James LF, 2005, ANN STAT, V33, P1771, DOI 10.1214/0090536050000000336
   Jeong HK, 2014, PROC ASME CONF SMART
   Lijoi A, 2005, J AM STAT ASSOC, V100, P1278, DOI 10.1198/016214505000000132
   Lijoi A, 2007, J R STAT SOC B, V69, P715, DOI 10.1111/j.1467-9868.2007.00609.x
   Regazzini E, 2003, ANN STAT, V31, P560
   Zhang N, 2013, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON NANOCHANNELS, MICROCHANNELS, AND MINICHANNELS, 2013
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101060
DA 2019-06-15
ER

PT S
AU Lee, K
   Zlateski, A
   Vishwanathan, A
   Seung, HS
AF Lee, Kisuk
   Zlateski, Aleksandar
   Vishwanathan, Ashwin
   Seung, H. Sebastian
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary
   Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ELECTRON-MICROSCOPY; RECONSTRUCTION; VOLUME
AB Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D maxpooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.
C1 [Lee, Kisuk; Zlateski, Aleksandar] MIT, Cambridge, MA 02139 USA.
   [Vishwanathan, Ashwin; Seung, H. Sebastian] Princeton Univ, Princeton, NJ 08544 USA.
RP Lee, K (reprint author), MIT, Cambridge, MA 02139 USA.
EM kisuklee@mit.edu; zlateski@mit.edu; ashwinv@princeton.edu;
   sseung@princeton.edu
FU Samsung Scholarship; Mathers Foundation; Keating Fund for Innovation;
   Simons Center for the Social Brain; DARPA [HR0011-14-2-0004]; ARO
   [W911NF-12-1-0594]
FX We thank Juan C. Tapia, Gloria Choi and Dan Stettler for initial help
   with tissue handling and Jeff Lichtman and Richard Schalek with help in
   setting up tape collection. Kisuk Lee was supported by a Samsung
   Scholarship. The recursive approach proposed in this paper was partially
   motivated by Matthew J. Greene's preliminary experiments. We are
   grateful for funding from the Mathers Foundation, Keating Fund for
   Innovation, Simons Center for the Social Brain, DARPA
   (HR0011-14-2-0004), and ARO (W911NF-12-1-0594).
CR Briggman KL, 2012, CURR OPIN NEUROBIOL, V22, P154, DOI 10.1016/j.conb.2011.10.022
   Chen MG, 2014, NEURON, V82, P682, DOI 10.1016/j.neuron.2014.03.023
   Ciresan D., 2012, NIPS
   Giusti  A., 2013, ICIP
   Hayworth KJ, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00068
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   Helmstaedter M, 2013, NAT METHODS, V10, P501, DOI [10.1038/NMETH.2476, 10.1038/nmeth.2476]
   Huang G. B., 2014, ICLR
   Jain V, 2010, CURR OPIN NEUROBIOL, V20, P653, DOI 10.1016/j.conb.2010.07.004
   Jurrus E, 2010, MED IMAGE ANAL, V14, P770, DOI 10.1016/j.media.2010.06.002
   Kasthuri N, 2015, CELL, V162, P648, DOI 10.1016/j.cell.2015.06.054
   Kim JS, 2014, NATURE, V509, P331, DOI 10.1038/nature13240
   Krizhevsky A., 2012, NIPS
   Liu T, 2014, J NEUROSCI METH, V226, P88, DOI 10.1016/j.jneumeth.2014.01.022
   Long  J., 2015, CVPR
   Masci J., 2013, ICIP
   Mathieu M., 2014, ICLR
   Pinheiro P. H., 2014, ICML
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Sermanet  P., 2014, ICLR
   Seyedhosseini M, 2013, IEEE T IMAGE PROCESS, V22, P4486, DOI 10.1109/TIP.2013.2274388
   Simonyan Karen, 2015, ICLR
   Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450
   Tapia JC, 2012, NAT PROTOC, V7, P193, DOI 10.1038/nprot.2011.439
   Tasdizen T., 2014, COMPUTATIONAL INTELL, P237
   Tran D., 2014, ARXIV14120767
   Tu Z., 2008, CVPR
   Turaga S. C., 2009, NIPS
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Vasilache N., 2015, ICLR
   Yao L., 2015, ARXIV150208029
   Zlateski A., 2015, ARXIV150500249
   Zlateski Aleksandar, 2015, ARXIV151006706
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100004
DA 2019-06-15
ER

PT S
AU Lee, M
   Bindel, D
   Mimno, D
AF Lee, Moontae
   Bindel, David
   Mimno, David
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust Spectral Inference for Joint Stochastic Matrix Factorization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.
C1 [Lee, Moontae; Bindel, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.
   [Mimno, David] Cornell Univ, Dept Informat Sci, Ithaca, NY 14850 USA.
RP Lee, M (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.
EM moontae@cs.cornell.edu; bindel@cs.cornell.edu; mimno@cornell.edu
FU NSF [HCC: Large-0910664]
FX This research is supported by NSF grant HCC: Large-0910664. We thank
   Adrian Lewis for valuable discussions on AP convergence.
CR Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25, P926
   Arora S., 2012, FOCS
   Arora S., 2013, ICML
   Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyle J. P., 1986, LECTURE NOTES STATIS, V37, P28
   Broadbent M. E., 2010, SIAM UNDERGRAD RES O, V3, P50
   Chen S., 2012, P 18 ACM SIGKDD INT, P714, DOI [10.1145/2339530.2339643, DOI 10.1145/2339530.2339643]
   Daniilidis A, 2008, J CONVEX ANAL, V15, P547
   Gomez C, 2007, INT J REMOTE SENS, V28, P5315, DOI 10.1080/01431160701227679
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Kuang  D., 2012, SDM
   Kumar A., 2012, COMM NCC 2012 NAT C, P1, DOI [DOI 10.3109/07388551.2012.716810), 10.3109/07388551.2012.716810]
   Lee Moontae, 2014, P 2014 C EMP METH NA, P1319
   Levy O., 2014, NIPS
   Lewis AS, 2009, FOUND COMPUT MATH, V9, P485, DOI 10.1007/s10208-008-9036-y
   Mimno D., 2011, EMNLP
   Mislove A., 2010, P 3 ACM INT C WEB SE
   Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293
   Nguyen Thang, 2014, ASS COMPUTATIONAL LI
   Nickel M., 2011, P 28 INT C MACH LEAR, P809
   Pennington Jeffrey, 2014, EMNLP
   Thurau C., 2010, P 19 ACM INT C INF K, P1785, DOI [DOI 10.1145/1871437.1871729, 10.1145/1871437.1871729]
   Zhou T., 2014, NIPS, P1242
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103008
DA 2019-06-15
ER

PT S
AU Lei, YW
   Dogan, U
   Binder, A
   Kloft, M
AF Lei, Yunwen
   Dogan, Urun
   Binder, Alexander
   Kloft, Marius
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to
   Novel Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper studies the generalization performance of multi-class classification algorithms, for which we obtain-for the first time-a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on l(p)-norm regularization, where the parameter p controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.
C1 [Lei, Yunwen] City Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.
   [Dogan, Urun] Microsoft Res, Cambridge CB1 2FB, England.
   [Binder, Alexander] Singapore Univ Technol, ISTD Pillar, Singapore, Singapore.
   [Binder, Alexander] TU Berlin, Design Machine Learning Grp, Berlin, Germany.
   [Kloft, Marius] Humboldt Univ, Dept Comp Sci, Berlin, Germany.
RP Lei, YW (reprint author), City Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.
EM yunwelei@cityu.edu.hk; udogan@microsoft.com;
   alexander_binder@sutd.edu.sg; kloft@hu-berlin.de
FU German Research Foundation (DFG) [KL 2698/2-1]
FX We thank Mehryar Mohri for helpful discussions. This work was partly
   funded by the German Research Foundation (DFG) award KL 2698/2-1.
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bengio S., 2010, ADV NEURAL INFORM PR, P163
   Beygelzimer A., 2009, P 25 C UNC ART INT, P51
   Cortes C., 2013, P 30 INT C MACH LEAR, P46
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dekel O, 2010, INT C ART INT STAT, P137
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Glasmachers T., 2010, ADV NEURAL INFORM PR, P739
   Guermeur Y, 2002, PATTERN ANAL APPL, V5, P168, DOI 10.1007/s100440200015
   Gupta MR, 2014, J MACH LEARN RES, V15, P1461
   Hill SI, 2007, J ARTIF INTELL RES, V30, P525, DOI 10.1613/jair.2251
   Hofmann T., 2003, NIPS WORKSH SYNT SEM
   Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651
   Jia Y., 2014, ARXIV14085093
   Keerthi SS, 2008, P 14 ACM SIGKDD INT, P408
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Koltchinskii V, 2000, PROG PROBAB, V47, P443
   Kuznetsov V., 2014, ADV NEURAL INF PROCE, P2501
   Lang K., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P331
   Ledoux M., 1991, PROBABILITY BANACH S, V23
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Micchelli CA, 2005, J MACH LEARN RES, V6, P1099
   Mohri M., 2012, FDN MACHINE LEARNING
   Oneto L., 2011, ADV NEURAL INFORM PR, P585
   RENNIE JDM, 2001, AIM2001026 MIT
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   Welinder P., 2010, CNSTR2011001 CAL I T
   Zhang T, 2004, J MACH LEARN RES, V5, P1225
   Zhang T., 2004, ADV NEURAL INFORM PR, P1625
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103050
DA 2019-06-15
ER

PT S
AU Li, CX
   Zhu, J
   Shi, TL
   Zhang, B
AF Li, Chongxuan
   Zhu, Jun
   Shi, Tianlin
   Zhang, Bo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Max-Margin Deep Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.
C1 [Li, Chongxuan; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Sys, Dept Comp Sci & Tech,TNList Lab, Beijing 100084, Peoples R China.
   [Shi, Tianlin] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Li, CX (reprint author), Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Sys, Dept Comp Sci & Tech,TNList Lab, Beijing 100084, Peoples R China.
EM licx14@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn; st1501@gmail.com;
   dcszb@tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2013CB329403,
   2012CB316301]; National NSF of China [61322308, 61332007]; Tsinghua
   TNList Lab Big Data Initiative; Tsinghua Initiative Scientific Research
   Program [20121088071, 20141080934]
FX The work was supported by the National Basic Research Program (973
   Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of
   China (Nos. 61322308, 61332007), Tsinghua TNList Lab Big Data
   Initiative, and Tsinghua Initiative Scientific Research Program (Nos.
   20121088071, 20141080934).
CR Altun Y., 2003, ICML, P6
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y., 2014, ICML
   Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dosovitskiy A., 2014, ARXIV14115928
   Goodfellow I, 2013, ICML
   Goodfellow I., 2014, NIPS
   Gregor K., 2014, ICML
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Larochelle H., 2011, AISTATS
   Lecun Y., 1998, P IEEE
   Lee C. Y., 2015, AISTATS
   Lee H., 2009, ICML
   Lin M., 2014, ICLR
   Little R. J., 1987, JMLR, V539
   Miller K., 2012, AISTATS
   Mnih A., 2014, ICML
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Ranzato M., 2011, CVPR
   Rezende D. J., 2014, ICML
   Salakhutdinov R., 2009, AISTATS
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Sermanet P., 2012, ICPR
   Shalev-Shwartz S., 2011, MATH PROGRAMMING B
   Tang Y., 2013, ICML
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2004, ICML
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yu C. N, 2009, ICML
   Zeiler M., 2013, ICLR
   Zhu J., 2008, NIPS
   Zhu J, 2014, J MACH LEARN RES, V15, P1799
   Zhu J, 2014, J MACH LEARN RES, V15, P1073
   Zhu J, 2012, J MACH LEARN RES, V13, P2237
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102106
DA 2019-06-15
ER

PT S
AU Li, H
   Lin, ZC
AF Li, Huan
   Lin, Zhouchen
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Accelerated Proximal Gradient Methods for Nonconvex Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID VARIABLE SELECTION; ALGORITHMS; MINIMIZATION
AB Nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning. However, solving the nonconvex and nonsmooth optimization problems remains a big challenge. Accelerated proximal gradient (APG) is an excellent method for convex programming. However, it is still unknown whether the usual APG can ensure the convergence to a critical point in nonconvex programming. In this paper, we extend APG for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property. Accordingly, we propose a monotone APG and a nonmonotone APG. The latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration. To the best of our knowledge, we are the first to provide APG-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain O(1/k(2)) when the problems are convex, in which k is the number of iterations. Numerical results testify to the advantage of our algorithms in speed.
C1 [Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.
   Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China.
RP Lin, ZC (reprint author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.
EM lihuanss@pku.edu.cn; zlin@pku.edu.cn
FU National Basic Research Program of China (973 Program) [2015CB352502];
   National Natural Science Foundation (NSF) of China [61272341, 61231002];
   Microsoft Research Asia Collaborative Research Program
FX Zhouchen Lin is supported by National Basic Research Program of China
   (973 Program) (grant no. 2015CB352502), National Natural Science
   Foundation (NSF) of China (grant nos. 61272341 and 61231002), and
   Microsoft Research Asia Collaborative Research Program. He is the
   corresponding author.
CR Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9
   Bot R. L., 2014, INERTIAL FORWARD BAC, V2, P7
   Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Foucart S, 2009, APPL COMPUT HARMON A, V26, P395, DOI 10.1016/j.acha.2008.09.001
   Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3
   GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335
   Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245
   Ghadimi S., 2013, ARXIV13103787, p[1, 2]
   Gong Pinghua, 2013, Proc Int Conf Mach Learn, V28, P37
   Mohan K, 2012, J MACH LEARN RES, V13, P3441
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y. E., 2007, TECHNICAL REPORT, V1
   Ochs P., 2014, SIAM J IMAGING SCI, P2
   Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954
   Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308
   Tseng P., 2008, TECHNICAL REPORT, V1
   Yun F., 2014, LOW RANK SPARSE MODE, V1
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   Zhang HC, 2004, SIAM J OPTIMIZ, V14, P1043, DOI 10.1137/S1052623403428208
   Zhang T, 2010, J MACH LEARN RES, V11, P1081
   Zhong W., 2014, AAAI, P2
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100096
DA 2019-06-15
ER

PT S
AU Li, S
   Xie, Y
   Dai, HJ
   Song, L
AF Li, Shuang
   Xie, Yao
   Dai, Hanjun
   Song, Le
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI M-Statistic for Kernel Change-Point Detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the average run length in the online case. In this paper we propose two related computationally efficient M-statistics for kernel-based change-point detection when the amount of background data is large. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. We show that our methods perform well in both synthetic and real world data.
C1 [Li, Shuang; Xie, Yao] Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
   [Dai, Hanjun; Song, Le] Georgia Inst Technol, Coll Comp, Computat Sci & Engn, Atlanta, GA 30332 USA.
RP Li, S (reprint author), Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
EM sli370@gatech.edu; yao.xie@isye.gatech.edu; hanjundai@gatech.edu;
   lsong@cc.gatech.edu
FU NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF
   [IIS-1218749]; NSF CAREER [IIS-1350983];  [CMMI-1538746];  [CCF-1442635]
FX This research was supported in part by CMMI-1538746 and CCF-1442635 to
   Y.X.; NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF
   IIS-1218749, NSF CAREER IIS-1350983 to L.S..
CR Desobry F., 2005, IEEE T SIG P
   Enikeeva F., 2014, ARXIV13121900
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Harchaoui Z., 2008, ADV NEURAL INFORM PR
   Harchaoui Z, 2013, IEEE SIGNAL PROC MAG, V30, P87, DOI 10.1109/MSP.2013.2253631
   Kifer D., 2004, P 30 VLDB C
   Liu S, 2013, NEURAL NETWORKS, V43, P72, DOI 10.1016/j.neunet.2013.01.012
   Ramdas Aaditya, 2015, 29 AAAI C ART INT
   Ross Z. E., 2014, GEOPHYS J INT
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Serfling R. J., 1980, U STAT
   SIEGMUND D, 1995, ANN STAT, V23, P255, DOI 10.1214/aos/1176324466
   Siegmund D., 1985, SEQUENTIAL ANAL TEST
   Siegmund D, 2008, STAT INTERFACE, V1, P3
   Xie Y, 2013, ANN STAT, V41, P670, DOI 10.1214/13-AOS1094
   Yakir B, 2013, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118720608
   Zaremba W., 2013, ADV NEURAL INFO PROC
   Zou S., 2014, ARXIV14052294
NR 18
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100052
DA 2019-06-15
ER

PT S
AU Li, TY
   Prasad, A
   Ravikumar, P
AF Li, Tianyang
   Prasad, Adarsh
   Ravikumar, Pradeep
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Classification Rates for High-dimensional Gaussian Generative
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGULARIZATION; CENTROIDS; FEATURES
AB We consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate Gaussian distributions. We focus on the setting where the covariance matrices for the two conditional distributions are the same. The corresponding generative model classifier, derived via the Bayes rule, also called Linear Discriminant Analysis, has been shown to behave poorly in high-dimensional settings. We present a novel analysis of the classification error of any linear discriminant approach given conditional Gaussian models. This allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier. As we show, under a natural sparsity assumption, and letting s denote the sparsity of the Bayes classifier, p the number of covariates, and n the number of samples, the simple (l(1)-regularized) logistic regression classifier achieves the fast misclassification error rates of O(s log p/n), which is much better than the other approaches, which are either inconsistent under high-dimensional settings, or achieve a slower rate of O (root s log p/n).
C1 [Li, Tianyang; Prasad, Adarsh; Ravikumar, Pradeep] UT Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Li, TY (reprint author), UT Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM lty@cs.utexas.edu; adarsh@cs.utexas.edu; pradeepr@cs.utexas.edu
FU ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574,
   DMS-1264033]; NIH as part of the Joint DMS/NIGMS Initiative to Support
   Research at the Interface of the Biological and Mathematical Sciences
   [R01 GM117594-01]
FX We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via
   IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033, and NIH via R01
   GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
   Research at the Interface of the Biological and Mathematical Sciences.
CR Andrew Y. Ng, 2001, ADV NEURAL INFORM PR
   Banerjee A., 2014, ADV NEURAL INFORM PR, P1556
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600
   Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847
   Bishop C. M., 2006, INFORM SCI STAT
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cai Tony, 2011, J AM STAT ASS, V106
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chen W., 2014, ADV NEURAL INFORM PR, P1332
   Devroye L., 1996, PROBABILISTIC THEORY
   Devroye L., 1996, PROBABILISTIC THEORY, V31
   Donoho D, 2008, P NATL ACAD SCI USA, V105, P14790, DOI 10.1073/pnas.0807471105
   Fan JQ, 2012, J R STAT SOC B, V74, P745, DOI 10.1111/j.1467-9868.2012.01029.x
   Fan JQ, 2008, ANN STAT, V36, P2605, DOI 10.1214/07-AOS504
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fan YY, 2013, ANN STAT, V41, P2537, DOI 10.1214/13-AOS1163
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Gopal S., 2013, ICML, P289
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Kolar M., 2013, P 30 INT C MACH LEAR, V28, P329
   Koltchinskii V, 2011, LECT NOTES MATH, V2033, P1, DOI 10.1007/978-3-642-22147-7
   Mai Qing, 2012, BIOMETRIKA
   Mammen E, 1999, ANN STAT, V27, P1808
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Shao J, 2011, ANN STAT, V39, P1241, DOI 10.1214/10-AOS870
   Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299
   Wang SJ, 2007, BIOINFORMATICS, V23, P972, DOI 10.1093/bioinformatics/btm046
   Zhang T, 2004, ANN STAT, V32, P56
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103037
DA 2019-06-15
ER

PT S
AU Li, WY
AF Li, Wenye
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Estimating Jaccard Index with Missing Observations: A Matrix Calibration
   Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data, our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints, through a simple alternating projection algorithm. Compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the un-calibrated matrix (except in special cases they are identical). We carried out a series of empirical experiments and the results confirmed our theoretical justification. The evaluation also reported significantly improved results in real learning tasks on benchmark datasets.
C1 [Li, Wenye] Macao Polytech Inst, Macau, Peoples R China.
RP Li, WY (reprint author), Macao Polytech Inst, Macau, Peoples R China.
EM wyli@ipm.edu.mo
FU Science and Technology Development Fund, Macao SAR, China [006/2014/A]
FX The work is supported by The Science and Technology Development Fund
   (Project No. 006/2014/A), Macao SAR, China.
CR Birgin EG, 2005, SIAM J SCI COMPUT, V26, P1405, DOI 10.1137/03060062X
   Bouchard M, 2013, INT J APPROX REASON, V54, P615, DOI 10.1016/j.ijar.2013.01.006
   Boyd S, 2005, SIAM J MATRIX ANAL A, V27, P532, DOI 10.1137/040609902
   Boyd S., 2004, CONVEX OPTIMIZATION
   Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Deutsch F. R., 2001, BEST APPROXIMATION I
   Duda R. O., 2000, PATTERN CLASSIFICATI
   DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193
   Escalante R, 2011, FUND ALGORITHMS, V8, P1
   Ghahramani Z., 1994, ADV NEURAL INFORM PR, P120
   Golub G. H., 1996, MATRIX COMPUTATIONS
   Higham NJ, 2002, IMA J NUMER ANAL, V22, P329, DOI 10.1093/imanum/22.3.329
   Jaccard P., 1912, NEW PHYTOL, V11, P37, DOI DOI 10.1111/J.1469-8137.1912.TB05611.X
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   KNOL DL, 1989, PSYCHOMETRIKA, V54, P53, DOI 10.1007/BF02294448
   Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1
   Leung K-S., 2007, P 24 INT C MACH LEAR, P529
   Li P, 2011, COMMUN ACM, V54, P101, DOI 10.1145/1978542.1978566
   Luenberger D. G., 1969, OPTIMIZATION VECTOR
   Malick J, 2004, SIAM J MATRIX ANAL A, V26, P272, DOI 10.1137/S0895479802413856
   Qi HD, 2006, SIAM J MATRIX ANAL A, V28, P360, DOI 10.1137/050624509
   ROGERS DJ, 1960, SCIENCE, V132, P1115, DOI 10.1126/science.132.3434.1115
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103021
DA 2019-06-15
ER

PT S
AU Li, X
   Ramchandran, K
AF Li, Xiao
   Ramchandran, Kannan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI An Active Learning Framework using Sparse-Graph Codes for Sparse
   Polynomials and Graph Sketching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DECISION TREES
AB Let f : {-1, 1}(n) -> R be an n-variate polynomial consisting of 2(n) monomials, in which only s << 2(n) coefficients are non-zero. The goal is to learn the polynomial by querying the values of f. We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding.
   The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2(delta n)) for any delta is an element of(0, 1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.
C1 [Li, Xiao; Ramchandran, Kannan] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Li, X (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM xiaoli@berkeley.edu; kannanr@berkeley.edu
FU  [NSF CCF EAGER 1439725]
FX This work was supported by grant NSF CCF EAGER 1439725.
CR Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746
   Bouvel M, 2005, LECT NOTES COMPUT SC, V3787, P16
   Bshouty NH, 2010, LEIBNIZ INT PR INFOR, V5, P143, DOI 10.4230/LIPIcs.STACS.2010.2496
   Bshouty NH, 1995, AN S FDN CO, P304, DOI 10.1109/SFCS.1995.492486
   Choi SS, 2011, J COMPUT SYST SCI, V77, P1039, DOI 10.1016/j.jcss.2010.08.011
   Goldman S. A., 2010, ALGORITHMS THEORY CO, P26
   Jackson J., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P42, DOI 10.1109/SFCS.1994.365706
   Kearns M. J., 1990, COMPUTATIONAL COMPLE
   Kocaoglu M., 2014, NIPS, P3122
   KUSHILEVITZ E, 1993, SIAM J COMPUT, V22, P1331, DOI 10.1137/0222080
   MANSOUR Y, 1995, SIAM J COMPUT, V24, P357, DOI 10.1137/S0097539792239291
   MANSOUR Y., 1994, THEORETICAL ADV NEUR, P391
   Mazzawi H., 2011, THESIS
   Negahban S, 2012, ANN ALLERTON CONF, P2032, DOI 10.1109/Allerton.2012.6483472
   Richardson T., 2008, MODERN CODING THEORY
   Scheibler R., 1803, ARXIV13101803
   Settles Burr, 2010, U WISCONSIN MADISON, V52, P55
   Stobbe P., 2012, P INT C ART INT STAT, P1125
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100065
DA 2019-06-15
ER

PT S
AU Li, YZ
   Hernandez-Lobato, JM
   Turner, RE
AF Li, Yingzhen
   Hernandez-Lobato, Jose Miguel
   Turner, Richard E.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Stochastic Expectation Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.
C1 [Li, Yingzhen; Turner, Richard E.] Univ Cambridge, Cambridge CB2 1PZ, England.
   [Hernandez-Lobato, Jose Miguel] Harvard Univ, Cambridge, MA 02138 USA.
RP Li, YZ (reprint author), Univ Cambridge, Cambridge CB2 1PZ, England.
EM yl494@cam.ac.uk; jmh@seas.harvard.edu; ret26@cam.ac.uk
FU Schlumberger Foundation Faculty; Rafael del Pino Foundation; EPSRC
   [EP/G050821/1, EP/L000776/1]
FX We thank the reviewers for valuable comments. YL thanks the Schlumberger
   Foundation Faculty for the Future fellowship on supporting her PhD
   study. JMHL acknowledges support from the Rafael del Pino Foundation.
   RET thanks EPSRC grant #EP/G050821/1 and EP/L000776/1.
CR Ahn S., 2014, P 31 INT C MACH LEAR, V32, P1044
   Amari SI, 2000, METHODS INFORM GEOME, V191
   Bardenet R., 2014, P 31 INT C MACH LEAR
   Barthelme S, 2014, J AM STAT ASSOC, V109, P315, DOI 10.1080/01621459.2013.864178
   Beal M.J., 2003, THESIS
   Cunningham J. P., 2011, ARXIV11116832
   Dehaene G., 2015, ARXIV150308060
   Gelman A, 2014, ARXIV14124869
   Herbrich R., 2006, ADV NEURAL INFORM PR, P569
   Hernandez-Lobato J. M., 2015, ARXIV150205336
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   Maybeck PS, 1982, STOCHASTIC MODELS ES
   Minka T., 2005, MSRTR2005173
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Opper M, 2005, J MACH LEARN RES, V6, P2177
   Power EP, 2004, MSRTR2004149
   Qi Yuan, 2010, UNCERTAINTY ARTIFICI
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Turner R., 2011, ADV NEURAL INF PROCE, P981
   Turner Richard E., 2011, BAYESIAN TIME SERIES, P109
   Winn J, 2005, J MACH LEARN RES, V6, P661
   Zhang Bo, 2014, NIPS
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101020
DA 2019-06-15
ER

PT S
AU Lienart, T
   Teh, YW
   Doucet, A
AF Lienart, Thibaut
   Teh, Yee Whye
   Doucet, Arnaud
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Expectation Particle Belief Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.
C1 [Lienart, Thibaut; Teh, Yee Whye; Doucet, Arnaud] Univ Oxford, Dept Stat, Oxford, England.
RP Lienart, T (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM lienart@stats.ox.ac.uk; teh@stats.ox.ac.uk; doucet@stats.ox.ac.uk
FU EPSRC [EP/K000276/1, EP/K009850/1, 1379622, EP/K009362/1]; Scatcherd
   European scholarship scheme; ERC under the EU's FP7 Programme [617411];
   AFOSR/AOARD [AOARD-144042]
FX We thank Alexander Ihler and Drew Frank for sharing their implementation
   of Particle Belief Propagation. TL gratefully acknowledges funding from
   EPSRC (grant 1379622) and the Scatcherd European scholarship scheme.
   YWT's research leading to these results has received funding from EPSRC
   (grant EP/K009362/1) and ERC under the EU's FP7 Programme (grant
   agreement no. 617411). AD's research was supported by the EPSRC (grant
   EP/K000276/1, EP/K009850/1) and by AFOSR/AOARD (grant AOARD-144042).
CR Briers M, 2005, 2005 7th International Conference on Information Fusion (FUSION), Vols 1 and 2, P705
   Briers M, 2010, ANN I STAT MATH, V62, P61, DOI 10.1007/s10463-009-0236-2
   Crick C., 2003, UNCERTAINTY ARTIFICI, P159
   Felzenszwalb Pedro F., 2004, INT J COMP VIS, V59
   Ihler A., 2009, P AISTATS, P256
   Ihler AT, 2005, IEEE J SEL AREA COMM, V23, P809, DOI 10.1109/JSAC.2005.843548
   Klaus A, 2006, INT C PATT RECOG, P15
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Naesseth C. A., 2014, ADV NEURAL INFORM PR, P1862
   Nikolova M, 2000, IEEE T SIGNAL PROCES, V48, P3437, DOI 10.1109/78.887035
   Noorshams N, 2013, J MACH LEARN RES, V14, P2799
   Pearl J, 1988, PROBABILISTIC REASON
   Power EP, 2004, MSRTR2004149
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Schiff J, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1369, DOI 10.1109/IROS.2009.5354772
   Sudderth EB, 2003, PROC CVPR IEEE, P605
   Sudderth EB, 2010, COMMUN ACM, V53, P95, DOI 10.1145/1831407.1831431
   Sudderth Erik B., 2004, P IEEE COMP VIS PATT
   Sun J, 2003, IEEE T PATTERN ANAL, V25, P787, DOI 10.1109/TPAMI.2003.1206509
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Yedidia Jonathan S., 2002, CONSTRUCTING FREE EN
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100042
DA 2019-06-15
ER

PT S
AU Lim, ZW
   Hsu, D
   Lee, WS
AF Lim, Zhan Wei
   Hsu, David
   Lee, Wee Sun
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Adaptive Stochastic Optimization: From Sets to Paths
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.
C1 [Lim, Zhan Wei; Hsu, David; Lee, Wee Sun] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.
RP Lim, ZW (reprint author), Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.
EM limzhanw@comp.nus.edu.sg; dyhsu@comp.nus.edu.sg; leews@comp.nus.edu.sg
FU NUS AcRF [R-252-000-587-112]; National Research Foundation Singapore
   through the SMART Phase 2 Pilot Program [09]; US Air Force Research
   Laboratory [FA2386-15-1-4010]
FX This work is supported in part by NUS AcRF grant R-252-000-587-112,
   National Research Foundation Singapore through the SMART Phase 2 Pilot
   Program (Subaward Agreement No. 09), and US Air Force Research
   Laboratory under agreement number FA2386-15-1-4010.
CR Asadpour A, 2008, LECT NOTES COMPUT SC, V5385, P477, DOI 10.1007/978-3-540-92185-1_53
   Calinescu G, 2005, J COMB OPTIM, V9, P281, DOI 10.1007/s10878-005-1412-9
   Cuong Nguyen Viet, 2013, ADV NEURAL INFORM PR
   Cuong Nguyen Viet, 2014, P UNC ART INT
   Golovin D., 2010, ADV NEURAL INFORM PR, P766
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Guillory Andrew, 2010, INT C MACH LEARN ICM
   Gupta A, 2010, LECT NOTES COMPUT SC, V6198, P690, DOI 10.1007/978-3-642-14165-2_58
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   Lim Zhan Wei, 2014, WORKSP ALG FDN ROB
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Ong SCW, 2010, INT J ROBOT RES, V29, P1053, DOI 10.1177/0278364910369861
   Silver  D., 2010, ADV NEURAL INFORM PR
   Somani A., 2013, ADV NEURAL INFORM PR, P1772
   Zheng Alice X., 2005, P UNC ART INT
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103043
DA 2019-06-15
ER

PT S
AU Lin, GS
   Shen, CH
   Reid, I
   van den Hengel, A
AF Lin, Guosheng
   Shen, Chunhua
   Reid, Ian
   van den Hengel, Anton
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deeply Learning the Messages in Message Passing Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that involve a large number of classes. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method.
C1 [Lin, Guosheng] Univ Adelaide, Adelaide, SA, Australia.
   Australian Ctr Robot Vis, Brisbane, Qld, Australia.
RP Lin, GS (reprint author), Univ Adelaide, Adelaide, SA, Australia.
EM guosheng.lin@adelaide.edu.au; chunhua.shen@adelaide.edu.au;
   ian.reid@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au
RI Lin, Guosheng/N-9110-2019
OI Lin, Guosheng/0000-0002-0329-7458
FU Data to Decisions Cooperative Research Centre; Australian Research
   Council through the ARC Centre for Robotic Vision [CE140100016];
   Australian Research Council through a Laureate Fellowship [FL130100102]
FX This research was supported by the Data to Decisions Cooperative
   Research Centre and by the Australian Research Council through the ARC
   Centre for Robotic Vision CE140100016 and through a Laureate Fellowship
   FL130100102 to I. Reid. Correspondence should be addressed to C. Shen.
CR Besag J., 1977, BIOMETRIKA
   Chen  L., 2014, SEMANTIC IMAGE SEGME
   Chen L., 2014, LEARNING DEEP STRUCT
   Dai J., 2015, BOXSUP EXPLOITING BO
   Everingham M., 2010, INT J COMP VIS
   Hariharan B., 2011, P INT C COMP VIS
   Hariharan B., 2014, P EUR C COMP VIS
   Kolmogorov V., 2006, IEEE T PATTERN ANAL
   Krahenbuhl P., 2012, P ADV NEUR INF PROC
   Lin G., 2015, EFFICIENT PIECEWISE
   Liu F., 2015, LEARNING DEPTH SINGL
   Liu FY, 2015, IEEE INFOCOM SER
   Long J., 2015, P IEEE C COMP VIS PA
   Mostajabi M., 2014, FEEDFORWARD SEMANTIC
   Noh H., 2015, P IEEE C COMP VIS PA
   Nowozin S., 2011, FDN TRENDS COMPUT GR
   Papandreou G., 2015, WEAKLY AND SEMISUPER
   Ross S, 2011, PROC CVPR IEEE
   Schwing A.G., 2015, FULLY CONNECTED DEEP
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Sutton C., 2005, P C UNC ART INT
   Tompson J, 2014, ADV NEUR IN, V27
   Vedaldi A., 2015, P ACM INT C MULT
   Yedidia J. S., 2000, P ADV NEUR INF PROC
   Zheng S., 2015, CONDITIONAL RANDOM F
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101051
DA 2019-06-15
ER

PT S
AU Lin, H
   Mairal, J
   Harchaoui, Z
AF Lin, Hongzhou
   Mairal, Julien
   Harchaoui, Zaid
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Universal Catalyst for First-Order Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHM; INEXACT
AB We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.
C1 [Lin, Hongzhou; Mairal, Julien; Harchaoui, Zaid] INRIA, Paris, France.
   [Harchaoui, Zaid] NYU, New York, NY 10003 USA.
RP Lin, H (reprint author), INRIA, Paris, France.
EM hongzhou.lin@inria.fr; julien.mairal@inria.fr; zaid.harchaoui@nyu.edu
FU ANR [MACARON ANR-14-CE23-0003-01]; MSR-Inria joint centre;
   CNRS-Mastodons program (Titan); NYU Moore-Sloan Data Science Environment
FX This work was supported by ANR (MACARON ANR-14-CE23-0003-01), MSR-Inria
   joint centre, CNRS-Mastodons program (Titan), and NYU Moore-Sloan Data
   Science Environment.
CR Agarwal A., 2015, P INT C MACH LEARN I
   Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas D. P., 2015, CONVEX OPTIMIZATION
   Defazio A., 2014, ADV NEURAL INFORM PR
   Defazio A. J., 2014, P INT C MACH LEARN I
   Frostig R., 2015, P INT C MACH LEARN I
   Guler O, 1992, SIAM J OPTIMIZ, V2, P649, DOI 10.1137/0802032
   He BS, 2012, J OPTIMIZ THEORY APP, V154, P536, DOI 10.1007/s10957-011-9948-6
   Hiriart-Urruty J-B, 1996, CONVEX ANAL MINIMIZA
   Lan  Guanghui, 2015, ARXIV150702000
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Salzo S, 2012, J CONVEX ANAL, V19, P1167
   Schmidt  M., 2013, ARXIV13092388
   Schmidt M., 2011, ADV NEURAL INFORM PR
   Shalev-Shwartz S., 2012, ARXIV12112717
   Shalev-Shwartz S., 2015, MATH PROGRAMMING
   Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang Y., 2015, P INT C MACH LEARN I
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102076
DA 2019-06-15
ER

PT S
AU Lin, T
   Li, J
   Chen, W
AF Lin, Tian
   Li, Jian
   Chen, Wei
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Stochastic Online Greedy Learning with Semi-bandit Feedbacks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The greedy algorithm is extensively studied in the field of combinatorial optimization for decades. In this paper, we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time. We first propose the greedy regret and epsilon-quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm. We then propose two online greedy learning algorithms with semi-bandit feedbacks, which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning, one for each of the regret metrics respectively. Both algorithms achieve O(log T) problem-dependent regret bound (T being the time horizon) for a general class of combinatorial structures and reward functions that allow greedy solutions. We further show that the bound is tight in T and other problem instance parameters.
C1 [Lin, Tian; Li, Jian] Tsinghua Univ, Beijing, Peoples R China.
   [Chen, Wei] Microsoft Res, Beijing, Peoples R China.
RP Lin, T (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM lintian06@gmail.com; lapordge@gmail.com; weic@microsoft.com
FU National Basic Research Program of China [2015CB358700, 2011CBA00300,
   2011CBA00301]; National NSFC [61202009, 61033001, 61361136003]
FX Jian Li was supported in part by the National Basic Research Program of
   China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National
   NSFC grants 61202009, 61033001, 61361136003.
CR Audibert  J.-Y., 2010, COLT
   Audibert J.-Y., 2011, ARXIV11054871
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bjorner A., 1992, MATROID APPL, P284
   Bubeck S., 2012, ARXIV12045721
   Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen  Shouyuan, 2014, NIPS
   Chen W., 2013, ICML
   Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233
   Gabillon V., 2013, NIPS
   Gai Y., 2010, DYSPAN
   Garivier A., 2011, ARXIV11022490
   HELMAN P, 1993, SIAM J DISCRETE MATH, V6, P274, DOI 10.1137/0406021
   Kalyanakrishnan S, 2012, ICML
   Kempe D., 2003, SIGKDD
   KORTE B, 1984, SIAM J ALGEBRA DISCR, V5, P229, DOI 10.1137/0605024
   Kruskal J, 1956, P AM MATH SOC, V7, P48, DOI [10.1090/S0002-9939-1956-0078686-7, DOI 10.1090/S0002-9939-1956-0078686-7]
   Kveton  B., 2014, ARXIV14035045
   Kveton Branislav, 2014, ARXIV14100949
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lin T., 2014, ICML
   Mirzasoleiman B., 2015, P C ART INT AAAI
   PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x
   Streeter M., 2009, NIPS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102078
DA 2019-06-15
ER

PT S
AU Liu, Q
   Fisher, J
   Ihler, A
AF Liu, Qiang
   Fisher, John, III
   Ihler, Alexander
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Probabilistic Variational Bounds for Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SAMPLING ALGORITHMS
AB Variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an "any-time" fashion, expending more computation for tighter bounds. On the other hand, Monte Carlo estimators such as importance sampling have excellent any-time behavior, but depend critically on the proposal distribution. We propose a simple Monte Carlo based inference method that augments convex variational bounds by adding importance sampling (IS). We argue that convex variational methods naturally provide good IS proposals that "cover" the target probability, and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our IS estimator. This both provides an accurate estimator and enables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art variational bounds, and provide certificates of accuracy given enough samples relative to the error in the initial bound.
C1 [Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA.
   [Fisher, John, III] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Ihler, Alexander] Univ Calif Irvine, Comp Sci, Irvine, CA USA.
RP Liu, Q (reprint author), Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA.
EM qliu@cs.dartmouth.edu; fisher@csail.mit.edu; ihler@ics.uci.edu
FU VITALITE under the ARO MURI program [W911NF-11-1-0391]; NSF
   [IIS-1065618, IIS-1254071]; United States Air Force [FA8750-14-C-0011]
FX This work is supported in part by VITALITE, under the ARO MURI program
   (Award number W911NF-11-1-0391); NSF grants IIS-1065618 and IIS-1254071;
   and by the United States Air Force under Contract No. FA8750-14-C-0011
   under the DARPA PPAML program.
CR Bengtsson T., 2008, PROBAB STAT, V2, P316, DOI DOI 10.1214/193940307000000518
   Cheng J, 2001, COMPUTATION STAT, V16, P1, DOI 10.1007/s001800100049
   Cheng J., 2000, J ARTIFICIAL INTELLI
   Dagum P, 2000, SIAM J COMPUT, V29, P1484, DOI 10.1137/S0097539797315306
   Dagum P, 1997, ARTIF INTELL, V93, P1, DOI 10.1016/S0004-3702(97)00013-1
   De Freitas N., 2001, UAI
   Dechter R, 2003, J ACM, V50, P107, DOI 10.1145/636865.636866
   Druzdzel M. J., 2002, UAI, P624
   Fung R., 1990, UAI
   Globerson A., 2007, UAI, P130
   Gogate V., 2009, THESIS
   Gogate V, 2011, INTELL ARTIF, V5, P171, DOI 10.3233/IA-2011-0026
   Hazan T., 2012, ICML
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Liu JS, 2008, MONTE CARLO STRATEGI
   Liu Q, 2014, THESIS
   Liu Q., 2011, ICML
   Mateescu R, 2010, J ARTIF INTELL RES, V37, P279, DOI 10.1613/jair.2842
   Maurer A., 2009, COLT, P115
   Mnih V., 2008, ICML
   Oh M.-S., 1992, J STAT COMPUT SIM, V41, P143
   Orabona F., 2014, ICML
   Papandreou G., 2011, ICCV
   Ruozzi N., 2012, NIPS
   Salimans T., 2015, ICML
   Shachter R., 1990, UAI
   Sudderth E., 2007, ADV NEURAL INF PROCE, P1425
   Wainwright MJ, 2006, J MACH LEARN RES, V7, P1829
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
   Wexler Y., 2007, UAI
   Yuan C., 2007, P 22 NAT C ART INT V, V7, P1296
   Yuan C, 2007, INT J APPROX REASON, V46, P320, DOI 10.1016/j.ijar.2006.09.006
   Yuan CH, 2006, MATH COMPUT MODEL, V43, P1189, DOI 10.1016/j.mcm.2005.05.020
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100063
DA 2019-06-15
ER

PT S
AU Liu, WW
   Tsang, IW
AF Liu, Weiwei
   Tsang, Ivor W.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the Optimality of Classifier Chain for Multi-label Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB To capture the interdependencies between labels in multi-label classification problems, classifier chain (CC) tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for CC. In this work, we first generalize the CC model over a random label order. Then, we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CC-Greedy algorithm achieves comparable prediction performance with CC-DP.
C1 [Liu, Weiwei; Tsang, Ivor W.] Univ Technol, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia.
RP Liu, WW (reprint author), Univ Technol, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia.
EM liuweiwei863@gmail.com; ivor.tsang@uts.edu.au
FU Australian Research Council Future Fellowship [FT130100746]
FX This research was supported by the Australian Research Council Future
   Fellowship FT130100746.
CR Bartlett Peter L., 1998, ADV KERNEL METHODS S, P43
   Barutcuoglu Z, 2006, BIOINFORMATICS, V22, P830, DOI 10.1093/bioinformatics/btk048
   Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Dembczynski Krzysztof, 2010, P 27 INT C MACH LEAR, P279
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Guo Y., 2011, P INT JOINT C ART IN, V2, P1300, DOI DOI 10.5591/978-1-57735-516-8/IJCA111-220
   Hsu D., 2009, ADV NEURAL INFORM PR, V22, P772
   Huang Sheng-Jun, 2012, P 26 AAAI C ART INT
   Kang F., 2006, P IEEE COMP SOC C CO, P1719, DOI DOI 10.1109/CVPR.2006.90
   Kearns M. J., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P382, DOI 10.1109/FSCS.1990.89557
   Liu Weiwei, 2015, AAAI, V15, P2800
   Mao Q, 2013, IEEE T IMAGE PROCESS, V22, P1583, DOI 10.1109/TIP.2012.2233490
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Tai F, 2012, NEURAL COMPUT, V24, P2508, DOI 10.1162/NECO_a_00320
   Tan Mingkui, 2015, IEEE C COMP VIS PATT
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Zhang M.-L., 2010, P 16 ACM SIGKDD INT, P999
   Zhang Y, 2011, P 14 INT C ART INT S, P873
   Zhang Y., 2012, P 29 INT C MACH LEAR, P1575
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103039
DA 2019-06-15
ER

PT S
AU Liu, YY
   Li, S
   Li, FX
   Song, L
   Rehg, JM
AF Liu, Yu-Ying
   Li, Shuang
   Li, Fuxin
   Song, Le
   Rehg, James M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Learning of Continuous-Time Hidden Markov Models for Disease
   Progression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models. We demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three approaches from the continuous time Markov chain literature to the CT-HMM domain. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset.
C1 [Liu, Yu-Ying; Li, Shuang; Li, Fuxin; Song, Le; Rehg, James M.] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
RP Liu, YY (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
FU NIH [R01 EY13178-15]; National Institute of Biomedical Imaging and
   Bioengineering through Big Data to Knowledge (BD2K) initiative
   [U54EB020404]; ADNI under NIH [U01 AG024904]; DOD [W81XWH-12-2-0012];
   NSF/NIH [BIGDATA 1R01GM108341]; ONR [N00014-15-1-2340]; NSF
   [IIS-1218749]; NSF CAREER [IIS-1350983]
FX Portions of this work were supported in part by NIH R01 EY13178-15 and
   by grant U54EB020404 awarded by the National Institute of Biomedical
   Imaging and Bioengineering through funds provided by the Big Data to
   Knowledge (BD2K) initiative (www.bd2k.nih.gov).Additionally, the
   collection and sharing of the Alzheimers data was funded by ADNI under
   NIH U01 AG024904 and DOD award W81XWH-12-2-0012. The research was also
   supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340,
   NSF IIS-1218749, and NSF CAREER IIS-1350983.
CR Bartolomeo N, 2011, BMC MED RES METHODOL, V11, DOI 10.1186/1471-2288-11-38
   Bladt M., 2005, J R STAT SOC B, V39
   Cox D. R., 1965, THEORY STOCHASTIC PR
   Fagan AM, 2009, ANN NEUROL, V65, P176, DOI 10.1002/ana.21559
   Higham N. J., 2008, FUNCTIONS MATRICES T
   Hobolth A, 2005, STAT APPL GENET MOL, V4
   Hobolth A, 2011, J APPL PROBAB, V48, P911, DOI 10.1239/jap/1324046009
   Jackson CH, 2011, J STAT SOFTW, V38, P1
   Kingman S., 2004, B WHO, V82
   Leiva-Murillo J. M., 2011, NIPS
   Liu YY, 2013, LECT NOTES COMPUT SC, V8150, P444, DOI 10.1007/978-3-642-40763-5_55
   Medeiros FA, 2012, AM J OPHTHALMOL, V153, P1197, DOI 10.1016/j.ajo.2011.11.015
   Metzner P., 2007, J COMPUT PHYS, V227
   Metzner P, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066702
   Nodelman U., 2005, P UNC AI UAI 05
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Tataru P, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-465
   VANLOAN CF, 1978, IEEE T AUTOMAT CONTR, V23, P395, DOI 10.1109/TAC.1978.1101743
   Wang X., 2014, SCI J EARTH SCI, V4, P85
   Wollstein G, 2005, ARCH OPHTHALMOL-CHIC, V123, P464, DOI 10.1001/archopht.123.4.464
   Wollstein G, 2012, BRIT J OPHTHALMOL, V96, P47, DOI 10.1136/bjo.2010.196907
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101052
DA 2019-06-15
ER

PT S
AU Lloyd, JR
   Ghahramani, Z
AF Lloyd, James Robert
   Ghahramani, Zoubin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Statistical Model Criticism using Kernel Two Sample Tests
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose an exploratory approach to statistical model criticism using maximum mean discrepancy (MMD) two sample tests. Typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model. MMD two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy. We demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on. We then apply the procedure to real data where the models being assessed are restricted Boltzmann machines, deep belief networks and Gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.
C1 [Lloyd, James Robert; Ghahramani, Zoubin] Univ Cambridge, Dept Engn, Cambridge, England.
RP Lloyd, JR (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
CR Bayarri MJ, 2007, STAT SCI, V22, P322, DOI 10.1214/07-STS235
   Bayarri M J, 1999, BAYES STAT
   Benjamini Y., 1995, J R STAT SOC B
   BICKEL PJ, 1969, ANN MATH STAT, V40, P1, DOI 10.1214/aoms/1177697800
   BOX GEP, 1980, J ROY STAT SOC A STA, V143, P383, DOI 10.2307/2982063
   Cook Dennis, 1982, MON STAT APP PROB
   Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683
   Gelfand A E, 1992, 462 STANF U CA DEP S
   Gelman A, 1996, STAT SINICA, V6, P733
   Gelman A., 2013, BAYESIAN DATA ANAL
   Gelman Andrew, 2003, INT STAT REV
   Gelman Andrew, 2013, ELEC J STAT
   Geweke J, 2004, J AM STAT ASSOC, V99, P799, DOI 10.1198/0162145040000001132
   Goodman Noah D, 2008, C UNC ART INT UAI
   GRETTON A., 2008, MACH LEARN, V1, P1
   Grosse Roger, 2012, C UNC ART INT UAI
   GUTTMAN I, 1967, J ROY STAT SOC B, V29, P83
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6
   Hotelling H., 1951, P 2 BERK S MATH STAT
   Iwata Tomoharu, 2013, C UNC ART INT UAI
   Koller D, 1997, ASS ADV ARTIFICIAL I
   Lloyd J. R., 2014, ASS ADV ARTIFICIAL I
   Marshall EC, 2007, BAYESIAN ANAL, V2, P409, DOI 10.1214/07-BA218
   Milch B, 2005, P INT JOINT C ART IN
   O'HAGAN A., 2003, HIGHLY STRUCTURED ST, V27, P423
   Parzen Emanuel, 1962, ANN MATH STAT
   Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081
   Popper K., 2005, LOGIC SCI DISCOVERY
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robins JM, 2000, J AM STAT ASSOC, V95, P1143, DOI 10.2307/2669750
   ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190
   RUBIN DB, 1984, ANN STAT, V12, P1151, DOI 10.1214/aos/1176346785
   Stan Development Team, 2014, STAN C PLUS PLUS LIB
   STIGLER SM, 1977, ANN STAT, V5, P1055, DOI 10.1214/aos/1176343997
   Thornton C., 2013, P 19 ACM SIGKDD INT, P847, DOI [10.1145/2487575.2487629, DOI 10.1145/2487575.2487629]
   Vehtari A, 2012, STAT SURV, V6, P142, DOI 10.1214/12-SS102
   Wilson A. G., 2013, P INT C MACH LEARN
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100025
DA 2019-06-15
ER

PT S
AU Lomeli, M
   Favaro, S
   Teh, YW
AF Lomeli, Maria
   Favaro, Stefano
   Teh, Yee Whye
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A hybrid sampler for Poisson-Kingman mixture models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DENSITY-ESTIMATION; DIRICHLET; REPRESENTATION
AB This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.
C1 [Lomeli, Maria] UCL, Gatsby Unit, London, England.
   [Favaro, Stefano] Univ Torino, Dept Econ & Stat, Turin, Italy.
   [Favaro, Stefano] Coll Carlo Alberto, Turin, Italy.
   [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England.
RP Lomeli, M (reprint author), UCL, Gatsby Unit, London, England.
EM mlomeli@gatsby.ucl.ac.uk; stefano.favaro@unito.it;
   y.w.teh@stats.ox.ac.uk
FU Gatsby Charitable Foundation; European Research Council [StG N-BNP
   306406]; European Research Council under the European Unions Seventh
   Framework Programme (FP7/2007-2013) ERC grant [617071]
FX We thank Konstantina Palla for her insightful comments. Maria Lomeli is
   funded by the Gatsby Charitable Foundation, Stefano Favaro is supported
   by the European Research Council through StG N-BNP 306406 and Yee Whye
   Teh is supported by the European Research Council under the European
   Unions Seventh Framework Programme (FP7/2007-2013) ERC grant agreement
   no. 617071.
CR De Blasi P, 2015, IEEE T PATTERN ANAL, V37, P212, DOI 10.1109/TPAMI.2013.217
   Devroye L, 1986, NONUNIFORM RANDOM VA
   Devroye L, 2009, ACM T MODEL COMPUT S, V19, DOI 10.1145/1596519.1596523
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   ESCOBAR MD, 1994, J AM STAT ASSOC, V89, P268, DOI 10.2307/2291223
   Favaro S, 2014, ELECTRON J STAT, V8, P1063, DOI 10.1214/14-E4S921
   Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422
   Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541
   Gnedin A, 2006, J MATH SCI, V138, P5674
   Hofert M, 2011, COMPUT STAT DATA AN, V55, P57, DOI 10.1016/j.csda.2010.04.025
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   James L. F, 2002, ARXIVMATH0205093
   KANTER M, 1975, ANN PROBAB, V3, P697, DOI 10.1214/aop/1176996309
   KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59
   KINGMAN JFC, 1978, J LOND MATH SOC, V18, P374, DOI 10.1112/jlms/s2-18.2.374
   Lomeli M., 2015, J COMPUTATIONAL GRAP
   Neal R.M., 1998, 9815 U TOR DEP STAT
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   Papaspiliopoulos O, 2008, BIOMETRIKA, V95, P169, DOI 10.1093/biomet/asm086
   PERMAN M, 1992, PROBAB THEORY REL, V92, P21, DOI 10.1007/BF01205234
   Pitman J, 2003, INST MATH S, V40, P1
   Pitman J, 1996, ADV APPL PROBAB, V28, P525, DOI 10.2307/1428070
   Pitman J., 2006, LECT NOTES MATH
   Regazzini E, 2003, ANN STAT, V31, P560
   ROEDER K, 1990, J AM STAT ASSOC, V85, P617, DOI 10.2307/2289993
   von Renesse MK, 2008, STOCH PROC APPL, V118, P2038, DOI 10.1016/j.spa.2007.11.008
   Walker S. G, 2012, J COMPUTATIONAL GRAP, V22, P830
   Walker SG, 2007, COMMUN STAT-SIMUL C, V36, P45, DOI 10.1080/03610910601096262
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101059
DA 2019-06-15
ER

PT S
AU Ma, TY
   Wigderson, A
AF Ma, Tengyu
   Wigderson, Avi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sum-of-Squares Lower Bounds for Sparse PCA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PRINCIPAL-COMPONENTS; SEMIDEFINITE RELAXATIONS; POSITIVSTELLENSATZ;
   OPTIMIZATION; POLYNOMIALS; COMPLEXITY; MATRICES; PROOFS
AB This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the Sparse Principal Component Analysis (Sparse PCA) problem, and the family of Sum-of-Squares (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension p, a planted k-sparse unit vector can be in principle detected using only n approximate to k log p (Gaussian or Bernoulli) samples, but all efficient (polynomial time) algorithms known require n approximate to k(2) samples. It was also known that this quadratic gap cannot be improved by the the most basic semi-definite (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or "pseudo-expectations") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.
C1 [Ma, Tengyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
   [Wigderson, Avi] Inst Adv Study, Sch Math, Princeton, NJ USA.
RP Ma, TY (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
FU Simons Award for Graduate Students in Theoretical Computer Science; NSF
   [CCF-1412958]
FX Supported in part by Simons Award for Graduate Students in Theoretical
   Computer Science; Supported in part by NSF grant CCF-1412958
CR Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745
   Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664
   Artin E., 1927, ABH MATH SEM HAMBURG, V5, P100
   Barak B., 2015, ABS150106521 CORR
   Barak B., 2014, STOC, P31, DOI DOI 10.1145/2591796.2591886
   Barak Boaz, 2015, P 47 ANN ACM S THEOR
   Barak Boaz, 2014, P INT C MATH ICM
   Berthet Q., 2013, C LEARN THEOR, V30, P1046
   Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127
   Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Daniely A., 2013, ADV NEURAL INFORM PR, V26, P145
   Decatur S., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P130, DOI 10.1145/267460.267489
   Deshpande Y., 2014, ADV NEURAL INFORM PR, P334
   Deshpande Y., 2015, ARXIV E PRINTS
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Donoho DL, 1998, ANN STAT, V26, P879
   Gao C., 2014, ARXIV E PRINTS
   Grigoriev D, 2001, COMPUT COMPLEX, V10, P139, DOI 10.1007/s00037-001-8192-0
   Grigoriev D, 2001, THEOR COMPUT SCI, V259, P613, DOI 10.1016/S0304-3975(00)00157-2
   Hopkins Samuel B., 2015, ABS150705230 CORR
   Jenatton R, 2010, P 13 INT C ART INT S, P366
   Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121
   Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544
   Johnstone IM, 2002, UNPUB
   Krauthgamer R, 2015, ANN STAT, V43, P1300, DOI 10.1214/15-AOS1310
   Krivine Jean-Louis, 1964, J ANAL MATH
   Lasserre JB, 2015, CAM T APP M
   Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802
   Laurent M, 2009, IMA VOL MATH APPL, V149, P157
   Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013
   Ma Tengyu, 2014, COMMUNICATION
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   Meka Raghu, 2015, ABS150306447 CORR
   Nesterov Y., 2000, HIGH PERFORMANCE OPT, P405, DOI DOI 10.1007/978-1-4757-3216-0_17
   Parrilo P. A., 2000, THESIS
   Paul D., 2012, ARXIV12021242
   PUTINAR M, 1993, INDIANA U MATH J, V42, P969, DOI 10.1512/iumj.1993.42.42045
   Raghavendra Prasad, 2015, ABS150705136 CORR
   SCHMUDGEN K, 1991, MATH ANN, V289, P203, DOI 10.1007/BF01446568
   Schoenebeck G, 2008, ANN IEEE SYMP FOUND, P593, DOI 10.1109/FOCS.2008.74
   Servedio RA, 2000, J COMPUT SYST SCI, V60, P161, DOI 10.1006/jcss.1999.1666
   SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036
   SHOR NZ, 1987, CYBERNETICS+, V23, P695
   STENGLE G, 1974, MATH ANN, V207, P87, DOI 10.1007/BF01362149
   Vu V. Q., 2012, P 15 INT C ART INT S, P1278
   Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151
   Wang Z, 2015, ARXIV E PRINTS
   Xi Chen, 2011, STAT APPL GENET MOL, P10
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100092
DA 2019-06-15
ER

PT S
AU Mahsereci, M
   Hennig, P
AF Mahsereci, Maren
   Hennig, Philipp
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Probabilistic Line Searches for Stochastic Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MINIMIZATION
AB In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.
C1 [Mahsereci, Maren; Hennig, Philipp] Max Planck Inst Intelligent Syst, Spemannstr 38, D-72076 Tubingen, Germany.
RP Mahsereci, M (reprint author), Max Planck Inst Intelligent Syst, Spemannstr 38, D-72076 Tubingen, Germany.
EM mmahsereci@tue.mpg.de; phennig@tue.mpg.de
CR Adler RJ, 1981, GEOMETRY RANDOM FIEL
   Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420
   ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   BROYDEN CG, 1969, NOT AM MATH SOC, V16, P670
   Drezner Z, 1990, J STAT COMPUT SIM, V35, P101, DOI DOI 10.1080/00949659008811236
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   FLETCHER R, 1964, COMPUT J, V7, P149, DOI 10.1093/comjnl/7.2.149
   FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317
   George AP, 2006, MACH LEARN, V65, P167, DOI 10.1007/S10994-006-8365-9
   GOLDFARB D, 1970, MATH COMPUT, V24, P23, DOI 10.2307/2004873
   Hennig P., 2013, 30 INT C MACH LEARN
   Hensman James, 2012, ADV NEURAL INFORM PR, P2888
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Papoulis A., 1991, PROBABILITY RANDOM V
   Ranganath R, 2013, JMLR W CP, P298
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roux N. L., 2010, P 27 INT C MACH LEAR, P623
   Sarkka S., 2013, BAYESIAN FILTERING S
   Schaul T, 2013, P 30 INT C MACH LEAR, P343
   Schraudolph NN, 1999, IEE CONF PUBL, P569, DOI 10.1049/cp:19991170
   SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840
   Wahba G., 1990, CBMS NSF REGIONAL C
   WOLFE P, 1969, SIAM REV, V11, P226, DOI 10.1137/1011036
   Zhang T., 2004, 21 INT C MACH LEARN
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101013
DA 2019-06-15
ER

PT S
AU Malkomes, G
   Kusner, MJ
   Chen, WL
   Weinberger, KQ
   Moseley, B
AF Malkomes, Gustavo
   Kusner, Matt J.
   Chen, Wenlin
   Weinberger, Kilian Q.
   Moseley, Benjamin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Distributed k-Center Clustering with Outliers on Massive Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Clustering large data is a fundamental problem with a vast number of applications. Due to the increasing size of data, practitioners interested in clustering have turned to distributed computation methods. In this work, we consider the widely used k center clustering problem and its variant used to handle noisy data, k-center with outliers. In the noise-free setting we demonstrate how a previously-proposed distributed method is actually an O(1)-approximation algorithm, which accurately explains its strong empirical performance. Additionally, in the noisy setting, we develop a novel distributed algorithm that is also an O(1)-approximation. These algorithms are highly parallel and lend themselves to virtually any distributed computing framework. We compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions. The algorithms are all one can hope for in distributed settings: they are fast, memory efficient and they match their sequential counterparts.
C1 [Malkomes, Gustavo; Kusner, Matt J.; Chen, Wenlin; Moseley, Benjamin] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
   [Weinberger, Kilian Q.] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.
RP Malkomes, G (reprint author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM luizgustavo@wustl.edu; mkusner@wustl.edu; wenlinchen@wustl.edu;
   kqw4@cornell.edu; bmoseley@wustl.edu
FU CAPES/BR; NSF [IIA-1355406, IIS-1149882, EFRI-1137211]; Google; Yahoo
FX GM was supported by CAPES/BR; MJK and KQW were supported by the NSF
   grants IIA-1355406, IIS-1149882, EFRI-1137211; and BM was supported by
   the Google and Yahoo Research Awards.
CR Agarwal PK, 2008, LECT NOTES COMPUT SC, V5193, P64, DOI 10.1007/978-3-540-87744-8_6
   Aggarwal C. C., 2004, US Patent, Patent No. [6,714,975, 6714975]
   Ailon N., 2009, NIPS, P10
   Andoni A., 2014, STOC, P574, DOI DOI 10.1145/2591796.2591805
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P454, DOI 10.14778/2140436.2140442
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915
   Balcan M. - F., 2013, NIPS, P1995
   Broder A., 2014, P 7 ACM INT C WEB SE, P233, DOI DOI 10.1145/2556195.2556260
   Charikar M, 2001, SIAM PROC S, P642
   Chen M., 2012, P INT C ART INT STAT, V22, P218
   Chierichetti F, 2010, WWW, P231
   da Ponte Barbosa R., 2015, ICML, P1236
   Dean J, 2004, USENIX Association Proceedings of the Sixth Symposium on Operating Systems Design and Implementation (OSDE '04), P137
   Ene A., 2011, P 17 ACM SIGKDD INT, P681, DOI DOI 10.1145/2020408.2020515
   Feldman J, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P710
   GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5
   Guha S, 2003, IEEE T KNOWL DATA EN, V15, P515, DOI 10.1109/TKDE.2003.1198387
   Guha S, 2004, NETWORK THEORY APPLI, V11, P35
   Hassani M., 2009, SENSORKDD 09 WORKSH, P39
   HOCHBAUM DS, 1985, MATH OPER RES, V10, P180, DOI 10.1287/moor.10.2.180
   KARLOFF H, 2010, SODA, V135, P938
   Kaufman L., 1990, FINDING GROUPS DATA
   Kumar R., 2013, P 25 ANN ACM S PAR A, P1
   MeCutchen RM, 2008, LECT NOTES COMPUT SC, V5171, P165
   Mirzasoleiman B., 2013, ADV NEURAL INFORM PR, P2049
   Shindler M., 2011, ADV NEURAL INFORM PR, P2375
   Suri Siddharth, 2011, WWW, P607, DOI DOI 10.1145/1963405.1963491
   Tsanas A, 2010, INT CONF ACOUST SPEE, P594, DOI 10.1109/ICASSP.2010.5495554
   Tyree Stephen, 2011, P 20 INT C WORLD WID, P387
   Zamir O., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P287
   Zhao Z, 2012, INT PARALL DISTRIB P, P390, DOI 10.1109/IPDPS.2012.44
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103035
DA 2019-06-15
ER

PT S
AU Mazumdar, A
   Rawat, AS
AF Mazumdar, Arya
   Rawat, Ankit Singh
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Associative Memory via a Sparse Recovery Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NEURAL-NETWORKS; ALGORITHMS
AB An associative memory is a structure learned from a dataset M of vectors (signals) in a way such that, given a noisy version of one of the vectors as input, the nearest valid vector from M(nearest neighbor) is provided as output, preferably via a fast iterative algorithm. Traditionally, binary (or q-ary) Hopfield neural networks are used to model the above structure. In this paper, for the first time, we propose a model of associative memory based on sparse recovery of signals. Our basic premise is simple. For a dataset, we learn a set of linear constraints that every vector in the dataset must satisfy. Provided these linear constraints possess some special properties, it is possible to cast the task of finding nearest neighbor as a sparse recovery problem. Assuming generic random models for the dataset, we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size O(n). Furthermore, given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates, the vector can be correctly recalled using a neurally feasible algorithm.
C1 [Mazumdar, Arya] Univ Minnesota Twin Cities, Dept ECE, Minneapolis, MN 55455 USA.
   [Rawat, Ankit Singh] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
RP Mazumdar, A (reprint author), Univ Minnesota Twin Cities, Dept ECE, Minneapolis, MN 55455 USA.
EM arya@umn.edu; asrawat@andrew.cmu.edu
CR Agarwal A., 2013, ABS13107991 CORR
   Arora S., 2015, ABS150300778 CORR
   Arora  Sanjeev, 2013, ARXIV13086273
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Foucart S, 2013, MATH INTRO COMPRESSI
   Gripon V, 2011, IEEE T NEURAL NETWOR, V22, P1087, DOI 10.1109/TNN.2011.2146789
   GROSS DJ, 1984, NUCL PHYS B, V240, P431, DOI 10.1016/0550-3213(84)90237-2
   Hebb D. O., 2005, ORG BEHAV NEUROPSYCH
   Hillar C., 2014, ARXIV14114625
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hu T, 2012, NEURAL COMPUT, V24, P2852, DOI 10.1162/NECO_a_00353
   Jankowski S, 1996, IEEE T NEURAL NETWOR, V7, P1491, DOI 10.1109/72.548176
   Karbasi A., 2014, ABS14076513 CORR
   Kumar KR, 2011, 2011 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Maleki A, 2009, ANN ALLERTON CONF, P236, DOI 10.1109/ALLERTON.2009.5394802
   McEliece R. J., 1985, TELECOMMUNICATIONS D, V83, P209
   MCELIECE RJ, 1987, IEEE T INFORM THEORY, V33, P461, DOI 10.1109/TIT.1987.1057328
   Muezzinoglu MK, 2003, IEEE T NEURAL NETWOR, V14, P891, DOI 10.1109/TNN.2003.813844
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Salavati AH, 2012, IEEE INT SYMP INFO
   TANAKA F, 1980, J PHYS F MET PHYS, V10, P2769, DOI 10.1088/0305-4608/10/12/017
   Vershynin  Roman, 2010, ARXIV10113027
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102064
DA 2019-06-15
ER

PT S
AU McInerney, J
   Ranganath, R
   Blei, D
AF McInerney, James
   Ranganath, Rajesh
   Blei, David
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Population Posterior and Bayesian Modeling on Streams
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFERENCE
AB Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which require conditioning on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We develop the population posterior for latent Dirichlet allocation and Dirichlet process mixtures. We study our method with several large-scale data sets.
C1 [McInerney, James; Blei, David] Columbia Univ, New York, NY 10027 USA.
   [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA.
RP McInerney, J (reprint author), Columbia Univ, New York, NY 10027 USA.
EM james@cs.columbia.edu; rajeshr@cs.princeton.edu; david.blei@columbia.edu
FU NSF [IIS-0745520, IIS-1247664, IIS-1009542]; ONR [N00014-11-1-0651];
   DARPA [FA8750-14-2-0009, N66001-15-C-4032]; NDSEG; Facebook; Adobe;
   Amazon; Siebel Scholar Foundation; John Templeton Foundation
FX We thank Allison Chaney, John Cunningham, Alp Kucukelbir, Stephan Mandt,
   Peter Orbanz, Theo Weber, FrankWood, and the anonymous reviewers for
   their comments. This work is supported by NSF IIS-0745520, IIS-1247664,
   IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009,
   N66001-15-C-4032, NDSEG, Facebook, Adobe, Amazon, and the Siebel Scholar
   and John Templeton Foundations.
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bernardo J. M., 2009, BAYESIAN THEORY, V405
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blondel V. D., 2012, ARXIV12100137
   Bottou L., 1998, ONLINE LEARNING NEUR, P9
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Efron B., 1994, INTRO BOOTSTRAP
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   Ghahramani Z., 2000, NIPS 2000 WORKSH ONL, P101
   Hoffman M. D., 2015, INT C ART INT STAT, P101
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Honkela A., 2003, 4 INT S IND COMP AN, P803
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2013, ARXIV13126114
   Ranganath R., 2014, P 17 INT C ART INT S, P805
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Saul LK, 1996, ADV NEUR IN, V8, P486
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Tank A., 2015, INT C ART INT STAT
   Teo C., 2011, P 14 INT C ART INT S, V15, P101
   Theis Lucas, 2015, INT C MACH LEARN
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wallach H., 2009, INT C MACH LEARN
   Yao LM, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P937
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101053
DA 2019-06-15
ER

PT S
AU Meeds, E
   Welling, M
AF Meeds, Edward
   Welling, Max
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Optimization Monte Carlo: Efficient and Embarrassingly Parallel
   Likelihood-Free Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.
C1 [Meeds, Edward; Welling, Max] Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.
   [Welling, Max] Univ Calif Irvine, Donald Bren Sch Informat & Comp Sci, Irvine, CA USA.
   [Welling, Max] Canadian Inst Adv Res, Toronto, ON, Canada.
RP Meeds, E (reprint author), Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.
EM tmeeds@gmail.com; welling.max@gmail.com
CR Ahn S., 2014, P 31 INT C MACH LEAR, V32, P1044
   Ahn S., 2015, KDD
   Beaumont MA, 2009, BIOMETRIKA, V96, P983, DOI 10.1093/biomet/asp052
   Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0
   Bonassi F. V., 2015, BAYESIAN ANAL, V10
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Drovandi CC, 2011, J R STAT SOC C-APPL, V60, P317, DOI 10.1111/j.1467-9876.2010.00747.x
   Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x
   Forneron J.-J., 2015, ARXIV150101265V2
   Forneron J.-J., 2015, ARXIV150604017V1
   GOURIEROUX C, 1993, J APPL ECONOM, V8, pS85, DOI 10.1002/jae.3950080507
   Gutmann M. U., 2015, J MACHINE LEARNING R
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Maclaurin D., 2015, AUTOGRAD
   Meeds E, 2015, UNCERTAINTY IN AI, V31
   Neal P, 2012, STAT COMPUT, V22, P1239, DOI 10.1007/s11222-010-9216-x
   Paige B., 2014, P ADV NEUR INF PROC, V27, P3410
   Shestopaloff A. Y., 2013, TECHNICAL REPORT
   Sisson S, 2007, P NATL ACAD SCI, V104
   Sisson S, 2009, P NATL ACAD SCI, V106
   Snoek J., 2012, ADV NEURAL INFORM PR, V25
   SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102030
DA 2019-06-15
ER

PT S
AU Meshi, O
   Mahdavi, M
   Schwing, AG
AF Meshi, Ofer
   Mahdavi, Mehrdad
   Schwing, Alexander G.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Smooth and Strong: MAP Inference with Linear Convergence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BELIEF PROPAGATION; RELAXATIONS
AB Maximum a-posteriori (MAP) inference is an important task for many applications. Although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years. We focus on linear programming (LP) relaxations, which have achieved state-of-the-art performance in many applications. However, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints.
   Therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity. Specifically, we introduce strong convexity by adding a quadratic term to the LP relaxation objective. We provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum. Further, we propose suitable optimization algorithms and analyze their convergence.
C1 [Meshi, Ofer; Mahdavi, Mehrdad] TTI Chicago, Chicago, IL 60637 USA.
   [Schwing, Alexander G.] Univ Toronto, Toronto, ON, Canada.
RP Meshi, O (reprint author), TTI Chicago, Chicago, IL 60637 USA.
CR Belanger D., 2014, UAI
   Borenstein E., 2004, CVPR
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Frank M., 1956, ALGORITHM QUADRATIC, V3, P95
   Garber  Dan, 2013, ARXIV13014666
   Globerson A., 2008, NIPS
   Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014
   JAGGI M., 2013, P INT C MACH LEARN, V28, P53
   Johnson J. S., 2008, THESIS
   Kappes J. H., 2012, CVPR
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   Komodakis N., 2010, IEEE PAMI
   Kumar MP, 2009, J MACH LEARN RES, V10, P71
   Li Y., 2014, 31 INT C MACH LEARN, P1368
   Martins A., 2011, P 28 INT C MACH LEAR, P169
   Meshi O., 2011, ECML
   Meshi O., 2015, AISTATS
   Meshi O., 2012, NIPS, P3023
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Prusa D, 2013, PROC CVPR IEEE, P1738, DOI 10.1109/CVPR.2013.227
   Ravikumar P, 2010, J MACH LEARN RES, V11, P1043
   Savchynskyy B., 2011, CVPR
   Schwing A. G., 2014, P ICML
   Schwing A. G., 2012, P NIPS
   Shalev-Shwartz S., 2014, ICML
   Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219
   Tarlow D., 2010, INT C ART INT STAT, P812
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938
   Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036
   Werner T, 2010, IEEE T PATTERN ANAL, V32, P1474, DOI 10.1109/TPAMI.2009.134
   Yanover C, 2006, J MACH LEARN RES, V7, P1887
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100078
DA 2019-06-15
ER

PT S
AU Mirzasoleiman, B
   Karbasi, A
   Badanidiyuru, A
   Krause, A
AF Mirzasoleiman, Baharan
   Karbasi, Amin
   Badanidiyuru, Ashwinkumar
   Krause, Andreas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Distributed Submodular Cover: Succinctly Summarizing Massive Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition prevalent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution. However, this sequential, centralized approach is impractical for truly large-scale problems. In this work, we develop the first distributed algorithm - DISCOVER - for submodular set cover that is easily implementable using MapReduce-style computations. We theoretically analyze our approach, and present approximation guarantees for the solutions returned by DISCOVER. We also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including active set selection, exemplar based clustering, and vertex cover on tens of millions of data points using Spark.
C1 [Mirzasoleiman, Baharan; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Karbasi, Amin] Yale Univ, New Haven, CT 06520 USA.
   [Badanidiyuru, Ashwinkumar] Google, Mountain View, CA USA.
RP Mirzasoleiman, B (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
FU ERC StG [307036]; Microsoft Faculty Fellowship; ETH Fellowship
FX This research was supported by ERC StG 307036, a Microsoft Faculty
   Fellowship and an ETH Fellowship.
CR Badanidiyuru Ashwinkumar, 2014, SIGKDD
   Barbosa Rafael, 2015, POWER RANDOMIZATION
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Berger Bonnie, 1994, J COMPUTER SYSTEM SC
   Blelloch Guy E., 2011, SPAA
   Chierichetti F., 2010, WWW
   Dean Jeffrey, 2004, OSDI
   El-Arini K., 2009, KDD
   El-Arini Khalid, 2011, KDD
   Feige U., 1998, J ACM
   Golovin Daniel, 2011, J ARTIFICIAL INTELLI
   Gomes R., 2010, ICML
   Iyer R. K., 2013, ADV NEURAL INFORM PR, P2436
   Kempe David, 2003, P 9 ACM SIGKDD
   Krause A., 2013, TRACTABILITY PRACTIC
   Krause Andreas, 2009, TUT INT JOINT C ART
   Kulesza Alex, 2012, MACH LEARN
   Kumar  Ravi, 2013, SPAA
   Lattanzi Silvio, 2011, SPAA
   Mirrokni V., 2015, STOC
   Mirzasoleiman B., 2015, AAAI
   Mirzasoleiman Baharan, 2013, NIPS
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Stergiou Stergios, 2015, SIGKDD
   Torralba Antonio, 2008, TPAMI
   Tsanas Athanasios, 2010, ICASSP
   Tschiatschek S., 2014, NIPS
   Wolsey Laurence A., 1982, COMBINATORICA
   Yang J, 2015, KNOWL INF SYST, V42, P181, DOI 10.1007/s10115-013-0693-z
   Zaharia Matei, 2010, SPARK CLUSTER COMPUT, P181
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101012
DA 2019-06-15
ER

PT S
AU Mirzazadeh, F
   Ravanbakhsh, S
   Ding, N
   Schuurmans, D
AF Mirzazadeh, Farzaneh
   Ravanbakhsh, Siamak
   Ding, Nan
   Schuurmans, Dale
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Embedding Inference for Structured Multilabel Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CLASSIFICATION
AB A key bottleneck in structured output prediction is the need for inference during training and testing, usually requiring some form of dynamic programming. Rather than using approximate inference or tailoring a specialized inference method for a particular structure-standard responses to the scaling challenge-we propose to embed prediction constraints directly into the learned representation. By eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved, particularly at test time. We demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints, where a relationship to maximum margin structured output prediction can be established. Experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated.
C1 [Mirzazadeh, Farzaneh; Ravanbakhsh, Siamak; Schuurmans, Dale] Univ Alberta, Edmonton, AB, Canada.
   [Ding, Nan] Google, Menlo Pk, CA USA.
RP Mirzazadeh, F (reprint author), Univ Alberta, Edmonton, AB, Canada.
EM mirzazad@ualberta.ca; mravanba@ualberta.ca; dingnan@google.com;
   daes@ualberta.ca
CR Bakir G., 2007, PREDICTING STRUCTURE
   Bi W., 2012, NEURAL INFORM PROCES
   Cisse M., 2013, P ADV NEUR INF PROC
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Dembczynski K., 2010, P ICML
   Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8
   Deng J., 2014, P ECCV
   Deng J., 2010, P EUR C COMP VIS ECC
   Guo Y., 2011, AAAI
   Haeffele B., 2014, INT C MACH LEARN ICM
   Hariharan B, 2012, MACH LEARN, V88, P127, DOI 10.1007/s10994-012-5291-x
   Jancsary J., 2013, P INT C MACH LEARN I
   Joachims T., 1999, ICML
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Kadri H., 2013, P INT C MACH LEARN I
   Kae A., 2013, P CVPR
   Kapoor A., 2012, P ADV NEUR INF PROC
   Klimt B., 2004, ECML
   Lafferty J. D., 2001, INT C MACH LEARN ICM
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li Q., 2013, P INT C MACH LEARN I
   Lin Z., 2014, P INT C MACH LEARN I
   Makela M., 2003, TECHNICAL REPORT
   Mirzazadeh F., 2014, P AAAI
   Rousu J, 2006, J MACH LEARN RES, V7, P1601
   Srikumar V, 2014, ADV NEUR IN, V27
   Sun X., 2014, P NIPS
   Taskar B., 2004, THESIS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Weinberger K., 2008, NEURAL INFORM PROCES
   Weston J., 2011, INT JOINT C ART INT
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100043
DA 2019-06-15
ER

PT S
AU Mittal, H
   Mahajan, A
   Gogate, V
   Singla, P
AF Mittal, Happy
   Mahajan, Anuj
   Gogate, Vibhav
   Singla, Parag
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Lifted Inference Rules with Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Lifted inference rules exploit symmetries for fast reasoning in statistical relational models. Computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference. In this paper, we propose a new constraint language, called setineq, which allows subset, equality and inequality constraints, to represent substitutions over the variables in the theory. Our constraint formulation is strictly more expressive than existing representations, yet easy to operate on. We reformulate the three main lifting rules: decomposer, generalized binomial and the recently proposed single occurrence for MAP inference, to work with our constraint representation. Experiments on benchmark MLNs for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques.
C1 [Mittal, Happy; Mahajan, Anuj; Singla, Parag] IIT Delhi, Dept Comp Sci & Engn, Hauz Khas, New Delhi 110016, India.
   [Gogate, Vibhav] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
RP Mittal, H (reprint author), IIT Delhi, Dept Comp Sci & Engn, Hauz Khas, New Delhi 110016, India.
EM happy.mittal@cse.iitd.ac.in; anujmahajan.iitd@gmail.com;
   vgogate@hlt.utdallas.edu; parags@cse.iitd.ac.in
FU TCS Research Scholar Program; DARPA Probabilistic Programming for
   Advanced Machine Learning Program [FA8750-14-C-0005]; Google travel
   grant
FX Happy Mittal was supported by TCS Research Scholar Program. Vibhav
   Gogate was partially supported by the DARPA Probabilistic Programming
   for Advanced Machine Learning Program under AFRL prime contract number
   FA8750-14-C-0005. Parag Singla is being supported by Google travel grant
   to attend the conference. We thank Somdeb Sarkhel for helpful
   discussions.
CR Apsel Udi, 2014, P AAAI 2014, P2403
   Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319
   Bui Hung Hai, 2013, UAI, P132
   de Salvo Braz R., 2007, INTRO STAT RELATIONA
   Domingos P.M., 2009, SYNTHESIS LECT ARTIF
   Gogate V., 2011, P 27 C UNC ART INT, P256
   Jha Abhay, 2010, P 24 ANN C NEUR INF, P973
   Kersting K., 2009, P 25 C UNC ART INT, P277
   Kisynski J., 2009, P UAI 09
   Mihalkova L., 2007, P 24 INT C MACH LEAR, P625, DOI DOI 10.1145/1273496.1273575.ICML'07
   Milch B., 2008, P AAAI 08
   Mittal H., 2014, P ADV NEUR INF PROC, P649
   Mladenov M., 2015, P UAI 15
   Mladenov M., 2014, P UAI, P603
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Russell S., 2010, ARTIFICIAL INTELLIGE
   Singla P., 2008, P 23 AAAI C ART INT, P1094
   Singla P., 2014, P C ART INT AAAI, P2497
   Taghipour N., 2012, P AISTATS 12
   Van den Broeck G., 2011, P IJCAI 11
   Van den Broeck G., 2013, P NIPS 13
   Van den Broeck G., 2012, P AAAI 12
   Van den Broeck Guy, 2011, ADV NEURAL INFORM PR, P1386
   VandenBroeck G., 2013, THESIS
   Venugopal D., 2012, P 26 AAAI C ART INT, P1910
   Venugopal D., 2012, P NIPS 12, V25, P1664
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102049
DA 2019-06-15
ER

PT S
AU Mohri, M
   Medina, AM
AF Mohri, Mehryar
   Medina, Andres Munoz
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Revenue Optimization against Strategic Buyers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGRET; AUCTIONS
AB We present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his gamma-discounted surplus. In order to analyze this problem we introduce the notion of epsilon-strategic buyer, a more natural notion of strategic behavior than what has been considered in the past. We improve upon the previous state-of-the-art and achieve an optimal regret bound in O(log T + 1/log (1/gamma)) when the seller selects prices from a finite set and provide a regret bound in (O) over tilde (root T + T-1/4/log(1/gamma)) when the prices offered are selected out of the interval [0, 1].
C1 [Mohri, Mehryar] Courant Inst Math Sci, 251 Mercer St, New York, NY 10012 USA.
   [Medina, Andres Munoz] Google Res, New York, NY 10011 USA.
RP Mohri, M (reprint author), Courant Inst Math Sci, 251 Mercer St, New York, NY 10012 USA.
FU NSF [CCF-1535987, IIS-1117591]
FX We thank Afshin Rostamizadeh and Umar Syed for useful discussions about
   the topic of this paper and the NIPS reviewers for their insightful
   comments. This work was partly funded by NSF IIS-1117591 and NSF
   CCF-1535987.
CR Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Amin Kareem, 2013, ADV NEURAL INF PROCE, P1169
   Amin Kareem, 2014, ADV NEURAL INFORM PR, P622
   Arora R., 2012, P ICML
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bikhchandani S., 2012, BE J THEOR ECON, V12, P1935
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Cui Ying, 2011, P 17 ACM SIGKDD INT, P265, DOI DOI 10.1145/2020408.2020454
   Dani V, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P937, DOI 10.1145/1109557.1109660
   Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008
   Kanoria Y, 2014, LECT NOTES COMPUT SC, V8877, P232, DOI 10.1007/978-3-319-13129-0_17
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Medina Andres Munoz, 2014, P 31 INT C MACH LEAR, P262
   Milgrom P., 2004, PUTTING AUCTION THEO
   MILGROM PR, 1982, ECONOMETRICA, V50, P1089, DOI 10.2307/1911865
   Mohri M., 2014, ADV NEURAL INFORM PR, P1871
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Nachbar JH, 1997, ECONOMETRICA, V65, P275, DOI 10.2307/2171894
   Nachbar JH, 2001, SOC CHOICE WELFARE, V18, P303, DOI 10.1007/PL00007181
   VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103064
DA 2019-06-15
ER

PT S
AU Monfort, M
   Lake, BM
   Ziebart, BD
   Lucey, P
   Tenenbaum, JB
AF Monfort, Mathew
   Lake, Brenden M.
   Ziebart, Brian D.
   Lucey, Patrick
   Tenenbaum, Joshua B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Softstar: Heuristic-Guided Probabilistic Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself. This higher-level abstraction improves generalization in different prediction settings, but computing predictions often becomes intractable in large decision spaces. We propose the Soft-star algorithm, a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior. This approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods. We present the algorithm, analyze approximation guarantees, and compare performance with simulation-based inference on two distinct complex decision tasks.
C1 [Monfort, Mathew; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
   [Lake, Brenden M.] NYU, Ctr Data Sci, New York, NY 10003 USA.
   [Lucey, Patrick] Disney Res Pittsburgh, Pittsburgh, PA 15232 USA.
   [Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.
RP Monfort, M (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM mmonfo2@uic.edu; brenden@nyu.edu; bziebart@uic.edu;
   patrick.lucey@disneyresearch.com; jbt@mit.edu
FU National Science Foundation [1227495]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. #1227495, Purposeful Prediction: Co-robot
   Interaction via Understanding Intent and Goals.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Andrew Y. Ng, 2000, P INT C MACH LEARN
   Babes Monica, 2011, INT C MACH LEARN
   Baker Chris L., 2007, C COGN SCI SOC
   Baum L. E., 1972, INEQUALITIES, V3, P1
   BELLMAN R, 1957, J MATH MECH, V6, P679
   Boularias A., 2011, P INT C ART INT STAT, P182
   Byravan Arunkumar, 2015, P INT JOINT C ART IN
   Dechter Rina, 1985, J ACM
   Dijkstra E. W., 1959, NUMERISCHE MATH
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136
   Huang De-An, 2015, AAAI
   Kalman R.K., 1964, Transactions of the ASME. Series D, Journal of Basic Engineering, V86, P51
   Lake B. M., 2013, NIPS
   Monfort M., 2015, AAAI
   Monfort Mathew, 2013, ICML WORKSH ROB LEAR
   Neu G., 2007, P 23 C UNC ART INT, P295
   Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586
   Ratliff ND, 2006, P 23 INT C MACH LEAR, P729
   Vernaza P., 2012, ADV NEURAL INFORM PR, P575
   Ziebart Brian D., 2010, INT C MACH LEARN
   Ziebart Brian D., 2008, ASS ADV ARTIFICAL IN
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102038
DA 2019-06-15
ER

PT S
AU Montanari, A
   Reichman, D
   Zeitouni, O
AF Montanari, Andrea
   Reichman, Daniel
   Zeitouni, Ofer
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the Limitation of Spectral Methods: From the Gaussian Hidden Clique
   Problem to Rank-One Perturbations of Gaussian Tensors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the following detection problem: given a realization of a symmetric matrix X of dimension n, distinguish between the hypothesis that all upper triangular variables are i.i.d. Gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix B of dimension L for which all upper triangular variables are i.i.d. Gaussians with mean 1 and variance 1, whereas all other upper triangular elements of X not in B are i.i.d. Gaussians variables with mean 0 and variance 1. We refer to this as the ` Gaussian hidden clique problem'. When L = (1 + epsilon)root n (epsilon > 0), it is possible to solve this detection problem with probability 1 o(n) (1) by computing the spectrum of X and considering the largest eigenvalue of X. We prove that when L < (1 epsilon)root n no algorithm that examines only the eigenvalues of X can detect the existence of a hidden Gaussian clique, with error probability vanishing as n -> infinity. The result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional Gaussian tensors. In this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal cannot be detected.
C1 [Montanari, Andrea] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Montanari, Andrea] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Reichman, Daniel] Univ Calif Berkeley, Dept Cognit & Brain Sci, Berkeley, CA 94720 USA.
   [Zeitouni, Ofer] Weizmann Inst Sci, Fac Math, IL-76100 Rehovot, Israel.
   [Zeitouni, Ofer] NYU, Courant Inst, New York, NY 10003 USA.
RP Montanari, A (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM montanari@stanford.edu; daniel.reichman@gmail.com;
   ofer.zeitouni@weizmann.ac.il
RI zeitouni, ofer/O-7764-2019
OI zeitouni, ofer/0000-0002-2520-1525
CR Addario-Berry L, 2010, ANN STAT, V38, P3063, DOI 10.1214/10-AOS817
   Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.3.CO;2-K
   Anderson GW, 2010, INTRO RANDOM MATRICE
   Arias-Castro E, 2008, ANN STAT, V36, P1726, DOI 10.1214/07-AOS526
   Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422
   Balakrishnan Sivaraman, 2011, NIPS WORKSH COMP TRA
   Bhamidi S., ARXIV12112284
   Dembo A., ARXIV14094606
   Deshpande Y., 2014, FDN COMPUTATIONAL MA, P1
   Feige U, 2000, RANDOM STRUCT ALGOR, V16, P195
   Feral D, 2007, COMMUN MATH PHYS, V272, P185, DOI 10.1007/s00220-007-0209-3
   FUREDI Z, 1981, COMBINATORICA, V1, P233, DOI 10.1007/BF02579329
   GRIMMETT GR, 1975, MATH PROC CAMBRIDGE, V77, P313, DOI 10.1017/S0305004100051124
   Guionnet A, 2005, J FUNCT ANAL, V222, P435, DOI 10.1016/j.jfa.2004.09.015
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   JERRUM M, 1992, RANDOM STRUCT ALGOR, V3, P347, DOI 10.1002/rsa.3240030402
   Knowles A, 2013, COMMUN PUR APPL MATH, V66, P1663, DOI 10.1002/cpa.21450
   KOLAR M., 2011, ADV NEURAL INFORM PR, P909
   Ma Z, ARXIV13095914
   Montanari A., 2014, ADV NEURAL INFORM PR, P2897
   Onatski A, 2013, ANN STAT, V41, P1204, DOI 10.1214/13-AOS1100
   Talagrand M, 2006, PROBAB THEORY REL, V134, P339, DOI 10.1007/s00440-005-0433-8
   WATERHOUSE WC, 1990, LINEAR ALGEBRA APPL, V128, P97, DOI 10.1016/0024-3795(90)90284-J
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100087
DA 2019-06-15
ER

PT S
AU Moore, DA
   Russell, SJ
AF Moore, David A.
   Russell, Stuart J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Gaussian Process Random Fields
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.
C1 [Moore, David A.; Russell, Stuart J.] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94709 USA.
RP Moore, DA (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94709 USA.
EM dmoore@cs.berkeley.edu; russell@cs.berkeley.edu
FU DTRA [HDTRA-11110026]; Microsoft Research
FX We thank the anonymous reviewers for their helpful suggestions. This
   work was supported by DTRA grant #HDTRA-11110026, and by computing
   resources donated by Microsoft Research under an Azure for Research
   grant.
CR [Anonymous], 2012, GPY GAUSSIAN PROCESS
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   Chalupka K, 2013, J MACH LEARN RES, V14, P333
   Damianou Andreas C., 2015, J MACHINE LEARNING R
   Deisenroth Marc Peter, 2015, INT C MACH LEARN ICM
   Ferris B, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2480
   Gal Yarin, 2014, ADV NEURAL INFORM PR
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   International Seismological Centre, 2015, ON LIN B
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lawrence Neil D, 2007, INT C ART INT STAT A
   Lawrence Neil D, 2004, ADV NEURAL INFORM PR
   McNames J, 2001, IEEE T PATTERN ANAL, V23, P964, DOI 10.1109/34.955110
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Nguyen T., 2014, P 31 INT C MACH LEAR, P145
   Nguyen-Tuong D, 2009, ADV ROBOTICS, V23, P2015, DOI 10.1163/016918609X12529286896877
   Park C, 2011, J MACH LEARN RES, V12, P1697
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2002, ADV NEUR IN, V14, P881
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Snelson E., 2007, ARTIFICIAL INTELLIGE
   Titsias M. K., 2009, INT C ART INT STAT A
   Titsias Michalis K, 2010, INT C ART INT STAT A
   Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908
   Vanhatalo Jarno, 2008, UNCERTAINTY ARTIFICI
   Yedidia Jonathan S, 2001, ADV NEURAL INFORM PR, V13
   Zhong Guoqiang, 2010, AAAI C ART INT
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102023
DA 2019-06-15
ER

PT S
AU Mordatch, I
   Lowrey, K
   Andrew, G
   Popovic, Z
   Todorov, E
AF Mordatch, Igor
   Lowrey, Kendall
   Andrew, Galen
   Popovic, Zoran
   Todorov, Emanuel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Interactive Control of Diverse Complex Characters with Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.
   [GRAPHICS]
   .
C1 [Mordatch, Igor; Lowrey, Kendall; Andrew, Galen; Popovic, Zoran; Todorov, Emanuel] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA.
RP Mordatch, I (reprint author), Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA.
EM mordatch@cs.washington.edu; lowrey@cs.washington.edu;
   galen@cs.washington.edu; zoran@cs.washington.edu;
   todorov@cs.washington.edu
CR Chen P, 2011, SIAM J NUMER ANAL, V49, P1417, DOI 10.1137/100799988
   Geyer H, 2010, IEEE T NEUR SYS REH, V18, P263, DOI 10.1109/TNSRE.2010.2047592
   Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9
   Hinton G. E, 2012, ARXIV12070580
   Hoerzer G. M., 2012, CEREBRAL CORTEX
   Huh D, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P42
   Ijspeert A. J., 2008, CENTRAL PATTERN GENE
   Ju E, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516976
   Levine S., 2014, ICML 14
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   MORDATCH I., 2014, ROBOTICS SCI SYSTEMS
   Mordatch I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185539
   Rebula JR, 2007, IEEE INT CONF ROBOT, P1467, DOI 10.1109/ROBOT.2007.363191
   SCHULMAN J., 2015, CORR
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Vukobratovi P. M., 2004, INT J HUM ROBOT, V1, P157, DOI DOI 10.1142/S0219843604000083
   Wager  Stefan, 2013, ADV NEURAL INFORM PR
   Wang JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778810
   Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101024
DA 2019-06-15
ER

PT S
AU Morgenstern, J
   Roughgarden, T
AF Morgenstern, Jamie
   Roughgarden, Tim
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Pseudo-Dimension of Near-Optimal Auctions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MECHANISM DESIGN
AB This paper develops a general approach, rooted in statistical learning theory, to learning an approximately revenue-maximizing auction from data. We introduce t-level auctions to interpolate between simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby balancing the competing demands of expressivity and simplicity. We prove that such auctions have small representation error, in the sense that for every product distribution F over bidders' valuations, there exists a t-level auction with small t and expected revenue close to optimal. We show that the set of t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning error. One consequence of our results is that, in arbitrary single-parameter settings, one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.
C1 [Morgenstern, Jamie] Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.
   [Roughgarden, Tim] Stanford Univ, Palo Alto, CA 94304 USA.
RP Morgenstern, J (reprint author), Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.
EM jamiemor@cis.upenn.edu; tim@cs.stanford.edu
FU Simons Award for Graduate Students in Theoretical Computer Science; NSF
   [CCF-1415460]
FX Part of this work done while visiting Stanford University. Partially
   supported by a Simons Award for Graduate Students in Theoretical
   Computer Science, as well as NSF grant CCF-1415460.
CR Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Babaioff M, 2014, SI GECOM EXCH, V13, P31
   Balcan MF, 2008, J COMPUT SYST SCI, V74, P1245, DOI 10.1016/j.jcss.2007.08.002
   Balcan Maria-Florina, 2007, TECHNICAL REPORT
   Cai Y, 2011, ANN IEEE SYMP FOUND, P522, DOI 10.1109/FOCS.2011.76
   Chawla S, 2010, ACM S THEORY COMPUT, P311
   Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Devanur Nikhil, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P122, DOI 10.1007/978-3-642-25510-6_11
   DHANGWATNOTAI P., 2010, P 11 ACM C EL COMM E, P129
   Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22
   Elkind E, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P736
   Hartline Jason, 2015, MECH DESIGN APPROXIM
   Hartline Jason D., 2009, ACM C EL COMM STANF
   Huang Zhiyi, 2014, ABS14072479
   Kearns M. J., 1994, INTRO COMPUTATIONAL
   Medina Andres Munoz, 2014, P 31 INT C MACH LEAR, P262
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Pollard D., 1984, CONVERGENCE STOCHAST
   ROUGHGARDEN T., 2012, P 13 ACM C EL COMM, P844
   Roughgarden Tim, 2015, IRONING DARK UNPUB
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Yao A. C., 2015, P 26 ANN ACM SIAM S, P92
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101026
DA 2019-06-15
ER

PT S
AU Mueller, J
   Jaakkola, T
AF Mueller, Jonas
   Jaakkola, Tommi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Principal Differences Analysis: Interpretable Characterization of
   Differences between Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SHRINKAGE; SELECTION
AB We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.
C1 [Mueller, Jonas; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Mueller, J (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM jonasmueller@csail.mit.edu; tommi@csail.mit.edu
FU NIH [T32HG004947]
FX This research was supported by NIH Grant T32HG004947.
CR Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas D. P., 1998, NETWORK OPTIMIZATION
   Bertsekas DP, 2012, OPTIMIZATION FOR MACHINE LEARNING, P85
   BERTSEKAS DP, 1988, MATH PROGRAM, V42, P203, DOI 10.1007/BF01589405
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82
   Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118
   Cramer H., 1936, J LOND MATH SOC, Vs1-11, P290, DOI 10.1112/jlms/s1-11.4.290
   Cuesta-Albertos JA, 2007, J THEOR PROBAB, V20, P201, DOI 10.1007/s10959-007-0060-7
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Geiler-Samerotte KA, 2013, CURR OPIN BIOTECH, V24, P752, DOI 10.1016/j.copbio.2013.03.010
   Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865
   Good P., 1994, PERMUTATION TESTS PR
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guyon I., 2006, FEATURE EXTRACTION F
   Jirak M, 2011, J MULTIVARIATE ANAL, V102, P1032, DOI 10.1016/j.jmva.2011.02.003
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Lopes M., 2011, ADV NEURAL INFORM PR, P1206
   Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x
   Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18
   Szekely G., 2004, INTERSTAT, V5
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   Wang Zhaoran, 2014, Adv Neural Inf Process Syst, V2014, P3383
   Wei S, 2015, J COMPUTATIONAL GRAP
   Wright SJ, 2010, OPTIMIZATION ALGORIT
   Zeisel A, 2015, SCIENCE, V347, P1138, DOI 10.1126/science.aaa1934
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102043
DA 2019-06-15
ER

PT S
AU Narasimhan, H
   Parkes, DC
   Singer, Y
AF Narasimhan, Harikrishna
   Parkes, David C.
   Singer, Yaron
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learnability of Influence in Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We show PAC learnability of influence functions for three common influence models, namely, the Linear Threshold (LT), Independent Cascade (IC) and Voter models, and present concrete sample complexity results in each case. Our results for the LT model are based on interesting connections with neural networks; those for the IC model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments; and those for the Voter model are based on a reduction to linear regression. We show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced. We also provide efficient polynomial time learning algorithms for a setting with full observation, i.e. where the cascades also contain the time steps in which nodes are influenced.
C1 [Narasimhan, Harikrishna; Parkes, David C.; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
RP Narasimhan, H (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM hnarasimhan@seas.harvard.edu; parkes@seas.harvard.edu;
   yaron@seas.harvard.edu
FU Indo-US Science & Technology Forum; NSF [CCF-1301976]; Google Faculty
   Research Award; NSF CAREER [CCF-1452961]
FX Part of this work was carried out while HN was visiting Harvard as a
   part of a student visit under the Indo-US Joint Center for Advanced
   Research in Machine Learning, Game Theory & Optimization supported by
   the Indo-US Science & Technology Forum. HN thanks Kevin Murphy, Shivani
   Agarwal and Harish G. Ramaswamy for helpful discussions. YS and DP were
   supported by NSF grant CCF-1301976 and YS by CAREER CCF-1452961 and a
   Google Faculty Research Award.
CR Abrahao B. D., 2013, KDD
   Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Balcan M., 2011, STOC
   Bartlett Peter L., 1995, HDB BRAIN THEORY NEU, P1188
   Daneshmand  H., 2014, ICML
   De Abir, 2014, CIKM 2014
   Domingos P., 2001, KDD
   Du Nan, 2013, NIPS
   Du Nan, 2014, ICML
   Du Nan, 2012, NIPS
   Even-Dar E, 2011, INFORM PROCESS LETT, V111, P184, DOI 10.1016/j.ipl.2010.11.015
   Feldman V., 2014, COLT
   Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741
   Gomez-Rodriguez Manuel, 2011, ICML
   Goyal Amit, 2010, KDD
   Honorio J, 2015, J MACH LEARN RES, V16, P1157
   Kempe D., 2003, KDD
   Mossel Elchanan, 2007, STOC
   Netrapalli Praneeth, 2012, SIGMETRICS
   Pouget-Abadie Jean, 2015, ICML
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Zhang T, 2004, ANN STAT, V32, P56
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103027
DA 2019-06-15
ER

PT S
AU Neyshabur, B
   Salakhutdinov, R
   Srebro, N
AF Neyshabur, Behnam
   Salakhutdinov, Ruslan
   Srebro, Nathan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Path-SGD: Path-Normalized Optimization in Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and Ada-Grad.
C1 [Neyshabur, Behnam; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
   [Salakhutdinov, Ruslan] Univ Toronto, Dept Stat, Toronto, ON, Canada.
   [Salakhutdinov, Ruslan] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
RP Neyshabur, B (reprint author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
EM bneyshabur@ttic.edu; rsalakhu@cs.toronto.edu; nati@ttic.edu
FU NSF [IIS-1302662]; Intel ICRI-CI
FX Research was partially funded by NSF award IIS-1302662 and Intel
   ICRI-CI. We thank Ryota Tomioka and Hao Tang for insightful discussions
   and Leon Bottou for pointing out the connection to natural gradient.
CR Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Glorot X., 2010, AISTATS
   Goodfellow Ian, 2013, JMLR W CP, P1319
   He K., 2015, ARXIV150201852
   Ioffe S., 2015, BATCH NORMALIZATION
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, V1, P7
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Martens James, 2015, ICML
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Neyshabur B., 2015, COLT
   Neyshabur Behnam, 2015, INT C LEARN REPR ICL
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Srebro  N., 2011, ADV NEURAL INFORM PR, P2645
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever  I., 2013, ICML
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101057
DA 2019-06-15
ER

PT S
AU Norouzi, M
   Collins, MD
   Johnson, M
   Fleet, DJ
   Kohli, P
AF Norouzi, Mohammad
   Collins, Maxwell D.
   Johnson, Matthew
   Fleet, David J.
   Kohli, Pushmeet
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Non-greedy Optimization of Decision Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d(2)), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.
C1 [Norouzi, Mohammad; Fleet, David J.] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
   [Collins, Maxwell D.] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
   [Johnson, Matthew; Kohli, Pushmeet] Microsoft Res, New York, NY USA.
RP Norouzi, M (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
FU Google fellowship; NSERC Canada; NCAP program of the CIFAR
FX MN was financially supported in part by a Google fellowship. DF was
   financially supported in part by NSERC Canada and the NCAP program of
   the CIFAR.
CR Bennett  K., 1994, COMPUTING SCI STAT, P156
   Bennett K. P., 1997, 97100 RENSS POL I DE, P2396
   Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Chang Chih-Chung, 2001, LIBSVM LIB SUPPORT V
   Criminisi A., 2013, DECISION FORESTCOM
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8
   Jancsary  J., 2012, ECCV
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Konukoglu E, 2012, LECT NOTES COMPUT SC, V7512, P75, DOI 10.1007/978-3-642-33454-2_10
   Lakshminarayanan B., 2014, ADV NEURAL INFORM PR, P3140
   Mingers J., 1989, Machine Learning, V4, P227, DOI 10.1023/A:1022604100933
   Murthy S. K., 1995, THESIS
   Norouzi M., 2011, ICML
   Norouzi  M., 2015, ARXIV150606155
   Nowozin S., 2012, ICML
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Shotton J., 2013, IEEE T PAMI
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2004, ICML
   Yu C. N, 2009, ICML
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102035
DA 2019-06-15
ER

PT S
AU Oh, S
   Thekumparampil, KK
   Xu, JM
AF Oh, Sewoong
   Thekumparampil, Kiran K.
   Xu, Jiaming
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Collaboratively Learning Preferences from Ordinal Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.
C1 [Oh, Sewoong; Thekumparampil, Kiran K.] Univ Illinois, Champaign, IL 61820 USA.
   [Xu, Jiaming] UPenn, Wharton Sch, Philadelphia, PA USA.
RP Oh, S (reprint author), Univ Illinois, Champaign, IL 61820 USA.
EM swoh@illinois.edu; thekump2@illinois.edu; jiamingx@wharton.upenn.edu
FU NSF CMMI award [MES-1450848]; NSF SaTC award [CNS-1527754]
FX This research is supported in part by NSF CMMI award MES-1450848 and NSF
   SaTC award CNS-1527754.
CR Agarwal A., 2010, ADV NEURAL INFORM PR, P37
   Ammar A., 2014, P ACM SIGMETRICS INT
   Boucheron S., 2013, CONCENTRATION INEQUA
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Caron F, 2012, J COMPUT GRAPH STAT, V21, P174, DOI 10.1080/10618600.2012.638220
   Diao H., 2013, ADV NEURAL INFORM PR, P73
   Ding W., 2014, TECHNICAL REPORT
   Gormley IC, 2009, BAYESIAN ANAL, V4, P265, DOI 10.1214/09-BA410
   Guiver J., 2009, P 26 ANN INT C MACH, P377
   HAJEK B., 2014, ADV NEURAL INFORM PR, P1475
   Hunter DR, 2004, ANN STAT, V32, P384
   Ledoux M., 2005, CONCENTRATION MEASUR
   Lu Yu, 2014, ARXIV14100860
   Luce D., 1959, INDIVIDUAL CHOICE BE
   Marschak Jacob, 1960, P S MATH METH SOC SC, V7, P19
   McFadden D., 1973, CONDITIONAL LOGIT AN
   Negahban S., 2012, NIPS, P2483
   Negahban S., 2012, J MACHINE LEARNING R
   Oh S., 2014, ADV NEURAL INFORM PR, P595
   Park Dohyung, 2015, PREFERENCE COMPLETIO
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Soufiani H.A., 2012, NIPS, P126
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Tropp J., 2011, USER FRIENDLY TAIL B
   van de Geer S. A, 2000, EMPIRICAL PROCESSES, V6
   Wu Rui, 2015, ARXIV150204631
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101077
DA 2019-06-15
ER

PT S
AU Ohsaka, N
   Yoshida, Y
AF Ohsaka, Naoto
   Yoshida, Yuichi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Monotone k-Submodular Function Maximization with Size Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID FUNCTION SUBJECT
AB A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain. Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions. In this paper, we give constant-factor approximation algorithms for maximizing monotone k-submodular functions subject to several size constraints. The running time of our algorithms are almost linear in the domain size. We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.
C1 [Ohsaka, Naoto] Univ Tokyo, Tokyo, Japan.
   [Yoshida, Yuichi] Natl Inst Informat & Preferred Infrastruct Inc, Tokyo, Japan.
RP Ohsaka, N (reprint author), Univ Tokyo, Tokyo, Japan.
EM ohsaka@is.s.u-tokyo.ac.jp; yyoshida@nii.ac.jp
FU JSPS [26730009]; MEXT [24106003]; JST, ERATO, Kawarabayashi Large Graph
   Project
FX Y. Y. is supported by JSPS Grant-in-Aid for Young Scientists (B) (No.
   26730009), MEXT Grant-in-Aid for Scientific Research on Innovative Areas
   (24106003), and JST, ERATO, Kawarabayashi Large Graph Project. N. O. is
   supported by JST, ERATO, Kawarabayashi Large Graph Project.
CR Barbieri N, 2012, IEEE DATA MINING, P81, DOI 10.1109/ICDM.2012.122
   Buchbinder N, 2012, ANN IEEE SYMP FOUND, P649, DOI 10.1109/FOCS.2012.73
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57
   Filmus Y, 2014, SIAM J COMPUT, V43, P514, DOI 10.1137/130920277
   Goldenberg J, 2001, MARKET LETT, V12, P211, DOI 10.1023/A:1011122126881
   Goldenberg J., 2001, ACAD MARK SCI REV, V9, P1
   Gridchyn I, 2013, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2013.288
   Huber Anna, 2012, Combinatorial Optimization. Second International Symposium, ISCO 2012. Revised Selected Papers, P451, DOI 10.1007/978-3-642-32147-4_40
   Iwata S., 2016, SODA
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2008, J MACH LEARN RES, V9, P2761
   Lin IL, 2010, HUMAN LANGUAGE TECHN, P912
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   Mirzasoleiman B., 2015, P 29 AAAI C ART INT, P1812
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Richardson M., 2002, P 8 ACM SIGKDD INT C, V2, P61, DOI DOI 10.1145/775047.775057
   Singh A., 2012, P AISTATS, P1055
   Sviridenko M, 2004, OPER RES LETT, V32, P41, DOI 10.1016/S0167-6377(03)00062-2
   Ward J., 2014, SODA, P1468
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100077
DA 2019-06-15
ER

PT S
AU Orlitsky, A
   Suresh, AT
AF Orlitsky, Alon
   Suresh, Ananda Theertha
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Competitive Distribution Estimation: Why is Good-Turing Good
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Estimating distributions over large alphabets is a fundamental machine-learning tenet. Yet no method is known to estimate all distributions well. For example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, Jelinek-Mercer, and Good-Turing are not known to be near optimal for essentially any distribution.
   We describe the first universally near-optimal probability estimators. For every discrete distribution, they are provably nearly the best in the following two competitive ways. First they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation. Second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times.
   Specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of Good-Turing estimator is always within KL divergence of (3 + o(n) (1))/n(1/3) from the best estimator, and that a more involved estimator is within (O) over tilde (n) (min(k/n, 1 root n)). Conversely, we show that any estimator must have a KL divergence at least (Omega) over tilde (n) (min(k/n, 1/n(2/3))) over the best estimator for the first comparison, and at least (Omega) over tilde (n) (min(k/n; 1/root n)) for the second.
C1 [Orlitsky, Alon; Suresh, Ananda Theertha] Univ Calif San Diego, San Diego, CA 92093 USA.
RP Orlitsky, A (reprint author), Univ Calif San Diego, San Diego, CA 92093 USA.
EM alon@ucsd.edu; asuresh@ucsd.edu
CR Abramovich Felix, 2006, ANN STAT
   Acharya J., 2012, COLT
   Acharya J., 2013, AISTATS
   Acharya J., 2011, COLT
   Acharya J., 2013, COLT
   Barron A, 1999, PROBAB THEORY REL, V113, P301, DOI 10.1007/s004400050210
   Bickel P. J., 1993, EFFICIENT ADAPTIVE E
   Bontemps D, 2014, IEEE T INFORM THEORY, V60, P808, DOI 10.1109/TIT.2013.2288914
   Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010
   Chen S. F., 1996, ACL
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Drukh E., 2004, COLT
   Gale William A., 1995, J QUANT LINGUIST, V2, P217, DOI DOI 10.1080/09296179508590051
   Gassiat Elisabeth, 2014, CORR
   GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344
   Jelinek Fredrick, 1984, IBM TECH DISCLOSURE
   Kamath S., 2015, COLT
   Krichevsky R., 1994, UNIVERSAL COMPRESSIO
   Mitzenmacher M., 2005, PROBABILITY COMPUTIN
   NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001
   Orlitsky Alon, 2003, FOCS
   Paninski L., 2004, NIPS
   Ryabko B. Y., 1990, PROBL PEREDACHI INF, V26, P24
   Ryabko Boris Yakovlevich, 1984, PROBLEMY PEREDACHI I
   Schapire Robert E., 2000, COLT
   Thomas M., 2006, ELEMENTS INFORM THEO
   Tsybakov Alexandre B., 2004, INTRO NONPARAMETRIC
   Valiant G., 2014, FOCS
   Valiant Gregory, 2015, ABS150405321 CORR, Vabs/1504.05321
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101022
DA 2019-06-15
ER

PT S
AU Pan, XH
   Papailiopoulos, D
   Oymak, S
   Recht, B
   Ramchandran, K
   Jordan, MI
AF Pan, Xinghao
   Papailiopoulos, Dimitris
   Oymak, Samet
   Recht, Benjamin
   Ramchandran, Kannan
   Jordan, Michael I.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Parallel Correlation Clustering on Big Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Given a similarity graph between items, correlation clustering (CC) groups similar items together and dissimilar ones apart. One of the most popular CC algorithms is KwikCluster: an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio. Unfortunately, in practice KwikCluster requires a large number of clustering rounds, a potential bottleneck for large graphs.
   We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly linear speedups. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3 approximation ratio.
   We demonstrate experimentally that both algorithms outperform the state of the art, both in terms of clustering accuracy and running time. We show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15 x speedup.
C1 [Pan, Xinghao; Papailiopoulos, Dimitris; Oymak, Samet; Recht, Benjamin; Jordan, Michael I.] Univ Calif Berkeley, AMP Lab, Berkeley, CA 94720 USA.
   [Pan, Xinghao; Papailiopoulos, Dimitris; Oymak, Samet; Recht, Benjamin; Ramchandran, Kannan; Jordan, Michael I.] Univ Calif Berkeley, EECS, Berkeley, CA USA.
   [Recht, Benjamin; Jordan, Michael I.] Univ Calif Berkeley, Stat, Berkeley, CA USA.
RP Pan, XH (reprint author), Univ Calif Berkeley, AMP Lab, Berkeley, CA 94720 USA.
CR Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513
   Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9
   Arasu A, 2009, PROC INT CONF DATA, P952, DOI 10.1109/ICDE.2009.43
   Bansal N, 2002, ANN IEEE SYMP FOUND, P238, DOI 10.1109/SFCS.2002.1181947
   Ben-Dor A, 1999, J COMPUT BIOL, V6, P281, DOI 10.1089/106652799318274
   Blelloch G.E., 2012, P 24 ANN ACM S PAR A, P308, DOI DOI 10.1145/2312005.2312058
   Boldi P, 2004, SOFTWARE PRACT EXPER, V34, P711, DOI 10.1002/spe.587
   Boldi P., 2004, WWW
   Boldi P., 2011, WWW
   Bonchi F., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P51, DOI 10.1109/ICDM.2011.114
   Bonchi F., 2012, P 18 ACM SIGKDD INT, P1321
   Bonchi Francesco, 2014, P 20 ACM SIGKDD INT, P1972
   Cesa-Bianchi N., 2012, ANN C LEARN THEOR MI, P34
   Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39
   Charikar M, 2003, ANN IEEE SYMP FOUND, P524, DOI 10.1109/SFCS.2003.1238225
   Chawla S, 2015, P 47 ANN ACM S THEOR, P219, DOI DOI 10.1145/2746539.2746604
   Chierichetti F., 2014, P 20 ACM SIGKDD INT, P641
   Demaine ED, 2006, THEOR COMPUT SCI, V361, P172, DOI 10.1016/j.tcs.2006.05.008
   Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581
   Elsner M., 2009, P WORKSH INT LIN PRO, P19
   Giotis I, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1167, DOI 10.1145/1109557.1109686
   Hussain Bilal, 2013, TECHNICAL REPORT
   Krivelevich Michael, 2014, ARXIV14045731
   Pan X., 2013, ADV NEURAL INFORM PR, P1403
   Puleo Gregory J, 2014, ARXIV14110547
   Swamy Chaitanya, 2004, P 15 ANN ACM SIAM S, P526
   Yang B, 2007, IEEE T KNOWL DATA EN, V19, P1333, DOI [10.1109/TKDE.2007.1061, 10.1109/TKDE.2007.1061.]
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101111
DA 2019-06-15
ER

PT S
AU Pan, YP
   Theodorou, EA
   Kontitsis, M
AF Pan, Yunpeng
   Theodorou, Evangelos A.
   Kontitsis, Michail
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sample Efficient Path Integral Control under Uncertainty
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a data-driven optimal control framework that is derived using the path integral (PI) control approach. We find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model. The proposed algorithm operates in a forward-backward manner which differentiate it from other PI-related methods that perform forward sampling to find optimal controls. Our method uses significantly less samples to find analytic control laws compared to other approaches within the PI control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion. In addition, the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework. We provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework.
C1 [Pan, Yunpeng; Theodorou, Evangelos A.; Kontitsis, Michail] Georgia Inst Technol, Sch Aerosp Engn, Autonomous Control & Decis Syst Lab, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
RP Pan, YP (reprint author), Georgia Inst Technol, Sch Aerosp Engn, Autonomous Control & Decis Syst Lab, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
EM ypan37@gatech.edu; evangelos.theodorou@gatech.edu; kontitsis@gatech.edu
FU NSF [NRI-1426945]
FX This research is supported by NSF NRI-1426945.
CR Barto A. G., 2004, HDB LEARNING APPROXI
   Bertsekas D. P., 1996, ATHENA SCI, V7, P15
   Candela J. Quinonero, 2003, IEEE INT C AC SPEECH
   Deisenroth M., 2015, IEEE T PATTERN ANAL, V27, P75
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Dvijotham K., 2012, REINFORCEMENT LEARNI, P119
   Fleming W. H., 1993, APPL MATH
   Fleming W. H., 1971, APPL MATH OPT, V9, P329, DOI DOI 10.1007/BF01442148AM0MBN1432-0606
   Gomez Vicenc, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P482, DOI 10.1007/978-3-662-44848-9_31
   Hennig P., 2011, ADV NEURAL INFORM PR, P325
   Kamthe S, 2014, INT CONF ACOUST SPEE
   Kappen HJ, 2007, AIP CONF PROC, V887, P149
   Kappen HJ, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11011
   Kappen HJ, 2005, PHYS REV LETT, V95, DOI [10.1103/PhysRevLett.95.200201, 10.1103/PhysRevLett.200201]
   Levine S., 2014, P 31 INT C MACH LEAR, P829
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Pan Y., 2014, ADV NEURAL INFORM PR, P1907
   Pan YP, 2014, 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), P63
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rawlik K., 2013, P 23 INT JOINT C ART, P1628
   Schulman J., 2015, ARXIV150205477
   Stulp F., 2012, P 29 INT C MACH LEAR, P281
   Theodorou EA, 2012, IEEE DECIS CONTR P, P1466, DOI 10.1109/CDC.2012.6426381
   Theodorou EA, 2010, J MACH LEARN RES, V11, P3137
   Thijssen S, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.032104
   Todorov E., 2009, ADV NEURAL INFORM PR, V22, P1856
   Todorov E, 2009, P NATL ACAD SCI USA, V106, P11478, DOI 10.1073/pnas.0710743106
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103022
DA 2019-06-15
ER

PT S
AU Papa, G
   Clemencon, S
   Bellet, A
AF Papa, Guillaume
   Clemencon, Stephan
   Bellet, Aurelien
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI SGD Algorithms based on Incomplete U-statistics: Large-Scale
   Minimization of Empirical Risk
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In many learning problems, ranging from clustering to ranking through metric learning, empirical estimates of the risk functional consist of an average over tuples (e.g., pairs or triplets) of observations, rather than over individual observations. In this paper, we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems. We argue that in the large-scale setting, gradient estimates should be obtained by sampling tuples of data points with replacement (incomplete U-statistics) instead of sampling data points without replacement (complete U-statistics based on subsamples). We develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the Stochastic Gradient Descent (SGD) algorithm. It reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost. Beyond the rate bound analysis, experiments on AUC maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach.
C1 [Papa, Guillaume; Clemencon, Stephan] Univ Paris Saclay, CNRS, Telecom ParisTech, LTCI, F-75013 Paris, France.
   [Bellet, Aurelien] INRIA Lille Nord Europe, Magnet Team, F-59650 Villeneuve Dascq, France.
RP Papa, G (reprint author), Univ Paris Saclay, CNRS, Telecom ParisTech, LTCI, F-75013 Paris, France.
EM guillaume.papa@telecom-paristech.fr;
   stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr
FU chair Machine Learning for Big Data of Telecom ParisTech
FX This work was supported by the chair Machine Learning for Big Data of
   Telecom ParisTech, and was conducted when A. Bellet was affiliated with
   Telecom ParisTech.
CR Bach F. R., 2011, NIPS
   Bellet A., 2013, ARXIV13066709
   Bellet A., 2015, METRIC LEARNING
   Bottou L., 2007, NIPS
   Clemencon S., 2011, ADV NEURAL INFORM PR, P37
   Clemencon S., 2013, SDM
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Defazio A., 2014, NIPS
   Delyon B., 2000, STOCHASTIC APPROXIMA
   Fort G., 2014, ESAIMPS
   Furnkranz J, 2009, LECT NOTES ARTIF INT, V5781, P359
   Herschtal Alan, 2004, P 21 INT C MACH LEAR, P49
   JANSON S, 1984, Z WAHRSCHEINLICHKEIT, V66, P495, DOI 10.1007/BF00531887
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kar P., 2013, ICML
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35
   Lee AJ, 1990, U STAT THEORY PRACTI
   Mairal J., 2014, ARXIV14024419
   Needell D., 2014, P ADV NEUR INF PROC, P1017
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Norouzi M., 2012, P ADV NEUR INF PROC, V25, P1070
   Pelletier M., 1998, ANN APPL PROB
   Qian Q, 2015, MACH LEARN, V99, P353, DOI 10.1007/s10994-014-5456-x
   Schmidt M. W., 2012, NIPS
   Sibony E, 2014, IEEE INT CONF BIG DA
   Zhao P., 2015, ICML
   Zhao Peilin, 2011, P 28 INT C MACH LEAR, P233
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101078
DA 2019-06-15
ER

PT S
AU Park, CC
   Kim, G
AF Park, Cesc Chunseong
   Kim, Gunhee
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Expressing an Image Stream with a Sequence of Natural Sentences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.
C1 [Park, Cesc Chunseong; Kim, Gunhee] Seoul Natl Univ, Seoul, South Korea.
RP Park, CC (reprint author), Seoul Natl Univ, Seoul, South Korea.
EM park.chunseong@snu.ac.kr; gunheeg@snu.ac.kr
FU Hancom; National Research Foundation of Korea [2015R1C1A1A02036562]
FX This research is partially supported by Hancom and Basic Science
   Research Program through National Research Foundation of Korea
   (2015R1C1A1A02036562).
CR Barzilay R., 2008, ACL
   Bird S., 2009, NATURAL LANGUAGE PRO
   Chen X, 2015, CVPR
   Choi F. Y. Y., 2001, EMNLP
   Donahue J., 2015, CVPR
   Gong  Y., 2014, ECCV
   He K., 2015, DELVING DEEP RECTIFI
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Karpathy A., 2015, CVPR
   Kim G, 2015, CVPR
   Kiros R., 2014, ICML
   Krizhevsky A., 2012, NIPS
   Kulkarni  G., 2011, CVPR
   Kuznetsova P., 2014, TACL
   Lavie S. B. A., 2005, ACL
   Le Q. V., 2014, ICML
   Manning Christopher D, 2014, ACL
   Mao J, 2015, ICLR
   Mikolov T., 2012, THESIS
   Ordonez V., 2011, NIPS
   Papineni K., 2002, ACL
   Rohrbach M., 2013, ICCV
   Schuster M., 1997, IEEE TSP
   Simonyan Karen, 2015, ICLR
   Socher R., 2013, TACL
   Srivastava N., 2012, NIPS
   Tieleman T., 2012, COURSERA
   Vedantam R., 2014, ARXIV14115726
   Vinyals O, 2015, CVPR
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Xu R., 2015, AAAI
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101036
DA 2019-06-15
ER

PT S
AU Park, G
   Raskutti, G
AF Park, Gunwoong
   Raskutti, Garvesh
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Large-Scale Poisson DAG Models based on OverDispersion Scoring
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we address the question of identifiability and learning algorithms for large-scale Poisson Directed Acyclic Graphical (DAG) models. We define general Poisson DAG models as models where each node is a Poisson random variable with rate parameter depending on the values of the parents in the underlying DAG. First, we prove that Poisson DAG models are identifiable from observational data, and present a polynomial-time algorithm that learns the Poisson DAG model under suitable regularity conditions. The main idea behind our algorithm is based on overdispersion, in that variables that are conditionally Poisson are overdispersed relative to variables that are marginally Poisson. Our algorithms exploits overdispersion along with methods for learning sparse Poisson undirected graphical models for faster computation. We provide both theoretical guarantees and simulation results for both small and large-scale DAGs.
C1 [Park, Gunwoong] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.
   [Raskutti, Garvesh] Univ Wisconsin, Optimizat Grp, Wisconsin Inst Discovery, Dept Comp Sci,Dept Stat, Madison, WI 53706 USA.
RP Park, G (reprint author), Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.
EM parkg@stat.wisc.edu; raskutti@cs.wisc.edu
CR Aliferis C F, 2003, AMIA Annu Symp Proc, P21
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Chickering D.M, 1996, LEARNING DATA ARTIFI, P121, DOI DOI 10.1007/978-1-4612-2404-4_12
   Cowell R. G., 1999, PROBABILISTIC NETWOR
   DEAN CB, 1992, J AM STAT ASSOC, V87, P451, DOI 10.2307/2290276
   Friedman J., 2009, R PACKAGE VERSION, V1
   Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P206
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Peters J., 2012, ARXIV12023757
   Peters J., 2013, BIOMETRIKA
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Spirtes P., 2000, CAUSATION PREDICTION
   Tsamardinos I, 2003, P 9 INT WORKSH ART I
   Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7
   Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080
   Verma T., 1991, UNCERTAINTY ARTIFICI, V6, P255
   Yang E., 2012, ADV NEURAL INFORM PR, V25, P1358
   Zheng T, 2006, J AM STAT ASSOC, V101, P409, DOI 10.1198/016214505000001168
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102045
DA 2019-06-15
ER

PT S
AU Park, M
   Jitkrittum, W
   Qamar, A
   Szabo, Z
   Buesing, L
   Sahani, M
AF Park, Mijung
   Jitkrittum, Wittawat
   Qamar, Ahmad
   Szabo, Zoltan
   Buesing, Lars
   Sahani, Maneesh
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bayesian Manifold Learning: The Locally Linear Latent Variable Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.
C1 [Park, Mijung; Jitkrittum, Wittawat; Qamar, Ahmad; Szabo, Zoltan; Buesing, Lars; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London, England.
   [Qamar, Ahmad] Thread Genius, New York, NY USA.
   [Buesing, Lars] Google DeepMind, London, England.
RP Park, M (reprint author), UCL, Gatsby Computat Neurosci Unit, London, England.
EM mijung@gatsby.ucl.ac.uk; wittawat@gatsby.ucl.ac.uk; atqamar@gmail.com;
   zoltan.szabo@gatsby.ucl.ac.uk; lbuesing@google.com;
   maneesh@gatsby.ucl.ac.uk
FU Gatsby Charitable Foundation
FX The authors were funded by the Gatsby Charitable Foundation.
CR Balasubramanian M, 2002, SCIENCE, V295
   Beal M.J., 2003, THESIS
   BELKIN M, 2002, NIPS, V14, P585
   Bishop C. M., 2006, PATTERN RECOGNITION
   Brand M., 2003, ADV NEURAL INFORM PR, P961, DOI DOI 10.1109/34.682189
   Cayton L., 2005, ALGORITHMS MANIFOLD, V12, P1
   Lawrence N., 2011, P INT C ART INT STAT, P51
   Lawrence N.D., 2003, P ADV NEUR INF PROC, P329
   Menne MJ, 2009, B AM METEOROL SOC, V90, P993, DOI 10.1175/2008BAMS2613.1
   Platt John C., 2005, P 10 INT WORKSH ART, P261
   Roweis S, 2002, ADV NEUR IN, V14, P889
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   van der Maaten L., 2008, DIMENSIONALITY REDUC
   Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166
   ZHAN YB, 2009, NIPS, V5863, P293
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103011
DA 2019-06-15
ER

PT S
AU Park, MJ
   Bohner, G
   Macke, JH
AF Park, Mijung
   Bohner, Gergo
   Macke, Jakob H.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Unlocking neural population non-stationarity using a hierarchical
   dynamics model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID STATE; VARIABILITY
AB Neural population activity often exhibits rich variability. This variability can arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as from modulations of neural firing properties on long time-scales, often referred to as neural non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics. We derive a Bayesian Laplace propagation algorithm for joint inference of parameters and population states. On neural population recordings from primary visual cortex, we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models.
C1 [Park, Mijung; Bohner, Gergo] UCL, Gatsby Computat Neurosci Unit, London, England.
   [Macke, Jakob H.] Bonn Max Planck Inst Biol Cybernet, Bernstein Ctr Computat Neurosci Tubingen, Res Ctr Caesar, Max Planck Soc, Bonn, Germany.
RP Park, M (reprint author), UCL, Gatsby Computat Neurosci Unit, London, England.
EM mijung@gatsby.ucl.ac.uk; gbohner@gatsby.ucl.ac.uk; jakob.macke@caesar.de
FU Gatsby Charitable Foundation through BMBF; German Federal Ministry of
   Education and Research through BMBF;  [FKZ:01GQ1002]
FX We thank Alexander Ecker and the lab of Andreas Tolias for sharing their
   data with us [5] (see
   http://toliaslab.org/publications/ecker-et-al-2014/), and for allowing
   us to use it in this publication, as well as Maneesh Sahani and
   Alexander Ecker for valuable comments. This work was funded by the
   Gatsby Charitable Foundation (MP and GB) and the German Federal Ministry
   of Education and Research (MP and JHM) through BMBF; FKZ: 01GQ1002
   (Bernstein Center Tubingen). Code available at
   http://www.mackelab.org/code.
CR Beal M.J., 2003, THESIS
   Brody CD, 1999, NEURAL COMPUT, V11, P1537, DOI 10.1162/089976699300016133
   Brown EN, 2001, P NATL ACAD SCI USA, V98, P12261, DOI 10.1073/pnas.201409398
   Czanner G, 2008, J NEUROPHYSIOL, V99, P2672, DOI 10.1152/jn.00343.2007
   Destexhe A, 2011, CURR OPIN NEUROBIOL, V21, P717, DOI 10.1016/j.conb.2011.06.002
   Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006
   Ecker Alexander S, 2015, BIORXIV
   Eden UT, 2004, NEURAL COMPUT, V16, P971, DOI 10.1162/089976604773135069
   Frank LM, 2002, J NEUROSCI, V22, P3817
   Gilbert CD, 2012, NEURON, V75, P250, DOI 10.1016/j.neuron.2012.06.030
   Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711
   Haefner Ralf M, 2014, ARXIV14090257
   Harris KD, 2011, NAT REV NEUROSCI, V12, P509, DOI 10.1038/nrn3084
   Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173
   Lesica NA, 2005, IEEE T NEUR SYS REH, V13, P194, DOI [10.1109/TNSRE.2005.848339, 10.1109/TMSRE.2005.848339]
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Maimon G, 2011, CURR OPIN NEUROBIOL, V21, P559, DOI 10.1016/j.conb.2011.05.001
   Murray I., 2010, ADV NEURAL INF PROCE, P1723
   Quiroga-Lombard CS, 2013, J NEUROPHYSIOL, V110, P562, DOI 10.1152/jn.00186.2013
   Rabinowitz N. C., 2015, ARXIV150701497
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Renart A, 2014, CURR OPIN NEUROBIOL, V25, P211, DOI 10.1016/j.conb.2014.02.013
   Sahani M., 2006, P IEEE NONL STAT SIG
   Scholvinck ML, 2015, J NEUROSCI, V35, P170, DOI 10.1523/JNEUROSCI.4994-13.2015
   Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622
   Smola A. J., 2003, ADV NEURAL INF PROCE, P441
   TOMKO GJ, 1974, BRAIN RES, V79, P405, DOI 10.1016/0006-8993(74)90438-7
   Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455
   vanVreeswijk C, 1996, SCIENCE, V274, P1724
   Ventura V., 2005, TRIAL TO TRIAL VARIA
   Ypma A, 2005, NEUROCOMPUTING, V69, P85, DOI 10.1016/j.neucom.2005.02.020
   Yu, 2009, GAUSSIAN PROCESS FAC, V102, P614
   Yu B.M., 2006, ADV NEURAL INF PROCE, V18, P1545
   ZammitMangion A, 2011, NEURAL COMPUT, V23, P1967, DOI 10.1162/NECO_a_00156
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101050
DA 2019-06-15
ER

PT S
AU Pehlevan, C
   Chklovskii, DB
AF Pehlevan, Cengiz
   Chklovskii, Dmitri B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Normative Theory of Adaptive Dimensionality Reduction in Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFORMATION; REPRESENTATIONS; MODEL; SET
AB To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.
C1 [Pehlevan, Cengiz; Chklovskii, Dmitri B.] Simons Fdn, Simons Ctr Data Anal, New York, NY 10010 USA.
RP Pehlevan, C (reprint author), Simons Fdn, Simons Ctr Data Anal, New York, NY 10010 USA.
EM cpehlevan@simonsfoundation.org; dchklovskii@simonsfoundation.org
CR Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308
   BARROW HG, 1992, ARTIFICIAL NEURAL NETWORKS, 2, VOLS 1 AND 2, P433
   Boyd S., 2004, CONVEX OPTIMIZATION
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Diamantaras K. I., 1996, PRINCIPAL COMPONENT
   Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012
   Druckmann S, 2012, NIPS
   Fairhall AL, 2001, NATURE, V412, P787, DOI 10.1038/35090500
   Foldiak P., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P401, DOI 10.1109/IJCNN.1989.118615
   Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003
   Goes J., 2014, P 17 INT C ART INT S, P266
   Hu T, 2013, CONF REC ASILOMAR C, P362, DOI 10.1109/ACSSC.2013.6810296
   Hubel D, 1995, EYE BRAIN VISION
   Kiani R, 2007, J NEUROPHYSIOL, V97, P4296, DOI 10.1152/jn.00024.2007
   King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013
   Koulakov AA, 2011, NEURON, V72, P124, DOI 10.1016/j.neuron.2011.07.031
   Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043
   Leen Todd K, 1990, NIPS, V3
   LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112
   Pehlevan C, 2015, ALL C COMM CONTR COM
   Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745
   Plumbley M. D., 1993, Third International Conference on Artificial Neural Networks (Conf. Publ. No.372), P86
   Plumbley MD, 1996, NETWORK-COMP NEURAL, V7, P301, DOI [10.1088/0954-898X/7/2/010, 10.1088/0954-898X_7_2_010]
   Plumbley MD, 1994, TECH REP
   RUBNER J, 1989, EUROPHYS LETT, V10, P693, DOI 10.1209/0295-5075/10/7/015
   SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0
   Seung HS, 1998, ADV NEUR IN, V10, P329
   Tkacik G, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.058104
   TORGERSON WS, 1952, PSYCHOMETRIKA, V17, P401
   Vertechi P., 2014, ADV NEURAL INFORM PR, P3653
   YANG B, 1995, IEEE T SIGNAL PROCES, V43, P95, DOI 10.1109/78.365290
   Young G, 1938, PSYCHOMETRIKA, V3, P19, DOI 10.1007/BF02287916
   Zhu MC, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004353
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102034
DA 2019-06-15
ER

PT S
AU Pennington, J
   Yu, FX
   Kumar, S
AF Pennington, Jeffrey
   Yu, Felix X.
   Kumar, Sanjiv
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Spherical Random Features for Polynomial Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l(2) norm. The question we address in this work is: if we know a priori that data is normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.
C1 [Pennington, Jeffrey; Yu, Felix X.; Kumar, Sanjiv] Google Res, Mountain View, CA 94043 USA.
RP Pennington, J (reprint author), Google Res, Mountain View, CA 94043 USA.
EM jpennin@google.com; felixyu@google.com; sanjivk@google.com
CR Bochner S., 1955, HARMONIC ANAL THEORY
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Hamid Raffay, 2014, P 31 INT C MACH LEAR, P19
   Isozaki H., 2002, P 19 INT C COMP LING, V1, P1
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Kar P., 2012, P 15 INT C ART INT S, V22, P583
   Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133
   Kummer EE., 1837, J REINE ANGEW MATH, V17, P228, DOI 10.1515/crll.1837.17.228
   Li FX, 2010, LECT NOTES COMPUT SC, V6376, P262
   Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46
   Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203
   Pham N., 2013, P 19 ACM SIGKDD INT, P239
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Sreekanth V, 2010, BRIT MACH VIS C
   Storcheus Dmitry, 2015, ARXIV150908880
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Yang JY, 2014, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2014.129
   Yu Felix X., 2015, ARXIV150303893
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102091
DA 2019-06-15
ER

PT S
AU Pentina, A
   Lampert, CH
AF Pentina, Anastasia
   Lampert, Christoph H.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Lifelong Learning with Non-i.i.d. Tasks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BOUNDS; INEQUALITIES
AB In this work we aim at extending the theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead, we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time in a consistent way. In the first case we prove a PAC-Bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.
C1 [Pentina, Anastasia; Lampert, Christoph H.] IST Austria, Klosterneuburg, Austria.
RP Pentina, A (reprint author), IST Austria, Klosterneuburg, Austria.
EM apentina@ist.ac.at; chl@ist.ac.at
FU European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013)/ERC grant [308036]
FX This work was in parts funded by the European Research Council under the
   European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant
   agreement no 308036.
CR Balcan  Maria-Florina, 2015, WORKSH COMP LEARN TH
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Blanchard Gilles, 2011, C NEUR INF PROC SYST
   DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1
   Germain Pascal, 2009, INT C MACH LEAR ICML
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Langford John, 2002, C NEUR INF PROC SYST
   Laviolette F, 2007, J MACH LEARN RES, V8, P1461
   Maurer A, 2005, J MACH LEARN RES, V6, P967
   Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7
   Maurer Andreas, 2013, INT C MACH LEAR ICML
   McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809
   McAllester David A., 2003, WORKSH COMP LEARN TH
   Pentina Anastasia, 2014, INT C MACH LEAR ICML
   Ralaivola Liva, 2010, J MACHINE LEARNING R
   Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386
   Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334
   Thrun Sebastian, 1993, TECHNICAL REPORT
   Ullman Daniel, 1997, WILEY INTERSCIENCE S
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vapnik V. N., 1982, ESTIMATION DEPENDENC
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103045
DA 2019-06-15
ER

PT S
AU Perrot, M
   Habrard, A
AF Perrot, Michael
   Habrard, Amaury
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regressive Virtual Metric Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We are interested in supervised metric learning of Mahalanobis like distances. Existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples. In this paper, instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points. Hence, each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy. We show that our approach admits a closed form solution which can be kernelized. We provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods. Furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport. Lastly, we evaluate our approach on several state of the art datasets.
C1 [Perrot, Michael; Habrard, Amaury] Univ Lyon, Univ Jean Monnet St Etienne, CNRS, Lab Hubert Curien,UMR5516, F-42000 St Etienne, France.
RP Perrot, M (reprint author), Univ Lyon, Univ Jean Monnet St Etienne, CNRS, Lab Hubert Curien,UMR5516, F-42000 St Etienne, France.
EM michael.perrot@univ-st-etienne.fr; amaury.habrard@univ-st-etienne.fr
CR Balcan Maria- Florina, 2008, P 21 ANN C LEARN THE, P287
   Bellet Aurelien, 2015, SYNTHESIS LECT ARTIF
   Bellet Aurelien, 2012, P ICML
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cortes C., 2005, P 22 INT C MACH LEAR, P153
   Courty Nicolas, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P274, DOI 10.1007/978-3-662-44848-9_18
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Globerson A, 2005, ADV NEURAL INFORM PR, P451
   Goldberger J, 2004, P ADV NEUR INF PROC, P513
   Jin  R., 2009, ADV NEURAL INFORM PR, P862
   Kadri H, 2013, P 30 INT C MACH LEAR, P471
   Kar P., 2011, ADV NEURAL INFORM PR, P1998
   Kedem Dor, 2012, ADV NEURAL INFORM PR, V4, P2582
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Lichman M., 2013, UCI MACHINE LEARNING
   Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583
   Shi Y, 2014, P 28 AAAI C ART INT, P2078
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Weinberger K. Q., 2005, ADV NEURAL INFORM PR, P1473
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Weston J., 2002, NIPS, P873
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100055
DA 2019-06-15
ER

PT S
AU Piech, C
   Bassen, J
   Huang, J
   Ganguli, S
   Sahami, M
   Guibas, L
   Sohl-Dickstein, J
AF Piech, Chris
   Bassen, Jonathan
   Huang, Jonathan
   Ganguli, Surya
   Sahami, Mehran
   Guibas, Leonidas
   Sohl-Dickstein, Jascha
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Knowledge Tracing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID TUTORS
AB Knowledge tracing-where a machine models the knowledge of a student as they interact with coursework-is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.
C1 [Piech, Chris; Bassen, Jonathan; Huang, Jonathan; Ganguli, Surya; Sahami, Mehran; Guibas, Leonidas; Sohl-Dickstein, Jascha] Stanford Univ, Stanford, CA 94305 USA.
   [Sohl-Dickstein, Jascha] Khan Acad, Mountain View, CA USA.
   [Huang, Jonathan] Google, Mountain View, CA USA.
RP Piech, C (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM piech@cs.stanford.edu; jbassen@cs.stanford.edu; jascha@stanford.edu
FU NSF-GRFP [DGE-114747]
FX Many thanks to John Mitchell for his guidance and Khan Academy for its
   support. Chris Piech is supported by NSF-GRFP grant number DGE-114747.
CR Baker RSJD, 2008, LECT NOTES COMPUT SC, V5091, P406
   Baker RSJD, 2011, LECT NOTES COMPUT SC, V6787, P13, DOI 10.1007/978-3-642-22362-4_2
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.90.9718
   Cen H, 2006, LECT NOTES COMPUT SC, V4053, P164
   Cohen GL, 2008, CURR DIR PSYCHOL SCI, V17, P365, DOI 10.1111/j.1467-8721.2008.00607.x
   Corbett A, 2001, LECT NOTES ARTIF INT, V2109, P137
   CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253
   Drasgow F., 1990, HDB IND ORG PSYCHOL, P577
   Elliot A. J., 2013, HDB COMPETENCE MOTIV
   Feng MY, 2009, USER MODEL USER-ADAP, V19, P243, DOI 10.1007/s11257-009-9063-7
   Fitch WT, 2005, COGNITION, V97, P179, DOI 10.1016/j.cognition.2005.02.005
   GENTNER D., COGNITIVE SCI, V7, P2
   GONG Y., INTELLIGENT TUTORING
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   HOCHREITER S., NEURAL COMPUTATION, V9, P8
   Karpathy Andrej, 2014, ARXIV14122306
   KHAJAH M., 2014, P 7 INT C ED DAT MIN
   KHAJAH M. M., 2014, P 4 INT WORKSH PERS
   Lan A. S., 2014, P 20 ACM SIGKDD INT, P452
   Linnenbrink E. A., 2004, MOTIVATION EMOTION C, P57
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Pardos ZA, 2011, LECT NOTES COMPUT SC, V6787, P243, DOI 10.1007/978-3-642-22362-4_21
   Pavlik Jr P. I., 2009, ONLINE SUBMISSION
   Piech C., 2015, P 2 ACM C LEARN SCAL, P195
   PIECH C., 2015, ABS150505969 CORR
   PIECH C., P 43 ACM S COMP SCI
   Polson M. C., 2013, FDN INTELLIGENT TUTO
   Rafferty AN, 2011, LECT NOTES ARTIF INT, V6738, P280, DOI 10.1007/978-3-642-21869-9_37
   Rohrer D, 2009, J RES MATH EDUC, V40, P4
   Schraagen J. M., 2000, COGNITIVE TASK ANAL
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Yudelson Michael V., 2013, Artificial Intelligence in Education. Proceedings of 16th International Conference (AIED 2013): LNCS 7926, P171, DOI 10.1007/978-3-642-39112-5_18
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100022
DA 2019-06-15
ER

PT S
AU Pinheiro, PO
   Collobert, R
   Dollar, P
AF Pinheiro, Pedro O.
   Collobert, Ronan
   Dollar, Piotr
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning to Segment Object Candidates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.
C1 [Pinheiro, Pedro O.; Collobert, Ronan; Dollar, Piotr] Facebook AI Res, Menlo Pk, CA USA.
RP Pinheiro, PO (reprint author), Idiap Res Inst, Martigny, Switzerland.
EM pedro@opinheiro.com; locronan@fb.com; pdollar@fb.com
CR Alexe B., 2012, PAMI, V2
   Chavali N., 2015, ARXIV150505836
   Chen L. C., 2015, ICLR
   Dalal  N., 2005, CVPR
   Deng J., 2009, CVPR
   Erhan  D., 2014, CVPR
   Everingham M., 2010, IJCV
   Felzenszwalb Pedro F., 2010, PAMI
   Girshick R., 2015, ARXIV150408083
   Girshick R., 2014, CVPR
   Hariharan B., 2015, CVPR
   He K., 2014, ECCV
   Hosang J., 2015, ARXIV150205082
   Humayun A., 2014, CVPR
   Kaiming H., 2014, ECCV
   Krahenbuhl P., 2014, ECCV
   Krahenbuhl P., 2015, CVPR
   Krizhevsky A., 2012, NIPS
   Kuo W., 2015, ARXIV50502146V1, V2
   Lecun Y., 1998, P IEEE
   Lin T., 2015, ARXIV14050312
   Oquab M, 2015, CVPR
   Pinheiro P. H., 2014, ICML
   Pont-Tuset J., 2015, ARXIV150300848
   Ren S., 2015, ARXIV150601497
   Sermanet  P., 2014, ICLR
   Simonyan Karen, 2015, ICLR
   Srivastava N., 2014, JMLR
   Szegedy C., 2015, CVPR
   Szegedy Christian, 2014, ARXIV14121441
   Uijlings J., 2013, IJCV
   Viola  P., 2004, IJCV
   Zhu Z. Y., 2015, CVPR, V1
   Zitnick C. L., 2014, ECCV
NR 34
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102001
DA 2019-06-15
ER

PT S
AU Plis, S
   Danks, D
   Freeman, C
   Calhoun, V
AF Plis, Sergey
   Danks, David
   Freeman, Cynthia
   Calhoun, Vince
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Rate-Agnostic (Causal) Structure Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Causal structure learning from time series data is a major scientific challenge. Extant algorithms assume that measurements occur sufficiently quickly; more precisely, they assume approximately equal system and measurement timescales. In many domains, however, measurements occur at a significantly slower rate than the underlying system changes, but the size of the timescale mismatch is often unknown. This paper develops three causal structure learning algorithms, each of which discovers all dynamic causal graphs that explain the observed measurement data, perhaps given undersampling. That is, these algorithms all learn causal structure in a "rate-agnostic" manner: they do not assume any particular relation between the measurement and system timescales. We apply these algorithms to data from simulations to gain insight into the challenge of undersampling.
C1 [Plis, Sergey] Mind Res Network, Albuquerque, NM 87106 USA.
   [Danks, David] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Freeman, Cynthia] Univ New Mexico, CS Dept, Mind Res Network, Albuquerque, NM 87131 USA.
   [Calhoun, Vince] Univ New Mexico, ECE Dept, Mind Res Network, Albuquerque, NM 87131 USA.
RP Plis, S (reprint author), Mind Res Network, Albuquerque, NM 87106 USA.
EM s.m.plis@gmail.com; ddanks@cmu.edu; cynthiaw2004@gmail.com;
   vcalhoun@mrn.org
FU NSF [IIS-1318759, IIS-1318815]; NIH [R01EB005846]; NIH (National Human
   Genome Research Institute) [U54HG008540]
FX SP & DD contributed equally. This work was supported by awards NIH
   R01EB005846 (SP); NSF IIS-1318759 (SP); NSF IIS-1318815 (DD); & NIH
   U54HG008540 (DD) (from the National Human Genome Research Institute
   through funds provided by the trans-NIH Big Data to Knowledge (BD2K)
   initiative). The content is solely the responsibility of the authors and
   does not necessarily represent the official views of the National
   Institutes of Health.
CR Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Danks D., 2013, JMLR WORKSH C P NEV, V1, P1
   Friedman N, 1999, 15 ANN C UNC ART INT, P139
   Glymour Clark, 1991, ERKENNTNIS, P151
   Gong M, 2015, JMLR ORG J MACH LEAR, P1898
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Johnson D. B., 1975, SIAM Journal on Computing, V4, P77, DOI 10.1137/0204007
   Lutkepohl H., 2007, NEW INTRO MULTIPLE T
   Moneta A., 2011, J MACHINE LEARNING R, V12, P95
   Murphy K. P., 2002, THESIS
   Plis Sergey, 2015, P 31 C ANN C UNC ART
   Richardson T, 2002, ANN STAT, V30, P962
   Seth AK, 2013, NEUROIMAGE, V65, P540, DOI 10.1016/j.neuroimage.2012.09.049
   Thiesson B., 2004, P 20 C UNC ART INT, P552
   Voortman Mark, 2010, LEARNING WHY THINGS, P641
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100057
DA 2019-06-15
ER

PT S
AU Podosinnikova, A
   Bach, F
   Lacoste-Julien, S
AF Podosinnikova, Anastasia
   Bach, Francis
   Lacoste-Julien, Simon
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Rethinking LDA: Moment Matching for Discrete ICA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider moment matching techniques for estimation in latent Dirichlet allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.
C1 [Podosinnikova, Anastasia; Bach, Francis; Lacoste-Julien, Simon] Ecole Normale Super Paris, INRIA, Paris, France.
RP Podosinnikova, A (reprint author), Ecole Normale Super Paris, INRIA, Paris, France.
FU MSR-Inria Joint Center
FX This work was partially supported by the MSR-Inria Joint Center. The
   authors would like to thank Christophe Dupuy for helpful discussions.
CR Anandkumar  A., 2012, NIPS
   Anandkumar A., 2013, CORR
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora S., 2013, ICML
   Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boucheron S., 2013, CONCENTRATION INEQUA
   BUNSEGERSTNER A, 1993, SIAM J MATRIX ANAL A, V14, P927, DOI 10.1137/0614062
   Buntine W. L., 2004, APPL DISCRETE PCA DA
   Buntine W. L., 2002, ECML
   Canny J., 2004, SIGIR
   Cardoso J.-F., 1993, IEE P F
   Cardoso J.-F., 1990, ICASSP
   Cardoso J.-F., 1989, ICASSP
   Cardoso J.-F., 1996, ISCAS
   Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546
   Cardoso JF, 1999, NEURAL COMPUT, V11, P157, DOI 10.1162/089976699300016863
   Cohen S., 2014, ACL
   Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Globerson A, 2007, J MACH LEARN RES, V8, P2265
   Griffiths T., 2002, TECHNICAL REPORT
   Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   Jutten C., 1987, THESIS
   Kuleshov  V., 2015, AISTATS
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Roweis S., 1998, NIPS
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Wallach H. M., 2009, ICML
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100039
DA 2019-06-15
ER

PT S
AU Procaccia, AD
   Shah, N
AF Procaccia, Ariel D.
   Shah, Nisarg
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Is Approval Voting Optimal Given Approval Votes?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives. It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved. We challenge this assertion by proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative, given k-approval votes. While the answer is generally positive, our theoretical and empirical results call attention to situations where approval voting is suboptimal.
C1 [Procaccia, Ariel D.; Shah, Nisarg] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Procaccia, AD (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM arielpro@cs.cmu.edu; nkshah@cs.cmu.edu
CR Alos-Ferrer C, 2006, SOC CHOICE WELFARE, V27, P621, DOI 10.1007/s00355-006-0145-8
   BARTHOLDI J, 1989, SOC CHOICE WELFARE, V6, P157, DOI 10.1007/BF00303169
   Baumeister D, 2010, STUD CHOICE WELF, P199, DOI 10.1007/978-3-642-02839-7_10
   Brams S., 2007, APPROVAL VOTING
   Brams Steven J., 2007, MATH DEMOCRACY DESIG
   Caragiannis I., 2013, P 14 ACM C EL COMM E, P143, DOI DOI 10.1145/2482540.2482570
   Caragiannis I., 2014, P 28 AAAI C ART INT, P616
   Elkind Edith, 2014, P 30 C UNC ART INT U, P182
   Erdelyi G, 2009, MATH LOGIC QUART, V55, P425, DOI 10.1002/malq.200810020
   FISHBURN PC, 1981, PUBLIC CHOICE, V36, P89
   FISHBURN PC, 1978, J ECON THEORY, V19, P180, DOI 10.1016/0022-0531(78)90062-5
   Goel A., 2015, P COLL INT
   Lee J, 2014, P NATL ACAD SCI USA, V111, P2122, DOI 10.1073/pnas.1313039111
   Little G., 2010, P 23 ANN ACM S US IN, P57, DOI DOI 10.1145/1866029.1866040
   Lu T., 2011, P 28 INT C MACH LEAR, P145
   MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244
   Mao A., 2013, P 27 AAAI C ART INT, P1142
   Procaccia A. D., 2012, P 28 UAI, P695
   SERTEL MR, 1988, J ECON THEORY, V45, P207, DOI 10.1016/0022-0531(88)90262-1
   Shah N. B., 2015, P 32 INT C MACH LEAR, V37, P10
   Soufiani H.A., 2012, NIPS, P126
   Soufiani H. Azari, 2014, P 31 INT C MACH LEAR, P360
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
   YOUNG HP, 1988, AM POLIT SCI REV, V82, P1231, DOI 10.2307/1961757
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102033
DA 2019-06-15
ER

PT S
AU Qian, C
   Yu, Y
   Zhou, ZH
AF Qian, Chao
   Yu, Yang
   Zhou, Zhi-Hua
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Subset Selection by Pareto Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID VARIABLE SELECTION; SPARSE; APPROXIMATION; RECOVERY
AB Selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection, sparse regression, dictionary learning, etc. In this paper, we propose the POSS approach which employs evolutionary Pareto optimization to find a small-sized subset with good performance. We prove that for sparse regression, POSS is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently. Particularly, for the Exponential Decay subclass, POSS is proven to achieve an optimal solution. Empirical study verifies the theoretical results, and exhibits the superior performance of POSS to greedy and convex relaxation methods.
C1 [Qian, Chao; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China.
RP Qian, C (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China.
EM qianc@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn
FU 973 Program [2014CB340501]; NSFC [61333014, 61375061]
FX We want to thank Lijun Zhang and Jianxin Wu for their helpful comments.
   This research was supported by 973 Program (2014CB340501) and NSFC
   (61333014, 61375061).
CR Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968
   Das A., 2011, P 28 INT C MACH LEAR, P1057
   Das A, 2008, ACM S THEORY COMPUT, P45
   Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Diekhoff G., 1992, STAT SOCIAL BEHAV SC
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Gilbert AC, 2003, SIAM PROC S, P243
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Johnson R. A., 2007, APPL MULTIVARIATE ST
   Miller A, 2002, SUBSET SELECTION REG
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Qian  C., 2015, P 29 AAAI C ART INT, P2935
   Qian C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P389
   Qian C, 2013, ARTIF INTELL, V204, P99, DOI 10.1016/j.artint.2013.09.002
   Tan MK, 2015, IEEE T SIGNAL PROCES, V63, P727, DOI 10.1109/TSP.2014.2385036
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tropp JA, 2003, IEEE IMAGE PROC, P37
   Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997
   Yu Y, 2012, ARTIF INTELL, V180, P20, DOI 10.1016/j.artint.2012.01.001
   Yu Y, 2008, IEEE C EVOL COMPUTAT, P835, DOI 10.1109/CEC.2008.4630893
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690
   Zhang T, 2009, J MACH LEARN RES, V10, P555
   Zhou H., 2012, ARXIV12013528
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101081
DA 2019-06-15
ER

PT S
AU Qiu, HT
   Han, F
   Liu, H
   Caffo, B
AF Qiu, Huitong
   Han, Fang
   Liu, Han
   Caffo, Brian
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust Portfolio Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID COVARIANCE-MATRIX; QUANTILE REGRESSION
AB We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.
C1 [Qiu, Huitong; Han, Fang; Caffo, Brian] Johns Hopkins Univ, Dept Biostat, Baltimore, MD 21205 USA.
   [Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Qiu, HT (reprint author), Johns Hopkins Univ, Dept Biostat, Baltimore, MD 21205 USA.
EM hqiu7@jhu.edu; fhan@jhu.edu; hanliu@princeton.edu; bcaffo@jhsph.edu
CR Andersen T. G., 2009, HDB FINANCIAL TIME S
   Bai JS, 2012, ANN STAT, V40, P436, DOI 10.1214/11-AOS966
   Bai JS, 2011, ANN ECON FINANC, V12, P199
   Belloni A, 2011, ANN STAT, V39, P82, DOI 10.1214/10-AOS827
   BEST MJ, 1991, REV FINANC STUD, V4, P315, DOI 10.1093/rfs/4.2.315
   Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752
   Chen YL, 2011, IEEE T SIGNAL PROCES, V59, P4097, DOI 10.1109/TSP.2011.2138698
   CHOPRA VK, 1993, J PORTFOLIO MANAGE, V19, P6, DOI 10.3905/jpm.1993.409440
   Couillet R, 2014, J MULTIVARIATE ANAL, V131, P99, DOI 10.1016/j.jmva.2014.06.018
   Dowd K., 2007, MEASURING MARKET RIS
   Fan JQ, 2013, J R STAT SOC B, V75, P603, DOI 10.1111/rssb.12016
   Fan JQ, 2012, J AM STAT ASSOC, V107, P592, DOI 10.1080/01621459.2012.682825
   Fan JQ, 2008, J ECONOMETRICS, V147, P186, DOI 10.1016/j.jeconom.2008.09.017
   Fang K. T., 1990, SYMMETRIC MULTIVARIA
   GNANADESIKAN R, 1972, BIOMETRICS, V28, P81, DOI 10.2307/2528963
   Hall A. R, 2005, GEN METHOD MOMENTS
   Huber P. J., 1981, ROBUST STAT
   Jagannathan R, 2003, J FINANC, V58, P1651, DOI 10.1111/1540-6261.00580
   Joe H, 1997, MULTIVARIATE MODELS
   KALLBERG JG, 1984, LECT NOTES ECON MATH, V227, P74
   Ledoit 0., 2003, J EMPIR FINANC, V10, P603, DOI DOI 10.1016/S0927-5398(03)00007-0
   Ledoit O, 2004, J PORTFOLIO MANAGE, V30, P110, DOI 10.3905/jpm.2004.110
   Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4
   Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x
   Maronna RA, 2002, TECHNOMETRICS, V44, P307, DOI 10.1198/004017002188618509
   MERTON RC, 1980, J FINANC ECON, V8, P323, DOI 10.1016/0304-405X(80)90007-0
   Rachev S.T., 2003, HDB HEAVY TAILED DIS
   Rachev ST, 2005, FAT TAILED SKEWED AS
   ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267
   Schmidt R, 2002, MATH METHOD OPER RES, V55, P301, DOI 10.1007/s001860200191
   Stock JH, 2002, J AM STAT ASSOC, V97, P1167, DOI 10.1198/016214502388618960
   VAN DE GEER S. A., 2000, EMPIRICAL PROCESSES
   Wang L, 2012, J AM STAT ASSOC, V107, P214, DOI 10.1080/01621459.2012.656014
   XU M., 2012, ADV OPER RES, V2012, P1
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100082
DA 2019-06-15
ER

PT S
AU Qu, C
   Xu, H
AF Qu, Chao
   Xu, Huan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Subspace Clustering with Irrelevant Features via Robust Dantzig Selector
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MOTION SEGMENTATION
AB This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed "robust Dantzig selector" which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.
C1 [Qu, Chao; Xu, Huan] Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore.
RP Qu, C (reprint author), Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore.
EM A0117143@u.nus.edu; mpexuh@nus.edu.sg
FU Ministry of Education of Singapore AcRF Tier Two grant
   [R-265-000-443-112]; A*STAR SERC PSF grant [R-265-000-540-305]
FX This work is partially supported by the Ministry of Education of
   Singapore AcRF Tier Two grant R-265-000-443-112, and A*STAR SERC PSF
   grant R-265-000-540-305.
CR Agarwal P., 2004, P 23 ACM SIGMOD SIGA, V23, P155
   Ball K., 1997, MATH SCI RES I PUBL, V31, P1, DOI DOI 10.2977/PRIMS/1195164788.MR1491097
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen Y., 2013, P 30 INT C MACH LEAR, P774
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Liu G., 2010, P INT C MACH LEARN, V27, P663
   Loh PL, 2011, ADV NEURAL INFORM PR, P2726
   Lu L., 2006, INT C MACH LEARN, P593
   Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI [10.1109/TPAMI.2007.1085, 10.1109/TP'AMI.2007.1085]
   Rao Shankar R, 2008, CVPR
   Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199
   Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   [王应德 Wang Yingde], 2013, [高分子通报, Polymer Bulletin], P89
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Zhu H, 2011, IEEE T SIGNAL PROCES, V59, P2002, DOI 10.1109/TSP.2011.2109956
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100075
DA 2019-06-15
ER

PT S
AU Qu, X
   Doshi, P
AF Qu, Xia
   Doshi, Prashant
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Individual Planning in Infinite-Horizon Multiagent Settings: Inference,
   Structure and Scalability
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONVERGENCE; ALGORITHM
AB This paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization (EM). Our formalization in the context of infinite-horizon and finitely-nested interactive POMDPs (I-POMDP) is distinct from EM formulations for POMDPs and cooperative multiagent planning frameworks. We exploit the graphical model structure specific to I-POMDPs, and present a new approach based on block-coordinate descent for further speed up. Forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure.
C1 [Qu, Xia] Epic Syst, Verona, WI 53593 USA.
   [Doshi, Prashant] Univ Georgia, THINC Lab, Dept Comp Sci, Athens, GA 30622 USA.
RP Qu, X (reprint author), Epic Syst, Verona, WI 53593 USA.
EM quxiapisces@gmail.com; pdoshi@cs.uga.edu
FU NSF CAREER grant [IIS-0845036]; ONR [N000141310870]
FX This research is supported in part by a NSF CAREER grant, IIS-0845036,
   and a grant from ONR, N000141310870. We thank Akshat Kumar for feedback
   that led to improvements in the paper.
CR ARIMOTO S, 1972, IEEE T INFORM THEORY, V18, P14, DOI 10.1109/TIT.1972.1054753
   Attias Hagai, 2003, 9 INT WORKSH AI STAT
   Cappe O, 2009, J ROY STAT SOC B, V71, P593, DOI 10.1111/j.1467-9868.2009.00698.x
   Carter CK, 1996, BIOMETRIKA, V83, P589, DOI 10.1093/biomet/83.3.589
   Doshi Prashant, 2009, J ARTIFICIAL INTELLI, V34, P297
   Fessler J.A., 2011, P 11 INT M FULL 3 DI, P262
   FESSLER JA, 1994, IEEE T SIGNAL PROCES, V42, P2664, DOI 10.1109/78.324732
   Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579
   Kumar A., 2010, PROCEEDINGS OF THE T, P294
   Kumar A., 2011, P 22 INT JOINT C ART, P2140
   Ng B, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P1814
   Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054
   Sonu E, 2015, AUTON AGENT MULTI-AG, V29, P455, DOI 10.1007/s10458-014-9261-5
   Sonu Ekhlas, 2015, INT C AUT PLANN SCHE, P202
   Toussaint M, 2006, P 23 INT C MACH LEAR, P945, DOI DOI 10.1145/1143844.1143963
   Toussaint M., 2008, P 24 C UNC ART INT U, P562
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Wu Feng, 2013, P 23 INT JOINT C ART, P397
   Zeng YF, 2012, J ARTIF INTELL RES, V43, P211, DOI 10.1613/jair.3461
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100102
DA 2019-06-15
ER

PT S
AU Qu, Z
   Richtarik, P
   Zhang, T
AF Qu, Zheng
   Richtarik, Peter
   Zhang, Tong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
DE empirical risk minimization; dual coordinate ascent; arbitrary sampling;
   data-driven speedup
ID OPTIMIZATION
AB We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial and mini-batch variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.
C1 [Qu, Zheng] Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.
   [Richtarik, Peter] Univ Edinburgh, Sch Math, Edinburgh EH9 3FD, Midlothian, Scotland.
   [Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.
RP Qu, Z (reprint author), Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.
EM zhengqu@maths.hku.hk; peter.richtarik@ed.ac.uk; tzhang@stat.rutgers.edu
CR Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Fercoq O., 2013, ARXIV13095885
   Fercoq O., 2013, SIAM J OPTIMIZATION
   Hsieh C.-J., 2008, P 25 INT C MACH LEAR, V951, P408, DOI DOI 10.1145/1390156.1390208
   Jaggi M., 2014, ADV NEURAL INFORM PR, P3068
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Konecny J., 2014, ARXIV14104744
   Konecny J., 2013, ARXIV13121666
   Lin Q., 2014, MSRTR201494
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Qu Z., 2014, ARXIV14128060
   Qu Z., 2014, ARXIV14128063
   Richtarik P., 2015, MATH PROGRAM
   Richtarik P., 2015, OPTIMIZATION LETT
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S., 2012, ARXIV12112717
   Shalev-Shwartz S., 2013, ADV NEURAL INF PROCE, P378
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Takac M., 2013, CORNELL U LIBR, P1022
   Yang T., 2013, ADV NEURAL INFORM PR, P629
   Zhang T., 2004, P 21 INT C MACH LEAR, P919, DOI DOI 10.1145/1015330.1015332
   Zhang Yuchen, 2015, P 32 INT C MACH LEAR, P353
   Zhao P., 2015, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102074
DA 2019-06-15
ER

PT S
AU Quanrud, K
   Khashabi, D
AF Quanrud, Kent
   Khashabi, Daniel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Learning with Adversarial Delays
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS
AB We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that online-gradient-descent [1] and follow-the-perturbed-leader [2] achieve regret O(root D) in the delayed setting, where D is the sum of delays of each round's feedback. This bound collapses to an optimal O(root T) bound in the usual setting of no delays (where D = T). Our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves. Our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.
C1 [Quanrud, Kent; Khashabi, Daniel] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
RP Quanrud, K (reprint author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
EM quanrud2@illinois.edu; khashab2@illinois.edu
CR Amuru S, 2014, IEEE MILIT COMMUN C, P1528, DOI 10.1109/MILCOM.2014.252
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Blum A, 2003, ALGORITHMICA, V36, P249, DOI 10.1007/s00453-003-1015-8
   Blum A, 1998, LECT NOTES COMPUT SC, V1442, P306
   Cesa-Bianchi N., 1997, J ASSOC COMPUT MACH, V44, P426
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188
   Duchi J., 2013, ADV NEURAL INFORM PR, P2832
   Duchi J. C., 2015, CORR
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hazan E, 2015, INTRO ONLINE CONVEX
   He X., 2014, P 8 INT WORKSH DAT M, DOI DOI 10.1145/2648584.2648589
   Helmbold D., 1997, MACH LEARN J, V27, P61
   Joulani P., 2013, P 30 INT C MACH LEAR, V28
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Liu J, 2015, J MACH LEARN RES, V16, P285
   McMahan H.B., 2014, P ADV NEUR INF PROC, V27, P2915
   Menache I., 2014, 11 INT C AUT COMP IC
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Riabko D., 2005, THESIS
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328
   Weinberger MJ, 2002, IEEE T INFORM THEORY, V48, P1959, DOI 10.1109/TIT.2002.1013136
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Zinkevich M., 2009, ADV NEURAL INFORM PR, P2331
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101092
DA 2019-06-15
ER

PT S
AU Nguyen, QP
   Low, KH
   Jaillet, P
AF Quoc Phong Nguyen
   Low, Kian Hsiang
   Jaillet, Patrick
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Inverse Reinforcement Learning with Locally Consistent Reward Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Existing inverse reinforcement learning (IRL) algorithms have assumed each expert's demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts' behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.
C1 [Quoc Phong Nguyen; Low, Kian Hsiang] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.
   [Jaillet, Patrick] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
RP Nguyen, QP (reprint author), Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.
EM qphong@comp.nus.edu.sg; lowkh@comp.nus.edu.sg; jaillet@mit.edu
FU Singapore-MIT Alliance for Research and Technology [52
   R-252-000-550-592]
FX This work was partially supported by Singapore-MIT Alliance for Research
   and Technology Subaward Agreement No. 52 R-252-000-550-592.
CR Abbeel P., 2004, P ICML
   Babes M., 2011, P 28 INT C MACH LEAR, P897
   Bilmes J., 1998, ICSITR9702 U CAL
   CHEN JT, 2013, P UAI, V826, P152, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.826.152
   Choi J., 2012, P NIPS, P314
   Choi JD, 2011, J MACH LEARN RES, V12, P691
   Dvijotham  K., 2010, P 27 INT C MACH LEAR, P335
   Hoang T. N., 2014, P ICML, P739
   Hoang T. N., 2015, P 32 INT C MACH LEAR, P569
   Hoang Trong Nghia, 2013, P IJCAI, P1394
   Levine S., 2011, ADV NEURAL INFORM PR, P19
   Low K. H., 2015, LNCS, V8964
   Low K. H., 2014, P ECML PKDD NECT TRA
   Low K. H., 2015, 29 AAAI C ART INT, P2821
   Neu G., 2007, P 23 C UNC ART INT, P295
   Neu G, 2009, MACH LEARN, V77, P303, DOI 10.1007/s10994-009-5110-1
   Newson P., 2009, P 17 ACM SIGSPATIAL, P336, DOI DOI 10.1145/1653771.1653818
   Ng A., 2000, P ICML
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586
   Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Syed U., 2007, ADV NEURAL INFORM PR, P1449
   Xu N., 2014, P AAAI, P2585
   Yu JB, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 2, P478, DOI 10.1109/WI-IAT.2012.216
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102031
DA 2019-06-15
ER

PT S
AU Rabinovich, M
   Angelino, E
   Jordan, MI
AF Rabinovich, Maxim
   Angelino, Elaine
   Jordan, Michael I.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Variational Consensus Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel [22]. A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC-achieving near-ideal speedup in some instances.
C1 [Rabinovich, Maxim; Angelino, Elaine; Jordan, Michael I.] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
RP Rabinovich, M (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
EM rabinovich@eecs.berkeley.edu; elaine@eecs.berkeley.edu;
   jordan@eecs.berkeley.edu
FU Miller Institute for Basic Research in Science, University of
   California, Berkeley; Hertz Foundation Fellowship; Google; NSF Graduate
   Research Fellowship; Amazon; ONR under the MURI program
   [N00014-11-1-0688]
FX We thank R.P. Adams, N. Altieri, T. Broderick, R. Giordano, M.J.
   Johnson, and S.L. Scott for helpful discussions. E.A. is supported by
   the Miller Institute for Basic Research in Science, University of
   California, Berkeley. M.R. is supported by a Hertz Foundation
   Fellowship, generously endowed by Google, and an NSF Graduate Research
   Fellowship. Support for this project was provided by Amazon and by ONR
   under the MURI program (N00014-11-1-0688).
CR Asuncion A., 2008, ADV NEURAL INFORM PR, P81
   Bardenet R., 2014, P 31 INT C MACH LEAR
   Bertsekas DP, 1990, NONLINEAR PROGRAMMIN
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Campbell T., 2014, 30 C UNC ART INT
   Cover T. M., 2006, WILEY SERIES TELECOM
   Dean J, 2008, COMMUN ACM, V51, P107, DOI 10.1145/1327452.1327492
   Doshi-Velez F., 2009, ADV NEURAL INFORM PR, V22, P1294
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gelman A., 2013, BAYESIAN DATA ANAL
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Johnson M. J., 2013, ADV NEURAL INFORM PR, P2715
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Korattikara A, 2014, P 31 INT C MACH LEAR
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109
   MACLAURIN D., 2014, P 30 C UNC ART INT
   Mandt S., 2014, ADV NEURAL INFORM PR, P2438
   Neiswanger W., 2014, 30 C UNC ART INT
   Nishihara R, 2014, J MACH LEARN RES, V15, P2087
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Ranganath R, 2013, JMLR W CP, P298
   Scott S. L., 2013, BAYES, V250
   Strathmann H., 2015, ARXIV150103326
   Wang X., 2013, ARXIV13124605
   Welling M., 2011, P 28 INT C MACH LEAR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102037
DA 2019-06-15
ER

PT S
AU Rahman, S
   Bruce, NDB
AF Rahman, Shafin
   Bruce, Neil D. B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Saliency, Scale and Information: Towards a Unifying Theory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODEL
AB In this paper we present a definition for visual saliency grounded in information theory. This proposal is shown to relate to a variety of classic research contributions in scale-space theory, interest point detection, bilateral filtering, and to existing models of visual saliency. Based on the proposed definition of visual saliency, we demonstrate results competitive with the state-of-the art for both prediction of human fixations, and segmentation of salient objects. We also characterize different properties of this model including robustness to image transformations, and extension to a wide range of other data types with 3D mesh models serving as an example. Finally, we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision.
C1 [Rahman, Shafin; Bruce, Neil D. B.] Univ Manitoba, Dept Comp Sci, Winnipeg, MB, Canada.
RP Rahman, S (reprint author), Univ Manitoba, Dept Comp Sci, Winnipeg, MB, Canada.
EM shafin109@gmail.com; bruce@cs.umanitoba.ca
RI Rahman, Shafin/N-1939-2019
OI Rahman, Shafin/0000-0001-7169-0318
FU NSERC Canada Discovery Grants program; University of Manitoba GETS
   funding; ONR [N00178-14-Q-4583]
FX The authors acknowledge financial support from the NSERC Canada
   Discovery Grants program, University of Manitoba GETS funding, and ONR
   grant #N00178-14-Q-4583.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Andreopoulos A, 2012, IEEE T PATTERN ANAL, V34, P110, DOI 10.1109/TPAMI.2011.91
   Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Bruce N, 2005, ADV NEURAL INFORM PR, P155
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Buades A, 2005, PROC CVPR IEEE, P60
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Florack L. M. J., 1994, Journal of Mathematical Imaging and Vision, V4, P171, DOI 10.1007/BF01249895
   Gao DS, 2009, NEURAL COMPUT, V21, P239, DOI 10.1162/neco.2009.11-06-391
   GARCIADIAZ A, 2012, J VISION, V12
   Harel J., 2007, ADV NEURAL INF PROCE, V19, P545
   Hou X., 2009, ADV NEURAL INFORM PR, P681
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JAGERSAND M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P195, DOI 10.1109/ICCV.1995.466786
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lindeberg T., 1994, J APPL STAT, V21, P225, DOI DOI 10.1080/757582976
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Park JC, 2008, J VISION, V8, DOI 10.1167/8.10.8
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Perona P., 1994, GEOMETRY DRIVEN DIFF, P73
   Toews M., 2010, CVPR WORKSH IEEE, P111
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102094
DA 2019-06-15
ER

PT S
AU Rai, P
   Hu, CW
   Henao, R
   Carin, L
AF Rai, Piyush
   Hu, Changwei
   Henao, Ricardo
   Carin, Lawrence
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Large-Scale Bayesian Multi-Label Learning via Topic-Based Label
   Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a scalable Bayesian multi-label learning model based on learning low-dimensional label embeddings. Our model assumes that each label vector is generated as a weighted combination of a set of topics (each topic being a distribution over labels), where the combination weights (i.e., the embeddings) for each label vector are conditioned on the observed feature vector. This construction, coupled with a Bernoulli-Poisson link function for each label of the binary label vector, leads to a model with a computational cost that scales in the number of positive labels in the label matrix. This makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse. Using a data-augmentation strategy leads to full local conjugacy in our model, facilitating simple and very efficient Gibbs sampling, as well as an Expectation Maximization algorithm for inference. Also, predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form. We report results on several benchmark data sets, comparing our model with various state-of-the art methods.
C1 [Rai, Piyush] IIT Kanpur, CSE Dept, Kanpur, Uttar Pradesh, India.
   [Rai, Piyush; Hu, Changwei; Henao, Ricardo; Carin, Lawrence] Duke Univ, ECE Dept, Durham, NC 27706 USA.
RP Rai, P (reprint author), IIT Kanpur, CSE Dept, Kanpur, Uttar Pradesh, India.; Rai, P (reprint author), Duke Univ, ECE Dept, Durham, NC 27706 USA.
EM piyush@cse.iitk.ac.in; ch237@duke.edu; r.henao@duke.edu; lcarin@duke.edu
FU ARO; DARPA; ONR; DOE; NGA
FX This research was supported in part by ARO, DARPA, DOE, NGA and ONR
CR Agrawal  R., 2013, WWW
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Blei D. M., 2003, JMLR
   Chen Jianfei, 2013, NIPS
   Gibaja E, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2716262
   Gibaja Eva, 2014, WILEY INTERDISCIPLIN
   Hsu  D., 2009, NIPS
   Hu C., 2015, UAI
   Jun Zhu, 2011, KDD
   Kapoor A., 2012, NIPS
   Karampatziakis N., 2015, ARXIV150202710
   Kim Dae I, 2011, NIPS
   Kong Xiangnan, 2014, SDM
   Li Xin, 2015, AISTATS
   Lin H. T., 2012, NIPS
   Mimno D. M., 2008, UAI
   Mineiro Paul, 2015, ICLR WORKSH
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Prabhu  Y., 2014, KDD
   Rabinovich Maxim, 2014, ICML
   Scott James G, 2013, ARXIV13060040
   Tai Farbound, 2012, NEURAL COMPUTATION
   Tipping ME, 2004, LECT NOTES ARTIF INT, V3176, P41
   Weston  J., 2011, IJCAI
   Yan Yan, 2010, KDD
   Yi Zhang, 2011, AISTATS
   Yu H. F., 2014, ICML
   Zhou M., 2015, AISTATS
   Zhou M., 2012, AISTATS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101030
DA 2019-06-15
ER

PT S
AU Ramasamy, D
   Madhow, U
AF Ramasamy, Dinesh
   Madhow, Upamanyu
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Compressive spectral embedding: sidestepping the SVD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used "preprocessing" step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For anmxn matrix with T non-zeros, its time complexity is O ((T + m + n) log(m + n)), and the embedding dimension is O (log(m + n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the l(2)-norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.
C1 [Ramasamy, Dinesh; Madhow, Upamanyu] UC Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA.
RP Ramasamy, D (reprint author), UC Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA.
EM dineshr@ece.ucsb.edu; madhow@ece.ucsb.edu
FU DARPA GRAPHS [BAA-12-01]; Systems on Nanoscale Information fabriCs
   (SONIC), one of the six SRC STARnet Centers - MARCO; DARPA
FX This work is supported in part by DARPA GRAPHS (BAA-12-01) and by
   Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC
   STARnet Centers, sponsored by MARCO and DARPA. Any opinions, findings,
   and conclusions or recommendations expressed in this material are those
   of the authors and do not necessarily reflect the views of the funding
   agencies.
CR Achlioptas D., 2001, P 20 ACM SIGMOD SIGA
   Candes E., 2008, SIGNAL PROCESSING MA
   DiNapoli E., 2013, ARXIV13084275CS
   Drineas P., 2005, J MACHINE LEARNING R
   Fortunato S., 2010, PHYS REPORTS, V486
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Gobel F., 1974, STOCHASTIC PROCESSES
   Halko  N., 2011, SIAM REV
   Kumar S., 2009, ADV NEURAL INFORM PR
   Li M., 2010, ICML
   Lin F., 2010, P 27 INT C MACH LEAR
   Lin Frank, 2012, THESIS
   Lovasz L, 1993, BOLYAI MATH STUD, V1, P9
   MCCORMICK SF, 1977, LINEAR ALGEBRA APPL, V16, P43, DOI 10.1016/0024-3795(77)90018-0
   Mika S., 1999, ADV NEURAL INFORM PR
   Nadakuditi RR, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.188701
   Scholkopf Bernhard, 1997, LECT NOTES COMPUTER, P583
   Silver RN, 1996, J COMPUT PHYS, V124, P115, DOI 10.1006/jcph.1996.0048
   Spielman D., 2014, SIAM J MATRIX ANAL A, V35
   Spielman D., 2011, SIAM J COMPUTING
   Spielman D. A., 2004, STOC 04
   TREFETHEN L. N., 1997, NUMERICAL LINEAR ALG
   White S., 2005, SDM, V5
   Yan Donghui, 2009, P 15 ACM SIGKDD INT
   Yan W., 2013, J PARALLEL DISTRIBUT
   Yang J., 2012, 2012 IEEE 12 INT C D
   Zhang K., 2008, P 25 INT C MACH LEAR
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103030
DA 2019-06-15
ER

PT S
AU Rao, N
   Yu, HF
   Ravikumar, P
   Dhillon, IS
AF Rao, Nikhil
   Yu, Hsiang-Fu
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Collaborative Filtering with Graph Information: Consistency and Scalable
   Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LAPLACIAN EIGENMAPS
AB Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.
C1 [Rao, Nikhil; Yu, Hsiang-Fu; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Rao, N (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM nikhilr@cs.utexas.edu; rofuyu@cs.utexas.edu; paradeepr@cs.utexas.edu;
   inderjit@cs.utexas.edu
FU NSF [CCF-1320746]; Intel PhD fellowship; ICES fellowship
FX This research was supported by NSF grant CCF-1320746. H.-F. Yu
   acknowledges support from an Intel PhD fellowship. NR was supported by
   an ICES fellowship.
CR Abernethy J, 2006, CS0611124 ARXIV
   Bach Francis, 2008, ABS08121869 CORR
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Dror Gideon, 2012, KDD CUP, P8
   Haeffele B., 2014, P 31 INT C MACH LEAR, P2007, DOI DOI 10.1109/ICGPR.2014.6970495
   JAIN P, 2013, ARXIV13060626
   Jamali M, 2010, P 4 ACM C REC SYST, V2010, P135, DOI [DOI 10.1145/1864708.1864736, 10.1145/1864708.1864736]
   Kalofolias Vassilis, 2014, EPFLCONF203064
   Li Wu-Jun, 2009, 21 INT JOINT C ART I
   Ma H., 2011, P 4 ACM INT C WEB SE, P287, DOI DOI 10.1145/1935826.1935877
   Massa P., 2006, P ECAI WORKSH REC SY, P29
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Rao  Nikhil, 2013, NIPS WORKSH GREED AL
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Salakhutdinov R., 2010, ADV NEURAL INFORM PR, P2056
   Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12
   Tewari A., 2011, ADV NEURAL INFORM PR, V24, P882
   Vershynin Roman, 2009, LECT NOTES
   Xu M., 2013, ADV NEURAL INFORM PR, P2301
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Zhao Zhou, 2014, KNOWLEDGE DATA ENG I
   Zhou T., 2012, SDM, V12, P403
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102086
DA 2019-06-15
ER

PT S
AU Razaviyayn, M
   Farnia, F
   Tse, D
AF Razaviyayn, Meisam
   Farnia, Farzan
   Tse, David
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Discrete Renyi Classifiers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MUTUAL INFORMATION
AB Consider the binary classification problem of predicting a target variable Y from a discrete feature vector X = (X-1, ... ,X-d). When the probability distribution P (X, Y) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability (MAP) decision rule. However, in practice, estimating the complete joint distribution P (X, Y) is computationally and statistically impossible for large values of d. Therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution P (X, Y) and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns.
   In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of (X, Y). We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, it is shown that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various UCI data repository datasets.
C1 [Razaviyayn, Meisam; Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Razaviyayn, M (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM meisamr@stanford.edu; farnia@stanford.edu; dntse@stanford.edu
FU Stanford University; Center for Science of Information (CSoI), an NSF
   Science and Technology Center [CCF-0939370]
FX The authors are grateful to Stanford University supporting a Stanford
   Graduate Fellowship, and the Center for Science of Information (CSoI),
   an NSF Science and Technology Center under grant agreement CCF-0939370,
   for the support during this research.
CR Anantharam V., 2013, ARXIV13046133
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Bertsimas D, 2000, HDB SEMIDEFINITE PRO, P469
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   De Loera J, 2004, SIAM J COMPUT, V33, P819, DOI 10.1137/S0097539702403803
   Eban Elad, 2014, P 31 INT C MACH LEAR, P1233
   Farnia F., 2015, ARXIV150406010
   Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199
   Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604
   Globerson Amir, 2004, P 20 C UNC ART INT, P193
   Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Renyi  A., 1959, ACTA MATH ACAD SCI H, V10, P441, DOI [10.1007/BF02024507, DOI 10.1007/BF02024507]
   Roughgarden T., 2013, ADV NEURAL INFORM PR, P1043
   Shapiro A, 2003, HDBK OPER R, V10, P353
   Shapiro Alexander, 2014, LECT STOCHASTIC PROG, V16
   Sion M., 1958, PAC J MATH, V8, P171, DOI DOI 10.2140/PJM.1958.8.171
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100066
DA 2019-06-15
ER

PT S
AU Recasens, A
   Khosla, A
   Vondrick, C
   Torralba, A
AF Recasens, Adria
   Khosla, Aditya
   Vondrick, Carl
   Torralba, Antonio
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Where are they looking?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gaze-following is a challenging and important problem that deserves more attention from the community.
C1 [Recasens, Adria; Khosla, Aditya; Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA.
RP Recasens, A (reprint author), MIT, Cambridge, MA 02139 USA.
EM recasens@csail.mit.edu; khosla@csail.mit.edu; vondrick@csail.mit.edu;
   torralba@csail.mit.edu
FU Obra Social "la Caixa" Fellowship for Post-Graduate Studies; Google PhD
   Fellowship
FX We thank Andrew Owens for helpful discussions. Funding for this research
   was partially supported by the Obra Social "la Caixa" Fellowship for
   Post-Graduate Studies to AR and a Google PhD Fellowship to CV.
CR BORJI A, 2014, J VIS, V14, P1, DOI DOI 10.1167/14.13.3
   Borji A., 2012, ECCV
   Emery N., 2000, NEUROSCIENCE BIOBEHA
   Everingham M., 2010, IJCV
   Fathi A., 2012, CVPR
   Fathi A., 2012, ECCV
   Hoffman M. W., 2006, NEURAL NETWORKS
   Itti L., 2001, NATURE REV NEUROSCIE
   Jasso H., 2006, ICDL
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Judd T., 2009, CVPR
   Krizhevsky A., 2012, NIPS
   Lin T.-Y., 2014, ECCV
   Marin-Jimenez M. J., 2014, IJCV
   Park H., 2013, ICCV
   Park H. S., 2015, CVPR
   Parks D., 2014, VISION RES
   Russakovsky Olga, 2015, IJCV
   Xiao J., 2010, CVPR
   Yao B., 2011, ICCV
   Zhou B, 2015, ICLR
   Zhou  B., 2014, NIPS
   Zhu  X., 2012, CVPR
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101107
DA 2019-06-15
ER

PT S
AU Reed, S
   Zhang, Y
   Zhang, YT
   Lee, H
AF Reed, Scott
   Zhang, Yi
   Zhang, Yuting
   Lee, Honglak
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Visual Analogy-Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in predicting image labels, annotations and captions, but have only just begun to be used for generating high-quality images. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.
C1 [Reed, Scott; Zhang, Yi; Zhang, Yuting; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Reed, S (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM reedscot@umich.edu; yeezhang@umich.edu; yutingzh@umich.edu;
   honglak@umich.edu
FU NSF GRFP grant [DGE-1256260]; ONR [N00014-13-1-0762]; NSF CAREER grant
   [IIS-1453651]; NSF [CMMI-1266184]
FX This work was supported in part by NSF GRFP grant DGE-1256260, ONR grant
   N00014-13-1-0762, NSF CAREER grant IIS-1453651, and NSF grant
   CMMI-1266184. We thank NVIDIA for donating a Tesla K40 GPU.
CR Bartha P, 2013, STANFORD ENCY PHILOS
   Benard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Cheung B., 2015, ICLR WORKSH
   Cohen T., 2014, ICML
   Cohen T. S., 2015, ICLR
   Desjardins G., 2012, ARXIV12105474
   Ding W., 2014, ARXIV14063010
   Dollar P., 2007, NIPS
   Dosovitskiy Alexey, 2015, CVPR
   Fidler S., 2012, NIPS
   Hertzmann A., 2001, SIGGRAPH
   Hwang S. J., 2013, NIPS
   Jia Y., 2014, ARXIV14085093
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Kulkarni Tejas D, 2015, NIPS
   Levy O., 2014, CONLL 2014
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Michalski V., 2014, NIPS
   Mikolov T., 2013, NIPS
   Pennington Jeffrey, 2014, EMNLP
   Reed S., 2014, ICML
   Rifai S., 2012, ECCV
   Susskind J., 2011, CVPR
   Tang Y., 2013, ICML
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Turney PD, 2006, COMPUT LINGUIST, V32, P379, DOI 10.1162/coli.2006.32.3.379
   Yang Jimei, 2015, NIPS
   Zhu Z, 2014, NIPS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101104
DA 2019-06-15
ER

PT S
AU Richard, E
   Goetz, G
   Chichilnisky, EJ
AF Richard, Emile
   Goetz, Georges
   Chichilnisky, E. J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Recognizing retinal ganglion cells in the dark
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets. Therefore, a key step in understanding neural systems is to reliably distinguish cell types. An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive. Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array. We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF). We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm. This can result in accurate, fully automated methods for cell type classification.
C1 [Richard, Emile; Goetz, Georges; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA.
RP Richard, E (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM emileric@stanford.edu; ggoetz@stanford.edu; ej@stanford.edu
FU Stanford Data Science Initiative; National Eye Institute [EY017992,
   EY018003];  [AFOSR/DARPA FA9550-12-1-0411];  [FA9550-13-1-0036]
FX We are grateful to A. Montanari and D. Palanker for inspiring
   discussions and valuable comments, and C. Rhoades for labeling the data.
   ER acknowledges support from grants AFOSR/DARPA FA9550-12-1-0411 and
   FA9550-13-1-0036. We thank the Stanford Data Science Initiative for
   financial support and NVIDIA Corporation for the donation of the Tesla
   K40 GPU we used. Data collection was supported by National Eye Institute
   grants EY017992 and EY018003 (EJC). Please contact EJC (ej@stanford.edu)
   for access to the data.
CR Arthur D., 2007, P 18 ANN ACM SIAM S
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Chichilnisky EJ, 2002, J NEUROSCI, V22, P2737, DOI 10.1523/JNEUROSCI.22-07-02737.2002
   Coates A, 2011, P 14 INT C ART INT S, P215
   Coates A., 2011, INT C MACH LEARN ICM, V28
   Dacey D, 2004, COGNITIVE NEUROSCIENCES III, THIRD EDITION, P281
   Dacey DM, 2003, CURR OPIN NEUROBIOL, V13, P421, DOI 10.1016/S0959-4388(03)00103-X
   DACEY DM, 1992, P NATL ACAD SCI USA, V89, P9666, DOI 10.1073/pnas.89.20.9666
   DeVries SH, 1997, J NEUROPHYSIOL, V78, P2048
   Greschner M, 2011, J PHYSIOL-LONDON, V589, P75, DOI 10.1113/jphysiol.2010.193888
   Jepson LH, 2014, NEURON, V83, P87, DOI 10.1016/j.neuron.2014.04.044
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Li PH, 2015, J NEUROSCI, V35, P4663, DOI 10.1523/JNEUROSCI.3675-14.2015
   Litke AM, 2004, IEEE T NUCL SCI, V51, P1434, DOI 10.1109/TNS.2004.832706
   Mairal J., 2009, P 26 ANN INT C MACH, P689, DOI DOI 10.1145/1553374.1553463
   MASTRONARDE DN, 1983, J NEUROPHYSIOL, V49, P303
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   SILVEIRA LCL, 1991, NEUROSCIENCE, V40, P217, DOI 10.1016/0306-4522(91)90186-R
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100020
DA 2019-06-15
ER

PT S
AU Rippel, O
   Snoek, J
   Adams, RP
AF Rippel, Oren
   Snoek, Jasper
   Adams, Ryan P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Spectral Representations for Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs).
   We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.
   Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.
C1 [Rippel, Oren] MIT, Dept Math, Cambridge, MA 02139 USA.
   [Snoek, Jasper; Adams, Ryan P.] Twitter, San Francisco, CA USA.
   [Snoek, Jasper; Adams, Ryan P.] Harvard SEAS, San Francisco, CA USA.
RP Rippel, O (reprint author), MIT, Dept Math, Cambridge, MA 02139 USA.
EM rippel@math.mit.edu; jsnoek@seas.harvard.edu; rpa@seas.harvard.edu
FU Applied Mathematics Program within the Office of Science Advanced
   Scientific Computing Research of the U.S. Department of Energy
   [DE-AC02-05CH11231]
FX We would like to thank Prabhat, Michael Gelbart and Matthew Johnson for
   useful discussions and assistance throughout this project. Jasper Snoek
   was a fellow in the Harvard Center for Research on Computation and
   Society. This work is supported by the Applied Mathematics Program
   within the Office of Science Advanced Scientific Computing Research of
   the U.S. Department of Energy under contract No. DE-AC02-05CH11231. This
   work used resources of the National Energy Research Scientific Computing
   Center (NERSC). We thank Helen He and Doug Jacobsen for providing us
   with access to the Babbage Xeon-Phi testbed at NERSC.
CR Bengio Yoshua, 2007, LARGE SCALE KERNEL M
   Goodfellow Ian J., 2013, ABS13024389 CORR
   Hinton Geoffrey, 2014, REDDIT MACHINE LEARN
   Hinton Geoffrey, 2014, MIT BRAIN COGNITIVE
   Ioffe Sergey, 2015, ABS150203167 CORR
   Karpathy A., 2014, COMPUTER VISION PATT
   Kingma D. P., 2015, ABS14126980 CORR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   LeCun Y., 1989, ADV NEURAL INFORM PR
   Lee Chen- Yu, 2014, ABS14095185 CORR
   Lin M, 2013, ABS13124400 CORR
   Mathieu M., 2013, ABS13125851 CORR, Vabs/1312. 5851
   Rippel Oren, 2014, INT C MACH LEARN
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Snoek J., 2015, INT C MACH LEARN
   Snoek Jasper, 2012, NEURAL INFORM PROCES
   Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302
   Vasilache Nicolas, 2014, ABS14127580 CORR
   Zeiler M. D., 2013, ABS13013557 CORR
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100017
DA 2019-06-15
ER

PT S
AU Rosasco, L
   Villa, S
AF Rosasco, Lorenzo
   Villa, Silvia
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning with Incremental Iterative Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID STOCHASTIC-APPROXIMATION; GRADIENT; OPERATORS
AB Within a statistical learning setting, we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method. In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and prove strong universal consistency, i.e. almost sure convergence of the risk, as well as sharp finite sample bounds for the iterates. Our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results.
C1 [Rosasco, Lorenzo] Univ Genoa, DIBRIS, Genoa, Italy.
   [Rosasco, Lorenzo; Villa, Silvia] IIT, LCSL, Chicago, IL 60616 USA.
   [Rosasco, Lorenzo; Villa, Silvia] MIT, Cambridge, MA 02139 USA.
RP Rosasco, L (reprint author), Univ Genoa, DIBRIS, Genoa, Italy.
EM lrosasco@mit.edu; Silvia.Villa@iit.it
FU CBMM - NSF STC [CCF-1231216]; MIUR FIRB project [RBFR12M3AC]
FX This material is based upon work supported by CBMM, funded by NSF STC
   award CCF-1231216. and by the MIUR FIRB project RBFR12M3AC. S. Villa is
   member of GNAMPA of the Istituto Nazionale di Alta Matematica (INdAM).
CR Bach F., 2014, ARXIV14080361
   Bartlett PL, 2007, J MACH LEARN RES, V8, P2347
   Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022
   Blanchard G., 2010, ADV NEURAL INFORM PR, P226
   Bottou L, 2012, OPTIMIZATION FOR MACHINE LEARNING, P351
   Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125
   Caponnetto A., 2006, FDN COMPUT MATH
   Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   De Vito E, 2004, J MACH LEARN RES, V5, P1363
   De Vito E, 2005, J MACH LEARN RES, V6, P883
   Engl H. W., 1996, REGULARIZATION INVER
   Huang P. - S., 2014, IEEE ICASSP
   Jiang WX, 2004, ANN STAT, V32, P13
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Nedic A, 2001, SIAM J OPTIMIZ, V12, P109, DOI 10.1137/S1052623499362111
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   NEMIROVSKII AS, 1986, USSR COMP MATH MATH+, V26, P7, DOI 10.1016/0041-5553(86)90002-9
   Orabona F., 2014, NIPS P
   Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI 10.1214/aop/1176988477
   Polyak B. T., 1987, INTRO OPTIMIZATION
   Ramsay JO, 2005, FUNCTIONAL DATA ANAL
   Raskutti Garvesh, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1318
   Smale S, 2005, APPL COMPUT HARMON A, V19, P285, DOI 10.1016/j.acha.2005.03.001
   Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y
   Srebro N., 2012, ARXIV10093896
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart Ingo, 2009, COLT
   Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103053
DA 2019-06-15
ER

PT S
AU Rosenbaum, D
   Weiss, Y
AF Rosenbaum, Dan
   Weiss, Yair
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Return of the Gating Network: Combining Generative Models and
   Discriminative Training in Natural Image Priors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SPARSE
AB In recent years, approaches based on machine learning have achieved state-of-the-art performance on image restoration problems. Successful approaches include both generative models of natural images as well as discriminative training of deep neural networks. Discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time. In contrast, generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of Bayes' rule.
   In this paper we show how to combine the strengths of both approaches by training a discriminative, feed-forward architecture to predict the state of latent variables in a generative model of natural images. We apply this idea to the very successful Gaussian Mixture Model (GMM) of natural images. We show that it is possible to achieve comparable performance as the original GMM but with two orders of magnitude improvement in run time while maintaining the advantage of generative models.
C1 [Rosenbaum, Dan; Weiss, Yair] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
RP Rosenbaum, D (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
FU ISF; Intel ICRI-CI; Gatsby Foundation
FX Support by the ISF, Intel ICRI-CI and the Gatsby Foundation is
   greatfully acknowledged.
CR Burger HC, 2013, LECT NOTES COMPUT SC, V8142, P121, DOI 10.1007/978-3-642-40602-7_13
   Burger HC, 2012, ARXIV12111544
   Chen YC, 2013, LECT NOTES COMPUT SC, V8142, P271, DOI 10.1007/978-3-642-40602-7_30
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fanello SR, 2014, PROC CVPR IEEE, P1709, DOI 10.1109/CVPR.2014.221
   Hel-Or Y, 2008, IEEE T IMAGE PROCESS, V17, P443, DOI 10.1109/TIP.2008.917204
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Jain V., 2009, ADV NEURAL INFORM PR, V21, P769
   Karklin Y, 2009, NATURE, V457, P83, DOI 10.1038/nature07481
   Levi Effi, 2009, THESIS
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309
   Lyu Siwei, 2006, ADV NEURAL INFORM PR, P945
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Rassmusen Carl E, 2006, MINIMIZE M
   Roth S, 2005, PROC CVPR IEEE, P860
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Schmidt U, 2010, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2010.5539844
   Sun LX, 2013, PART FIBRE TOXICOL, V10, DOI 10.1186/1743-8977-10-43
   Uria Benigno, 2013, ADV NEURAL INFORM PR, P2175
   Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
   Zoran Daniel, 2012, ADV NEURAL INFORM PR, P1745
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102002
DA 2019-06-15
ER

PT S
AU Rudi, A
   Camoriano, R
   Rosasco, L
AF Rudi, Alessandro
   Camoriano, Raffaello
   Rosasco, Lorenzo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Less is More: Nystrom Computational Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MATRIX; APPROXIMATION; ALGORITHMS
AB We study Nystrom type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystrom Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.
C1 [Rudi, Alessandro; Camoriano, Raffaello; Rosasco, Lorenzo] Univ Genoa, DIBRIS, Via Dodecaneso 35, Genoa, Italy.
   [Camoriano, Raffaello] Ist Italiano Tecnol, iCub Facil, Genoa, Italy.
   [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA.
   [Rosasco, Lorenzo] Ist Italiano Tecnol, Lab Computat & Stat Learning, Cambridge, MA 02139 USA.
RP Rudi, A (reprint author), Univ Genoa, DIBRIS, Via Dodecaneso 35, Genoa, Italy.
EM ale_rudi@mit.edu; raffaello.camoriano@iit.it; lrosasco@mit.edu
FU Center for Brains, Minds and Machines (CBMM) - NSF STC award
   [CCF-1231216]; FIRB project - Italian Ministry of Education, University
   and Research [RBFR12M3AC]
FX The work described in this paper is supported by the Center for Brains,
   Minds and Machines (CBMM), funded by NSF STC award CCF-1231216; and by
   FIRB project RBFR12M3AC, funded by the Italian Ministry of Education,
   University and Research.
CR Alaoui A., 2014, FAST RANDOMIZED KERN
   Bach Francis, 2013, COLT, V30
   Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564
   Cohen M. B., 2015, P 2015 C INN THEOR C, P181
   Cortes C., 2010, JMLR P TRACK, P113
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Jin R, 2013, INFORM THEORY IEEE T, V59
   Kumar S., 2009, ADV NEURAL INFORM PR, P1060
   Kumar S, 2012, J MACH LEARN RES, V13, P981
   Le Q., 2013, JMLR W CP, P244
   Li M., 2010, P 27 INT C MACH LEAR, P631
   Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517
   Mendelson S, 2010, ANN STAT, V38, P526, DOI 10.1214/09-AOS728
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rudi  A., 2013, ADV NEURAL INFORM PR, P2067
   Scholkopf B, 2002, ADAPTIVE COMPUTATION
   Si S., 2014, P 31 INT C MACH LEAR, V32, P701
   Smola A.J., 2000, P 17 INT C MACH LEAR, P911
   Steinwart I., 2009, COLT
   Steinwart I, 2008, INFORM SCI STAT, P1
   Wang S., 2014, JMLR P, P996
   Wang SS, 2013, J MACH LEARN RES, V14, P2729
   Williams Christopher K.I., 2000, ADV NEURAL INFORM PR, V13, P682
   Yang J., 2014, P 31 INT C MACH LEAR, P485
   Yang T., 2012, NIPS, P485
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Zhang K., 2008, P 25 INT C MACH LEAR, P1232, DOI DOI 10.1145/1390156.1390311
   Zhang Y., 2013, C LEARN THEOR, P592
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102084
DA 2019-06-15
ER

PT S
AU Ruozzi, N
AF Ruozzi, Nicholas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Exactness of Approximate MAP Inference in Continuous MRFs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BELIEF PROPAGATION; CONVERGENCE
AB Computing the MAP assignment in graphical models is generally intractable. As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations. Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog. In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight. We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models. We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.
C1 [Ruozzi, Nicholas] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
RP Ruozzi, N (reprint author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
CR Bayati M, 2011, SIAM J DISCRETE MATH, V25, P989, DOI 10.1137/090753115
   Globerson A., 2007, P 21 NEUR INF PROC S
   Iwata S., 1999, J ACM
   KARLIN S, 1980, J MULTIVARIATE ANAL, V10, P467, DOI 10.1016/0047-259X(80)90065-2
   Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P65
   Malioutov D. M., 2008, THESIS
   Malioutov DM, 2006, J MACH LEARN RES, V7, P2031
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Moallemi CC, 2010, IEEE T INFORM THEORY, V56, P2041, DOI 10.1109/TIT.2010.2040863
   Moallemi CC, 2009, IEEE T INFORM THEORY, V55, P2413, DOI 10.1109/TIT.2009.2016055
   Ruozzi N, 2013, J MACH LEARN RES, V14, P2287
   Ruozzi N, 2013, IEEE T INFORM THEORY, V59, P5860, DOI 10.1109/TIT.2013.2259576
   Sanghavi S, 2011, IEEE T INFORM THEORY, V57, P2203, DOI 10.1109/TIT.2011.2110170
   Sanghavi S, 2009, IEEE T INFORM THEORY, V55, P4822, DOI 10.1109/TIT.2009.2030448
   Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989
   Sontag D., 2008, UAI, P503
   Vontobel P. O., 2005, ABSCS0512078 CORR
   Vontobel P. O., 2013, INFORM THEORY IEEE T
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938
   Wald Y., 2014, P 30 UNC ART INT UAI
   Weiss Y, 2001, NEURAL COMPUT, V13, P2173, DOI 10.1162/089976601750541769
   Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102080
DA 2019-06-15
ER

PT S
AU Saade, A
   Krzakala, F
   Zdeborova, L
AF Saade, Alaa
   Krzakala, Florent
   Zdeborova, Lenka
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Matrix Completion from Fewer Entries: Spectral Detectability and Rank
   Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank r reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries. We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank r of a large n x m m matrix from C(r)r root nm entries, where C(r) is a constant close to 1. We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.
C1 [Saade, Alaa; Krzakala, Florent] CNRS, Lab Phys Stat, Paris, France.
   [Krzakala, Florent] Univ Pierre & Marie Curie Paris 06, Sorbonne Univ, F-75005 Paris, France.
   [Zdeborova, Lenka] CEA Saclay, Inst Phys Theor, F-91191 Gif Sur Yvette, France.
   [Zdeborova, Lenka] CNRS, UMR 3681, F-91191 Gif Sur Yvette, France.
RP Saade, A (reprint author), CNRS, Lab Phys Stat, Paris, France.
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU European Research Council under the European Union's 7th Framework
   Programme (FP/2007-2013/ERC Grant) [307087-SPARCS]
FX Our research has received funding from the European Research Council
   under the European Union's 7th Framework Programme (FP/2007-2013/ERC
   Grant Agreement 307087-SPARCS).
CR AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007
   Bordenave C, 2015, ARXIV150106087
   Bordenave C, 2010, RANDOM STRUCT ALGOR, V37, P332, DOI 10.1002/rsa.20313
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Castillo IP, 2004, J PHYS A-MATH GEN, V37, P9087, DOI 10.1088/0305-4470/37/39/003
   Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Johnson  S.G., 2014, NLOPT NONLINEAR OPTI
   Kabashima Y., 2014, ARXIV14021298
   Keshavan RH, 2009, ANN ALLERTON CONF, P1216, DOI 10.1109/ALLERTON.2009.5394534
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Mooij J. M., 2004, ADV NEURAL INFORM PR, P945
   Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015
   Rogers T, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.031116
   Saade  A., 2014, ADV NEURAL INFORM PR, P406
   Saade A, 2015, IEEE INT SYMP INFO, P1184, DOI 10.1109/ISIT.2015.7282642
   Wemmenhove B, 2003, J PHYS A-MATH GEN, V36, P9617, DOI 10.1088/0305-4470/36/37/302
   Yedidia Jonathan S, 2001, ADV NEURAL INFORM PR, V13
   Zdeborova L, 2009, ACTA PHYS SLOVACA, V59, P169, DOI 10.2478/v10155-010-0096-6
   Zhang P, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.042120
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100072
DA 2019-06-15
ER

PT S
AU Sadeghi, F
   Zitnick, CL
   Farhadi, A
AF Sadeghi, Fereshteh
   Zitnick, C. Lawrence
   Farhadi, Ali
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI VISALOGY: Answering Visual Analogy Questions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture. We introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images.
C1 [Sadeghi, Fereshteh] Univ Washington, Seattle, WA 98195 USA.
   [Zitnick, C. Lawrence] Microsoft Res, New York, NY USA.
   [Farhadi, Ali] Univ Washington, Allen Inst AI, Seattle, WA 98195 USA.
RP Sadeghi, F (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM fsadeghi@cs.washington.edu; larryz@microsoft.com; ali@cs.washington.edu
FU ONR [N00014-13-1-0720]; NSF [IIS-IIS- 1338054, IIS-1218683]; Allen
   Distinguished Investigator Award
FX This work was in part supported by ONR N00014-13-1-0720, NSF
   IIS-1218683, NSF IIS-IIS- 1338054, and Allen Distinguished Investigator
   Award.
CR Antol S., 2015, ICCV
   Aubry M., 2014, CVPR
   Barbella D. M, 2011, AAAI
   Baroni M, 2010, COMPUT LINGUIST
   Chang M. D, 2014, SPATIAL COGNITION 9
   Chopra  S., 2005, CVPR
   Farhadi A., 2009, CVPR
   Forbus K., 2011, TOPICS COGNITIVE SCI
   Forbus K. D, 2014, AI MAGAZINE
   Forbus K. D, 2005, AAAI
   Geman D., 2015, PNAS
   Gentner Dedre, 2001, ANALOGICAL MIND PERS
   Hertzmann A., 2001, SIGGRAPH
   Hwang S. J, 2013, ICML
   Jia Y., 2014, ARXIV14085093
   Jurgens D. A., 2012, SEMEVAL 2012 TASK 2
   Juthe A, 2005, ARGUMENT BY ANALOGY
   Kiros  R., 2014, ARXIV14112539
   Krizhevsky A., 2012, NIPS
   Levy  O., 2014, CONLL
   Malinowski M., 2015, ICCV
   Malinowski M, 2014, NIPS
   Mikolov T, 2013, HLT NAACL
   Parikh D., 2011, ICCV
   Sadeghi F., 2015, CVPR
   Shelley C, 2003, MULTIPLE ANALOGIES S
   Tenenbaum JB, 2000, NEURAL COMPUTATION
   Turney P. D, 2010, J ARTIF INT RES
   Turney P. D., 2005, CORR
   Turney P. D, 2006, COMPUT LINGUIST
   Yu L., 2015, ICCV
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101037
DA 2019-06-15
ER

PT S
AU Sarkar, P
   Chakrabarti, D
   Bickel, P
AF Sarkar, Purnamrita
   Chakrabarti, Deepayan
   Bickel, Peter
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Consistency of Common Neighbors for Link Prediction in Stochastic
   Blockmodels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Link prediction and clustering are key problems for network-structured data. While spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks, it can be expensive for large graphs. On the other hand, the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast, and works very well in practice. We show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough, and can do so even in sparser graphs with the addition of a "cleaning" step. Empirical results on simulated and real-world data support our conclusions.
C1 [Sarkar, Purnamrita] Univ Texas Austin, Dept Stat, Austin, TX 78712 USA.
   [Chakrabarti, Deepayan; Bickel, Peter] Univ Texas Austin, McCombs Sch Business, IROM, Austin, TX 78712 USA.
RP Sarkar, P (reprint author), Univ Texas Austin, Dept Stat, Austin, TX 78712 USA.
EM purnamritas@austin.utexas.edu; deepay@utexas.edu;
   bickel@stat.berkeley.edu
CR Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1
   Backstrom L., 2011, P 4 ACM INT C WEB SE, P635, DOI DOI 10.1145/1935826.1935914
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Chaudhuri K., 2012, J MACHINE LEARNING R, V23, P35
   Handcock MS, 2007, J ROY STAT SOC A STA, V170, P301, DOI 10.1111/j.1467-985X.2007.00471.x
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   KATZ L, 1953, PSYCHOMETRIKA, V18, P39
   Liben-Nowell D., 2003, C INF KNOWL MAN
   Lu L.Y., 2011, PHYSICA A, V390, P6
   MCSHERRY F, 2001, FOCS 2001, P529
   Olhede SC, 2014, P NATL ACAD SCI USA, V111, P14722, DOI 10.1073/pnas.1400374111
   Raftery A. E., 2002, J AM STAT ASSOC, V15, P460
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Sarkar P., 2007, P UAI
   Sarkar P., 2014, ANN STAT
   Sarkar P., 2010, C LEARN THEOR
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101069
DA 2019-06-15
ER

PT S
AU Sarkhel, S
   Singla, P
   Gogate, V
AF Sarkhel, Somdeb
   Singla, Parag
   Gogate, Vibhav
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Lifted MAP Inference via Partitioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.
C1 [Sarkhel, Somdeb; Gogate, Vibhav] Univ Texas Dallas, Richardson, TX 75083 USA.
   [Singla, Parag] IIT Delhi, New Delhi, India.
RP Sarkhel, S (reprint author), Univ Texas Dallas, Richardson, TX 75083 USA.
FU DARPA Probabilistic Programming for Advanced Machine Learning Program
   under AFRL prime [FA8750-14-C-0005]
FX This work was supported in part by the DARPA Probabilistic Programming
   for Advanced Machine Learning Program under AFRL prime contract number
   FA8750-14-C-0005.
CR Apsel U., 2014, P 28 AAAI C ART INT
   Apsel Udi, 2012, P 28 C UNC ART INT C, P74
   Braz R. de Salvo, 2007, THESIS
   Bui H., 2013, P 29 C UNC ART INT
   Domingos Pedro, 2009, MARKOV LOGIC INTERFA
   Gogate V., 2011, UAI, P256
   Gurobi Optimization. Inc, 2014, GUROBI OPTIMIZER REF
   Hadiji F., 2013, P 27 AAAI C ART INT
   Jha A., 2010, P 24 ANN C NEUR INF
   Kisynski J., 2009, P 25 C UNC ART INT U, P293
   Kok S., 2008, TECHNICAL REPORT
   Marinescu R, 2009, ARTIF INTELL, V173, P1457, DOI 10.1016/j.artint.2009.07.003
   Mittal H., 2014, ADV NEURAL INFORM PR
   Mladenov M., 2014, P 17 INT C ART INT S
   Niu F., 2011, P VLDB ENDOWMENT
   Noessner J., 2013, P 27 AAAI C ART INT
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Sarkhel S., 2014, P 17 INT C ART INT S
   Sarkhel S., 2014, ADV NEURAL INFORM PR
   Selman B, 1996, CLIQUES COLORING SAT
   Van Den Broeck G., 2011, IJCAI, V3, P2178
   VandenBroeck G., 2013, ADV NEURAL INFORM PR
   Venugopal D., 2014, MACHINE LEARNING KNO
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103025
DA 2019-06-15
ER

PT S
AU Scaman, K
   Lemonnier, R
   Vayatis, N
AF Scaman, Kevin
   Lemonnier, Remi
   Vayatis, Nicolas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Anytime Influence Bounds and the Explosive Behavior of Continuous-Time
   Diffusion Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The paper studies transition phenomena in information cascades observed along a diffusion process over some graph. We introduce the Laplace Hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time. Using this concept, we prove tight non-asymptotic bounds for the influence of a set of nodes, and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical. Our contributions include formal definitions and tight lower bounds of critical explosion time. We illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models. Finally, we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds.
C1 [Scaman, Kevin; Lemonnier, Remi; Vayatis, Nicolas] Univ Paris Saclay, CNRS, ENS Cachan, CMLA, Paris, France.
   [Lemonnier, Remi] 1000Mercis, Paris, France.
RP Scaman, K (reprint author), Univ Paris Saclay, CNRS, ENS Cachan, CMLA, Paris, France.
EM scaman@cmla.ens-cachan.fr; lemonnier@cmla.ens-cachan.fr;
   vayatis@cmla.ens-cachan.fr
FU French Government within the program of "Investments for the Future -
   Big Data"
FX This research is part of the SODATECH project funded by the French
   Government within the program of "Investments for the Future - Big
   Data".
CR Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168
   Chen W., 2010, P 16 ACM SIGKDD INT, P1029, DOI DOI 10.1145/1835804.1835934
   Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047
   Draief M, 2008, ANN APPL PROBAB, V18, P359, DOI 10.1214/07-AAP470
   Du N., 2013, P 16 INT C ART INT S, P229
   Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147
   Farajtabar M., 2014, ADV NEURAL INFORM PR, P2474
   Gomez-Rodriguez M., 2011, P 28 INT C MACH LEAR, P561
   Gomez-Rodriguez Manuel, 2015, J MACHINE LEARNING R
   HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kermack WO, 1932, P R SOC LOND A-CONTA, V138, P55, DOI 10.1098/rspa.1932.0171
   Lemonnier Remi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P161, DOI 10.1007/978-3-662-44851-9_11
   Lemonnier R., 2014, ADV NEURAL INFORM PR, P846
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Newman M., 2010, NETWORKS INTRO
   PENROSE M, 2003, RANDOM GEOMETRIC GRA, V5
   Pouget-Abadie J., 2015, P 32 INT C MACH LEAR, P977
   Rodriguez M.G., 2012, P 29 INT C MACH LEAR, P313
   Trusov M, 2009, J MARKETING, V73, P90, DOI 10.1509/jmkg.73.5.90
   Zhou Ke, 2013, P 30 INT C MACH LEAR, V28, P1301
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100069
DA 2019-06-15
ER

PT S
AU Scanagatta, M
   de Campos, CP
   Corani, G
   Zaffalon, M
AF Scanagatta, Mauro
   de Campos, Cassio P.
   Corani, Giorgio
   Zaffalon, Marco
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Bayesian Networks with Thousands of Variables
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.
C1 [Scanagatta, Mauro; Corani, Giorgio] USI, SUPSI, IDSIA, Lugano, Switzerland.
   [de Campos, Cassio P.] Queens Univ Belfast, Belfast, Antrim, North Ireland.
   [Zaffalon, Marco] IDSIA, Lugano, Switzerland.
RP Scanagatta, M (reprint author), USI, SUPSI, IDSIA, Lugano, Switzerland.
EM mauro@idsia.ch; c.decampos@qub.ac.uk; giorgio@idsia.ch;
   zaffalon@idsia.ch
RI Zaffalon, Marco/M-7035-2017
OI Zaffalon, Marco/0000-0001-8908-1502
FU Swiss NSF [200021_146606 / 1]
FX Work partially supported by the Swiss NSF grant n. 200021_146606 / 1.
CR Bartlett M., 2015, ARTIFICIAL INTELLIGE
   Chickering D.M., 2003, P 19 C UNC ART INT, P124
   COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110
   Cussens J., 2011, UNCERTAINTY ARTIFICI, P153
   Cussens J., 2013, IJCAI 2013 TUTORIAL
   de Campos C.P., 2009, P 26 ANN INT C MACH, P113
   de Campos CP, 2011, J MACH LEARN RES, V12, P663
   Haaren J. V., 2012, P 26 AAAI C ART INT
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503
   Jaakkola T., 2010, P 13 INT C ART INT S, P358
   Koivisto M, 2004, J MACH LEARN RES, V5, P549
   Koivisto M, 2006, LECT NOTES ARTIF INT, V4005, P289, DOI 10.1007/11776420_23
   Lowd D, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P334, DOI 10.1109/ICDM.2010.128
   MCGILL WJ, 1954, PSYCHOMETRIKA, V19, P97
   Moore A., 2003, P 20 INT C MACH LEAR, V3, P552
   Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063
   Silander T., 2006, P 22 ANN C UNC ART I, P445
   Teyssier M., 2005, UAI, P584
   Yuan C., 2012, P 28 C UNC ART INT U
   Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101063
DA 2019-06-15
ER

PT S
AU Schulam, P
   Saria, S
AF Schulam, Peter
   Saria, Suchi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Framework for Individualizing Predictions of Disease Trajectories by
   Exploiting Multi-Resolution Structure
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODELS
AB For many complex diseases, there is a wide variety of ways in which an individual can manifest the disease. The challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual's disease, which can in turn enable clinicians to optimize treatments. We represent an individual's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time. We propose a hierarchical latent variable model that individualizes predictions of disease trajectories. This model shares statistical strength across observations at different resolutions-the population, subpopulation and the individual level. We describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters. Finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma. We compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy.
C1 [Schulam, Peter; Saria, Suchi] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
RP Schulam, P (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM pschulam@jhu.edu; ssaria@cs.jhu.edu
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Allanore Y, 2015, NAT REV DIS PRIMERS, V1, DOI 10.1038/nrdp.2015.2
   Castaldi P. J., 2014, THORAX
   Craig J., 2008, NAT ED, V1, P184
   Gelman A., 2006, DATA ANAL USING REGR
   Khanna D, 2011, ARTHRITIS RHEUM-US, V63, P3078, DOI 10.1002/art.30467
   Lee DS, 2003, JAMA-J AM MED ASSOC, V290, P2581, DOI 10.1001/jama.290.19.2581
   Lotvall J, 2011, J ALLERGY CLIN IMMUN, V127, P355, DOI 10.1016/j.jaci.2010.11.037
   Marlin B. M., 2003, ADV NEURAL INFORM PR
   Murphy KP, 2012, MACHINE LEARNING PRO
   Proust-Lima C, 2014, STAT METHODS MED RES, V23, P74, DOI 10.1177/0962280212445839
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rizopoulos D, 2011, BIOMETRICS, V67, P819, DOI 10.1111/j.1541-0420.2010.01546.x
   Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550
   Ross J., 2013, P 30 INT C MACH LEAR, V28, P1346
   Saria S., 2015, IEEE INTELLIGENT SYS, V30
   Schulam P, 2015, P 29 AAAI C ART INT
   Shi JQ, 2012, STAT MED, V31, P3165, DOI 10.1002/sim.4502
   Shi JQ, 2005, STAT COMPUT, V15, P31, DOI 10.1007/s11222-005-4787-7
   Sontag D, 2012, P 5 ACM INT C WEB SE, P433, DOI DOI 10.1145/2124295.2124348
   Varga J, 2012, SCLERODERMA: FROM PATHOGENESIS TO COMPREHENSIVE MANAGEMENT, P1, DOI 10.1007/978-1-4419-5774-0
   Wang H., 2012, ADV NEURAL INFORM PR, V2, P1277
   Wiggins LD, 2012, J AUTISM DEV DISORD, V42, P191, DOI 10.1007/s10803-011-1230-0
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102022
DA 2019-06-15
ER

PT S
AU Seguy, V
   Cuturi, M
AF Seguy, Vivien
   Cuturi, Marco
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Principal Geodesic Analysis for Probability Measures under the Optimal
   Transport Metric
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PROJECTIONS; BARYCENTERS
AB Given a family of probability measures in P (X), the space of probability measures on a Hilbert space X, our goal in this paper is to highlight one ore more curves in P (X) that summarize efficiently that family. We propose to study this problem under the optimal transport (Wasserstein) geometry, using curves that are restricted to be geodesic segments under that metric. We show that concepts that play a key role in Euclidean PCA, such as data centering or orthogonality of principal directions, find a natural equivalent in the optimal transport geometry, using Wasserstein means and differential geometry. The implementation of these ideas is, however, computationally challenging. To achieve scalable algorithms that can handle thousands of measures, we propose to use a relaxed definition for geodesics and regularized optimal transport distances. The interest of our approach is demonstrated on images seen either as shapes or color histograms.
C1 [Seguy, Vivien; Cuturi, Marco] Kyoto Univ, Grad Sch Informat, Kyoto, Japan.
RP Seguy, V (reprint author), Kyoto Univ, Grad Sch Informat, Kyoto, Japan.
EM vivien.seguy@iip.ist.i.kyoto-u.ac.jp; mcuturi@i.kyoto-u.ac.jp
FU JSPS [26700002]
FX MC acknowledges the support of JSPS young researcher A grant 26700002.
CR Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741
   Ambrosio L, 2006, LECT MATH
   Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439
   Bigot Jeremie, 2015, ANN I H POINCARE B
   Boissard E, 2015, BERNOULLI, V21, P740, DOI 10.3150/13-BEJ585
   Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3
   Carlier Guillaume, 2015, ESAIM MATH MODELLING
   Cuturi M., 2014, INT C MACH LEARN, P685
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   FLETCHER PT, 2004, SYSTEMS MAN CYBERN A, V23, P995, DOI DOI 10.1109/TMI.2004.831793
   Frechet M, 1948, ANN I H POINCARE, V10, P215
   Gramfort Alexandre, 2015, INFORM PROCESSING ME
   HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
   Hunter DR, 2000, J COMPUT GRAPH STAT, V9, P60
   McCann RJ, 1997, ADV MATH, V128, P153, DOI 10.1006/aima.1997.1634
   Pitie F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Reich S, 2013, SIAM J SCI COMPUT, V35, pA2013, DOI 10.1137/130907367
   Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Srivastava S, 2015, P 18 INT C ART INT S, P912
   Verbeek JJ, 2002, PATTERN RECOGN LETT, V23, P1009, DOI 10.1016/S0167-8655(02)00032-6
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang W, 2013, INT J COMPUT VISION, V101, P254, DOI 10.1007/s11263-012-0566-z
   Westdickenberg M, 2010, J HYPERBOL DIFFER EQ, V7, P605, DOI 10.1142/S0219891610002244
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100048
DA 2019-06-15
ER

PT S
AU Shafieezadeh-Abadeh, S
   Esfahani, PM
   Kuhn, D
AF Shafieezadeh-Abadeh, Soroosh
   Esfahani, Peyman Mohajerin
   Kuhn, Daniel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Distributionally Robust Logistic Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID OPTIMIZATION; ALGORITHM
AB This paper proposes a distributionally robust approach to logistic regression. We use theWasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples. If the radius of this ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence. We then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball. We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases. We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier. These bounds are given by the optimal values of two highly tractable linear programs. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.
C1 [Shafieezadeh-Abadeh, Soroosh; Esfahani, Peyman Mohajerin; Kuhn, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
RP Shafieezadeh-Abadeh, S (reprint author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
EM soroosh.shafiee@epfl.ch; peyman.mohajerin@epfl.ch; daniel.kuhn@epfl.ch
FU Swiss National Science Foundation [BSCGI0_157733]
FX This research was supported by the Swiss National Science Foundation
   under grant BSCGI0_157733.
CR BACHE K., 2013, UCI MACHINE LEARNING
   Ben-Tal A, 2002, MATH PROGRAM, V92, P453, DOI 10.1007/s101070100286
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Bertsimas D, 2004, OPER RES, V52, P35, DOI 10.1287/opre.1030.0065
   Birge JR, 2011, SPRINGER SER OPER RE, P3, DOI 10.1007/978-1-4614-0237-4
   Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741
   Ding N., 2013, J MACHINE LEARNING R, V5, P1
   Erdogan E, 2006, MATH PROGRAM, V107, P37, DOI 10.1007/s10107-005-0678-0
   Feng J., 2014, ADV NEURAL INFORM PR, V1, P253
   Fournier Nicolas, 2014, PROBAB THEORY REL, P1
   Goh J, 2010, OPER RES, V58, P902, DOI 10.1287/opre.1090.0795
   Hosmer DW, 2004, APPL LOGISTIC REGRES
   Hu Z., 2013, TECHNICAL REPORT
   Koh KM, 2007, J MACH LEARN RES, V8, P1519
   Kuhn D., DATA DRIVEN DISTRIBU
   Liu C., 2005, ROBIT REGRESSION SIM, P227
   Lofberg J., 2004, 2004 IEEE International Symposium on Computer Aided Control Systems Design (IEEE Cat. No.04TH8770), P284, DOI 10.1109/CACSD.2004.1393890
   Ng A. Y., 2004, P 21 INT C MACH LEAR, P78, DOI [DOI 10.1145/1015330.1015435, 10.1145/1015330.1015435]
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Rockafellar R, 2000, J RISK, V2, P21, DOI DOI 10.21314/JOR.2000.038
   Rousseeuw PJ, 2003, COMPUT STAT DATA AN, V43, P315, DOI 10.1016/S0167-9473(02)00304-3
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Shapiro A, 2009, LECT STOCHASTIC PROG
   Shi JN, 2010, J MACH LEARN RES, V11, P713
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y
   Wiesemann W, 2014, OPER RES, V62, P1358, DOI 10.1287/opre.2014.1314
   Xu HA, 2010, IEEE T INFORM THEORY, V56, P3561, DOI 10.1109/TIT.2010.2048503
   Xu H, 2009, J MACH LEARN RES, V10, P1485
   Yun S, 2011, COMPUT OPTIM APPL, V48, P273, DOI 10.1007/s10589-009-9251-8
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101005
DA 2019-06-15
ER

PT S
AU Shah, NB
   Zhou, DY
AF Shah, Nihar B.
   Zhou, Dengyong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.
C1 [Shah, Nihar B.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Zhou, Dengyong] Microsoft Res, Redmond, WA USA.
RP Shah, NB (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM nihar@eecs.berkeley.edu; dengyong.zhou@microsoft.com
CR Bohannon J, 2011, SCIENCE, V334, P307, DOI 10.1126/science.334.6054.307
   Carlson A, 2010, P 3 ACM INT C WEB SE, P101, DOI DOI 10.1145/1718487.1718501
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Ipeirotis PG, 2014, DATA MIN KNOWL DISC, V28, P402, DOI 10.1007/s10618-013-0306-1
   Jagabathula S., 2014, ADV NEURAL INFORM PR, V27, P2492
   Karger D. R., 2011, NIPS, V24, P1953
   Kazai G., 2011, P 34 INT ACM SIGIR C, P205, DOI DOI 10.1145/2009916.2009947
   Liu Q., 2012, NIPS, P701
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Shah Nihar B, 2014, ARXIV14081387
   Shah Nihar B, 2015, INT C MACH LEARN ICM
   Vuurens J., 2011, P ACM SIGIR WORKSH C, P21
   Wais Paul, 2010, NIPS WORKSH COMP SOC
   Zhou Dengyong, 2015, ARXIV150307240
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100045
DA 2019-06-15
ER

PT S
AU Shah, P
   Rao, N
   Tang, GG
AF Shah, Parikshit
   Rao, Nikhil
   Tang, Gongguo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sparse and Low-Rank Tensor Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODELS
AB Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algoirthm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.
EM parikshit@yahoo-inc.com; nikhilr@cs.utexas.edu; gtang@mines.edu
CR Anandhi A, 2015, THEOR APPL CLIMATOL, V119, P551, DOI 10.1007/s00704-013-1043-5
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2239
   Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043
   Bhaskara A., 2014, P 46 ANN ACM S THEOR, P594
   Bhojanapalli S., 2015, ARXIV150205023
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Cattell RB, 1944, PSYCHOMETRIKA, V9, P267, DOI 10.1007/BF02288739
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chen Y., 2011, P 28 INT C MACH LEAR, P873
   Duan PF, 2015, IEEE IC COMP COM NET
   Goyal N., 2014, S THEOR COMP STOC 20, P584, DOI DOI 10.1145/2591796.2591875
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   Huang B, 2015, PAC J OPTIM, V11, P339
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR
   KRUSKAL JB, 1977, LINEAR ALGEBRA APPL, V18, P95, DOI 10.1016/0024-3795(77)90069-6
   Kuleshov V., 2015, TENSOR FACTORIZATION
   LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071
   Mesgarani N, 2006, IEEE T AUDIO SPEECH, V14, P920, DOI 10.1109/TSA.2005.858055
   Mu C., 2013, ARXIV13075870
   Netrapalli P., 2014, ADV NEURAL INFORM PR
   Rao N., 2014, SIGN SYST COMP 2013
   Shah P., 2015, OPTIMAL LOW RANK TEN
   Tang G., 2015, P 32 INT C MACH LEAR, P1491
   Tomioka R., 2011, ARXIV10100789
   Yuan M., 2014, ARXIV14051773
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103055
DA 2019-06-15
ER

PT S
AU Shang, XC
   Zhu, ZX
   Leimkuhler, B
   Storkey, AJ
AF Shang, Xiaocheng
   Zhu, Zhanxing
   Leimkuhler, Benedict
   Storkey, Amos J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale
   Bayesian Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.
C1 [Shang, Xiaocheng; Zhu, Zhanxing; Leimkuhler, Benedict; Storkey, Amos J.] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
RP Shang, XC (reprint author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.
EM x.shang@ed.ac.uk; zhanxing.zhu@ed.ac.uk; b.leimkuhler@ed.ac.uk;
   a.storkey@ed.ac.uk
CR Abdulle A, 2015, SIAM J NUMER ANAL, V53, P1, DOI 10.1137/140962644
   Ahn S., 2012, P 29 INT C MACH LEAR, P1591
   Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Frenkel D, 2001, UNDERSTANDING MOL SI
   Hoover W. G., 1991, COMPUTATIONAL STAT M
   HOROWITZ AM, 1991, PHYS LETT B, V268, P247, DOI 10.1016/0370-2693(91)90812-5
   Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941
   Larochelle Hugo, 2008, P 25 INT C MACH LEAR, P536, DOI DOI 10.1145/1390156.1390224
   Leimkuhler B., 2015, ARXIV150506889
   Leimkuhler B., 2015, IMA J NUMERICAL ANAL
   Leimkuhler B, 2015, MOL DYNAMICS DETERMI
   Leimkuhler B, 2013, APPL MATH RES EXPRES, P34, DOI 10.1093/amrx/abs010
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   NOSE S, 1984, J CHEM PHYS, V81, P511, DOI 10.1063/1.447334
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Robert C. P., 2004, MONTE CARLO STAT MET
   Vollmer S. J., 2015, ARXIV150100438
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103016
DA 2019-06-15
ER

PT S
AU Shanmugam, K
   Kocaoglu, M
   Dimakis, AG
   Vishwanath, S
AF Shanmugam, Karthikeyan
   Kocaoglu, Murat
   Dimakis, Alexandros G.
   Vishwanath, Sriram
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Causal Graphs with Small Interventions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MARKOV EQUIVALENCE CLASSES
AB We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms.
   For general chordal graphs, we derive worst case lower bounds on the number of interventions. Building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely. In the worst case, our achievable scheme is an.-approximation algorithm where. is the independence number of the graph. We also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound. In the other extreme, there are graph classes for which the required number of experiments is multiplicatively sigma away from our lower bound.
   In simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs.
C1 [Shanmugam, Karthikeyan; Kocaoglu, Murat; Dimakis, Alexandros G.; Vishwanath, Sriram] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
RP Shanmugam, K (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM karthiksh@utexas.edu; mkocaoglu@utexas.edu; dimakis@austin.utexas.edu;
   sriram@ece.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [CCF 1344179, 1344364, 1407278, 1422549]; ARO YIP award
   [W911NF-14-1-0258]
FX Authors acknowledge the support from grants: NSF CCF 1344179, 1344364,
   1407278, 1422549 and a ARO YIP award (W911NF-14-1-0258). We also thank
   Frederick Eberhardt for helpful discussions.
CR Andersson SA, 1997, ANN STAT, V25, P505
   Eberhardt F., P 21 C UNC ART INT U, P178
   Eberhardt Frederick, 2007, THESIS
   Hauser A., 2012, P 6 EUR WORKSH PROB
   Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   Hoyer Patrik O, 2008, P NIPS 2008
   Hu H., 2014, P NIPS 2014 MONTR CA
   Hyttinen A, 2013, J MACH LEARN RES, V14, P3041
   Katona Gyula, 1966, J COMB THEORY, V1, P174
   LIPTON RJ, 1979, SIAM J APPL MATH, V36, P177, DOI 10.1137/0136016
   Meek C., 1995, P 11 INT C UNC ART I
   Pearl J, 2009, CAUSALITY MODELS REA
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Spirtes P., 2001, CAUSATION PREDICTION
   Verma Thomas, 1992, P 8 INT C UNC ART IN
   WEGENER I, 1979, DISCRETE MATH, V28, P219, DOI 10.1016/0012-365X(79)90101-8
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102058
DA 2019-06-15
ER

PT S
AU Shibagaki, A
   Suzuki, Y
   Karasuyama, M
   Takeuchi, I
AF Shibagaki, Atsushi
   Suzuki, Yoshiki
   Karasuyama, Masayuki
   Takeuchi, Ichiro
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regularization Path of Cross-Validation Error Lower Bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs.
C1 [Shibagaki, Atsushi; Suzuki, Yoshiki; Karasuyama, Masayuki; Takeuchi, Ichiro] Nagoya Inst Technol, Nagoya, Aichi 4668555, Japan.
RP Shibagaki, A (reprint author), Nagoya Inst Technol, Nagoya, Aichi 4668555, Japan.
EM shibagaki.a.mllab.nit@gmail.com; suzuki.mllab.nit@gmail.com;
   karasuyama@nitech.ac.jp; takeuchi.ichiro@nitech.ac.jp
CR [Anonymous], 1996, NATURE STAT LEARNING
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bertsekas Dimitri P, 1999, NONLINEAR PROGRAMMIN
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387
   Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155
   Chung K., 2003, NEURAL COMPUTATION
   Efron B, 2004, ANN STAT, V32, P407
   El Ghaoui L., 2012, PACIFIC J OPTIMIZATI
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Giesen J., 2012, ADV NEURAL INFORM PR
   Giesen J., 2014, INT C MACH LEARN
   Giesen J, 2012, ACM T ALGORITHMS, V9, DOI 10.1145/2390176.2390186
   Hastie T, 2004, J MACH LEARN RES, V5, P1391
   Joachims T., 2000, INT C MACH LEARN
   Lee MMS, 2004, IEEE T NEURAL NETWOR, V15, P750, DOI 10.1109/TNN.2004.824266
   Lin CJ, 2008, J MACH LEARN RES, V9, P627
   Liu J., 2014, INT C MACH LEARN, V32
   Mairal J., 2012, INT C MACH LEARN
   Ogawa K., 2013, INT C MACH LEARN
   Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370
   Snoek J, 2012, ADV NEURAL INFORM PR
   Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042
   Wang J., 2014, ADV NEURAL INFORM PR
   Xiang Z. J., 2011, ADV NEURAL INFORM PR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101076
DA 2019-06-15
ER

PT S
AU Shivanna, R
   Chatterjee, B
   Sankaran, R
   Bhattacharyya, C
   Bach, F
AF Shivanna, Rakesh
   Chatterjee, Bibaswan
   Sankaran, Raman
   Bhattacharyya, Chiranjib
   Bach, Francis
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Spectral Norm Regularization of Orthonormal Representations for Graph
   Transduction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction. However, the choice of optimal embedding and an efficient algorithm to compute the same remains open. In this paper, we show that orthonormal representations, a class of unit-sphere graph embeddings are PAC learnable. Existing PAC-based analysis do not apply as the VC dimension of the function class is infinite. We propose an alternative PAC-based bound, which do not depend on the VC dimension of the underlying function class, but is related to the famous Lovasz v function. The main contribution of the paper is SPORE, a SPectral regularized ORthonormal Embedding for graph transduction, derived from the PAC bound. SPORE is posed as a non-smooth convex function over an elliptope. These problems are usually solved as semi-definite programs (SDPs) with time complexity O(n(6)). We present, Infeasible Inexact proximal (IIP): an Inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible. IIP is more scalable than SDP, has an O(1/root T) convergence, and is generally applicable whenever a suitable approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O(1/root T). The proposed algorithm easily scales to 1000' s of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting.
C1 [Shivanna, Rakesh] Google Inc, Mountain View, CA 94043 USA.
   [Chatterjee, Bibaswan; Sankaran, Raman; Bhattacharyya, Chiranjib] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India.
   [Bach, Francis] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.
RP Shivanna, R (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM rakeshshivanna@google.com; bibaswan.chatterjee@csa.iisc.ernet.in;
   ramans@csa.iisc.ernet.in; chiru@csa.iisc.ernet.in; francis.bach@ens.fr
FU Indo-French Center for Applied Mathematics (IFCAM)
FX We acknowledge support from a grant from Indo-French Center for Applied
   Mathematics (IFCAM).
CR Ando R. K., 2007, NIPS
   Balcan N., 2006, SEMISUPERVISED LEARN
   Boyle J. P., 1986, LECTURE NOTES STATIS, V37, P28
   Cormen T. H., 2001, INTRO ALGORITHMS, V2
   Erdem A, 2012, NEURAL COMPUT, V24, P700, DOI 10.1162/NECO_a_00233
   Goemans MX, 1997, MATH PROGRAM, V79, P143, DOI 10.1007/BF02614315
   Jethava V., 2012, NEURAL INFORM PROCES, P1169
   Johnson R, 2007, J MACH LEARN RES, V8, P1489
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Leordeanu M, 2011, IEEE I CONF COMP VIS, P2274, DOI 10.1109/ICCV.2011.6126507
   Lichman M., 2013, UCI MACHINE LEARNING
   LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985
   Nagy ME, 2014, J COMB THEORY B, V108, P40, DOI 10.1016/j.jctb.2014.02.011
   Parikh N., 2013, FDN TRENDS OPTIM, V1, P123, DOI DOI 10.1561/2400000003
   Schmidt  M., 2011, ADV NEURAL INFORM PR, P1458
   Shivanna R., 2014, NIPS, P3635
   Tran L., 2013, IJBB
   Villa S, 2013, SIAM J OPTIMIZ, V23, P1607, DOI 10.1137/110844805
   Zhang T., 2005, NIPS, V18, P1601
   Zhou D., 2007, P 24 INT C MACH LEAR, P1159, DOI DOI 10.1145/1273496.1273642
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100080
DA 2019-06-15
ER

PT S
AU Shpitser, I
AF Shpitser, Ilya
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Segregated Graphs and Marginals of Chain Graph Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MARKOV PROPERTIES
AB Bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables. Markov random fields (MRFs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks. A chain graph model under the Lauritzen-Wermuth-Frydenberg interpretation (hereafter a chain graph model) generalizes both Bayesian networks and MRFs, and can represent asymmetric and symmetric relationships together.
   As in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model. One recent approach to the study of marginal graphical models is to consider a well-behaved supermodel. Such a supermodel of marginals of Bayesian networks, defined only by conditional independences, and termed the ordinary Markov model, was studied at length in [6].
   In this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a Markov property, with supermodels of marginals of chain graphs defined only by conditional independences. Special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and the ordinary Markov model carry over. Our results suggest that segregated graphs define an analogue of the ordinary Markov model for marginals of chain graph models.
   We illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets.
C1 [Shpitser, Ilya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
RP Shpitser, I (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM ilyas@cs.jhu.edu
FU NIH [R01 AI104459-01A1]
FX The author would like to thank Thomas Richardson for suggesting mixed
   graphs where -and <-> edges do not meet as interesting objects to think
   about, and Elizabeth Ogburn and Eric Tchetgen Tchetgen for clarifying
   discussions of interference. This work was supported in part by an NIH
   grant R01 AI104459-01A1.
CR Andersson SA, 1997, ANN STAT, V25, P505
   Bell J.S., 1964, Physics, V1, P195
   Cai ZH, 2008, BIOMETRICS, V64, P695, DOI 10.1111/j.1541-0420.2007.00949.x
   Drton M, 2009, BERNOULLI, V15, P736, DOI 10.3150/08-BEJ172
   Evans R. J., 2010, P 26 C UNC ART INT
   Evans R. J., 2014, ANN STAT, P1
   Koster JTA, 2002, BERNOULLI, V8, P817
   Lauritzen SL, 2002, J R STAT SOC B, V64, P321, DOI 10.1111/1467-9868.00340
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Ogburn EL, 2014, STAT SCI, V29, P559, DOI 10.1214/14-STS501
   Pearl J, 1988, PROBABILISTIC REASON
   Pearl J, 2009, CAUSALITY MODELS REA
   Richardson T, 2003, SCAND J STAT, V30, P145, DOI 10.1111/1467-9469.00323
   Richardson T, 2002, ANN STAT, V30, P962
   Sadeghi K, 2014, BERNOULLI, V20, P676, DOI 10.3150/12-BEJ502
   Shpitser I., 2014, BEHAVIORMETRIKA, V41, P3
   Studeny M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P496
   van der Laan M. J., 2012, WORKING PAPER
   Van der Laan MJ, 2014, J CAUSAL INFERENCE, V2, P13, DOI 10.1515/jci-2013-0002
   VanderWeele TJ, 2012, EPIDEMIOLOGY, V23, P751, DOI 10.1097/EDE.0b013e31825fb7a0
   Verma T. S., 1990, R150 U CAL DEP COMP
   Wermuth N, 2011, BERNOULLI, V17, P845, DOI 10.3150/10-BEJ309
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102053
DA 2019-06-15
ER

PT S
AU Shvartsman, M
   Srivastava, V
   Cohen, JD
AF Shvartsman, Michael
   Srivastava, Vaibhav
   Cohen, Jonathan D.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Theory of Decision Making Under Dynamic Context
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID TIME; PROBABILITY; CHOICE; MODELS; CORTEX
AB The dynamics of simple decisions are well understood and modeled as a class of random walk models [e.g. 1-4]. However, most real-life decisions include a dynamically-changing influence of additional information we call context. In this work, we describe a computational theory of decision making under dynamically shifting context. We show how the model generalizes the dominant existing model of fixed-context decision making [2] and can be built up from a weighted combination of fixed-context decisions evolving simultaneously. We also show how the model generalizes recent work on the control of attention in the Flanker task [5]. Finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the AX Continuous Performance Test [6], using the same model parameters.
C1 [Shvartsman, Michael; Cohen, Jonathan D.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Srivastava, Vaibhav] Princeton Univ, Dept Mech & Aerosp Engn, Princeton, NJ 08544 USA.
RP Shvartsman, M (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
EM ms44@princeton.edu; vaibhavs@princeton.edu; jdc@princeton.edu
CR Bogacz R, 2007, NEURAL COMPUT, V19, P442, DOI 10.1162/neco.2007.19.2.442
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Braver TS, 2012, TRENDS COGN SCI, V16, P106, DOI 10.1016/j.tics.2011.12.010
   Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012
   Frazier PI, 2008, ADV NEURAL INFORM PR, P1
   GRATTON G, 1988, J EXP PSYCHOL HUMAN, V14, P331, DOI 10.1037/0096-1523.14.3.331
   Hanks TD, 2011, J NEUROSCI, V31, P6339, DOI 10.1523/JNEUROSCI.5613-10.2011
   Hanks Timothy D., 2015, NATURE
   Kira S, 2015, NEURON, V85, P861, DOI 10.1016/j.neuron.2015.01.007
   Laming D, 1968, INFORM THEORY CHOICE
   LIU Y, 1992, IEEE T INFORM THEORY, V38, P177, DOI 10.1109/18.108268
   Liu YS, 2009, NEURAL COMPUT, V21, P1520, DOI 10.1162/neco.2009.03-07-495
   Lositsky O., 2015, MULT C REINF LEARN D, P103
   Norris D, 2006, PSYCHOL REV, V113, P327, DOI 10.1037/0033-295X.113.2.327
   O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909
   RATCLIFF R, 1978, PSYCHOL REV, V85, P59, DOI 10.1037//0033-295X.85.2.59
   ServanSchreiber D, 1996, ARCH GEN PSYCHIAT, V53, P1105
   Sheppard JP, 2013, J VISION, V13, DOI 10.1167/13.6.4
   SIMON JR, 1963, ERGONOMICS, V6, P99, DOI 10.1080/00140136308930679
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2312
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037//0096-3445.121.1.15
   Turner BM, 2015, PSYCHOL REV, V122, P312, DOI 10.1037/a0038894
   Usher M, 2001, PSYCHOL REV, V108, P550, DOI 10.1037//0033-295X.108.3.550
   van Vugt MK, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00106
   WALD A, 1948, ANN MATH STAT, V19, P326, DOI 10.1214/aoms/1177730197
   Wong KF, 2006, J NEUROSCI, V26, P1314, DOI 10.1523/JNEUROSCI.3733-05.2006
   Yu AJ, 2009, J EXP PSYCHOL HUMAN, V35, P700, DOI 10.1037/a0013553
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100018
DA 2019-06-15
ER

PT S
AU Simsek, O
   Buckmann, M
AF Simsek, Oezguer
   Buckmann, Marcus
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning From Small Samples: An Analysis of Simple Decision Heuristics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID FRUGAL; MODELS
AB Simple decision heuristics are models of human and animal behavior that use few pieces of information-perhaps only a single piece of information-and integrate the pieces in simple ways, for example, by considering them sequentially, one at a time, or by giving them equal weight. We focus on three families of heuristics: single-cue decision making, lexicographic decision making, and tallying. It is unknown how quickly these heuristics can be learned from experience. We show, analytically and empirically, that substantial progress in learning can be made with just a few training samples. When training samples are very few, tallying performs substantially better than the alternative methods tested. Our empirical analysis is the most extensive to date, employing 63 natural data sets on diverse subjects.
C1 [Simsek, Oezguer; Buckmann, Marcus] Max Planck Inst Human Dev, Ctr Adapt Behav & Cognit, Lentzeallee 94, D-14195 Berlin, Germany.
RP Simsek, O (reprint author), Max Planck Inst Human Dev, Ctr Adapt Behav & Cognit, Lentzeallee 94, D-14195 Berlin, Germany.
EM ozgur@mpib-berlin.mpg.de; buckmann@mpib-berlin.mpg.de
FU Deutsche Forschungsgemeinschaft (DFG) [SI 1732/1-1];  [SPP 1516]
FX Thanks to Gerd Gigerenzer, Konstantinos Katsikopoulos, Malte
   Lichtenberg, Laura Martignon, Perke Jacobs, and the ABC Research Group
   for their comments on earlier drafts of this article. This work was
   supported by Grant SI 1732/1-1 to Ozgur Simsek from the Deutsche
   Forschungsgemeinschaft (DFG) as part of the priority program "New
   Frameworks of Rationality" (SPP 1516).
CR Breiman L., 1984, CLASSIFICATION REGRE
   Brighton H., 2012, EVOLUTION RATIONALIT, P84, DOI DOI 10.1017/CBO9780511792601.006
   Brighton H, 2008, PROBABILISTIC MIND P, P189
   Brighton Henry, 2006, AAAI SPRING S COGN S, P17
   Chater N, 2003, ORGAN BEHAV HUM DEC, V90, P63, DOI 10.1016/S0749-5978(02)00508-3
   Czerlinski J., 1999, SIMPLE HEURISTICS MA, P97
   Dougherty MR, 2008, PSYCHOL REV, V115, P199, DOI 10.1037/0033-295X.115.1.199
   Gigerenzer G, 1996, PSYCHOL REV, V103, P650, DOI 10.1037//0033-295X.103.4.650
   Gigerenzer G., 2011, HEURISTICS FDN ADAPT
   Gigerenzer G., 1999, SIMPLE HEURISTICS MA
   Hogarth RM, 2005, J MATH PSYCHOL, V49, P115, DOI 10.1016/j.jmp.2005.01.001
   Katsikopoulos KV, 2011, DECIS ANAL, V8, P10, DOI 10.1287/deca.1100.0191
   Katsikopoulos KV, 2010, PSYCHOL REV, V117, P1259, DOI 10.1037/a0020418
   Laskey K, 2014, P 9 INT C TEACH STAT
   Luan S, 2014, PSYCHOL REV, V121, P501, DOI 10.1037/a0037025
   Martignon L, 2002, THEOR DECIS, V52, P29, DOI 10.1023/A:1015516217425
   Martignon L., 1999, BAYESIAN BENCHMARKS, P169
   Martignon L, 2008, J MATH PSYCHOL, V52, P352, DOI 10.1016/j.jmp.2008.04.003
   Newell BR, 2005, TRENDS COGN SCI, V9, P11, DOI 10.1016/j.tics.2004.11.005
   Rose C., TELEVISION PROGRAM
   Schmitt M, 2006, J MACH LEARN RES, V7, P55
   Schmitt M., 2006, ADV NEURAL INFORM PR, V18, P1177
   Simek o, 2013, ADV NEURAL INFORM PR, V26, P2904
   Therneau T., 2014, R PACKAGE VERSION, V4, P1
   Todd P. M., 2005, ADV NEURAL INFORM PR, V17, P1393
   Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100011
DA 2019-06-15
ER

PT S
AU Sindhwani, V
   Sainath, TN
   Kumar, S
AF Sindhwani, Vikas
   Sainath, Tara N.
   Kumar, Sanjiv
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Structured Transforms for Small-Footprint Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.
C1 [Sindhwani, Vikas; Sainath, Tara N.; Kumar, Sanjiv] Google, New York, NY 10011 USA.
RP Sindhwani, V (reprint author), Google, New York, NY 10011 USA.
EM sindhwani@google.com; tsainath@google.com; sanjivk@google.com
CR [Anonymous], 2015, SUPPLEMENTARY MAT ST
   Chen G., 2014, ICASSP
   Chen  Wenlin, 2015, ICML
   Cheng Y., 2015, ARXIV150203436
   Ciresan D. C., 2011, ARXIV11020183
   Collins M. D., 2013, ICASSP
   Courbariaux M., 2015, ICLR
   Dean J., 2012, NIPS
   Denil  Misha, 2013, NIPS
   Gray R. M., 2005, FDN TRENDS COMMUNICA
   Hinton G., 2014, NIPS WORKSH
   KAILATH T, 1995, SIAM REV, V37, P297, DOI 10.1137/1037082
   KAILATH T, 1979, J MATH ANAL APPL, V68, P395, DOI 10.1016/0022-247X(79)90124-0
   Kailath T., 1994, SIAM J MATRIX ANAL A, V15
   Larochelle H., 2007, ICML
   Le  Q., 2013, ICML
   Liberty E., 2009, INFORM PROCESSING LE
   Pan V. Y, 2001, STRUCTURED MATRICES
   Pan VY, 2003, SIAM J MATRIX ANAL A, V24, P660, DOI 10.1137/S089547980238627X
   Rahimi A., 2007, NIPS
   Rakhuba MV, 2015, SIAM J SCI COMPUT, V37, pA565, DOI 10.1137/140958529
   Sainath  T., 2015, P INTERSPEECH
   Sainath Tara N., 2013, ICASSP
   Vanhoucke V., 2011, NIPS WORKSH DEEP LEA
   Yang Z., 2015, ARXIV14127149
NR 25
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102018
DA 2019-06-15
ER

PT S
AU Singer, Y
   Vondrak, J
AF Singer, Yaron
   Vondrak, Jan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Information-theoretic lower bounds for convex optimization with
   erroneous oracles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function x -> f (x) we consider optimization when one is given access to absolute error oracles that return values in [f (x) - epsilon f (x) + epsilon] or relative error oracles that return value in [(1 - epsilon) f (x); (1 + epsilon) f (x)], for some epsilon > 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.
C1 [Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
   [Vondrak, Jan] IBM Almaden Res Ctr, San Jose, CA 95120 USA.
RP Singer, Y (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM yaron@seas.harvard.edu; jvondrak@us.ibm.com
FU NSF [CCF-1301976, CCF-1452961]; Google Faculty Research Award
FX YS was supported by NSF grant CCF-1301976, CAREER CCF-1452961 and a
   Google Faculty Research Award.
CR Agarwal A, 2010, P 23 ANN C LEARN THE, P28
   Agarwal A, 2013, SIAM J OPTIMIZ, V23, P213, DOI 10.1137/110850827
   Belloni Alexandre, 2015, COLT 2015
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Dubhashi Devdatt, 1996, MPII961020
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Jamieson K. G., 2012, ADV NEURAL INFORM PR, P2681
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Nesterov Yu, 2011, CORE DISCUSSION PAPE, V2011001
   Ramdas Aaditya, 2014, P 17 INT C ART INT S, P805
   Ramdas Aaditya, 2013, P 30 INT C MACH LEAR, P365
   Shamir O., 2013, COLT 2013, P3
   Stich Sebastian U., 2011, CORR
   Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101100
DA 2019-06-15
ER

PT S
AU Slawski, M
   Li, P
   Hein, M
AF Slawski, Martin
   Li, Ping
   Hein, Matthias
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regularization-Free Estimation in Trace Regression with Symmetric
   Positive Semidefinite Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LEAST-SQUARES; RECOVERY
AB Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.
C1 [Slawski, Martin; Li, Ping] Rutgers State Univ, Dept Stat & Biostat, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Hein, Matthias] Saarland Univ, Dept Math, Dept Comp Sci, Saarbrucken, Germany.
RP Slawski, M (reprint author), Rutgers State Univ, Dept Stat & Biostat, Dept Comp Sci, Piscataway, NJ 08854 USA.
EM martin.slawski@rutgers.edu; pingli@stat.rutgers.edu;
   hein@cs.uni-saarland.de
FU [NSF-DMS-1444124];  [NSF-III-1360971];  [ONR-N00014-13-1-0764]; 
   [AFOSR-FA9550-13-1-0137]
FX The work of Martin Slawski and Ping Li is partially supported by
   NSF-DMS-1444124, NSF-III-1360971, ONR-N00014-13-1-0764, and
   AFOSR-FA9550-13-1-0137.
CR Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005
   Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267
   Candes E., 2009, FDN COMPUTATIONAL MA, V9, P2053
   Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   Chen YX, 2015, IEEE T INFORM THEORY, V61, P4034, DOI 10.1109/TIT.2015.2429594
   Davidson KR, 2001, HANDBOOK OF THE GEOMETRY OF BANACH SPACES, VOL 1, P317, DOI 10.1016/S1874-5849(01)80010-3
   Demanet L, 2014, J FOURIER ANAL APPL, V20, P199, DOI 10.1007/s00041-013-9305-2
   Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401
   Horn R. A., 1985, MATRIX ANAL
   Kabanva M., 2015, ARXIV150707184
   KLIBANOV MV, 1995, INVERSE PROBL, V11, P1, DOI 10.1088/0266-5611/11/1/001
   Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894
   Levina E, 2012, PROBAB THEORY REL, V153, P405, DOI 10.1007/s00440-011-0349-4
   Meinshausen N, 2013, ELECTRON J STAT, V7, P1607, DOI 10.1214/13-EJS818
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860
   Scholkopf B., 2002, LEARNING KERNELS
   Slawski M., 2015, ARXIV150406305
   Slawski M, 2013, ELECTRON J STAT, V7, P3004, DOI 10.1214/13-EJS868
   Srebro N., 2005, ADV NEURAL INFORM PR, P1329
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp J., 2014, USER FRIENDLY TOOLS
   Wang M, 2011, IEEE T SIGNAL PROCES, V59, P1007, DOI 10.1109/TSP.2010.2089624
   Williams CKI, 2001, ADV NEUR IN, V13, P682
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100094
DA 2019-06-15
ER

PT S
AU Slawski, M
   Li, P
AF Slawski, Martin
   Li, Ping
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI b-bit Marginal Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RECOVERY
AB We consider the problem of sparse signal recovery from m linear measurements quantized to b bits. b-bit Marginal Regression is proposed as recovery algorithm. We study the question of choosing b in the setting of a given budget of bits B = m . b and derive a single easy-to-compute expression characterizing the trade-off between m and b. The choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive Gaussian noise before quantization as well as for adversarial noise. For b >= 2, we show that Lloyd-Max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending [15].
C1 [Slawski, Martin; Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, New Brunswick, NJ 08901 USA.
RP Slawski, M (reprint author), Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, New Brunswick, NJ 08901 USA.
EM martin.slawski@rutgers.edu; pingli@stat.rutgers.edu
FU [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764]; 
   [AFOSR-FA9550-13-1-0137]
FX This work is partially supported by NSF-Bigdata-1419210,
   NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.
CR Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Boufounos P., 2008, INFORM SCI SYSTEMS
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Chen S., 2015, AISTATS
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Genovese CR, 2012, J MACH LEARN RES, V13, P2107
   Gopi S., 2013, ICML
   Jacques L., 2013, ARXIV13051786
   Jacques L, 2013, IEEE T INFORM THEORY, V59, P2082, DOI 10.1109/TIT.2012.2234823
   Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310
   KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622
   Laska J., 2011, ARXIV11103450
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Li P., 2014, COLT
   Li P., 2015, ARXIV150302346
   Li P., 2015, ARXIV150306876
   Liu J, 2014, APPL COMPUT HARMON A, V37, P325, DOI 10.1016/j.acha.2013.12.006
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Vershynin R., 2012, COMPRESSED SENSING T
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399
   Zhang L., 2014, ICML
   Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690
   Zhu R., 2015, ICML
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100106
DA 2019-06-15
ER

PT S
AU Smith, D
   Gogate, V
AF Smith, David
   Gogate, Vibhav
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bounding the Cost of Search-Based Lifted Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recently, there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models (SRMs). These lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation. One drawback of these algorithms is that they use an inference-blind representation of the search space, which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion. In this paper, we present a principled approach to address this problem. We introduce a lifted analogue of the propositional And/Or search space framework, which we call a lifted And/Or schematic. Given a schematic-based representation of an SRM, we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic. We show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective Rao-Blackwellisation, and demonstrate experimentally that the Rao-Blackwellised version of the algorithm yields more accurate estimates on several real-world datasets.
C1 [Smith, David; Gogate, Vibhav] Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA.
RP Smith, D (reprint author), Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA.
EM dbs014200@utdallas.edu; vibhav.gogate@utdallas.edu
FU Defense Advanced Research Projects Agency (DARPA) Probabilistic
   Programming for Advanced Machine Learning Program under Air Force
   Research Laboratory (AFRL) [FA8750-14-C-0005]
FX We gratefully acknowledge the support of the Defense Advanced Research
   Projects Agency (DARPA) Probabilistic Programming for Advanced Machine
   Learning Program under Air Force Research Laboratory (AFRL) prime
   contract no. FA8750-14-C-0005. Any opinions, findings, and conclusions
   or recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the view of DARPA, AFRL, or the US
   government.
CR Bidyuk B, 2007, J ARTIF INTELL RES, V28, P1
   Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319
   Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81
   Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002
   de Raedt L., 2008, PROBABILISTIC INDUCT
   Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003
   Genesereth Michael R., 2013, INTRO LOGIC
   Gogate V., 2012, AAAI
   Gogate V., 2011, UAI, P256
   Gogate Vibhav, 2010, STAT RELATIONAL ARTI
   Jha Abhay, 2010, P 24 ANN C NEUR INF, P973
   Milch B., 2007, INTRO STAT RELATIONA, P373
   Niepert M., 2013, 27 AAAI C ART INT, P725
   Niepert M., 2012, UAI 2012 WORKSH STAT
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Poole D., 2011, ARXIV11074035
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Sang T., 2005, P 20 NAT C ART INT A, P475
   Suciu Dan, 2010, NIPS
   Taghipour Nima, 2013, ADV NEURAL INFORM PR, P1052
   Van Den Broeck G., 2011, IJCAI, V3, P2178
   Venugopal D., 2012, NIPS, V25, P1655
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103019
DA 2019-06-15
ER

PT S
AU Sohn, K
   Yan, XC
   Lee, H
AF Sohn, Kihyuk
   Yan, Xinchen
   Lee, Honglak
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Structured Output Representation using Deep Conditional
   Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.
C1 [Sohn, Kihyuk] NEC Labs Amer Inc, Princeton, NJ 08540 USA.
   [Sohn, Kihyuk; Yan, Xinchen; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Sohn, K (reprint author), NEC Labs Amer Inc, Princeton, NJ 08540 USA.
EM ksohn@nec-labs.com; xcyan@umich.edu; honglak@umich.edu
FU ONR [N00014-13-1-0762]; NSF CAREER grant [IIS-1453651]
FX This work was supported in part by ONR grant N00014-13-1-0762 and NSF
   CAREER grant IIS-1453651. We thank NVIDIA for donating a Tesla K40 GPU.
CR Andrew G., 2013, ICML
   Bengio Y., 2014, ICML
   Ciresan D., 2012, NIPS
   Farabet C., 2012, ICML
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Goodfellow I., 2013, NIPS
   Goodfellow I., 2014, NIPS
   He K., 2014, ECCV
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G. B., 2007, 0749 U MASS
   Huang G. B., 2008, CVPR WORKSH PERC ORG
   Kae A., 2013, CVPR
   Kingma D. P., 2013, ICLR
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Krizhevsky A., 2012, NIPS
   Larochelle H., 2011, P 14 INT C ART INT S, P29
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Li Y., 2013, CVPR
   Long  J., 2015, CVPR
   Pinheiro P., 2013, ICML
   Rezende D. J., 2014, ICML
   Salakhutdinov R., 2009, AISTATS
   Sermanet P., 2013, ICLR
   Simonyan K., 2014, ICLR
   Sohn K., 2014, NIPS
   Song HO, 2015, IEEE T PATTERN ANAL, V37, P1001, DOI 10.1109/TPAMI.2014.2353631
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2015, CVPR
   Szegedy C., 2013, NIPS
   Tang Y., 2013, NIPS
   Vedaldi A., 2015, ACMMM
   Vincent P., 2008, ICML
   Wang N., 2012, CVPR
   Welinder P, 2010, CNSTR2010001 CALTECH
   Yang J, 2014, CVPR
NR 37
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101035
DA 2019-06-15
ER

PT S
AU Soma, T
   Yoshida, Y
AF Soma, Tasuku
   Yoshida, Yuichi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Generalization of Submodular Cover via the Diminishing Return Property
   on the Integer Lattice
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice. We are motivated by real scenarios in machine learning that cannot be captured by (traditional) submodular set functions. We show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm. Our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy. The running time of our algorithm is roughly O (n log(nr) log r), where n is the size of the ground set and r is the maximum value of a coordinate. The dependency on r is exponentially better than the naive reduction algorithms. Several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms, while the running time is several orders of magnitude faster.
C1 [Soma, Tasuku] Univ Tokyo, Tokyo, Japan.
   [Yoshida, Yuichi] Natl Inst Informat, Tokyo, Japan.
   [Yoshida, Yuichi] Preferred Infrastruct Inc, Tokyo, Japan.
RP Soma, T (reprint author), Univ Tokyo, Tokyo, Japan.
EM tasukusoma@mist.i.u-tokyo.ac.jp; yyoshida@nii.ac.jp
FU JSPS [26730009]; MEXT [24106003]; JST, ERATO, Kawarabayashi Large Graph
   Project
FX The first author is supported by JSPS Grant-in-Aid for JSPS Fellows. The
   second author is supported by JSPS Grant-in-Aid for Young Scientists (B)
   (No. 26730009), MEXT Grant-in-Aid for Scientific Research on Innovative
   Areas (24106003), and JST, ERATO, Kawarabayashi Large Graph Project. The
   authors thank Satoru Iwata and Yuji Nakatsukasa for reading a draft of
   this paper.
CR Alon N., 2012, P 21 INT C WORLD WID, P381
   Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497, DOI DOI 10.1137/1.9781611973402.110
   Chen Yuxin, 2014, P 31 INT C MACH LEAR, P55
   Iyer R. K., 2013, ADV NEURAL INFORM PR, P2436
   Kapralov M., 2012, P SODA, P1216
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Krause A, 2008, J WATER RES PL-ASCE, V134, P516, DOI 10.1061/(ASCE)0733-9496(2008)134:6(516)
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2014, TRACTABILITY, P71
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   Lin IL, 2010, HUMAN LANGUAGE TECHN, P912
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   Ostfeld A, 2008, J WATER RES PLAN MAN, V134, P556, DOI 10.1061/(ASCE)0733-9496(2008)134:6(556)
   Raz R., 1997, P 29 ANN ACM S THEOR, P475, DOI DOI 10.1145/258533.258641
   Soma T., 2014, P ICML
   Song H. O., 2014, P ICML
   Sviridenko  M., 2015, P 26 ANN ACM SIAM S, P1134
   Wan P.-J., 2009, COMPUTATIONAL OPTIMI, V45, P463
   WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102075
DA 2019-06-15
ER

PT S
AU Sriperumbudur, BK
   Szabo, Z
AF Sriperumbudur, Bharath K.
   Szabo, Zoltan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Optimal Rates for Random Fourier Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID EMPIRICAL CHARACTERISTIC FUNCTION
AB Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in L-r (1 <= r < infinity) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.
C1 [Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
   [Szabo, Zoltan] UCL, Sainsbury Wellcome Ctr, Gatsby Unit, CSML, London W1T 4JG, England.
RP Sriperumbudur, BK (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
EM bks18@psu.edu; zoltan.szabo@gatsby.ucl.ac.uk
FU Gatsby Charitable Foundation
FX Z. Szabo wishes to thank the Gatsby Charitable Foundation for its
   generous support.
CR Alaoui A., 2015, NIPS
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Cotter A., 2011, TECHNICAL REPORT
   CSORGO S, 1981, Z WAHRSCHEINLICHKEIT, V55, P203, DOI 10.1007/BF00535160
   CSORGO S, 1983, ACTA SCI MATH, V45, P141
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   FEUERVERGER A, 1977, ANN STAT, V5, P88, DOI 10.1214/aos/1176343742
   Folland GB, 1999, REAL ANAL MODERN TEC
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lopez-Paz D., 2015, INT C MACHINE LEARNI, P1452
   Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62
   Oliva J, 2015, INT C ARTIFICIAL INT, P717
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607
   Rosasco L., 2010, JMLR WORKSHOP C P, V9, P653
   Rosasco L, 2013, J MACH LEARN RES, V14, P1665
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Shi L, 2010, J COMPUT APPL MATH, V233, P3046, DOI 10.1016/j.cam.2009.11.059
   Shi Q., 2009, INT C ART INT STAT, P496
   Sriperumbudur B. K., 2014, TECHNICAL REPORT
   Strathmann H., 2015, NIPS
   Sutherland D. J., 2015, P UAI, P862
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wendland  H., 2005, SCATTERED DATA APPRO
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Ying YM, 2012, ADV COMPUT MATH, V37, P355, DOI 10.1007/s10444-011-9211-6
   YUKICH JE, 1987, PROBAB THEORY REL, V74, P71
   Zhou DX, 2008, J COMPUT APPL MATH, V220, P456, DOI 10.1016/j.cam.2007.08.023
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100108
DA 2019-06-15
ER

PT S
AU Steinhardt, J
   Liang, P
AF Steinhardt, Jacob
   Liang, Percy
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning with Relaxed Supervision
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.
C1 [Steinhardt, Jacob; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA.
RP Steinhardt, J (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM jsteinhardt@cs.stanford.edu; pliang@cs.stanford.edu
FU Fannie & John Hertz Fellowship; NSF Graduate Research Fellowship;
   Microsoft Research Faculty Fellowship
FX The first author was supported by a Fannie & John Hertz Fellowship and
   an NSF Graduate Research Fellowship. The second author was supported by
   a Microsoft Research Faculty Fellowship. We are also grateful to the
   referees for their valuable comments.
CR Artzi Y, 2013, T ASS COMPUT LINGUIS, V1, P49
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   Chang Angel X., 2014, EMPIRICAL METHODS NA
   Chang M. W., 2007, P 45 ANN M ASS COMP, P280
   Clarke J., 2010, P 14 C COMP NAT LANG, P18
   Druck  G., 2008, P 31 ANN INT ACM SIG, P595
   Fisher M., 2012, ACM SIGGRAPH ASIA, P12
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Gill PE, 2002, SIAM J OPTIMIZ, V12, P979, DOI 10.1137/S1052623499350013
   Gimpel K., 2010, HUM LANG TECHN 2010, P733
   Graca J., 2008, NIPS
   Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423
   He H., 2013, EMNLP
   He H., 2012, ICML INF WORKSH
   Jiang J., 2012, ADV NEURAL INFORM PR
   Liang P., 2008, P 25 INT C MACH LEAR, P584
   Liang P., 2011, HLT 11, P590
   Liang P., 2009, INT C MACH LEARN ICM
   Mann G., 2008, ACL 08 ASS COMPUTATI, P870
   Mansinghka V., 2013, ADV NEURAL INFORM PR, P1520
   Mintz M., 2009, P JOINT C 47 ANN M A, P1003, DOI DOI 10.3115/1690219.1690287
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng A. Y., 1999, INT C MACH LEARN ICM
   Nielsen Frank, 2009, ARXIV09114863
   Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10
   Shi T., 2015, AISTATS
   Steinhardt J, 2015, ICML
   Taskar B., 2013, ADV NEURAL INFORM PR, P953
   Van der Vaart AW, 1998, ASYMPTOTIC STAT
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100051
DA 2019-06-15
ER

PT S
AU Strathmann, H
   Sejdinovic, D
   Livingstone, S
   Szabo, Z
   Gretton, A
AF Strathmann, Heiko
   Sejdinovic, Dino
   Livingstone, Samuel
   Szabo, Zoltan
   Gretton, Arthur
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential
   Families
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONVERGENCE; HASTINGS
AB We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.
C1 [Strathmann, Heiko; Szabo, Zoltan; Gretton, Arthur] UCL, Gatsby Unit, London, England.
   [Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England.
   [Livingstone, Samuel] Univ Bristol, Sch Math, Bristol, Avon, England.
RP Strathmann, H (reprint author), UCL, Gatsby Unit, London, England.
CR Andrieu C, 2008, STAT COMPUT, V18, P343, DOI 10.1007/s11222-008-9110-y
   Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574
   BACHE K., 2013, UCI MACHINE LEARNING
   Beaumont MA, 2003, GENETICS, V164, P1139
   Berlinet A, 2004, REPRODUCING KERNEL H
   Betancourt M., 2015, ARXIV150301916
   Betancourt M, 2015, ARXIV150201510
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Filippone M., 2014, IEEE T PATTERN ANAL
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Haario H, 1999, COMPUTATION STAT, V14, P375, DOI 10.1007/s001800050022
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Le  Q., 2013, ICML
   Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100
   Meeds E., 2015, UAI
   Mengersen KL, 1996, ANN STAT, V24, P101
   Neal RM, 2011, HDB MARKOV CHAIN MON, V2
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2003, BAYESIAN STATISTICS 7, P651
   Roberts GO, 2007, J APPL PROBAB, V44, P458, DOI 10.1239/jap/1183667414
   Roberts GO, 1996, BIOMETRIKA, V83, P95, DOI 10.1093/biomet/83.1.95
   Sejdinovic D., 2014, ICML
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Sisson S. A., 2010, HDB MARKOV CHAIN MON
   Sriperumbudur  B., 2015, NIPS
   Sriperumbudur B., 2014, ARXIV13123516
   Wasserman L, 2006, ALL NONPARAMETRIC ST
   Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319
   Zhang C., 2015, ARXIV150605555
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102039
DA 2019-06-15
ER

PT S
AU Sugiyama, M
   Borgwardt, KM
AF Sugiyama, Mahito
   Borgwardt, Karsten M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Halting in RandomWalk Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Random walk kernels measure graph similarity by counting matching walks in two graphs. In their most popular form of geometric random walk kernels, longer walks of length k are downweighted by a factor of lambda(k) (lambda < 1) to ensure convergence of the corresponding geometric series. We know from the fi eld of link prediction that this downweighting often leads to a phenomenon referred to as halting: Longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1. This is a naive kernel between edges and vertices. We theoretically show that halting may occur in geometric random walk kernels. We also empirically quantify its impact in simulated datasets and popular graph classi fi cation benchmark datasets. Our fi ndings promise to be instrumental in future graph kernel development and applications of random walk kernels.
C1 [Sugiyama, Mahito] Osaka Univ, ISIR, Suita, Osaka 565, Japan.
   [Sugiyama, Mahito] JST, PRESTO, Kawaguchi, Saitama, Japan.
   [Borgwardt, Karsten M.] Swiss Fed Inst Technol, D BSSE, Basel, Switzerland.
RP Sugiyama, M (reprint author), Osaka Univ, ISIR, Suita, Osaka 565, Japan.
EM mahito@ar.sanken.osaka-u.ac.jp; karsten.borgwardt@bsse.ethz.ch
FU JSPS KAKENHI [26880013]; Alfried Krupp von Bohlen und Halbach-Stiftung;
   Marie Curie Initial Training Network MLPM2012 [316861]; SNSF Starting
   Grant 'Significant Pattern Mining'
FX This work was supported by JSPS KAKENHI Grant Number 26880013 (MS), the
   Alfried Krupp von Bohlen und Halbach-Stiftung (KB), the SNSF Starting
   Grant 'Significant Pattern Mining' (KB), and the Marie Curie Initial
   Training Network MLPM2012, Grant No. 316861 (KB).
CR Borgwardt K. M., 2007, THESIS
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Brualdi R. A., 2011, MUTUALLY BENEFICIAL
   Costa F., 2010, P 26 INT C MACH LEAR, P255
   Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Kashima H., 2003, P 20 INT C MACH LEAR, P321
   KATZ L, 1953, PSYCHOMETRIKA, V18, P39
   Kriege N, 2014, IEEE DATA MINING, P881, DOI 10.1109/ICDM.2014.129
   Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591
   Mahe P., 2004, P 21 INT C MACH LEAR
   Shervashidze N., 2009, P NIPS, P1660
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100056
DA 2019-06-15
ER

PT S
AU Sun, K
   Wang, J
   Kalousis, A
   Marchand-Maillet, S
AF Sun, Ke
   Wang, Jun
   Kalousis, Alexandros
   Marchand-Maillet, Stephane
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Space-Time Local Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Space-time is a profound concept in physics. This concept was shown to be useful for dimensionality reduction. We present basic definitions with interesting counter-intuitions. We give theoretical propositions to show that space-time is a more powerful representation than Euclidean space. We apply this concept to manifold learning for preserving local information. Empirical results on non-metric datasets show that more information can be preserved in space-time.
C1 [Sun, Ke; Kalousis, Alexandros; Marchand-Maillet, Stephane] Univ Geneva, Comp Vis & Multimedia Lab, Viper Grp, Geneva, Switzerland.
   [Wang, Jun] Expedia, Geneva, Switzerland.
   [Kalousis, Alexandros] Univ Appl Sci, Business Informat Dept, Delemont, Switzerland.
RP Sun, K (reprint author), Univ Geneva, Comp Vis & Multimedia Lab, Viper Grp, Geneva, Switzerland.
EM sunk.edu@gmail.com; jwang1@expedia.com; Alexandros.Kalousis@hesge.ch;
   Stephane.Marchand-Maillet@unige.ch
FU Department of Computer Science, University of Geneva; Swiss National
   Science Foundation Project MAAYA [144238]
FX This work has been supported be the Department of Computer Science,
   University of Geneva, in collaboration with Swiss National Science
   Foundation Project MAAYA (Grant number 144238).
CR Cook J., 2007, P INT C ART INT STAT, P67
   GOLDFARB L, 1984, PATTERN RECOGN, V17, P575, DOI 10.1016/0031-3203(84)90056-6
   Hinton G., 2003, ADV NEURAL INFORM PR, V15, P833
   Jost J., 2011, RIEMANNIAN GEOMETRY
   Laub J, 2004, J MACH LEARN RES, V5, P801
   Laub J., 2007, ADV NEURAL INFORM PR, P777
   Lawrence N., 2011, P INT C ART INT STAT, P51
   Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727
   Lunga D, 2013, IEEE T GEOSCI REMOTE, V51, P857, DOI 10.1109/TGRS.2012.2205004
   Nelson D. L., 1998, U S FLORIDA WORD ASS
   O'Neill B, 1983, PURE APPL MATH
   Pekalska E., 2005, DISSIMILARITY REPRES
   van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Weinberger K.Q., 2004, P 21 INT C MACH LEAR, P839
   Wilson RC, 2010, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2010.5539863
   Zeger K., 1994, Proceedings. 1994 IEEE International Symposium on Information Theory (Cat. No.94CH3467-8), DOI 10.1109/ISIT.1994.394879
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103009
DA 2019-06-15
ER

PT S
AU Sun, Q
   Batra, D
AF Sun, Qing
   Batra, Dhruv
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image. Since the number of possible bounding boxes in an image is very large O(#pixels(2)), even a single linear scan to perform the greedy augmentation for submodular maximization is intractable. Thus, we formulate the greedy augmentation step as a Branch-and-Bound scheme. In order to speed up repeated application of B&B, we propose a novel generalization of Minoux's 'lazy greedy' algorithm to the B&B tree. Theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as Sliding Window+ Non-Maximal Suppression (NMS) and and Efficient Subwindow Search (ESS) as special cases. Empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.
C1 [Sun, Qing; Batra, Dhruv] Virginia Tech, Blacksburg, VA 24061 USA.
RP Sun, Q (reprint author), Virginia Tech, Blacksburg, VA 24061 USA.
EM sunqing@vt.edu
FU National Science Foundation CAREER award; Army Research Office YIP
   award; Office of Naval Research grant; AWS in Education Research Grant;
   NVIDIA
FX This work was partially supported by a National Science Foundation
   CAREER award, an Army Research Office YIP award, an Office of Naval
   Research grant, an AWS in Education Research Grant, and GPU support by
   NVIDIA. The views and conclusions contained herein are those of the
   authors and should not be interpreted as necessarily representing the
   official policies or endorsements, either expressed or implied, of the
   U.S. Government or any sponsor.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Arbelaez Pablo, 2014, CVPR
   Blaschko M., 2008, ECCV
   Blaschko Matthew B., 2011, Energy Minimization Methods in Computer Vision and Pattern Recognition. Proceedings 8th International Conference, EMMCVPR 2011, P385, DOI 10.1007/978-3-642-23094-3_28
   Buchbinder N., 2012, FOCS
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Carreira J., 2010, CVPR
   Cheng M. M., 2014, CVPR
   Dalal  N., 2005, CVPR
   Deselaers T., 2010, ECCV
   Dey D, 2012, ROBOTICS SCI SYSTEMS
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Everingham M., 2007, PASCAL VISUAL OBJECT
   Feige U., 2007, FOCS
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R., 2014, CVPR
   Gonzalez-Garcia Abel, 2015, CVPR
   He K., 2014, ECCV
   Hosang J., 2014, BMVC
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/s10994-009-5108-8, 10.1007/S10994-009-5108-8]
   Kempe D., 2003, ACM SIGKDD C KNOWL D
   Krahenbuhl P., 2015, CVPR
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2014, TRACTABILITY, P71
   Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144
   LAWLER EL, 1966, OPER RES, V14, P699, DOI 10.1287/opre.14.4.699
   Lin H., 2011, ACL
   Lin T.-Y., 2014, ECCV
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Prasad A., 2014, NIPS
   Ren S., 2015, NIPS
   Ross S., 2013, ICML
   Sermanet  P., 2014, ICLR
   Streeter  Matthew, 2008, NIPS
   Szegedy C., 2013, NIPS
   Szegedy C., 2014, CVPR
   Taskar B, 2003, NIPS
   Uijlings J., 2013, IJCV
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Zitnick C. L., 2014, ECCV
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101039
DA 2019-06-15
ER

PT S
AU Sun, RY
   Hong, MY
AF Sun, Ruoyu
   Hong, Mingyi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent
   for Convex Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONVERGENCE; OPTIMIZATION; MINIMIZATION
AB The iteration complexity of the block-coordinate descent (BCD) type algorithm has been under extensive investigation. It was recently shown that for convex problems the classical cyclic BCGD (block coordinate gradient descent) achieves an O(1/r) complexity (r is the number of passes of all blocks). However, such bounds are at least linearly depend on K (the number of variable blocks), and are at least K times worse than those of the gradient descent (GD) and proximal gradient (PG) methods. In this paper, we close such theoretical performance gap between cyclic BCD and GD/PG. First we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic Block Coordinate Proximal Gradient (BCPG), a popular variant of BCD, can match those of the GD/PG in terms of dependency on K (up to a log(2) (K) factor). Second, we establish an improved complexity bound for Coordinate Gradient Descent (CGD) for general convex problems which can match that of GD in certain scenarios. Our bounds are sharper than the known bounds as they are always at least K times worse than GD. Our analyses do not depend on the update order of block variables inside each cycle, thus our results also apply to BCD methods with random permutation (random sampling without replacement, another popular variant).
C1 [Sun, Ruoyu] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.
   [Hong, Mingyi] Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA USA.
   [Hong, Mingyi] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA USA.
RP Sun, RY (reprint author), Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.
EM ruoyu@stanford.edu; mingyi@iastate.edu
CR ANGELOS JR, 1992, LINEAR ALGEBRA APPL, V170, P117, DOI 10.1016/0024-3795(92)90414-6
   Beck A., 2015, CYCLIC BLOCK COORDIN
   Beck A, 2015, SIAM J OPTIMIZ, V25, P185, DOI 10.1137/13094829X
   Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   Hong M., 2013, ITERATION COMPLEXITY
   Lu Z., 2013, RANDOMIZED BLOCK COO
   Lu Z., 2013, MATH PROGRAMMING
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nutini J., 2015, P 30 INT C MACH LEAR
   Powell M. J. D., 1973, MATH PROGRAM, V4, P193, DOI DOI 10.1007/BF01584660
   Razaviyayn M., 2014, P NEUR INF PROC NIPS
   Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103066
DA 2019-06-15
ER

PT S
AU Sun, SQ
   Kolar, M
   Xu, JB
AF Sun, Siqi
   Kolar, Mladen
   Xu, Jinbo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Structured Densities via Infinite Dimensional Exponential
   Families
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODEL; SELECTION
AB Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.
C1 [Sun, Siqi; Xu, Jinbo] TTI Chicago, Chicago, IL 60637 USA.
   [Kolar, Mladen] Univ Chicago, Chicago, IL 60637 USA.
RP Sun, SQ (reprint author), TTI Chicago, Chicago, IL 60637 USA.
EM siqi.sun@ttic.edu; mkolar@chicagobooth.edu; jinbo.xu@gmail.com
FU National Institutes of Health [R01GM0897532]; National Science
   Foundation CAREER award [CCF-1149811]; IBM Corporation Faculty Research
   Fund at the University of Chicago Booth School of Business
FX The authors are grateful to the financial support from National
   Institutes of Health R01GM0897532, National Science Foundation CAREER
   award CCF-1149811 and IBM Corporation Faculty Research Fund at the
   University of Chicago Booth School of Business. This work was completed
   in part with resources provided by the University of Chicago Research
   Computing Center.
CR Albert R, 2005, J CELL SCI, V118, P4947, DOI 10.1242/jcs.02714
   Ambroise C, 2009, ELECTRON J STAT, V3, P205, DOI 10.1214/08-EJS314
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009
   Defazio A, 2012, ADV NEURAL INFORM PR, P1250
   Friedman J., 2010, ARXIV10010736
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Fukumizu K, 2007, J MACH LEARN RES, V8, P361
   Geman S., 1986, P INT C MATH PROV RI, V1, P2
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Jeon YH, 2006, STAT SINICA, V16, P353
   Kindermann R, 1980, MARKOV RANDOM FIELDS, V1
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kourmpetis YAI, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0009293
   Lafferty J., 2001, CONDITIONAL RANDOM F
   Li S. Z., 2011, MARKOV RANDOM FIELD
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Liu Q., 2011, INT C ART INT STAT, P40
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   RAVIKUMAR P., 2008, HIGH DIMENSIONAL GRA
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Rockafellar R T, 1970, CONVEX ANAL
   Sriperumbudur B., 2013, ARXIV13123516
   Sun S., 2015, P 18 INT C ART INT S, P939
   Wei Z, 2007, BIOINFORMATICS, V23, P1537, DOI 10.1093/bioinformatics/btm129
   Yang E., 2012, ADV NEURAL INFORM PR, V25, P1358
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
   Zhao T, 2012, J MACH LEARN RES, V13, P1059
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103044
DA 2019-06-15
ER

PT S
AU Swaminathan, A
   Joachims, T
AF Swaminathan, Adith
   Joachims, Thorsten
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Self-Normalized Estimator for Counterfactual Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm - Normalized Policy Optimizer for Exponential Models (Norm-POEM) - for structured output prediction using linear rules. We evaluate the empirical effectiveness of Norm-POEM on several multi-label classification problems, finding that it consistently outperforms the conventional estimator.
C1 [Swaminathan, Adith; Joachims, Thorsten] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Swaminathan, A (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
EM adith@cs.cornell.edu; tj@cs.cornell.edu
FU NSF [IIS-1247637, IIS-1217686, IIS-1513692]; JTCII Cornell-Technion
   Research Fund
FX This research was funded in part through NSF Awards IIS-1247637,
   IIS-1217686, IIS-1513692, the JTCII Cornell-Technion Research Fund, and
   a gift from Bloomberg.
CR Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Boyle P, 1997, J ECON DYN CONTROL, V21, P1267, DOI 10.1016/S0165-1889(97)00028-6
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cortes C, 2010, ADV NEURAL INFORM PR, P442
   Dudik M., 2011, P 28 INT C MACH LEAR, P1097
   HESTERBERG T, 1995, TECHNOMETRICS, V37, P185, DOI 10.2307/1269620
   Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456
   KONG A., 1992, 348 U CHIC DEP STAT
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Langford J, 2008, ICML 2008, P528
   Lewis AS, 2013, MATH PROGRAM, V141, P135, DOI 10.1007/s10107-012-0514-2
   Li L., 2015, AISTATS
   Maurer Andreas, 2009, COLT
   Owen AB, 2013, MONTE CARLO THEORY M
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41
   Rubinstein R Y, 2008, SIMULATION MONTE CAR
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Strehl A., 2010, ADV NEURAL INFORM PR, P2217
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Swaminathan A., 2015, ICML
   Thomas Philip S., 2015, AAAI, P3000
   Trotter H. F., 1956, S MONTE CARLO METHOD, P64
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Yu J, 2010, J MACH LEARN RES, V11, P1145
   Zadrozny B, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P435
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101008
DA 2019-06-15
ER

PT S
AU Syrgkanis, V
   Agarwal, A
   Luo, HP
   Schapire, RE
AF Syrgkanis, Vasilis
   Agarwal, Alekh
   Luo, Haipeng
   Schapire, Robert E.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Convergence of Regularized Learning in Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T-3/4), while the sum of utilities converges to an approximate optimum at O(T-1)-an improvement upon the worst case O(T-1/2) rates. We show a black-box reduction for any algorithm in the class to achieve (O) over tilde (T-1/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [17] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.
C1 [Syrgkanis, Vasilis; Agarwal, Alekh; Schapire, Robert E.] Microsoft Res, New York, NY 10011 USA.
   [Luo, Haipeng] Princeton Univ, Princeton, NJ 08544 USA.
RP Syrgkanis, V (reprint author), Microsoft Res, New York, NY 10011 USA.
EM vasy@microsoft.com; alekha@microsoft.com; haipengl@cs.princeton.edu;
   schapire@microsoft.com
CR Blum A, 2007, ALGORITHMIC GAME THEORY, P79
   Blum A, 2008, ACM S THEORY COMPUT, P373
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003
   Edelman B, 2005, 11765 NAT BUR EC RES
   Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   HOEFFDING W, 1958, ANN MATH STAT, V29, P700, DOI 10.1214/aoms/1177706531
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Peysakhovich Alexander, 2014, P 15 ACM C EC COMP, P971
   Rakhlin Alexander, 2013, COLT, P993
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Roughgarden T, 2009, ACM S THEORY COMPUT, P513
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Syrgkanis V., 2013, P 45 ANN ACM S THEOR, P211, DOI DOI 10.1145/2488608.2488635
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101023
DA 2019-06-15
ER

PT S
AU Szorenyi, B
   Busa-Fekete, R
   Paul, A
   Hullermeier, E
AF Szorenyi, Balazs
   Busa-Fekete, Robert
   Paul, Adil
   Huellermeier, Eyke
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of online rank elicitation, assuming that rankings of a set of alternatives obey the Plackett-Luce distribution. Following the setting of the dueling bandits problem, the learner is allowed to query pairwise comparisons between alternatives, i.e., to sample pairwise marginals of the distribution in an online fashion. Using this information, the learner seeks to reliably predict the most probable ranking (or top-alternative). Our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure, for which the pairwise marginals provably coincide with the marginals of the Plackett-Luce distribution. In addition to a formal performance and complexity analysis, we present first experimental studies.
C1 [Szorenyi, Balazs] Technion, Haifa, Israel.
   [Szorenyi, Balazs] MTA SZTE Res Grp Artificial Intelligence, Szeged, Hungary.
   [Busa-Fekete, Robert; Paul, Adil; Huellermeier, Eyke] Univ Paderborn, Dept Comp Sci, Paderborn, Germany.
RP Szorenyi, B (reprint author), Technion, Haifa, Israel.
EM szorenyibalazs@gmail.com; busarobi@upb.de; adil.paul@upb.de; eyke@upb.de
CR Braverman M., 2009, ABS09101191 CORR
   Braverman M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P268
   Bubeck S., 2013, P 30 INT C MACH LEAR, V28, P258
   Bubeck S, 2009, LECT NOTES ARTIF INT, V5809, P23
   Busa-Fekete R, 2014, AAAI, P1701
   Busa-Fekete R., 2014, P 31 INT C MACH LEAR, V32, P1071
   Busa-Fekete R., 2013, P 30 ICML JMLR W CP, V28
   Busa-Fekete R, 2014, LECT NOTES ARTIF INT, V8776, P18, DOI 10.1007/978-3-319-11662-4_3
   Clopper CJ, 1934, BIOMETRIKA, V26, P404, DOI 10.2307/2331986
   Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255
   FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877
   Gabillon V., 2011, ADV NEURAL INFORM PR, P2222
   Guiver J., 2009, P 26 ANN INT C MACH, P377
   HOARE CAR, 1962, COMPUT J, V5, P10, DOI 10.1093/comjnl/5.1.10
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Hunter DR, 2004, ANN STAT, V32, P384
   Luce R., 1965, HDB MATH PSYCHOL, V3, P249
   Luce R. D., 1959, INDIVIDUAL CHOICE BE
   MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244
   Marden J.I., 1995, ANAL MODELING RANK D
   McDiarmid CJH, 1996, J ALGORITHM, V21, P476, DOI 10.1006/jagm.1996.0055
   Negahban S., 2012, NIPS, P2483
   Nir Ailon, 2008, ADV NEURAL INFORM PR, V21, P25
   PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193
   Rajkumar Arun, 2014, P 31 INT C MACH LEAR, P118
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
   Urvoy T., 2013, P 30 INT C MACH LEAR, V28, P91
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Zoghi M, 2014, P INT C MACH LEARN I, P10
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102052
DA 2019-06-15
ER

PT S
AU Takenouchi, T
   Kanamori, T
AF Takenouchi, Takashi
   Kanamori, Takafumi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Empirical Localization of Homogeneous Divergences on Discrete Sample
   Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the information geometry. Some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost.
C1 [Takenouchi, Takashi] Future Univ Hakodate, Dept Complex & Intelligent Syst, 116-2 Kamedanakano, Hakodate, Hokkaido 0408655, Japan.
   [Kanamori, Takafumi] Nagoya Univ, Dept Comp Sci & Math Informat, Chikusa Ku, Nagoya, Aichi 4648601, Japan.
RP Takenouchi, T (reprint author), Future Univ Hakodate, Dept Complex & Intelligent Syst, 116-2 Kamedanakano, Hakodate, Hokkaido 0408655, Japan.
EM ttakashi@fun.ac.jp; kanamori@is.nagoya-u.ac.jp
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   AMARI S, 1992, IEEE T NEURAL NETWOR, V3, P260, DOI 10.1109/72.125867
   Amari S. - I., 2000, TRANSLATIONS MATH MO, V191
   Dawid P, 2012, ANN STAT, V40, P593, DOI 10.1214/12-AOS972
   Fujisawa H, 2008, J MULTIVARIATE ANAL, V99, P2053, DOI 10.1016/j.jmva.2008.02.004
   Good I. J, 1971, FDN STAT INFERENCE, P337
   Gutmann M, 2012, ARXIV12023727
   Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P282
   Hinton G. E., 2012, ADV NEURAL INFORM PR, P2447
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Opper  M., 2001, ADV MEAN FIELD METHO
   R Core Team, 2013, R LANG ENV STAT COMP
   Sejnowski T. J., 1986, AIP Conference Proceedings, P398
   Van der Vaart AW, 1998, ASYMPTOTIC STAT
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101096
DA 2019-06-15
ER

PT S
AU Talwar, K
   Thakurta, A
   Zhang, L
AF Talwar, Kunal
   Thakurta, Abhradeep
   Zhang, Li
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Nearly-Optimal Private LASSO
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present a nearly optimal differentially private version of the well known LASSO estimator. Our algorithm provides privacy protection with respect to each training example. The excess risk of our algorithm, compared to the non-private version, is (O) over tilde (1/n(2/3)), assuming all the input data has bounded l(infinity) norm. This is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix. In addition, we show that this error bound is nearly optimal amongst all differentially private algorithms.
C1 [Talwar, Kunal; Zhang, Li] Google Res, Mountain View, CA 94043 USA.
   [Thakurta, Abhradeep] Yahoo Labs, New York, NY USA.
   [Talwar, Kunal; Thakurta, Abhradeep; Zhang, Li] Microsoft Res, Mountain View, CA USA.
RP Talwar, K (reprint author), Google Res, Mountain View, CA 94043 USA.
EM kunal@google.com; guhathakurta.abhradeep@gmail.com; liqzhang@google.com
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bassily R., 2014, FOCS
   Bhaskar Raghav, 2010, KDD
   Bun Mark, 2014, STOC
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chaudhuri K., 2008, NIPS
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Clarkson KL, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1824777.1824783
   Duchi John C., 2013, FOCS
   Dwork C., 2013, ARXIV13081385
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork Cynthia, 2014, STOC
   Dwork Cynthia, 2014, FDN TRENDS THEORETIC
   Dwork Cynthia, 2010, FOCS
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Hazan E., 2012, ICML
   Jaggi  M., 2013, ICML
   Jain P., 2014, INT C MACH LEARN ICM
   Jain P., 2012, COLT
   Kifer D., 2012, COLT
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Nikolov Aleksandar, 2013, STOC
   Shalev-Shwartz S., 2010, SIAM J OPTIMIZATION
   Smith A., 2013, NIPS
   Smith A., 2013, UNPUB
   Smith Adam, 2013, COLT
   Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3
   Tibshirani Robert, 1996, J ROYAL STAT SOC B
   Ullman J., 2014, ABS14071571 CORR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100097
DA 2019-06-15
ER

PT S
AU Tamar, A
   Chow, Y
   Ghavamzadeh, M
   Mannor, S
AF Tamar, Aviv
   Chow, Yinlam
   Ghavamzadeh, Mohammad
   Mannor, Shie
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Policy Gradient for Coherent Risk Measures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID OPTIMIZATION
AB Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.
C1 [Tamar, Aviv] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Chow, Yinlam] Stanford Univ, Stanford, CA 94305 USA.
   [Ghavamzadeh, Mohammad] Adobe Res, San Jose, CA USA.
   [Ghavamzadeh, Mohammad] INRIA, Rocquencourt, France.
   [Mannor, Shie] Technion, Haifa, Israel.
RP Tamar, A (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM avivt@berkeley.edu; ychow@stanford.edu; mohammad.ghavamzadeh@inria.fr;
   shie@ee.technion.ac.il
FU European Research Council under the European Unions Seventh Framework
   Program (FP7/2007-2013) / ERC [306638]; Croucher Foundation
FX The research leading to these results has received funding from the
   European Research Council under the European Unions Seventh Framework
   Program (FP7/2007-2013) / ERC Grant Agreement n. 306638. Yinlam Chow is
   partially supported by Croucher Foundation Doctoral Scholarship.
CR Acerbi C, 2002, J BANK FINANC, V26, P1505, DOI 10.1016/S0378-4266(02)00281-9
   Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068
   Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0
   Bardou O, 2009, MONTE CARLO METHODS, V15, P173, DOI 10.1515/MCMA.2009.011
   Bertsekas D, 2012, DYNAMIC PROGRAMMING
   Borkar VS, 2001, SYST CONTROL LETT, V44, P339, DOI 10.1016/S0167-6911(01)00152-9
   Boyd S., 2009, CONVEX OPTIMIZATION
   Chow Y., 2014, NIPS 27
   Chow Y, 2014, AM CONTR C
   Delage E., 2010, OPER RES, V58
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   HADAR J, 1969, AM ECON REV, V59, P25
   Iancu D., 2011, ARXIV11066102
   Konda V., 2000, NIPS
   Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687
   Markowitz H. M., 1959, PORTFOLIO SELECTION
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   Moody J, 2001, IEEE T NEURAL NETWOR, V12, P875, DOI 10.1109/72.935097
   Osogami T., 2012, NIPS
   Petrik M., 2012, UAI
   Prashanth L., 2013, NIPS 26
   Rachev S., 2000, STABLE PARETIAN MODE
   Rockafellar R, 2000, J RISK, V2, P21, DOI DOI 10.21314/JOR.2000.038
   Ruszczynski A, 2006, MATH OPER RES, V31, P433, DOI 10.1287/moor.1050.0186
   Ruszczynski A, 2010, MATH PROGRAM, V125, P235, DOI 10.1007/s10107-010-0393-3
   Shapiro A., 2009, LECT STOCHASTIC PROG, P253
   Sutton R., 2000, NIPS 13
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tamar A., 2012, INT C MACH LEARN
   Tamar A., 2014, INT C MACH LEARN
   Tamar A., 2015, AAAI
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102071
DA 2019-06-15
ER

PT S
AU Thomas, PS
   Niekum, S
   Theocharous, G
   Konidaris, G
AF Thomas, Philip S.
   Niekum, Scott
   Theocharous, Georgios
   Konidaris, George
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Policy Evaluation Using the Omega-Return
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose the Omega-return as an alternative to the Omega-return currently used by the TD (lambda) family of algorithms. The benefit of the Omega-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly, we suggest one way of approximating the Omega-return. We provide empirical studies that suggest that it is superior to the lambda-return and gamma-return for a variety of problems.
C1 [Thomas, Philip S.] Carnegie Mellon Univ, Univ Massachusetts Amherst, Pittsburgh, PA 15213 USA.
   [Niekum, Scott] Univ Texas Austin, Austin, TX 78712 USA.
   [Theocharous, Georgios] Adobe Res, San Jose, CA USA.
   [Konidaris, George] Duke Univ, Durham, NC 27706 USA.
RP Thomas, PS (reprint author), Carnegie Mellon Univ, Univ Massachusetts Amherst, Pittsburgh, PA 15213 USA.
CR Blana D, 2009, MED BIOL ENG COMPUT, V47, P533, DOI 10.1007/s11517-009-0479-3
   Bradtke SJ, 1996, MACH LEARN, V22, P33, DOI 10.1023/A:1018056104778
   Downey C., 2010, P 27 INT C MACH LEAR, P311
   Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398
   Jagodnik K., 2007, 12 ANN C INT FES SOC
   Kariya T., 2004, GEN LEAST SQUARES
   Konidaris G., 2011, ADV NEURAL INFORM PR, P2402
   Konidaris G., 2011, P 25 C ART INT, P380
   Mahmood AR, 2014, ADV NEUR IN, V27
   Pilarski Patrick M, 2011, IEEE Int Conf Rehabil Robot, V2011, P5975338, DOI 10.1109/ICORR.2011.5975338
   Precup D., 2000, INT C MACH LEARN, P759
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tetreault J. R., 2006, P HUM LANG TECHN N A
   Theocharous G., 2013, 1 MULT C REINF LEARN
   Thomas P., 2015, P 29 C ART INT
   Thomas Philip, 2009, Proc Innov Appl Artif Intell Conf, V2009, P165
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101067
DA 2019-06-15
ER

PT S
AU Thrampoulidis, C
   Abbasi, E
   Hassibi, B
AF Thrampoulidis, Christos
   Abbasi, Ehsan
   Hassibi, Babak
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI LASSO with Non-linear Measurements is Equivalent to One With Linear
   Measurements
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGRESSION; SENSITIVITY; SELECTION; RISK
AB Consider estimating an unknown, but structured (e.g. sparse, low-rank, etc.), signal x(0) is an element of R-n from a vector y is an element of R-m of measurements of the form y(i) = g(i)(a(i)(T) x(0)), where the a(i)'s are the rows of a known measurement matrix A, and, g(.) is a (potentially unknown) nonlinear and random link-function. Such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties. It could also arise by design, e.g., g(i)(x) = sign(x + z(i)), corresponds to noisy 1-bit quantized measurements. Motivated by the classical work of Brillinger, and more recent work of Plan and Vershynin, we estimate x(0) via solving the Generalized-LASSO, i.e., (x) over cap := arg min(x) parallel to y - Ax(0)parallel to(2) + lambda f(x) for some regularization parameter lambda > 0 and some (typically non-smooth) convex regularizer f(.) that promotes the structure of x(0), e.g. l(1)-norm, nuclear-norm, etc. While this approach seems to naively ignore the nonlinear function g(.), both Brillinger (in the non-constrained case) and Plan and Vershynin have shown that, when the entries of A are iid standard normal, this is a good estimator of x(0) up to a constant of proportionality mu, which only depends on g (.). In this work, we considerably strengthen these results by obtaining explicit expressions for parallel to(x) over cap - mu x(0)parallel to(2), for the regularized Generalized-LASSO, that are asymptotically precise when m and n grow large. A main result is that the estimation performance of the Generalized LASSO with non-linear measurements is asymptotically the same as one whose measurements are linear y(i) = mu a(i)(T)x(0) + sigma z(i), with mu = E gamma g(gamma) and sigma(2) = E(g(gamma) - mu gamma)(2), and, gamma standard normal. To the best of our knowledge, the derived expressions on the estimation performance are the first-known precise results in this context. One interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the Generalized LASSO is the celebrated Lloyd-Max quantizer.
C1 [Thrampoulidis, Christos; Abbasi, Ehsan; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
RP Thrampoulidis, C (reprint author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
EM cthrampo@caltech.edu; eabbasi@caltech.edu; hassibi@caltech.edu
FU National Science Foundation [CNS-0932428, CCF-1018927, CCF-1423663,
   CCF-1409204]; Qualcomm Inc.; NASA's Jet Propulsion Laboratory; King
   Abdulaziz University; King Abdullah University of Science and Technology
FX This work was supported in part by the National Science Foundation under
   grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204, by a grant
   from Qualcomm Inc., by NASA's Jet Propulsion Laboratory through the
   President and Directors Fund, by King Abdulaziz University, and by King
   Abdullah University of Science and Technology.
CR Bach Francis R, 2010, ADV NEURAL INFORM PR, P118
   Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612
   Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043
   Brillinger D. R, 1982, FESTSCHRIFT EL LEHMA, P97
   BRILLINGER DR, 1977, BIOMETRIKA, V64, P509, DOI 10.1093/biomet/64.3.509
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356
   Donoho DL, 2011, IEEE T INFORM THEORY, V57, P6920, DOI 10.1109/TIT.2011.2165823
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026
   El Halabi Marwa, 2014, ARXIV14111990
   Garnham AL, 2013, ELECTRON J STAT, V7, P1983, DOI 10.1214/13-EJS831
   Gordon Y., 1988, MILMANS INEQUALITY R
   ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K
   LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254
   Oymak Samet, 2013, ARXIV13110830
   Plan Y., 2015, ARXIV150204071
   Stojnic  Mihailo, 2013, ARXIV13037291
   Thrampoulidis C., 2015, P 28 C LEARN THEOR, P1683
   Thrampoulidis C, 2015, IEEE INT SYMP INFO, P2021
   Thrampoulidis C, 2015, INT CONF ACOUST SPEE, P3467, DOI 10.1109/ICASSP.2015.7178615
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   YI X., 2015, ARXIV150503257
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100107
DA 2019-06-15
ER

PT S
AU Tian, T
   Zhu, J
AF Tian, Tian
   Zhu, Jun
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Max-Margin Majority Voting for Learning from Crowds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID POSTERIOR REGULARIZATION
AB Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting ((MV)-V-3) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and (MV)-V-3. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.
C1 [Tian, Tian] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   Tsinghua Univ, Ctr Bioinspired Comp Res, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
RP Tian, T (reprint author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM tiant13@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2013CB329403,
   2012CB316301]; National NSF of China [61322308, 61332007]; Tsinghua
   National Laboratory for Information Science and Technology Big Data
   Initiative; Tsinghua Initiative Scientific Research Program
   [20121088071, 20141080934]
FX The work was supported by the National Basic Research Program (973
   Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of
   China (Nos. 61322308, 61332007), Tsinghua National Laboratory for
   Information Science and Technology Big Data Initiative, and Tsinghua
   Initiative Scientific Research Program (Nos. 20121088071, 20141080934).
CR Carlson  Andrew, 2010, AAAI
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen C., 2014, NIPS
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Dudik M., 2007, JMLR, V8
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Jagabathula S., 2014, NIPS
   Karger D. R., 2011, NIPS
   Li H., 2014, ARXIV14114086
   Liu Q., 2012, NIPS
   MICHAEL JR, 1976, AM STAT, V30, P88, DOI 10.2307/2683801
   Otto C., 2014, IEEE T PAMI
   Polson NG, 2011, BAYESIAN ANAL, V6, P1, DOI 10.1214/11-BA601
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Shi T., 2014, ICML
   Snow R., 2008, EMNLP
   Tian T., 2015, PAKDD
   Welinder P., 2010, NIPS
   Whitehill J., 2009, NIPS
   Xu L., 2005, AAAI
   Zaidan O. F., 2011, ACL
   Zhang Y., 2014, NIPS
   Zhou D., 2014, ICML
   Zhou D., 2012, NIPS
   Zhu J, 2014, J MACH LEARN RES, V15, P1799
   Zhu J, 2014, J MACH LEARN RES, V15, P1073
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103014
DA 2019-06-15
ER

PT S
AU Titsias, MK
   Lazaro-Gredilla, M
AF Titsias, Michalis K.
   Lazaro-Gredilla, Miguel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Local Expectation Gradients for Black Box Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance. Our method works efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, can be trivially parallelized.
C1 [Titsias, Michalis K.] Athens Univ Econ & Business, Athens, Greece.
   [Lazaro-Gredilla, Miguel] Vicarious, San Francisco, CA USA.
RP Titsias, MK (reprint author), Athens Univ Econ & Business, Athens, Greece.
EM mtitsias@aueb.gr; miguel@vicarious.com
CR Bishop C. M., 2006, PATTERN RECOGNITION
   Bornschein Jrg, 2014, REWEIGHTED WAKE SLEE, P1
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hoffman M., 2010, ADV NEURAL INFORM PR, P856
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D.P., 2013, ARXIV13126114
   Kucukelbir A, 2015, ADV NEUR IN, V28
   Mnih Andriy, 2014, 31 INT C MACH LEARN
   Mohamed Shakir, 2014, 31 INT C MACH LEARN
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Paisley J. W., 2012, ICML
   Peters J., 2006, P IEEE INT C INT ROB
   Ranganath Rajesh, 2014, P 17 INT C ART INT S
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   Salimans Tim, 2014, USING CONTROL VARIAT
   Titsias Michalis K., 2014, 31 INT C MACH LEARN
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100046
DA 2019-06-15
ER

PT S
AU Tobar, F
   Bui, TD
   Turner, RE
AF Tobar, Felipe
   Bui, Thang D.
   Turner, Richard E.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning Stationary Time Series using Gaussian Processes with
   Nonparametric Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce the Gaussian Process Convolution Model (GPCM), a two-stage non-parametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from Gaussian process. The GPCM is a continuous-time nonparametric-window moving average process and, conditionally, is itself a Gaussian process with a nonparametric kernel defined in a probabilistic fashion. The generative model can be equivalently considered in the frequency domain, where the power spectral density of the signal is specified using a Gaussian process. One of the main contributions of the paper is to develop a novel variational free-energy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process. In turn, this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios. Additionally, the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data, leading to new Bayesian nonparametric approaches to spectrum estimation. The proposed GPCM is validated using synthetic and real-world signals.
C1 [Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago, Chile.
   [Bui, Thang D.; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England.
RP Tobar, F (reprint author), Univ Chile, Ctr Math Modeling, Santiago, Chile.
EM ftobar@dim.uchile.cl; tdb40@cam.ac.uk; ret26@cam.ac.uk
FU CONICYT-PAI grant [82140061]; Basal-CONICYT Center for Mathematical
   Modeling (CMM); EPSRC [EP/L000776/1, EP/M026957/1]
FX Part of this work was carried out when F.T. was with the University of
   Cambridge. F.T. thanks CONICYT-PAI grant 82140061 and Basal-CONICYT
   Center for Mathematical Modeling (CMM). R.T. thanks EPSRC grants
   EP/L000776/1 and EP/M026957/1. T.B. thanks Google. We thank Mark
   Rowland, Shane Gu and the anonymous reviewers for insightful feedback.
CR Archambeau C., 2007, J MACHINE LEARN RES, V1, P1
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bui T., 2014, ADV NEURAL INFORM PR, P2213
   Duvenaud D., 2011, ADV NEURAL INFORM PR, P226
   Duvenaud DK, 2013, P INT C MACH LEARN I, V3, P1166
   Figueiras-vidal Anibal, 2009, ADV NEURAL INFORM PR, P1087
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   GULL SF, 1989, FUND THEOR, V36, P53
   Hensman J., 2015, ARXIV150407027
   Jazwinski A. H., 1970, STOCHASTIC PROCESSES
   MacKay D. J, 2003, INFORM THEORY INFERE
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   Minka  T., 2000, TECH REP
   Oksendal B., 2003, STOCHASTIC DIFFERENT
   Oppenheim A. V., 1997, SIGNALS SYSTEMS
   Percival DB, 1993, SPECTRAL ANAL PHYS A
   Qi Y, 2002, INT CONF ACOUST SPEE, P1473
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Titsias M, 2009, ARTIF INTELL, P567
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   Tobar FA, 2014, IEEE T NEUR NET LEAR, V25, P265, DOI 10.1109/TNNLS.2013.2272594
   Turner R., 2010, THESIS
   Turner RE, 2014, IEEE T SIGNAL PROCES, V62, P6171, DOI 10.1109/TSP.2014.2362100
   Turner Richard E., 2011, BAYESIAN TIME SERIES, P109
   Wilson A. G., 2013, P INT C MACH LEARN
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101032
DA 2019-06-15
ER

PT S
AU Tran, D
   Blei, DM
   Airoldi, EM
AF Tran, Dustin
   Blei, David M.
   Airoldi, Edoardo M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Copula variational inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.
C1 [Tran, Dustin; Airoldi, Edoardo M.] Harvard Univ, Cambridge, MA 02138 USA.
   [Blei, David M.] Columbia Univ, New York, NY 10027 USA.
RP Tran, D (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
FU NSF [IIS-0745520, IIS-1247664, IIS-1009542]; ONR [N00014-11-1-0651];
   DARPA [FA8750-14-2-0009, N66001-15-C-4032]; John Templeton Foundation;
   Facebook; Adobe; Amazon
FX We thank Luke Bornn, Robin Gong, and Alp Kucukelbir for their insightful
   comments. This work is supported by NSF IIS-0745520, IIS-1247664,
   IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009,
   N66001-15-C-4032, Facebook, Adobe, Amazon, and the John Templeton
   Foundation.
CR Dempster A. P., 1977, J ROYAL STAT SOC B, V39
   Dissmann J., 2012, ARXIV12022002
   Frechet M., 1960, TRABAJOS ESTADISTICA, V11, P3
   Genest C, 2009, INSUR MATH ECON, V44, P143, DOI 10.1016/j.insmatheco.2008.10.005
   Giordano R., 2015, NEURAL INFORM PROCES
   Gruber L., 2015, INT SOC BAYESIAN ANA
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   Hoffman M. D., 2015, ARTIFICIAL INTELLIGE
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Joe H., 1996, FAMILIES M VARIATE D, P120
   Kappen H. J., 2001, NEURAL INFORM PROCES
   Kingma D. P., 2015, INT C LEARN REPR
   Kurowicka D., 2006, UNCERTAINTY ANAL HIG
   Nelsen R. B., 2006, SPRINGER SERIES STAT
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Ranganath  R., 2014, AISTATS, P814
   Rezende D. J., 2014, INT C MACH LEARN
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Saul L. K., 1995, P ADV NEUR INF PROC, P486
   Seeger M., 2010, INT C MACH LEARN
   Sklar M, 1959, PUBL I STAT U PARIS, V8, P229
   Stan Development Team, 2015, STAN C PLUS PLUS LIB
   Toulis P., 2014, ARXIV14082923
   Tran D., 2015, ARXIV150906459
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100037
DA 2019-06-15
ER

PT S
AU Tripuraneni, N
   Gu, SX
   Ge, H
   Ghahramani, Z
AF Tripuraneni, Nilesh
   Gu, Shixiang
   Ge, Hong
   Ghahramani, Zoubin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Particle Gibbs for Infinite Hidden Markov Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametric generalization of the classical Hidden Markov Model which can automatically infer the number of hidden states in the system. However, due to the infinite-dimensional nature of the transition dynamics, performing inference in the iHMM is difficult. In this paper, we present an infinite-state Particle Gibbs (PG) algorithm to re-sample state trajectories for the iHMM. The proposed algorithm uses an efficient proposal optimized for iHMMs and leverages ancestor sampling to improve the mixing of the standard PG algorithm. Our algorithm demonstrates significant convergence improvements on synthetic and real world data sets.
C1 [Tripuraneni, Nilesh; Ge, Hong; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England.
   [Gu, Shixiang] Univ Cambridge, MPI Intelligent Syst, Cambridge, England.
RP Tripuraneni, N (reprint author), Univ Cambridge, Cambridge, England.
EM nt357@cam.ac.uk; sg717@cam.ac.uk; hg344@cam.ac.uk; zoubin@eng.cam.ac.uk
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Beal M., 2001, ADV NEURAL INFORM PR, P577
   Bishop C. M., 2006, PATTERN RECOGNITION, V4
   Fox E. B., 2008, P 25 INT C MACH LEAR, P312, DOI DOI 10.1145/1390156.1390196
   Lindsten F, 2014, J MACH LEARN RES, V15, P2145
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Palla K., 2014, ARXIV14034206
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rosenstein JK, 2013, NANO LETT, V13, P2682, DOI 10.1021/nl400822r
   Scott S. L., 2002, J AM STAT ASS, V97
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Van Gael J., 2008, P INT C MACH LEARN, V25
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103006
DA 2019-06-15
ER

PT S
AU Vacher, J
   Meso, AI
   Perrinet, L
   Peyre, G
AF Vacher, Jonathan
   Meso, Andrew Isaac
   Perrinet, Laurent
   Peyre, Gabriel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Biologically Inspired Dynamic Textures for Probing Motion Perception
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PRIOR EXPECTATIONS
AB Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.
C1 [Vacher, Jonathan] Univ Paris 09, UNIC, CNRS, F-75775 Paris 16, France.
   [Vacher, Jonathan; Peyre, Gabriel] Univ Paris 09, CEREMADE, F-75775 Paris 16, France.
   [Meso, Andrew Isaac; Perrinet, Laurent] Aix Marseille Univ, CNRS, Inst Neurosci Timone, UMR 7289, F-13385 Marseille 05, France.
   [Peyre, Gabriel] Univ Paris 09, CNRS, F-75775 Paris 16, France.
RP Vacher, J (reprint author), Univ Paris 09, UNIC, CNRS, F-75775 Paris 16, France.
EM vacher@ceremade.dauphine.fr; andrew.meso@univ-amu.fr;
   laurent.perrinet@univ-amu.fr; peyre@ceremade.dauphine.fr
RI Perrinet, Laurent U./C-4900-2009
OI Perrinet, Laurent U./0000-0002-9536-010X
FU EC [FP7-269921]; European Research Council (ERC project SIGMA-Vision);
   SPEED [ANR-13-SHS2-0006]
FX We thank Guillaume Masson for useful discussions during the development
   of the experiments. We also thank Manon Bouye and Elise Amfreville for
   proofreading. LUP was supported by EC FP7-269921, "BrainScaleS". The
   work of JV and GP was supported by the European Research Council (ERC
   project SIGMA-Vision). AIM and LUP were supported by SPEED
   ANR-13-SHS2-0006.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Dong DW, 2010, DYNAMICS OF VISUAL MOTION PROCESSING: NEURONAL, BEHAVIORAL, AND COMPUTATIONAL APPROACHES, P261, DOI 10.1007/978-1-4419-0781-3_12
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Galerne B., 2011, IMAGE PROCESSING LIN, V1
   Galerne B., 2011, THESIS
   GREGORY RL, 1980, PHILOS T ROY SOC B, V290, P181, DOI 10.1098/rstb.1980.0090
   Jogan M, 2015, J NEUROSCI, V35, P9381, DOI 10.1523/JNEUROSCI.4801-14.2015
   Leon PS, 2012, J NEUROPHYSIOL, V107, P3217, DOI 10.1152/jn.00737.2011
   Nestares O, 2000, PROC CVPR IEEE, P523, DOI 10.1109/CVPR.2000.855864
   Simoncini C, 2012, NAT NEUROSCI, V15, P1596, DOI 10.1038/nn.3229
   Sotiropoulos G, 2014, VISION RES, V97, P16, DOI 10.1016/j.visres.2014.01.012
   Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669
   Unser M, 2014, INTRO SPARSE STOCHAS
   Unser M, 2014, IEEE T INFORM THEORY, V60, P3036, DOI 10.1109/TIT.2014.2311903
   WEI L.-Y., 2009, EUROGRAPHICS 2009 ST
   Wei X.-X., 2012, ADV NEURAL INFORM PR, V25, P1313
   Weiss Y, 2002, NAT NEUROSCI, V5, P598, DOI 10.1038/nn858
   Weiss Y., 2001, PROBABILISTIC MODELS, P81
   Xia GS, 2014, SIAM J IMAGING SCI, V7, P476, DOI 10.1137/130918010
   Young RA, 2001, SPATIAL VISION, V14, P321, DOI 10.1163/156856801753253591
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101029
DA 2019-06-15
ER

PT S
AU Vainsencher, D
   Liu, H
   Zhang, T
AF Vainsencher, Daniel
   Liu, Han
   Zhang, Tong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Local Smoothness in Variance Reduced Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems, our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.
C1 [Vainsencher, Daniel; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
   [Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.
RP Vainsencher, D (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
EM daniel.vainsencher@princeton.edu; han.liu@princeton.edu;
   tzhang@stat.rutgers.edu
CR Csiba Dominik, 2015, ARXIV150208053
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Johnson R., 2013, MATH PROGRAM, P1
   Lin Q., 2014, ARXIV14071296
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Xiao Lin, 2014, SIAM J OPTIMIZ, V24, P2057
   Zhang Yuchen, 2014, ARXIV14093257
   Zhao P., 2015, P 32 INT C MACH LEAR
   Zhao P., 2014, ARXIV14012753
NR 10
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102062
DA 2019-06-15
ER

PT S
AU Valera, I
   Ruiz, FJR
   Svensson, L
   Perez-Cruz, F
AF Valera, Isabel
   Ruiz, Francisco J. R.
   Svensson, Lennart
   Perez-Cruz, Fernando
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Infinite Factorial Dynamical Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID TRACKING
AB We propose the infinite factorial dynamic model (iFDM), a general Bayesian non-parametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.
C1 [Valera, Isabel] Max Planck Inst Software Syst, Saarbrucken, Germany.
   [Ruiz, Francisco J. R.] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Svensson, Lennart] Chalmers Univ Technol, Dept Signals & Syst, Gothenburg, Sweden.
   [Perez-Cruz, Fernando] Univ Carlos III Madrid, Madrid, Spain.
   [Perez-Cruz, Fernando] Alcatel Lucent, Bell Labs, Boulogne, France.
RP Valera, I (reprint author), Max Planck Inst Software Syst, Saarbrucken, Germany.
EM ivalera@mpi-sws.org; f.ruiz@columbia.edu; lennart.svensson@chalmers.se;
   fernandop@ieee.org
FU Plan Regional-Programas I+D of Comunidad de Madrid [AGES-CM
   S2010/BMD-2422]; FPU fellowship from the Spanish Ministry of Education
   [AP2010-5333]; Ministerio de Economia of Spain (project COMPREHENSION)
   [TEC2012-38883-C02-01]; Ministerio de Economia of Spain (project ALCIT)
   [TEC2012-38800-C03-01]; Comunidad de Madrid (project CASI-CAM-CM)
   [S2013/ICE-2845]; Office of Naval Research [ONR N00014-11-1-0651];
   European Union 7th Framework Programme through the Marie Curie Initial
   Training Network 'Machine Learning for Personalized Medicine' (MLPM2012)
   [316861]; Humboldt research fellowship for postdoctoral researchers
   program
FX I. Valera is currently supported by the Humboldt research fellowship for
   postdoctoral researchers program and acknowledges the support of Plan
   Regional-Programas I+D of Comunidad de Madrid (AGES-CM S2010/BMD-2422).
   F. J. R. Ruiz is supported by an FPU fellowship from the Spanish
   Ministry of Education (AP2010-5333). This work is also partially
   supported by Ministerio de Economia of Spain (projects COMPREHENSION,
   id. TEC2012-38883-C02-01, and ALCIT, id. TEC2012-38800-C03-01), by
   Comunidad de Madrid (project CASI-CAM-CM, id. S2013/ICE-2845), by the
   Office of Naval Research (ONR N00014-11-1-0651), and by the European
   Union 7th Framework Programme through the Marie Curie Initial Training
   Network 'Machine Learning for Personalized Medicine' (MLPM2012, Grant
   No. 316861).
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Beal M, 2002, ADV NEURAL INFORM PR, V14
   FORTUNE SJ, 1995, IEEE COMPUT SCI ENG, V2, P58, DOI 10.1109/99.372944
   Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395
   Fox EB, 2010, IEEE SIGNAL PROC MAG, V27, P43, DOI 10.1109/MSP.2010.937999
   Jiang L., 2014, ARXIV14102046
   Johnson MJ, 2013, J MACH LEARN RES, V14, P673
   Jordan M. I., 2010, HIERARCHICAL MODELS
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Knowles D, 2011, ANN APPL STAT, V5, P1534, DOI 10.1214/10-AOAS435
   Kolter J.Z., 2012, J MACHINE LEARNING R, P1472
   Lim J., 2015, INT J DISTRIBUTED SE
   Lindsten F, 2014, J MACH LEARN RES, V15, P2145
   Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045
   Makonin S, 2013, 2013 IEEE ELECTRICAL POWER & ENERGY CONFERENCE (EPEC), DOI 10.1109/EPEC.2013.6802949
   Oh S, 2004, IEEE DECIS CONTR P, P735, DOI 10.1109/CDC.2004.1428740
   Orbanz P., 2010, ENCY MACHINE LEARNIN
   Sarkka S, 2007, INFORM FUSION, V8, P2, DOI 10.1016/j.inffus.2005.09.009
   Teh Y. W., 2007, P INT C ART INT STAT, V11
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Thouin F., 2011, P 14 INT C INF FUS F, P1
   Titsias MK, 2014, ADV NEUR IN, V27
   Van Gael J., 2009, ADV NEURAL INFORM PR, V21
   Vazquez MA, 2013, IEEE T VEH TECHNOL, V62, P3188, DOI 10.1109/TVT.2013.2251024
   Whiteley N., 2010, TECHNICAL REPORT
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100035
DA 2019-06-15
ER

PT S
AU van Rooyen, B
   Menon, AK
   Williamson, RC
AF van Rooyen, Brendan
   Menon, Aditya Krishna
   Williamson, Robert C.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning with Symmetric Label Noise: The Importance of Being Unhinged
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CLASSIFICATION; RISK
AB Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l(2) regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss' SLN-robustness is borne out in practice. So, with apologies to Wilde [1895], while the truth is rarely pure, it can be simple.
C1 [van Rooyen, Brendan; Menon, Aditya Krishna; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia.
   [van Rooyen, Brendan; Menon, Aditya Krishna; Williamson, Robert C.] Natl ICT Australia, Sydney, NSW, Australia.
RP van Rooyen, B (reprint author), Australian Natl Univ, Canberra, ACT, Australia.
EM brendan.vanrooyen@nicta.com.au; aditya.menon@nicta.com.au;
   bob.williamson@nicta.com.au
FU Australian Government through the Department of Communications;
   Australian Research Council through the ICT Centre of Excellence Program
FX NICTA is funded by the Australian Government through the Department of
   Communications and the Australian Research Council through the ICT
   Centre of Excellence Program. The authors thank Cheng Soon Ong for
   valuable comments on a draft of this paper.
CR Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bishop C. M., 2006, PATTERN RECOGNITION
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Denchev V., 2012, P 29 INT C MACH LEAR, P863
   Devroye L., 1996, PROBABILISTIC THEORY
   Ding N., 2010, ADV NEURAL INFORM PR, V23, P514
   Ferguson T.S., 1967, MATH STAT DECISION T
   Ghosh A, 2015, NEUROCOMPUTING, V160, P93, DOI 10.1016/j.neucom.2014.09.081
   Hastie T, 2004, J MACH LEARN RES, V5, P1391
   Kearns Michael, 1998, J ACM, V5, P392
   Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z
   Manning C. D., 2008, INTRO INFORM RETRIEV
   Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Saberian MJ, 2011, PROC CVPR IEEE
   Scholkopf Bernhard, 2002, LEARNING KERNELS, V129
   Servedio Rocco A., 1999, C COMP LEARN THEOR C
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Smola Alex, 2007, ALGORITHMIC LEARNING
   Sriperumbudur Bharath K., 2009, ADV NEURAL INFORM PR
   Stempfel G, 2009, LECT NOTES COMPUT SC, V5768, P884
   Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299
   Wilde O., 1895, IMPORTANCE BEING EAR
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102089
DA 2019-06-15
ER

PT S
AU Verma, N
   Branson, K
AF Verma, Nakul
   Branson, Kristin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sample Complexity of Learning Mahalanobis Distance Metrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Metric learning seeks a transformation of the feature space that enhances prediction quality for a given task. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower-and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. In addition, by leveraging the structure of the data distribution, we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset, allowing us to relax the dependence on representation dimension. We show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset's intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported in previous works.
C1 [Verma, Nakul; Branson, Kristin] HHMI, Janelia Res Campus, Chevy Chase, MD 20815 USA.
RP Verma, N (reprint author), HHMI, Janelia Res Campus, Chevy Chase, MD 20815 USA.
EM verman@janelia.hhmi.org; bransonk@janelia.hhmi.org
CR Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Balcan M-F., 2008, C COMP LEARN THEOR C
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bellet A., 2014, CORR
   Bellet A., 2012, INT C MACH LEARN ICM
   Bellet A., 2012, CORR
   Bian W., 2011, P 22 INT JOINT C ART, P1186
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cao Q., 2013, CORR
   Cortes C., 2010, INT C MACH LEARN ICM
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Jin  R., 2009, ADV NEURAL INFORM PR, P862
   Law M. T., 2014, COMPUTER VISION PATT
   Lim D. K. H., 2013, INT C MACH LEARN ICM
   McFee B., 2010, INT C MACH LEARN ICM
   Schultz M., 2004, NEURAL INFORM PROCES
   Shaw B., 2011, NEURAL INFORM PROCES
   Vershynin R., 2010, COMPRESSED SENSING T
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xing E.P., 2002, ADV NEURAL INFORM PR, P505
   Ying Y., 2009, C COMP LEARN THEOR C
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101071
DA 2019-06-15
ER

PT S
AU Vincent, P
   de Brebisson, A
   Bouthillier, X
AF Vincent, Pascal
   de Brebisson, Alexandre
   Bouthillier, Xavier
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Exact Gradient Update for training Deep Networks with Very
   Large Sparse Targets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D -dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d(2)) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d, i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.
C1 [de Brebisson, Alexandre; Bouthillier, Xavier] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ, Canada.
   [Vincent, Pascal] CIFAR, Toronto, ON, Canada.
RP Vincent, P (reprint author), CIFAR, Toronto, ON, Canada.
FU NSERC; Ubisoft
FX We wish to thank Yves Grandvalet for stimulating discussions, Caglar
   Gulcehre for pointing us to [14], the developers of Theano [17, 18] and
   Blocks [19] for making these libraries available to build on, and NSERC
   and Ubisoft for their financial support.
CR Bahdanau D., 2015, ARXIV E PRINTS
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y, 2001, ADV NEUR IN, V13, P932
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Chelba C., 2014, P ANN C INT SPEECH C, P2635
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dauphin Y., 2011, P 28 INT C MACH LEAR
   Gutmann Michael, 2010, P 13 INT C ART INT S
   Hill F., 2014, ABS14083456 CORR
   Jean S., 2015, ACL IJCNLP 2015
   LeCun Y., 1986, DISORDERED SYSTEMS B, P233
   LeCun Y., 1985, DISORDERED SYSTEMS B, P233
   LeCun Y., 1985, COGNITIVA 85 FRONTIE, P599
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mnih A., 2013, ADV NEURAL INFORM PR, P2265
   Morin F., 2005, P AISTATS, P246
   Ollivier Y., 2013, ABS13030818 CORR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shrivastava A, 2014, NIPS, V27, P2321
   Vijayanarasimhan Sudheendra, 2014, ARXIV14127479
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102014
DA 2019-06-15
ER

PT S
AU Vinyals, O
   Kaiser, L
   Koo, T
   Petrov, S
   Sutskever, I
   Hinton, G
AF Vinyals, Oriol
   Kaiser, Lukasz
   Koo, Terry
   Petrov, Slav
   Sutskever, Ilya
   Hinton, Geoffrey
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Grammar as a Foreign Language
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.
C1 [Vinyals, Oriol; Kaiser, Lukasz; Koo, Terry; Petrov, Slav; Sutskever, Ilya; Hinton, Geoffrey] Google, Mountain View, CA 94043 USA.
RP Vinyals, O (reprint author), Google, Mountain View, CA 94043 USA.
EM vinyals@google.com; lukaszkaiser@google.com; terrykoo@google.com;
   slav@google.com; ilyasu@google.com; geoffhinton@google.com
CR Bahdanau D., 2014, ARXIV14090473
   Chorowski J., 2014, ARXIV14121602
   Collins M, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P16
   Collins M., 2004, P 42 ANN M ASS COMP, P111
   Collobert R., 2011, INT C ART INT STAT
   Ghahramani Zoubin, 1990, THESIS
   Graves A, 2013, ARXIV13080850
   Hall David, 2014, ACL
   Henderson J., 2004, P 42 ANN M ASS COMP, P95
   Henderson James, 2003, NAACL
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hovy Eduard, 2006, NAACL
   Huang Zhongqiang, 2009, EMNLP
   Jean  S., 2014, ARXIV14122007
   Judge J, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P497
   Kalchbrenner N., 2013, P 2013 C EMP METH NA, P1700
   Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423
   Li Zhenghua, 2014, P 52 ANN M ACL ASS C, P457
   Luong T., 2014, ARXIV14108206
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   McClosky David, 2006, NAACL
   McDonald R, 2012, 1 WORKSH SYNT AN NON
   Mikolov T., 2013, ARXIV13013781
   Petrov Slav, 2010, HUMAN LANGUAGE TECHN, P19
   Petrov Slav, 2006, ACL
   Ratnaparkhi Adwait, 1997, 2 C EMP METH NAT LAN
   Socher R., 2011, ICML
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Titov Ivan, 2007, ACL
   Vinyals O, 2014, ARXIV14114555
   Zhu Muhua, 2013, ACL
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100003
DA 2019-06-15
ER

PT S
AU Vinyals, O
   Fortunato, M
   Jaitly, N
AF Vinyals, Oriol
   Fortunato, Meire
   Jaitly, Navdeep
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Pointer Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.
C1 [Vinyals, Oriol; Jaitly, Navdeep] Google Brain, Mountain View, CA 94043 USA.
   [Fortunato, Meire] Univ Calif Berkeley, Dept Math, Berkeley, CA USA.
RP Vinyals, O (reprint author), Google Brain, Mountain View, CA 94043 USA.
CR Bahdanau Dzmitry, 2014, ICLR 2015
   BELLMAN R, 1962, J ACM, V9, P61, DOI 10.1145/321105.321111
   Donahue Jeff, 2014, CVPR 2015
   Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2
   Graves A, 2013, ARXIV13080850
   Graves A, 2014, ARXIV14105401
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jarvis R. A., 1973, Information Processing Letters, V2, P18, DOI 10.1016/0020-0190(73)90020-3
   PREPARATA FP, 1977, COMMUN ACM, V20, P87, DOI 10.1145/359423.359430
   REBAY S, 1993, J COMPUT PHYS, V106, P125, DOI 10.1006/jcph.1993.1097
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Rumelhart David E, 1985, TECHNICAL REPORT
   Srivastava Nitish, 2015, ICML 2015
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Vinyals Oriol, 2014, CVPR 2015
   Vinyals  Oriol, 2014, ARXIV14127449
   Weston Jason, 2014, ICLEAR 2015
   Zaremba W., 2014, ARXIV14104615
NR 18
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102015
DA 2019-06-15
ER

PT S
AU Vondrick, C
   Pirsiavash, H
   Oliva, A
   Torralba, A
AF Vondrick, Carl
   Pirsiavash, Hamed
   Oliva, Aude
   Torralba, Antonio
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning visual biases from human imagination
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.
C1 [Vondrick, Carl; Oliva, Aude; Torralba, Antonio] MIT, Cambridge, MA 02139 USA.
   [Pirsiavash, Hamed] Univ Maryland Baltimore Cty, Baltimore, MD 21228 USA.
RP Vondrick, C (reprint author), MIT, Cambridge, MA 02139 USA.
EM vondrick@mit.edu; hpirsiav@umbc.edu; oliva@mit.edu; torralba@mit.edu
FU Google; ONR MURI [N000141010933]
FX We thank Aditya Khosla for important discussions, and Andrew Owens and
   Zoya Bylinskii for helpful comments. Funding for this research was
   partially supported by a Google PhD Fellowship to CV, and a Google
   research award and ONR MURI N000141010933 to AT.
CR Ahumada Jr A., 1996, PERCEPTUAL CLASSIFIC
   Aytar Y., 2011, TABULA RASA MODEL TR
   Beard B. L., 1998, TECHNIQUE EXTRACT RE
   Blais C, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003022
   Branson S., 2010, VISUAL RECOGNITION H
   Chua H. F., 2005, P NATL ACAD SCI US
   Dalal N., 2005, HISTOGRAMS ORIENTED
   Deng J., 2009, CVPR
   Eckstein M. P., 2002, J VISION
   Ellis Willis, 1999, SOURCE BOOK GESTALT
   Epshteyn A., 2005, ECML
   Everingham M., 2010, IJCV
   Fei-Fei L., 2006, ONE SHOT LEARNING OB
   Ferecatu M., 2009, STAT FRAMEWORK IMAGE
   Gosselin F., 2003, PSYCHOL SCI
   Greene M. R., 2014, VISUAL NOISE NATURAL
   Krizhevsky A., 2012, NIPS
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Li S., 2011, ASIAN J MATH STAT
   Liu R., 2006, SIGCHI HUMAN FACTORS
   Lovell J., 1971, J ACOUSTICAL SOC AM
   Mahendran A., 2015, CVPR
   Mangini M. C., 2004, COGNITIVE SCI
   Mezuman E., 2012, NIPS
   Murray RF, 2011, J VISION, V11, DOI 10.1167/11.5.2
   Palmer S. E., 1981, ATTENTION PERFORMANC, V9
   Parikh D., 2011, NIPS WCSSWC
   Ponce J., 2006, CATEGORY LEVEL OBJEC
   Salakhutdinov R., 2011, CVPR
   Sekuler A. B., 2004, CURRENT BIOL
   Sorokin A., 2008, CVPR WORKSH
   Torralba A., CVPR
   Vijayanarasimhan S., 2011, CVPR, P3
   Vondrick C., 2013, HOGGLES VISUALIZING
   Weinzaepfel P., 2011, CVPR
   Yang J., 2007, ICDM WORKSH
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101041
DA 2019-06-15
ER

PT S
AU Voss, J
   Belkin, M
   Rademacher, L
AF Voss, James
   Belkin, Mikhail
   Rademacher, Luis
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BLIND IDENTIFICATION; SEPARATION
AB Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product") space. The use of this indefinite "inner product" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing.
   Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.
C1 [Voss, James; Belkin, Mikhail; Rademacher, Luis] Ohio State Univ, Columbus, OH 43210 USA.
RP Voss, J (reprint author), Ohio State Univ, Columbus, OH 43210 USA.
EM vossj@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu;
   lrademac@cse.ohio-state.edu
CR Albera L, 2004, LINEAR ALGEBRA APPL, V391, P3, DOI 10.1016/j.laa.2004.05.007
   Arora S., 2012, ADV NEURAL INFORM PR, P2384
   Cardoso J.-F., 2005, MATLAB JADE REAL VAL
   CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054
   CARDOSO JF, 1991, INT CONF ACOUST SPEE, P3109, DOI 10.1109/ICASSP.1991.150113
   Chevalier P, 1999, SIGNAL PROCESS, V73, P27, DOI 10.1016/S0165-1684(98)00183-2
   Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1
   De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943
   DeLathauwer L, 1996, 8TH IEEE SIGNAL PROCESSING WORKSHOP ON STATISTICAL SIGNAL AND ARRAY PROCESSING, PROCEEDINGS, P356, DOI 10.1109/SSAP.1996.534890
   Gavert H., 2005, MATLAB FASTICA V 2 5
   Goyal N., 2014, S THEOR COMP STOC 20, P584, DOI DOI 10.1145/2591796.2591875
   Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Joho M., 2000, Second International Workshop on Independent Component Analysis and Blind Signal Separation. Proceedings, P81
   Koldovsky Z., 2007, TECHNICAL REPORT
   Koldovsky Z, 2007, LECT NOTES COMPUT SC, V4666, P730
   Koldovsky Z, 2006, INT CONF ACOUST SPEE, P873
   Makino S, 2007, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4020-6479-1
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   VIGARIO R, 2000, SYSTEMS MAN CYBERN A, V47, P589
   Voss J. R., 2013, P 26 INT C NEUR INF, V26, P2544
   Yeredor A, 2002, IEEE T SIGNAL PROCES, V50, P1545, DOI 10.1109/TSP.2002.1011195
   Yeredor A, 2000, SIGNAL PROCESS, V80, P897, DOI 10.1016/S0165-1684(00)00062-1
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103028
DA 2019-06-15
ER

PT S
AU Wan, YL
   Meila, M
AF Wan, Yali
   Meila, Marina
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A class of network models recoverable by spectral clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID GRAPHS
AB Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with "communities" for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectral clustering using the Normalized Laplacian of the graph can recover the communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.
C1 [Wan, Yali; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
RP Wan, YL (reprint author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.
EM yaliwan@washington.edu; mmp@stat.washington.edu
CR Arora S., 2012, P 13 ACM C EL COMM, P37, DOI DOI 10.1145/2229012.2229020
   Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954
   Balcan Maria-Florina, 2012, ARXIV12014899V2
   Bollobas B, 2001, RANDOM GRAPHS
   Chaudhuri K., 2012, J MACHINE LEARNING R, P1
   Chen Y., 2014, ARXIV14021267
   Coja-Oghlan A, 2009, SIAM J DISCRETE MATH, V23, P1682, DOI 10.1137/070699354
   Jackson MO, 2008, SOCIAL AND ECONOMIC NETWORKS, P1
   Le C. M., 2015, CONCENTRATION REGULA
   MCKAY BD, 1991, COMBINATORICA, V11, P369, DOI 10.1007/BF01275671
   MCKAY BD, 1990, J ALGORITHM, V11, P52, DOI 10.1016/0196-6774(90)90029-E
   MCKAY BD, 1985, ARS COMBINATORIA, V19A, P15
   Meila M, 2001, ADV NEUR IN, V13, P873
   Meila Marina, 2001, ARTIFICIAL INTELLIGE
   Newman M. E. J., 2014, EQUITABLE RANDOM GRA
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Norris J.R., 1997, MARKOV CHAINS
   ROHE K., 2013, ADV NEURAL INFORM PR, V26, P3120
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Stewart Gilbert W, 1990, MATRIX PERTURBATION, V175
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100076
DA 2019-06-15
ER

PT S
AU Wang, H
   Xing, W
   Asif, K
   Ziebart, BD
AF Wang, Hong
   Xing, Wei
   Asif, Kaiser
   Ziebart, Brian D.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Adversarial Prediction Games for Multivariate Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.
C1 [Wang, Hong; Xing, Wei; Asif, Kaiser; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
RP Wang, H (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM hwang27@uic.edu; wxing3@uic.edu; kasif2@uic.edu; bziebart@uic.edu
FU National Science Foundation [1526379]; Robust Optimization of Loss
   Functions with Application to Active Learning
FX This material is based upon work supported by the National Science
   Foundation under Grant No. #1526379, Robust Optimization of Loss
   Functions with Application to Active Learning.
CR Asif Kaiser, 2015, P C UNC ART INT
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cao Zhe, 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513
   Cortes C, 2004, ADV NEUR IN, V16, P313
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dembczynski K. J., 2011, ADV NEURAL INFORM PR, P1404
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933
   Gilpin Andrew, 2008, AAAI C ART INT, P75
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   Hazan Tamir, 2010, ADV NEURAL INFORM PR, P1594
   HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011
   Jansche M., 2005, P C HUM LANG TECHN E, P692
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Jun Xu, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P391
   Karp R.M., 1972, REDUCIBILITY COMBINA
   Lewis A. S., 2008, NONSMOOTH OPTIMIZATI
   Lichman M., 2013, UCI MACHINE LEARNING
   Lipton R. J., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P734, DOI 10.1145/195058.195447
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   McMahan H. B., 2003, P 20 INT C MACH LEAR, P536
   Musicant D. R., 2003, P 16 INT FLOR ART IN, P356
   Parambath S. Puthiya, 2014, ADV NEURAL INFORM PR, V27, P2123
   Qin T., 2013, ARXIV13062597
   Ranjbar M, 2010, LECT NOTES COMPUT SC, V6312, P580, DOI 10.1007/978-3-642-15552-9_42
   Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464
   TOPSOE F, 1979, KYBERNETIKA, V15, P8
   Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341
   VAPNIK V, 1992, ADV NEUR IN, V4, P831
   von Neumann J., 1947, THEORY GAMES EC BEHA
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100054
DA 2019-06-15
ER

PT S
AU Wang, J
   Ye, JP
AF Wang, Jie
   Ye, Jieping
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Multi-Layer Feature Reduction for Tree Structured Group Lasso via
   Hierarchical Projection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SELECTION; RULES
AB Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features. It has been applied successfully in many real-world applications. However, with extremely large feature dimensions, solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper, we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover, we can integrate MLFre-that has a low computational cost-with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.
C1 [Wang, Jie; Ye, Jieping] Univ Michigan, Computat Med & Bioinformat, Ann Arbor, MI 48109 USA.
   [Ye, Jieping] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
RP Wang, J (reprint author), Univ Michigan, Computat Med & Bioinformat, Ann Arbor, MI 48109 USA.
EM jwangumi@umich.edu; jpye@umich.edu
FU NIH [R01 LM010730, U54 EB020403]; NSF [IIS-0953662, III-1539991,
   III-1539722]
FX This work is supported in part by research grants from NIH (R01
   LM010730, U54 EB020403) and NSF (IIS-0953662, III-1539991, III-1539722).
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Bazaraa M. S., 2006, NONLINEAR PROGRAMMIN
   Borwein J., 2006, CONVEX ANAL NONLINEA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai M, 2014, INT CONF ACOUST SPEE
   Castro J, 2015, IEEE MTT S INT MICR
   Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514
   Deng W, 2011, TECHNICAL REPORT
   El Ghaoui L, 2012, PAC J OPTIM, V8, P667
   Hiriart-Urruty J. - B., 1988, NONSMOOTH OPTIMIZATI
   Jenatton R, 2012, SIAM J IMAGING SCI, V5, P835, DOI 10.1137/110832380
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Jia K., 2012, EUR C COMP VIS
   Kim S., 2012, ANN APPL STAT
   Kim S, 2010, INT C MACH LEARN
   Liu J., 2009, SLEP SPARSE LEARNING
   Liu J., 2010, ADV NEURAL INFORM PR
   Liu M., 2012, MED IMAGE COMPUTING
   Nan F, 2014, INT CONF ACOUST SPEE
   Ogawa K., 2013, ICML
   Ruszczynski A, 2006, NONLINEAR OPTIMIZATI
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Wang J., 2014, ADV NEURAL INFORM PR
   Wang J, 2015, J MACH LEARN RES, V16, P1063
   Xiang Z. J., 2011, NIPS
   Yogatama D., 2015, INT C MACH LEARN
   Yogatama D., 2014, P ANN M ASS COMP LIN
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhao P., 2009, ANN STAT
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101097
DA 2019-06-15
ER

PT S
AU Wang, J
   Trapeznikov, K
   Saligrama, V
AF Wang, Joseph
   Trapeznikov, Kirill
   Saligrama, Venkatesh
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient Learning by Directed Acyclic Graph For Resource Constrained
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.
C1 [Wang, Joseph; Saligrama, Venkatesh] Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA.
   [Trapeznikov, Kirill] Syst & Technol Res, Woburn, MA 01801 USA.
RP Wang, J (reprint author), Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA.
EM joewang@bu.edu; kirill.trapeznikov@stresearch.com; srv@bu.edu
FU U.S. National Science Foundation [1330008]; Department of Homeland
   Security, Science and Technology Directorate, Office of University
   Programs [2013-ST-061-ED0001]; ONR [50202168]; US AF [FA8650-14-C-1728]
FX This material is based upon work supported in part by the U.S. National
   Science Foundation Grant 1330008, by the Department of Homeland
   Security, Science and Technology Directorate, Office of University
   Programs, under Grant Award 2013-ST-061-ED0001, by ONR Grant 50202168
   and US AF contract FA8650-14-C-1728. The views and conclusions contained
   in this document are those of the authors and should not be interpreted
   as necessarily representing the social policies, either expressed or
   implied, of the U.S. DHS, ONR or AF.
CR Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Beygelzimer A., 2007, MULTICLASS CLASSIFIC
   Busa-Fekete Robert, 2012, P 29 INT C MACH LEAR
   Chen M., 2012, INT C ART INT STAT
   Dulac-Arnold G, 2011, LECT NOTES ARTIF INT, V6911, P375, DOI 10.1007/978-3-642-23780-5_34
   Gao T., 2011, ADV NEURAL INFORM PR, V24, P1062
   He H., 2012, NIPS, P3158
   Ji S., 2007, PATTERN RECOGNITION, V40
   Kanani  Pallika, 2008, ADV NEURAL INFORM PR
   Karayev S., 2013, INT C MACH LEARN WOR
   Kusner M., 2014, 28 AAAI C ART INT
   Leskovec J., 2007, INT C KNOWL DISC DAT
   Maaten L., 2013, P 30 INT C MACH LEAR
   Nan F., 2015, P 32 INT C MACH LEAR
   Nan F, 2014, INT CONF ACOUST SPEE
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Sheng V. S., 2006, P 23 INT C MACH LEAR, P809
   Steinwart I, 2005, IEEE T INFORM THEORY, V51, P128, DOI 10.1109/TIT.2004.839514
   Trapeznikov K., 2013, P 16 INT C ART INT S, P581
   Wang J., 2014, P INT C ART INT STAT, P987
   Wang J., 2013, AS C MACH LEARN, P451
   Wang J, 2014, LECT NOTES COMPUT SC, V8690, P647, DOI 10.1007/978-3-319-10605-2_42
   Wang Joseph, 2012, P ADV NEUR INF PROC, P91
   Xu Z., 2013, P 30 INT C MACH LEAR, P133
   Xu Z., 2012, P 29 INT C MACH LEAR
   Zhang C, 2010, C ELECT INSUL DIEL P
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103020
DA 2019-06-15
ER

PT S
AU Wang, SI
   Chaganty, AT
   Liang, P
AF Wang, Sida I.
   Chaganty, Arun Tejasvi
   Liang, Percy
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Estimating Mixture Models via Mixtures of Polynomials
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. Simulations show good empirical performance on several models.
C1 [Wang, Sida I.; Chaganty, Arun Tejasvi; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Wang, SI (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM sidaw@cs.stanford.edu; chaganty@cs.stanford.edu; pliang@cs.stanford.edu
FU Microsoft Faculty Research Fellowship; NSERC PGS-D fellowship
FX This work was supported by a Microsoft Faculty Research Fellowship to
   the third author and a NSERC PGS-D fellowship for the first author.
CR Anandkumar A., 2014, ARXIV14080553
   Anandkumar A., 2013, C LEARN THEOR, P867
   Anandkumar A., 2013, TENSOR DECOMPOSITION
   Anandkumar A., 2012, C LEARN THEOR COLT
   Anandkumar Animashree, 2012, ADV NEURAL INFORM PR
   Balle B, 2014, MACH LEARN, V96, P33, DOI 10.1007/s10994-013-5416-x
   Chaganty Arun, 2013, INT C MACH LEARN ICM
   Corless RM, 2009, J SYMB COMPUT, V44, P1536, DOI 10.1016/j.jsc.2008.11.009
   Curto R. E., 1996, SOLUTION TRUNCATED C, V568
   Ge R., 2015, ARXIV150300424
   Hardt M., 2014, ARXIV14044997
   Henrion D, 2005, LECT NOTES CONTR INF, V312, P293
   Hsu D., 2012, ADV NEURAL INFORM PR
   Hsu D., 2013, INNOVATIONS THEORETI
   Kalai AT, 2010, ACM S THEORY COMPUT, P553
   Lasserre J. B., 2011, MOMENTS POSITIVE POL
   Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802
   Lasserre JB, 2008, MATH PROGRAM, V112, P65, DOI 10.1007/s10107-006-0085-1
   Laurent M, 2009, IMA VOL MATH APPL, V149, P157
   McLachlan G., 2004, FINITE MIXTURE MODEL
   MOLLER HM, 1995, NUMER MATH, V70, P311, DOI 10.1007/s002110050122
   Ozay N, 2010, PROC CVPR IEEE, P3209, DOI 10.1109/CVPR.2010.5540075
   Parrilo P. A., 2003, Algorithmic and Quantitative Real Algebraic Geometry. DIMACS Workshop. Algorithmic and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer Science (Discrete Mathematics & Theoretical Comput. Sci. Vol.60), P83
   Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5
   Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003
   Stetter H. J., 1993, WORLD SCI SERIES APP, V2, P355
   Sturmfels B., 2008, ALGORITHMS INVARIANT
   Sturmfels B., 2002, SOLVING SYSTEMS POLY
   Titterington D. M., 1985, STAT ANAL FINITE MIX, V7
   Viele K, 2002, STAT COMPUT, V12, P315, DOI 10.1023/A:1020779827503
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100070
DA 2019-06-15
ER

PT S
AU Wang, XY
   Leng, CL
   Dunson, DB
AF Wang, Xiangyu
   Leng, Chenlei
   Dunson, David B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On the consistency theory of high dimensional variable screening
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SELECTION; REGRESSION; LASSO
AB Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS and HOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((rho s+sigma/tau)(2) log p) under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of SIS.
C1 [Wang, Xiangyu; Dunson, David B.] Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.
   [Leng, Chenlei] Univ Warwick, Dept Stat, Coventry, W Midlands, England.
RP Wang, XY (reprint author), Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.
EM xw56@stat.duke.edu; C.Leng@warwick.ac.uk; dunson@stat.duke.edu
FU National Institute of Environmental Health Sciences [NIH R01-ES017436]
FX This research was partly support by grant NIH R01-ES017436 from the
   National Institute of Environmental Health Sciences.
CR Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.90.9718
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Cho HR, 2012, J R STAT SOC B, V74, P593, DOI 10.1111/j.1467-9868.2011.01023.x
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan JQ, 2008, J ROY STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Jia Jinzhu, 2012, ARXIV12085584
   Lee Jason D, 2013, ADV NEURAL PROCESSIN
   Li GR, 2012, ANN STAT, V40, P1846, DOI 10.1214/12-AOS1024
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vershynin  Roman, 2010, ARXIV10113027
   Wainwright M. J., 2009, IEEE T INFORM THEORY
   Wang HS, 2009, J AM STAT ASSOC, V104, P1512, DOI 10.1198/jasa.2008.tm08516
   Wang Xiangyu, 2015, HIGH DIMENSIONAL ORD
   Xue LZ, 2011, BIOMETRIKA, V98, P371, DOI 10.1093/biomet/asr010
   Zhang CH, 2008, ANN STAT, V36, P1567, DOI 10.1214/07-AOS520
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhou Shuheng, 2009, ARXIV09124045
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102081
DA 2019-06-15
ER

PT S
AU Wang, XY
   Guo, FJ
   Heller, KA
   Dunson, DB
AF Wang, Xiangyu
   Guo, Fangjian
   Heller, Katherine A.
   Dunson, David B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Parallelizing MCMC with Random Partition Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to re-sample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.
C1 [Wang, Xiangyu; Heller, Katherine A.; Dunson, David B.] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.
   [Guo, Fangjian] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.
RP Wang, XY (reprint author), Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.
EM xw56@stat.duke.edu; guo@cs.duke.edu; kheller@stat.duke.edu;
   dunson@stat.duke.edu
CR BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Blackard Jock A, 1998, P 2 SO FOR GIS C, P189
   Blum M., 1973, Journal of Computer and System Sciences, V7, P448, DOI 10.1016/S0022-0000(73)80033-9
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737
   Haario H, 2006, STAT COMPUT, V16, P339, DOI 10.1007/s11222-006-9438-0
   HJORT NL, 1995, ANN STAT, V23, P882, DOI 10.1214/aos/1176324627
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu Linxi, 2014, ARXIV14012597
   Maclaurin Dougal, 2014, P C UNC ART INT UAI
   Minsker Stanislav, 2014, P 31 INT C MACH LEAR
   Neiswanger W, 2014, P 30 C UNC ART INT, P623
   Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018
   Scott Steven L, 2013, EFABBAYES 250 C, V16
   SHEN XT, 1994, ANN STAT, V22, P580, DOI 10.1214/aos/1176325486
   Srivastava Sanvesh, 2015, P 18 INT C ART INT S, V38
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wang X., 2013, ARXIV13124605
   Welling M., 2011, P 28 INT C MACH LEAR
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103024
DA 2019-06-15
ER

PT S
AU Wang, Y
   Dunson, D
AF Wang, Ye
   Dunson, David
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic
   Gaussian Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHM
AB Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning. One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold. There are a rich variety of manifold learning methods available, which allow mapping of data points to the manifold. However, there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data. The best attempt is the Gaussian process latent variable model (GP-LVM), but identifiability issues lead to poor performance. We solve these issues by proposing a novel Coulomb repulsive process (Corp) for locations of points on the manifold, inspired by physical models of electrostatic interactions among particles. Combining this process with a GP prior for the mapping function yields a novel electrostatic GP (electroGP) process. Focusing on the simple case of a one-dimensional manifold, we develop efficient inference algorithms, and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video.
C1 [Wang, Ye; Dunson, David] Duke Univ, Dept Stat, Durham, NC 27705 USA.
RP Wang, Y (reprint author), Duke Univ, Dept Stat, Durham, NC 27705 USA.
EM eric.ye.wang@duke.edu; dunson@stat.duke.edu
CR Belkin M, 2002, ADV NEUR IN, V14, P585
   Buades A, 2005, PROC CVPR IEEE, P60
   Chen MH, 2010, IEEE T SIGNAL PROCES, V58, P6140, DOI 10.1109/TSP.2010.2070796
   HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
   Hough J. B, 2009, ZEROS GAUSSIAN ANAL, V51
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   Lawrence N. D., 2006, P 23 INT C MACH LEAR, P513
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rao V., 2013, ARXIV13081136
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   Urtasun R., 2008, P 25 INT C MACH LEAR, V307, P1080
   Wang Y., 2014, ARXIV14107692
   Weinberger K. Q., 2006, AAAI, P1683
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101054
DA 2019-06-15
ER

PT S
AU Wang, YN
   Tung, HY
   Smola, A
   Anandkumar, A
AF Wang, Yining
   Tung, Hsiao-Yu
   Smola, Alex
   Anandkumar, Anima
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast and Guaranteed Tensor Decomposition via Sketching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
DE Tensor CP decomposition; count sketch; randomized methods; spectral
   methods; topic modeling
AB Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.
C1 [Wang, Yining; Tung, Hsiao-Yu; Smola, Alex] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Anandkumar, Anima] Univ Calif Irvine, Dept EECS, Irvine, CA 92697 USA.
RP Wang, YN (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM yiningwa@cs.cmu.edu; htung@cs.cmu.edu; alex@smola.org;
   a.anandkumar@uci.edu
FU Microsoft Faculty Fellowship; Sloan Foundation; Google Faculty Research
   Grant
FX Anima Anandkumar is supported in part by the Microsoft Faculty
   Fellowship and the Sloan Foundation. Alex Smola is supported in part by
   a Google Faculty Research Grant.
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Bhojanapalli S., 2015, ARXIV150205023
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Carlson  Andrew, 2010, AAAI
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chaganty A., 2014, ICML
   Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6
   Choi J. H., 2014, NIPS
   Dwork Cynthia, 2015, STOC
   FIELD A S, 1991, Brain Topography, V3, P407, DOI 10.1007/BF01129000
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Harshman R. A., 1970, UCLA WORKING PAPERS, V16, P1
   Huang F., 2014, NIPS OPT WORKSH
   Huang Furong, 2013, ARXIV13090787
   Jain A., 1989, FUNDAMENTALS DIGITAL
   Kang U., 2012, KDD
   Klimt B., 2004, CEAS
   Kolda T. G., 2008, ICDM
   Kolda T. G., 2006, WORKSH LINK AN COUNT
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Morup M, 2006, NEUROIMAGE, V29, P938, DOI 10.1016/j.neuroimage.2005.08.005
   Pagh R., 2012, ITCS
   Patrascu M, 2012, J ACM, V59, DOI 10.1145/2220357.2220361
   Pham N., 2013, KDD
   Phan AH, 2013, IEEE T SIGNAL PROCES, V61, P4834, DOI 10.1109/TSP.2013.2269903
   Phan X. H., 2007, GIBBSLDA C C IMPLEME
   Tsourakakis C. E., 2010, SDM
   Tung H.-Y., 2014, NIPS
   Wang C., 2014, ECML PKDD
   Wang  Y., 2014, NIPS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102092
DA 2019-06-15
ER

PT S
AU Wang, YN
   Wang, YX
   Singh, A
AF Wang, Yining
   Wang, Yu-Xiang
   Singh, Aarti
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Differentially Private Subspace Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ROBUST
AB Subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple "clusters" so that data points in a single cluster lie approximately on a low-dimensional linear subspace. It is originally motivated by 3D motion segmentation in computer vision, but has recently been generically applied to a wide range of statistical machine learning problems, which often involves sensitive datasets about human subjects. This raises a dire concern for data privacy. In this work, we build on the framework of differential privacy and present two provably private subspace clustering algorithms. We demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice. Along the course of the proof, we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.
C1 [Wang, Yining; Wang, Yu-Xiang; Singh, Aarti] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
RP Wang, YN (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM yiningwa@cs.cmu.edu; yuxiangw@cs.cmu.edu; aarti@cs.cmu.edu
FU NSF CAREER [IIS-1252412]; NSF [BCS-0941518]; Singapore National Research
   Foundation under its International Research Centre@ Singapore Funding
   Initiative
FX This research is supported in part by grant NSF CAREER IIS-1252412, NSF
   Award BCS-0941518, and a grant by Singapore National Research Foundation
   under its International Research Centre@ Singapore Funding Initiative
   administered by the IDM Programme Office.
CR Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Blum A., 2015, PODS
   Bradley P. S., 2000, J GLOBAL OPTIMIZATIO, V16
   Chaudhuri Kamalika, 2012, NIPS
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21
   Dwork C., 2006, TCC
   Dwork C., 2006, EUROCRYPT
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork Cynthia, 2014, STOC
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Feldman Dan, 2013, SODA
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Heckel R., 2013, ARXIV13074891
   Ho J., 2003, CVPR
   Hoff PD, 2009, J COMPUT GRAPH STAT, V18, P438, DOI 10.1198/jcgs.2009.07177
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   McSherry F, 2007, FOCS
   McWilliams B, 2014, DATA MIN KNOWL DISC, V28, P736, DOI 10.1007/s10618-013-0317-y
   Mir D. J., 2013, THESIS
   Nasihatkon B., 2011, CVPR
   Nissim K, 2007, STOC
   Ostrovksy R., 2006, FOCS
   Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199
   Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034
   Su D., 2015, DIFFERENTIALLY PRIVA
   Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728
   Wang Y., 2015, ICML
   Wang Y., 2015, CLUSTERING CONSISTEN
   [王应德 Wang Yingde], 2013, [高分子通报, Polymer Bulletin], P89
   Zhang  A., 2012, GUESS WHO RATED THIS
   Zhang Z., 2004, AAAI
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103029
DA 2019-06-15
ER

PT S
AU Wang, ZR
   Gu, QQ
   Ning, Y
   Liu, H
AF Wang, Zhaoran
   Gu, Quanquan
   Ning, Yang
   Liu, Han
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI High Dimensional EM Algorithm: Statistical Optimization and Asymptotic
   Normality
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CONFIDENCE-INTERVALS; MIXTURE; LASSO
AB We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-) optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.
C1 [Wang, Zhaoran; Ning, Yang; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA.
   [Gu, Quanquan] Univ Virginia, Charlottesville, VA 22903 USA.
RP Wang, ZR (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
FU NSF [IIS1116730, IIS1332109, IIS1408910, IIS1546482-BIGDATA,
   DMS1454377-CAREER]; NIH [R01GM083084, R01HG06841, R01MH102339]; FDA
   [HHSF223201000072C]
FX Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910,
   NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH
   R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   [Anonymous], 2014, ARXIV14013889
   Balakrishnan S., 2014, ARXIV14082156
   Bartholomew D, 2011, WILEY SER PROBAB ST, P1, DOI 10.1002/9781119970583
   Belloni A, 2012, ECONOMETRICA, V80, P2369, DOI 10.3982/ECTA9626
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Boucheron S., 2013, CONCENTRATION INEQUA
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   CHAGANTY A. T, 2013, ARXIV13063729
   CHAUDHURI K, 2009, ARXIV09120086
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Javanmard A, 2014, J MACH LEARN RES, V15, P2869
   Khalili A, 2007, J AM STAT ASSOC, V102, P1025, DOI 10.1198/016214507000000590
   Knight K, 2000, ANN STAT, V28, P1356
   Lee J, 2013, ARXIV13116238
   Lockhart R, 2014, ANN STAT, V42, P413, DOI 10.1214/13-AOS1175
   McLachlan G., 2007, EM ALGORITHM EXTENSI, V382
   Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x
   Meinshausen N, 2009, J AM STAT ASSOC, V104, P1671, DOI 10.1198/jasa.2009.tm08647
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Nickl R, 2013, ANN STAT, V41, P2852, DOI 10.1214/13-AOS1170
   Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z
   Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073
   Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221
   Van der Vaart A.W., 2000, ASYMPTOTIC STAT, V3
   Vershynin  Roman, 2010, ARXIV10113027
   Wasserman L, 2009, ANN STAT, V37, P2178, DOI 10.1214/08-AOS646
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   YI X., 2013, ARXIV13103745
   Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102063
DA 2019-06-15
ER

PT S
AU Wei, K
   Iyer, R
   Wang, SJ
   Bai, WR
   Bilmes, J
AF Wei, Kai
   Iyer, Rishabh
   Wang, Shengjie
   Bai, Wenruo
   Bilmes, Jeff
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Mixed Robust/Average Submodular Partitioning: Fast Algorithms,
   Guarantees, and Applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call Submodular Partitioning. These problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (SFA) [12] and min-max submodular load balancing (SLB) [25], and also average-case instances, that is the submodular welfare problem (SWP) [26] and submodular multiway partition (SMP) [5]. While the robust versions have been studied in the theory community [11, 12, 16, 25, 26], existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.
C1 [Wei, Kai; Iyer, Rishabh; Bai, Wenruo; Bilmes, Jeff] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Wang, Shengjie] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA.
RP Wei, K (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM kaiwei@u.washington.edu; rkiyer@u.washington.edu;
   wangsj@u.washington.edu; wrbai@u.washington.edu; bilmes@u.washington.edu
FU National Science Foundation [IIS-1162606]; National Institutes of Health
   [R01GM103544]; Google; Microsoft; Intel; Microsoft Research Ph.D
   Fellowship; TerraSwarm, one of six centers of STARnet, a Semiconductor
   Research Corporation program - MARCO; TerraSwarm, one of six centers of
   STARnet, a Semiconductor Research Corporation program - DARPA
FX This material is based upon work supported by the National Science
   Foundation under Grant No. IIS-1162606, the National Institutes of
   Health under award R01GM103544, and by a Google, a Microsoft, and an
   Intel research award. R. Iyer acknowledges support from a Microsoft
   Research Ph.D Fellowship. This work was supported in part by TerraSwarm,
   one of six centers of STARnet, a Semiconductor Research Corporation
   program sponsored by MARCO and DARPA.
CR Garcia-Escudero LA, 2010, ADV DATA ANAL CLASSI, V4, P89, DOI 10.1007/s11634-010-0064-5
   Arthur  David, 2007, SODA
   Asadpour A., 2010, SICOMP
   Boyd S., 2011, FDN TRENDS MACHINE L
   Buchbinder N., 2012, FOCS
   Chekuri C., 2011, FOCS
   Chekuri C, 2011, LECT NOTES COMPUT SC, V6755, P354, DOI 10.1007/978-3-642-22006-7_30
   Ene A., 2013, SODA
   Fisher M. L., 1978, POLYHEDRAL COMBINATO
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Goemans M. X., 2009, SODA
   Golovin D., 2005, CMUCS05144
   Iyer R., 2013, ICML
   Iyer R., MONOTONE CLOSURE REL
   Jegelka S., 2011, CVPR
   Khot S., 2007, APPROX
   Kolmogorov V., 2004, TPAMI
   Krause Andreas, 2008, JMLR
   Lenstra J, 1990, MATH PROGRAMMING
   Li M., 2015, ARXIV150504636
   Minoux M., 1978, OPTIMIZATION TECHNIQ
   Narasimhan M., 2005, NIPS
   Orlin J., 2009, MATH PROGRAMMING
   Povey Daniel, 2014, ARXIV14107455
   Svitkina Z., 2008, FOCS
   Vondrak J., 2008, STOC
   Wei K., MIXED ROBUST AVERAGE
   Wei Kai, 2015, ICML
   Zhao L, 2004, DISCRETE APPL MATH, V143, P130, DOI 10.1016/j.dam.2003.10.007
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100074
DA 2019-06-15
ER

PT S
AU Werling, K
   Chaganty, A
   Liang, P
   Manning, CD
AF Werling, Keenon
   Chaganty, Arun
   Liang, Percy
   Manning, Christopher D.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On-the-Job Learning with Bayesian Decision Theory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an on-the-job setting, where as inputs arrive, we use real-time crowd-sourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets-named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F-1 improvement over having a single human label the whole set, and a 28% F-1 improvement over online learning.
C1 [Werling, Keenon; Chaganty, Arun; Liang, Percy; Manning, Christopher D.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Werling, K (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM keenon@cs.stanford.edu; chaganty@cs.stanford.edu;
   pliang@cs.stanford.edu; manning@cs.stanford.edu
FU Sloan Fellowship
FX We are grateful to Kelvin Guu and Volodymyr Kuleshov for useful feedback
   regarding the calibration of our models and Amy Bearman for providing
   the image embeddings for the face classification experiments. We would
   also like to thank our anonymous reviewers for their helpful feedback.
   Finally, our work was sponsored by a Sloan Fellowship to the third
   author.
CR Angeli G., 2014, EMPIRICAL METHODS NA
   Bernstein M. S., 2010, P 23 ANN ACM S US IN, P313, DOI [DOI 10.1145/1866029.1866078, 10.1145/1866029.1866078]
   Bernstein M. S., 2011, P 24 ANN ACM S US IN, P33, DOI DOI 10.1145/2047196.2047201
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chai XY, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P51, DOI 10.1109/ICDM.2004.10092
   Cheng J, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P600, DOI 10.1145/2675133.2675214
   Chenliang Li, 2012, Proceedings of the 35th Annual International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR 2012), P721, DOI 10.1145/2348283.2348380
   Chu W., 2011, P 17 ACM SIGKDD INT, P195
   Coulom R., 2007, COMP GAM WORKSH
   Dai P., 2010, ASS ADV ARTIFICIAL I
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donmez P., 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165
   Esmeir S., 2007, ADV NEURAL INFORM PR, P425
   Finkel J. R., 2005, P 43 ANN M ASS COMP, P363, DOI DOI 10.3115/1219840.1219885
   Gao T., 2011, ADV NEURAL INFORM PR, V24, P1062
   Golovin D., 2010, ADV NEURAL INFORM PR, P766
   Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6
   Helmbold D., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P218, DOI 10.1145/267460.267502
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Kokalis Nicolas, 2013, P 2013 C COMP SUPP C, P1291
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kumar N., 2009, ICCV
   Lasecki Walter S, 2013, P 2013 C COMP SUPP C
   Liang P., 2009, INT C MACH LEARN ICM
   Maas A. L., 2011, P 49 ANN M ASS COMP, V1, P142
   Sculley D., 2007, C EM ANT CEAS
   Settles B., 2010, TECHNICAL REPORT
   Socher R., 2013, EMPIRICAL METHODS NA
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102009
DA 2019-06-15
ER

PT S
AU Wilson, AG
   Dann, C
   Lucas, CG
   Xing, EP
AF Wilson, Andrew Gordon
   Dann, Christoph
   Lucas, Christopher G.
   Xing, Eric P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Human Kernel
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID EXTRAPOLATION
AB Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.
C1 [Wilson, Andrew Gordon; Dann, Christoph; Xing, Eric P.] CMU, Mt Pleasant, MI 48859 USA.
   [Lucas, Christopher G.] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
RP Wilson, AG (reprint author), CMU, Mt Pleasant, MI 48859 USA.
CR Bishop C. M., 2006, PATTERN RECOGNITION
   Busemeyer J. R., 1997, CONCEPTS CATEGORIES
   Carroll J Douglas, 1963, ETS RES B SERIES, V1963
   DeLosh EL, 1997, J EXP PSYCHOL LEARN, V23, P968, DOI 10.1037/0278-7393.23.4.968
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Doya K., 2007, BAYESIAN BRAIN PROBA
   Gershman SJ, 2012, NEURAL COMPUT, V24, P1, DOI 10.1162/NECO_a_00226
   Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541
   Griffiths TL, 2006, PSYCHOL SCI, V17, P767, DOI 10.1111/j.1467-9280.2006.01780.x
   Griffiths TL, 2012, CURR DIR PSYCHOL SCI, V21, P263, DOI 10.1177/0963721412447619
   Jaakkola Tommi, 1998, ADV NEURAL INFORM PR, V11, P487
   Johnson S. G. B., 2014, P 36 ANN C COGN SCI, P701
   Kalish ML, 2007, PSYCHON B REV, V14, P288, DOI 10.3758/BF03194066
   Knill D. C., 1996, PERCEPTION BAYESIAN
   KOH KH, 1991, J EXP PSYCHOL LEARN, V17, P811, DOI 10.1037/0278-7393.17.5.811
   Little D.R., 2009, P 31 ANN C COGN SCI, P1157
   Lucas C. E., 2015, PSYCHONOMIC B REV, P1
   Lucas Chris, 2009, NEURAL INFORM PROCES
   Lucas Christopher G, 2012, SUPERSPACE EXTRAPOLA
   MacKay D. J, 2003, INFORM THEORY INFERE
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   McDaniel MA, 2005, PSYCHON B REV, V12, P24, DOI 10.3758/BF03196347
   Neal RM, 1996, BAYESIAN LEARNING NE
   Rasmussen C. E., 2001, NEURAL INFORM PROCES
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788
   Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101
   Wilson A., 2012, TECHNICAL REPORT
   Wilson A. G., 2013, INT C MACH LEARN ICM
   Wilson A. G., 2014, ADV NEURAL INFORM PR
   Wilson Andrew Gordon, 2014, THESIS
   WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101025
DA 2019-06-15
ER

PT S
AU Wu, AQ
   Park, IM
   Pillow, JW
AF Wu, Anqi
   Park, Il Memming
   Pillow, Jonathan W.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convolutional Spike-triggered Covariance Analysis for Neural Subunit
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RECOGNITION; RESPONSES; MECHANISM; CELLS
AB Subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli. They are defined by a cascade of two linear-nonlinear (LN) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an output nonlinearity. Recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. However, fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima. Here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models. Specifically, we show that a "convolutional" decomposition of a spike-triggered average (STA) and covariance (STC) matrix provides an asymptotically efficient estimator for class of quadratic subunit models. We establish theoretical conditions for identifiability of the subunit and pooling weights, and show that our estimator performs well even in cases of model mismatch. Finally, we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model (GQM), and achieves nearly the same prediction performance as the full maximum-likelihood estimator, yet at substantially lower cost.
C1 [Wu, Anqi; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Park, Il Memming] SUNY Stony Brook, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA.
RP Wu, AQ (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
EM anqiw@princeton.edu; memming.park@stonybrook.edu; pillow@princeton.edu
CR Arcas BAY, 2003, NEURAL COMPUT, V15, P1789, DOI 10.1162/08997660360675044
   BARLOW HB, 1965, J PHYSIOL-LONDON, V178, P477, DOI 10.1113/jphysiol.1965.sp007638
   Crook JD, 2008, J NEUROSCI, V28, P11277, DOI 10.1523/JNEUROSCI.2982-08.2008
   Davis P.J., 1979, CIRCULANT MATRICES
   Demb JB, 2001, J NEUROSCI, V21, P7447
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   HOCHSTEIN S, 1976, J PHYSIOL-LONDON, V262, P265, DOI 10.1113/jphysiol.1976.sp011595
   Joris PX, 2004, PHYSIOL REV, V84, P541, DOI 10.1152/physrev.00029.2003
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Park I. M., 2011, ADV NEURAL INF PROCE, V24, P1692
   Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454
   Pillow JW, 2006, J VISION, V6, P414, DOI 10.1167/6.4.9
   Rajan K, 2013, NEURAL COMPUT, V25, P1661, DOI 10.1162/NECO_a_00463
   Ramirez Alexandro D., 2013, J COMPUTATIONAL NEUR, P1
   Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021
   Sahani M., 2003, NIPS, P15
   Schwartz O, 2006, J VISION, V6, P484, DOI 10.1167/6.4.13
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Sharpee T, 2004, NEURAL COMPUT, V16, P223, DOI 10.1162/089976604322742010
   Touryan J, 2002, J NEUROSCI, V22, P10811
   VANSTEVENINCK RD, 1988, PROC R SOC SER B-BIO, V234, P379
   Vintch B, 2012, ADV NEURAL INFORM PR, V25
   Vintch Brett, 2015, J NEURSOCI
   Williamson RS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004141
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102110
DA 2019-06-15
ER

PT S
AU Wu, HS
   Srikant, R
   Liu, X
   Jiang, C
AF Wu, Huasen
   Srikant, R.
   Liu, Xin
   Jiang, Chong
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Algorithms with Logarithmic or Sublinear Regret for Constrained
   Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming ( ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.
C1 [Wu, Huasen; Liu, Xin] Univ Calif Davis, Davis, CA 95616 USA.
   [Srikant, R.; Jiang, Chong] Univ Illinois, Champaign, IL USA.
RP Wu, HS (reprint author), Univ Calif Davis, Davis, CA 95616 USA.
EM hswu@ucdavis.edu; rsrikant@illinois.edu; liu@cs.ucdavis.edu;
   jiang17@illinois.edu
FU NSF [CCF-1423542, CNS-1457060, CNS-1547461]; AFOSR MURI [FA
   9550-10-1-0573]
FX This research was supported in part by NSF Grants CCF-1423542,
   CNS-1457060, CNS-1547461, and AFOSR MURI Grant FA 9550-10-1-0573.
CR Agarwal Alekh, 2014, INT C MACH LEARN ICM
   Agrawal S., 2015, ARXIV150603374
   Agrawal S., 2015, ARXIV150706738
   Agrawal Shipra, 2014, P 15 ACM C EC COMP E, P989
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Badanidiyuru A., 2014, C LEARN THEOR COLT
   Badanidiyuru  A., 2012, ACM C EL COMM, P128
   Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30
   Combes R., 2015, ACM SIGMETRICS
   Combes R, 2014, IEEE INFOCOM SER, P2760, DOI 10.1109/INFOCOM.2014.6848225
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Golovin D., 2009, DEALING PARTIAL FEED
   Jiang C, 2013, IEEE DECIS CONTR P, P5345, DOI 10.1109/CDC.2013.6760730
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lai TL, 2012, SEQUENTIAL ANAL, V31, P441, DOI 10.1080/07474946.2012.719433
   Langford J., 2007, ADV NEURAL INFORM PR, P817
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Lu Tyler, 2010, INT C ART INT STAT, V9, P485
   Ortner P., 2007, P 2006 C ADV NEUR IN, P49
   Slivkins A., 2013, ARXIV13060155
   Slivkins A, 2014, J MACH LEARN RES, V15, P2533
   Tran- Thanh L., 2012, AAAI C ART INT
   Veatch MH, 2013, MATH OPER RES, V38, P535, DOI 10.1287/moor.1120.0574
   Xia Y., 2015, INT JOINT C ART INT
   Zhou L, 2015, ARXIV150803326
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103046
DA 2019-06-15
ER

PT S
AU Wu, JJ
   Yildirim, I
   Lim, JJ
   Freeman, WT
   Tenenbaum, JB
AF Wu, Jiajun
   Yildirim, Ilker
   Lim, Joseph J.
   Freeman, William T.
   Tenenbaum, Joshua B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Galileo: Perceiving Physical Object Properties by Integrating a Physics
   Engine with Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.
C1 [Wu, Jiajun; Lim, Joseph J.; Freeman, William T.] MIT, EECS, Cambridge, MA 02139 USA.
   [Yildirim, Ilker] Rockefeller Univ, BCS MIT, New York, NY 10021 USA.
   [Tenenbaum, Joshua B.] MIT, BCS, Cambridge, MA 02139 USA.
RP Wu, JJ (reprint author), MIT, EECS, Cambridge, MA 02139 USA.
EM jiajunwu@mit.edu; ilkery@mit.edu; lim@csail.mit.edu; billf@mit.edu;
   jbt@mit.edu
FU NSF [1212849]; Center for Brains, Minds, and Machines (NSF STC award)
   [CCF-1231216]
FX This work was supported by NSF Robust Intelligence 1212849
   Reconstructive Recognition and the Center for Brains, Minds, and
   Machines (funded by NSF STC award CCF-1231216).
CR Baillargeon R, 2004, CURR DIR PSYCHOL SCI, V13, P89, DOI 10.1111/j.0963-7214.2004.00281.x
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Carey S., 2009, ORIGIN CONCEPTS
   Coumans E, 2010, OPEN SOURCE SOFTWARE
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Jia Zhaoyin, 2014, IEEE TPAMI
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Sanborn AN, 2013, PSYCHOL REV, V120, P411, DOI 10.1037/a0031912
   Schulman J, 2013, IEEE INT CONF ROBOT, P1130, DOI 10.1109/ICRA.2013.6630714
   Tomasi  C., 1991, INT J COMPUTER VISIO
   Ullman Tomer, 2014, COGSCI
   Yildirim Ilker, 2015, 37 ANN C COGN SCI SO
   Zheng Bo, 2014, ICRA
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101040
DA 2019-06-15
ER

PT S
AU Wu, YF
   Gyorgy, A
   Szepesvari, C
AF Wu, Yifan
   Gyorgy, Andras
   Szepesvari, Csaba
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Online Learning with Gaussian Payoffs and Side Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i; j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).
C1 [Wu, Yifan; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
   [Gyorgy, Andras] Imperial Coll London, Dept Elect & Elect Engn, London, England.
RP Wu, YF (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM ywu12@ualberta.ca; a.gyorgy@imperial.ac.uk; szepesva@ualberta.ca
FU Alberta Innovates Technology Futures through the Alberta Ingenuity
   Centre for Machine Learning (AICML); NSERC
FX This work was supported by the Alberta Innovates Technology Futures
   through the Alberta Ingenuity Centre for Machine Learning (AICML) and
   NSERC. During this work, A. Gyorgy was with the Department of Computing
   Science, University of Alberta.
CR Alon N., 2013, ADV NEURAL INFORM PR, P1610
   Alon N, 2015, P 28 C LEARN THEOR C, V40, P23
   Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Buccapatnam Swapna, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P289, DOI 10.1145/2591971.2591989
   Caron S., 2012, P 28 C UNC ART INT, P142
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Combes R., 2014, P 31 INT C MACH LEAR, P521
   Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440
   Kaufmann E., 2015, J MACHINE LEARNING R
   Kocak T., 2014, ADV NEURAL INFORM PR, P613
   Lattimore T, 2014, LECT NOTES ARTIF INT, V8776, P200, DOI 10.1007/978-3-319-11662-4_15
   Li  Lihong, 2015, P INT C ART INT STAT, P608
   Lin  Tian, 2014, P 31 INT C MACH LEAR
   Magureanu  S., 2014, P C LEARN THEOR COLT, P975
   Mannor S., 2011, ADV NEURAL INFORM PR, P684
   Wu Yifan, 2015, ARXIV151008108
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103071
DA 2019-06-15
ER

PT S
AU Xie, B
   Liang, YY
   Song, L
AF Xie, Bo
   Liang, Yingyu
   Song, Le
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they cannot scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees that it converges at the rate (O) over tilde (1/t) to the global optimum, even for the top k eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.
C1 [Xie, Bo; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Liang, Yingyu] Princeton Univ, Princeton, NJ 08544 USA.
RP Xie, B (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM bo.xie@gatech.edu; yingyul@cs.princeton.edu; lsong@cc.gatech.edu
FU NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF
   [IIS-1218749, CAREER IIS-1350983, CCF-0832797, CCF-1117309, CCF-1302518,
   DMS-1317308]; Simons Investigator Award; Simons Collaboration Grant
FX The research was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR
   N00014-15-1-2340, NSF IIS-1218749, NSF CAREER IIS-1350983, NSF
   CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator
   Award, and Simons Collaboration Grant.
CR Anandkumar Anima, 2012, CORR
   Arora R., 2013, ADV NEURAL INFORM PR, V26, P1815
   Balsubramani A., 2013, ADV NEURAL INFORM PR, V26, P3174
   Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998
   Chin TJ, 2007, IEEE T IMAGE PROCESS, V16, P1662, DOI 10.1109/TIP.2007.896668
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Honeine P, 2012, IEEE T PATTERN ANAL, V34, P1814, DOI 10.1109/TPAMI.2011.270
   Kim KI, 2005, IEEE T PATTERN ANAL, V27, P1351, DOI 10.1109/TPAMI.2005.181
   Kim M., 2009, INT C ART INT STAT, P280
   Le Quoc, 2013, INT C MACH LEARN
   Lopez-Paz D., 2014, INT C MACH LEARN ICM
   Meier F., 2014, ADV NEURAL INFORM PR, P972
   Muller K., 2012, ADV NEURAL INFORM PR, V25, P449
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Oja E., 1983, SUBSPACE METHODPAT
   Rahimi A., 2008, ADV NEURAL INFORM PR, V20
   Rahimi A., 2009, NEURAL INFORM PROCES
   SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0
   Schraudolph N. N., 2007, ADV NEURAL INFORM PR, V19
   Shamir O., 2014, ARXIV14092848
   Song L., 2014, INT C MACH LEARN ICM
   Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z
   Williams C, 2000, INT C P 17 MACH LEAR, P1159
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101073
DA 2019-06-15
ER

PT S
AU Yanardag, P
   Vishwanathan, SVN
AF Yanardag, Pinar
   Vishwanathan, S. V. N.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Structural Smoothing Framework For Robust Graph-Comparison
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we propose a general smoothing framework for graph kernels by taking structural similarity into account, and apply it to derive smoothed variants of popular graph kernels. Our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (NLP). However, unlike NLP applications that primarily deal with strings, we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs. Moreover, we discuss extensions of the Pitman-Yor process that can be adapted to smooth structured objects, thereby leading to novel graph kernels. Our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features. Experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants, but also outperform several other graph kernels in the literature. Our kernels are competitive in terms of runtime, and offer a viable option for practitioners.
C1 [Yanardag, Pinar] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
   [Vishwanathan, S. V. N.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA USA.
RP Yanardag, P (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
EM ypinar@purdue.edu; vishy@ucsc.edu
FU National Science Foundation [1219015]
FX We thank to Hyokun Yun for his tremendous help in implementing
   Pitman-Yor Processes. We also thank to anonymous NIPS reviewers for
   their constructive comments, and Jiasen Yang, Joon Hee Choi, Amani Abu
   Jabal and Parameswaran Raman for reviewing early drafts of the paper.
   This work is supported by the National Science Foundation under grant
   No. #1219015.
CR Borgwardt K. M., 2005, ISMB
   Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132
   Chen S. F., 1996, P ACL 1996 SANT CRUZ, V34, P310, DOI DOI 10.3115/981863.981904
   Croce D., 2011, P C EMP METH NAT LAN, P1034
   DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046
   Feragen A., 2013, ADV NEURAL INFORM PR, P216
   GARTNER T, 2003, COLT, V2777, P129
   Goldwater S., 2006, NIPS
   Goldwater S, 2011, J MACH LEARN RES, V12, P2335
   Haussler D., 1999, UCSCRL9910
   Kandola J, 2003, LECT NOTES ARTIF INT, V2777, P288, DOI 10.1007/978-3-540-45167-9_22
   Kneser R., 1995, ICASSP
   McKay BD, 2007, NAUTY USERS GUIDE VE
   Neumann M., 2012, ICML 2012 WORKSH MIN
   NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001
   Pitman J, 1997, ANN PROBAB, V25, P855
   Przulj N., 2006, ECCB
   Ramon J., 2003, 1 INT WORKSH MIN GRA
   Scholkopf B., 2002, LEARNING KERNELS
   Severyn A, 2012, DATA MIN KNOWL DISC, V25, P325, DOI 10.1007/s10618-012-0276-8
   Shervashidze N., 2010, NIPS
   Shervashidze N., 2009, AISTATS
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   SMOLA AJ, 2003, COMPUTATIONAL LEARNI, V2777, P144
   Teh Y. W., 2006, ACL
   Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130
   Vishwanathan S. V. N., 2010, JMLR
   Wale N, 2008, KNOWL INF SYST, V14, P347, DOI 10.1007/s10115-007-0103-5
   Yanardag P., 2015, P 21 ACM SIGKDD INT, P1365, DOI DOI 10.1145/2783258.2783417
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102029
DA 2019-06-15
ER

PT S
AU Yang, EH
   Lozano, AC
   Ravikumar, P
AF Yang, Eunho
   Lozano, Aurelie C.
   Ravikumar, Pradeep
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Closed-form Estimators for High-dimensional Generalized Linear Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGULARIZATION; SPARSE
AB We propose a class of closed-form estimators for GLMs under high-dimensional sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under high-dimensional settings, and (b) available in closed-form. We then perform thresholding operations on this MLE variant to obtain our class of estimators. We derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized GLM MLEs, even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators, as well as corollaries of our general theorem, for the special cases of logistic, exponential and Poisson regression models. We corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.
C1 [Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Armonk, NY 10504 USA.
   [Ravikumar, Pradeep] Univ Texas Austin, Austin, TX 78712 USA.
RP Yang, EH (reprint author), IBM TJ Watson Res Ctr, Armonk, NY 10504 USA.
EM eunhyang@us.ibm.com; aclozano@us.ibm.com; pradeepr@cs.utexas.edu
CR Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521
   Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600
   Bunea F, 2008, ELECTRON J STAT, V2, P1153, DOI 10.1214/08-EJS287
   Cohen M. B., 2014, P 46 ANN ACM S THEOR, P343
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Hoffman G. E., 2013, PLOS COMPUTATIONAL B
   Kakade S. M., 2010, INT C AI STAT AISTAT
   Kim Y, 2006, STAT SINICA, V16, P375
   Koh K, 2007, J MACHINE LEARNING R, V1, P1519
   McCullagh P, 1989, MONOGRAPHS STAT APPL, V37
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   Negahban S., 2010, ARXIV10102731V1
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Rothman AJ, 2009, J AM STAT ASSOC, V104, P177, DOI 10.1198/jasa.2009.0101
   Spielman DA, 2003, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2003.1238215
   Spielman DA, 2014, SIAM J MATRIX ANAL A, V35, P835, DOI 10.1137/090771430
   van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Witten DM, 2010, STAT METHODS MED RES, V19, P29, DOI 10.1177/0962280209105024
   Yang E., 2012, NEUR INFO PROC SYS N, P25
   Yang E., 2014, INT C MACH LEARN ICM, P31
   Yang E., 2014, NEUR INFO PROC SYS N, P27
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101031
DA 2019-06-15
ER

PT S
AU Yang, E
   Lozano, AC
AF Yang, Eunho
   Lozano, Aurelie C.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID REGRESSION; SELECTION
AB Gaussian Graphical Models (GGMs) are popular tools for studying network structures. However, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the Gaussian distribution. In this paper, we propose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Our method guards against outliers by an implicit trimming mechanism akin to the popular Least Trimmed Squares method used for linear regression. We provide a rigorous statistical analysis of our estimator in the high-dimensional setting. In contrast, existing approaches for robust sparse GGMs estimation lack statistical guarantees. Our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach.
C1 [Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Yang, EH (reprint author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM eunhyang@us.ibm.com; aclozano@us.ibm.com
CR Alfons A, 2013, ANN APPL STAT, V7, P226, DOI 10.1214/12-AOAS575
   Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brem RB, 2005, P NATL ACAD SCI USA, V102, P1572, DOI 10.1073/pnas.0408709102
   CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341
   Daye ZJ, 2012, BIOMETRICS, V68, P316, DOI 10.1111/j.1541-0420.2011.01652.x
   Finegold M, 2011, ANN APPL STAT, V5, P1057, DOI 10.1214/10-AOAS410
   Friedman J., 2007, BIOSTATISTICS
   Hassner M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P538
   Hsieh C. J., 2011, NEUR INFO PROC SYS N, V24
   Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577
   Kanehisa M, 2014, NUCLEIC ACIDS RES, V42, pD199, DOI 10.1093/nar/gkt1076
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Loh P-L, 2013, ARXIV13052436V2
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Nesterov Y., 2007, 76 CORE UCL
   Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347
   Oh JH, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-S7-S5
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Ripley B.D, 1981, SPATIAL STAT
   Sun H, 2012, BIOMETRICS, V68, P1197, DOI 10.1111/j.1541-0420.2012.01785.x
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   WOODS JW, 1978, IEEE T AUTOMAT CONTR, V23, P846, DOI 10.1109/TAC.1978.1101866
   Yang E., 2012, NEUR INFO PROC SYS N, V25
   Yang E., 2015, ARXIV151008512
   Yang E, 2014, ADV NEUR IN, V27
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100071
DA 2019-06-15
ER

PT S
AU Yang, JM
   Reed, S
   Yang, MH
   Lee, H
AF Yang, Jimei
   Reed, Scott
   Yang, Ming-Hsuan
   Lee, Honglak
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Weakly-supervised Disentangling with Recurrent Transformations for 3D
   View Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture longterm dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.
C1 [Yang, Jimei; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA.
   [Reed, Scott; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Yang, JM (reprint author), Univ Calif Merced, Merced, CA 95343 USA.
EM jyang44@ucmerced.edu; reedscot@umich.edu; mhyang@ucmerced.edu;
   honglak@umich.edu
CR Aubry M., 2014, CVPR
   Aubry M., 2015, ICCV
   Ba  Jimmy, 2015, ICLR
   Bengio Y., 2009, ICML
   Blanz Volker, 1999, SIGGRAPH
   Cheung B., 2015, ICLR
   Ding W., 2014, NIPS DEEP LEARN REPR
   Dosovitskiy Alexey, 2015, CVPR
   Flynn J., 2015, ARXIV150606825
   Girshick R., 2015, ICCV
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hinton G. E., 2011, ICANN
   Jia Y., 2014, ARXIV14085093
   Kholgade N, 2014, SIGGRAPH
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, NIPS
   Kulkarni Tejas D, 2015, NIPS
   Long  J., 2015, CVPR
   Michalski V., 2014, NIPS
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Reed S., 2014, ICML
   SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701
   Simonyan Karen, 2015, ICLR
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Tieleman T., 2014, THESIS
   Vinyals O, 2015, CVPR
   Zaremba W., 2014, ARXIV14104615
   Zhu Xiangyu, 2015, CVPR
   Zhu Z, 2014, NIPS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100007
DA 2019-06-15
ER

PT S
AU Yarkony, J
   Fowlkes, CC
AF Yarkony, Julian
   Fowlkes, Charless C.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Planar Ultrametrics for Image Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.
C1 [Yarkony, Julian] Experian Data Lab, San Diego, CA 92130 USA.
   [Fowlkes, Charless C.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA.
RP Yarkony, J (reprint author), Experian Data Lab, San Diego, CA 92130 USA.
EM julian.yarkony@experian.com; fowlkes@ics.uci.edu
FU NSF [IIS-1253538, DBI-1262547]; Experian
FX JY acknowledges the support of Experian, CF acknowledges support of NSF
   grants IIS-1253538 and DBI-1262547
CR Ailon N, 2005, ANN IEEE SYMP FOUND, P73, DOI 10.1109/SFCS.2005.36
   Andres B, 2011, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2011.6126550
   Andres Bjoern, 2012, P ECCV
   Andres Bjoern, 2013, P EMMCVPR
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bachrach Yoram, 2013, P AAAI
   Bagon Shai, 2011, CORR
   BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023
   BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028
   Barahona F, 1991, MATH PROGRAM, V36, P53
   Beier T, 2014, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2014.17
   Deza M M, 1997, GEOMETRY CUTS METRIC, V15
   FISHER ME, 1966, J MATH PHYS, V7, P1776, DOI 10.1063/1.1704825
   Kim S., 2011, ADV NEURAL INFORM PR, P1530
   Kolmogorov V, 2009, MATH PROGRAM COMPUT, V1, P43, DOI 10.1007/s12532-009-0002-8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Yarkony Julian, 2014, NEW FRONTIERS MINING
   Yarkony Julian, 2014, NIPS 2014 WORKSH
   Yarkony Julian, 2012, P ECCV
   Zhang C, 2014, LECT NOTES COMPUT SC, V8673, P9, DOI 10.1007/978-3-319-10404-1_2
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101062
DA 2019-06-15
ER

PT S
AU Yen, IEH
   Zhong, K
   Hsieh, CJ
   Ravikumar, P
   Dhillon, IS
AF Yen, Ian E. H.
   Zhong, Kai
   Hsieh, Cho-Jui
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Sparse Linear Programming via Primal and Dual Augmented Coordinate
   Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Over the past decades, Linear Programming (LP) has been widely used in different areas and considered as one of the mature technologies in numerical optimization. However, the complexity offered by state-of-the-art algorithms (i.e. interior-point method and primal, dual simplex methods) is still unsatisfactory for problems in machine learning with huge number of variables and constraints. In this paper, we investigate a general LP algorithm based on the combination of Augmented Lagrangian and Coordinate Descent (AL-CD), giving an iteration complexity of O((log(1/epsilon))(2)) with O(nnz(A)) cost per iteration, where nnz(A) is the number of non-zeros in the m x n constraint matrix A, and in practice, one can further reduce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy. The algorithm thus yields a tractable alternative to standard LP methods for large-scale problems of sparse solutions and nnz(A) << mn. We conduct experiments on large-scale LP instances from l(1)-regularized multi-class SVM, Sparse Inverse Covariance Estimation, and Nonnegative Matrix Factorization, where the proposed approach finds solutions of 10(-3) precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods.(1)
C1 [Yen, Ian E. H.; Zhong, Kai; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
   [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA.
RP Yen, IEH (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM ianyen@cs.utexas.edu; zhongkai@ices.utexas.edu; chohsieh@ucdavis.edu;
   pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu
FU ARO [W911NF-12-1-0390]; NSF [CCF-1320746, CCF-1117055, IIS-1149803,
   IIS-1320894, IIS-1447574, DMS-1264033]; NIH as part of the Joint
   DMS/NIGMS Initiative to Support Research at the Interface of the
   Biological and Mathematical Sciences [R01 GM117594-01]
FX We acknowledge the support of ARO via W911NF-12-1-0390, and the support
   of NSF via grants CCF-1320746, CCF-1117055, IIS-1149803, IIS-1320894,
   IIS-1447574, DMS-1264033, and NIH via R01 GM117594-01 as part of the
   Joint DMS/NIGMS Initiative to Support Research at the Interface of the
   Biological and Mathematical Sciences.
CR Bello D, 2006, 2006 IEEE SYSTEMS AND INFORMATION ENGINEERING DESIGN SYMPOSIUM, P90, DOI 10.1109/SIEDS.2006.278719
   Chen C., 2014, MATH PROGRAMMING
   Delbos F., 2003, GLOBAL LINEAR CONVER
   Dhillon I. S., 2011, NIPS
   Eleuterio Vania Lucia Dos Santos, 2009, THESIS
   Evtushenko YG, 2005, OPTIM METHOD SOFTW, V20, P515, DOI 10.1080/10556780500139690
   Gillis N., 2014, JMLR
   Gondzio J., 2012, EJOR
   Gondzio J., 2012, COMPUTATIONAL OPTIMI
   GULER O, 1992, J OPTIMIZ THEORY APP, V75, P445, DOI 10.1007/BF00940486
   HOFFMAN AJ, 1952, J RES NAT BUR STAND, V49, P263, DOI 10.6028/jres.049.027
   Hong M., 2012, LINEAR CONVERGENCE A
   Hsieh C., 2008, ICML, V307
   Joachims  T., 2006, KDD
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lin Ting-Wei, 2014, NIPS
   Meindl B., 2012, EUROSTAT STAT NETHER
   More JJ, 1991, SIAM J OPTIMIZ, V1, P93, DOI 10.1137/0801008
   Nellore A., 2013, RECOVERY GUARANTEES
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Rahimi A., 2007, NIPS
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Wang H., 2014, NIPS
   Wang PW, 2014, J MACH LEARN RES, V15, P1523
   Yen I., 2013, KDD
   Yen I., 2015, ICML
   Yen I. E.-H., 2015, NIPS
   Yuan G., 2010, JMLR, V11
   Yuan M., 2010, JMLR
   Zhong K., 2014, NIPS
   Zhu J., 2004, NIPS
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102066
DA 2019-06-15
ER

PT S
AU Yen, IEH
   Lin, SW
   Lin, SD
AF Yen, Ian E. H.
   Lin, Shan-Wei
   Lin, Shou-De
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Dual-Augmented Block Minimization Framework for Learning with Limited
   Memory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In past few years, several techniques have been proposed for training of linear Support Vector Machine (SVM) in limited-memory setting, where a dual block-coordinate descent (dual-BCD) method was used to balance cost spent on I/O and computation. In this paper, we consider the more general setting of regularized Empirical Risk Minimization (ERM) when data cannot fit into memory. In particular, we generalize the existing block minimization framework based on strong duality and Augmented Lagrangian technique to achieve global convergence for general convex ERM. The block minimization framework is flexible in the sense that, given a solver working under sufficient memory, one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition. We conduct experiments on L1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.
C1 [Yen, Ian E. H.] Univ Texas Austin, Austin, TX 78712 USA.
   [Lin, Shan-Wei; Lin, Shou-De] Natl Taiwan Univ, Taipei, Taiwan.
RP Yen, IEH (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM ianyen@cs.utexas.edu; r03922067@csie.ntu.edu.tw; sdlin@csie.ntu.edu.tw
FU Telecommunication Lab., Chunghwa Telecom Co., Ltd [TL-103-8201]; AOARD
   [FA2386-13-1-4045]; Ministry of Science and Technology; National Taiwan
   University; Intel Co. [MOST102-2911-I-002-001, NTU103R7501,
   102-2923-E-002-007-MY2, 102-2221-E-002-170, 103-2221-E-002-104-MY2]
FX We thank to the support of Telecommunication Lab., Chunghwa Telecom Co.,
   Ltd via TL-103-8201, AOARD via No. FA2386-13-1-4045, Ministry of Science
   and Technology, National Taiwan University and Intel Co. via
   MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2,
   102-2221-E-002-170, 103-2221-E-002-104-MY2.
CR Boyd S., 2011, FDN TRENDS MACHINE L
   Chang K., 2011, SIGKDD
   Deng J., 2009, CVPR
   Hoffman A., 1952, J RES NBS
   Hong M., 2012, LINEAR CONVERGENCE A
   Hsieh C., 2014, NIPS
   Jaggi M., 2014, NIPS
   Joachims T., 2005, ICML
   Kakade S., 2009, CORR
   Lin Ting-Wei, 2014, NIPS
   Ma C., 2015, ICML
   Obozinski G., 2011, GROUP LASSO OVERLAPS
   Rahimi A., 2007, NIPS
   Richtarik P., 2014, MATH PROGRAMMING
   Shalev-Shwartz S., 2011, JMLR
   Shalev-Shwartz S., 2011, MATH PROGRAMMING
   Srebro N., 2011, NIPS
   Tibshirani R., 1996, J ROYAL STAT SOC
   Tomioka R., 2011, JMLR
   Trofimov I., 2014, DISTRIBUTED COORDINA
   Wang P., 2014, JMLR
   Yen I., 2013, SIGKDD
   Yen I., 2015, ICML
   Yen I. E.-H., 2015, NIPS
   Yu H., 2010, SIGKDD
   Yuan G. X., 2010, JMLR
   Zhong K., 2014, NIPS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102072
DA 2019-06-15
ER

PT S
AU Yi, XY
   Wang, ZR
   Caramanis, C
   Liu, H
AF Yi, Xinyang
   Wang, Zhaoran
   Caramanis, Constantine
   Liu, Han
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Optimal Linear Estimation under Unknown Nonlinear Transform
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID PRINCIPAL COMPONENT ANALYSIS; PHASE RETRIEVAL; SPARSE
AB Linear regression studies the problem of estimating a model parameter beta* is an element of R-p, from n observations {(y(i), X-i)}(i=1)(n) from linear model y(i) = < X-i, beta*> + is an element of(i). We consider a significant generalization in which the relationship between < X-i, beta*> and y(i) is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover beta* in settings (i.e., classes of link function f) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between y(i) and < X-i, beta*>. We also consider the high dimensional setting where beta* is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p >> n. For a broad class of link functions between < X-i, beta*> and y(i), we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.
C1 [Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA.
   [Wang, Zhaoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA.
RP Yi, XY (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM yixy@utexas.edu; zhaoran@princeton.edu; constantine@utexas.edu;
   hanliu@princeton.edu
FU NSF [1056028, 1302435, 1116955, IIS1408910, IIS1332109]; U.S. Department
   of Transportation through the Data-Supported Transportation Operations
   and Planning (D-STOP) Tier 1 University Transportation Center; NSF
   CAREER Award [DMS1454377]; NIH [R01MH102339, R01GM083084, R01HG06841];
   MSR PhD fellowship
FX XY and CC would like to acknowledge NSF grants 1056028, 1302435 and
   1116955. This research was also partially supported by the U.S.
   Department of Transportation through the Data-Supported Transportation
   Operations and Planning (D-STOP) Tier 1 University Transportation
   Center. HL is grateful for the support of NSF CAREER Award DMS1454377,
   NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and
   NIH R01HG06841. ZW was partially supported by MSR PhD fellowship while
   this work was done.
CR Alquier P, 2013, J MACH LEARN RES, V14, P243
   Berthet Q., 2013, C LEARN THEOR
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178
   Candes EJ, 2013, SIAM J IMAGING SCI, V6, P199, DOI 10.1137/110848074
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Delecroix M, 2006, J STAT PLAN INFER, V136, P730, DOI 10.1016/j.jspi.2004.09.006
   DELECROIX M., 2000, TECH REP
   Eldar YC, 2014, APPL COMPUT HARMON A, V36, P473, DOI 10.1016/j.acha.2013.08.003
   GOPI S., 2013, INT C MACH LEARN
   HARDLE W, 1993, ANN STAT, V21, P157, DOI 10.1214/aos/1176349020
   Hristache M, 2001, ANN STAT, V29, P595
   JACQUES L., 2011, ARXIV11043160
   Kakade S. M., 2011, ADV NEURAL INFORM PR
   Kalai A, 2009, C LEARN THEOR
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2
   Natarajan N., 2013, ADV NEURAL INFORM PR
   PLAN Y., 2014, ARXIV14043749
   Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442
   POWELL JL, 1989, ECONOMETRICA, V57, P1403, DOI 10.2307/1913713
   Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007
   STOKER TM, 1986, ECONOMETRICA, V54, P1461, DOI 10.2307/1914309
   Tibshirani Julie, 2013, ARXIV13054987
   Vershynin  Roman, 2010, ARXIV10113027
   VU V. Q., 2013, ADV NEURAL INFORM PR
   Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008
   YI X., 2015, ARXIV150503257
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103051
DA 2019-06-15
ER

PT S
AU Yi, XY
   Caramanis, C
AF Yi, Xinyang
   Caramanis, Constantine
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Regularized EM Algorithms: A Unified Framework and Statistical
   Guarantees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MATRIX RECOVERY; RANK
AB Latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art highdimensional prescriptions (e.g., a la [19]) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.
C1 [Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
RP Yi, XY (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM yixy@utexas.edu; constantine@utexas.edu
FU NSF [1056028, 1302435, 1116955]; U.S. Department of Transportation
   through the Data-Supported Transportation Operations and Planning
   (D-STOP) Tier 1 University Transportation Center
FX The authors would like to acknowledge NSF grants 1056028, 1302435 and
   1116955. This research was also partially supported by the U.S.
   Department of Transportation through the Data-Supported Transportation
   Operations and Planning (D-STOP) Tier 1 University Transportation
   Center.
CR Balakrishnan S., 2014, ARXIV14082156
   Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   CHAGANTY A. T, 2013, ARXIV13063729
   Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205
   Chen Yudong, 2014, C LEARN THEOR
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Loh PL, 2011, ADV NEURAL INFORM PR, P2726
   Ma JW, 2005, NEUROCOMPUTING, V68, P105, DOI 10.1016/j.neucom.2004.12.009
   McLachlan G., 2007, EM ALGORITHM EXTENSI, V382
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Po-Ling Loh, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2601, DOI 10.1109/ISIT.2012.6283989
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z
   Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073
   Vershynin  Roman, 2010, ARXIV10113027
   Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643
   Wang Z., 2014, ARXIV14128729
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   YI X., 2013, ARXIV13103745
NR 22
TC 0
Z9 0
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100088
DA 2019-06-15
ER

PT S
AU Yoshikawa, Y
   Iwata, T
   Sawada, H
   Yamada, T
AF Yoshikawa, Yuya
   Iwata, Tomoharu
   Sawada, Hiroshi
   Yamada, Takeshi
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of
   Latent Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID SPACE
AB We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.
C1 [Yoshikawa, Yuya] Nara Inst Sci & Technol, Nara 6300192, Japan.
   [Iwata, Tomoharu; Yamada, Takeshi] NTT Commun Sci Labs, Kyoto 6190237, Japan.
   [Sawada, Hiroshi] NTT Serv Evolut Labs, Yokosuka, Kanagawa 2390847, Japan.
RP Yoshikawa, Y (reprint author), Chiba Inst Technol, Software Technol & Artificial Intelligence Res La, Narashino, Chiba, Japan.
EM yoshikawa.yuya.yl9@is.naist.jp; iwata.tomoharu@lab.ntt.co.jp;
   sawada.hiroshi@lab.ntt.co.jp; yamada.tak@lab.ntt.co.jp
FU JSPS [259867]
FX This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867).
CR Akaho S, 2001, P INT M PSYCH SOC
   Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Dudik M, 2007, J MACH LEARN RES, V8, P1217
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gretton A., 2008, ADV NEURAL INFORM PR
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Iwata Tomoharu, 2009, ADV NEURAL INFORM PR
   Iwata Tomoharu, 2011, P 22 INT JOINT C ART
   Kamencay P, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58251
   Li Bin, 2009, P 26 ANN INT C MACH
   Li YY, 2006, J INTELL INF SYST, V27, P117, DOI 10.1007/s10844-006-1627-y
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Muandet Krikamol, 2012, ADVANCES IN NEURAL I
   Muandet Krikamol, 2013, P 29 C UNC ART INT
   Ngiam J, 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.1145/2647868
   Rasiwasia N., 2010, P INT C MULT, P251
   Sejdinovic Dino, 2013, ADV NEURAL INFORM PR
   Smola Alex, 2007, ALGORITHMIC LEARNING
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Vinokourov Alexei, 2003, ADV NEURAL INFORM PR
   Yoshikawa Yuya, 2015, P 29 AAAI C ART INT
   Yoshikawa Yuya, 2014, ADV NEURAL INFORM PR
   Zhang T, 2013, P 23 INT JOINT C ART
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102107
DA 2019-06-15
ER

PT S
AU Yun, SY
   Lelarge, M
   Proutiere, A
AF Yun, Se-Young
   Lelarge, Marc
   Proutiere, Alexandre
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast and Memory Optimal Low-Rank Matrix Approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we revisit the problem of constructing a near-optimal rank k approximation of a matrix M is an element of [0, 1](mxn) under the streaming data model where the columns of M are revealed sequentially. We present SLA (Streaming Low-rank Approximation), an algorithm that is asymptotically accurate, when ks(k+1) (M) = o(root mn) where s(k+1)(M) is the (k + 1)-th largest singular value of M. This means that its average mean-square error converges to 0 as m and n grow large (i.e., parallel to(M) over cap ((k)) - M-(k)parallel to(2)(F) = o(mn) with high probability, where (M) over cap ((k)) and M-(k) denote the output of SLA and the optimal rank k approximation of M, respectively). Our algorithm makes one pass on the data if the columns of M are revealed in a random order, and two passes if the columns of M arrive in an arbitrary order. To reduce its memory footprint and complexity, SLA uses random sparsification, and samples each entry of M with a small probability delta. In turn, SLA is memory optimal as its required memory space scales as k (m + n), the dimension of its output. Furthermore, SLA is computationally efficient as it runs in O(delta kmn) time (a constant number of operations is made for each observed entry of M), which can be as small as O (k log(m)(4)n) for an appropriate choice of delta and if n >= m.
C1 [Yun, Se-Young] MSR, Cambridge, England.
   [Lelarge, Marc] Inria, Rocquencourt, France.
   [Lelarge, Marc] ENS, Paris, France.
   [Proutiere, Alexandre] KTH, EE Sch, ACL, Stockholm, Sweden.
RP Yun, SY (reprint author), MSR, Cambridge, England.
EM seyoung.yun@inria.fr; marc.lelarge@ens.fr; alepro@kth.se
FU French Agence Nationale de la Recherche (ANR) [ANR-11-JS02-005-01]; ERC
   FSA grant; SSF ICT-Psi project
FX Work performed as part of MSR-INRIA joint research centre. M.L.
   acknowledges the support of the French Agence Nationale de la Recherche
   (ANR) under reference ANR-11-JS02-005-01 (GAP project).; A. Proutiere's
   research is supported by the ERC FSA grant, and the SSF ICT-Psi project.
CR Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097
   Bhojanapalli S., 2015, P 26 ANN ACM SIAM S, P902
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Clarkson KL, 2009, ACM S THEORY COMPUT, P205
   Ghashami M., 2014, SODA, P707, DOI DOI 10.1137/1.9781611973402.53
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Liberty E., 2013, P 19 ACM SIGKDD INT, P581, DOI DOI 10.1145/2487575.2487623
   Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR
   Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Woodruff D., 2014, ADV NEURAL INFORM PR, P1781
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102077
DA 2019-06-15
ER

PT S
AU Yurtsever, A
   Quoc, TD
   Cevher, V
AF Yurtsever, Alp
   Quoc Tran-Dinh
   Cevher, Volkan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Universal Primal-Dual Convex Optimization Framework
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown Holder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.
C1 [Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland.
   [Quoc Tran-Dinh] UNC, Dept Stat & Operat Res, Chapel Hill, NC USA.
RP Yurtsever, A (reprint author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland.
EM alp.yurtsever@epfl.ch; quoctd@email.unc.edu; volkan.cevher@epfl.ch
FU ERC Future Proof; SNF [200021-146750, CRSII2-147633]
FX This work was supported in part by ERC Future Proof, SNF 200021-146750
   and SNF CRSII2-147633. We would like to thank Dr. Stephen Becker of
   University of Colorado at Boulder for his support in preparing the
   numerical experiments.
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397
   Combettes PL, 2008, INVERSE PROBL, V24, DOI 10.1088/0266-5611/24/6/065014
   Goldstein T., 2013, ADAPTIVE PRIMAL DUAL
   Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401
   Jaggi M., 2010, P 27 INT C MACH LEAR, P471
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   Juditsky A, 2016, MATH PROGRAM, V156, P221, DOI 10.1007/s10107-015-0876-3
   Lan GH, 2016, MATH PROGRAM, V155, P511, DOI 10.1007/s10107-015-0861-x
   Larsen R. M, 1969, PROPACK SOFTWARE LAR
   Nemirovskii A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2015, TECH REP
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Rockafellar RT, 1970, PRINCETON MATH SERIE
   Shefi R, 2014, SIAM J OPTIMIZ, V24, P269, DOI 10.1137/130910774
   Tran-Dinh Q., 2014, ADV NEURAL INFORM PR, V27
   Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643
   Yu Y. L., 2014, THESIS
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101085
DA 2019-06-15
ER

PT S
AU Zhang, CC
   Chaudhuri, K
AF Zhang, Chicheng
   Chaudhuri, Kamalika
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Active Learning from Weak and Strong Labelers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.
   This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.
C1 [Zhang, Chicheng; Chaudhuri, Kamalika] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Zhang, CC (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM chichengzhang@ucsd.edu; kamalika@eng.ucsd.edu
FU NSF [IIS 1162581]
FX We thank NSF under IIS 1162581 for research support and Jennifer Dy for
   introducing us to the problem of active learning from multiple labelers.
CR Balcan M.-F., 2013, COLT
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Beygelzimer A., 2009, ACTIVE LEARNING ERM
   Beygelzimer A., 2010, NIPS
   Bshouty NH, 2005, MACH LEARN, V59, P99, DOI 10.1007/s10994-005-0464-5
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Crammer K., 2005, NIPS
   Dasgupta S., 2007, NIPS
   Dasgupta S., 2005, NIPS
   Dekel O, 2012, J MACH LEARN RES, V13, P2655
   Donmez P., 2008, CIKM
   Fang M, 2012, IEEE DATA MINING, P858, DOI 10.1109/ICDM.2012.64
   Hanneke S., 2007, ICML
   Hsu Daniel, 2010, THESIS
   Ipeirotis PG, 2014, DATA MIN KNOWL DISC, V28, P402, DOI 10.1007/s10618-013-0306-1
   Kalai AT, 2012, J COMPUT SYST SCI, V78, P1481, DOI 10.1016/j.jcss.2011.12.026
   Kanade Varun, 2014, COLT
   Lin C. H., 2015, ICML WORKSH CROWDS M
   Lin C. H., 2014, HCOMP
   Malago L., 2014, NIPS WORKSH LEARN WI
   Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298
   Simon HU, 2014, ANN MATH ARTIF INTEL, V71, P283, DOI 10.1007/s10472-012-9325-7
   Song S., 2015, AISTATS
   Urner R, 2012, 15 INT C ART INT STA, P1252
   Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4
   Vijayanarasimhan S, 2009, PROC CVPR IEEE, P2262, DOI 10.1109/CVPRW.2009.5206705
   Welinder P., 2010, ADV NEURAL INFORM PR, P2424
   Yan Y., 2012, P 15 INT C ART INT S, P1350
   Yan Y, 2011, P 28 INT C MACH LEAR, P1161
   Zhang C., 2014, NIPS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103026
DA 2019-06-15
ER

PT S
AU Zhang, CC
   Song, JM
   Chen, KC
   Chaudhuri, K
AF Zhang, Chicheng
   Song, Jimin
   Chen, Kevin C.
   Chaudhuri, Kamalika
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Spectral Learning of Large Structured HMMs for Comparative Epigenomics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DISCOVERY
AB We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure.
   The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.
C1 [Zhang, Chicheng; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA.
   [Song, Jimin; Chen, Kevin C.] Rutgers State Univ, New Brunswick, NJ USA.
RP Zhang, CC (reprint author), Univ Calif San Diego, San Diego, CA 92103 USA.
EM chz038@eng.ucsd.edu; song@dls.rutgers.edu; kcchen@dls.rutgers.edu;
   kamalika@eng.ucsd.edu
FU NSF [IIS 1162581]
FX KC and CZ thank NSF under IIS 1162581 for research support.
CR Anandkumar Anima, 2012, ABS12107559 CORR
   Anandkumar Animashree, 2012, ABS12030683 CORR
   Balle  B., 2014, ICML, P1386
   Balle B., 2014, MACHINE LEARNING, V96
   Bernstein BE, 2010, NAT BIOTECHNOL, V28, P1045, DOI 10.1038/nbt1010-1045
   Biesinger J, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S5-S4
   Chaganty A., 2014, ICML
   Creyghton MP, 2010, P NATL ACAD SCI USA, V107, P21931, DOI 10.1073/pnas.1016071107
   Djebali S., 2012, NATURE
   Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247
   Ernst J, 2011, NATURE, V473, P43, DOI 10.1038/nature09906
   Ernst J, 2010, NAT BIOTECHNOL, V28, P817, DOI 10.1038/nbt.1662
   Foster D., 2012, CORR
   Foti N., 2014, NIPS
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hoffman MM, 2012, NAT METHODS, V9, P473, DOI [10.1038/NMETH.1937, 10.1038/nmeth.1937]
   Hsu Daniel J., 2009, COLT
   Melnyk I., 2015, AISTATS
   Mossel E., 2006, ANN APPL PROBAB, V16
   Parikh AP, 2012, UAI
   Siddiqi Sajid M., 2010, AISTATS
   Song JM, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0598-0
   Song L., 2011, P 28 INT C MACH LEAR, P1065
   Song L., 2013, ICML
   Zhu Jun, 2010, PLOS COMPUT BIOL, V6
   Zou J., 2013, NIPS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102028
DA 2019-06-15
ER

PT S
AU Zhang, SX
   Choromanska, A
   LeCun, Y
AF Zhang, Sixin
   Choromanska, Anna
   LeCun, Yann
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep learning with Elastic Averaging SGD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.
C1 [Zhang, Sixin; Choromanska, Anna] NYU, Courant Inst, New York, NY 10012 USA.
   [LeCun, Yann] NYU, Ctr Data Sci, New York, NY USA.
   [LeCun, Yann] Facebook AI Res, New York, NY USA.
RP Zhang, SX (reprint author), NYU, Courant Inst, New York, NY 10012 USA.
EM zsx@cims.nyu.edu; achoroma@cims.nyu.edu; yann@cims.nyu.edu
CR Agarwal A., 2011, NIPS
   Azadi S, 2014, ICML
   Bekkerman R., 2011, SCALING MACHINE LEAR
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Borkar VS, 1998, SIAM J CONTROL OPTIM, V36, P840, DOI 10.1137/S0363012995282784
   Bottou L., 1998, ONLINE LEARNING NEUR
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Choromanska Anna, 2015, AISTATS
   Dean J., 2012, NIPS
   Hestenes MR, 1975, OPTIMIZATION THEORY
   Ho Q., 2013, NIPS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lan GH, 2012, MATH PROGRAM, V133, P365, DOI 10.1007/s10107-010-0434-y
   Langford J., 2009, NIPS
   LeCun Y, 2013, ARXIV
   Nedic A., 2001, STUDIES COMPUT MATH, V8, P381
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Ouyang H., 2013, P INT C MACH LEARN, P80
   Paine T, 2013, ARXIV
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Ranzato M., 2013, ARXIV
   Recht B., 2011, NIPS
   Seide F, 2014, INT CONF ACOUST SPEE
   Shamir O., 2014, NIPS
   Sutskever  I., 2013, ICML
   Wan L., 2013, ICML
   Zhang R., 2014, ICML
   Zinkevich M. A., 2010, NIPS
NR 31
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101021
DA 2019-06-15
ER

PT S
AU Zhao, T
   Wang, ZR
   Liu, H
AF Zhao, Tuo
   Wang, Zhaoran
   Liu, Han
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Nonconvex Optimization Framework for Low Rank Matrix Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID COMPLETION
AB We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.
C1 [Zhao, Tuo] Johns Hopkins Univ, Baltimore, MD 21218 USA.
   [Wang, Zhaoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA.
RP Zhao, T (reprint author), Johns Hopkins Univ, Baltimore, MD 21218 USA.
FU NSF [IIS1116730, IIS1332109, IIS1408910, IIS1546482-BIGDATA,
   DMS1454377-CAREER]; NIH [R01GM083084, R01HG06841, R01MH102339]; FDA
   [HHSF223201000072C]
FX Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910,
   NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH
   R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.
CR Arora S., 2015, ARXIV150300778
   Balakrishnan S., 2014, ARXIV14082156
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen Yudong, 2013, ARXIV13100154
   Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Hardt Moritz, 2014, ARXIV14022331
   Hardt Moritz, 2014, ARXIV14074070
   Hastie Trevor, 2014, ARXIV14102596
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Jain P., 2010, ADV NEURAL INFORM PR, V23, P937
   Jain P., 2014, ARXIV14111087
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Koren Yehuda, 2009, NETFLIX PRIZE DOCUME, V81
   Lee K, 2010, IEEE T INFORM THEORY, V56, P4402, DOI 10.1109/TIT.2010.2054251
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Paterek Arkadiusz, 2007, P KDD CUP WORKSH, V2007, P5
   Recht B, 2013, MATH PROGRAM COMPUT, V5, P201, DOI 10.1007/s12532-013-0053-8
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860
   Stewart Gilbert W, 1990, MATRIX PERTURBATION, V175
   Sun Ruoyu, 2014, ARXIV14118003
   Takacs G., 2007, SIGKDD EXPLORATIONS, V9, P80, DOI DOI 10.1145/1345448.134546
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100101
DA 2019-06-15
ER

PT S
AU Zheng, QQ
   Lafferty, J
AF Zheng, Qinqing
   Lafferty, John
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Convergent Gradient Descent Algorithm for Rank Minimization and
   Semidefinite Programming from Random Linear Measurements
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With O(r(3)kappa(2)n log n) random measurements of a positive semidefinite nxn matrix of rank r and condition number kappa, our method is guaranteed to converge linearly to the global optimum.
C1 [Zheng, Qinqing; Lafferty, John] Univ Chicago, Chicago, IL 60637 USA.
RP Zheng, QQ (reprint author), Univ Chicago, Chicago, IL 60637 USA.
EM qinqing@cs.uchicago.edu; lafferty@galton.uchicago.edu
FU NSF [IIS-1116730]; ONR [N00014-12-1-0762]
FX Research supported in part by NSF grant IIS-1116730 and ONR grant
   N00014-12-1-0762.
CR Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664
   Bach F., 2011, ADV NEURAL INFORM PR
   Bach F, 2014, J MACH LEARN RES, V15, P595
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes E., 2014, ARXIV14071065
   d'Aspremont A., 2004, ADV NEURAL INFORM PR
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hoffman Matt, 2013, J MACHINE LEARNING R, V14
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Jain P., 2010, ADV NEURAL INFORM PR, V23, P937
   Laurent B, 2000, ANN STAT, V28, P1302
   Ledoux M, 2010, ELECTRON J PROBAB, V15, P1319, DOI 10.1214/EJP.v15-798
   Meka R., 2008, P 25 INT C MACH LEAR, P656
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Tomioka  R., 2010, ARXIV10100789
   Tropp Joel A., 2015, ARXIV150101571
   Tu S., 2015, ARXIV150703566
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101089
DA 2019-06-15
ER

PT S
AU Zheng, QQ
   Tomioka, R
AF Zheng, Qinqing
   Tomioka, Ryota
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Interpolating Convex and Non-Convex Tensor Decompositions via the
   Subspace Norm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio O (n(inverted right perpendicularK/2inverted left perpendicular/2)) for recovering a K th order rank one tensor of size n x...x n by recursive unfolding. In this paper, we first improve this bound to O (n(K/4)) by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal O (root n + root HK-1) bound, in which the parameter H controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with H = O (1).
C1 [Zheng, Qinqing] Univ Chicago, Chicago, IL 60637 USA.
   [Tomioka, Ryota] Toyota Technol Inst, Chicago, IL USA.
RP Zheng, QQ (reprint author), Univ Chicago, Chicago, IL 60637 USA.
EM qinqing@cs.uchicago.edu; tomioka@ttic.edu
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101082
DA 2019-06-15
ER

PT S
AU Zhong, MJ
   Goddard, N
   Sutton, C
AF Zhong, Mingjun
   Goddard, Nigel
   Sutton, Charles
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Latent Bayesian melding for integrating individual and population models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID INFERENCE
AB In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.
C1 [Zhong, Mingjun; Goddard, Nigel; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Zhong, MJ (reprint author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
EM mzhong@inf.ed.ac.uk; nigel.goddard@inf.ed.ac.uk; csutton@inf.ed.ac.uk
FU Engineering and Physical Sciences Research Council, UK [EP/K002732/1,
   EP/M008223/1]
FX This work is supported by the Engineering and Physical Sciences Research
   Council, UK (grant numbers EP/K002732/1 and EP/M008223/1).
CR Alkema L, 2007, ANN APPL STAT, V1, P229, DOI 10.1214/07-AOAS111
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Batra N., 2014, P 5 INT C FUT EN SYS, P265, DOI DOI 10.1145/2602044.2602051
   BORDLEY RF, 1982, MANAGE SCI, V28, P1137, DOI 10.1287/mnsc.28.10.1137
   Caticha A., 2007, 27 INT WORKSH BAYES
   Chiu GS, 2010, ENVIRONMETRICS, V21, P728, DOI 10.1002/env.1035
   Elhamifar E., 2015, P 29 AAAI C ART INT, P629
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   HART GW, 1992, P IEEE, V80, P1870, DOI 10.1109/5.192069
   Hyungsul Kim, 2011, SDM, V11, P747, DOI DOI 10.1137/1.9781611972818.64
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Kelly Jack, 2015, UK DALE DATASET DOME, V2
   Kolter J.Z., 2012, J MACHINE LEARNING R, P1472
   Liang P., 2009, 26 ANN INT C MACH LE, P641
   MacKinnon JG, 1998, J ECONOMETRICS, V85, P205, DOI 10.1016/S0304-4076(97)00099-7
   Mann G., 2008, ACL 08 ASS COMPUTATI, P870
   Myerscough Keith, 2014, ARXIV14116011
   Parson O., 2012, P 26 C ART INT AAAI, V22, P356
   Poole D, 2000, J AM STAT ASSOC, V95, P1244, DOI 10.2307/2669764
   Rufo MJ, 2012, BAYESIAN ANAL, V7, P411, DOI 10.1214/12-BA714
   Sevcikova H, 2011, TRANSPORT RES A-POL, V45, P540, DOI 10.1016/j.tra.2011.03.009
   Villelaa Daniel AM, 2015, CHOICE WEIGHTS LOGAR
   WOLPERT RL, 1995, J AM STAT ASSOC, V90, P426, DOI 10.2307/2291052
   Wytock M., 2014, AAAI C ART INT, P486
   Zhong M., 2014, ADV NEURAL INFORM PR, P3590
   Zimmermann JP, 2012, HOUSEHOLD ELECT SURV
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101016
DA 2019-06-15
ER

PT S
AU Zhou, MY
   Cong, YL
   Chen, B
AF Zhou, Mingyuan
   Cong, Yulai
   Chen, Bo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI The Poisson Gamma Belief Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.
C1 [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.
   [Cong, Yulai; Chen, Bo] Xidian Univ, Natl Lab RSP, Xian, Shaanxi, Peoples R China.
RP Zhou, MY (reprint author), Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.
FU Thousand Young Talent Program of China [61372132, NCET-13-0945]
FX M. Zhou thanks TACC for computational support. B. Chen thanks the
   support of the Thousand Young Talent Program of China, NSC-China
   (61372132), and NCET-13-0945.
CR Acharya  A., 2015, AISTATS
   Bengio Y., 2007, NIPS
   Bengio Y., 2015, DEEP LEARNING
   Bengio Yoshua, 2007, LARGE SCALE KERNEL M
   Blei D. M., 2003, J MACH LEARN RES
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gan Zhe, 2015, ICML
   Griffiths T.L., 2004, PNAS
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Larochelle H., 2012, NIPS
   Nair V., 2010, ICML
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Ranganath R., 2015, AISTATS
   Ranzato M., 2007, CVPR
   Salakhutdinov R., 2009, AISTATS
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Srivastava N., 2013, UAI
   Teh Y. W., 2006, J AM STAT ASS
   Welling M., 2004, ADV NEURAL INFORM PR, P1481
   Xing E. P., 2005, UAI
   Zhou M., 2014, NIPS
   Zhou M., 2012, AISTATS
   Zhou M., 2015, J AM STAT ASS
   Zhou M, 2015, 2015 IEEE International Wireless Symposium (IWS 2015)
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100013
DA 2019-06-15
ER

PT S
AU Acerbi, L
   Ma, WJ
   Vijayakumar, S
AF Acerbi, Luigi
   Ma, Wei Ji
   Vijayakumar, Sethu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Framework for Testing Identifiability of Bayesian Models of Perception
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID REPRESENTATIONS
AB Bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data. However, the intrinsic degeneracy of the Bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models, with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.
C1 [Acerbi, Luigi; Vijayakumar, Sethu] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
   [Acerbi, Luigi; Ma, Wei Ji] NYU, Ctr Neural Sci, New York, NY 10003 USA.
   [Acerbi, Luigi; Ma, Wei Ji] NYU, Dept Psychol, New York, NY 10003 USA.
RP Acerbi, L (reprint author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
EM luigi.acerbi@nyu.edu; weijima@nyu.edu; sethu.vijayakumar@ed.ac.uk
CR Acerbi L, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003661
   Acerbi L, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002771
   ANDERSON JR, 1978, PSYCHOL REV, V85, P249, DOI 10.1037//0033-295X.85.4.249
   Bowers JS, 2012, PSYCHOL BULL, V138, P389, DOI 10.1037/a0026450
   Carreira-Perpinan MA, 2000, IEEE T PATTERN ANAL, V22, P1318, DOI 10.1109/34.888716
   Chalk M, 2010, J VISION, V10, DOI 10.1167/10.8.2
   Geisler WS, 2011, VISION RES, V51, P771, DOI 10.1016/j.visres.2010.09.027
   Gekas N, 2013, J VISION, V13, DOI 10.1167/13.4.8
   Girshick AR, 2011, NAT NEUROSCI, V14, P926, DOI 10.1038/nn.2831
   Hedges JH, 2011, J VISION, V11, DOI 10.1167/11.6.14
   Houlsby NMT, 2013, CURR BIOL, V23, P2169, DOI 10.1016/j.cub.2013.09.012
   Jazayeri M, 2010, NAT NEUROSCI, V13, P1020, DOI 10.1038/nn.2590
   Jones M, 2011, BEHAV BRAIN SCI, V34, P169, DOI [10.1017/S0140525X10003134, 10.1017/S0140525X11001439]
   Knill D. C., 1996, PERCEPTION BAYESIAN
   Knill DC, 2003, VISION RES, V43, P831, DOI 10.1016/S0042-6989(03)00003-8
   Kording KP, 2004, P NATL ACAD SCI USA, V101, P9839, DOI 10.1073/pnas.0308394101
   Kording KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Kwon OS, 2013, P NATL ACAD SCI USA, V110, P381, DOI 10.1073/pnas.1214869110
   Maloney LT, 2009, VISUAL NEUROSCI, V26, P147, DOI 10.1017/S0952523808080905
   Mamassian P, 2010, NAT NEUROSCI, V13, P914, DOI 10.1038/nn0810-914
   NATARAJAN R, 2009, ADV NEURAL INFORM PR, V21, P1153
   Navarro DJ, 2004, COGNITIVE PSYCHOL, V49, P47, DOI 10.1016/j.cogpsych.2003.11.001
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Sanborn A. N., 2008, ADV NEURAL INFORM PR, V20, P1265
   Simoncelli E. P., 2009, COGNITIVE NEUROSCIEN, P525
   Spiegelhalter DJ, 2002, J ROY STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353
   Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669
   Tassinari H, 2006, J NEUROSCI, V26, P10154, DOI 10.1523/JNEUROSCI.2779-06.2006
   Trommershauser J, 2008, TRENDS COGN SCI, V12, P291, DOI 10.1016/j.tics.2008.04.010
   Vilares I, 2012, CURR BIOL, V22, P1641, DOI 10.1016/j.cub.2012.07.010
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103004
DA 2019-06-15
ER

PT S
AU Acharya, J
   Jafarpour, A
   Orlitsky, A
   Suresh, AT
AF Acharya, Jayadev
   Jafarpour, Ashkan
   Orlitsky, Alon
   Suresh, Ananda Theertha
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Many important distributions are high dimensional, and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning, it approximates mixtures of k spherical Gaussians in d-dimensions to within l(1) distance epsilon using O(dk(9) (log(2) d)/epsilon(4)) samples and O-k,O-epsilon (d(3) log(5) d) computation time. Conversely, we show that any estimator requires Omega dk/epsilon(2) samples, hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor O-k,O-epsilon is exponential in k, but much smaller than previously known.
   We also construct a simple estimator for one-dimensional Gaussian mixtures that uses (O) over tilde (k/epsilon(2)) samples and (O) over tilde((k/epsilon)(3k+1)) computation time.
C1 [Acharya, Jayadev] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Acharya, Jayadev; Jafarpour, Ashkan; Orlitsky, Alon; Suresh, Ananda Theertha] Univ Calif San Diego, San Diego, CA USA.
RP Acharya, J (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM jayadev@mit.edu; ashkan@ucsd.edu; alon@ucsd.edu; asuresh@ucsd.edu
CR Acharya J., 2014, ISIT
   Achlioptas D., 2005, COLT
   Anderson J., 2014, COLT
   Assouad B. Yu., 1997, FESTSCHRIFT L LECAM
   Azizyan M., 2013, NIPS
   Belkin M., 2010, FOCS
   Chan S., 2014, STOC
   Chan S. O., 2013, SODA
   Chaudhuri K., 2009, CORR
   Dasgupta S., 2000, UAI
   Dasgupta Sanjoy, 1999, FOCS
   Daskalakis  C., 2012, SODA
   Daskalakis C., 2014, COLT
   Devroye  Luc, 2001, COMBINATORIAL METHOD
   Dhillon I. S., 2002, ICDM
   Feldman J., 2006, COLT
   Feldman J., 2005, FOCS
   Freund Y., 1999, COLT
   Hsu Daniel, 2013, ITCS
   Kalai A. T., 2010, STOC
   Kearns M. J., 1994, STOC
   Ma J., 2001, NEURAL COMPUTATION, V12
   Moitra A., 2010, FOCS
   Orlitsky A., 2004, UAI
   Paninski L., 2004, NIPS
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   TITTERINGTON DM, 1985, STAT ANAL FINITE MIX
   Valiant G., 2011, STOC
   Vempala S., 2002, FOCS
   VERSHYNIN R., 2010, CORR
   Xing E. P., 2001, ICML
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100030
DA 2019-06-15
ER

PT S
AU Ackerman, M
   Dasgupta, S
AF Ackerman, Margareta
   Dasgupta, Sanjoy
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Incremental Clustering: The Case for Extra Clusters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.
C1 [Ackerman, Margareta] Florida State Univ, 600 W Coll Ave, Tallahassee, FL 32306 USA.
   [Dasgupta, Sanjoy] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Ackerman, M (reprint author), Florida State Univ, 600 W Coll Ave, Tallahassee, FL 32306 USA.
EM mackerman@fsu.edu; dasgupta@eng.ucsd.edu
CR Ackerman M., 2013, P AISTATS 09 JMLR W, V31
   Ackerman M., 2010, COLT
   Ackerman M., 2009, P AISTATS 09 JMLR W, V5, P53
   Ackerman M., 2010, NIPS
   Ackerman Margareta, 2012, P 26 AAAI C ART INT
   Aggarwal Charu C, 2013, SURVEY STREAM CLUSTE
   Balcan M.-F., 2010, COLT, P282
   Balcan MF, 2008, ACM S THEORY COMPUT, P671
   Epter S., 1999, INT C KNOWL DISC DAT, V7
   Guha S, 2000, ANN IEEE SYMP FOUND, P359, DOI 10.1109/SFCS.2000.892124
   HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840
   Jardine N., 1971, MATH TAXONOMY
   Kleinberg J., 2003, ADV NEURAL INFORM PR, P463
   Knuth D. E., 1981, ART COMPUTER PROGRAM, V2
   Kohonen T., 2001, SELF ORGANIZING MAPS
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Zadeh R. B., 2009, P 25 C UNC ART INT, P639
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103059
DA 2019-06-15
ER

PT S
AU Adametz, D
   Roth, V
AF Adametz, David
   Roth, Volker
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Distance-Based Network Recovery under Feature Correlation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID MARGINAL LIKELIHOOD; REPAIR; MODELS
AB We present an inference method for Gaussian graphical models when only pair-wise distances of n objects are observed. Formally, this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances d(MH) (x(i), x(j)), where object x(i) lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby, we generalize TiWnet [19], which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.
C1 [Adametz, David; Roth, Volker] Univ Basel, Dept Math & Comp Sci, Basel, Switzerland.
RP Adametz, D (reprint author), Univ Basel, Dept Math & Comp Sci, Basel, Switzerland.
EM david.adametz@unibas.ch; volker.roth@unibas.ch
CR Allen GI, 2010, ANN APPL STAT, V4, P764, DOI 10.1214/09-AOAS314
   Bhattacharyya A., 1943, Bulletin of the Calcutta Mathematical Society, V35, P99
   Daniels MJ, 2009, J MULTIVARIATE ANAL, V100, P2352, DOI 10.1016/j.jmva.2009.04.015
   de Vos A., 2008, TECHNICAL REPORT
   Ein-Dor L, 2006, P NATL ACAD SCI USA, V103, P5923, DOI 10.1073/pnas.0601231103
   Fortini P, 2003, BIOCHIMIE, V85, P1053, DOI 10.1016/j.biochi.2003.11.003
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Gupta A. K., 1999, PMS SERIES
   HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796
   Iranmanesh A, 2010, IRAN J MATH SCI INFO, V5, P33, DOI 10.7508/ijmsi.2010.02.004
   JEBARA T, 2003, C LEARN THEOR
   KALBFLEI.JD, 1970, J ROY STAT SOC B, V32, P175
   McCullagh P, 2009, STAT SINICA, V19, P631
   McCullagh P, 2008, BERNOULLI, V14, P593, DOI 10.3150/07-BEJ119
   MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129
   Murphy SA, 2000, J AM STAT ASSOC, V95, P449, DOI 10.2307/2669386
   PATTERSON HD, 1971, BIOMETRIKA, V58, P545, DOI 10.1093/biomet/58.3.545
   Peltomaki P, 2001, MUTAT RES-REV MUTAT, V488, P77, DOI 10.1016/S1383-5742(00)00058-2
   Prabhakaran S, 2013, MACH LEARN, V92, P251, DOI 10.1007/s10994-013-5370-7
   Sheffer M, 2009, P NATL ACAD SCI USA, V106, P7131, DOI 10.1073/pnas.0902232106
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102031
DA 2019-06-15
ER

PT S
AU Agarwal, A
   Beygelzimer, A
   Hsu, D
   Langford, J
   Telgarsky, M
AF Agarwal, Alekh
   Beygelzimer, Alma
   Hsu, Daniel
   Langford, John
   Telgarsky, Matus
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Scalable Nonlinear Learning with Adaptive Polynomial Expansions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ONLINE
AB Can we effectively learn a nonlinear representation in time comparable to linear learning ? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.
C1 [Agarwal, Alekh; Langford, John] Microsoft Res, Redmond, WA 98052 USA.
   [Beygelzimer, Alma] Yahoo Labs, Sunnyvale, CA USA.
   [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA.
   [Telgarsky, Matus] Rutgers State Univ, New Brunswick, NJ USA.
   [Telgarsky, Matus] Microsoft Res, New York, NY USA.
RP Agarwal, A (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM alekha@microsoft.com; beygel@yahoo-inc.com; djhsu@cs.columbia.edu;
   jcl@microsoft.com; mtelgars@cs.ucsd.edu
CR Agarwal A., 2014, ARXIV14100440CSLG
   Agarwal A, 2014, J MACH LEARN RES, V15, P1111
   Andoni Alexandr, 2014, SODA
   Blum A., 1999, COLT
   Bordes A, 2005, J MACH LEARN RES, V6, P1579
   Bubeck S., 2014, ARXIV14054980MATHOC
   Dimakis A. G., 2014, CORR
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hall K., 2010, WORKSH LEARN COR CLU
   Hamid Raffay, 2014, ICML
   IVAKHNENKO AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320
   Kalai A. T., 2009, FOCS
   Kar Purushottam, 2012, AISTATS
   Karampatziakis N., 2011, P C UNC ART INT, P392
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   McMahan H. B, 2010, P 23 C LEARN THEOR, P244
   Mukherjee I., 2013, P EUR C MACH LEARN P
   Pham N., 2013, P 19 ACM SIGKDD INT
   Rahimi A., 2008, ADV NEURAL INFORM PR, V20
   Ross S., 2013, UAI
   Sanger T. D., 1992, ADV NEURAL INFORM PR, V4
   Scholkopf B., 2002, LEARNING KERNELS
   Shamir Ohad, 2013, ICML
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Williams C. K. I., 2001, ADV NEURAL INFORM PR, V13
   Zinkevich Martin, 2003, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101068
DA 2019-06-15
ER

PT S
AU Akram, S
   Simon, JZ
   Shamma, S
   Babadi, B
AF Akram, Sahar
   Simon, Jonathan Z.
   Shamma, Shihab
   Babadi, Behtash
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A State-Space Model for Decoding Auditory Attentional Modulation from
   MEG in a Competing-Speaker Environment
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SPEECH
AB Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.
C1 [Akram, Sahar; Simon, Jonathan Z.; Shamma, Shihab; Babadi, Behtash] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Akram, Sahar; Simon, Jonathan Z.; Shamma, Shihab; Babadi, Behtash] Univ Maryland, Inst Syst Res, College Pk, MD 20742 USA.
   [Simon, Jonathan Z.] Univ Maryland, Dept Biol, College Pk, MD 20742 USA.
RP Akram, S (reprint author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM sakram@umd.edu; jzsimon@umd.edu; sas@umd.edu; behtash@umd.edu
RI Simon, Jonathan Z/A-8196-2008
OI Simon, Jonathan Z/0000-0003-0858-0698
CR Ba D, 2014, IEEE T SIGNAL PROCES, V62, P183, DOI 10.1109/TSP.2013.2287685
   Bregman A. S., 1998, COMPUTATIONAL AUDITO, P1
   Bregman A.S., 1994, AUDITORY SCENE ANAL
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   David SV, 2007, NETWORK-COMP NEURAL, V18, P191, DOI 10.1080/09548980701609235
   de Cheveigne A, 2008, J NEUROSCI METH, V171, P331, DOI 10.1016/j.jneumeth.2008.03.015
   de Cheveigne A, 2007, J NEUROSCI METH, V165, P297, DOI 10.1016/j.jneumeth.2007.06.003
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Ding N, 2012, P NATL ACAD SCI USA, V109, P11854, DOI 10.1073/pnas.1205381109
   Ding N, 2012, J NEUROPHYSIOL, V107, P78, DOI 10.1152/jn.00297.2011
   Elhilali M, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000129
   Fisher N. I, 1995, STAT ANAL CIRCULAR D
   Fisher RA, 1914, BIOMETRIKA, V10, P507
   Griffiths TD, 2004, NAT REV NEUROSCI, V5, P887, DOI 10.1038/nrn1538
   Mesgarani N, 2012, NATURE, V485, P233, DOI 10.1038/nature11020
   Shamma SA, 2011, TRENDS NEUROSCI, V34, P114, DOI 10.1016/j.tins.2010.11.002
   Shinn-Cunningham BG, 2008, TRENDS COGN SCI, V12, P182, DOI 10.1016/j.tics.2008.02.003
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
   Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004
   Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622
NR 20
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102084
DA 2019-06-15
ER

PT S
AU Amin, K
   Rostamizadeh, A
   Syed, U
AF Amin, Kareem
   Rostamizadeh, Afshin
   Syed, Umar
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Repeated Contextual Auctions with Strategic Buyers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer's valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear ((O) over tilde (T-2/3)) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.
C1 [Amin, Kareem] Univ Penn, Philadelphia, PA 19104 USA.
   [Rostamizadeh, Afshin; Syed, Umar] Google Res, Mountain View, CA USA.
RP Amin, K (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
EM akareem@cis.upenn.edu; rostami@google.com; usyed@google.com
CR Amin Kareem, 2013, ADV NEURAL INF PROCE, P1169
   Bar-Yossef Z, 2002, SIAM PROC S, P964
   Blum A, 2003, SIAM PROC S, P202
   Cary M, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P262
   Cesa-Bianchi Nicolo, 2013, P S DISCR ALG
   Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008
   Hajiaghayi Mohammad Taghi, 2004, P 5 ACM C EL COMM, P71
   Kitts B., 2004, Electronic Markets, V14, P186, DOI 10.1080/1019678042000245119
   Kitts Brendan, 2005, WORKSH SPONS SEARCH
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   Medina Andres Munoz, 2014, P 31 INT C MACH LEAR, P262
   Mohri M., 2012, FDN MACHINE LEARNING
   Parkes DC, 2007, ALGORITHMIC GAME THEORY, P411
   Rakhlin A., 2012, P 29 INT C MACH LEAR, P449
   Rakhlin A., 2011, ARXIV11095647
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Varian H. R, 2010, INTERMEDIATE MICROEC, V6
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103040
DA 2019-06-15
ER

PT S
AU Andersen, MR
   Winther, O
   Hansen, LK
AF Andersen, Michael Riis
   Winther, Ole
   Hansen, Lars Kai
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Bayesian Inference for Structured Spike and Slab Priors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SELECTION
AB Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.
C1 [Andersen, Michael Riis; Winther, Ole; Hansen, Lars Kai] Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark.
RP Andersen, MR (reprint author), Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark.
EM miri@dtu.dk; olwi@dtu.dk; lkh@dtu.dk
CR Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275
   Baldassarre L., 2012, 2012 2nd International Workshop on Pattern Recognition in NeuroImaging (PRNI), P5, DOI 10.1109/PRNI.2012.31
   Bishop C. M., 2006, PATTERN RECOGNITION
   Cevher V., 2008, NIPS
   Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172
   Efron B, 2004, ANN STAT, V32, P407
   Hernandez-Lobato D, 2013, J MACH LEARN RES, V14, P1891
   Jenatton R, 2010, P 13 INT C ART INT S, P366
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129
   Obozinski G., 2009, ACM INT C P SERIES, V382
   Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881
   Petersen K. B., 2012, MATRIX COOKBOOK
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schniter P., 2012, 2012 46 ANN C INF SC
   Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   van Gerven Marcel, 2009, ADV NEURAL INFORM PR, P1901
   Wipf DP, 2007, IEEE T SIGNAL PROCES, V55, P3704, DOI 10.1109/TSP.2007.894265
   Yu L, 2012, SIGNAL PROCESS, V92, P259, DOI 10.1016/j.sigpro.2011.07.015
   Ziniel J, 2013, IEEE T SIGNAL PROCES, V61, P5270, DOI 10.1109/TSP.2013.2273196
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102025
DA 2019-06-15
ER

PT S
AU Arpit, D
   Nwogu, I
   Govindaraju, V
AF Arpit, Devansh
   Nwogu, Ifeoma
   Govindaraju, Venu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Dimensionality Reduction with Subspace Structure Preservation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FACE RECOGNITION; SEGMENTATION; MODELS
AB Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that 2K projection vectors are sufficient for the independence preservation of any K class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving state-of-the-art results compared to popular dimensionality reduction techniques.
C1 [Arpit, Devansh; Nwogu, Ifeoma; Govindaraju, Venu] SUNY Buffalo, Dept Comp Sci, Buffalo, NY 14260 USA.
RP Arpit, D (reprint author), SUNY Buffalo, Dept Comp Sci, Buffalo, NY 14260 USA.
EM devansha@buffalo.edu; inwogu@buffalo.edu; govind@buffalo.edu
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Cai D, 2007, IEEE DATA MINING, P427, DOI 10.1109/ICDM.2007.88
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   He X., 2004, P NIPS ADV NEUR INF, P103
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   Ho J, 2003, PROC CVPR IEEE, P11
   Hong W, 2006, IEEE T IMAGE PROCESS, V15, P3655, DOI 10.1109/TIP.2006.882016
   Liu G., 2010, ICML
   Ma Yi, 2007, IEEE T PATTERN ANAL, V3
   Martinez A, 1998, AR FACE DATABASE
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   Vidal R, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P167
   Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100089
DA 2019-06-15
ER

PT S
AU Aslam, O
   Zhang, XH
   Schuurmans, D
AF Aslam, Ozlem
   Zhang, Xinhua
   Schuurmans, Dale
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Convex Deep Learning via Normalized Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.
C1 [Aslam, Ozlem; Schuurmans, Dale] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
   [Zhang, Xinhua] NICTA, Machine Learning Grp, Sydney, NSW, Australia.
   [Zhang, Xinhua] ANU, Sydney, NSW, Australia.
RP Aslam, O (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM ozlem@cs.ualberta.ca; xizhang@nicta.com.au; dale@cs.ualberta.ca
CR Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Arora Sanjeev, 2014, ICML
   Aslan O., 2013, NIPS
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cheng H., 2013, UAI
   Cho Y., 2010, NEURAL COMPUT, V22
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dinuzzo F., 2011, ICML
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gens R., 2012, NIPS, V25
   GORI M, 1992, IEEE T PATTERN ANAL, V14, P76, DOI 10.1109/34.107014
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   HAJNAL A, 1993, J COMPUT SYST SCI, V46, P129, DOI 10.1016/0022-0000(93)90001-D
   Hinton G., 2006, NEUR COMP, V18
   Hinton G. E, 2012, ARXIV12070580
   Hoeffgen K., 1995, JCSS, V52, P114
   Jaggi  M., 2013, ICML
   Joachims T., 1999, ICML
   Joulin A., 2012, P ICML
   Joulin A., 2012, P CVPR
   KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3
   Krizhevsky A., 2012, NIPS
   Le Q. V., 2012, P ICML
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Livni R., 2014, ARXIV13047045V2
   Nair V., 2010, ICML
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   OVERTON ML, 1993, MATH PROGRAM, V62, P321, DOI 10.1007/BF01585173
   Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983
   Razborov A., 1992, ALGORITHM THEORY SWA
   Rifkin RM, 2007, J MACH LEARN RES, V8, P441
   Schoelkopf B., 2002, LEARNING KERNELS
   Sindhwani V., 2006, SIGIR
   Socher R., 2011, ICML
   Sutskever  I., 2013, ICML
   Tesauro G., 1995, CACM, V38
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Zhuang J., 2011, AISTATS
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102057
DA 2019-06-15
ER

PT S
AU Awasthi, P
   Blum, A
   Sheffet, O
   Vijayaraghavan, A
AF Awasthi, Pranjal
   Blum, Avrim
   Sheffet, Or
   Vijayaraghavan, Aravindan
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Mixtures of Ranking Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved.
C1 [Awasthi, Pranjal] Princeton Univ, Princeton, NJ 08544 USA.
   [Blum, Avrim] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Sheffet, Or] Harvard Univ, Cambridge, MA 02138 USA.
   [Vijayaraghavan, Aravindan] NYU, New York, NY 10003 USA.
RP Awasthi, P (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM pawashti@cs.princeton.edu; avrim@cs.cmu.edu; osheffet@seas.harvard.edu;
   vijayara@cims.nyu.edu
FU NSF [CCF-1101215, CCF-1116892]; Simons Institute; Simons Foundation
FX This work was supported in part by NSF grants CCF-1101215, CCF-1116892,
   the Simons Institute, and a Simons Foundation Postdoctoral fellowhsip.
   Part of this work was performed while the 3rd author was at the Simons
   Institute for the Theory of Computing at the University of California,
   Berkeley and the 4th author was at CMU.
CR Achlioptas D., 2005, COLT
   Anandkumar A., 2012, COLT
   Anandkumar Anima, 2012, CORR
   Arora Sanjeev, 2001, STOC
   Bhaskara A., 2013, CORR
   Bhaskara A., 2014, S THEOR COMP STOC
   BRAVERMAN M., 2009, CORR
   Busse L., 2007, ICML
   Dasgupta Sanjoy, 1999, FOCS
   Diaconis P, 1988, GROUP REPRESENTATION
   Goyal N., 2014, S THEOR COMP STOC
   Hsu Daniel, 2013, ITCS
   Kalai A. T., 2010, STOC
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.1093/biomet/30.1-2.81
   Lebanon Guy, 2002, ICML
   Lu Tyler, 2011, ICML
   MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244
   Mandhani Bhushan, 2009, J MACHINE LEARNING R, V5
   Marden J.I., 1995, ANAL MODELING RANK D
   Meila Marina, 2010, UAI
   Meila Marina, 2007, TECHNICAL REPORT
   Moitra A., 2010, FDN COMP SCI FOCS 20
   Murphy Thomas Brendan, 2003, COMPUTATIONAL STAT D, V41
   Oren Joel, 2013, JCAI
   Stanley R. P., 2002, CAMBRIDGE STUDIES AD
   Vempala Santosh, 2004, J COMPUT SYST SCI, V68
   Young H Peyton, 1988, AM POLITICAL SCI REV
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101108
DA 2019-06-15
ER

PT S
AU Bachman, P
   Alsharif, O
   Precup, D
AF Bachman, Philip
   Alsharif, Ouais
   Precup, Doina
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning with Pseudo-Ensembles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.
C1 [Bachman, Philip; Alsharif, Ouais; Precup, Doina] McGill Univ, Montreal, PQ, Canada.
RP Bachman, P (reprint author), McGill Univ, Montreal, PQ, Canada.
EM phil.bachman@gmail.com; ouais.alsharif@gmail.com; dprecup@cs.mcgill.ca
CR Baldi P., 2013, NIPS
   Bengio Y., 2014, ARXIV13061091V5CSLG
   Bergstra J., 2010, PYTH SCI COMP C SCIP
   Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510
   Goodfellow I., 2012, ICML
   Grandvalet Y., 2006, SEMISUPERVISED LEARN
   Hastie T., 2008, ELEMENTS STAT LEARNI
   Hinton G. E., 2012, ARXIV12070580V1CSNE
   Kalchbrenner N., 2014, ACL
   Krizhevsky A., 2009, THESIS
   Le Q. V., 2011, NIPS
   Le Q. V., 2014, ICML
   Lee D.-H., 2013, ICML
   Pacanu R., 2013, ICML
   Rifai  S., 2011, NIPS
   Rippel O., 2014, ICML
   Shapiro A, 2009, LECT STOCHASTIC PROG
   Socher R., 2013, EMNLP
   Van der Maaten L., 2013, ICML
   Vincent P., 2008, ICML
   Wager  S., 2013, NIPS
   Warde-Farley D., 2014, ICLR
   Weston J., 2008, ICML
   Xu H., 2009, JMLR, V10
   Xu  Huan, 2009, NIPS
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102048
DA 2019-06-15
ER

PT S
AU Bahadori, MT
   Yu, Q
   Liu, Y
AF Bahadori, Mohammad Taha
   Yu, Qi (Rose)
   Liu, Yan
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.
C1 [Bahadori, Mohammad Taha] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
   [Yu, Qi (Rose); Liu, Yan] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
RP Bahadori, MT (reprint author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
EM mohammab@usc.edu; qiyu@usc.edu; yanliu.cs@usc.edu
FU NSF [IIS-1134990, IIS-1254206]; Okawa Foundation Research Award
FX We thank the anonymous reviewers for their helpful feedback and
   comments. The research was sponsored by the NSF research grants
   IIS-1134990, IIS-1254206 and Okawa Foundation Research Award. The views
   and conclusions are those of the authors and should not be interpreted
   as representing the official policies of the funding agency, or the U.S.
   Government.
CR ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580
   Barron A., 2008, ANN STAT
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Bonilla E. V., 2007, NIPS
   Chen J., 2011, MALSAR MULTITASK LEA
   Chu W., 2006, NIPS
   Cressie N., 1999, JASA
   Cressie N., 2010, J COMP GRAPH STAT
   Cressie NAC, 2011, STAT SPATIO TEMPORAL
   Cressie N, 2008, J ROY STAT SOC B, V70, P209, DOI 10.1111/j.1467-9868.2007.00633.x
   Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Gu M., 2000, 5 GEN HERMITIAN EIGE
   Isaaks E., 2011, APPL GEOSTATISTICS
   Jalali A., 2010, NIPS
   Kolda T. G., 2009, SIAM REV
   Li W.-J., 2009, IJCAI
   Long X., 2012, UBICOMP
   Lozano A. C., 2009, KDD
   Nie F., 2010, NIPS
   Romera-paredes B., 2013, ICML
   Shalev-Shwartz S., 2010, SIAM J OPTIMIZATION
   Shalev-Shwartz Shai, 2011, ICML
   Tomioka R., 2013, NIPS
   Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690
   Zhou D., 2003, NIPS
   Zhou H., 2013, JASA
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101100
DA 2019-06-15
ER

PT S
AU Balcan, MF
   Kanchanapally, V
   Liang, YY
   Woodruff, D
AF Balcan, Maria-Florina
   Kanchanapally, Vandana
   Liang, Yingyu
   Woodruff, David
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Improved Distributed Principal Component Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as k-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for k-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.
C1 [Balcan, Maria-Florina] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Kanchanapally, Vandana] Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA.
   [Liang, Yingyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
   [Woodruff, David] IBM Res, Almaden Res Ctr, Yorktown Hts, NY USA.
RP Balcan, MF (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM ninamf@cs.cmu.edu; vvandana@gatech.edu; yingyu1@cs.princeton.edu;
   dpwoodru@us.ibm.com
FU NSF [CCF-0953192, CCF-1451177, CCF-1101283, CCF-1422910]; ONR
   [N00014-09-1-0751]; AFOSR [FA9550-09-1-0538]; XDATA program of the
   Defense Advanced Research Projects Agency (DARPA) [FA8750-12-C0323]
FX This work was supported in part by NSF grants CCF-0953192, CCF-1451177,
   CCF-1101283, and CCF-1422910, ONR grant N00014-09-1-0751, and AFOSR
   grant FA9550-09-1-0538. David Woodruff would like to acknowledge the
   XDATA program of the Defense Advanced Research Projects Agency (DARPA),
   administered through Air Force Research Laboratory contract
   FA8750-12-C0323, for supporting this work.
CR BACHE K., 2013, UCI MACHINE LEARNING
   Balcan M. - F., 2013, ADV NEURAL INFORM PR
   Blei D. M., 2003, J MACHINE LEARNING R
   Boutsidis C., 2011, CORR
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR
   Cohen M., 2014, ARXIV14106801
   Corbett J. C., 2012, P USENIX S OP SYST D
   Feldman D., 2011, P ANN ACM S THEOR CO
   Feldman D., 2013, P ANN ACM SIAM S DIS
   Ghashami M., 2014, ACM SIAM S DISCR ALG
   Halko  N., 2011, SIAM REV
   Kannan R., 2014, P C LEARN THEOR
   Karampatziakis N., 2013, CORR
   Le Borgne Y. - A., 2008, SENSORS
   Lee Daniel D, 2001, ADV NEURAL INFORM PR
   Macua S. V., 2010, P IEEE INT WORKSH SI
   Meng X., 2013, P ANN ACM S S THEOR
   Mitra S, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961662
   Nelson J., 2013, IEEE ANN S FDN COMP
   Olston C., 2003, P ACM SIGMOD INT C M
   Qu Y., 2002, P IEEE INT C DAT MIN
   Sarlos T., 2006, IEEE S FDN COMP SCI
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103070
DA 2019-06-15
ER

PT S
AU Balcan, MF
   Berlind, C
   Blum, A
   Cohen, E
   Patnaik, K
   Song, L
AF Balcan, Maria-Florina
   Berlind, Christopher
   Blum, Avrim
   Cohen, Emma
   Patnaik, Kaushik
   Song, Le
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Active Learning and Best-Response Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.
C1 [Balcan, Maria-Florina; Blum, Avrim] Carnegie Mellon, Pittsburgh, PA 15213 USA.
   [Berlind, Christopher; Cohen, Emma; Patnaik, Kaushik; Song, Le] Georgia Tech, Atlanta, GA USA.
RP Balcan, MF (reprint author), Carnegie Mellon, Pittsburgh, PA 15213 USA.
EM ninamf@cs.cmu.edu; cberlind@gatech.edu; avrim@cs.cmu.edu;
   ecohen@gatech.edu; kpatnaik3@gatech.edu; lsong@cc.gatech.edu
FU NSF [CCF-0953192, CCF-1101283, CCF-1116892, IIS-1065251, IIS1116886];
   AFOSR [FA9550-09-1-0538]; ONR [N00014-09-1-0751]; Raytheon Faculty
   Fellowship; NSF CAREER [IIS1350983]; NSF/NIH BIGDATA [1R01GM108341]
FX This work was supported in part by NSF grants CCF-0953192, CCF-1101283,
   CCF-1116892, IIS-1065251, IIS1116886, NSF/NIH BIGDATA 1R01GM108341, NSF
   CAREER IIS1350983, AFOSR grant FA9550-09-1-0538, ONR grant
   N00014-09-1-0751, and Raytheon Faculty Fellowship.
CR Awasthi P., 2014, STOC
   Balcan M.-F., 2009, EC
   Balcan M. F., 2013, NIPS
   Balcan M.-F., 2014, SICOMP
   Beygelzimer A., 2009, ICML
   BLUME LE, 1993, GAME ECON BEHAV, V5, P387, DOI 10.1006/game.1993.1023
   Boucheron S., 2013, CONCENTRATION INEQUA
   Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493
   Golovin Daniel, 2010, NIPS
   Hanneke S., 2013, FDN TRENDS MACHINE L, P1
   Hanneke S., 2013, COMMUNICATION
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Morris S, 2000, REV ECON STUD, V67, P57, DOI 10.1111/1467-937X.00121
   Settles B., 2012, SYNTHESIS LECT ARTIF
   Yang L, 2013, THESIS
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102035
DA 2019-06-15
ER

PT S
AU Banerjee, A
   Chen, S
   Fazayeli, F
   Sivakumar, V
AF Banerjee, Arindam
   Chen, Sheng
   Fazayeli, Farideh
   Sivakumar, Vidyashankar
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Estimation with Norm Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID REGRESSION; SELECTION; RECOVERY; LASSO
AB Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects. We characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for a variety of noise models, design matrices, including sub-Gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models. Gaussian width, a geometric measure of size of sets, and associated tools play a key role in our generalized analysis.
C1 [Banerjee, Arindam; Chen, Sheng; Fazayeli, Farideh; Sivakumar, Vidyashankar] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Banerjee, A (reprint author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM banerjee@cs.umnedu; shengc@cs.umnedu; farideh@cs.umnedu;
   sivakuma@cs.umnedu
FU NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711]; NASA [NNX12AQ39A]
FX We thank the anonymous reviewers for helpful comments and suggestions on
   related work. We thank Sergey Bobkov, Snigdhansu Chatterjee, and Pradeep
   Ravikumar for discussions related to the paper. The research was
   supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986,
   CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.
CR Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   GORDON Y, 1988, LECT NOTES MATH, V1317, P84
   Ledoux M., 2013, PROBABILITY BANACH S
   Ledoux M., MATH SURVEYS MONOGRA
   Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Oymak S., 2013, ARXIV13110830V2
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201
   Talagrand M., 2005, THE GENERIC CHAINING
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp J. A., 2014, SAMPLING THEORY RENA
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Vizcarra AB, 2008, PROG PROBAB, V59, P363
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102026
DA 2019-06-15
ER

PT S
AU Bansal, T
   Bhattacharyya, C
   Kannan, R
AF Bansal, Trapit
   Bhattacharyya, C.
   Kannan, Ravindran
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A provable SVD-based algorithm for learning topics in dominant admixture
   corpus
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures, is NP-hard. Making a strong assumption called separability, [4] gave the first provable algorithm for inference. For the widely used LDA model, [6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn topic vectors with bounded l(1) error (a natural measure for probability vectors).
   Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded l1 error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on w(0), the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].
C1 [Bansal, Trapit; Bhattacharyya, C.] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.
   [Kannan, Ravindran] Microsoft Res, Madras, Tamil Nadu, India.
RP Bansal, T (reprint author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.
EM trapitbansal@gmail.com; chiru@csa.iisc.ernet.in; kannan@microsoft.com
FU Department of Science and Technology (DST) grant
FX TB was supported by a Department of Science and Technology (DST) grant.
CR Anandkumar A., 2012, NEURAL INFORM PROCES
   Arora S., 2013, INT C MACH LEARN
   Arora S., 2012, FDN COMPUTER SCI
   Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Awashti P., 2012, P APPR RAND
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Kumar A., 2010, FDN COMPUTER SCI
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mimno D., 2011, P C EMP METH NAT LAN, P262
   Papadimitriou CH, 2000, J COMPUT SYST SCI, V61, P217, DOI 10.1006/jcss.2000.1711
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100081
DA 2019-06-15
ER

PT S
AU Bartz, D
   Muller, KR
AF Bartz, Daniel
   Mueller, Klaus-Robert
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Covariance shrinkage for autocorrelated data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.
C1 [Bartz, Daniel] TU Berlin, Dept Comp Sci, Berlin, Germany.
   [Mueller, Klaus-Robert] TU Berlin, Berlin, Germany.
   [Mueller, Klaus-Robert] Korea Univ, Seoul, South Korea.
RP Bartz, D (reprint author), TU Berlin, Dept Comp Sci, Berlin, Germany.
EM daniel.bartz@tu-berlin.de; klaus-robert.mueller@tu-berlin.de
FU National Research Foundation - Korean government [2012-005741]
FX This research was also supported by the National Research Foundation
   grant (No. 2012-005741) funded by the Korean government. We thank
   Johannes Hohne, Sebastian Bach and Duncan Blythe for valuable
   discussions and comments.
CR ANDREWS DWK, 1991, ECONOMETRICA, V59, P817, DOI 10.2307/2938229
   Bai Z, 2010, SPECTRAL ANAL LARGE
   Bartz D., 2013, ADV NEURAL INFORM PR, V26, P1869
   Blankertz B., 2007, ADV NEURAL INFORM PR, P113
   Blankertz B, 2008, IEEE SIGNAL PROC MAG, V25, P41, DOI [10.1109/MSP.2008.4408441, 10.1109/MSP.200790.900,9]
   Blankertz B, 2011, NEUROIMAGE, V56, P814, DOI 10.1016/j.neuroimage.2010.06.048
   Blankertz B, 2010, NEUROIMAGE, V51, P1303, DOI 10.1016/j.neuroimage.2010.03.022
   Chen YL, 2010, IEEE T SIGNAL PROCES, V58, P5016, DOI 10.1109/TSP.2010.2053029
   Edelman A, 2005, ACT NUMERIC, V14, P233, DOI 10.1017/S0962492904000236
   Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027
   Hastie T., 2008, ELEMENTS STAT LEARNI
   Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4
   Ledoit O, 2012, ANN STAT, V40, P1024, DOI 10.1214/12-AOS989
   Lotte F, 2011, IEEE T BIO-MED ENG, V58, P355, DOI 10.1109/TBME.2010.2082539
   Marchenko V. A., 1967, MATH USSR SB, V72, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]
   Samek W., 2013, ADV NEURAL INFORM PR, P1007
   Sancetta A, 2008, J MULTIVARIATE ANAL, V99, P949, DOI 10.1016/j.jmva.2007.06.004
   Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175
   Stein C., 1956, P 3 BERK S MATH STAT, V1, P197
   THIEBAUX HJ, 1984, J CLIM APPL METEOROL, V23, P800, DOI 10.1175/1520-0450(1984)023<0800:TIAEOE>2.0.CO;2
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101070
DA 2019-06-15
ER

PT S
AU Bateni, M
   Bhaskara, A
   Lattanzi, S
   Mirrokni, V
AF Bateni, MohammadHossein
   Bhaskara, Aditya
   Lattanzi, Silvio
   Mirrokni, Vahab
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Distributed Balanced Clustering via Mapping Coresets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the "balanced clustering" problem. Although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem. In this paper we develop a new framework based on "mapping coresets" to tackle this issue. Our technique results in first distributed approximation algorithms for balanced clustering problems for a wide range of clustering objective functions such as k-center, k-median, and k-means.
C1 [Bateni, MohammadHossein; Bhaskara, Aditya; Lattanzi, Silvio; Mirrokni, Vahab] Google NYC, New York, NY 10011 USA.
RP Bateni, M (reprint author), Google NYC, New York, NY 10011 USA.
EM bateni@google.com; bhaskaraaditya@google.com; silviol@google.com;
   mirrokni@google.com
CR Agarwal P. K., 2012, P 31 S PRINC DAT SYS, P23, DOI DOI 10.1145/2213556.2213562
   Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736
   AN H.-C., 2013, ABS13042983 CORR
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915
   Balcan M. F., 2013, NIPS
   BARILAN J, 1993, J ALGORITHM, V15, P385, DOI 10.1006/jagm.1993.1047
   Charikar M., 2003, P 35 ANN ACM S THEOR, P30, DOI DOI 10.1145/780542.780548
   Charikar M., 1997, STOC, P626, DOI DOI 10.1145/258533.258657
   Chuzhoy J, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P952
   Cygan M, 2012, ANN IEEE SYMP FOUND, P273, DOI 10.1109/FOCS.2012.63
   Ene A., 2011, P 17 ACM SIGKDD INT, P681, DOI DOI 10.1145/2020408.2020515
   GUHA S., 2001, STOC
   Gupta A., 2008, CORR
   INDYK P., 2014, COMPOSABLE COR UNPUB
   Karloff H, 2010, PROC APPL MATH, V135, P938
   Khuller S, 2000, SIAM J DISCRETE MATH, V13, P403, DOI 10.1137/S0895480197329776
   Korupolu M., 1998, SODA 98, P1
   Lattanzi S, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P85
   LIN JH, 1992, INFORM PROCESS LETT, V44, P245, DOI 10.1109/ICASSP.1992.226074
   Rahimian F, 2013, INT CONF SELF SELF, P51, DOI 10.1109/SASO.2013.13
   Ugander J., 2013, P 6 ACM INT C WEB SE, P507
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100090
DA 2019-06-15
ER

PT S
AU Benson, AR
   Lee, JD
   Rajwa, B
   Gleich, DF
AF Benson, Austin R.
   Lee, Jason D.
   Rajwa, Bartek
   Gleich, David F.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Scalable Methods for Nonnegative Matrix Factorizations of Near-separable
   Tall-and-skinny Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ALGORITHM
AB Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called "tall-and-skinny matrices." One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.
C1 [Benson, Austin R.; Lee, Jason D.] Stanford Univ, ICME, Stanford, CA 94305 USA.
   [Rajwa, Bartek] Purdue Univ, Bindley Biosci Ctr, W Lafayette, IN 47907 USA.
   [Gleich, David F.] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
RP Benson, AR (reprint author), Stanford Univ, ICME, Stanford, CA 94305 USA.
EM arbenson@stanford.edu; jd117@stanford.edu; brajwa@purdue.edu;
   dgleich@purdue.edu
FU Office of Technology Licensing Stanford Graduate Fellowship; NSF
   Graduate Research Fellowship; NSF CAREER award [CCF-1149756]; NIH
   [1R21EB015707-01]
FX ARB and JDL are supported by an Office of Technology Licensing Stanford
   Graduate Fellowship. JDL is also supported by a NSF Graduate Research
   Fellowship. DFG is supported by NSF CAREER award CCF-1149756. BR is
   supported by NIH grant 1R21EB015707-01.
CR Anderson M., 2011, Proceedings of the 25th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2011), P48, DOI 10.1109/IPDPS.2011.15
   Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8
   Arora S., 2012, P 44 ANN ACM S THEOR, P145
   Benson AR, 2013, IEEE INT CONF BIG DA
   Bioucas-Dias JM, 2011, INT GEOSCI REMOTE SE, P1135, DOI 10.1109/IGARSS.2011.6049397
   Boardman J., 1993, P ANN JPL AIRB GEOSC, V1, P11
   CHAN TF, 1982, ACM T MATH SOFTWARE, V8, P72, DOI 10.1145/355984.355990
   Chu MT, 2008, SIAM J SCI COMPUT, V30, P1131, DOI 10.1137/070680436
   Cichocki A, 2007, LECT NOTES COMPUT SC, V4493, P793
   Constantine P. G., 2014, SIAM J SCI COMPUT
   Constantine Paul G, 2011, P 2 INT WORKSH MAPRE, P43
   Damle A., 2014, ARXIV14054275
   Demmel J., 2012, SIAM J SCI COMP, V34
   Dongmin Kim, 2008, Statistical Analysis and Data Mining, V1, P38, DOI 10.1002/sam.104
   Donoho D., 2003, NIPS
   Gillis N., 2013, SYSTEMS MAN CYBERN A, VPP, P1
   Guillamet D, 2002, LECT NOTES ARTIF INT, V2504, P336
   Jia S, 2009, IEEE T GEOSCI REMOTE, V47, P161, DOI 10.1109/TGRS.2008.2002882
   Kim W, 2011, EXPERT SYST APPL, V38, P13198, DOI 10.1016/j.eswa.2011.04.133
   Kuang D, 2012, SDM, V12, P106, DOI DOI 10.1137/1.9781611972825.10
   Kumar A., 2013, ICML
   Lee D. D., 2000, NIPS
   Liu C., 2010, P 19 INT C WORLD WID, P681
   Recht B., 2012, ADV NEURAL INFORM PR, P1223
   Thurau C., 2010, P 19 ACM INT C INF K, P1785, DOI [DOI 10.1145/1871437.1871729, 10.1145/1871437.1871729]
   Vavasis SA, 2009, SIAM J OPTIMIZ, V20, P1364, DOI 10.1137/070709967
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101006
DA 2019-06-15
ER

PT S
AU Berend, D
   Kontorovich, A
AF Berend, Daniel
   Kontorovich, Aryeh
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Consistency of weighted majority votes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID THEOREM
AB We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed.
C1 [Berend, Daniel; Kontorovich, Aryeh] Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel.
   [Berend, Daniel] Ben Gurion Univ Negev, Math Dept, Beer Sheva, Israel.
RP Berend, D (reprint author), Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel.
EM berend@cs.bgu.ac.il; karyeh@cs.bgu.ac.il
CR Audibert J. - Y., 2007, ALT
   Baharad E, 2012, THEOR DECIS, V72, P113, DOI 10.1007/s11238-010-9240-5
   Baharad E, 2011, AUTON AGENT MULTI-AG, V22, P31, DOI 10.1007/s10458-009-9120-y
   Berend D, 1998, SOC CHOICE WELFARE, V15, P481, DOI 10.1007/s003550050118
   Berend D., 2014, PREPRINT
   Berend D, 2007, SOC CHOICE WELFARE, V28, P507, DOI 10.1007/s00355-006-0179-y
   Berend D, 2013, STAT PROBABIL LETT, V83, P1254, DOI 10.1016/j.spl.2013.01.023
   Berend D, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2359
   BOLAND PJ, 1989, J APPL PROBAB, V26, P81, DOI 10.2307/3214318
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Condorcet J. A. N. de Caritat marquis de, 1785, AMS CHELSEA PUBLISHI
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Eban E., 2014, ICML
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gao C., 2014, ARXIV13105764
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Helmbold DP, 2012, J MACH LEARN RES, V13, P2145
   Kearns Michael J., 1998, UAI
   Kontorovich A, 2012, MARKOV PROCESS RELAT, V18, P613
   Lacasse A., 2006, NIPS
   Laviolette F, 2007, J MACH LEARN RES, V8, P1461
   Li H., 2013, ABS13072674 CORR
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Littlestone N., 1989, FOCS
   Mansour Y., 2013, PREPRINT
   Maurer Andreas, 2009, COLT
   Mnih V., 2008, ICML
   Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009
   NITZAN S, 1982, INT ECON REV, V23, P289, DOI 10.2307/2526438
   Ortiz Luis E., 2003, J MACHINE LEARNING R, V4, P895
   Parisi F., 2014, P NAT ACAD SCI
   Raginsky M, 2013, FOUND TRENDS COMMUN, V10, P1, DOI 10.1561/0100000064
   Roy J. - F., 2011, ICML
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100032
DA 2019-06-15
ER

PT S
AU Berg-Kirkpatrick, T
   Andreas, J
   Klein, D
AF Berg-Kirkpatrick, Taylor
   Andreas, Jacob
   Klein, Dan
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Unsupervised Transcription of Piano Music
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording-specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F-1 on real piano audio.
C1 [Berg-Kirkpatrick, Taylor; Andreas, Jacob; Klein, Dan] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
RP Berg-Kirkpatrick, T (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
EM tberg@cs.berkeley.edu; jda@cs.berkeley.edu; klein@cs.berkeley.edu
CR [Anonymous], 2014, INT MUSIC SCORE LIB
   Benetos Emmanouil, 2013, EXPLICIT DURATION HI
   Besag Julian, 1986, J ROYAL STAT SOC
   Bock Sebastian, 2012, IEEE INT C AC SPEECH
   Emiya Valentin, 2010, IEEE T AUDIO SPEECH
   Kivinen Jyrki, 1997, INFORM COMPUTATION
   Levinson S. E., 1986, COMPUTER SPEECH LANG
   Lienhart R., 2009, MULTIMEDIA EXP ICME
   Nakano Masahiro, 2012, IEEE INT C AC SPEECH
   O'Hanlon K, 2014, INT CONF ACOUST SPEE
   Peeling Paul H., 2010, IEEE T AUDIO SPEECH
   Poliner GE, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/48317
   Ryynanen Matti P., 2005, IEEE WORKSH APPL SIG
   Vincent Emmanuel, 2010, IEEE T AUDIO SPEECH
   Viro Vladimir, 2011, PEACHNOTE MUSIC SCOR
   Weninger Felix, 2013, IEEE INT C AC SPEECH
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101103
DA 2019-06-15
ER

PT S
AU Bishopl, WE
   Byron, MY
AF Bishopl, William E.
   Byron, M. Yu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Deterministic Symmetric Positive Semidefinite Matrix Completion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We consider the problem of recovering a symmetric, positive semidefinite (SPSD) matrix from a subset of its entries, possibly corrupted by noise. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix. We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met. The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. Finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.
C1 [Bishopl, William E.] Carnegie Mellon Univ, Machine Learning, Pittsburgh, PA 15213 USA.
   [Bishopl, William E.; Byron, M. Yu] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
   [Byron, M. Yu] Carnegie Mellon Univ, Biomed Engn, Pittsburgh, PA 15213 USA.
   [Byron, M. Yu] Carnegie Mellon Univ, Elect & Comp Engn, Pittsburgh, PA 15213 USA.
RP Bishopl, WE (reprint author), Carnegie Mellon Univ, Machine Learning, Pittsburgh, PA 15213 USA.
EM wbishop@cmu.edu; byronyu@cmu.edu
FU NDSEG fellowship; NIH [R90 DA023426-06, T90 DA022762]; Craig H. Nielsen
   Foundation
FX This work was supported by an NDSEG fellowship, NIH grant T90 DA022762,
   NIH grant R90 DA023426-06 and by the Craig H. Nielsen Foundation. We
   thank Martin Azizyan, Geoff Gordon, Akshay Krishnamurthy and Aarti Singh
   for their helpful discussions and Rob Kass for his guidance.
CR Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen Jie, 2013, ARXIV13055826
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Heiman Eyal, 2013, RANDOM STRUCTURES AL
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Keshri S., 2013, ARXIV13093724
   Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR, P836
   Kumar S., 2009, P INT C ART INT STAT, P304
   Laurent M, 2014, LINEAR ALGEBRA APPL, V452, P292, DOI 10.1016/j.laa.2014.03.015
   Laurent M, 2014, MATH PROGRAM, V145, P291, DOI 10.1007/s10107-013-0648-x
   Laurent Monique, 2009, ENCY OPTIMIZATION, P1967
   Lee JA, 2007, INFORM SCI STAT, P1
   Lee Troy, 2013, ADV NEURAL INFORM PR, P1781
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Scholkopf B, 2002, LEARNING KERNELS SUP
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Turaga S., 2013, ADV NEURAL INFORM PR, V26, P539
   Williams C., 2001, ADV NEURAL INFORM PR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102028
DA 2019-06-15
ER

PT S
AU Blum, A
   Haghtalab, N
   Procaccia, AD
AF Blum, Avrim
   Haghtalab, Nika
   Procaccia, Ariel D.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Optimal Commitment to Overcome Insecurity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Game-theoretic algorithms for physical security have made an impressive real-world impact. These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender's strategy and best-responds. In order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies. We design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities. In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.
C1 [Blum, Avrim; Haghtalab, Nika; Procaccia, Ariel D.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Blum, A (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM avrim@cs.cmu.edu; nika@cmu.edu; arielpro@cs.cmu.edu
FU National Science Foundation [CCF-1116892, CCF-1101215, CCF-1215883,
   IIS-1350598]
FX This material is based upon work supported by the National Science
   Foundation under grants CCF-1116892, CCF-1101215, CCF-1215883, and
   IIS-1350598.
CR Conitzer V., 2006, P 7 ACM C EL COMM EC, P82
   Fukuda K., 1996, Combinatorics and Computer Science. 8th Franco-Japanese and 4th Franco-Chinese Conference. Selected Papers, P91
   GACS P, 1981, MATH PROGRAM STUD, V14, P61
   Grotschel M., 1993, GEOMETRIC ALGORITHMS
   Kalai AT, 2006, MATH OPER RES, V31, P253, DOI 10.1287/moor.1060.0194
   Kiekintveld C., 2011, 10 INT C AUT AG MULT, V3, P1005
   Korzhyk D, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P805
   Korzhyk D, 2011, J ARTIF INTELL RES, V41, P297, DOI 10.1613/jair.3269
   Letchford J, 2009, LECT NOTES COMPUT SC, V5814, P250
   Marecki J., 2012, AAMAS, P821
   Motzkin T. S., 1953, CONTRIBUTIONS THEORY, VII, P51
   Paruchuri P., 2008, P 7 INT C AUT AG MUL, V2, P895
   Pita J, 2010, ARTIF INTELL, V174, P1142, DOI 10.1016/j.artint.2010.07.002
   Tambe M, 2012, SECURITY GAME THEORY
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103037
DA 2019-06-15
ER

PT S
AU Boix, X
   Roig, G
   Diether, S
   Van Gool, L
AF Boix, Xavier
   Roig, Gemma
   Diether, Salomon
   Van Gool, Luc
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Self-Adaptable Templates for Feature Coding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P) [1, 2, 3, 4, 5]. In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.
C1 [Boix, Xavier; Roig, Gemma; Diether, Salomon; Van Gool, Luc] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Boix, Xavier; Roig, Gemma] MIT, LCSL, Cambridge, MA 02139 USA.
   [Boix, Xavier; Roig, Gemma] Ist Italiano Tecnol, Genoa, Italy.
RP Boix, X (reprint author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM xboix@mit.edu; gemmar@mit.edu; sdiether@vision.ee.ethz.ch;
   vangool@vision.ee.ethz.ch
FU ERC; AdG VarCity
FX We thank the ERC for support from AdG VarCity.
CR Arsigny V., 2007, J MATRIX ANAL APPL
   Bergstra James, 2013, ICML
   Bhatia R., 2009, POSITIVE DEFINITE MA
   Boix X., 2012, ECCV
   Boix X., 2013, CVPR
   Carreira J., 2012, ECCV
   Coates A., 2011, ICML
   Csurka G., 2004, ECCV
   Duchenne  O., 2011, ICCV
   Everingham M., 2010, IJCV
   Fan R. E., 2008, JMLR
   Fei-Fei L., 2006, TPAMI
   Fukushima K., 1980, BIOL CYBERNETICS
   Girshick R., 2014, CVPR
   Le Bihan D., 2001, J MAGNETIC RESONANCE
   Li P., 2012, ECCV
   Lowe D. G., 2004, IJCV
   Riesenhuber M, 1999, NATURE NEUROSCIENCE
   Sanchez Jorge, 2013, IJCV
   Tuzel O., 2006, ECCV
   Weickert J., 2006, VISUALIZATION PROCES
   Zhou X., 2010, ECCV
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100065
DA 2019-06-15
ER

PT S
AU Bresler, G
   Gamarnik, D
   Shah, D
AF Bresler, Guy
   Gamarnik, David
   Shah, Devavrat
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Hardness of parameter estimation in graphical models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID COMPLEXITY
AB We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see [1]) but no proof was known.
   Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).
C1 [Bresler, Guy; Shah, Devavrat] MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.
   [Gamarnik, David] MIT, Sloan Sch Management, Cambridge, MA 02139 USA.
RP Bresler, G (reprint author), MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.
EM gbresler@mit.edu; gamarnik@mit.edu; devavrat@mit.edu
FU NSF [CMMI-1335155, CNS-1161964]; Army Research Office MURI Award
   [W911NF-11-1-0036]
FX GB thanks Sahand Negahban for helpful discussions. Also we thank Andrea
   Montanari for sharing his unpublished manuscript [9]. This work was
   supported in part by NSF grants CMMI-1335155 and CNS-1161964, and by
   Army Research Office MURI Award W911NF-11-1-0036.
CR Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Bogdanov A, 2008, LECT NOTES COMPUT SC, V5171, P331
   Borwein J. M., 2010, CONVEX FUNCTIONS CON, V109
   Bubeck S., THEORY CONVEX OPTIMI
   Deza M. M., 1997, GEOMETRY CUTS METRIC
   Dyer M, 2002, SIAM J COMPUT, V31, P1527, DOI 10.1137/S0097539701383844
   Galanis  A., 2012, ARXIV12032226
   Istrail S., 2000, STOC, P87
   JAEGER F, 1990, MATH PROC CAMBRIDGE, V108, P35, DOI 10.1017/S0305004100068936
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   JERRUM MR, 1986, THEOR COMPUT SCI, V43, P169, DOI 10.1016/0304-3975(86)90174-X
   Jiang LB, 2010, IEEE T INFORM THEORY, V56, P6182, DOI 10.1109/TIT.2010.2081490
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Karger D, 2001, SIAM PROC S, P392
   Luby M, 1999, RANDOM STRUCT ALGOR, V15, P229, DOI 10.1002/(SICI)1098-2418(199910/12)15:3/4<229::AID-RSA3>3.0.CO;2-X
   Montanari A., 2014, COMPUTATIONAL UNPUB
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Papadimitriou C. H., 2003, COMPUTATIONAL COMPLE
   Rockafellar R, 1997, CONVEX ANAL, V28
   Roughgarden T., 2013, ADV NEURAL INFORM PR, P1043
   Shah D, 2011, IEEE T INFORM THEORY, V57, P7810, DOI 10.1109/TIT.2011.2168897
   Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56
   Sly A, 2010, ANN IEEE SYMP FOUND, P287, DOI 10.1109/FOCS.2010.34
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Weitz D., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P140
   Ziegler GM, 2000, DMV SEMINAR, V29, P1
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103020
DA 2019-06-15
ER

PT S
AU Bresler, G
   Chen, GH
   Shah, D
AF Bresler, Guy
   Chen, George H.
   Shah, Devavrat
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Latent Source Model for Online Collaborative Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the "online" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of n users either likes or dislikes each of m items. We assume there to be k types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly log (km) initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing k. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).
C1 [Bresler, Guy; Chen, George H.; Shah, Devavrat] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
RP Bresler, G (reprint author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
EM gbresler@mit.edu; georgehc@mit.edu; devavrat@mit.edu
FU NSF [CNS-1161964]; Army Research Office MURI Award [W911NF-11-1-0036];
   NDSEG fellowship
FX This work was supported in part by NSF grant CNS-1161964 and by Army
   Research Office MURI Award W911NF-11-1-0036. GHC was supported by an
   NDSEG fellowship.
CR Aiolli Fabio, 2013, P IT INF RETR WORKSH, P73
   Anandkumar A., 2012, ARXIV12107559
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Barman K, 2012, IEEE T INFORM THEORY, V58, P7110, DOI 10.1109/TIT.2012.2216980
   Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16
   Bertin-Mahieux  T., 2011, P 12 INT C MUS INF R
   Biau G, 2010, ANN STAT, V38, P1568, DOI 10.1214/09-AOS759
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bui Loc, 2012, ARXIV12064169
   Chaudhuri K., 2008, COLT, P9
   Dabeer O, 2013, IEEE INT SYMP INFO, P1197, DOI 10.1109/ISIT.2013.6620416
   Deshpande Yash, 2013, ARXIV13011722
   Grosse R., 2012, UNCERTAINTY ARTIFICI, P306
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7
   Koren Y., 2009, BELLKOR SOLUTION NET
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Moitra Ankur, 2010, P 51 ANN IEEE S FDN
   Piotte M, 2009, PRAGMATIC THEORY SOL
   Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175
   Sutskever Ilya, 2009, ADV NEURAL INFORM PR, P1821
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Toscher A, 2009, BIGCHAOS SOLUTION NE
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101001
DA 2019-06-15
ER

PT S
AU Bresler, G
   Gamarnik, D
   Shah, D
AF Bresler, Guy
   Gamarnik, David
   Shah, Devavrat
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Structure learning of antiferromagnetic Ising models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of Omega (p(d/2)) for learning general graphical models on p nodes of maximum degree d, for the class of so-called statistical algorithms recently introduced by Feldman et al. [1]. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound suggests that the (O) over tilde (p(d+2)) runtime required by Bresler, Mossel, and Sly's [2] exhaustive-search algorithm cannot be significantly improved without restricting the class of models.
   Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari [3] showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time (O) over tilde (p(2)). We provide an algorithm whose performance interpolates between (O) over tilde (p(2)) and (O) over tilde (p(d+2)) depending on the strength of the repulsion.
C1 [Bresler, Guy; Shah, Devavrat] MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.
   [Gamarnik, David] MIT, Lab Informat & Decis Syst, Sloan Sch Management, Cambridge, MA 02139 USA.
RP Bresler, G (reprint author), MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.
EM gbresler@mit.edu; gamarnik@mit.edu; devavrat@mit.edu
FU NSF [CMMI-1335155, CNS-1161964]; Army Research Office MURI Award
   [W911NF-11-1-0036]
FX This work was supported in part by NSF grants CMMI-1335155 and
   CNS-1161964, and by Army Research Office MURI Award W911NF-11-1-0036.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Anandkumar A, 2012, ANN STAT, V40, P1346, DOI 10.1214/12-AOS1009
   Bento J., 2009, NIPS
   Bresler G, 2008, LECT NOTES COMPUT SC, V5171, P343
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Csiszar I, 2006, ANN STAT, V34, P123, DOI 10.1214/009053605000000912
   Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134
   Dobrushin R. L., 1985, Statistical Physics and Dynamical Systems: Rigorous Results, P347
   DOBRUSHIN RL, 1970, THEOR PROBAB APPL+, V15, P458, DOI 10.1137/1115049
   FELDMAN V., 2013, P 45 ANN ACM S THEOR, P655
   Gamarnik D, 2014, MATH OPER RES, V39, P229, DOI 10.1287/moor.2013.0609
   Gamarnik D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1245
   Jalali A., 2011, NIPS, V24, P1935
   Jalali A., 2011, INT C AI STAT AISTAT, V14
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Lee S. - I., 2006, ADV NEURAL INF PROCE, P817
   Netrapalli P, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1295, DOI 10.1109/ALLERTON.2010.5707063
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Ray A., 2012, 50 ALL C
   Salas J, 1997, J STAT PHYS, V86, P551, DOI 10.1007/BF02199113
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Sinclair A, 2014, J STAT PHYS, V155, P666, DOI 10.1007/s10955-014-0947-5
   Srebro N., 2001, P 17 C UNC ART INT U, P504
   Weitz D., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P140
   Wu R., 2013, STOCHASTIC SYST, V3, P362
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100098
DA 2019-06-15
ER

PT S
AU Bruer, JJ
   Tropp, JA
   Cevher, V
   Becker, SR
AF Bruer, John J.
   Tropp, Joel A.
   Cevher, Volkan
   Becker, Stephen R.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Time-Data Tradeoffs by Aggressive Smoothing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID CONVEX; MINIMIZATION; GRADIENT
AB This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.
C1 [Bruer, John J.; Tropp, Joel A.] CALTECH, Dept Comp Math Sci, Pasadena, CA 91125 USA.
   [Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland.
   [Becker, Stephen R.] Univ Colorado Boulder, Dept Appl Math, Boulder, CO USA.
RP Bruer, JJ (reprint author), CALTECH, Dept Comp Math Sci, Pasadena, CA 91125 USA.
EM jbruer@cms.caltech.edu
FU ONR [N00014-11-1002]; AFOSR [FA9550-09-1-0643]; Sloan Research
   Fellowship; European Commission [MIRG-268398]; ERC Future Proof [SNF
   200021-132548, SNF 200021-146750, SNF CRSII2-147633]
FX JJB's and JAT's work was supported under ONR award N00014-11-1002, AFOSR
   award FA9550-09-1-0643, and a Sloan Research Fellowship. VC's work was
   supported in part by the European Commission under Grant MIRG-268398,
   ERC Future Proof, SNF 200021-132548, SNF 200021-146750 and SNF
   CRSII2-147633. SRB was previously with IBM Research, Yorktown Heights,
   NY 10598 during the completion of this work.
CR Agarwal A., 2012, 12080129V1 ARXIV
   Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032
   Amelunxen D., 2014, INFORM INFERENCE
   Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664
   Auslender A, 2006, SIAM J OPTIMIZ, V16, P697, DOI 10.1137/S1052623403427823
   Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5
   Berthet Q., 2013, 13040828V2 ARXIV
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bruer J. J., 2014, IEEE J SELECTED TOPI
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai JF, 2009, MATH COMPUT, V78, P1515, DOI 10.1090/S0025-5718-08-02189-3
   Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   CLAERBOUT JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378
   Daniely A., 2013, ADV NEURAL INFORM PR, V26, P145
   Fazel M, 2002, THESIS
   Jordan MI, 2013, BERNOULLI, V19, P1378, DOI 10.3150/12-BEJSP17
   Lai MJ, 2013, SIAM J IMAGING SCI, V6, P1059, DOI 10.1137/120863290
   Mesbahi M, 1997, IEEE T AUTOMAT CONTR, V42, P239, DOI 10.1109/9.554402
   Nemirovsky AS, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Osher S, 2010, COMMUN MATH SCI, V8, P93
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087
   Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, P928, DOI DOI 10.1145/1390156.1390273
   Shender D., 2013, P 30 INT C MACH LEAR, P756
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100038
DA 2019-06-15
ER

PT S
AU Bui, T
   Turner, R
AF Bui, Thang
   Turner, Richard
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Tree-structured Gaussian Process Approximations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Gaussian process regression can be accelerated by constructing a small pseudodataset to summarize the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimization. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.
C1 [Bui, Thang; Turner, Richard] Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Trumpington St, Cambridge CB2 1PZ, England.
RP Bui, T (reprint author), Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Trumpington St, Cambridge CB2 1PZ, England.
EM tdb40@cam.ac.uk; ret26@cam.ac.uk
FU EPSRC [EP/G050821/1, EP/L000776/1]; Google
FX We would like to thank the EPSRC (grant numbers EP/G050821/1 and
   EP/L000776/1) and Google for funding.
CR Chalupka K, 2013, J MACH LEARN RES, V14, P333
   Figueiras-vidal Anibal, 2009, ADV NEURAL INFORM PR, P1087
   Gilboa E., 2013, PATT AN MACH INT IEE
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Qi Y., 2010, P 26 C UNC ART INT, P450
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sarkka S, 2013, IEEE SIGNAL PROC MAG, V30, P51, DOI 10.1109/MSP.2013.2246292
   Seeger M., 2003, THESIS
   Seeger M., 2003, INT C ART INT STAT
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Snelson E., 2007, INT C ART INT STAT, P524
   Snelson EL, 2007, THESIS
   Titsias M, 2009, ARTIF INTELL, P567
   Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908
   Turner R., 2011, ADV NEURAL INF PROCE, P981
   Turner R., 2010, THESIS
   Turner R. E., 2014, SIGNAL PROCESSING IE
   Turner Richard E., 2011, BAYESIAN TIME SERIES, P109
   Wilson A., 2013, P 30 INT C MACH LEAR, P1067
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102020
DA 2019-06-15
ER

PT S
AU Calandriello, D
   Lazaric, A
   Restelli, M
AF Calandriello, Daniele
   Lazaric, Alessandro
   Restelli, Marcello
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Sparse Multi-Task Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB In multi-task reinforcement learning (MTRL), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t. single-task learning. In this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features. This is equivalent to assuming that the weight vectors of the task value functions are jointly sparse, i.e., the set of their non-zero components is small and it is shared across tasks. Building on existing results in multi-task regression, we develop two multi-task extensions of the fitted Q-iteration algorithm. While the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation. For both algorithms we provide a sample complexity analysis and numerical simulations.
C1 [Calandriello, Daniele; Lazaric, Alessandro] INRIA Lille Nord Europe, Team SequeL, Lille, France.
   [Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy.
RP Calandriello, D (reprint author), INRIA Lille Nord Europe, Team SequeL, Lille, France.
EM daniele.calandriello@inria.fr; alessandro.lazaric@inria.fr;
   fmarcello.restelli@polimi.it
FU French Ministry of Higher Education and Research; European Community
   [270327]; French National Research Agency (ANR) [ANR-14-CE24-0010-01]
FX This work was supported by the French Ministry of Higher Education and
   Research, the European Community's Seventh Framework Programme under
   grant agreement 270327 (project CompLACS), and the French National
   Research Agency (ANR) under project ExTra-Learn n.ANR-14-CE24-0010-01.
CR Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Calandriello Daniele, 2014, SPARSE MULTI TASK RE
   Castelletti A, 2011, IEEE ADPRL
   Ernst  D., 2005, J MACHINE LEARNING R, V6
   Ghavamzadeh Mohammad, 2011, ICML
   Grunewalder S., 2012, ICML
   Hachiya H., 2010, ECML PKDD
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Hoffman Matthew W., 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P102, DOI 10.1007/978-3-642-29946-9_13
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Kolter J. Z., 2009, ICML
   Lazaric A., 2011, REINFORCEMENT LEARNI
   Lazaric Alessandro, 2011, NIPS
   Lazaric Alessandro, 2010, ICML
   Li H, 2009, J MACH LEARN RES, V10, P1131
   Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896
   Munos R, 2008, J MACH LEARN RES, V9, P815
   Painter-wakefield C., 2012, ICML
   Scherrer Bruno, 2012, ICML
   Snel Matthijs, 2011, EWRL
   Sutton R., 1998, INTRO REINFORCEMENT
   Tanaka F, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P1108
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Wilson A., 2007, P 24 INT C MACH LEAR, P1015
   Zhang Y., 2010, ADV NEURAL INFORM PR, P2550
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100026
DA 2019-06-15
ER

PT S
AU Carlson, DE
   Borg, JS
   Dzirasa, K
   Carin, L
AF Carlson, David E.
   Borg, Jana Schaich
   Dzirasa, Kafui
   Carin, Lawrence
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI On the Relationship Between LFP & Spiking Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.
C1 [Carlson, David E.; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Duham, NC 27701 USA.
   [Borg, Jana Schaich; Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Duham, NC 27701 USA.
RP Carlson, DE (reprint author), Duke Univ, Dept Elect & Comp Engn, Duham, NC 27701 USA.
EM david.carlson@duke.edu; jana.borg@duke.edu; kafui.dzirasa@duke.edu;
   lcarin@duke.edu
FU ARO; DARPA; DOE; NGA; ONR
FX The research reported here was funded in part by ARO, DARPA, DOE, NGA
   and ONR. We thank the reviewers for their helpful comments.
CR Blei D.M., 2006, BAYESIAN ANAL
   Calabrese A., 2010, J NEUROSCI METHODS
   Carlson D.E., 2013, IEEE TBME
   Chen B., 2011, ICML
   Dzirasa K., 2006, J NEUROSCI
   Dzirasa K, 2010, J NEUROSCI
   Ferguson T.S., 1973, ANN STAT         MAR
   Heller K.A., 2005, ICML
   Hughes M. C., 2013, NIPS
   Ishwaran H., 2001, JASA
   Kelly R.C., 2010, J COMP NEUROSCI
   Kim D.I., 2012, NIPS
   Larson-Prior L.J., 2009, PNAS
   Le Van Quyen M., 2007, TRENDS NEUROSCIENCES
   Mehring C, 2003, NAT NEUROSCI, V6, P1253, DOI 10.1038/nn1158
   Nauhaus I., 2009, NATURE NEURO     JAN
   Pesaran B, 2002, NAT NEUROSCI, V5, P805, DOI 10.1038/nn890
   Pillow J., 2007, NIPS
   Rasch M., 2009, J NEUROSCI
   Rasch M.J., 2008, J NEUROPHYSIOLOGY
   Roberts S.J., 2002, IEEE T SIGNAL PROCES
   Rue H., 2009, J ROYAL STAT SOC
   Uhlhaas P.J., 2010, NATURE REV NEURO
   Varela F., 2001, NATURE REV NEURO
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101075
DA 2019-06-15
ER

PT S
AU Carpentier, A
   Valko, M
AF Carpentier, Alexandra
   Valko, Michal
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Extreme bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the EXTREMEHUNTER algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.
C1 [Carpentier, Alexandra] Univ Cambridge, CMS, Stat Lab, Cambridge, England.
   [Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France.
RP Carpentier, A (reprint author), Univ Cambridge, CMS, Stat Lab, Cambridge, England.
EM a.carpentier@statslab.cam.ac.uk; michal.valko@inria.fr
FU Intel Corporation; French Ministry of Higher Education and Research;
   European Community's Seventh Framework Programme (FP7/2007-2013)
   [270327]
FX We would like to thank John Mark Agosta and Jennifer Healey for the
   network traffic data. The research presented in this paper was supported
   by Intel Corporation, by French Ministry of Higher Education and
   Research, and by European Community's Seventh Framework Programme
   (FP7/2007-2013) under grant agreement no 270327 (CompLACS).
CR Abe N., 2006, P 12 ACM SIGKDD INT, P504, DOI DOI 10.1145/1150402.1150459
   Agosta John Mark, IEEE P INFOCOM, P225
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bubeck S, 2013, IEEE T INFORM THEORY, V59, P7711, DOI 10.1109/TIT.2013.2277869
   Bubeck S, 2009, LECT NOTES ARTIF INT, V5809, P23
   Carpentier Alexandra, 2014, STAT SINICA
   Carpentier Alexandra, 2014, ELECT J STAT
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Cicirello Vincent A., 2005, AAAI C ART INT
   De Haan L., 2006, SPRING S OPERAT RES
   Fisher RA, 1928, P CAMB PHILOS SOC, V24, P180
   Gnedenko B, 1943, ANN MATH, V44, P423, DOI 10.2307/1968974
   HALL P, 1984, ANN STAT, V12, P1079, DOI 10.1214/aos/1176346723
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Liu KQ, 2012, IEEE INT SYMP INFO
   Markou M, 2003, SIGNAL PROCESS, V83, P2481, DOI 10.1016/j.sigpro.2003.018
   Neill DB, 2010, MACH LEARN, V79, P261, DOI 10.1007/s10994-009-5144-4
   Priebe C. Y., 2005, Computational & Mathematical Organization Theory, V11, P229, DOI 10.1007/s10588-005-5378-z
   Steinwart I, 2005, J MACH LEARN RES, V6, P211
   Streeter MJ, 2006, LECT NOTES COMPUT SC, V4204, P560
   Streeter Matthew J., 2006, AAAI, P135
   Turner Ryan, 2010, IEEE WORKSH MACH LEA
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101050
DA 2019-06-15
ER

PT S
AU Chan, H
   Ortiz, LE
AF Chan, Hau
   Ortiz, Luis E.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Computing Nash Equilibria in Generalized Interdependent Security Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents' voluntary investment decisions when facing potential direct risk and transfer-risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP-complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.
C1 [Chan, Hau; Ortiz, Luis E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
RP Chan, H (reprint author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM hauchan@cs.stonybrook.edu; leortiz@cs.stonybrook.edu
FU NSF Graduate Research Fellowship; NSF CAREER Award [IIS-1054541]
FX This material is based upon work supported by an NSF Graduate Research
   Fellowship (first author) and an NSF CAREER Award IIS-1054541 (second
   author).
CR Chan H., 2012, P 28 C UNC ART INT U, P152
   Daskalakis C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P71
   Elkind Edith, 2006, P 7 ACM C EL COMM, P100
   Fudenberg D., 1998, THEORY LEARNING GAME, V1
   Garey M. R, 1979, COMPUTERS INTRACTABI
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Gkonis KG, 2010, J TRANSP SECUR, V3, P197, DOI 10.1007/s12198-010-0047-y
   Gottlob G., 2003, P 9 C THEOR ASP RAT, V24, P215
   Heal G, 2005, J CONFLICT RESOLUT, V49, P201, DOI 10.1177/0022002704272833
   Heal Geoffrey, 2005, WORKING PAPER
   Heal Geoffrey, 2004, 10706 NAT BUR EC RES
   Kearns M, 2004, ADV NEUR IN, V16, P561
   Kearns M. J., 2001, P 17 C UNC ART INT, P253
   Klimt B., 2004, CEAS
   Knuth D. E., 1993, STANFORD GRAPHBASE P
   Laszka A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2635673
   Leskovec J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1361
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   Shoham Y., 2009, MULTIAGENT SYSTEMS A
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Xi Chen, 2009, J ACM, V56
   ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103036
DA 2019-06-15
ER

PT S
AU Chan, SO
   Diakonikolas, I
   Servedio, RA
   Sun, XR
AF Chan, Siu-On
   Diakonikolas, Ilias
   Servedio, Rocco A.
   Sun, Xiaorui
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width
   Histograms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID RESTRICTIONS
AB Let p be an unknown and arbitrary probability distribution over [0, 1). We consider the problem of density estimation, in which a learning algorithm is given i.i.d. draws from p and must (with high probability) output a hypothesis distribution that is close to p. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function.
   In more detail, for any k and epsilon, we give an algorithm that makes (O) over tilde (k/epsilon(2)) draws from p, runs in (O) over tilde (k/epsilon(2)) time, and outputs a hypothesis distribution h that is piecewise constant with O(k log(2)(1/epsilon)) pieces. With high probability the hypothesis h satisfies d(TV)(p, h) <= C . opt(k)(p) + epsilon, where d(TV) denotes the total variation distance (statistical distance), C is a universal constant, and opt(k)(p) is the smallest total variation distance between p and any k-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The "approximation factor" C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and epsilon can achieve C < 2 regardless of what kind of hypothesis distribution it uses.
C1 [Chan, Siu-On] Microsoft Res, Redmond, WA 98052 USA.
   [Diakonikolas, Ilias] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Servedio, Rocco A.; Sun, Xiaorui] Columbia Univ, New York, NY 10027 USA.
RP Chan, SO (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM sochan@gmail.com; ilias.d@ed.ac.uk; rocco@cs.columbia.edu;
   xiaoruisun@cs.columbia.edu
CR Acharya J., 2014, TECHNICAL REPORT
   Barlow R, 1972, STAT INFERENCE ORDER
   Birge L, 1997, ANN STAT, V25, P970, DOI 10.1214/aos/1069362733
   BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488
   BRUNK HD, 1958, ANN MATH STAT, V29, P437, DOI 10.1214/aoms/1177706621
   Chan S., 2014, STOC, P604, DOI DOI 10.1145/2591796.2591848
   Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380
   Chaudhuri S., 1998, SIGMOD Record, V27, P436
   Daskalakis Constantinos, 2014, COLT, P1183
   De A., 2012, INVERSE PROBLEMS APP
   Devroye L., 2001, SPRINGER SERIES STAT
   Devroye L., 1985, NONPARAMETRIC DENSIT
   Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389
   Grenander U, 1956, SKAND AKTUARIETIDSK, V39, P125
   GROENEBOOM P., 1985, P BERK C HON J NEYM, VII, P539
   Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873
   HANSON DL, 1976, ANN STAT, V4, P1038, DOI 10.1214/aos/1176343640
   Indyk P., 2012, PODS, P15, DOI DOI 10.1145/2213556.2213561
   Jagadish H. V., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P275
   Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155
   Massart P., 2003, LECT NOTES MATH, V33
   Pearson K., 1895, PHILOS T R SOC A, V186, P343, DOI [10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010]
   RAO BLSP, 1969, SANKHYA SER A, V31, P23
   Reboul L, 2005, ANN STAT, V33, P1330, DOI 10.1214/009053605000000138
   Scott DW, 1992, MULTIVARIATE DENSITY
   Silverman B. W., 1986, DENSITY ESTIMATION
   Valiant L. G, 1984, P 16 ANN ACM S THEOR, P436, DOI DOI 10.1145/800057.808710
   WEGMAN EJ, 1970, ANN MATH STAT, V41, P457, DOI 10.1214/aoms/1177697085
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100005
DA 2019-06-15
ER

PT S
AU Chandar, APS
   Lauly, S
   Larochelle, H
   Khapra, MM
   Ravindran, B
   Raykar, V
   Saha, A
AF Chandar, Sarath A. P.
   Lauly, Stanislas
   Larochelle, Hugo
   Khapra, Mitesh M.
   Ravindran, Balaraman
   Raykar, Vikas
   Saha, Amrita
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI An Autoencoder Approach to Learning BilingualWord Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Cross-language learning allows one to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are coherent between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.
C1 [Chandar, Sarath A. P.; Ravindran, Balaraman] Indian Inst Technol Madras, Madras, Tamil Nadu, India.
   [Lauly, Stanislas; Larochelle, Hugo] Univ Sherbrooke, Sherbrooke, PQ, Canada.
   [Khapra, Mitesh M.; Raykar, Vikas; Saha, Amrita] IBM Res India, Bengaluru, India.
RP Chandar, APS (reprint author), Indian Inst Technol Madras, Madras, Tamil Nadu, India.
EM apsarathchandar@gmail.com; stanislas.lauly@usherbrooke.ca;
   hugo.larochelle@usherbrooke.ca; mikhapra@in.ibm.com;
   viraykar@in.ibm.com; ravi@cse.iitm.ac.in; amrsaha4@in.ibm.com
FU Google
FX We would like to thank Alexander Klementiev and Ivan Titov for providing
   the code for the classifier and data indices. This work was supported in
   part by Google.
CR Collobert R., 2011, J MACHINE LEARNING R, V12
   Das D., 2011, P 49 ANN M ASS COMP, V1, P600
   Dauphin Y., 2011, P 28 INT C MACH LEAR, P945
   Dumais Susan T, 1997, AAAI SPRING S CROSS, V15, P21
   Faruqui M, 2014, P 14 C EUR CHAPT ASS, P462, DOI DOI 10.3115/V1/E14-1049
   Gao J., 2014, P 52 ANN M ASS COMP, V1, P699
   Hermann K. M., 2014, P 52 ANN M ASS COMP, P58
   Hermann Karl Moritz, 2014, P INT C LEARN REPR I
   Klementiev Alexandre, 2012, P INT C COMP LING
   Koehn P., 2005, MT SUMMIT
   Larochelle Hugo, 2012, ADV NEURAL INFORM PR
   Liu  B., 2012, SENTIMENT ANAL OPINI
   Mihalcea Rada, 2007, P ANN M ASS COMP LIN, P976
   Mikolov T., 2013, EXPLOITING SIMILARIT
   Mnih A, 2008, ADV NEURAL INF PROCE, V21, P1081
   Morin F., 2005, P AISTATS, P246
   Pado S, 2009, J ARTIF INTELL RES, V36, P307, DOI 10.1613/jair.2863
   Platt J., 2010, P EMNLP 2010, P251
   Socher Richard, 2013, P 51 ANN M ASS COMP, P455
   Steven Edward Loper Bird, 2009, NATURAL LANGUAGE PRO
   Toutanova K., 2003, P 2003 C N AM CHAPT, V1, P173, DOI DOI 10.3115/1073445.1073478
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Wan X., 2009, P JOINT C 47 ANN M A, V1, P235
   Yarowsky D., 2001, P 1 INT C HUM LANG T, P1, DOI DOI 10.3115/1072133.1072187
   Yih W, 2011, P 15 C COMP NAT LANG, P247
   Zou Will Y., 2013, EMPIRICAL METHODS NA
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100049
DA 2019-06-15
ER

PT S
AU Chang, J
   Fisher, JW
AF Chang, Jason
   Fisher, John W., III
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Parallel Sampling of HDPs using Sub-Cluster Splits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We develop a sampling technique for Hierarchical Dirichlet process models. The parallel algorithm builds upon [1] by proposing large split and merge moves based on learned sub-clusters. The additional global split and merge moves drastically improve convergence in the experimental results. Furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.
C1 [Chang, Jason; Fisher, John W., III] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Chang, J (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM jchang7@csail.mit.edu; fisher@csail.mit.edu
FU Office of Naval Research Multidisciplinary Research Initiative program
   [N000141110688]; VITALITE from the Army Research Office
   Multidisciplinary Research Initiative program [W911NF-11-1-0391]
FX This research was partially supported by the Office of Naval Research
   Multidisciplinary Research Initiative program, award N000141110688 and
   by VITALITE, which receives support from Army Research Office
   Multidisciplinary Research Initiative program, award W911NF-11-1-0391.
CR ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871
   BACHE K., 2013, UCI MACHINE LEARNING
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bryant M., 2012, ADV NEURAL INFORM PR
   Chang J., 2013, ADV NEURAL INFORM PR
   Dahl D. B., 2003, TECHNICAL REPORT
   Fox E. B., 2008, INT C MACH LEARN
   Gal Y., 2013, WORKSH BIG LEARN NIP
   Green PJ, 2001, SCAND J STAT, V28, P355, DOI 10.1111/1467-9469.00242
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Ishwaran H, 2002, CAN J STAT, V30, P269, DOI 10.2307/3315951
   Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001
   Jain S, 2007, BAYESIAN ANAL, V2, P445, DOI 10.1214/07-BA219
   Johnson M. J., 2013, ADV NEURAL INFORM PR
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Newman D, 2009, J MACH LEARN RES, V10, P1801
   Niu Feng, 2011, ADV NEURAL INFORM PR
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Sudderth E. B., 2006, THESIS
   Teh Y. W., 2008, ADV NEURAL INFORM PR, V20
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Wang C., 2012, ARXIV12071657STATML
   Williamson S., 2013, INT C MACH LEARN
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100014
DA 2019-06-15
ER

PT S
AU Chaudhuri, K
   Dasgupta, S
AF Chaudhuri, Kamalika
   Dasgupta, Sanjoy
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Rates of convergence for nearest neighbor classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find, for instance, that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.
C1 [Chaudhuri, Kamalika; Dasgupta, Sanjoy] Univ Calif San Diego, Comp Sci & Engn, San Diego, CA 92103 USA.
RP Chaudhuri, K (reprint author), Univ Calif San Diego, Comp Sci & Engn, San Diego, CA 92103 USA.
EM kamalika@cs.ucsd.edu; dasgupta@cs.ucsd.edu
FU National Science Foundation [IIS-1162581]
FX The authors are grateful to the National Science Foundation for support
   under grant IIS-1162581.
CR Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217
   Cover T. M., 1968, P HAW INT C SYST SCI
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dasgupta S., 2012, 25 C LEARN THEOR
   DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633
   Federer H., 1969, GEOMETRIC MEASURE TH
   Fix  E., 1951, 2149004 USAF SCH AV
   FRITZ J, 1975, IEEE T INFORM THEORY, V21, P552, DOI 10.1109/TIT.1975.1055443
   GYORFI L, 1981, IEEE T INFORM THEORY, V27, P362, DOI 10.1109/TIT.1981.1056344
   Heinonen J., 2001, LECT ANAL METRIC SPA, DOI 10.1007/978-1-4613-0131-8
   Kaas R., 1980, STAT NEERL, V34, P13, DOI DOI 10.1111/J.1467-9574.1980.TB00681.X
   KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248
   Mammen E, 1999, ANN STAT, V27, P1808
   SLUD EV, 1977, ANN PROBAB, V5, P404, DOI 10.1214/aop/1176995801
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Urner R., 2011, INT C MACH LEARN
   WAGNER TJ, 1971, IEEE T INFORM THEORY, V17, P566, DOI 10.1109/TIT.1971.1054698
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101110
DA 2019-06-15
ER

PT S
AU Chaudhuri, K
   Hsu, D
   Song, S
AF Chaudhuri, Kamalika
   Hsu, Daniel
   Song, Shuang
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI The Large Margin Mechanism for Differentially Private Maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning.
   Previous algorithms for this problem are either range-dependent-i.e., their utility diminishes with the size of the universe-or only apply to very restricted function classes. This work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.
C1 [Chaudhuri, Kamalika; Song, Shuang] Univ Calif San Diego, La Jolla, CA 92093 USA.
   [Hsu, Daniel] Columbia Univ, New York, NY USA.
RP Chaudhuri, K (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM kamalika@cs.ucsd.edu; djhsu@cs.columbia.edu; shs037@eng.ucsd.edu
FU NIH [U54 HL108460]; NSF [IIS 1253942]
FX We thank an anonymous reviewer for suggesting the simpler variant of LMM
   based on the exponential mechanism. (The original version of LMM used a
   max of truncated exponentials mechanism, which gives the same guarantees
   up to constant factors.) This work was supported in part by the NIH
   under U54 HL108460 and the NSF under IIS 1253942.
CR Bassily Raef, 2014, ARXIV14057085
   Beimel A., 2013, ITCS, P97, DOI DOI 10.1145/2422436.2422450
   Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437
   Beimel Amos, 2013, RANDOM
   Bhaskar Raghav, 2010, KDD
   Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148
   Blum Avrim, 2005, PODS
   Bonomi L, 2013, PROC VLDB ENDOW, V6, P1422, DOI 10.14778/2536274.2536329
   Bun Mark, 2014, STOC
   Chaudhuri K., 2012, ADV NEURAL INFORM PR, P998
   Chaudhuri K., 2013, ADV NEURAL INFORM PR, P2652
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Chaudhuri Kamalika, 2012, ICML
   Chaudhuri Kamalika, 2011, COLT
   Chen Rui, 2011, VLDB
   De A, 2012, LECT NOTES COMPUT SC, V7194, P321, DOI 10.1007/978-3-642-28914-9_18
   Dwork C., 2006, THEORY CRYPTOGRAPHY
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2009, ACM S THEORY COMPUT, P371
   Dwork Cynthia, 2006, EURO CRYPT
   Friedman A., 2010, KDD
   Hardt M, 2010, ACM S THEORY COMPUT, P705
   Hardt Moritz, 2010, FOCS
   Jain P., 2012, COLT
   Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090
   Langford J, 2004, J MACH LEARN RES, V5, P529
   Li Ninghui, 2012, VLDB
   McSherry F, 2007, FOCS
   Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911
   Smith A., 2011, STOC
   Smith Adam, 2013, COLT
   Uhler Caroline, 2012, ARXIV12050739
   Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651
   Zeng Chen, 2012, VLDB
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101062
DA 2019-06-15
ER

PT S
AU Chen, CY
   Zhu, J
   Zhang, XH
AF Chen, Changyou
   Zhu, Jun
   Zhang, Xinhua
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Robust Bayesian Max-Margin Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.
C1 [Chen, Changyou] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, Tsinghua Natl TNList Lab, Beijing 100084, Peoples R China.
   [Zhang, Xinhua] Australian Natl Univ, Canberra, ACT, Australia.
   [Zhang, Xinhua] Natl ICT Australia NICTA, Canberra, ACT, Australia.
RP Chen, CY (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM cchangyou@gmail.com; dcszj@tsinghua.edu.cn; xinhua.zhang@anu.edu.au
FU Australia China Science and Research Fund from the Department of
   Industry, Innovation, Climate Change, Science, Research and Tertiary
   Education of the Australian Government [ACSRF-06283]; National Key
   Project for Basic Research of China [2013CB329403]; NSF of China
   [61322308, 61332007]; Australian Government; Australian Research Council
   through the ICT Centre of Excellence program
FX This work was supported by an Australia China Science and Research Fund
   grant (ACSRF-06283) from the Department of Industry, Innovation, Climate
   Change, Science, Research and Tertiary Education of the Australian
   Government, the National Key Project for Basic Research of China (No.
   2013CB329403), and NSF of China (Nos. 61322308, 61332007). NICTA is
   funded by the Australian Government as represented by the Department of
   Broadband, Communications and the Digital Economy and the Australian
   Research Council through the ICT Centre of Excellence program.
CR BACHE K., 2013, UCI MACHINE LEARNING
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Ben-Hur A, 2002, J MACH LEARN RES, V2, P125, DOI 10.1162/15324430260185565
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cheng H., 2013, UAI
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Davy M, 2010, IEEE T PATTERN ANAL, V32, P1781, DOI 10.1109/TPAMI.2010.21
   Favaro S., 2013, STAT SCI
   Fox C., 1987, INTRO CALCULUS VARIA
   Franti P, 2006, PATTERN RECOGN, V39, P761, DOI 10.1016/j.patcog.2005.09.012
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Gomes R., 2010, NIPS
   Heller K.A., 2005, ICML
   Hu Y., 2013, NIPS
   Jain AK, 2005, LECT NOTES COMPUT SC, V3776, P1, DOI 10.1145/1083091.1083101
   Jorgensen B., 1982, LECT NOTES STAT
   Li Y. F., 2009, AISTATS
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   Murphy K.P., 2007, TECHNICAL REPORT
   Rasmussen C. E., 2000, NIPS
   Shah A., 2013, UAI
   Shi J., 2000, TPAMI, V22, P705
   Sindhwani V., 2006, NIPS WORKSH MACH LEA
   Teh Y. W., 2008, NIPS
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   Wallach H. M., 2008, THESIS
   Wang L., 2012, AAAI
   Wang P., 2011, SDM
   Xu L., 2005, NIPS
   Xu M., 2013, ICML
   Zhao B., 2008, ICML
   Zhou G. T., 2013, NIPS
   Zhu J., 2014, JMLR
   Zhu J., 2013, ICML
   Zhu J., 2012, ICML
   Zhu J, 2012, J MACH LEARN RES, V13, P2237
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100084
DA 2019-06-15
ER

PT S
AU Chen, C
   Liu, H
   Metaxas, DN
   Zhao, TQ
AF Chen, Chao
   Liu, Han
   Metaxas, Dimitris N.
   Zhao, Tianqi
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Mode Estimation for High Dimensional Discrete Tree Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading (delta, rho)-modes of the underlying distributions. A point is defined to be a (delta, rho)-mode if it is a local optimum of the density within a delta-neighborhood under metric rho. As we increase the "scale" parameter delta, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the (delta, rho)-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.
C1 [Chen, Chao; Metaxas, Dimitris N.] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Liu, Han; Zhao, Tianqi] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Chen, C (reprint author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
EM chao.chen.cchen@gmail.com; hanliu@princeton.edu; dnm@cs.rutgers.edu;
   tianqi@princeton.edu
FU [NSF IIS 1451292];  [NSF CNS 1229628];  [NSF IIS1408910];  [NSF
   IIS1332109];  [NIH R01MH102339];  [NIH R01GM083084];  [NIH R01HG06841]
FX Chao Chen thanks Vladimir Kolmogorov and Christoph H. Lampert for
   helpful discussions. The research of Chao Chen and Dimitris N. Metaxas
   is partially supported by the grants NSF IIS 1451292 and NSF CNS
   1229628. The research of Han Liu is partially supported by the grants
   NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and
   NIH R01HG06841.
CR Bach F.R., 2003, J MACHINE LEARNING R, V4, P1205
   Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1
   Chaudhuri P, 1999, J AM STAT ASSOC, V94, P807, DOI 10.2307/2669996
   Chazal F, 2011, COMPUTATIONAL GEOMETRY (SCG 11), P97
   Chen C., 2013, INT C ART INT STAT A
   Chen C., 2014, TECHNICAL REPORT
   Chen C, 2011, IEEE I CONF COMP VIS, P423, DOI 10.1109/ICCV.2011.6126271
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fromer M, 2009, PROTEINS, V75, P682, DOI 10.1002/prot.22280
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Li J, 2007, J MACH LEARN RES, V8, P1687
   Lindeberg T., 1993, SCALE SPACE THEORY C
   Liu H, 2011, J MACH LEARN RES, V12, P907
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maitra R, 2009, IEEE ACM T COMPUT BI, V6, P144, DOI 10.1109/TCBB.2007.70244
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Minnotte M. C., 1993, J COMPUT GRAPH STAT, V2, P51
   Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Ray S, 2005, ANN STAT, V33, P2042, DOI 10.1214/00905360500000417
   SILVERMAN BW, 1981, J ROY STAT SOC B MET, V43, P97
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wille A, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-11-r92
   Witkin Andrew P, 1987, READINGS COMPUTER VI, P329
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102094
DA 2019-06-15
ER

PT S
AU Chen, DQ
AF Chen, Dongqu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Shuffle Ideals Under Restricted Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FINITE-STATE TRANSDUCERS; IDENTIFICATION; COMPLEXITY; EFFICIENT
AB The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set U is the collection of all strings containing some string u is an element of U as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.
C1 [Chen, Dongqu] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
RP Chen, DQ (reprint author), Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.
EM dongqu.chen@yale.edu
CR ANGLUIN D, 1978, INFORM CONTROL, V39, P337, DOI 10.1016/S0019-9958(78)90683-6
   ANGLUIN D, 1987, INFORM COMPUT, V75, P87, DOI 10.1016/0890-5401(87)90052-6
   Angluin D, 2013, J MACH LEARN RES, V14, P1513
   BACHE K., 2013, UCI MACHINE LEARNING
   Bottou  L., 2007, LARGE SCALE KERNEL M, P301
   Bshouty NH, 1997, MACH LEARN, V26, P25, DOI 10.1023/A:1007320031970
   de la Higuera C, 2005, PATTERN RECOGN, V38, P1332, DOI 10.1016/j.patcog.2005.01.003
   Gnedenko B., 1949, ADDISON WESLEY SERIE
   GOLD EM, 1978, INFORM CONTROL, V37, P302, DOI 10.1016/S0019-9958(78)90562-4
   Ibragimov I. A., 1956, THEORY PROBABILITY I, V1, P255, DOI 10.1137/1101021
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Kontorovich L, 2008, THEOR COMPUT SCI, V405, P223, DOI 10.1016/j.tcs.2008.06.037
   Kontorovich L, 2009, J MACH LEARN RES, V10, P1095
   Koskenniemi Kimmo, 1983, P IJCAI 83, V2, P683
   Mohri M, 1997, COMPUT LINGUIST, V23, P269
   Mohri M, 2002, COMPUT SPEECH LANG, V16, P69, DOI 10.1006/csla.2001.0184
   MOHRI M, 1996, NAT LANG ENG, V2, P61, DOI DOI 10.1017/S135132499600126X
   Mohri M, 2010, IEEE T AUDIO SPEECH, V18, P197, DOI 10.1109/TASL.2009.2023170
   PITT L, 1993, J ACM, V40, P95, DOI 10.1145/138027.138042
   Rambow O., 2002, P 19 INT C COMP LING, V2, P1, DOI DOI 10.3115/1071884.1071910
   Sproat R, 1996, COMPUT LINGUIST, V22, P377
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103057
DA 2019-06-15
ER

PT S
AU Chen, SY
   Lin, T
   King, I
   Lyu, MR
   Chen, W
AF Chen, Shouyuan
   Lin, Tian
   King, Irwin
   Lyu, Michael R.
   Chen, Wei
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Combinatorial Pure Exploration of Multi-Armed Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.
C1 [Chen, Shouyuan; King, Irwin; Lyu, Michael R.] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Lin, Tian] Tsinghua Univ, Beijing, Peoples R China.
   [Chen, Shouyuan; Lin, Tian; Chen, Wei] Microsoft Res Asia, Beijing, Peoples R China.
RP Chen, SY (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM sychen@cse.cuhk.edu.hk; lint10@mails.tsinghua.edu.cn;
   king@cse.cuhk.edu.hk; lyt@cse.cuhk.edu.hk; weic@microsoft.com
FU National Grand Fundamental Research 973 Program of China [2014CB340401,
   2014CB340405]; Research Grants Council of the Hong Kong Special
   Administrative Region, China [CUHK 413212, CUHK 415113]; Microsoft
   Research Asia Regional Seed Fund in Big Data Research
   [FY13-RES-SPONSOR-036]
FX The work described in this paper was partially supported by the National
   Grand Fundamental Research 973 Program of China (No. 2014CB340401 and
   No. 2014CB340405), the Research Grants Council of the Hong Kong Special
   Administrative Region, China (Project No. CUHK 413212 and CUHK 415113),
   and Microsoft Research Asia Regional Seed Fund in Big Data Research
   (Grant No. FY13-RES-SPONSOR-036).
CR Audibert  J.-Y., 2010, COLT
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Berge C., 1957, PNAS
   Bubeck S., 2013, P 30 INT C MACH LEAR, V28, P258
   Bubeck S., 2012, COLT
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Even-Dar E., 2006, JMLR
   Gabillon V., 2011, NIPS
   Gabillon V., 2012, NIPS
   Gopalan A., 2014, P 31 INT C MACH LEAR, P100
   Jamieson K, 2014, INF SCI SYST CISS 20, P1
   Jamieson K., 2014, COLT
   Kale S., 2010, NIPS
   Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Kaufmann E., 2013, COLT
   Kveton B., 2014, UAI
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lin T., 2014, ICML
   Mannor S, 2004, J MACH LEARN RES, V5, P623
   Neu G., 2010, COLT 2010, P231
   Oxley J. G., 2006, MATROID THEORY
   Pollard David, 2000, ASYMPTOPIA UNPUB
   Rivasplata  Omar, 2012, SUBGAUSSIAN RANDOM V
   Ross S. M., 1996, STOCHASTIC PROCESSES, V2
   Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039
   Zhou Y., 2014, ICML
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101104
DA 2019-06-15
ER

PT S
AU Chen, WZ
   Wang, ZH
   Zhou, JR
AF Chen, Weizhu
   Wang, Zhenghao
   Zhou, Jingren
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Large-scale L-BFGS using MapReduce
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.
C1 [Chen, Weizhu; Wang, Zhenghao; Zhou, Jingren] Microsoft, Albuquerque, NM 87107 USA.
RP Chen, WZ (reprint author), Microsoft, Albuquerque, NM 87107 USA.
EM wzchen@microsoft.com; zhwang@microsoft.com; jrzhou@microsoft.com
CR Agarwal A, 2014, J MACH LEARN RES, V15, P1111
   Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI DOI 10.1145/1273496.1273501
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Chu C., 2007, ADV NEURAL INF PROCE, V19, P281, DOI DOI 10.1234/12345678
   Daume III H., 2004, NOTES CG LM BFGS OPT
   Dean J., 2012, ADV NEURAL INFORM PR, P1232
   Dean J, 2008, COMMUN ACM, V51, P107, DOI 10.1145/1327452.1327492
   Gopal S, 2013, P 30 INT C MACH LEAR, V28, P287
   Graepel T., 2010, P 27 INT C MACH LEAR, P13
   Ling C.X., 2003, P 18 INT C ART INT I, P329
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Low Y, 2010, UNCERTAINTY ARTIFICI
   NASH SG, 1989, MATH PROGRAM, V45, P529, DOI 10.1007/BF01589117
   Nocedal J, 1999, SPRINGER SERIES OPER, V43
   Nocedal Jorge, 1980, UPDATING QUASINEWTON
   Schraudolph N. N., 2007, P 11 INT C ART INT S, P436
   Shanno DF, 1985, J OPTIMIZATION THEOR
   Teo C, 2007, ACM SIGKDD C KNOWL D
   Weinberger K, 2009, INT C MACH LEARN
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
   Zinkevich M., 2009, ADV NEURAL INFORM PR, P2331
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101004
DA 2019-06-15
ER

PT S
AU Chen, XJ
   Yuille, A
AF Chen, Xianjie
   Yuille, Alan
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Articulated Pose Estimation by a Graphical Model with Image Dependent
   Pairwise Relations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.
C1 [Chen, Xianjie; Yuille, Alan] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
RP Chen, XJ (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
EM cxj@ucla.edu; yuille@stat.ucla.edu
FU  [ONR MURI N000014-10-1-0933];  [ONR N00014-12-1-0883];  [ARO 62250-CS]
FX This research has been supported by grants ONR MURI N000014-10-1-0933,
   ONR N00014-12-1-0883 and ARO 62250-CS. The GPUs used in this research
   were generously donated by the NVIDIA Corporation.
CR Chen X., 2014, COMPUTER VISION PATT
   Cho N.-G., 2013, PATTERN RECOGNITION
   Dalal N., 2005, COMPUTER VISION PATT
   Eichner M., 2012, INT J COMPUTER VISIO
   Eichner M., 2012, AS C COMP VIS ACCV
   Felzenszwalb P. F., 2005, INT J COMPUTER VISIO
   Ferrari V., 2008, COMPUTER VISION PATT
   Fischler M. A., 1973, IEEE T COMPUTERS
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Johnson S., 2010, BRIT MACH VIS C BMVC
   Karlinsky L., 2012, EUR C COMP VIS ECCV
   Krizhevsky Alex, 2012, NEURAL INFORM PROCES
   Lafferty J. D., 2001, INT C MACH LEARN ICM
   Ouyang W., 2014, COMPUTER VISION PATT
   Pishchulin L., 2013, INT C COMP VIS ICCV
   Pishchulin L., 2013, COMPUTER VISION PATT
   Ramanan D., 2006, NEURAL INFORM PROCES
   Rother C., 2004, ACM T GRAPHICS TOG
   Sapp B., 2013, COMPUTER VISION PATT
   Sapp B., 2010, EUR C COMP VIS ECCV
   Sapp B., 2010, COMPUTER VISION PATT
   Sermanet P., 2014, INT C LEARN REPR ICL
   Toshev A., 2014, COMPUTER VISION PATT
   Tsochantaridis I., 2004, INT C MACH LEARN ICM
   Wang C., 2013, COMPUTER VISION PATT
   Yang Y, 2013, IEEE MTT S INT MICR
   Yang Y., 2011, COMPUTER VISION PATT
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100070
DA 2019-06-15
ER

PT S
AU Chen, X
   Cheng, XY
   Mallat, S
AF Chen, Xu
   Cheng, Xiuyuan
   Mallat, Stephane
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Unsupervised Deep Haar Scattering on Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.
C1 [Chen, Xu] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
   [Chen, Xu; Cheng, Xiuyuan; Mallat, Stephane] Ecole Normale Super, Dept Informat, Paris, France.
RP Chen, X (reprint author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
FU ERC [InvariantClass 320959]
FX This work was supported by the ERC grant InvariantClass 320959.
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bruna J., 2014, ICLR
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Edmonds J., 1965, CANADIAN J MATH
   Gavish M., 2010, P 27 INT C MACH LEAR, P367
   Goodfellow I.J., 2013, ARXIV13024389
   Hein M, 2011, NIPS
   Hinton G. E, 2012, ARXIV12070580
   Le  Q., 2013, ICML
   LeCun Y., 2010, P IEEE INT SUMP CIRC
   Mallat S., 2010, P EUSICO C DENM
   Mehmood T, 2012, CHEMOMETR INTELL LAB, V118, P62, DOI 10.1016/j.chemolab.2012.07.010
   Rothberg E., JACM, V23
   Roux N. L., 2008, ADV NEURAL INF PROCE, V20, P841
   Rustamov R., 2013, NIPS
   Schwartz W. R., 2009, COMPUTER VISION ICCV
   Shuman D., 2013, IEEE SIGNAL P MA MAY
   Yu D., 2011, P INTERSPEECH, P2285
   Zhang H., 2014, SIGN PROC C EUSIPCO
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102106
DA 2019-06-15
ER

PT S
AU Choi, JH
   Vishwanathan, SVN
AF Choi, Joon Hee
   Vishwanathan, S. V. N.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI DFacTo: Distributed Factorization of Tensors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two sparse matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.
C1 [Choi, Joon Hee] Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Vishwanathan, S. V. N.] Purdue Univ, Stat & Comp Sci, W Lafayette, IN 47907 USA.
RP Choi, JH (reprint author), Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM choi240@purdue.edu; vishy@stat.purdue.edu
CR Acar E, 2011, J CHEMOMETR, V25, P67, DOI 10.1002/cem.1335
   Acar Evrim, 2011, MLG 11 P MIN LEARN G
   Bader BW, 2007, SIAM J SCI COMPUT, V30, P205, DOI 10.1137/060676489
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bernstein D. S., 2005, MATRIX MATH
   Carlson A., 2010, P C ART INT AAAI
   Horn R. A., 1990, MATRIX ANAL
   Kang U, 2012, P 18 ACM SIGKDD INT, P316, DOI DOI 10.1145/2339530.2339583
   Karatzoglou Alexandros, 2010, P 4 ACM C REC SYST R
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893
   McAuley J., 2013, P 7 ACM C REC SYST, V13, P165, DOI DOI 10.1145/2507157.2507163
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Smilde A., 2004, MULTIWAY ANAL APPL C
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101066
DA 2019-06-15
ER

PT S
AU Chow, Y
   Ghavamzadeh, M
AF Chow, Yinlam
   Ghavamzadeh, Mohammad
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Algorithms for CVaR Optimization in MDPs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID VALUE-AT-RISK; STOCHASTIC-APPROXIMATION; DECISION; COST
AB In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.
C1 [Chow, Yinlam] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.
   [Chow, Yinlam; Ghavamzadeh, Mohammad] Adobe Res, San Jose, CA USA.
   [Ghavamzadeh, Mohammad] INRIA Lille, Team SequeL, Lille, France.
RP Chow, Y (reprint author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.
CR Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068
   Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0
   Bardou O, 2009, MONTE CARLO METHODS, V15, P173, DOI 10.1515/MCMA.2009.011
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Bhatnagar S., 2013, STOCHASTIC RECURSIVE, V434
   Bhatnagar S., 2008, ADV NEURAL INFORM PR, V20, P105
   Bhatnagar S, 2010, SYST CONTROL LETT, V59, P760, DOI 10.1016/j.sysconle.2010.08.013
   Bhatnagar S, 2009, AUTOMATICA, V45, P2471, DOI 10.1016/j.automatica.2009.07.008
   Boda K, 2006, MATH METHOD OPER RES, V63, P169, DOI 10.1007/s00186-005-0045-1
   Borkar V., 2014, IEEE T AUTOMATIC CON
   Borkar VS, 2002, MATH OPER RES, V27, P294, DOI 10.1287/moor.27.2.294.324
   Borkar VS, 2001, SYST CONTROL LETT, V44, P339, DOI 10.1016/S0167-6911(01)00152-9
   Chow Y., 2014, ARXIV14063339
   FILAR JA, 1995, IEEE T AUTOMAT CONTR, V40, P2, DOI 10.1109/9.362904
   FILAR JA, 1989, MATH OPER RES, V14, P147, DOI 10.1287/moor.14.1.147
   Ghavamzadeh M., 2013, ADV NEURAL INFORM PR, V26, P252
   HOWARD RA, 1972, MANAGE SCI, V18, P356, DOI 10.1287/mnsc.18.7.356
   Markowitz H. M., 1959, PORTFOLIO SELECTION
   Ott J., 2010, THESIS
   Peters J, 2005, LECT NOTES ARTIF INT, V3720, P280
   Petrik M., 2012, P 28 INT C UNC ART I
   Rockafellar RT, 2002, J BANK FINANC, V26, P1443, DOI 10.1016/S0378-4266(02)00271-6
   SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832
   SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632
   Tamar A., 2012, P 29 INT C MACH LEAR, P387
   Tamar A., 2014, ARXIV14043862V1
   Tanaka T., 2010, P 27 INT C MACH LEAR, P799
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100025
DA 2019-06-15
ER

PT S
AU Chwialkowski, K
   Sejdinovic, D
   Gretton, A
AF Chwialkowski, Kacper
   Sejdinovic, Dino
   Gretton, Arthur
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Wild Bootstrap for Degenerate Kernel Tests
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID U-STATISTICS
AB A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and nondegenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler.
C1 [Chwialkowski, Kacper] UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.
   [Sejdinovic, Dino; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.
RP Chwialkowski, K (reprint author), UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.
EM kacper.chwialkowski@gmail.com; dino.sejdinovic@gmail.com;
   arthur.gretton@gmail.com
CR Arcones MA, 1998, ELECTRON COMMUN PROB, V3, P13, DOI 10.1214/ECP.v3-988
   Bauwens L, 2006, J APPL ECONOM, V21, P79, DOI 10.1002/jae.842
   Berlinet A, 2004, REPRODUCING KERNEL H
   Besserve M., 2013, ADV NEURAL INFORM PR, P2535
   Borisov I.S., 2008, SIBERIAN ADV MATH, V11, p[25, 242]
   Borovkova S, 2001, T AM MATH SOC, V353, P4261, DOI 10.1090/S0002-9947-01-02819-7
   Bradley R., 2005, PROBAB SURV, V2, P37
   Chwialkowski K., 2014, ICML
   Chwialkowski Kacper, 2014, ARXIV14085404
   Dedecker J, 2005, PROBAB THEORY REL, V132, P203, DOI 10.1007/s00440-004-0394-3
   Dedecker J., 2007, WEAK DEPENDENCE EXAM, V190
   DOUKHAN P, 1994, MIXING
   Dudley Richard M., 2002, REAL ANAL PROBABILIT, V74
   Fukumizu K., 2007, ADV NIPS, P489
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Gretton A., 2007, NIPS, P585
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Harchaoui Zaid, 2008, NIPS
   Hehrmann P., 2011, THESIS
   Leucht A, 2013, J MULTIVARIATE ANAL, V117, P257, DOI 10.1016/j.jmva.2013.03.003
   Leucht A, 2012, BERNOULLI, V18, P552, DOI 10.3150/11-BEJ354
   Lyons R, 2013, ANN PROBAB, V41, P3051
   PICKANDS J, 1975, ANN STAT, V3, P119
   Sejdinovic D., 2014, ICML
   Sejdinovic D., 2013, ADV NEURAL INFORM PR, P1124
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   SERFLING RJ., 1980, APPROXIMATION THEORE
   Shao XF, 2010, J AM STAT ASSOC, V105, P218, DOI 10.1198/jasa.2009.tm08744
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Sugiyama M, 2011, NEURAL NETWORKS, V24, P735, DOI 10.1016/j.neunet.2011.04.003
   Zhang K, 2011, P 27 C UNC ART INT U, P804
   Zhang X., 2008, NIPS, V22
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102013
DA 2019-06-15
ER

PT S
AU Cohen, H
   Crammer, K
AF Cohen, Haim
   Crammer, Koby
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Multiple Tasks in Parallel with a Shared Annotator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ALGORITHMS
AB We introduce a new multi-task framework, in which K online learners are sharing a single annotator with limited bandwidth. On each round, each of the K learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the K inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and then we proceed to the next round. We develop an online algorithm for multi-task binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allow to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially achieves more (accuracy) for the same labour of the annotator.
C1 [Cohen, Haim; Crammer, Koby] Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
RP Cohen, H (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM hcohen@tx.technion.ac.il; koby@ee.technion.ac.il
CR Agarwal Alekh, 2008, UCBEECS2008138
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Avner Orly, 2012, ICML
   Cavallanti G, 2010, J MACH LEARN RES, V11, P2901
   Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P31
   Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P1205
   Cesa-Bianchi Nicolo, 2009, ICML
   Crammer K, 2013, MACH LEARN, V90, P347, DOI 10.1007/s10994-012-5321-8
   Crammer K, 2012, J MACH LEARN RES, V13, P1891
   Crammer Koby, 2012, ADV NEURAL INFORM PR
   Daume III Hal, 2010, DANLP 2010
   Dekel O, 2010, COLT, P346
   Dekel O, 2006, LECT NOTES ARTIF INT, V4005, P453, DOI 10.1007/11776420_34
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   Furnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605
   Hazan Elad, 2011, ADV NEURAL INFORM PR
   Kakade S., 2008, P 25 INT C MACH LEAR, V307, P440
   Lin  Hui, 2009, 10 ANN C INT SPEECH
   Lugosi Gabor, 2009, COLT
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Shivaswamy Pannagadatta K., 2011, CORR
   Tong S., 2000, P 17 INT C MACH LEAR, P999
   Yu Jia Yuan, 2009, ICML
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Yue Yisong, 2009, COLT
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103080
DA 2019-06-15
ER

PT S
AU Conejo, B
   Komodakis, N
   Leprince, S
   Avouac, JP
AF Conejo, Bruno
   Komodakis, Nikos
   Leprince, Sebastien
   Avouac, Jean Philippe
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Inference by Learning: Speeding-up Graphical Model Optimization via a
   Coarse-to-Fine Cascade of Pruning Classifiers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or in short as IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line [4].
C1 [Conejo, Bruno; Leprince, Sebastien; Avouac, Jean Philippe] CALTECH, GPS Div, Pasadena, CA 91125 USA.
   [Conejo, Bruno; Komodakis, Nikos] Univ Paris Est, Ecole Ponts ParisTech, Marne La Vallee, France.
RP Conejo, B (reprint author), CALTECH, GPS Div, Pasadena, CA 91125 USA.
EM bconejo@caltech.edu; nikos.komodakis@enpc.fr; leprincs@caltech.edu;
   avouac@gps.caltech.edu
FU USGS; LiDAR project (USGS Award) [G13AP00037]; Terrestrial Hazard
   Observation and Reporting Center of Caltech; Moore foundation through
   the Advanced Earth Surface Observation Project (AESOP) [2808]
FX This work was supported by USGS through the Measurements of surface
   ruptures produced by continental earthquakes from optical imagery and
   LiDAR project (USGS Award G13AP00037), the Terrestrial Hazard
   Observation and Reporting Center of Caltech, and the Moore foundation
   through the Advanced Earth Surface Observation Project (AESOP Grant
   2808).
CR Baker S., 2007, ICCV 2007
   Bergtholdt M., 2010, IJCV
   Boykov Y., 2001, PAMI
   Conejo B., 2014, ISPRS ANN PHOTOGRAMM
   Felzenszwalb P.F., 2004, CVPR
   Felzenszwalb P.F., 2005, IJCV
   Felzenszwalb P. F., 2004, IJCV
   Freeman W.T., 1999, ICCV
   Hu Xiaoyan, 2012, PAMI
   Kappes Jorg Hendrik, 2013, CVPR
   Kim J., 2003, ICCV
   Kim S., 2014, IMAGE SEGMENTATION U
   Kim Taesup, 2011, CVPR
   Kohlberger T., 2005, IEEE T INFORM THEORY
   Kohli Pushmeet, 2010, DAGM S
   Kolmogorov V., 2006, PAMI
   Kolmogorov V., 2004, PAMI
   Komodakis N, 2007, CVPR
   KUMAR MP, 2005, CVPR
   Lombaert H., 2005, ICCV 2005
   Meltzer T., 2005, ICCV
   Pawan Kumar M., 2009, UAI
   Perez P., 1996, IEEE T INFORM THEORY
   Roth S., 2005, CVPR
   Rother C, 2007, CVPR
   Shekhovtsov Alexander, 2014, CVPR
   Shi J., 2000, PAMI
   Szeliski R., 2008, PAMI
   Wainwright M.J., 2005, IEEE T INFORM THEORY
NR 29
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101028
DA 2019-06-15
ER

PT S
AU Cui, Z
   Chang, H
   Shan, SG
   Chen, XL
AF Cui, Zhen
   Chang, Hong
   Shan, Shiguang
   Chen, Xilin
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Generalized Unsupervised Manifold Alignment
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID DIMENSIONALITY REDUCTION
AB In this paper, we propose a Generalized Unsupervised Manifold Alignment (GUMA) method to build the connections between different but correlated datasets without any known correspondences. Based on the assumption that datasets of the same theme usually have similar manifold structures, GUMA is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space. The main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsupervised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases. Experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.
C1 [Cui, Zhen; Chang, Hong; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China.
   [Cui, Zhen] Huaqiao Univ, Sch Comp Sci & Technol, Xiamen, Peoples R China.
RP Cui, Z (reprint author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China.
EM zhen.cui@vipl.ict.ac.cn; hong.chang@vipl.ict.ac.cn; sgshan@ict.ac.cn;
   xlchen@ict.ac.cn
FU Natural Science Foundation of China [61272319, 61222211, 61202297,
   61390510]
FX This work is partially supported by Natural Science Foundation of China
   under contracts Nos. 61272319, 61222211, 61202297, and 61390510.
CR Bay H., 2006, ECCV
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Brualdi R.A., 2006, COMBINATORIAL MATRIX
   Cevikalp H., 2010, CVPR
   Cui Z., 2013, CVPR
   Cui Z., IEEE T CYBERNETICS
   Cui Z., 2012, CVPR
   Fernando B., 2013, ICCV
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Gong B., 2012, CVPR
   Gopalan R., 2011, ICCV
   Griffin G., 2007, TECHNICAL REPORT
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Haghighi A., 2008, ACL, P771
   Ham J., 2005, UAI
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hu Y., 2011, CVPR
   Kim T., 2007, T PAMI
   Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1784, DOI 10.1109/TPAMI.2006.223
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Pei YR, 2012, IEEE T PATTERN ANAL, V34, P1658, DOI 10.1109/TPAMI.2011.229
   Quadrianto N., 2009, NIPS
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Saenko K., 2010, ECCV
   Shi Y., 2012, ICML
   Shon A., 2005, NIPS
   Sugar C. A., 2003, J AM STAT ASS, V98
   Wang C., 2009, IJCAI
   Wang C., 2011, IJCAI
   Wang Chang, 2008, ICML
   Wang R., 2008, CVPR
   Wolf L., 2011, CVPR
   Xiong L., 2007, ECML
   Yamaguchi O., 1998, FGR
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103071
DA 2019-06-15
ER

PT S
AU Dasgupta, S
   Kpotufe, S
AF Dasgupta, Sanjoy
   Kpotufe, Samory
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Optimal rates for k-NN density and mode estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID MEAN SHIFT
AB We present two related contributions of independent interest: (1) high-probability finite sample rates for k-NN density estimation, and (2) practical mode estimators-based on k-NN - which attain minimax-optimal rates under surprisingly general distributional conditions.
C1 [Dasgupta, Sanjoy] Univ Calif San Diego, CSE, La Jolla, CA 92093 USA.
   [Kpotufe, Samory] Princeton Univ, ORFE, Princeton, NJ 08544 USA.
   [Kpotufe, Samory] TTI Chicago, Chicago, IL USA.
RP Dasgupta, S (reprint author), Univ Calif San Diego, CSE, La Jolla, CA 92093 USA.
EM dasgupta@eng.ucsd.edu; samory@princeton.edu
CR Arias-Castro Ery, 2013, ESTIMATION GRA UNPUB
   Balakrishnan Sivaraman, 2013, ADV NEURAL INFORM PR, P2679
   Biau G, 2011, ELECTRON J STAT, V5, P204, DOI 10.1214/11-EJS606
   Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169
   Cadre B, 2004, ESAIM-PROBAB STAT, P81
   Chaudhuri K., 2010, ADV NEURAL INFORM PR
   Chaudhuri K., 2014, CONSISTENT PROCEDURE
   Chazal F, 2013, J ACM, V60, DOI 10.1145/2535927
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   CHERNOFF H, 1964, ANN I STAT MATH, V16, P31, DOI 10.1007/BF02868560
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DEVROYE LP, 1977, ANN STAT, V5, P536, DOI 10.1214/aos/1176343851
   Devroye Luc, 1979, CANADIAN J STAT, V7, P159
   DONOHO DL, 1991, ANN STAT, V19, P668, DOI 10.1214/aos/1176348115
   EDDY WF, 1980, ANN STAT, V8, P870, DOI 10.1214/aos/1176345080
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Genovese Christopher, 2013, ARXIV13127567
   GRENANDER U, 1965, ANN MATH STAT, V36, P131, DOI 10.1214/aoms/1177700277
   Grund B, 1995, ANN STAT, V23, P2264
   Klemela J, 2005, J NONPARAMETR STAT, V17, P83, DOI 10.1080/10485250410001723151
   Kpotufe S., 2011, INT C MACH LEARN
   Li Jia, 2007, J MACHINE LEARNING R, V8
   LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079
   Maier M, 2009, THEOR COMPUT SCI, V410, P1749, DOI 10.1016/j.tcs.2009.01.009
   Moore David S, 1976, TECHNICAL REPORT
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Tsybakov A. B., 1990, PROBL PEREDACHI INF, V26, P38
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101058
DA 2019-06-15
ER

PT S
AU Dekel, O
   Hazan, E
   Koren, T
AF Dekel, Ofer
   Hazan, Elad
   Koren, Tomer
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI The Blinded Bandit: Learning with Adaptive Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID REGRET
AB We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of adaptive feedback naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the blinded multi-armed bandit, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.
C1 [Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA.
   [Hazan, Elad; Koren, Tomer] Technion, Haifa, Israel.
RP Dekel, O (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM oferd@microsoft.com; ehazan@ie.technion.ac.il; tomerk@technion.ac.il
FU Microsoft-Technion EC center; European Union's Seventh Framework
   Programme (FP7/2007-2013]) [336078 ERC-SUBLRN]
FX The research leading to these results has received funding from the
   Microsoft-Technion EC center, and the European Union's Seventh Framework
   Programme (FP7/2007-2013]) under grant agreement no 336078 ERC-SUBLRN.
CR Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Antos A., 2012, THEORETICAL COMPUTER
   Arora R., 2012, P 29 INT C MACH LEAR
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Awerbuch  B., 2004, P 36 ANN ACM S THEOR, P45
   Bubeck S., 2012, P 25 ANN C LEARN THE, V23
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N, 2005, IEEE T INFORM THEORY, V51, P2152, DOI 10.1109/TIT.2005.847729
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   DANI V, 2006, P 17 ANN ACM SIAM S
   Dani Varsha, 2007, ADV NEURAL INFORM PR, P345
   Dekel O., 2013, ARXIV13102997
   Hazan E., 2013, ARXIV13126214
   Kohavi R, 2009, DATA MIN KNOWL DISC, V18, P140, DOI 10.1007/s10618-008-0114-1
   Kohavi Ron, 2012, P 18 ACM SIGKDD INT, P786, DOI DOI 10.1145/2339530.2339653
   Mesterharm C., 2005, P 16 INT C ALG LEARN
   Neu Gergely, 2010, ADV NEURAL INFORM PR, P1804
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102088
DA 2019-06-15
ER

PT S
AU Denton, E
   Zaremba, W
   Bruna, J
   LeCun, Y
   Fergus, R
AF Denton, Emily
   Zaremba, Wojciech
   Bruna, Joan
   LeCun, Yann
   Fergus, Rob
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Exploiting Linear Structure Within Convolutional Networks for Efficient
   Evaluation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model.
C1 [Denton, Emily; Zaremba, Wojciech; Bruna, Joan; LeCun, Yann; Fergus, Rob] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
RP Denton, E (reprint author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
EM denton@cs.nyu.edu; zaremba@cs.nyu.edu; bruna@cs.nyu.edu;
   lecun@cs.nyu.edu; fergus@cs.nyu.edu
FU ONR [N00014-13-1-0646]; NSF [1116923, 1149633]; Microsoft Research
FX The authors are grateful for support from ONR #N00014-13-1-0646, NSF
   #1116923, #1149633 and Microsoft Research.
CR Deng J., 2009, CVPR09
   Denil M., 2013, ARXIV13060543
   Guennebaud G., 2010, EIGEN V3
   Hinton G. E, 2012, ARXIV12070580
   Jaderberg M., 2014, ARXIV14053866
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Le Q. V., 2010, ADV NEURAL INFORM PR
   Le Q. V., 2011, ARXIV11126209
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mathieu Michael, 2013, ARXIV13125851
   Sermanet P., 2013, ARXIV13126229
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA
   Zeiler M. D., 2013, ARXIV13112901
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang T, 2001, SIAM J MATRIX ANAL A, V23, P534, DOI 10.1137/S0895479899352045
NR 15
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102105
DA 2019-06-15
ER

PT S
AU Derezinski, M
   Warmuth, MK
AF Derezinski, Michal
   Warmuth, Manfred K.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI The limits of squared Euclidean distance regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID BOUNDS
AB Some of the simplest loss functions considered in Machine Learning are the square loss, the logistic loss and the hinge loss. The most common family of algorithms, including Gradient Descent (GD) with and without Weight Decay, always predict with a linear combination of the past instances. We give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal. Our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function.
   This type of result was known for the square loss. However, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above). We also show that algorithms that regularize with the squared Euclidean distance are easily confused by random features. Finally, we conclude by discussing related open problems regarding feed forward neural networks. We conjecture that our hardness results hold for any training algorithm that is based on the squared Euclidean distance regularization (i.e. Back-propagation with the Weight Decay heuristic).
C1 [Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Comp Sci Dept, Santa Cruz, CA 95064 USA.
RP Derezinski, M (reprint author), Univ Calif Santa Cruz, Comp Sci Dept, Santa Cruz, CA 95064 USA.
EM mderezin@soe.ucsc.edu; manfred@cse.ucsc.edu
FU NSF [IIS-1118028]
FX This research was supported by the NSF grant IIS-1118028.
CR Alon N., 1985, P 26 S FDN COMP SCI, P277
   Balcan Maria- Florina, 2008, P 21 ANN C LEARN THE, P287
   BENDAVID S, 2002, J MACHINE LEARNING R, V3, P441
   Forster J, 2006, THEOR COMPUT SCI, V350, P40, DOI 10.1016/j.tcs.2005.10.015
   Forster J, 2002, LECT NOTES ARTIF INT, V2533, P128
   Helmbold DP, 1999, IEEE T NEURAL NETWOR, V10, P1291, DOI 10.1109/72.809075
   Hinton G. E, 2012, ABS12070580 CORR
   KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3
   Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Ng A.Y., 2004, P 21 INT C MACH LEAR, P615
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Srebro N, 2004, THESIS
   Warmuth M. K., 2014, J THEORETICAL COMPUT
   WARMUTH MK, 2005, P 18 ANN C LEARN THE
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102001
DA 2019-06-15
ER

EF
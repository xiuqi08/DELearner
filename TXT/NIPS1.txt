FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Zhou, DY
   Bousquet, O
   Lal, TN
   Weston, J
   Scholkopf, B
AF Zhou, DY
   Bousquet, O
   Lal, TN
   Weston, J
   Scholkopf, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning with local and global consistency
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Zhou, DY (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
RI Scholkopf, Bernhard/A-7570-2013
OI Scholkopf, Bernhard/0000-0002-8177-0925
CR Anderson J.R., 1983, ARCHITECTURE COGNITI
   BELKIN M, IN PRESS MACHINE LEA
   Blum A., 2001, ICML
   CHAPELLE O, 2002, NIPS
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Joachims T., 2003, ICML
   Kandola J., 2002, NIPS
   Kondor R. I., 2002, ICML
   NG A., 2001, NIPS
   Seeger M., 2000, LEARNING LABELED UNL
   SHRAGER J, 1987, SCIENCE, V236, P1092, DOI 10.1126/science.236.4805.1092
   SMOLA A, 2003, LEARNING THEORY KERN
   Szummer M., 2001, NIPS, p[2369, 2370, 2371]
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Zhu  X., 2003, ICML
NR 15
TC 932
Z9 1016
U1 1
U2 25
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 321
EP 328
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500041
DA 2019-06-15
ER

PT S
AU He, XF
   Niyogi, P
AF He, XF
   Niyogi, P
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Locality preserving projections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID RECOGNITION; EIGENFACES
AB Many problems in information processing involve some form of dimensionality reduction. In this paper, we introduce Locality Preserving Projections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Component Analysis (PCA) - a classical linear technique that projects the data along the directions of maximal variance. When the high dimensional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points. This is borne out by illustrative examples on some high dimensional data sets.
C1 Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
RP He, XF (reprint author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
EM xiaofei@cs.uchicago.edu; niyogi@cs.uchicago.edu
CR Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BELKIN M, 2002, ADV NEURAL INFORMATI, V14
   Blake C., 1998, UCI REPOSITORY MACHI
   Chung F. R. K., 1997, REGIONAL C SERIES MA, V92
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   *YAL U, FAC DAT
NR 8
TC 469
Z9 480
U1 2
U2 18
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 153
EP 160
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500020
DA 2019-06-15
ER

PT J
AU Thomson, AM
   Bannister, AP
AF Thomson, AM
   Bannister, AP
TI Interlaminar connections in the neocortex
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, CO
ID PRIMARY VISUAL-CORTEX; DUAL INTRACELLULAR-RECORDINGS; CAT STRIATE
   CORTEX; RAT FRONTAL-CORTEX; PRIMARY SOMATOSENSORY CORTEX; SPINY STELLATE
   NEURONS; LOCAL-CIRCUIT NEURONS; BOUQUET CELL AXONS; INTRINSIC
   CONNECTIONS; IN-VITRO
AB This review summarizes the local circuit, interlaminar connections in adult mammalian neocortex. These were first demonstrated with anatomical techniques, which indicate some of the exquisite spatial precision present in the circuitry. Details, such as the class(es) of neurons targeted by some of these projections, have begun to be added in studies that combine paired/triple intracellular recordings with dye-filling of connected neurons. Clear patterns are emerging from these studies, with 'forward' projections from layer 4 to 3 and from 3 to 5 targeting both selected pyramidal cells and interneurons, while 'back' projections from layer 5 to 3 and from 3 to 4 target only interneurons. To place these data in a wider context, the major afferent inputs to and efferent outputs from each of the layers are discussed first.
C1 UCL Royal Free & Univ Coll Med Sch, Dept Physiol, London NW3 2PF, England.
RP Thomson, AM (reprint author), UCL Royal Free & Univ Coll Med Sch, Dept Physiol, Rowland Hill St, London NW3 2PF, England.
EM alext@rfc.ucl.ac.uk
CR AHMED B, 1994, J COMP NEUROL, V341, P39, DOI 10.1002/cne.903410105
   Ali AB, 1998, J PHYSIOL-LONDON, V507, P185, DOI 10.1111/j.1469-7793.1998.185bu.x
   ANDERSON JC, 1994, J COMP NEUROL, V341, P25, DOI 10.1002/cne.903410104
   BLASCOIBANEZ JM, 1995, EUR J NEUROSCI, V7, P2170, DOI 10.1111/j.1460-9568.1995.tb00638.x
   BODEGREUEL KM, 1987, EXP BRAIN RES, V69, P213
   BURKHALTER A, 1989, J COMP NEUROL, V279, P171, DOI 10.1002/cne.902790202
   CastroAlamancos MA, 1997, PROG NEUROBIOL, V51, P581, DOI 10.1016/S0301-0082(97)00002-6
   CATSMANBERREVOETS CE, 1978, BRAIN RES, V154, P359, DOI 10.1016/0006-8993(78)90706-0
   CONDE F, 1994, J COMP NEUROL, V341, P95, DOI 10.1002/cne.903410109
   Czeiger D, 1997, J COMP NEUROL, V379, P198
   Dantzker JL, 2000, NAT NEUROSCI, V3, P701, DOI 10.1038/76656
   DEFELIPE J, 1992, PROG NEUROBIOL, V39, P563, DOI 10.1016/0301-0082(92)90015-7
   DEFELIPE J, 1992, EUR J NEUROSCI, V4, P46, DOI 10.1111/j.1460-9568.1992.tb00108.x
   delRio MR, 1997, J CHEM NEUROANAT, V13, P243, DOI 10.1016/S0891-0618(97)00050-1
   DEUCHARS J, 1994, J PHYSIOL-LONDON, V478, P423, DOI 10.1113/jphysiol.1994.sp020262
   Ding Y, 1997, VISUAL NEUROSCI, V14, P691, DOI 10.1017/S0952523800012657
   Fairen A, 1984, CEREB CORTEX, V1, P201
   FELDMAN ML, 1978, J COMP NEUROL, V179, P761, DOI 10.1002/cne.901790406
   FREUND TF, 1996, HIPPOCAMPUS, V6, P345
   Fujita I, 1996, J COMP NEUROL, V368, P467, DOI 10.1002/(SICI)1096-9861(19960513)368:4<467::AID-CNE1>3.0.CO;2-2
   Gabbott PLA, 1997, J COMP NEUROL, V377, P465, DOI 10.1002/(SICI)1096-9861(19970127)377:4<465::AID-CNE1>3.0.CO;2-0
   GILBERT CD, 1979, NATURE, V280, P120, DOI 10.1038/280120a0
   GILBERT CD, 1983, ANNU REV NEUROSCI, V6, P217, DOI 10.1146/annurev.ne.06.030183.001245
   GILBERT CD, 1983, J NEUROSCI, V3, P1116
   Gonchar Y, 1999, CEREB CORTEX, V9, P683, DOI 10.1093/cercor/9.7.683
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hajos F, 1997, NEUROSCI LETT, V228, P179, DOI 10.1016/S0304-3940(97)00399-6
   Hirsch JA, 1998, J NEUROSCI, V18, P8086
   Jones EG, 2001, TRENDS NEUROSCI, V24, P595, DOI 10.1016/S0166-2236(00)01922-6
   Kawaguchi Y, 1997, CEREB CORTEX, V7, P476, DOI 10.1093/cercor/7.6.476
   KAWAGUCHI Y, 1993, J NEUROPHYSIOL, V70, P387
   Kawaguchi Y, 1998, NEUROSCIENCE, V85, P677, DOI 10.1016/S0306-4522(97)00685-4
   KELLER A, 1993, CEREB CORTEX, V3, P430, DOI 10.1093/cercor/3.5.430
   KISVARDAY ZF, 1990, BRAIN, V113, P793, DOI 10.1093/brain/113.3.793
   KISVARDAY ZF, 1986, EXP BRAIN RES, V64, P541, DOI 10.1007/BF00340492
   KRITZER MF, 1995, J COMP NEUROL, V359, P131, DOI 10.1002/cne.903590109
   Kubota Y, 1997, BRAIN RES, V752, P175, DOI 10.1016/S0006-8993(96)01446-1
   LEWIS DA, 1990, J COMP NEUROL, V293, P599, DOI 10.1002/cne.902930406
   Lorente de No R, 1922, TRAB LAB BIOL U MADR, V20, P41
   LOWENSTEIN PR, 1991, J COMP NEUROL, V310, P253, DOI 10.1002/cne.903100209
   LUND JS, 1988, ANNU REV NEUROSCI, V11, P253, DOI 10.1146/annurev.ne.11.030188.001345
   LUND JS, 1973, J COMP NEUROL, V147, P455, DOI 10.1002/cne.901470404
   LUND JS, 1993, CEREB CORTEX, V3, P148, DOI 10.1093/cercor/3.2.148
   LUND JS, 1975, J COMP NEUROL, V159, P305, DOI 10.1002/cne.901590303
   MCGUIRE BA, 1991, J COMP NEUROL, V305, P370, DOI 10.1002/cne.903050303
   O'Leary JL, 1941, J COMP NEUROL, V75, P131, DOI 10.1002/cne.900750107
   PARNAVELAS JG, 1977, J ANAT, V124, P305
   Pawelzik H, 2002, J COMP NEUROL, V443, P346, DOI 10.1002/cne.10118
   Peters A, 1997, J NEUROCYTOL, V26, P779, DOI 10.1023/A:1018518515982
   Rockland KS, 1996, J COMP NEUROL, V373, P529
   Rockland KS, 1996, J COMP NEUROL, V368, P57, DOI 10.1002/(SICI)1096-9861(19960422)368:1<57::AID-CNE5>3.0.CO;2-J
   ROCKLAND KS, 1982, SCIENCE, V215, P1532, DOI 10.1126/science.7063863
   SOMOGYI P, 1977, BRAIN RES, V136, P345, DOI 10.1016/0006-8993(77)90808-3
   Somogyi P., 1984, CEREBRAL CORTEX CELL, V1, P337
   SPATZ WB, 1970, J COMP NEUROL, V140, P155, DOI 10.1002/cne.901400203
   Staiger JF, 1996, ANAT EMBRYOL, V194, P533
   Tamas G, 1998, J NEUROSCI, V18, P4255
   Tarczy-Hornoch K, 1999, CEREB CORTEX, V9, P833, DOI 10.1093/cercor/9.8.833
   THOMSON AM, 1995, NEUROSCIENCE, V69, P727, DOI 10.1016/0306-4522(95)00287-S
   Thomson AM, 2002, CEREB CORTEX, V12, P936, DOI 10.1093/cercor/12.9.936
   THOMSON AM, 1993, J NEUROPHYSIOL, V70, P2354
   Thomson AM, 1998, NEUROSCIENCE, V84, P669, DOI 10.1016/S0306-4522(97)00557-5
   Thomson AM, 1996, J PHYSIOL-LONDON, V496, P81, DOI 10.1113/jphysiol.1996.sp021667
   Thomson AM, 1997, CEREB CORTEX, V7, P510, DOI 10.1093/cercor/7.6.510
   VALVERDE F, 1976, J NEUROCYTOL, V5, P509, DOI 10.1007/BF01175566
   VALVERDE F, 1983, RY CAJALS CONTRIBUTI, P149
   Wang Y, 2002, CEREB CORTEX, V12, P395, DOI 10.1093/cercor/12.4.395
   WANG Z, 1993, J NEUROSCI, V13, P2199
   WHITE EL, 1982, J NEUROCYTOL, V11, P137, DOI 10.1007/BF01258009
   WHITE EL, 1986, CEREB CORTEX, V5, P271
   Wiser AK, 1996, J NEUROSCI, V16, P2724
   Yoshioka T., 1994, VISUAL NEUROSCI, V11, P1, DOI [10.1017/S0952523800002406, DOI 10.1017/S0952523800002406]
   Zhang ZW, 1997, J NEUROSCI, V17, P6365
NR 73
TC 282
Z9 287
U1 1
U2 17
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
EI 1460-2199
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 5
EP 14
DI 10.1093/cercor/13.1.5
PG 10
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500003
PM 12466210
DA 2019-06-15
ER

PT J
AU Swadlow, HA
AF Swadlow, HA
TI Fast-spike interneurons and feedforward inhibition in awake sensory
   neocortex
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID GABA-IMMUNOREACTIVE NEURONS; INTRINSIC FIRING PATTERNS; PRIMARY
   VISUAL-CORTEX; MOUSE BARREL CORTEX; CAT STRIATE CORTEX; RAT
   FRONTAL-CORTEX; RECEPTIVE-FIELDS; ORIENTATION SELECTIVITY; SUSPECTED
   INTERNEURONS; SOMATOSENSORY CORTEX
AB 'Fast-spike' interneurons of layer 4 mediate thalamocortical feedforward inhibition and can, with some confidence, be identified using extracellular methods. In somatosensory barrel cortex of awake rabbits, these 'suspected inhibitory interneurons' (SINs) have distinct receptive field properties: they respond to vibrissa displacement with very high sensitivity and temporal fidelity. However, they lack the directional specificity that is clearly seen in most of their ventrobasal thalamocortical afferents. Several lines of evidence show that layer-4 SINs receive a potent and highly convergent and divergent functional input from topographically aligned thalamocortical neurons. Whereas the unselective pooling of convergent thalamocortical inputs onto SINs generates sensitive and broadly tuned inhibitory receptive fields, the potent divergence of single thalamocortical neurons onto many SINs generates sharply synchronous (+/-1 ms) activity (because of coincident EPSPs). Synchronous discharge of these interneurons following thalamocortical impulses will generate a synchronous feedforward release of GABA within the barrel. Thalamocortical impulses will, therefore, generate only a brief 'window of excitability' during which spikes can occur in the post-synaptic targets of fast-spike interneurons. This fast, synchronous, highly sensitive and broadly tuned feed-forward inhibitory network is well suited to suppress spike generation in spiny neurons following all but the most optimal feedforward excitatory inputs.
C1 Univ Connecticut, Dept Psychol, Storrs, CT 06269 USA.
RP Swadlow, HA (reprint author), Univ Connecticut, Dept Psychol, U-20, Storrs, CT 06269 USA.
FU NIMH NIH HHS [MH-64024]
CR Abeles Moshe, 1991, CORTICONICS NEURAL C
   AGMON A, 1992, J NEUROSCI, V12, P319
   Alonso JM, 2001, J NEUROSCI, V21, P4002, DOI 10.1523/JNEUROSCI.21-11-04002.2001
   Alonso JM, 1996, NATURE, V383, P815, DOI 10.1038/383815a0
   AMITAI Y, 2001, SOC NEUR ABSTR
   Amitai Y, 1995, CEREBR CORT, P299
   ANDERSEN P, 1964, J NEUROPHYSIOL, V27, P1080
   ANDERSEN P, 1964, J PHYSIOL-LONDON, V174, P370, DOI 10.1113/jphysiol.1964.sp007493
   ARMSTRONGJAMES M, 1992, J NEUROPHYSIOL, V68, P1345
   Azouz R, 1997, CEREB CORTEX, V7, P534, DOI 10.1093/cercor/7.6.534
   Cauli B, 1997, J NEUROSCI, V17, P3894
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   CONNORS BW, 1986, J NEUROSCI, V6, P164
   Deans MR, 2001, NEURON, V31, P477, DOI 10.1016/S0896-6273(01)00373-7
   DEFELIPE J, 1993, CEREB CORTEX, V3, P273, DOI 10.1093/cercor/3.4.273
   DYKES RW, 1984, J NEUROPHYSIOL, V52, P1066
   DYKES RW, 1988, BRAIN RES, V441, P48, DOI 10.1016/0006-8993(88)91382-0
   Eccles JC., 1957, PHYSL NERVE CELLS
   Fairen A, 1984, CEREB CORTEX, V1, P201
   Feldmeyer D, 1999, J PHYSIOL-LONDON, V521, P169, DOI 10.1111/j.1469-7793.1999.00169.x
   Ferster D, 2000, ANNU REV NEUROSCI, V23, P441, DOI 10.1146/annurev.neuro.23.1.441
   FERSTER D, 1986, J NEUROSCI, V6, P1284
   Galarreta M, 1999, NATURE, V402, P72
   Gibson JR, 1999, NATURE, V402, P75
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   GRIFFITH JS, 1963, BIOPHYS J, V3, P299, DOI 10.1016/S0006-3495(63)86822-8
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   HENDRY SHC, 1987, J NEUROSCI, V7, P1503
   HIRSCH JA, 2000, SOC NEUR ABSTR
   Houser C. R., 1984, CEREB CORTEX, V2, P63
   JENSEN KF, 1987, J NEUROSCI, V7, P3529
   KAWAGUCHI Y, 1993, J NEUROPHYSIOL, V70, P387
   KAWAGUCHI Y, 1993, J NEUROPHYSIOL, V69, P416
   Maldonado PE, 1997, SCIENCE, V276, P1551, DOI 10.1126/science.276.5318.1551
   MARTIN KAC, 1983, EXP BRAIN RES, V50, P193
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782
   MEINECKE DL, 1987, J COMP NEUROL, V261, P388, DOI 10.1002/cne.902610305
   MENDELL LM, 1971, J NEUROPHYSIOL, V34, P171
   Miller LM, 2001, NEURON, V32, P151, DOI 10.1016/S0896-6273(01)00445-7
   MOORE GP, 1970, BIOPHYS J, V10, P876, DOI 10.1016/S0006-3495(70)86341-X
   MOUNTCASTLE VB, 1969, J NEUROPHYSIOL, V32, P452
   NELSON S, 1994, SCIENCE, V265, P774, DOI 10.1126/science.8047882
   Porter JT, 2001, J NEUROSCI, V21, P2699, DOI 10.1523/JNEUROSCI.21-08-02699.2001
   PRIETO JJ, 1994, J COMP NEUROL, V344, P349, DOI 10.1002/cne.903440304
   REID RC, 1995, NATURE, V378, P281
   SEARS TA, 1976, J PHYSIOL-LONDON, V263, P357, DOI 10.1113/jphysiol.1976.sp011635
   SILLITO AM, 1984, CEREB CORTEX, V2, P91
   SIMONS DJ, 1978, J NEUROPHYSIOL, V41, P798
   SIMONS DJ, 1989, J NEUROPHYSIOL, V61, P311
   Swadlow HA, 2001, NAT NEUROSCI, V4, P402, DOI 10.1038/86054
   SWADLOW HA, 1988, J NEUROPHYSIOL, V59, P1162
   SWADLOW HA, 1995, J NEUROPHYSIOL, V73, P1584
   SWADLOW HA, 1994, J NEUROPHYSIOL, V71, P437
   Swadlow HA, 1998, J NEUROPHYSIOL, V79, P567
   SWADLOW HA, 1989, J NEUROPHYSIOL, V62, P288
   Swadlow HA, 2002, NAT NEUROSCI, V5, P403, DOI 10.1038/nn847
   SWADLOW HA, 1991, J NEUROPHYSIOL, V66, P1392
   SWADLOW HA, 1990, J NEUROPHYSIOL, V63, P1477
   SWADLOW HA, 2000, J NEUROPHYSIOL, V83, P2803
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   Vidyasagar TR, 1996, TRENDS NEUROSCI, V19, P272, DOI 10.1016/S0166-2236(96)20027-X
   White E, 1989, CORTICAL CIRCUITS
   WHITE EL, 1981, J COMP NEUROL, V195, P265, DOI 10.1002/cne.901950207
   WHITE EL, 1986, CEREB CORTEX, V5, P271
NR 64
TC 238
Z9 242
U1 0
U2 3
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 25
EP 32
DI 10.1093/cercor/13.1.25
PG 8
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500005
PM 12466212
OA Bronze
DA 2019-06-15
ER

PT S
AU Bengio, Y
   Paiement, JFO
   Vincent, P
   Delalleau, O
   Le Roux, N
   Ouimet, M
AF Bengio, Y
   Paiement, JFO
   Vincent, P
   Delalleau, O
   Le Roux, N
   Ouimet, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Out-of-sample extensions for LLE, isomap, MDS, eigenmaps, and spectral
   clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NONLINEAR DIMENSIONALITY REDUCTION
AB Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data.
C1 Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.
RP Bengio, Y (reprint author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.
CR Baker C.T.H., 1977, NUMERICAL TREATMENT
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   BENGIO Y, 2003, SPECTRAL CLUSTERING
   Cox T. F., 1994, MULTIDIMENSIONAL SCA
   GOWER JC, 1968, BIOMETRIKA, V55, P582, DOI 10.2307/2334268
   Koltchinskii V, 2000, BERNOULLI, V6, P113, DOI 10.2307/3318636
   NG AY, 2002, ADV NEURAL INFORMATI, V14
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Saul L., 2003, J MACHINE LEARNING R, V4, P119
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   SHAWETAYLOR J, 2003, ADV NEURAL INFORMATI, V15
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Silva VD, 2003, ADV NEURAL INFORM PR, P705
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Weiss Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P975, DOI 10.1109/ICCV.1999.790354
   Williams CKI, 2000, P 17 INT C MACH LEAR
NR 16
TC 230
Z9 241
U1 1
U2 13
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 177
EP 184
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500023
DA 2019-06-15
ER

PT S
AU Fanti, C
   Polito, M
   Perona, P
AF Fanti, C
   Polito, M
   Perona, P
BE Thrun, S
   Saul, K
   Scholkopf, B
TI An improved scheme for detection and labelling in Johansson displays
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MODEL
AB Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can effortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body's centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more significantly, to the introduction of the centroid variable.
C1 CALTECH, Computat Vis Lab, Pasadena, CA 91125 USA.
RP Fanti, C (reprint author), CALTECH, Computat Vis Lab, Pasadena, CA 91125 USA.
CR Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   FREEMAN WT, 2001, IEEE T INFORMATION T, V47, P723
   Giudici P, 2003, MACH LEARN, V50, P127, DOI 10.1023/A:1020202028934
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   SONG Y, 2001, ADV NEURAL INFORMATI, V14
   SONG Y, 2000, P ECCV, V2, P719
   SONG Y, 2001, P IEEE C COMP VIS PA, V2, P771
   Tomasi C., 1991, CMUCS91132
   YEDIDIA JS, 2000, ADV NEURAL INFORMATI, V13
NR 10
TC 218
Z9 225
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1603
EP 1610
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500199
DA 2019-06-15
ER

PT S
AU Zhu, J
   Rosset, S
   Hastie, T
   Tibshirani, R
AF Zhu, J
   Rosset, S
   Hastie, T
   Tibshirani, R
BE Thrun, S
   Saul, K
   Scholkopf, B
TI 1-norm support vector machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID CLASSIFICATION; SELECTION; CANCER
AB The standard 2-norm SVM is known for its good performance in two-class classipoundcation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVNI may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an efpoundcient algorithm that computes the whole solution path of the 1-norm SVNI, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM.
C1 Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Zhu, J (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
CR Bradley Paul S., 1998, ICML 98
   EVGENIOU T, 1999, ADV LARGE MARGIN CLA
   FRIEDMAN J, 2004, IN PRESS ANN STAT
   Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   HASTIE T, 2001, ELEMENTS STAT LEANRI
   MUKHERJEE S, 1999, 1677 MIT
   ROSSET S, 2003, TECHNICAL REPORT DEP
   SONG M, J CHEM INFORMATI SEP
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vapnik VN, 1995, NATURE STAT LEARNING
   Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69
   ZHU J, 2003, IN PRESS CLASSIFICAT
   ZHU J, 2003, THESIS STANFORD U
NR 14
TC 194
Z9 203
U1 0
U2 14
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 49
EP 56
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500007
DA 2019-06-15
ER

PT J
AU Tanaka, K
AF Tanaka, K
TI Columns for complex visual object features in the inferotemporal cortex:
   Clustering of cells with similar but slightly different stimulus
   selectivities
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID INFERIOR TEMPORAL CORTEX; MACAQUE MONKEY; SHAPE REPRESENTATION;
   CARTESIAN GRATINGS; NEURONAL SYNCHRONY; RECEPTIVE-FIELDS; RHESUS-MONKEY;
   AREA TE; DISPARITY; PROJECTIONS
AB Cells in the inferotemporal cortex (area TE) selectively respond to complex visual object features and those that respond to similar features cluster in a columnar region elongated vertical to the cortical surface. What are the functional roles of the column structure in the inferotemporal cortex? Selectivity of cells within a column is similar but not identical. If we emphasize the similarity among cells within a column, we can regard the columns as units for description of object features. The variety of stimulus selectivity in a column may work as a tool to disregard subtle changes in input images when the system is directed to invariant recognition. Alternatively, if we emphasize the differences in selectivity of cells within a column, the columns can be compared to differential amplifiers, each of which represents variety within a group of features. The enormous number of objects present in nature can be efficiently described by combining outputs of the multiple differential amplifiers in the inferotemporal cortex. The two modes may work in parallel, with a graded balance changing according to the behavioral context. Determining whether or not these hypotheses are valid will require further studies.
C1 RIKEN, Brain Res Inst, Wako, Saitama 3510198, Japan.
RP Tanaka, K (reprint author), RIKEN, Brain Res Inst, Wako, Saitama 3510198, Japan.
RI Tanaka, Keiji/N-5917-2015
OI Tanaka, Keiji/0000-0002-1910-3427
CR Barbas H, 1992, Adv Neurol, V57, P91
   BAYLIS GC, 1987, J NEUROSCI, V7, P330
   BRUCE C, 1981, J NEUROPHYSIOL, V46, P369
   Cheng K, 1997, J NEUROSCI, V17, P7902
   Cumming BG, 2001, ANNU REV NEUROSCI, V24, P203, DOI 10.1146/annurev.neuro.24.1.203
   de Beeck Hans Op, 2000, Journal of Comparative Neurology, V426, P505
   DEAN P, 1976, PSYCHOL BULL, V83, P41, DOI 10.1037//0033-2909.83.1.41
   Fujita I, 1996, J COMP NEUROL, V368, P467, DOI 10.1002/(SICI)1096-9861(19960513)368:4<467::AID-CNE1>3.0.CO;2-2
   FUJITA I, 1992, NATURE, V360, P343, DOI 10.1038/360343a0
   GALLANT JL, 1993, SCIENCE, V259, P100, DOI 10.1126/science.8418487
   Gallant JL, 1996, J NEUROPHYSIOL, V76, P2718
   GILBERT CD, 1989, J NEUROSCI, V9, P2432
   Gross C. G., 1973, HDB SENSORY PHYSIOLO, P451
   HOSSEIN E, 1998, SOC NEUR ABSTR, V24, P899
   ITO M, 1995, J NEUROPHYSIOL, V73, P218
   ITO M, 1994, CEREB CORTEX, V5, P499
   Janssen P, 2000, NEURON, V27, P385, DOI 10.1016/S0896-6273(00)00045-3
   Janssen P, 2000, SCIENCE, V288, P2054, DOI 10.1126/science.288.5473.2054
   Janssen P, 1999, P NATL ACAD SCI USA, V96, P8217, DOI 10.1073/pnas.96.14.8217
   Keysers C, 2001, J COGNITIVE NEUROSCI, V13, P90, DOI 10.1162/089892901564199
   KOBATAKE E, 1994, J NEUROPHYSIOL, V71, P856
   Lamme VAF, 1998, NATURE, V396, P362, DOI 10.1038/24608
   Logothetis N, 1998, CURR OPIN NEUROBIOL, V8, P536
   LOGOTHETIS NK, 1995, CURR BIOL, V5, P552, DOI 10.1016/S0960-9822(95)00108-4
   LUESCHOW A, 1994, CEREB CORTEX, V4, P523, DOI 10.1093/cercor/4.5.523
   MAUNSELL JHR, 1983, J NEUROPHYSIOL, V49, P1148
   Mel BW, 2000, NEURAL COMPUT, V12, P731, DOI 10.1162/089976600300015574
   Missal M, 1997, CEREB CORTEX, V7, P758, DOI 10.1093/cercor/7.8.758
   Missal M, 1999, J NEUROPHYSIOL, V82, P131
   Pasupathy A, 2001, J NEUROPHYSIOL, V86, P2505
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   POGGIO GF, 1977, J NEUROPHYSIOL, V40, P1392
   RICHMOND BJ, 1987, J NEUROPHYSIOL, V57, P132
   Saleem KS, 1996, J NEUROSCI, V16, P4757
   SARY G, 1993, SCIENCE, V260, P995, DOI 10.1126/science.8493538
   SATO T, 1995, EXP BRAIN RES, V105, P209
   SATO T, 1989, EXP BRAIN RES, V77, P23
   SCHWARTZ EL, 1983, P NATL ACAD SCI-BIOL, V80, P5776, DOI 10.1073/pnas.80.18.5776
   SELTZER B, 1978, BRAIN RES, V149, P1, DOI 10.1016/0006-8993(78)90584-X
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   SUZUKI WA, 1995, J COMP NEUROL, V349, P1
   Tanaka H, 2001, J NEUROPHYSIOL, V85, P735
   TANAKA K, 1991, J NEUROPHYSIOL, V66, P170
   Tanifuji M., 2001, Society for Neuroscience Abstracts, V27, P1633
   Tanigawa H, 1998, J COMP NEUROL, V401, P129, DOI 10.1002/(SICI)1096-9861(19981109)401:1<129::AID-CNE8>3.0.CO;2-D
   Tsunoda K, 2001, NAT NEUROSCI, V4, P832, DOI 10.1038/90547
   Uka T, 2000, J NEUROPHYSIOL, V84, P120
   UNGERLEIDER LG, 1989, EXP BRAIN RES, V76, P473, DOI 10.1007/BF00248903
   Vogels R, 2001, J COGNITIVE NEUROSCI, V13, P444, DOI 10.1162/08989290152001871
   Wang G, 1998, NEUROSCI RES, V32, P33, DOI 10.1016/S0168-0102(98)00062-5
   Wang G, 1996, SCIENCE, V272, P1665, DOI 10.1126/science.272.5268.1665
   Wang Y, 2000, NAT NEUROSCI, V3, P807
   YAGINUMA S, 1993, BRAIN MECH PERCEPTIO, P1
   Yamane Y., 2001, Society for Neuroscience Abstracts, V27, P1050
   YUKIE M, 1990, VISION, MEMORY AND THE TEMPORAL LOBE, P129
NR 55
TC 185
Z9 187
U1 0
U2 13
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 90
EP 99
DI 10.1093/cercor/13.1.90
PG 10
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500013
PM 12466220
OA Bronze
DA 2019-06-15
ER

PT S
AU Lawrence, ND
AF Lawrence, ND
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Gaussian process latent variable models for visualisation of high
   dimensional data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID PRINCIPAL COMPONENT ANALYSIS; GTM
AB In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.
C1 Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.
RP Lawrence, ND (reprint author), Univ Sheffield, Dept Comp Sci, Regent Court,211 Portobello St, Sheffield S1 4DP, S Yorkshire, England.
CR BECKER S, 2003, ADV NEURAL INFORMATI, V15
   BISHOP CM, 1993, NUCL INSTRUM METH A, V327, P580, DOI 10.1016/0168-9002(93)90728-Z
   Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953
   Bishop CM, 1997, ADV NEUR IN, V9, P354
   Hinton G. E., 2003, ADV NEURAL INFORM PR, P857
   Lawrence N., 2003, ADV NEURAL INFORMATI, V15, P625
   NABNEY I, 2001, NETLAB ALGORITHMS PA
   Roweis S, 2002, ADV NEUR IN, V14, P889
   Scholkopf B., 1997, P INT C ART NEUR NET, P583
   Tipping ME, 2001, ADV NEUR IN, V13, P633
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
NR 11
TC 159
Z9 161
U1 1
U2 7
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 329
EP 336
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500042
DA 2019-06-15
ER

PT J
AU Raizada, RDS
   Grossberg, S
AF Raizada, RDS
   Grossberg, S
TI Towards a theory of the laminar architecture of cerebral cortex:
   Computational clues from the visual system
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID MACAQUE STRIATE CORTEX; LOCAL CIRCUIT NEURONS; INTERMODAL SELECTIVE
   ATTENTION; LATERAL GENICULATE-NUCLEUS; MONKEY PREFRONTAL CORTEX;
   OBJECT-BASED ATTENTION; BAT AUDITORY-SYSTEM; RAT MOTOR CORTEX; INTRINSIC
   CONNECTIONS; HORIZONTAL CONNECTIONS
AB One of the most exciting and open research frontiers in neuroscience is that of seeking to understand the functional roles of the layers of cerebral cortex. New experimental techniques for probing the laminar circuitry of cortex have recently been developed, opening up novel opportunities for investigating how its six-layered architecture contributes to perception and cognition. The task of trying to interpret this complex structure can be facilitated by theoretical analyses of the types of computations that cortex is carrying out, and of how these might be implemented in specific cortical circuits. We have recently developed a detailed neural model of how the parvocellular stream of the visual cortex utilizes its feedforward, feedback and horizontal interactions for purposes of visual filtering, attention and perceptual grouping. This model, called LAMINART, shows how these perceptual processes relate to the mechanisms that ensure the stable development of cortical circuits in the infant, and to the continued stability of learning in the adult. The present article reviews this laminar theory of visual cortex, considers how it may be generalized towards a more comprehensive theory that encompasses other cortical areas and cognitive processes, and shows how its laminar framework generates a variety of testable predictions.
C1 Boston Univ, Dept Cognit & Neural Syst, Boston, MA 02215 USA.
   Harvard Univ, Sch Med, MGH NMR Ctr, Charlestown, MA 02129 USA.
RP Grossberg, S (reprint author), Boston Univ, Dept Cognit & Neural Syst, 677 Beacon St, Boston, MA 02215 USA.
CR AHISSAR M, 1993, P NATL ACAD SCI USA, V90, P5718, DOI 10.1073/pnas.90.12.5718
   Ahmed B, 1997, J COMP NEUROL, V380, P230
   AMIR Y, 1993, J COMP NEUROL, V334, P19, DOI 10.1002/cne.903340103
   AMUNTS K, 1995, ANAT EMBRYOL, V192, P557
   ARONIADOU VA, 1993, J NEUROPHYSIOL, V70, P1553
   BARBAS H, 1989, J COMP NEUROL, V286, P353, DOI 10.1002/cne.902860306
   Barbas H, 1997, CEREB CORTEX, V7, P635, DOI 10.1093/cercor/7.7.635
   BLASDEL GG, 1983, J NEUROSCI, V3, P1389
   BLASDEL GG, 1985, J NEUROSCI, V5, P3350
   Bosking WH, 1997, J NEUROSCI, V17, P2112
   BOWDEN DM, 1995, NEUROIMAGE, V2, P63, DOI 10.1006/nimg.1995.1009
   Bradski G, 1995, NEURAL NETWORKS, V8, P1053, DOI 10.1016/0893-6080(95)00053-4
   Briggs F, 2001, J NEUROSCI, V21, P3600, DOI 10.1523/JNEUROSCI.21-10-03600.2001
   BROWN JW, 2000, CASCNS2000023
   Bullier J, 1996, J PHYSIOLOGY-PARIS, V90, P217, DOI 10.1016/S0928-4257(97)81426-X
   Callaway EM, 1996, VISUAL NEUROSCI, V13, P907, DOI 10.1017/S0952523800009159
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   Caputo G, 1998, VISION RES, V38, P669, DOI 10.1016/S0042-6989(97)00189-2
   CARPENTER GA, 1995, IEEE T NEURAL NETWOR, V6, P805, DOI 10.1109/72.392245
   Chey J, 1997, J OPT SOC AM A, V14, P2570, DOI 10.1364/JOSAA.14.002570
   Dantzker JL, 2000, NAT NEUROSCI, V3, P701, DOI 10.1038/76656
   De Weerd P, 1999, NAT NEUROSCI, V2, P753, DOI 10.1038/11234
   Dombrowski SM, 2001, CEREB CORTEX, V11, P975, DOI 10.1093/cercor/11.10.975
   DOWNING CJ, 1988, J EXP PSYCHOL HUMAN, V14, P188, DOI 10.1037//0096-1523.14.2.188
   Dresp B, 1997, VISION RES, V37, P913, DOI 10.1016/S0042-6989(96)00227-1
   Elder JH, 1998, VISION RES, V38, P143, DOI 10.1016/S0042-6989(97)00138-7
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Ferster D, 1996, NATURE, V380, P249, DOI 10.1038/380249a0
   FINKEL LH, 1989, J NEUROSCI, V9, P3188
   FITZPATRICK D, 1985, J NEUROSCI, V5, P3329
   Fitzpatrick D, 1996, CEREB CORTEX, V6, P329, DOI 10.1093/cercor/6.3.329
   Fung SH, 1998, J NEUROSCI METH, V80, P215, DOI 10.1016/S0165-0270(98)00003-X
   Galuske RAW, 1996, CEREB CORTEX, V6, P417, DOI 10.1093/cercor/6.3.417
   Gao E, 1998, P NATL ACAD SCI USA, V95, P12663, DOI 10.1073/pnas.95.21.12663
   GILBERT CD, 1989, J NEUROSCI, V9, P2432
   GILBERT CD, 1979, NATURE, V280, P120, DOI 10.1038/280120a0
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   GRIEVE KL, 1991, EXP BRAIN RES, V87, P521
   GRIEVE KL, 1995, EXP BRAIN RES, V104, P12
   GRIEVE KL, 1991, EXP BRAIN RES, V84, P319
   GROSOF DH, 1993, NATURE, V365, P550, DOI 10.1038/365550a0
   Grossberg S, 2000, PSYCHOL REV, V107, P735, DOI 10.1037//0033-295X.107.4.735
   Grossberg S, 1999, CONSCIOUS COGN, V8, P1, DOI 10.1006/ccog.1998.0372
   Grossberg S, 1997, TRENDS NEUROSCI, V20, P106, DOI 10.1016/S0166-2236(96)01002-8
   Grossberg S, 1997, J COGNITIVE NEUROSCI, V9, P117, DOI 10.1162/jocn.1997.9.1.117
   Grossberg S, 1999, SPATIAL VISION, V12, P163, DOI 10.1163/156856899X00102
   GROSSBERG S, 1995, AM SCI, V83, P438
   GROSSBERG S, 1994, PERCEPT PSYCHOPHYS, V55, P48, DOI 10.3758/BF03206880
   GROSSBERG S, 1980, PSYCHOL REV, V87, P1, DOI 10.1037/0033-295X.87.1.1
   Grossberg S, 2001, VISION RES, V41, P2521, DOI 10.1016/S0042-6989(01)00131-6
   GROSSBERG S, 1985, PSYCHOL REV, V92, P173, DOI 10.1037/0033-295X.92.2.173
   Grossberg S, 2000, TRENDS COGN SCI, V4, P233, DOI 10.1016/S1364-6613(00)01464-9
   Grossberg S, 2000, J INT NEUROPSYCH SOC, V6, P583, DOI 10.1017/S135561770065508X
   Grossberg S, 2000, VISION RES, V40, P1413, DOI 10.1016/S0042-6989(99)00229-1
   GROSSBERG S, 1991, NEURAL NETWORKS, V4, P453, DOI 10.1016/0893-6080(91)90041-3
   Grossberg S, 2001, CEREB CORTEX, V11, P37, DOI 10.1093/cercor/11.1.37
   GROSSBERG S, 1973, STUD APPL MATH, V52, P217
   HARTH E, 1987, SCIENCE, V237, P184, DOI 10.1126/science.3603015
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   Heitger F, 1998, IMAGE VISION COMPUT, V16, P407, DOI 10.1016/S0262-8856(97)00083-8
   HIRSCH JA, 1991, J NEUROSCI, V11, P1800
   HOWE PDL, 2001, SOC NEUR ABSTR
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   Ito M, 1999, NEURON, V22, P593, DOI 10.1016/S0896-6273(00)80713-8
   Kaas JH, 1999, P NATL ACAD SCI USA, V96, P7622, DOI 10.1073/pnas.96.14.7622
   Kanizsa G, 1979, ORG VISION ESSAYS GE
   Kapadia MK, 2000, J NEUROPHYSIOL, V84, P2048
   KISVARDAY ZF, 1989, J NEUROSCI, V9, P667
   KNIERIM JJ, 1992, J NEUROPHYSIOL, V67, P961
   Kohn A, 2000, NEUROSCIENCE, V95, P51
   Kotter R, 1998, NEUROSCIENCE, V86, P265, DOI 10.1016/S0306-4522(98)00010-4
   Krimer LS, 1997, CEREB CORTEX, V7, P722, DOI 10.1093/cercor/7.8.722
   KRITZER MF, 1995, J COMP NEUROL, V359, P131, DOI 10.1002/cne.903590109
   Krupa DJ, 1999, P NATL ACAD SCI USA, V96, P8200, DOI 10.1073/pnas.96.14.8200
   Lamme VAF, 1999, CEREB CORTEX, V9, P406, DOI 10.1093/cercor/9.4.406
   Lee TS, 2001, P NATL ACAD SCI USA, V98, P1907, DOI 10.1073/pnas.031579998
   Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1
   LEVITT JB, 1993, J COMP NEUROL, V338, P360, DOI 10.1002/cne.903380304
   Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557
   Luck SJ, 1997, J NEUROPHYSIOL, V77, P24
   Lund JS, 1997, J COMP NEUROL, V384, P109, DOI 10.1002/(SICI)1096-9861(19970721)384:1<109::AID-CNE7>3.0.CO;2-5
   LUND JS, 1988, J COMP NEUROL, V276, P1, DOI 10.1002/cne.902760102
   LUND JS, 1975, J COMP NEUROL, V159, P305, DOI 10.1002/cne.901590303
   LUND JS, 1987, J COMP NEUROL, V257, P60, DOI 10.1002/cne.902570106
   MCGUIRE BA, 1984, J NEUROSCI, V4, P3021
   MCGUIRE BA, 1991, J COMP NEUROL, V305, P370, DOI 10.1002/cne.903050303
   Mehta AD, 2000, CEREB CORTEX, V10, P359, DOI 10.1093/cercor/10.4.359
   Mehta SD, 2000, CEREB CORTEX, V10, P343, DOI 10.1093/cercor/10.4.343
   Melchitzky DS, 1998, J COMP NEUROL, V390, P211, DOI 10.1002/(SICI)1096-9861(19980112)390:2<211::AID-CNE4>3.0.CO;2-4
   Miller B, 2001, J COMP NEUROL, V436, P17
   Miller KD, 2001, CURR OPIN NEUROBIOL, V11, P488, DOI 10.1016/S0959-4388(00)00239-7
   MONTERO VM, 1991, EXP BRAIN RES, V86, P257
   Moore CI, 1999, TRENDS NEUROSCI, V22, P513, DOI 10.1016/S0166-2236(99)01452-6
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408
   Mounts JRW, 2000, PERCEPT PSYCHOPHYS, V62, P969, DOI 10.3758/BF03212082
   MUMFORD D, 1992, BIOL CYBERN, V66, P241, DOI 10.1007/BF00198477
   Neumann H, 1999, BIOL CYBERN, V81, P425, DOI 10.1007/s004220050573
   NORTHCUTT RG, 1995, TRENDS NEUROSCI, V18, P373
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   PARADISO MA, 1991, VISION RES, V31, P1221, DOI 10.1016/0042-6989(91)90047-9
   Parker JL, 1999, J NEUROSCI, V19, P8623
   Pessoa L, 1998, BEHAV BRAIN SCI, V21, P723
   PETERHANS E, 1989, J NEUROSCI, V9, P1749
   PETERS A, 1991, J COMP NEUROL, V306, P1, DOI 10.1002/cne.903060102
   Polat U, 1998, NATURE, V391, P580, DOI 10.1038/35372
   Pollen DA, 1999, CEREB CORTEX, V9, P4, DOI 10.1093/cercor/9.1.4
   Raizada RDS, 2001, VIS COGN, V8, P431, DOI 10.1080/13506280143000070
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Read HL, 2001, P NATL ACAD SCI USA, V98, P8042, DOI 10.1073/pnas.131591898
   REDIES C, 1986, EXP BRAIN RES, V61, P469
   Rempel-Clower NL, 2000, CEREB CORTEX, V10, P851, DOI 10.1093/cercor/10.9.851
   Reynolds JH, 1999, J NEUROSCI, V19, P1736
   Rioult-Pedotti MS, 1998, NAT NEUROSCI, V1, P230, DOI 10.1038/678
   ROCKLAND KS, 1989, J COMP NEUROL, V285, P54, DOI 10.1002/cne.902850106
   ROCKLAND KS, 1990, VISUAL NEUROSCI, V4, P11, DOI 10.1017/S095252380000273X
   Roelfsema P. R., 1999, SOC NEUR ABSTR, V7.2
   Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475
   Roelfsema PR, 2001, NEURON, V31, P853, DOI 10.1016/S0896-6273(01)00408-1
   RogersRamachandran DC, 1998, VISION RES, V38, P71, DOI 10.1016/S0042-6989(97)00131-4
   Rossi AF, 2001, J NEUROSCI, V21, P1698
   Ruthazer ES, 1996, J NEUROSCI, V16, P7253
   SALIN PA, 1995, PHYSIOL REV, V75, P107
   SANDELL JH, 1982, J NEUROPHYSIOL, V48, P38
   Schmidt KE, 1997, EUR J NEUROSCI, V9, P1083, DOI 10.1111/j.1460-9568.1997.tb01459.x
   Sengpiel F, 1997, EXP BRAIN RES, V116, P216, DOI 10.1007/PL00005751
   Sharma J, 2000, NATURE, V404, P841, DOI 10.1038/35009043
   Sheth BR, 1996, SCIENCE, V274, P2110, DOI 10.1126/science.274.5295.2110
   SILLITO AM, 1994, NATURE, V369, P479, DOI 10.1038/369479a0
   Sincich LC, 2001, J NEUROSCI, V21, P4416, DOI 10.1523/JNEUROSCI.21-12-04416.2001
   Skoglund TS, 1997, CEREB CORTEX, V7, P178, DOI 10.1093/cercor/7.2.178
   Smith AT, 2000, NEUROREPORT, V11, P271, DOI 10.1097/00001756-200002070-00010
   Smith PH, 2001, J COMP NEUROL, V436, P508, DOI 10.1002/cne.1084
   SNODDERLY DM, 1995, J NEUROPHYSIOL, V74, P2100
   Somers DC, 1998, CEREB CORTEX, V8, P204, DOI 10.1093/cercor/8.3.204
   STEINMAN BA, 1995, VISION RES, V35, P1859, DOI 10.1016/0042-6989(94)00276-R
   STEMMLER M, 1995, SCIENCE, V269, P1877, DOI 10.1126/science.7569930
   STEPNIEWSKA I, 1993, J COMP NEUROL, V330, P238, DOI 10.1002/cne.903300207
   Stratford KJ, 1996, NATURE, V382, P258, DOI 10.1038/382258a0
   SWAMINATHAN G, 2001, SOC NEUR ABSTR, V619, P49
   Tamas G, 1998, J NEUROSCI, V18, P4255
   TANIFUJI M, 1994, SCIENCE, V266, P1057, DOI 10.1126/science.7973662
   TANIFUJI M, 2001, SOC NEUR ABSTR
   TEMEREANCA S, 2001, SOC NEUR ABSTR
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Toth LJ, 1996, P NATL ACAD SCI USA, V93, P9869, DOI 10.1073/pnas.93.18.9869
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   ULLMAN S, 1995, CEREB CORTEX, V5, P1, DOI 10.1093/cercor/5.1.1
   Ungerleider L. G., 1982, ANAL VISUAL BEHAV, P549
   Usher M, 1996, J COGNITIVE NEUROSCI, V8, P311, DOI 10.1162/jocn.1996.8.4.311
   Valverde F., 1985, P207
   Vanduffel W, 2000, CEREB CORTEX, V10, P109, DOI 10.1093/cercor/10.2.109
   VANESSEN DC, 1986, J COMP NEUROL, V244, P451, DOI 10.1002/cne.902440405
   VONDERHEYDT R, 1984, SCIENCE, V224, P1260, DOI 10.1126/science.6539501
   Watanabe T, 2001, NATURE, V413, P844, DOI 10.1038/35101601
   Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837
   Yen SC, 1998, VISION RES, V38, P719, DOI 10.1016/S0042-6989(97)00197-1
   Yuste R, 1997, CEREB CORTEX, V7, P546, DOI 10.1093/cercor/7.6.546
   Zhang YF, 1997, NATURE, V387, P900, DOI 10.1038/43180
NR 159
TC 149
Z9 154
U1 0
U2 8
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 100
EP 113
DI 10.1093/cercor/13.1.100
PG 14
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500014
PM 12466221
OA Bronze, Green Published
DA 2019-06-15
ER

PT J
AU Oztop, E
   Kawato, M
   Arbib, M
AF Oztop, Erhan
   Kawato, Mitsuo
   Arbib, Michael
TI Mirror neurons and imitation: A computationally guided review
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE mirror neuron; action understanding; imitation; language; computational
   model
ID MOTOR CONTROL; ACTION RECOGNITION; PREMOTOR CORTEX;
   FUNCTIONAL-ORGANIZATION; ACTION REPRESENTATION; INTERNAL-MODELS;
   INFERIOR AREA-6; MACAQUE MONKEY; MOVEMENTS; SYSTEM
AB Neurophysiology reveals the properties of individual mirror neurons in the macaque while brain imaging reveals the presence of 'mirror systems' (not individual neurons) in the human. Current conceptual models attribute high level functions such as action understanding, imitation, and language to mirror neurons. However, only the first of these three functions is well-developed in monkeys. We thus distinguish current opinions (conceptual models) on mirror neuron function from more detailed computational models. We assess the strengths and weaknesses of current computational models in addressing the data and speculations on mirror neurons (macaque) and mirror systems (human). In particular, our mirror neuron system (MNS), mental state inference (MSI) and modular selection and identification for control (MOSAIC) models are analyzed in more detail. Conceptual models often overlook the computational requirements for posited functions, while too many computational models adopt the erroneous hypothesis that mirror neurons are interchangeable with imitation ability. Our meta-analysis underlines the gap between conceptual and computational models and points out the research effort required from both sides to reduce this gap. (c) 2006 Elsevier Ltd. All rights reserved.
C1 ICORP, JST, Computat Brain Project, Kyoto, Japan.
   Univ So Calif, USC Brain Project, Los Angeles, CA 90089 USA.
EM erhan@atr.jp
RI Oztop, Erhan/K-8111-2012
OI Oztop, Erhan/0000-0002-3051-6038
FU NCRR NIH HHS [1 P20 RR020700-01]
CR Arbib M. A., 1998, NEURAL ORG STRUCTURE
   Arbib M.A., 1997, COMMUN COGNITION, V29, P393
   Arbib M. A., 1981, HDB PHYSL NERVOUS SY, P1449, DOI DOI 10.1016/J.JPHYSPARIS.2008.03.001
   Arbib MA, 2005, BEHAV BRAIN SCI, V28, P105, DOI 10.1017/S0140525X05000038
   Arbib MA, 2002, FROM ANIM ANIMAT, P229
   Barrett AM, 2005, BEHAV BRAIN SCI, V28, P125, DOI 10.1017/S0140525X05220034
   Billard A, 1999, ADAPT BEHAV, V7, P35, DOI 10.1177/105971239900700103
   Billard A, 2001, ROBOT AUTON SYST, V37, P145, DOI 10.1016/S0921-8890(01)00155-5
   Blakemore SJ, 2001, NAT REV NEUROSCI, V2, P561, DOI 10.1038/35086023
   BONAIUTO J, 2005, WORKSH MOD NAT ACT S
   BORESTEIN E, 2005, COGNITIVE SYSTEMS RE, V6
   Brass M, 2000, BRAIN COGNITION, V44, P124, DOI 10.1006/brcg.2000.1225
   Buccino G, 2004, BRAIN LANG, V89, P370, DOI 10.1016/S0093-934X(03)00356-0
   Buccino G, 2001, EUR J NEUROSCI, V13, P400, DOI 10.1046/j.1460-9568.2001.01385.x
   Byrne RW, 1998, BEHAV BRAIN SCI, V21, P667, DOI 10.1017/S0140525X98001745
   Byrne RW, 2003, PHILOS T R SOC B, V358, P529, DOI 10.1098/rstb.2002.1219
   BYRNE RW, 1998, BEHAV BRAIN SCI, V21, P684
   Carr L, 2003, P NATL ACAD SCI USA, V100, P5497, DOI 10.1073/pnas.0935845100
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   DEMIRIS Y, 2002, IMITATION ANIMALS AR
   DEMIRIS Y, 2003, CONNECTION SCI, V15
   DOYA K, 2000, TL200011 IEICE
   Elshaw M., 2004, INT JOINT C NEUR NET
   Fadiga L, 2004, J CLIN NEUROPHYSIOL, V21, P157, DOI 10.1097/00004691-200405000-00004
   Fadiga L, 2002, EUR J NEUROSCI, V15, P399, DOI 10.1046/j.0953-816x.2001.01874.x
   FOGASSI L, 1992, EXP BRAIN RES, V89, P686, DOI 10.1007/BF00229894
   Fogassi L, 2001, BRAIN, V124, P571, DOI 10.1093/brain/124.3.571
   Gallese V, 2004, TRENDS COGN SCI, V8, P396, DOI 10.1016/j.tics.2004.07.002
   Gallese V, 1998, TRENDS COGN SCI, V2, P493, DOI 10.1016/S1364-6613(98)01262-5
   Gallese V, 1996, BRAIN, V119, P593, DOI 10.1093/brain/119.2.593
   GENTILUCCI M, 1988, EXP BRAIN RES, V71, P475, DOI 10.1007/BF00248741
   Hari R, 1998, P NATL ACAD SCI USA, V95, P15061, DOI 10.1073/pnas.95.25.15061
   Haruno M, 2001, NEURAL COMPUT, V13, P2201, DOI 10.1162/089976601750541778
   HASSOUN M. H., 1993, ASS NEURAL MEMORIES
   Iacoboni M, 1999, SCIENCE, V286, P2526, DOI 10.1126/science.286.5449.2526
   Iacoboni M, 2005, PLOS BIOL, V3, P529, DOI 10.1371/journal.pbio.0030079
   Ijspeert A.J., 2003, ADV NEURAL INFORM PR, P1547
   Ito M, 2004, ADAPT BEHAV, V12, P93, DOI 10.1177/105971230401200202
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Kawato M, 1998, NOVART FDN SYMP, V218, P291
   KAWATO M, 1992, BIOL CYBERN, V68, P95, DOI 10.1007/BF00201431
   Kilner JM, 2003, CURR BIOL, V13, P522, DOI 10.1016/S0960-9822(03)00165-9
   Kohler E, 2002, SCIENCE, V297, P846, DOI 10.1126/science.1070311
   Kumashiro M, 2003, INT J PSYCHOPHYSIOL, V50, P81, DOI 10.1016/S0167-8760(03)00126-0
   Kuniyoshi Y., 2003, INT C ROB AUT TAIP T
   Miall RC, 1996, NEURAL NETWORKS, V9, P1265, DOI 10.1016/S0893-6080(96)00035-4
   Miall RC, 2003, NEUROREPORT, V14, P2135, DOI 10.1097/01.wnr.0000098751.87269.77
   Morita M, 1996, NEURAL NETWORKS, V9, P1477, DOI 10.1016/S0893-6080(96)00021-4
   MURATA A, 2005, J JAPANESE NEURAL NE, V12, P52
   Myowa-Yamakoshi M, 1999, J COMP PSYCHOL, V113, P128, DOI 10.1037/0735-7036.113.2.128
   Oztop E, 2005, COGNITIVE BRAIN RES, V22, P129, DOI 10.1016/j.cogbrainres.2004.08.004
   Oztop E, 2002, BIOL CYBERN, V87, P116, DOI 10.1007/s00422-002-0318-1
   OZTOP E, 2005, IEEE RAS INT C HUM R
   Rizzolatti G, 2001, NAT REV NEUROSCI, V2, P661, DOI 10.1038/35090060
   RIZZOLATTI G, 1988, EXP BRAIN RES, V71, P491, DOI 10.1007/BF00248742
   Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230
   Rizzolatti G, 1998, TRENDS NEUROSCI, V21, P188, DOI 10.1016/S0166-2236(98)01260-0
   Rizzolatti G, 1996, COGNITIVE BRAIN RES, V3, P131, DOI 10.1016/0926-6410(95)00038-0
   RIZZOLATTI G, 2005, ANAT EMBRYOLOGY
   Schaal S, 2003, PHILOS T R SOC B, V358, P537, DOI 10.1098/rstb.2002.1258
   SCHAAL S, 2004, INT S ROB RES CIEN I
   Skipper JI, 2005, NEUROIMAGE, V25, P76, DOI 10.1016/j.neuroimage.2004.11.006
   Tani J, 2004, NEURAL NETWORKS, V17, P1273, DOI 10.1016/j.neunet.2004.05.007
   Tunik E, 2005, NAT NEUROSCI, V8, P505, DOI 10.1038/nn1430
   Umilta MA, 2001, NEURON, V31, P155, DOI 10.1016/S0896-6273(01)00337-3
   Wicker B, 2003, NEURON, V40, P655, DOI 10.1016/S0896-6273(03)00679-2
   Wohlschlager A, 2003, PHILOS T R SOC B, V358, P501, DOI 10.1098/rstb.2002.1257
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
   Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238
   Wolpert DM, 1998, TRENDS COGN SCI, V2, P338, DOI 10.1016/S1364-6613(98)01221-2
   Wolpert DM, 2001, TRENDS COGN SCI, V5, P487, DOI 10.1016/S1364-6613(00)01773-3
NR 71
TC 143
Z9 149
U1 1
U2 23
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 254
EP 271
DI 10.1016/j.neunet.2006.02.002
PG 18
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600002
PM 16595172
DA 2019-06-15
ER

PT J
AU Lund, JS
   Angelucci, A
   Bressloff, PC
AF Lund, JS
   Angelucci, A
   Bressloff, PC
TI Anatomical substrates for functional columns in macaque monkey primary
   visual cortex
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID LOCAL CIRCUIT NEURONS; STRIATE CORTEX; ORIENTATION SELECTIVITY; OCULAR
   DOMINANCE; CONTRAST SENSITIVITY; CEREBRAL-CORTEX; HORIZONTAL
   CONNECTIONS; INTRINSIC CONNECTIONS; FEEDBACK CONNECTIONS; SPATIAL
   SUMMATION
AB In this review we re-examine the concept of a cortical column in macaque primary visual cortex, and consider to what extent a functionally defined column reflects any sort of anatomical entity that subdivides cortical territory. Functional studies have shown that columns relating to different response properties are mapped in cortex at different spatial scales. We suggest that these properties first emerge in mid-layer 4C through a combination of thalamic afferent inputs and local intracortical circuitry, and are then transferred to other layers in a columnar fashion, via interlaminar relays, where additional processing occurs. However, several properties are not strictly columnar since they do not appear in all cortical layers. In contrast to the functional column, an anatomically based cortical column is defined most clearly in terms of the reciprocal connections it makes, both via intra-areal lateral connections and inter-areal feedback/feedforward pathways. The column boundaries are reinforced by interplay between lateral inhibition spreading beyond the column boundary and disinhibition within the column. The anatomical column acts as a functionally tuned unit and point of information collation from laterally offset regions and feedback pathways. Thalamic inputs provide the highcontrast receptive field sizes of the column's neurons, intra-areal lateral connections provide their low contrast summation field sizes, and feedback pathways provide surround modulation of receptive fields responses.
C1 Univ Utah, Moran Eye Ctr, Salt Lake City, UT 84132 USA.
   Univ Utah, Dept Math, Salt Lake City, UT 84112 USA.
RP Lund, JS (reprint author), Univ Utah, Moran Eye Ctr, 50 N Med Dr, Salt Lake City, UT 84132 USA.
FU NEI NIH HHS [R01 EY015262]
CR Adorjan P, 1999, VISUAL NEUROSCI, V16, P303, DOI 10.1017/S0952523899162114
   ANDERSON JC, 1993, CEREB CORTEX, V3, P412, DOI 10.1093/cercor/3.5.412
   Angelucci A, 2000, EUR J NEUROSCI, V12, P285
   Angelucci A, 2002, PROG BRAIN RES, V136, P373
   Angelucci A, 2002, J NEUROSCI, V22, P8633
   Asi H., 1996, Society for Neuroscience Abstracts, V22, P1608
   Bauer U, 1999, VISION RES, V39, P613, DOI 10.1016/S0042-6989(98)00172-2
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   BLASDEL GG, 1984, J NEUROSCI, V4, P880
   BLASDEL GG, 1983, J NEUROSCI, V3, P1389
   BLASDEL GG, 1985, J NEUROSCI, V5, P3350
   BLASDEL GG, 1992, J NEUROSCI, V12, P3139
   BLASDEL GG, 1992, J NEUROSCI, V12, P3115
   BONHOEFFER T, 1991, NATURE, V353, P429, DOI 10.1038/353429a0
   Bressloff PC, 2002, NEURAL COMPUT, V14, P493, DOI 10.1162/089976602317250870
   Bressloff PC, 2001, PHILOS T R SOC B, V356, P299, DOI 10.1098/rstb.2000.0769
   BRESSLOFF PC, 2002, IN PRESS PHILOS T R
   Buzas P, 2001, J COMP NEUROL, V437, P259, DOI 10.1002/cne.1282
   Calford MB, 1999, P ROY SOC B-BIOL SCI, V266, P499, DOI 10.1098/rspb.1999.0665
   DAS A, 1995, J NEUROPHYSIOL, V74, P779
   De Valois RJ, 1988, SPATIAL VISION
   Dragoi V, 2000, J NEUROPHYSIOL, V83, P1019
   FERTSTER D, 1997, NATURE, V380, P249
   GILBERT CD, 1992, NATURE, V356, P150, DOI 10.1038/356150a0
   Girard P, 2001, J NEUROPHYSIOL, V85, P1328
   Girman SV, 1999, J NEUROPHYSIOL, V82, P301
   HAWKEN MJ, 1988, J NEUROSCI, V8, P3541
   HAWKEN MJ, 1984, EXP BRAIN RES, V54, P367
   HUBEL DH, 1978, J COMP NEUROL, V177, P361, DOI 10.1002/cne.901770302
   HUBEL DH, 1974, J COMP NEUROL, V158, P295, DOI 10.1002/cne.901580305
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   HUBEL DH, 1974, J COMP NEUROL, V158, P267, DOI 10.1002/cne.901580304
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hubener M, 1997, J NEUROSCI, V17, P9270
   HUMPHREY AL, 1983, J NEUROSCI, V3, P345
   Hupe JM, 1998, NATURE, V394, P784, DOI 10.1038/29537
   Issa NP, 2000, J NEUROSCI, V20, P8504, DOI 10.1523/JNEUROSCI.20-22-08504.2000
   Kapadia MK, 1999, P NATL ACAD SCI USA, V96, P12073, DOI 10.1073/pnas.96.21.12073
   Kayser AS, 2002, NEURON, V33, P131, DOI 10.1016/S0896-6273(01)00570-0
   LEVITT JB, 2002, IN PRESS VIS NEUROSC
   LIVINGSTONE MS, 1984, J NEUROSCI, V4, P2830
   LIVINGSTONE MS, 1984, J NEUROSCI, V4, P309
   Lund J.S., 1985, P157
   Lund JS, 1995, J ANAT, V187, P563
   Lund JS, 1997, J COMP NEUROL, V384, P109, DOI 10.1002/(SICI)1096-9861(19970721)384:1<109::AID-CNE7>3.0.CO;2-5
   LUND JS, 1991, J COMP NEUROL, V311, P234, DOI 10.1002/cne.903110206
   LUND JS, 1988, J COMP NEUROL, V276, P1, DOI 10.1002/cne.902760102
   LUND JS, 1975, J COMP NEUROL, V164, P287, DOI 10.1002/cne.901640303
   LUND JS, 1993, CEREB CORTEX, V3, P148, DOI 10.1093/cercor/3.2.148
   LUND JS, 1987, J COMP NEUROL, V257, P60, DOI 10.1002/cne.902570106
   LUND JS, 1999, INVEST OPHTH VIS SCI, V645, P3397
   MALACH R, 1993, P NATL ACAD SCI USA, V90, P10469, DOI 10.1073/pnas.90.22.10469
   MCGUIRE BA, 1991, J COMP NEUROL, V305, P370, DOI 10.1002/cne.903050303
   McLaughlin D, 2000, P NATL ACAD SCI USA, V97, P8087, DOI 10.1073/pnas.110135097
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408
   NELSON S, 1994, SCIENCE, V265, P774, DOI 10.1126/science.8047882
   NELSON SB, 1991, J NEUROSCI, V11, P369
   NEWSOME WT, 1986, J NEUROPHYSIOL, V55, P1340
   OBERMAYER K, 1993, J NEUROSCI, V13, P4114
   POWELL TPS, 1959, B JOHNS HOPKINS HOSP, V105, P133
   REID RC, 1995, NATURE, V378, P281
   Ringach DL, 1997, NATURE, V387, P281, DOI 10.1038/387281a0
   ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307
   Roerig B, 2002, CEREB CORTEX, V12, P187, DOI 10.1093/cercor/12.2.187
   Sceniak MP, 1999, NAT NEUROSCI, V2, P733, DOI 10.1038/11197
   SCHMUEL A, 1998, SOC NEUR ABSTR, V24, P767
   SHIPP S, 1989, EUR J NEUROSCI, V1, P309, DOI 10.1111/j.1460-9568.1989.tb00798.x
   SILLITO AM, 1980, BRAIN RES, V194, P517, DOI 10.1016/0006-8993(80)91234-2
   Sincich LC, 2002, SCIENCE, V295, P1734, DOI 10.1126/science.1067902
   SOKOLOFF L, 1977, J NEUROCHEM, V28, P897, DOI 10.1111/j.1471-4159.1977.tb10649.x
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   Somers DC, 1998, CEREB CORTEX, V8, P204, DOI 10.1093/cercor/8.3.204
   Somogyi P, 1998, BRAIN RES REV, V26, P113, DOI 10.1016/S0165-0173(97)00061-1
   SOMOGYI P, 1983, NEUROSCIENCE, V10, P261, DOI 10.1016/0306-4522(83)90133-1
   STETTER M, 2000, BIOL CYBERN, V87, P291
   TOOTELL RBH, 1988, J NEUROSCI, V8, P1594
   Tsodyks M, 1999, SCIENCE, V286, P1943, DOI 10.1126/science.286.5446.1943
   Yabuta NH, 1998, VISUAL NEUROSCI, V15, P1007, DOI 10.1017/S0952523898156018
   Yoshioka T., 1992, Society for Neuroscience Abstracts, V18, P299
   YOSHIOKA T, 1994, VISUAL NEUROSCI, V11, P467, DOI 10.1017/S0952523800002406
   Yoshioka T, 1996, CEREB CORTEX, V6, P297, DOI 10.1093/cercor/6.2.297
   Yousef T, 2001, NEUROREPORT, V12, P1693, DOI 10.1097/00001756-200106130-00035
NR 82
TC 137
Z9 137
U1 0
U2 7
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 15
EP 24
DI 10.1093/cercor/13.1.15
PG 10
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500004
PM 12466211
OA Bronze
DA 2019-06-15
ER

PT S
AU Zhou, DY
   Weston, J
   Gretton, A
   Bousquet, O
   Scholkopf, B
AF Zhou, DY
   Weston, J
   Gretton, A
   Bousquet, O
   Scholkopf, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Ranking on data manifolds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NONLINEAR DIMENSIONALITY REDUCTION
AB The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Zhou, DY (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
RI Scholkopf, Bernhard/A-7570-2013
OI Scholkopf, Bernhard/0000-0002-8177-0925
CR Albert R, 1999, NATURE, V401, P130
   Brin Sergey, 1998, P 7 INT WORLD WID WE
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Golub G. H., 1989, MATRIX COMPUTATIONS
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Zhou D., 2003, 18 ANN C NEUR INF PR
NR 8
TC 132
Z9 145
U1 0
U2 7
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 169
EP 176
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500022
DA 2019-06-15
ER

PT J
AU Cesa-Bianchi, N
   Conconi, A
   Gentile, C
AF Cesa-Bianchi, N
   Conconi, A
   Gentile, C
TI On the generalization ability of on-line learning algorithms
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT Conference on Advaces in Neural Information Processing Systems
CY DEC, 2001
CL Vancouver, CANADA
DE kernel functions; on-line learning; pattern recognition; oerceptron
   algorithm; statistical learning theory
ID INDIVIDUAL SEQUENCES; EXPERT ADVICE; PREDICTION; CONVERGENCE;
   PERCEPTRON; COMPLEXITY; MODEL; BRAIN
AB In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i.i.d.) sample of data. Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M-n associated with the on-line performance of the ensemble. Via sharp pointwise bounds on M-n, we then obtain risk tail bounds for kernel Perceptron algorithms in terms of the spectrum of the empirical kernel matrix. These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results.
   A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i.e., a model making no probabilistic assumptions on the source generating the data. In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds.
C1 Univ Milan, DSI, I-20135 Milan, Italy.
   Univ Insubria, Dipartimento Informat & Commun, I-21100 Varese, Italy.
RP Cesa-Bianchi, N (reprint author), Univ Milan, DSI, I-20135 Milan, Italy.
RI Cesa-Bianchi, Nicolo/C-3721-2013
OI Cesa-Bianchi, Nicolo/0000-0001-8477-4748
CR Aizerman M, 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678
   ANTOS A, 2002, J MACHINE LEARNING R, V3, P73
   Auer P, 1998, MACH LEARN, V32, P127, DOI 10.1023/A:1007472513967
   Azuma K., 1967, TOHOKU MATH J, V19, P357, DOI DOI 10.2748/TMJ/1178243286
   BARTLETT P, 2001, MACH LEARN, V48, P85
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   BLOCK HD, 1962, REV MOD PHYS, V34, P123, DOI 10.1103/RevModPhys.34.123
   Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439
   Boucheron S, 2000, RANDOM STRUCT ALGOR, V16, P277, DOI 10.1002/(SICI)1098-2418(200005)16:3<277::AID-RSA4>3.0.CO;2-1
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179
   CESABIANCHI N, 2002, LECT NOTES ARTIF INT, V2375, P121
   Cristianini N., 2001, INTRO SUPPORT VECTOR
   Devroye L., 1996, PROBABILISTIC THEORY
   FEDER M, 1992, IEEE T INFORM THEORY, V38, P1258, DOI 10.1109/18.144706
   FREUND Y, 1998, P 11 ANN C COMP LEAR, P127
   Gentile C, 2003, MACH LEARN, V53, P265, DOI 10.1023/A:1026319107706
   GENTILE C, 1999, ADV NEURAL INFORM PR, V10, P225
   Grove AJ, 2001, MACH LEARN, V43, P173, DOI 10.1023/A:1010844028087
   Haussler D, 1998, IEEE T INFORM THEORY, V44, P1906, DOI 10.1109/18.705569
   HERBRICH R, 2002, JMLR, V3, P175
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Langford J, 2003, MACH LEARN, V51, P165, DOI 10.1023/A:1022806918936
   Langford J., 2001, P 18 INT C MACH LEAR, P290
   LITTLESTONE N, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P147
   Littlestone N., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P269
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Long PM, 1999, MACH LEARN, V37, P337, DOI 10.1023/A:1007666507971
   Meir R., 2003, J MACHINE LEARNING R, V4, P839
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   REUND Y, 1999, MACH LEARN, P277
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   SCHATTEN G, 1998, J LAW MED ETHICS, V26, P5
   Scholkopf B., 2002, LEARNING KERNELS
   Vapnik V. N., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P3
   Vapnik V. N., 1999, NATURE STAT LEARNING
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556
   WILLIAMSON R, 1999, NCTR99055 NEUROCOLT
NR 40
TC 118
Z9 121
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 0018-9448
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD SEP
PY 2004
VL 50
IS 9
BP 2050
EP 2057
DI 10.1109/TIT.2004.833339
PG 8
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 849ZE
UT WOS:000223579500014
DA 2019-06-15
ER

PT J
AU Moallemi, CC
   Van Roy, B
AF Moallemi, Ciamac C.
   Van Roy, Benjamin
TI Consensus propagation
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE belief propagation; distributed averaging; distributed consensus;
   distributed signal processing; Gaussian Markov random fields;
   max-product algorithm; message-passing algorithms; min-sum algorithm;
   sum-product algorithm
ID BELIEF PROPAGATION; GRAPHICAL MODELS; MARKOV-CHAIN; CORRECTNESS;
   ALGORITHMS
AB We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than painvise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge.
C1 Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.
RP Moallemi, CC (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM ciamac@stanford.edu; bvr@stanford.edu
CR AJI SM, 1998, P C INF SCI SYST PRI
   Bawa M, 2003, ESTIMATING AGGREGATE
   BAYATI M, 2005, P IEEE INT S INF THE, P1763, DOI DOI 10.1109/ISIT.2005.1523648
   Bertsekas D. P., 1997, PARALLEL DISTRIBUTED
   Blondel V.D., 2005, P 44 IEEE C DEC CONT, P2996
   Boyd S, 2005, IEEE INFOCOM SER, P1653
   Boyd S, 2006, AM MATH MON, V113, P70, DOI 10.2307/27641840
   BOYD S, 2005, P SIAM WORKSH AN ALG
   Boyd S, 2005, INTERNET MATH, V2, P31, DOI 10.1080/15427951.2005.10129100
   Considine J, 2004, PROC INT CONF DATA, P449, DOI 10.1109/ICDE.2004.1320018
   Diaconis P, 2000, ANN APPL PROBAB, V10, P726
   FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8
   FORNEY GD, 1998, P 1998 INF THEOR WOR
   Heskes T, 2004, NEURAL COMPUT, V16, P2379, DOI 10.1162/0899766041941943
   IHLER AT, 2005, ADV NEURAL INFORM PR, V17, P609
   Intanagonwiwat C., 2000, P 6 ANN INT C MOB CO, P56, DOI DOI 10.1145/345910.345920
   Jelasity M, 2004, INT CON DISTR COMP S, P102, DOI 10.1109/ICDCS.2004.1281573
   JOHNSON JK, 2006, ADV NEURAL INFORM PR, V18, P579
   KEMPE D, 2003, FOCS, P482
   Madden S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131, DOI 10.1145/1060289.1060303
   Madden S, 2002, FOURTH IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P49, DOI 10.1109/MCSA.2002.1017485
   Moallemi C. C., 2004, ADV NEURAL INFORM PR, V16
   MOALLEMI CC, 2006, CONVERGENCE MINSUM M
   MONTRESOR A, 2004, P 2004 INT C DEP SYS, P19
   MOOIJ JM, 2005, SUFFICIENT CONDITION
   MOSKAOYAMA D, 2005, INFORM DISSEMINATION
   Richardson TJ, 2001, IEEE T INFORM THEORY, V47, P599, DOI 10.1109/18.910577
   Roch S, 2005, ELECTRON COMMUN PROB, V10, P282, DOI 10.1214/ECP.v10-1169
   Rusmevichientong P, 2001, IEEE T INFORM THEORY, V47, P745, DOI 10.1109/18.910586
   Saligrama V., 2005, ASYNCHRONOUS DISTRIB
   TATIKONDA S, 2002, P UNC ART INT AUG, V18, P493
   Tsitsiklis JN, 1984, THESIS MIT CAMBRIDGE
   Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642
   Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880
   Weiss Y, 2001, NEURAL COMPUT, V13, P2173, DOI 10.1162/089976601750541769
   XIAO L, 2005, DISTRIBUTED AVERAGE
   Xiao L., 2005, P 4 INT S INF PROC S, P63
   Zhao J, 2003, PROCEEDINGS OF THE FIRST IEEE INTERNATIONAL WORKSHOP ON SENSOR NETWORK PROTOCOLS AND APPLICATIONS, P139, DOI 10.1109/SNPA.2003.1203364
NR 38
TC 114
Z9 114
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9448
EI 1557-9654
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD NOV
PY 2006
VL 52
IS 11
BP 4753
EP 4766
DI 10.1109/TIT.2006.883539
PG 14
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 102JA
UT WOS:000241805700001
DA 2019-06-15
ER

PT J
AU Rehder, B
AF Rehder, B
TI Categorization as causal reasoning
SO COGNITIVE SCIENCE
LA English
DT Article; Proceedings Paper
CT 15th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 03-08, 2001
CL VANCOUVER, CANADA
DE categorization; causal reasoning; causal knowledge; conceptual
   representation; causal models
ID PRIOR KNOWLEDGE; FEATURE CENTRALITY; CONCEPTUAL COHERENCE; LINEAR
   SEPARABILITY; NATURAL CATEGORIES; CLASSIFICATION; MODELS; COVARIATION;
   ATTRIBUTION; INDUCTION
AB A theory of categorization is presented in which knowledge of causal relationships between category features is represented in terms of asymmetric and probabilistic causal mechanisms. According to causal-model theory, objects are classified as category members to the extent they are likely to have been generated or produced by those mechanisms. The empirical results confirmed that participants rated exemplars good category members to the extent their features manifested the expectations that causal knowledge induces, such as correlations between feature pairs that are directly connected by causal relationships. These expectations also included sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships. Quantitative fits of causal-model theory were superior to those obtained with extensions to traditional similarity-based models that represent causal knowledge either as higher-order relational features or "prior exemplars" stored in memory. (C) 2003 Cognitive Science Society, Inc. All rights reserved.
C1 NYU, Dept Psychol, New York, NY 10003 USA.
RP Rehder, B (reprint author), NYU, Dept Psychol, 6 Washington Pl, New York, NY 10003 USA.
EM bob.rehder@nyu.edu
CR AHN AH, 1995, P 17 ANN C COGN SCI, P521
   AHN W, 2001, CAUSAL STATUS EFFECT
   AHN W, 2002, MEMORY COGNITION, V30
   Ahn WK, 1998, COGNITION, V69, P135, DOI 10.1016/S0010-0277(98)00063-8
   AHN WK, 1995, COGNITION, V54, P299, DOI 10.1016/0010-0277(94)00640-7
   Ahn WK, 2000, COGNITIVE PSYCHOL, V41, P361, DOI 10.1006/cogp.2000.0741
   Bartlett F.C., 1932, REMEMBERING
   BRANSFORD JD, 1972, COGNITIVE PSYCHOL, V3, P193, DOI 10.1016/0010-0285(72)90003-5
   Carey S, 1995, SYMP SYSSEN FDN, P268
   Carey S., 1985, CONCEPTUAL CHANGE CH
   Chapman L. J., 1969, J ABNORM PSYCHOL, V74, P272
   CHAPMAN LJ, 1967, J ABNORM PSYCHOL, V72, P193, DOI 10.1037/h0024670
   CHASE WG, 1973, COGNITIVE PSYCHOL, V4, P55, DOI 10.1016/0010-0285(73)90004-2
   CHENG PW, 1991, COGNITION, V40, P83, DOI 10.1016/0010-0277(91)90047-8
   CHENG PW, 1990, J PERS SOC PSYCHOL, V58, P545, DOI 10.1037/0022-3514.58.4.545
   CHENG PW, 1992, PSYCHOL REV, V99, P365, DOI 10.1037//0033-295X.99.2.365
   Cheng PW, 1997, PSYCHOL REV, V104, P367, DOI 10.1037//0033-295X.104.2.367
   CHI M, 1993, COGNITIVE MODELS SCI
   CHI MTH, 1989, COGNITIVE SCI, V13, P145, DOI 10.1207/s15516709cog1302_1
   CHI MTH, 1981, COGNITIVE SCI, V5, P121, DOI 10.1207/s15516709cog0502_2
   Estes W. K., 1994, CLASSIFICATION COGNI
   GELMAN SA, 1991, COGNITION, V38, P213, DOI 10.1016/0010-0277(91)90007-Q
   Gelman Susan A, 1994, MAPPING MIND DOMAIN, P341, DOI DOI 10.1017/CB09780511752902.014
   GENTNER D, 1983, COGNITIVE PSYCHOL, V15, P1
   GLUCK MA, 1988, J EXP PSYCHOL GEN, V117, P227, DOI 10.1037/0096-3445.117.3.227
   Glymour C, 1998, MIND MACH, V8, P39, DOI 10.1023/A:1008234330618
   GLYMOUR C, 1998, RATIONAL MODELS COGN, P296
   Gopnik A., 1994, MAPPING MIND DOMAIN, P257, DOI DOI 10.1017/CBO9780511752902.011
   HAMPTON JA, 1979, J VERB LEARN VERB BE, V18, P441, DOI 10.1016/S0022-5371(79)90246-9
   HAYESROTH B, 1977, J VERB LEARN VERB BE, V16, P321, DOI 10.1016/S0022-5371(77)80054-6
   HEIT E, 1994, J EXP PSYCHOL LEARN, V20, P1264, DOI 10.1037/0278-7393.20.6.1264
   Heit E, 1998, J EXP PSYCHOL LEARN, V24, P712, DOI 10.1037/0278-7393.24.3.712
   HEIT E, 2000, PSYCHOL LEARN MOTIV, P163
   Heit Evan, 2001, SIMILARITY CATEGORIZ, P155
   Jordan M., 1999, LEARNING GRAPHICAL M
   Karmiloff-Smith A., 1974, COGNITION, V3, P195
   Keil F. C, 1979, SEMANTIC CONCEPTUAL
   Keil F. C, 1989, CONCEPTS KINDS COGNI
   Keil FC, 1995, SYMP SYSSEN FDN, P234
   KELLEY HH, 1973, AM PSYCHOL, V28, P107, DOI 10.1037/h0034225
   KEMLERNELSON DG, 1984, J VERB LEARN VERB BE, V23, P734
   KINTSCH W, 1988, PSYCHOL REV, V95, P163, DOI 10.1037/0033-295X.95.2.163
   MALT BC, 1984, J VERB LEARN VERB BE, V23, P250, DOI 10.1016/S0022-5371(84)90170-1
   MCNAMARA TP, 1983, J VERB LEARN VERB BE, V22, P449, DOI 10.1016/S0022-5371(83)90291-8
   MEDIN D, 1989, SIMILARITY AND ANALOGICAL REASONING, P179, DOI 10.1017/CBO9780511529863.009
   MEDIN DL, 1981, J EXP PSYCHOL-HUM L, V7, P355, DOI 10.1037/0278-7393.7.5.355
   MEDIN DL, 1988, COGNITIVE PSYCHOL, V20, P158, DOI 10.1016/0010-0285(88)90018-7
   MEDIN DL, 1982, J EXP PSYCHOL-HUM L, V8, P37, DOI 10.1037/0278-7393.8.1.37
   MEDIN DL, 1978, PSYCHOL REV, V85, P207, DOI 10.1037//0033-295X.85.3.207
   Meltzoff A. N., 1998, WORDS THOUGHTS THEOR
   Minsky M. L., 1988, PERCEPTRONS
   MORRIS MW, 1995, PSYCHOL REV, V102, P331, DOI 10.1037/0033-295X.102.2.331
   Murphy G. L, 1989, ADV COGNITIVE SCI, V2, P23
   Murphy G. L., 1993, CATEGORIES CONCEPTS, P173
   MURPHY GL, 1994, J EXP PSYCHOL LEARN, V20, P904, DOI 10.1037/0278-7393.20.4.904
   MURPHY GL, 1985, PSYCHOL REV, V92, P289, DOI 10.1037/0033-295X.92.3.289
   NEUMANN PG, 1974, MEM COGNITION, V2, P241, DOI 10.3758/BF03208990
   Nisbett R. E., 1980, HUMAN INFERENCE STRA
   NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39
   PAZZANI MJ, 1991, J EXP PSYCHOL LEARN, V17, P416, DOI 10.1037/0278-7393.17.3.416
   Pearl J, 1988, PROBABILISTIC REASON
   Pearl J., 2000, CAUSALITY MODELS REA
   Rehder B, 1999, PROCEEDINGS OF THE TWENTY FIRST ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P595
   Rehder B, 2001, J EXP PSYCHOL GEN, V130, P323, DOI 10.1037//0096-3445.130.3.323
   Rehder B, 2001, J EXP PSYCHOL LEARN, V27, P1261, DOI 10.1037//0278-7393.27.5.1261
   REHDER B, IN PRESS J EXPT PSYC
   REHDER B, IN PRESS PSYCHONOMIC
   Reichenbach H, 1956, DIRECTION TIME
   REITMAN JS, 1973, COGNITIVE PSYCHOL, V4, P194, DOI 10.1016/0010-0285(73)90011-X
   RIPS LJ, 1989, SIMILARITY AND ANALOGICAL REASONING, P21, DOI 10.1017/CBO9780511529863.004
   ROSCH E, 1975, COGNITIVE PSYCHOL, V7, P573, DOI 10.1016/0010-0285(75)90024-9
   ROSCH EH, 1973, COGNITIVE PSYCHOL, V4, P328, DOI 10.1016/0010-0285(73)90017-0
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Salmon WC, 1984, SCI EXPLANATION CAUS
   SCHUSTACK MW, 1981, J EXP PSYCHOL GEN, V110, P101, DOI 10.1037//0096-3445.110.1.101
   Sloman SA, 1998, COGNITIVE SCI, V22, P189, DOI 10.1016/S0364-0213(99)80039-1
   Smith E. E., 1981, CATEGORIES CONCEPTS
   Spalding TL, 1999, MEM COGNITION, V27, P856, DOI 10.3758/BF03198538
   Sperber D, 1995, CAUSAL COGNITION MUL
   WALDMANN MR, 1992, J EXP PSYCHOL GEN, V121, P222, DOI 10.1037/0096-3445.121.2.222
   Waldmann MR, 2000, J EXP PSYCHOL LEARN, V26, P53, DOI 10.1037/0278-7393.26.1.53
   WALDMANN MR, 1995, J EXP PSYCHOL GEN, V124, P181, DOI 10.1037/0096-3445.124.2.181
   WATTENMAKER WD, 1986, COGNITIVE PSYCHOL, V18, P158, DOI 10.1016/0010-0285(86)90011-3
   WELLMAN HM, 1992, ANNU REV PSYCHOL, V43, P337, DOI 10.1146/annurev.ps.43.020192.002005
   WISNIEWSKI EJ, 1995, J EXP PSYCHOL LEARN, V21, P449, DOI 10.1037/0278-7393.21.2.449
   WISNIEWSKI EJ, 1994, COGNITIVE SCI, V18, P221, DOI 10.1016/0364-0213(94)90002-7
NR 86
TC 103
Z9 108
U1 0
U2 13
PU WILEY-BLACKWELL
PI MALDEN
PA COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA
SN 0364-0213
J9 COGNITIVE SCI
JI Cogn. Sci.
PD SEP-OCT
PY 2003
VL 27
IS 5
BP 709
EP 748
DI 10.1016/S0364-0213(03)00068-5
PG 40
WC Psychology, Experimental
SC Psychology
GA 735DJ
UT WOS:000186096900002
OA Bronze
DA 2019-06-15
ER

PT J
AU Pinto, DJ
   Hartings, JA
   Simons, DJ
AF Pinto, DJ
   Hartings, JA
   Simons, DJ
TI Cortical damping: Analysis of thalamocortical response transformations
   in rodent barrel cortex
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID SPINY STELLATE NEURONS; SIMULATED WHISKER BARRELS; PRIMARY
   VISUAL-CORTEX; MOUSE SML CORTEX; SOMATOSENSORY CORTEX; ORIENTATION
   SELECTIVITY; LAYER-IV; LAMINAR DIFFERENCES; RECEPTIVE-FIELDS; STRIATE
   CORTEX
AB In the whisker-barrel system, layer IV excitatory neurons respond preferentially to high-velocity deflections of their principal whisker, and these responses are inhibited by deflections of adjacent whiskers. Thalamic input neurons are amplitude and velocity sensitive and have larger excitatory and weaker inhibitory receptive fields than cortical neurons. Computational models based on known features of barrel circuitry capture these and other differences between thalamic and cortical neuron response properties. The models' responses are highly sensitive to thalamic firing synchrony, a finding subsequently confirmed in real barrels by in vivo experiments. Here, we use dynamic systems analysis to examine how barrel circuitry attains its sensitivity to input timing, and how this sensitivity explains the transformation of receptive fields between thalamus and cortex. We find that strong inhibition renders the net effect of intracortical connections suppressive or damping, distinguishing it from previous amplifying models of cortical microcircuits. In damping circuits, recurrent excitation enhances response tuning not by amplifying responses to preferred inputs, but by enabling them to better withstand strong inhibitory influences. Dense interconnections among barrel neurons result in considerable response homogeneity. Neurons outside the barrel layer respond more heterogeneously, possibly reflecting diverse networks and multiple transformations within the cortical output layers.
C1 Brown Univ, Dept Neurosci, Providence, RI 02912 USA.
   Univ Pittsburgh, Dept Neurobiol, Pittsburgh, PA 15261 USA.
   CUNY Queens Coll, Dept Psychol, Flushing, NY 11367 USA.
RP Pinto, DJ (reprint author), Brown Univ, Dept Neurosci, Box 1953, Providence, RI 02912 USA.
FU NIDA NIH HHS [DA12500]; NIMH NIH HHS [K01-MH01944-01A1]; NINDS NIH HHS
   [NS19950, NS25983]
CR Abbott LF, 1997, SCIENCE, V275, P220
   Adorjan P, 1999, VISUAL NEUROSCI, V16, P303, DOI 10.1017/S0952523899162114
   AGMON A, 1991, NEUROSCIENCE, V41, P365, DOI 10.1016/0306-4522(91)90333-J
   AHMED B, 1994, J COMP NEUROL, V341, P39, DOI 10.1002/cne.903410105
   Angulo MC, 1999, J NEUROPHYSIOL, V82, P1295
   BENSHALOM G, 1986, J COMP NEUROL, V253, P303, DOI 10.1002/cne.902530303
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   Brumberg JC, 1996, J NEUROPHYSIOL, V76, P130
   Brumberg JC, 1999, J NEUROPHYSIOL, V82, P1808
   BRUNO RM, 2001, SOC NEUR ABSTR, V27, P393
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   CARVELL GE, 1988, BRAIN RES, V448, P186, DOI 10.1016/0006-8993(88)91118-3
   CHAPIN JK, 1986, EXP BRAIN RES, V62, P549
   CHMIELOWSKA J, 1989, J COMP NEUROL, V285, P325, DOI 10.1002/cne.902850304
   CRANDALL JE, 1986, NEUROSCI LETT, V67, P19, DOI 10.1016/0304-3940(86)90201-6
   Deschenes M, 1998, BRAIN RES REV, V28, P286, DOI 10.1016/S0165-0173(98)00017-4
   DOUGLAS RJ, 1991, J PHYSIOL-LONDON, V440, P735, DOI 10.1113/jphysiol.1991.sp018733
   DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624
   Douglas RJ, 1989, NEURAL COMPUT, V1, P480, DOI 10.1162/neco.1989.1.4.480
   Ermentrout B, 1998, REP PROG PHYS, V61, P353, DOI 10.1088/0034-4885/61/4/002
   Ferster D, 2000, ANNU REV NEUROSCI, V23, P441, DOI 10.1146/annurev.neuro.23.1.441
   Ghazanfar AA, 2001, CEREB CORTEX, V11, P183, DOI 10.1093/cercor/11.3.183
   Gibson JR, 1999, NATURE, V402, P75
   Gil Z, 1997, NEURON, V19, P679, DOI 10.1016/S0896-6273(00)80380-3
   Gil Z, 1999, NEURON, V23, P385, DOI 10.1016/S0896-6273(00)80788-6
   Goldreich D, 1999, J NEUROPHYSIOL, V82, P1311
   GRIEVE KL, 1995, J NEUROSCI, V15, P4868
   HARTINGS JA, 2000, THESIS U PITTSBURGH
   HARVEY AR, 1980, J PHYSIOL-LONDON, V302, P507
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   KELLER A, 1995, CEREBR CORT, V11, P221
   Kelly MK, 1999, J NEUROSCI, V19, P9117, DOI 10.1523/JNEUROSCI.19-20-09117.1999
   Kim U, 1999, J COMP NEUROL, V408, P489
   Kyriazi H, 1998, SOMATOSENS MOT RES, V15, P146, DOI 10.1080/08990229870871
   KYRIAZI HT, 1993, J NEUROSCI, V13, P1601
   KYRIAZI HT, 1994, J NEUROPHYSIOL, V72, P392
   Kyriazi HT, 1996, J NEUROPHYSIOL, V75, P547
   McCasland JS, 1997, J NEUROSCI, V17, P5509
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782
   Miller KD, 2001, CURR OPIN NEUROBIOL, V11, P488, DOI 10.1016/S0959-4388(00)00239-7
   MONOGHAN DT, 1985, J NEUROSCI, V5, P2909
   MOORE C, 1998, J NEURPHYSIOL, V8, P2882
   PETERSEN CH, 2001, J NEUROSCI, V20, P7579
   Pinto DJ, 1996, J COMPUT NEUROSCI, V3, P247, DOI 10.1007/BF00161134
   Pinto DJ, 2000, J NEUROPHYSIOL, V83, P1158
   Rema V, 1996, J COMP NEUROL, V368, P165
   Rieke Fred, 1997, SPIKES EXPLORING NEU
   Rinzel J., 1998, METHODS NEURONAL MOD, P251
   SCLAR G, 1982, EXP BRAIN RES, V46, P457
   Shoykhet M, 2000, SOMATOSENS MOT RES, V17, P171
   SIMONS DJ, 1987, NEUROSCI LETT, V81, P100, DOI 10.1016/0304-3940(87)90347-8
   Simons DJ, 1997, NEWS PHYSIOL SCI, V12, P268
   SIMONS DJ, 1985, J NEUROPHYSIOL, V54, P615
   SIMONS DJ, 1978, J NEUROPHYSIOL, V41, P198
   SIMONS DJ, 1989, J NEUROPHYSIOL, V61, P611
   SKOTTUN BC, 1987, J NEUROPHYSIOL, V57, P773
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   SUAREZ H, 1995, J NEUROSCI, V15, P6700
   SWADLOW HA, 1995, J NEUROPHYSIOL, V73, P1584
   Swadlow HA, 2000, J NEUROPHYSIOL, V83, P2802
   TEMEREANCA S, 2000, SOC NEUR ABSTR, V26, P132
   WELKER C, 1971, BRAIN RES, V26, P259, DOI 10.1016/S0006-8993(71)80004-5
   WHITE EL, 1977, J COMP NEUROL, V175, P455, DOI 10.1002/cne.901750405
   WHITE EL, 1978, J COMP NEUROL, V181, P627, DOI 10.1002/cne.901810310
   WHITE EL, 1981, J COMP NEUROL, V195, P265, DOI 10.1002/cne.901950207
   WHITE EL, 1980, J NEUROCYTOL, V9, P6156
   White EL, 1989, CORTICAL CIRCUITS SY
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   WOOLSEY TA, 1970, BRAIN RES, V17, P205, DOI 10.1016/0006-8993(70)90079-X
   YUAN B, 1986, J NEUROSCI, V6, P3611
NR 70
TC 98
Z9 100
U1 0
U2 2
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 33
EP 44
DI 10.1093/cercor/13.1.33
PG 12
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500006
PM 12466213
DA 2019-06-15
ER

PT S
AU Blei, DM
   Griffiths, TL
   Jordan, MI
   Tenenbaum, JB
AF Blei, DM
   Griffiths, TL
   Jordan, MI
   Tenenbaum, JB
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Hierarchical topic models and the nested chinese restaurant process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MIXTURE
AB We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting-which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.
C1 Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Blei, DM (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
CR Aldous D, 1985, ECOLE ETE PROBABILIT, P1, DOI 10.1007/BFb0099421
   BEAL M, ADV NEURAL INFORMATI, V14
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   GriffIths T. L, 2002, P 24 ANN C COGN SCI
   Hofmann T, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P682
   Ishwaran H, 2003, STAT SINICA, V13, P1211
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Pitman J., 2002, COMBINATORIAL STOCHA
   RASMUSSEN C, ADV NEURAL INFORMATI, V14
   ROWEIS S, 1987, NIPS ABSTR
   SEGAL E, ADV NEURAL INFORMATI, V14
   WEST M, ASPECTS UNCERTAINTY
NR 14
TC 97
Z9 103
U1 2
U2 19
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 17
EP 24
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500003
DA 2019-06-15
ER

PT J
AU Linden, JF
   Schreiner, CE
AF Linden, JF
   Schreiner, CE
TI Columnar transformations in auditory cortex? A comparison to visual and
   somatosensory cortices
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID RECEPTIVE-FIELD PROPERTIES; SOMATIC SENSORY CORTEX; SIMPLE CELLS;
   FUNCTIONAL-ORGANIZATION; LAMINAR DIFFERENCES; RESPONSE PROPERTIES;
   COMPLEX CELLS; BARREL SYSTEM; MODULAR ORGANIZATION; EPTESICUS-FUSCUS
AB Auditory cortical columns have been studied for decades, but intracolumnar processing in auditory cortex is still poorly understood, relative to what is known about such processsing in visual cortex and somatosensory cortex. While there are certainly striking similarities in cortical structure across the modalities, investigations of auditory cortex anatomy and synaptic physiology have also found important differences from the columnar organization of other sensory cortices. In vitro and in vivo studies of thalamocortical transformations in the auditory system have begun to reveal the functional significance of these differences, and have defined the earliest stages of auditory cortical processing. However, the question of what transformations are performed within auditory cortical columns remains unresolved. Attempts to find laminar differences in auditory cortex, which could provide the key to understanding columnar transformations, have so far produced contradictory and inconclusive results. Direct analogies to primary visual and somatic sensory cortices would suggest that response properties such as bandwidth, inhibitory sideband structure, preferred modulation rate and modulation phase sensitivity might vary across layers in auditory cortex. While such analogies could prove useful as guidelines for future research, the best hope for understanding auditory columnar transformations may lie instead with a more modality-specific, functional approach.
C1 Univ Calif San Francisco, WM Keck Ctr Integrat Neurosci, San Francisco, CA 94143 USA.
RP Linden, JF (reprint author), Univ Calif San Francisco, WM Keck Ctr Integrat Neurosci, Room HSE804,513 Parnassus Ave, San Francisco, CA 94143 USA.
OI Schreiner, Christoph/0000-0002-4571-4328
FU NIDCD NIH HHS [R01 DC002260, DC02260, DC00399, R01 DC002260-08]
CR ABELES M, 1970, J NEUROPHYSIOL, V33, P172
   AGMON A, 1991, NEUROSCIENCE, V41, P365, DOI 10.1016/0306-4522(91)90333-J
   Ahissar E, 2000, NATURE, V406, P302, DOI 10.1038/35018568
   Ahissar E, 2001, J NEUROPHYSIOL, V86, P354
   Alonso JM, 1998, NAT NEUROSCI, V1, P395, DOI 10.1038/1609
   Alonso JM, 2001, J NEUROSCI, V21, P4002, DOI 10.1523/JNEUROSCI.21-11-04002.2001
   Angelucci A, 1998, J COMP NEUROL, V400, P417, DOI 10.1002/(SICI)1096-9861(19981026)400:3<417::AID-CNE10>3.0.CO;2-O
   Atzori M, 2001, NAT NEUROSCI, V4, P1230, DOI 10.1038/nn760
   BRUGGE JF, 1973, J NEUROPHYSIOL, V36, P1138
   Brumberg JC, 1996, J NEUROPHYSIOL, V76, P130
   Brumberg JC, 1999, J NEUROPHYSIOL, V82, P1808
   Buonomano DV, 2000, J NEUROSCI, V20, P1129, DOI 10.1523/JNEUROSCI.20-03-01129.2000
   Chance FS, 1999, NAT NEUROSCI, V2, P277
   CHAPIN JK, 1986, EXP BRAIN RES, V62, P549
   CLAREY JC, 1994, J NEUROPHYSIOL, V72, P2383
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Cruikshank SJ, 2002, J NEUROPHYSIOL, V87, P361, DOI 10.1152/jn.00549.2001
   DAW NW, 1992, J NEUROPHYSIOL, V67, P197
   DEAR SP, 1993, J NEUROPHYSIOL, V70, P1988
   DEFELIPE J, 1986, J NEUROSCI, V6, P3749
   Desai NS, 2002, NAT NEUROSCI, V5, P783, DOI 10.1038/nn878
   DIAMOND ME, 1994, SCIENCE, V265, P1885, DOI 10.1126/science.8091215
   DiCarlo JJ, 2002, BEHAV BRAIN RES, V135, P167, DOI 10.1016/S0166-4328(02)00162-6
   Eggermont JJ, 1996, AUDIT NEUROSCI, V2, P79
   Foeller E, 2001, JARO, V2, P279
   GILBERT CD, 1979, NATURE, V280, P120, DOI 10.1038/280120a0
   GILBERT CD, 1977, J PHYSIOL-LONDON, V268, P391, DOI 10.1113/jphysiol.1977.sp011863
   Hefti BJ, 2000, J NEUROPHYSIOL, V83, P2626
   HEFTI BJ, 2002, J ASS RES OTOLARYNGO
   Hirsch JA, 1998, J NEUROSCI, V18, P9517
   Huang CL, 2000, J COMP NEUROL, V427, P302, DOI 10.1002/1096-9861(20001113)427:2<302::AID-CNE10>3.0.CO;2-J
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hubener M, 1997, J NEUROSCI, V17, P9270
   IMIG TJ, 1978, J COMP NEUROL, V182, P637, DOI 10.1002/cne.901820406
   IMIG TJ, 1977, BRAIN RES, V138, P241, DOI 10.1016/0006-8993(77)90743-0
   INNOCENTI GM, 1980, ARCH ITAL BIOL, V118, P124
   Innocenti GM, 1986, CEREB CORTEX, V5, P291, DOI DOI 10.1007/978-1-4613-2149-1_9
   JEN PHS, 1989, J COMP PHYSIOL A, V165, P1, DOI 10.1007/BF00613794
   Jones EG, 2000, P NATL ACAD SCI USA, V97, P5019, DOI 10.1073/pnas.97.10.5019
   JONES EG, 1975, J COMP NEUROL, V160, P205, DOI 10.1002/cne.901600204
   Kilgard MP, 2001, J NEUROPHYSIOL, V86, P326
   Kral A, 2000, CEREB CORTEX, V10, P714, DOI 10.1093/cercor/10.7.714
   LANDRY P, 1981, J COMP NEUROL, V199, P345, DOI 10.1002/cne.901990304
   LEVAY S, 1976, BRAIN RES, V113, P1, DOI 10.1016/0006-8993(76)90002-0
   LINDEN JF, 2002, SOC NEUR ABSTR, V32
   LUND JS, 1979, J COMP NEUROL, V184, P599, DOI 10.1002/cne.901840402
   MANGINI NJ, 1980, J COMP NEUROL, V193, P203, DOI 10.1002/cne.901930114
   MANZONI T, 1980, EXP BRAIN RES, V39, P1
   Martinez LM, 2002, J PHYSIOL-LONDON, V540, P321, DOI 10.1113/jphysiol.2001.012776
   Martinez LM, 2001, NEURON, V32, P515, DOI 10.1016/S0896-6273(01)00489-5
   MATSUBARA JA, 1988, J COMP NEUROL, V268, P38, DOI 10.1002/cne.902680105
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782
   Mel BW, 1998, J NEUROSCI, V18, P4325
   MENDELSON JR, 1993, EXP BRAIN RES, V94, P65
   Mendelson JR, 1997, J COMP PHYSIOL A, V181, P615, DOI 10.1007/s003590050145
   MERZENICH MM, 1975, J NEUROPHYSIOL, V38, P231
   Metherate R, 1999, DEV BRAIN RES, V115, P131, DOI 10.1016/S0165-3806(99)00058-9
   METIN C, 1988, EXP BRAIN RES, V69, P594
   MIDDLEBROOKS JC, 1980, BRAIN RES, V181, P31, DOI 10.1016/0006-8993(80)91257-3
   Miller LM, 2001, NEURON, V32, P151, DOI 10.1016/S0896-6273(01)00445-7
   MITANI A, 1985, J COMP NEUROL, V235, P430, DOI 10.1002/cne.902350403
   MITANI A, 1985, J COMP NEUROL, V235, P417, DOI 10.1002/cne.902350402
   Ojima H, 1991, CEREB CORTEX, V1, P80, DOI 10.1093/cercor/1.1.80
   OONISHI S, 1965, JPN J PHYSIOL, V15, P342
   PHILLIPS DP, 1981, J NEUROPHYSIOL, V45, P48
   PHILLIPS DP, 1983, J NEUROPHYSIOL, V49, P383
   Pinto DJ, 2000, J NEUROPHYSIOL, V83, P1158
   Popper AN, 1992, MAMMALIAN AUDITORY P, P232
   Read HL, 2001, P NATL ACAD SCI USA, V98, P8042, DOI 10.1073/pnas.131591898
   RECANZONE GH, 1993, J NEUROSCI, V13, P87
   REID RC, 1995, NATURE, V378, P281
   Reser DH, 2000, CEREB CORTEX, V10, P574, DOI 10.1093/cercor/10.6.574
   Ringach DL, 2002, J NEUROSCI, V22, P5639
   ROCKEL AJ, 1980, BRAIN, V103, P221, DOI 10.1093/brain/103.2.221
   ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307
   ROE AW, 1990, SCIENCE, V250, P818, DOI 10.1126/science.2237432
   ROE AW, 1992, J NEUROSCI, V12, P3651
   SAHANI M, 2002, ADV NEURAL INFORMATI, V15
   Schreiner CE, 2000, ANNU REV NEUROSCI, V23, P501, DOI 10.1146/annurev.neuro.23.1.501
   SCHREINER CE, 1990, J NEUROPHYSIOL, V64, P1442
   SCHWARK HD, 1989, EXP BRAIN RES, V78, P501
   Sharma J, 2000, NATURE, V404, P841, DOI 10.1038/35009043
   Shen JX, 1999, HEARING RES, V137, P174, DOI 10.1016/S0378-5955(99)00149-5
   SIMONS DJ, 1984, J COMP NEUROL, V230, P119, DOI 10.1002/cne.902300111
   SIMONS DJ, 1978, J NEUROPHYSIOL, V41, P798
   SIMONS DJ, 1989, J NEUROPHYSIOL, V61, P311
   Smith PH, 2001, J COMP NEUROL, V436, P508, DOI 10.1002/cne.1084
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   SRETAVAN D, 1983, J COMP NEUROL, V213, P381, DOI 10.1002/cne.902130403
   Stern EA, 2001, NEURON, V31, P305, DOI 10.1016/S0896-6273(01)00360-9
   SUGA N, 1965, J PHYSIOL-LONDON, V181, P671, DOI 10.1113/jphysiol.1965.sp007791
   Sugimoto S, 1997, HEARING RES, V112, P175, DOI 10.1016/S0378-5955(97)00119-6
   SUR M, 1984, J NEUROPHYSIOL, V51, P724
   SUR M, 1988, SCIENCE, V242, P1437, DOI 10.1126/science.2462279
   SUR M, 1985, BRAIN RES, V342, P391, DOI 10.1016/0006-8993(85)91144-8
   SUTTER ML, 1991, J NEUROPHYSIOL, V65, P1207
   Swadlow HA, 2002, NAT NEUROSCI, V5, P403, DOI 10.1038/nn847
   Swadlow HA, 2000, J NEUROPHYSIOL, V83, P2802
   Trachtenberg JT, 2000, SCIENCE, V287, P2029, DOI 10.1126/science.287.5460.2029
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   WALLACE MN, 1991, EXP BRAIN RES, V86, P527
   Weinberger NM, 1998, NEUROBIOL LEARN MEM, V70, P226, DOI 10.1006/nlme.1998.3850
   WINER JA, 1992, MAMMALIAN AUDITORY P, P222
   Zhang LI, 2001, NAT NEUROSCI, V4, P1123, DOI 10.1038/nn745
NR 104
TC 96
Z9 98
U1 0
U2 6
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 83
EP 89
DI 10.1093/cercor/13.1.83
PG 7
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500012
PM 12466219
OA Bronze
DA 2019-06-15
ER

PT J
AU Ahissar, E
   Kleinfeld, D
AF Ahissar, E
   Kleinfeld, D
TI Closed-loop neuronal computations: Focus on vibrissa somatosensation in
   rat
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID TEMPORAL FREQUENCY; WHISKER MOVEMENT; CORTEX; RESPONSES;
   REPRESENTATIONS; ORGANIZATION; OSCILLATIONS; TIME; CODE
AB Two classes of neuronal architectures dominate in the ongoing debate on the nature of computing by nervous systems. The first is a predominantly feedforward architecture, in which local interactions among neurons within each processing stage play a less influential role compared with the drive of the input to that stage. The second class is a recurrent network architecture, in which the local interactions among neighboring neurons dominate the dynamics of neuronal activity so that the input acts only to bias or seed the state of the network. The study of sensorimotor networks, however, serves to highlight a third class of architectures, which is neither feedforward nor locally recurrent and where computations depend on large-scale feedback loops. Findings that have emerged from our laboratories and those of our colleagues suggest that the vibrissa sensorimotor system is involved in such closed-loop computations. In particular, single unit responses from vibrissa sensory and motor areas show generic signatures of phase-sensitive detection and control at the level of thalamocortical and corticocortical loops. These loops are likely to be components within a greater closed-loop vibrissa sensorimotor system, which optimizes sensory processing.
C1 Weizmann Inst Sci, Dept Neurobiol, IL-76100 Rehovot, Israel.
   Univ Calif San Diego, Grad Program Neurosci, La Jolla, CA 92093 USA.
   Univ Calif Santa Barbara, Inst Theoret Phys, Santa Barbara, CA 93106 USA.
   Univ Calif San Diego, Dept Phys, La Jolla, CA 92093 USA.
RP Ahissar, E (reprint author), Weizmann Inst Sci, Dept Neurobiol, IL-76100 Rehovot, Israel.
FU NIMH NIH HHS [MH59867]
CR Ahissar E, 2000, NATURE, V406, P302, DOI 10.1038/35018568
   AHISSAR E, 1990, P NATL ACAD SCI USA, V87, P8935, DOI 10.1073/pnas.87.22.8935
   Ahissar E, 2001, Prog Brain Res, V130, P75
   Ahissar E, 2001, J NEUROPHYSIOL, V86, P354
   Ahissar E, 2001, NEURON, V32, P185, DOI 10.1016/S0896-6273(01)00466-4
   Ahissar E, 2001, P NATL ACAD SCI USA, V98, P13367, DOI 10.1073/pnas.201400998
   Ahissar E, 1998, NEURAL COMPUT, V10, P597, DOI 10.1162/089976698300017683
   Ahissar E, 1997, P NATL ACAD SCI USA, V94, P11633, DOI 10.1073/pnas.94.21.11633
   AHISSAR E, 1995, GCEA954 WEIZM I SCI
   AHRENS KF, 2002, IN PRESS P NATL ACAD
   Amit D. J., 1989, MODELING BRAIN FUNCT
   Bellescize H., 1932, ONDE ELECTR, V11, P230
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   BERG RW, 2002, IN PRESS J NEUROPHYS
   BRECHT M, 2001, SOC NEUROSCI, V130, P135
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Fee MS, 1997, J NEUROPHYSIOL, V78, P1144
   GARDNER FM, 1979, PHASELOCK TECHNIQU
   GEORGOPOULOS AP, 1986, ANNU REV NEUROSCI, V9, P147, DOI 10.1146/annurev.ne.09.030186.001051
   GIBSON JM, 1983, SOMATOSENS RES, V1, P51, DOI 10.3109/07367228309144540
   GORDON J, 1991, PRINCIPLES NEURAL SC, P580
   Hartings JA, 1998, J NEUROPHYSIOL, V80, P1016
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hoppensteadt F., 1986, INTRO MATH NEURONS
   Kleinfeld D, 1999, SOMATOSENS MOT RES, V16, P69, DOI 10.1080/08990229970528
   Kleinfeld D, 2002, NEURON, V34, P1021, DOI 10.1016/S0896-6273(02)00732-8
   LICHTENSTEIN SH, 1990, SOMATOSENS MOT RES, V7, P47, DOI 10.3109/08990229009144697
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   O'Connor SM, 2002, J NEUROPHYSIOL, V87, P2137, DOI 10.1152/jn.00229.2001
   PERKEL DH, 1964, SCIENCE, V145, P61, DOI 10.1126/science.145.3627.61
   PERKEL DH, 1968, NEUROSCI RES PROGRAM, V6, P221
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED, VI
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V2
   SCHONER G, 1992, NEUROREPORT, V3, P579, DOI 10.1097/00001756-199207000-00008
   Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339
   Sherman SM, 1996, J NEUROPHYSIOL, V76, P1367
   Shoykhet M, 2000, SOMATOSENS MOT RES, V17, P171
   SILVA LR, 1991, SCIENCE, V251, P432, DOI 10.1126/science.1824881
   Sosnik R, 2001, J NEUROPHYSIOL, V86, P339
   TALBOT WH, 1968, J NEUROPHYSIOL, V31, P301
   Wessberg J, 2000, NATURE, V408, P361, DOI 10.1038/35042582
   WHITE EL, 1987, J COMP NEUROL, V262, P13, DOI 10.1002/cne.902620103
   Wiener N., 1949, CYBERNETICS
   ZUCKER E, 1969, BRAIN RES, V12, P138, DOI 10.1016/0006-8993(69)90061-4
NR 44
TC 79
Z9 79
U1 0
U2 3
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 53
EP 62
DI 10.1093/cercor/13.1.53
PG 10
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500008
PM 12466215
OA Bronze
DA 2019-06-15
ER

PT S
AU Taskar, B
   Guestrin, C
   Koller, D
AF Taskar, B
   Guestrin, C
   Koller, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Max-margin Markov networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their Popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M-3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M-3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in Structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.
C1 Stanford Univ, Stanford, CA 94305 USA.
RP Taskar, B (reprint author), Stanford Univ, Stanford, CA 94305 USA.
CR ALTUN Y, 2003, P ICML
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   COLLINS M, 2001, IWPT
   Cowell R. G., 1999, PROBABILISTIC NETWOR
   Crammer K., 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628
   KASSEL R, 1995, THESIS MIT SPOK LANG
   Lafferty J., 2001, P ICML01
   Pearl J, 1988, PROBABILISTIC REASON
   Platt John C., 1999, NIPS
   TASKAR B, 2002, P UAI02
   Vapnik VN, 1995, NATURE STAT LEARNING
   YEDIDIA J, 2000, NIPS
   Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713
NR 13
TC 78
Z9 81
U1 0
U2 5
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 25
EP 32
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500004
DA 2019-06-15
ER

PT S
AU Donoho, D
   Stodden, V
AF Donoho, D
   Stodden, V
BE Thrun, S
   Saul, K
   Scholkopf, B
TI When does non-negative matrix factorization give a correct decomposition
   into parts?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MOLECULAR LINE SPECTRA; ATMOSPHERIC AEROSOL; ALASKA
AB We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of 'parts' and NMF correctly identifies the 'parts'. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases.
C1 Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Donoho, D (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
CR CRAIG MD, 1994, IEEE T GEOSCI REMOTE, V32, P542, DOI 10.1109/36.297973
   JUVELA M, 1994, ASTR SOC P, V65, P176
   Juvela M, 1996, MON NOT R ASTRON SOC, V280, P616
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Plumbley M, 2002, IEEE SIGNAL PROC LET, V9, P177, DOI 10.1109/LSP.2002.800502
   Polissar AV, 1998, J GEOPHYS RES-ATMOS, V103, P19035, DOI 10.1029/98JD01365
   Polissar AV, 1998, J GEOPHYS RES-ATMOS, V103, P19045, DOI 10.1029/98JD01212
   Rockefellar R T, 1970, CONVEX ANAL
   SIZE W, 1987, USE ABUSE STAT METHO, P33
NR 9
TC 74
Z9 78
U1 1
U2 9
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1141
EP 1148
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500142
DA 2019-06-15
ER

PT J
AU Kienzle, W
   Franz, MO
   Scholkopf, B
   Wichmann, FA
AF Kienzle, Wolf
   Franz, Matthias O.
   Schoelkopf, Bernhard
   Wichmann, Felix A.
TI Center-surround patterns emerge as optimal predictors for human saccade
   targets
SO JOURNAL OF VISION
LA English
DT Article; Proceedings Paper
CT 20th Annual Conference on Neural Information Processing Systems (NIPS
   2006)
CY 2006
CL Vancouver, CANADA
DE visual saliency; eye movements; receptive field analysis; classification
   images; kernel methods; support vector machines; natural scenes
ID SUPERIOR COLLICULUS; VISUAL-ATTENTION; RECEPTIVE-FIELDS; EYE-MOVEMENTS;
   CLASSIFICATION IMAGES; SYSTEM-IDENTIFICATION; FIXATION SELECTION;
   STRIATE CORTEX; NATURAL SCENES; SEARCH
AB The human visual system is foveated, that is, outside the central visual field resolution and acuity drop rapidly. Nonetheless much of a visual scene is perceived after only a few saccadic eye movements, suggesting an effective strategy for selecting saccade targets. It has been known for some time that local image structure at saccade targets influences the selection process. However, the question of what the most relevant visual features are is still under debate. Here we show that center-surround patterns emerge as the optimal solution for predicting saccade targets from their local image structure. The resulting model, a one-layer feed-forward network, is surprisingly simple compared to previously suggested models which assume much more complex computations such as multi-scale processing and multiple feature channels. Nevertheless, our model is equally predictive. Furthermore, our findings are consistent with neurophysiological hardware in the superior colliculus. Bottom-up visual saliency may thus not be computed cortically as has been thought previously.
C1 [Wichmann, Felix A.] Tech Univ Berlin, Modelling Cognit Proc Grp, D-10587 Berlin, Germany.
   [Wichmann, Felix A.] Bernstein Ctr Computat Neurosci, Berlin, Germany.
   [Kienzle, Wolf; Franz, Matthias O.; Schoelkopf, Bernhard; Wichmann, Felix A.] Max Planck Inst Biol Cybernet, Empir Inference Dept, Tubingen, Germany.
   [Franz, Matthias O.] Univ Appl Sci, Cognit Syst Grp, Constance, Germany.
RP Wichmann, FA (reprint author), Tech Univ Berlin, Modelling Cognit Proc Grp, Sekr FR 6-4,Franklinstr 28-29, D-10587 Berlin, Germany.
EM felix.wichmann@tu-berlin.de
RI Scholkopf, Bernhard/A-7570-2013
OI Scholkopf, Bernhard/0000-0002-8177-0925
CR Baddeley RJ, 2006, VISION RES, V46, P2824, DOI 10.1016/j.visres.2006.02.024
   Basso MA, 1997, NATURE, V389, P66, DOI 10.1038/37975
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bruce ND, 2006, ADV NEURAL INFORM PR, P155
   CRONER LJ, 1995, VISION RES, V35, P7, DOI 10.1016/0042-6989(94)E0066-T
   CYNADER M, 1972, J NEUROPHYSIOL, V35, P187
   GOLDBERG ME, 1972, J NEUROPHYSIOL, V35, P560
   Harel J., 2007, ADV NEURAL INFORM PR, V19
   Henderson JM, 2003, TRENDS COGN SCI, V7, P498, DOI 10.1016/j.tics.2003.09.006
   Hopfinger JB, 2000, NAT NEUROSCI, V3, P284
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2006, VIS COGN, V14, P959, DOI 10.1080/13506280500195672
   Jakel F, 2007, J MATH PSYCHOL, V51, P343, DOI 10.1016/j.jmp.2007.06.002
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233
   JUNG R, 1970, EARLY EXPERIENCE VIS, P181
   KIENZLE W, 2007, ADV NEURAL INFORM PR, V19
   KIENZLE W, 2007, COMP SYST NEUR M COS
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krauzlis RJ, 2004, VISION RES, V44, P1445, DOI 10.1016/j.visres.2004.01.005
   Krauzlis RJ, 2002, NEURON, V35, P355, DOI 10.1016/S0896-6273(02)00756-0
   Krieger G, 2000, SPATIAL VISION, V13, P201, DOI 10.1163/156856800741216
   Kustov AA, 1996, NATURE, V384, P74, DOI 10.1038/384074a0
   Lau B, 2002, P NATL ACAD SCI USA, V99, P8974, DOI 10.1073/pnas.122173499
   LEHKY SR, 1992, J NEUROSCI, V12, P3568
   Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9
   Mannan SK, 1997, PERCEPTION, V26, P1059, DOI 10.1068/p261059
   Mannan SK, 1996, SPATIAL VISION, V10, P165, DOI 10.1163/156856896X00123
   MATIN E, 1974, PSYCHOL BULL, V81, P899, DOI 10.1037/h0037368
   McPeek RM, 2004, NAT NEUROSCI, V7, P757, DOI 10.1038/nn1269
   McPeek RM, 2002, J NEUROPHYSIOL, V88, P2019, DOI 10.1152/jn.00181.2002
   Neri P, 2006, VISION RES, V46, P2465, DOI 10.1016/j.visres.2006.02.002
   OLIVA A, 2003, IEEE P INT C IM PROC, V1, P253
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Parkhurst DJ, 2003, SPATIAL VISION, V16, P125, DOI 10.1163/15685680360511645
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Prenger R, 2004, NEURAL NETWORKS, V17, P663, DOI 10.1016/j.neunet.2004.03.008
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Raj R, 2005, J OPT SOC AM A, V22, P2039, DOI 10.1364/JOSAA.22.002039
   RAJASHEKAR U, 2002, VIS SCI SOC 2 ANN M
   Rajashekar U, 2006, J VISION, V6, P379, DOI 10.1167/6.4.7
   Reinagel P, 1999, NETWORK-COMP NEURAL, V10, P341, DOI 10.1088/0954-898X/10/4/304
   Renninger Laura Walker, 2005, Adv Neural Inf Process Syst, V17, P1121
   Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641
   Scholkopf B., 2002, LEARNING KERNELS
   Steinwart I., 2001, J MACHINE LEARNING R, V2, P67
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Tavassoli A, 2007, PERCEPT PSYCHOPHYS, V69, P103, DOI 10.3758/BF03194457
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Treue S, 2003, CURR OPIN NEUROBIOL, V13, P428, DOI 10.1016/S0959-4388(03)00105-3
   Victor JD, 2005, NAT NEUROSCI, V8, P1651, DOI 10.1038/nn1607
   WICHMANN FA, 2005, ADV NEURAL INFORM PR, V17, P1489
   Wu MCK, 2006, ANNU REV NEUROSCI, V29, P477, DOI 10.1146/annurev.neuro.29.051605.113024
   Yarbus AL., 1967, EYE MOVEMENTS VISION
NR 55
TC 70
Z9 72
U1 0
U2 10
PU ASSOC RESEARCH VISION OPHTHALMOLOGY INC
PI ROCKVILLE
PA 12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA
SN 1534-7362
J9 J VISION
JI J. Vision
PY 2009
VL 9
IS 5
AR 7
DI 10.1167/9.5.7
PG 15
WC Ophthalmology
SC Ophthalmology
GA 461RN
UT WOS:000267288600007
PM 19757885
OA Other Gold, Bronze
DA 2019-06-15
ER

PT S
AU Paciorek, CJ
   Schervish, MJ
AF Paciorek, CJ
   Schervish, MJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Nonstationary covariance functions for Gaussian process regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID BAYESIAN REGRESSION; SPLINES
AB We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matern stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from fixing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP fitting may allow for implementation of the method on larger datasets.
C1 Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
RP Paciorek, CJ (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
CR Biller C, 2000, J COMPUT GRAPH STAT, V9, P122, DOI 10.2307/1390616
   BRUNTZ SM, 1974, S ATM DIFF AIR POLL, P125
   Damian D, 2001, ENVIRONMETRICS, V12, P161, DOI 10.1002/1099-095X(200103)12:2<161::AID-ENV452>3.0.CO;2-G
   Denison DGT, 1998, STAT COMPUT, V8, P337, DOI 10.1023/A:1008824606259
   DiMatteo I, 2001, BIOMETRIKA, V88, P1055, DOI 10.1093/biomet/88.4.1055
   FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
   GIBBS MN, 1997, BAYESIAN GAUSSIAN PR
   HIGDON D, 1999, BAYESIAN STAT, V6, P761
   Holmes CC, 2001, J ROY STAT SOC B, V63, P3, DOI 10.1111/1467-9868.00272
   Lockwood JR, 2001, J AM STAT ASSOC, V96, P1184, DOI 10.1198/016214501753381832
   MACKAY D, 1995, INTERPOLATION MODELS
   Paciorek C, 2003, THESIS CARNEGIE MELL
   RASMUSSEN CE, 2002, ADV NEURAL INFORMATI, V14
   SCHMIDT AM, 2000, 49800 U SHEFFI
   Seeger M., 2003, WORKSH AI STAT, V9
   SMOLA AJ, 2001, ADV NEURAL INFORMATI, V13
   Stein M. L., 1999, INTERPOLATION SPATIA
   Tresp V, 2001, ADV NEUR IN, V13, P654
   VIVARELLI F, 1999, ADV NEURAL INFORMATI, V11
   Wood SA, 2002, BIOMETRIKA, V89, P513, DOI 10.1093/biomet/89.3.513
NR 20
TC 63
Z9 64
U1 0
U2 4
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 273
EP 280
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500035
DA 2019-06-15
ER

PT S
AU Cortes, C
   Mohri, M
AF Cortes, C
   Mohri, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI AUC optimization vs. error rate minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classification algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the first exact expression of the expected value and the variance of the AUC for a fixed error rate. Our results show that the average AUC is monotonically increasing as a function of the classification accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the benefits of an algorithm specifically designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC.
C1 AT&T Labs Res, Florham Pk, NJ 07932 USA.
RP Cortes, C (reprint author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.
CR Breiman L., 1984, CLASSIFICATION REGRE
   CHAUCHAT JH, 2001, TARGETING CUSTOMER G
   EGAN JP, 1975, SIGNAL DETECTION THE
   FERRI C, 2002, ICML 2002
   FREUND Y, 1995, P 2 EUR C COMP LEARN, V2
   FREUND YR, 1998, ICML 98
   GREEN DM, 1966, SIGNAL DETECTION THE
   Hanley JA, 1982, RADIOLOGY
   MOZER MC, 2002, NIPS 2002
   PERLICH C, 2003, J MACHINE LEARNING R
   PIATETSKYSHAPIR.G, 2000, SIGKDD EXPLORATIONS
   PROVOST F, 1997, KDD 97
   ROSSET S, 2001, KDD 2001
   ROSSET S, 1999, THESIS TEL AVIV U
   YAN L, 2003, ICML 2003
NR 15
TC 62
Z9 65
U1 0
U2 5
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 313
EP 320
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500040
DA 2019-06-15
ER

PT S
AU Schultz, M
   Joachims, T
AF Schultz, M
   Joachims, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning a distance metric from relative comparisons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB This paper presents a method for learning a distance metric from relative comparison such as "A is closer to B than A is to C". Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a flexible way of describing qualitative training data as a set of constraints. We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training. We empirically evaluate the performance and the modelling flexibility of the algorithm on a collection of text documents.
C1 Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Schultz, M (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
CR BUJA A, IN PRESS J COMPUTATI
   Cohn D., 2003, TR20031892 CORN U
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cox T. F., 1994, MULTIDIMENSIONAL SCA
   CRAVEN M, 1998, P 15 NAT C ART INT
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Joachims T, 2002, P ACM C KNOWL DISC D
   TSANG I, 2003, P INT C ART NEUR NET
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wagstaff K., 2001, P 18 INT C MACH LEAR, V18, P577, DOI 10.1109/TPAMI.2002.1017616
   XING EP, 2002, ADV NEURAL INFORMATI
NR 11
TC 62
Z9 62
U1 0
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 41
EP 48
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500006
DA 2019-06-15
ER

PT J
AU Ito, M
   Noda, K
   Hoshino, Y
   Tani, J
AF Ito, Masato
   Noda, Kuniaki
   Hoshino, Yukiko
   Tani, Jun
TI Dynamic and interactive generation of object handling behaviors by a
   small humanoid robot using a dynamic neural network model
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE learning of object handling behavior; dynamical systems approach;
   recurrent neural network
ID CHIMPANZEES PAN-TROGLODYTES; MANIPULATORY ACTIONS; IMITATION; SYSTEMS;
   TIME; TASK
AB This study presents experiments on the learning of object handling behaviors by a small humanoid robot using a dynamic neural network model, the recurrent neural network with parametric bias (RNNPB). The first experiment showed that after the robot learned different types of ball handling behaviors using human direct teaching, the robot was able to generate adequate ball handling motor sequences situated to the relative position between the robot's hands and the ball. The same scheme was applied to a block handling learning task where it was shown that the robot can switch among learned different block handling sequences, situated to the ways of interaction by human supporters. Our analysis showed that entrainment of the internal memory structures of the RNNPB through the interactions of the objects and the human supporters are the essential mechanisms for those observed situated behaviors of the robot (c) 2006 Elsevier Ltd. All rights reserved.
C1 Sony Intelligence Dynam Labs Inc, Shinagawa Ku, Tokyo 1410022, Japan.
   RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
RP Ito, M (reprint author), Sony Intelligence Dynam Labs Inc, Shinagawa Ku, Takanawa Muse Bldg 4F,3-14-13, Tokyo 1410022, Japan.
EM masato@idl.sony.co.jp; noda@idl.sony.co.jp; yukiko@idl.sony.co.jp;
   tani@brain.riken.go.jp
CR Andry P, 2001, IEEE T SYST MAN CY A, V31, P431, DOI 10.1109/3468.952717
   Baron-Cohen S., 1996, MINDBLINDNESS ESSAY
   BEER RD, 1995, ARTIF INTELL, V72, P173, DOI 10.1016/0004-3702(94)00005-L
   Bianco R, 2004, ADAPT BEHAV, V12, P37, DOI 10.1177/105971230401200102
   Billard A, 2002, FROM ANIM ANIMAT, P281
   CALINON S, 2005, P IEEE INT C ROB AUT
   Dautenhahn K, 2002, IMITATION ANIMALS AR
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Ijspeert A.J., 2003, ADV NEURAL INFORM PR, P1547
   Ito M, 2004, ADAPT BEHAV, V12, P93, DOI 10.1177/105971230401200202
   ITO M, 2004, P 11 INT C NEUR INF, P592
   Ito S, 2003, SCIENCE, V302, P120, DOI 10.1126/science.1087847
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Kosuge K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3459
   Maitland F., 1994, J CONTIN EDUC HEALTH, V14, P4, DOI DOI 10.1002/CHP.4750140102
   MILNER B, 1963, ARCH NEUROL-CHICAGO, V9, P90, DOI 10.1001/archneur.1963.00460070100010
   Myowa-Yamakoshi M, 2000, J COMP PSYCHOL, V114, P381, DOI 10.1037//0735-7036.114.4.381
   Myowa-Yamakoshi M, 1999, J COMP PSYCHOL, V113, P128, DOI 10.1037/0735-7036.113.2.128
   Nakahara K, 2002, SCIENCE, V295, P1532, DOI 10.1126/science.1067653
   Nehaniv C, 2001, CYBERNET SYST, V32, P1, DOI 10.1080/019697201300001795
   Ogata T, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P162
   POLLACK JB, 1991, MACH LEARN, V7, P227, DOI 10.1007/BF00114845
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED
   Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3
   Schaal S, 2003, PHILOS T R SOC B, V358, P537, DOI 10.1098/rstb.2002.1258
   Schaal S, 1996, J MOTOR BEHAV, V28, P165, DOI 10.1080/00222895.1996.9941743
   Sugita Y, 2005, ADAPT BEHAV, V13, P33, DOI 10.1177/105971230501300102
   Tani J, 1999, NEURAL NETWORKS, V12, P1131, DOI 10.1016/S0893-6080(99)00060-X
   Tani J, 2003, IEEE T SYST MAN CY A, V33, P481, DOI 10.1109/TSMCA.2003.809171
   TANI J, 1995, BIOL CYBERN, V72, P365, DOI 10.1007/s004220050138
   Tani J, 2003, NEURAL NETWORKS, V16, P11, DOI 10.1016/S0893-6080(02)00214-9
   Tani J., 2005, P 2 INT WORKSH MAN M, P123
   VANGELDER T, 1998, BEHAV BRAIN SCI, V150, P45
   Weigend AS, 1995, INT J NEURAL SYST, V6, P373, DOI 10.1142/S0129065795000251
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
   Yokoyama K., 2003, P IEEE INT C ROB AUT, P2985
NR 36
TC 59
Z9 59
U1 1
U2 6
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 323
EP 337
DI 10.1016/j.neunet.2006.02.007
PG 15
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600007
PM 16618536
DA 2019-06-15
ER

PT S
AU Bakir, GH
   Weston, J
   Scholkopf, B
AF Bakir, GH
   Weston, J
   Scholkopf, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning to find pre-images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difficult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Bakir, GH (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
RI Scholkopf, Bernhard/A-7570-2013
OI Scholkopf, Bernhard/0000-0002-8177-0925
CR Altun Y., 2003, 20 INT C MACH LEARN
   BURGES CJC, 1996, P 13 INT C MACH LEAR, P77
   HAUSSLER D, 1999, UCSCCRL9910 COMP SCI
   Hua SJ, 2001, J MOL BIOL, V308, P397, DOI 10.1006/jmbi.2001.4580
   KWOK JT, 2002, NIPS 2002 WORKSH KER
   Leslie C, 2002, P PAC S BIOC
   Lodhi H, 2002, J MACH LEARN RES, V2, P419, DOI 10.1162/153244302760200687
   Mika S, 1999, ADV NEUR IN, V11, P536
   Romdhani S., 1999, P BMVC, P483
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf B., 2002, LEARNING KERNELS
   WESTON J, 2002, ADV NEURAL INFORMATI, V15
NR 12
TC 54
Z9 55
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 449
EP 456
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500057
DA 2019-06-15
ER

PT J
AU Paninski, L
AF Paninski, L
TI Asymptotic theory of information-theoretic experimental design
SO NEURAL COMPUTATION
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
ID OPTIMIZATION; STIMULUS; CORTEX
AB We discuss an idea for collecting data in a relatively efficient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. In particular, we calculate the asymptotic efficiency of the information-maximization strategy and demonstrate that this method is in a well-defined sense never less efficient-and is generically more efficient-than the nonadaptive strategy. For example, we are able to explicitly calculate the asymptotic relative efficiency of the staircase method widely employed in psychophysics research and to demonstrate the dependence of this efficiency on the form of the psychometric function underlying the output responses.
C1 UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.
RP Paninski, L (reprint author), UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.
EM liam@gatsby.ucl.ac.uk
CR AXELROD S, 2001, INFORMATION OBSERVAT
   Barron A, 1999, ANN STAT, V27, P536
   BELL RM, 1980, MATH OPER RES, V5, P161, DOI 10.1287/moor.5.2.161
   Berger J. O., 1989, RECENT DEV STAT THEI, P1
   Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   CLARKE BS, 1990, IEEE T INFORM THEORY, V36, P453, DOI 10.1109/18.54897
   CLARKE BS, 1994, J STAT PLAN INFER, V41, P37, DOI 10.1016/0378-3758(94)90153-8
   Clyde M, 1996, J AM STAT ASSOC, V91, P1236, DOI 10.2307/2291742
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   COVER TM, 1991, ELEMENTS INFORMATION
   Dayan P, 2001, THEORETICAL NEUROSCI
   DEIGNAN P, 2000, AM CONTR C 2000 CHIC
   Dembo  A., 1993, LARGE DEVIATIONS TEC
   DENZLER J, 2000, 732 U ROCH
   Fedorov V. V., 1972, THEORY OPTIMAL EXPT
   Foldiak P, 2001, NEUROCOMPUTING, V38, P1217, DOI 10.1016/S0925-2312(01)00570-7
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Kontsevich LL, 1999, VISION RES, V39, P2729, DOI 10.1016/S0042-6989(98)00285-5
   LEE T, 1998, ADV NEURAL INFORMATI, V12
   Lewis AS, 1996, SIAM J OPTIMIZ, V6, P164, DOI 10.1137/0806009
   LINDLEY D, 1956, ANN MATH STAT, V29, P986
   LUTTRELL SP, 1985, INVERSE PROBL, V1, P199, DOI 10.1088/0266-5611/1/3/006
   Machens CK, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.228104
   Mackay D, 1992, NEURAL COMPUT, V4, P589, DOI DOI 10.1162/NECO.1992.4.4.590
   MASCARO M, 2002, UNPUB OPTIMIZED NEUR
   Milman V. D., 1986, ASYMPTOTIC THEORY FI
   NELKEN I, 1994, HEARING RES, V72, P237, DOI 10.1016/0378-5955(94)90222-4
   NELSON J, 2000, ADV NEURAL INFORMATI, V13
   PARMIGIANI G, 1994, ASPECTS UNCERTAINTY, P333
   Parmigiani G., 1998, SANKHYA, V60, P446
   PELLI D G, 1987, Investigative Ophthalmology and Visual Science, V28, P366
   Rudin W, 1973, FUNCTIONAL ANAL
   SAHANI M, 1997, NIC97 M SNOWB UT
   Schervish M. J., 1995, THEORY STAT
   SCHOLL HR, 1998, TEST, V7
   Schwartz L., 1965, Z WAHRSCH VERW GEBIE, V4, P10, DOI DOI 10.1007/BF00535479
   SIMONCELLI E, 2004, COGNITIVE NERUOSCIEN
   Sollich P, 1996, PHYS REV E, V53, pR2060, DOI 10.1103/PhysRevE.53.R2060
   TALAGRAND M, 1995, PUBLICATIONS IHES, V81, P73
   TZANAKOU E, 1979, BIOL CYBERN, V35, P161, DOI 10.1007/BF00337061
   VANDERVAARD A, 1998, ASYMPTOTIC STAT
   WATSON AB, 1990, PERCEPT PSYCHOPHYS, V47, P87, DOI 10.3758/BF03208169
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
NR 44
TC 52
Z9 52
U1 0
U2 4
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN 0899-7667
EI 1530-888X
J9 NEURAL COMPUT
JI Neural Comput.
PD JUL
PY 2005
VL 17
IS 7
BP 1480
EP 1507
DI 10.1162/0899766053723032
PG 28
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 924LK
UT WOS:000228980100003
PM 15901405
DA 2019-06-15
ER

PT S
AU Hamerly, G
   Elkan, C
AF Hamerly, G
   Elkan, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning the k in k-means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB When clustering a dataset, the right number k of clusters to use is often not obvious, and choosing k automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning k while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs k-means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level a. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model's complexity.
C1 Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.
RP Hamerly, G (reprint author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.
CR Bischof H, 1999, PATTERN ANAL APPL, V2, P59, DOI 10.1007/s100440050015
   Blake C., 1998, UCI REPOSITORY MACHI
   DELCORSO GM, 2000, SIAM J MATRIX ANAL A, P143
   DING C, 2002, P 2 IEEE INT C DAT M
   Farnstrom F., 2000, SIGKDD EXPLORATIONS, V2, P51, DOI DOI 10.1145/360402.360419
   HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327
   KEARNS MJ, 1995, COMPUTATIONAL LEARNI, P21
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   NG AY, 2002, NEURAL INFORMATION P, V14
   Pelleg D, 2000, P 17 INT C MACH LEAR
   SAND P, 2001, P 18 INT C MACH LEAR
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   STEPHENS MA, 1974, J AM STAT ASSOC, V69, P730, DOI 10.2307/2286009
NR 16
TC 50
Z9 55
U1 0
U2 6
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 281
EP 288
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500036
DA 2019-06-15
ER

PT J
AU Hirsch, JA
AF Hirsch, JA
TI Synaptic physiology and receptive field structure in the early visual
   pathway of the cat
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID LATERAL GENICULATE-NUCLEUS; CORTICAL SIMPLE CELLS; NEURONS IN-VIVO;
   STRIATE CORTEX; ORIENTATION SELECTIVITY; SOMATOSENSORY CORTEX; SPATIAL
   SUMMATION; COMPLEX CELLS; SPATIOTEMPORAL ORGANIZATION; LOCAL CIRCUITS
AB How does the cortical circuitry analyze the visual scene? Here we explore the earliest levels of striate cortical processing: the first stage, where orientation sensitivity emerges, and the second stage, where stimulus selectivity is further refined. The approach is wholecell recording from cat in vivo. Neurons in the lateral geniculate nucleus of the thalamus have circular receptive fields whose subregions, center and surround are concentrically arranged and have the reverse sign, on or off. These neurons supply cortical simple cells, whose receptive fields have on and off subregions that are elongated and lie side by side. Feedforward models hold that orientation sensitivity depends on this thalamocortical change in receptive field structure and an arrangement within subregions such that stimuli of the reverse contrast evoke synaptic responses of the opposite polarity-push-pull. Our work provides support for feedforward models and emphasizes that push-pull is key in the geniculostriate pathway, preserved from retina by thalamic relay cells and reiterated, point by point, by cortical simple cells. Also, we help define the cortical push-pull circuit by identifying inhibitory simple cells. Lastly, separate experiments that compare the first and second levels of cortical processing suggest that differences in the synaptic physiology of connections at the two (thalamocortical versus intracortical) stages underlie differential selectivity for properties such as motion.
C1 Univ So Calif, Dept Biol Sci, Los Angeles, CA 90089 USA.
   Univ So Calif, Grad Program Neurosci, Los Angeles, CA 90089 USA.
RP Hirsch, JA (reprint author), Univ So Calif, Dept Biol Sci, HNB 328,M-C 2520,3641 Watt Way, Los Angeles, CA 90089 USA.
RI Hirsch, Judith/E-9006-2013
FU NEI NIH HHS [EY09395]
CR Adorjan P, 1999, VISUAL NEUROSCI, V16, P303, DOI 10.1017/S0952523899162114
   ALLEN C, 1994, P NATL ACAD SCI USA, V91, P10380, DOI 10.1073/pnas.91.22.10380
   Alonso JM, 2001, J NEUROSCI, V21, P4002, DOI 10.1523/JNEUROSCI.21-11-04002.2001
   Anderson JS, 2001, J NEUROSCI, V21, P2104, DOI 10.1523/JNEUROSCI.21-06-02104.2001
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   BERNANDER O, 1991, P NATL ACAD SCI USA, V88, P11569, DOI 10.1073/pnas.88.24.11569
   Borg-Graham LJ, 1998, NATURE, V393, P369, DOI 10.1038/30735
   Brumberg JC, 1999, J NEUROPHYSIOL, V82, P1808
   BULLIER J, 1979, J NEUROPHYSIOL, V42, P1271
   BULLIER J, 1979, J NEUROPHYSIOL, V42, P274
   Cai DQ, 1997, J NEUROPHYSIOL, V78, P1045
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   CHAPMAN B, 1991, J NEUROSCI, V11, P1347
   Chung S, 1998, NEURON, V20, P1177, DOI 10.1016/S0896-6273(00)80498-5
   DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1118
   DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1091
   DEANGELIS GC, 1995, TRENDS NEUROSCI, V18, P451, DOI 10.1016/0166-2236(95)94496-R
   Debanne D, 1998, J PHYSIOL-LONDON, V508, P523, DOI 10.1111/j.1469-7793.1998.00523.x
   Destexhe A, 1999, J NEUROPHYSIOL, V81, P1531
   DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624
   FATT P, 1951, J PHYSIOL-LONDON, V115, P320, DOI 10.1113/jphysiol.1951.sp004675
   Feldmeyer D, 1999, J PHYSIOL-LONDON, V521, P169, DOI 10.1111/j.1469-7793.1999.00169.x
   Feldmeyer D, 2002, J PHYSIOL-LONDON, V538, P803, DOI 10.1013/jphysiol.2001.012959
   Feldmeyer D, 2000, J PHYSIOL-LONDON, V525, P31, DOI 10.1111/j.1469-7793.2000.00031.x
   Ferster D, 2000, ANNU REV NEUROSCI, V23, P441, DOI 10.1146/annurev.neuro.23.1.441
   FERSTER D, 1986, J NEUROSCI, V6, P1284
   FERSTER D, 1983, J PHYSIOL-LONDON, V342, P181, DOI 10.1113/jphysiol.1983.sp014846
   FERSTER D, 1988, J NEUROSCI, V8, P1172
   Ferster D, 1996, NATURE, V380, P249, DOI 10.1038/380249a0
   Fitzpatrick D, 1996, CEREB CORTEX, V6, P329, DOI 10.1093/cercor/6.3.329
   Fregnac Y, 1996, J PHYSIOL-PARIS, V90, P113, DOI 10.1016/S0928-4257(97)81412-X
   Gardner JL, 1999, VISUAL NEUROSCI, V16, P1115, DOI 10.1017/S0952523899166112
   Gil Z, 1999, NEURON, V23, P385, DOI 10.1016/S0896-6273(00)80788-6
   GILBERT CD, 1979, NATURE, V280, P120, DOI 10.1038/280120a0
   GILBERT CD, 1977, J PHYSIOL-LONDON, V268, P391, DOI 10.1113/jphysiol.1977.sp011863
   HEGGELUND P, 1986, J PHYSIOL-LONDON, V373, P293, DOI 10.1113/jphysiol.1986.sp016048
   Hirsch JA, 1998, J NEUROSCI, V18, P9517
   HIRSCH JA, 1995, J PHYSIOL-LONDON, V483, P183, DOI 10.1113/jphysiol.1995.sp020577
   HIRSCH JA, 2000, SOC NEUR ABSTR, V26, P108
   HIRSCH JA, 2002, J PHYSL, V540, P235
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1961, J PHYSIOL-LONDON, V155, P385, DOI 10.1113/jphysiol.1961.sp006635
   HUMPHREY AL, 1985, J COMP NEUROL, V233, P159, DOI 10.1002/cne.902330203
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1187
   KUFFLER SW, 1953, J NEUROPHYSIOL, V16, P37
   Lampl I, 2001, NEURON, V30, P263, DOI 10.1016/S0896-6273(01)00278-1
   LEVAY S, 1976, BRAIN RES, V113, P1, DOI 10.1016/0006-8993(76)90002-0
   MARTIN KAC, 1984, J PHYSIOL-LONDON, V353, P463, DOI 10.1113/jphysiol.1984.sp015347
   Martinez LM, 2002, J PHYSIOL-LONDON, V540, P321, DOI 10.1113/jphysiol.2001.012776
   MARTINEZ LM, 1998, SOC NEUR ABSTR, V24, P1048
   Martinez Luis M., 1999, Society for Neuroscience Abstracts, V25, P1048
   MCILWAIN JT, 1967, J NEUROPHYSIOL, V30, P1
   McLaughlin D, 2000, P NATL ACAD SCI USA, V97, P8087, DOI 10.1073/pnas.110135097
   Moore CI, 1998, J NEUROPHYSIOL, V80, P2882
   MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P53, DOI 10.1113/jphysiol.1978.sp012488
   MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P79, DOI 10.1113/jphysiol.1978.sp012489
   PALMER LA, 1981, J NEUROPHYSIOL, V46, P260
   Pare D, 1998, J NEUROPHYSIOL, V79, P1450
   REID RC, 1995, NATURE, V378, P281
   Ringach DL, 1997, NATURE, V387, P281, DOI 10.1038/387281a0
   SHAPLEY R, 1985, ANNU REV NEUROSCI, V8, P547, DOI 10.1146/annurev.ne.08.030185.002555
   Sherman SM, 2001, EXPLORING THALAMUS
   SILLITO AM, 1985, MODELS VISUAL CORTEX, P396
   SKOTTUN BC, 1991, VISION RES, V31, P1079
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   SOMPOLINSKY H, 1997, CURR OPIN NEUROBIOL, V7, P515
   Stratford KJ, 1996, NATURE, V382, P258, DOI 10.1038/382258a0
   Swadlow HA, 2000, J NEUROPHYSIOL, V83, P2802
   SZULBORSKI RG, 1990, VISION RES, V30, P249, DOI 10.1016/0042-6989(90)90040-R
   TANAKA K, 1983, J NEUROPHYSIOL, V49, P1303
   TOLHURST DJ, 1987, EXP BRAIN RES, V66, P607, DOI 10.1007/BF00270694
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   Usrey WM, 1999, J NEUROPHYSIOL, V82, P3527
   Usrey WM, 2000, J NEUROSCI, V20, P5461, DOI 10.1523/JNEUROSCI.20-14-05461.2000
   VOLGUSHEV M, 1993, EXCITATION INHIBITIO
   Wielaard DJ, 2001, J NEUROSCI, V21, P5203, DOI 10.1523/JNEUROSCI.21-14-05203.2001
   Wolfe J, 1998, VISUAL NEUROSCI, V15, P653, DOI 10.1017/S0952523898154068
   Zhu JJ, 1999, J NEUROPHYSIOL, V81, P1171
NR 78
TC 50
Z9 50
U1 0
U2 3
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 63
EP 69
DI 10.1093/cercor/13.1.63
PG 7
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500009
PM 12466216
OA Bronze
DA 2019-06-15
ER

PT J
AU Jutten, C
   Babaie-Zadeh, M
   Hosseini, S
AF Jutten, C
   Babaie-Zadeh, M
   Hosseini, S
TI Three easy ways for separating nonlinear mixtures?
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE source separation; independence; nonlinear; mutual information
ID INDEPENDENT COMPONENT ANALYSIS; BLIND SOURCE SEPARATION; LEARNING
   ALGORITHM; SIGNAL SEPARATION; MODELS; IDENTIFICATION; SYSTEMS
AB In this paper, we consider the nonlinear Blind Source Separation BSS and independent component analysis (ICA) problems, and especially uniqueness issues, presenting some new results. A fundamental difficulty in the nonlinear BSS problem and even more so in the nonlinear ICA problem is that they are nonunique without a suitable regularization. In this paper, we mainly discuss three different ways for regularizing the solutions, that have been recently explored. (C) 2003 Elsevier B.V. All rights reserved.
C1 Inst Natl Polytech Grenoble, CNRS, UMR 5083, Lab Images & Signaux, F-38031 Grenoble, France.
RP Jutten, C (reprint author), Inst Natl Polytech Grenoble, CNRS, UMR 5083, Lab Images & Signaux, Ave Felix Viallet, F-38031 Grenoble, France.
EM christian.jutten@inpg.fr; mbzadeh@yahoo.com; hosseini@cict.fr
CR Aczel J., 1966, LECT FUNCTIONAL EQUA
   ALMEIDA L, 2001, REALISTIC MODELS NON
   Almeida LB, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P117, DOI 10.1109/ASSPCC.2000.882457
   Amari S, 1996, ADV NEUR IN, V8, P757
   Babaie-Zadeh M., 2002, P 11 EUR SIGN PROC C, V2, P11
   Babaie- Zadeh M., 2001, P 3 WORKSH IND COMP, P138
   BABAIEZADEH M, 2002, THESIS INPG GRENOBLE
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307
   CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054
   Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250
   Cardoso JF, 1996, IEEE T SIGNAL PROCES, V44, P3017, DOI 10.1109/78.553476
   CICHOCKI A, 1994, ELECTRON LETT, V30, P1386, DOI 10.1049/el:19940956
   Cichocki A., 2002, ADAPTIVE BLIND SIGNA
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   DARMOIS G, 1951, P INT STAT C 1947 A, V3, P231
   Deville Y, 2002, NEUROCOMPUTING, V49, P369, DOI 10.1016/S0925-2312(02)00514-3
   ERIKSSON J, 2002, P 11 EUR SIGN PROC C, V2, P7
   Haykin  S.S., 1998, NEURAL NETWORKS COMP
   Hosseini S, 2003, IEEE SIGNAL PROC LET, V10, P43, DOI 10.1109/LSP.2002.807871
   Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   JUTTEN C, 2000, P 2 INT WORKSH IND C, P15
   Kagan A. M., 1973, COMM STAT, V1, P471
   KORENBERG M, 1995, BIOL CYBERN, V43, P125
   LUKACS E, 1955, ANN MATH STAT, V26, P319, DOI 10.1214/aoms/1177728549
   Makeig S, 1997, P NATL ACAD SCI USA, V94, P10979, DOI 10.1073/pnas.94.20.10979
   MATSUOKA K, 1995, NEURAL NETWORKS, V8, P411, DOI 10.1016/0893-6080(94)00083-X
   Nguyen Thi H. L., 1996, Applied Signal Processing, V3, P177
   Paraschiv-Ionescu A, 2002, IEEE SENS J, V2, P663, DOI 10.1109/JSEN.2002.807302
   Parra L, 2000, ADV NEUR IN, V12, P942
   Pham DT, 2001, IEEE T SIGNAL PROCES, V49, P1837, DOI 10.1109/78.942614
   Popovic R. S., 1991, HALL EFFECT DEVICES
   Prakriya S, 1995, IEEE T SIGNAL PROCES, V43, P3007, DOI 10.1109/78.476444
   STOCKHAM TG, 1975, P IEEE, V63, P678, DOI 10.1109/PROC.1975.9800
   Taleb A, 2002, IEEE T SIGNAL PROCES, V50, P1819, DOI 10.1109/TSP.2002.800399
   Taleb A, 2001, IEEE T SIGNAL PROCES, V49, P917, DOI 10.1109/78.917796
   Taleb A, 1999, IEEE T SIGNAL PROCES, V47, P2807, DOI 10.1109/78.790661
   THEIS F, 2001, P INT C IND COMP AN, P669
   TONG L, 1990, P IEEE INT S CIRC SY
   Torkkola K., 1999, P 1 INT WORKSH IND C, P239
   Valpola H, 2002, NEURAL COMPUT, V14, P2647, DOI 10.1162/089976602760408017
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577
   Vigario R, 2000, IEEE T BIO-MED ENG, V47, P589, DOI 10.1109/10.841330
   Yang HH, 1998, SIGNAL PROCESS, V64, P291, DOI 10.1016/S0165-1684(97)00196-5
   ZIEHE A, 2001, P INT WORKSH IND COM, P433
NR 49
TC 48
Z9 48
U1 0
U2 0
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1872-7557
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 217
EP 229
DI 10.1016/j.sigpro.2003.10.011
PG 13
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600002
DA 2019-06-15
ER

PT S
AU Bach, FR
   Jordan, MI
AF Bach, FR
   Jordan, MI
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning spectral clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors.
C1 Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Bach, FR (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
CR BACH FR, LEARNING SPECTRAL CL
   BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016
   CHAN PK, 1994, IEEE T COMPUT AID D, V13, P1088, DOI 10.1109/43.310898
   CRISTIANINI N, 2002, NIPS, V14
   Cu M., 2001, SPECTRAL RELAXATION
   Golub G. H., 1996, MATRIX COMPUTATIONS
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Kamvar S. D., 2003, IJCAI
   Martin  D., 2001, ICCV
   MEILA M, 2002, NIPS, V13
   Ng A. Y, 2001, NIPS, V14
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Wagstaff K., 2001, ICML
   Xing E. P., 2003, NIPS, V15
   YU SX, 2002, NIPS, V14
   ZHA H, 2002, NIPS, V14
NR 16
TC 46
Z9 46
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 305
EP 312
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500039
DA 2019-06-15
ER

PT S
AU Rasmussen, CE
   Kuss, M
AF Rasmussen, CE
   Kuss, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Gaussian processes in reinforcement learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Rasmussen, CE (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
CR Attias H, 2003, P 9 INT WORKSH ART I
   DEARDEN RN, 1998, 15 NAT C ART INT AAA
   Dietterich TG, 2002, ADV NEUR IN, V14, P1491
   GIRARD A, 2002, ADV NEURAL INFORMATI, V15
   Moore AW, 1995, MACH LEARN, V21, P199, DOI 10.1007/BF00993591
   QUINONEROCANDEL.J, 2003, P 2003 IEEE C AC SPE
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   WILLIAMS CKJ, 1996, ADV NEURAL INFORMATI, V8
NR 8
TC 45
Z9 45
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 751
EP 758
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500094
DA 2019-06-15
ER

PT S
AU Shental, N
   Bar-Hillel, A
   Hertz, T
   Weinshall, D
AF Shental, N
   Bar-Hillel, A
   Hertz, T
   Weinshall, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Computing Gaussian mixture models with EM using equivalence constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are defined on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information.
C1 Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.
RP Shental, N (reprint author), Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.
RI Hillel, Aharon Bar/R-2656-2016; Hertz, Tomer/S-5744-2016
OI Hertz, Tomer/0000-0002-0561-1578; Bar-Hillel, Aharon/0000-0002-7303-0687
CR DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Georghiades A. S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P277, DOI 10.1109/AFGR.2000.840647
   KLEIN D, 2002, ICML
   Miller DJ, 1997, ADV NEUR IN, V9, P571
   Nigam K., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence, P792
   PHILLIPS PJ, 1998, NIPS, V11, P803
   SHENTAL N, 2002, COMPUTER VISION ECCV, V4, P776
   SZUMMER M, 2001, NIPS, V14
   Wagstaff K., 2001, P 18 INT C MACH LEAR, V18, P577, DOI 10.1109/TPAMI.2002.1017616
   XING EP, 2002, ADV NEURAL INFORMATI, V15
NR 10
TC 45
Z9 47
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 465
EP 472
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500059
DA 2019-06-15
ER

PT J
AU Miller, KD
AF Miller, KD
TI Understanding layer 4 of the cortical circuit: A model based on cat V1
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID PRIMARY VISUAL-CORTEX; FUNCTIONALLY CHARACTERIZED SITES; GABA-INDUCED
   INACTIVATION; SIMPLE CELLS; ORIENTATION SELECTIVITY; STRIATE CORTEX;
   DIRECTION SELECTIVITY; POSTSYNAPTIC POTENTIALS; SYNAPTIC POTENTIALS;
   LAMINAR DIFFERENCES
AB This paper reviews theoretical and experimental results on the processing of layer 4, the input-recipient layer, of cat primary visual cortex (V1). A wide range of experimental data can be understood from a model in which response tuning of layer 4 cells is largely determined by a local interplay of feedforward excitation (from thalamus) and feedforward inhibition (from layer 4 inhibitory interneurons driven by thalamus). Feedforward inhibition dominates excitation, inherits its tuning from the thalamic input and sharpens the tuning of excitatory cells. At least a strong component of the feedforward inhibition received by a cell is spatially opponent to the excitation it receives, meaning that inhibition is driven by dark in regions of the visual field in which excitation is driven by light, and vice versa. The idea of opponent inhibition can be generalized to mean inhibition driven by input patterns that are strongly anti-correlated with the patterns that excite a cell. This paper argues that dominant feedforward opponent inhibition may be a general principle of cortical layer 4. This leads to the suggestion that the properties that show columnar organization - invariance across the vertical depth of cortex - may be properties that are shared by 'opposite' (anticorrelated) stimulus pairs. This contrasts with the more common idea that a column represents a set of cells that all share similar stimulus preferences.
C1 Univ Calif San Francisco, Dept Physiol, Sloan Swartz Ctr Theoret Neurobiol, WM Keck Ctr Integrat Neurosci, San Francisco, CA 94143 USA.
   Univ Calif San Francisco, Dept Otolaryngol, Sloan Swartz Ctr Theoret Neurobiol, WM Keck Ctr Integrat Neurosci, San Francisco, CA 94143 USA.
RP Miller, KD (reprint author), Univ Calif San Francisco, Dept Physiol, Sloan Swartz Ctr Theoret Neurobiol, WM Keck Ctr Integrat Neurosci, 513 Parnassus, San Francisco, CA 94143 USA.
FU NEI NIH HHS [R01-EY11001]
CR Anderson JS, 2000, J NEUROPHYSIOL, V84, P909
   Anderson JS, 2000, SCIENCE, V290, P1968, DOI 10.1126/science.290.5498.1968
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   Borg-Graham LJ, 1998, NATURE, V393, P369, DOI 10.1038/30735
   BULLIER J, 1979, J NEUROPHYSIOL, V42, P1271
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   Carandini M, 2000, J NEUROSCI, V20, P470, DOI 10.1523/JNEUROSCI.20-01-00470.2000
   Carandini M., 1999, CEREB CORTEX, P401, DOI DOI 10.1007/978-1-4615-4903-1_7
   Chung S, 1998, NEURON, V20, P1177, DOI 10.1016/S0896-6273(00)80498-5
   CRAIR MC, 1995, NATURE, V375, P325, DOI 10.1038/375325a0
   Crook JM, 1996, J NEUROPHYSIOL, V75, P2071
   Crook JM, 1997, VISUAL NEUROSCI, V14, P141, DOI 10.1017/S095252380000883X
   Crook JM, 1998, EUR J NEUROSCI, V10, P2056, DOI 10.1046/j.1460-9568.1998.00218.x
   DeAngelis GC, 1999, J NEUROSCI, V19, P4046
   Dragoi V, 2000, NEURON, V28, P287, DOI 10.1016/S0896-6273(00)00103-3
   Ferster D, 2000, ANNU REV NEUROSCI, V23, P441, DOI 10.1146/annurev.neuro.23.1.441
   FERSTER D, 1986, J NEUROSCI, V6, P1284
   FERSTER D, 1988, J NEUROSCI, V8, P1172
   Ferster D, 1996, NATURE, V380, P249, DOI 10.1038/380249a0
   FERSTER D, 1992, J NEUROSCI, V12, P1262
   Gardner JL, 1999, VISUAL NEUROSCI, V16, P1115, DOI 10.1017/S0952523899166112
   Gibson JR, 1999, NATURE, V402, P75
   GILBERT CD, 1977, J PHYSIOL-LONDON, V268, P391, DOI 10.1113/jphysiol.1977.sp011863
   Gillespie DC, 2001, NAT NEUROSCI, V4, P1014, DOI 10.1038/nn731
   Hansel D, 2002, J NEUROSCI, V22, P5118, DOI 10.1523/JNEUROSCI.22-12-05118.2002
   Hirsch JA, 1998, J NEUROSCI, V18, P9517
   HIRSCH JA, 2000, SOC NEUR ABSTR, V26, P1083
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Humphrey AL, 1998, J NEUROPHYSIOL, V80, P3005
   Humphrey AL, 1998, J NEUROPHYSIOL, V80, P2991
   Jagadeesh B, 1997, J NEUROPHYSIOL, V78, P2772
   Kayser A, 2001, J NEUROPHYSIOL, V85, P2130
   Kayser AS, 2002, NEURON, V33, P131, DOI 10.1016/S0896-6273(01)00570-0
   Krukowski AE, 2001, NAT NEUROSCI, V4, P424, DOI 10.1038/86084
   KRUKOWSKI AE, 2000, THESIS U CALIFORNIA
   Lampl I, 2001, NEURON, V30, P263, DOI 10.1016/S0896-6273(01)00278-1
   Lauritzen TZ, 2001, J NEUROPHYSIOL, V86, P1803
   Maldonado PE, 1997, SCIENCE, V276, P1551, DOI 10.1126/science.276.5318.1551
   Martinez Luis M., 1998, Society for Neuroscience Abstracts, V24, P766
   MASTRONARDE DN, 1989, TRENDS NEUROSCI, V12, P75, DOI 10.1016/0166-2236(89)90140-9
   McLaughlin D, 2000, P NATL ACAD SCI USA, V97, P8087, DOI 10.1073/pnas.110135097
   Miller KD, 2002, J NEUROPHYSIOL, V87, P653, DOI 10.1152/jn.00425.2001
   Miller KD, 2001, CURR OPIN NEUROBIOL, V11, P488, DOI 10.1016/S0959-4388(00)00239-7
   Murthy A, 1998, VISUAL NEUROSCI, V15, P239, DOI 10.1017/S0952523898152045
   ORBAN GA, 1991, VISION VISUAL DYSFUN, V4, P173
   Porter JT, 2001, J NEUROSCI, V21, P2699, DOI 10.1523/JNEUROSCI.21-08-02699.2001
   REID RC, 1995, NATURE, V378, P281
   Ringach DL, 1997, NATURE, V387, P281, DOI 10.1038/387281a0
   RUTHAZER ES, 1996, SOC NEUR ABSTR, V22, P1996
   Saul AB, 1999, VISUAL NEUROSCI, V16, P667, DOI 10.1017/S095252389916406X
   SAUL AB, 1992, J NEUROPHYSIOL, V68, P1190
   SCLAR G, 1982, EXP BRAIN RES, V46, P457
   SKOTTUN BC, 1987, J NEUROPHYSIOL, V57, P773
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1
   TANAKA K, 1983, J NEUROPHYSIOL, V49, P1303
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   TROYER TW, 2002, IN PRESS J NEUROPHYS, V87
   Volgushev M, 2000, EUR J NEUROSCI, V12, P257, DOI 10.1046/j.1460-9568.2000.00909.x
   Volgushev M, 1996, J PHYSIOL-LONDON, V496, P597, DOI 10.1113/jphysiol.1996.sp021711
   VOLGUSHEV M, 1995, VISUAL NEUROSCI, V12, P621, DOI 10.1017/S0952523800008919
   Wielaard DJ, 2001, J NEUROSCI, V21, P5203, DOI 10.1523/JNEUROSCI.21-14-05203.2001
   Yousef T, 1999, EUR J NEUROSCI, V11, P4291, DOI 10.1046/j.1460-9568.1999.00863.x
NR 63
TC 44
Z9 44
U1 0
U2 3
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 73
EP 82
DI 10.1093/cercor/13.1.73
PG 10
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500011
PM 12466218
OA Bronze
DA 2019-06-15
ER

PT S
AU Snelson, E
   Rasmussen, CE
   Ghahramani, Z
AF Snelson, E
   Rasmussen, CE
   Ghahramani, Z
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Warped Gaussian processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.
C1 Univ Coll London, Gatsby Comp Neurosci Unit, London WC1N 3AR, England.
RP Snelson, E (reprint author), Univ Coll London, Gatsby Comp Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.
OI Rasmussen, Carl Edward/0000-0001-8899-7850
CR Blake C., 1998, UCI REPOSITORY MACHI
   CAMACHO R, 2000, THESIS U PORTO
   Cole D, 2000, SCI TECHNOL WELD JOI, V5, P81, DOI 10.1179/136217100101538065
   DIGGLE PJ, 1998, APPL STAT
   GIBBS M. N., 1997, THESIS CAMBRIDGE U
   GOLDBERG PW, 1998, ADV NEURAL INFORMATI, V10
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   Neal R., 1997, 9702 U TOR
   OHAGAN A, 2000, 49800 U SHEFF
   Rasmussen C., 1996, THESIS U TORONTO
   WILLIAMS CKJ, 1996, ADV NEURAL INFORMATI, V8
NR 11
TC 41
Z9 43
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 337
EP 344
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500043
DA 2019-06-15
ER

PT J
AU Panzeri, S
   Petroni, F
   Petersen, RS
   Diamond, ME
AF Panzeri, S
   Petroni, F
   Petersen, RS
   Diamond, ME
TI Decoding neuronal population activity in rat somatosensory cortex: Role
   of columnar organization
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, CO
ID BARREL CORTEX; STIMULUS LOCATION; INFORMATION; RESPONSES; MONKEY; SI
AB The present study asks in what way the activity of a neuronal population responding to a sensory stimulus could be most efficiently decoded, or 'read off', by the target neurons. A simple solution to this problem has been proposed - pooling the activity of responding neurons. However, pooling can be inefficient if sensory information is encoded by the 'label' of each neuron firing a spike. We have tested the efficiency of pooling by quantifying the extent to which information about a sensory stimulus is diminished when the identity of the individual neurons is lost by pooling. Analyzing the response of small groups of neurons in rat barrel cortex to single-whisker deflection, we found that pooling neurons within the same column is efficient for representing stimulus position; it causes a loss of only 1% of the information about whether the principal whisker was stimulated, and a loss of 5-12% of the finer information about which of nine possible whiskers (the principal and its neighbors) was stimulated. Cross-column pooling led to larger information losses, in the range of 25-55%. Thus, to decode stimulus position from the discharge of barrel cortex populations, 'downstream' neurons could pool the activity arising from neurons of the same column, while maintaining the activity arising from neurons of separate columns at least partially segregated. Since such parcellation is present in some of the projections from barrel cortex, these findings suggest that columnar organization of barrel cortex serves to facilitate decoding of the location of the stimulated whisker.
C1 Newcastle Univ, Sch Med, Dept Psychol, Newcastle Upon Tyne NE2 4HH, Tyne & Wear, England.
   Scuola Int Super Studi Avanzati, Cognit Neurosci Sector, I-34014 Trieste, Italy.
RP Panzeri, S (reprint author), Newcastle Univ, Sch Med, Dept Psychol, Henry Wellcome Bldg Neuroecol,Framlington Pl, Newcastle Upon Tyne NE2 4HH, Tyne & Wear, England.
EM stefano.panzeri@ncl.ac.uk
RI Petroni, Filippo/K-3366-2012; Panzeri, Stefano/L-5977-2013
OI Petroni, Filippo/0000-0001-8467-8481; Panzeri,
   Stefano/0000-0003-1700-8909; DIAMOND, Mathew Ernest/0000-0003-2286-4566
FU Telethon [GGP02459]
CR Alloway KD, 1999, J NEUROSCI, V19, P10908
   ARMSTRONGJAMES M, 1987, J COMP NEUROL, V263, P265, DOI 10.1002/cne.902630209
   Berry MJ, 1998, J NEUROSCI, V18, P2200
   COVER TM, 1991, ELEMENTS INFORMATION
   DARIANSMITH I, 1973, J NEUROPHYSIOL, V36, P325
   Harris JA, 1999, P NATL ACAD SCI USA, V96, P7587, DOI 10.1073/pnas.96.13.7587
   Lebedev MA, 2000, CEREB CORTEX, V10, P23, DOI 10.1093/cercor/10.1.23
   Leergaard TB, 2000, J NEUROSCI, V20, P8474, DOI 10.1523/JNEUROSCI.20-22-08474.2000
   Logothetis NK, 2001, NATURE, V412, P150, DOI 10.1038/35084005
   LU SM, 1993, SOMATOSENS MOT RES, V10, P1, DOI 10.3109/08990229309028819
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Nirenberg S, 2001, NATURE, V411, P698, DOI 10.1038/35079612
   Oram MW, 1998, TRENDS NEUROSCI, V21, P259, DOI 10.1016/S0166-2236(97)01216-2
   Panzeri S, 1999, P ROY SOC B-BIOL SCI, V266, P1001, DOI 10.1098/rspb.1999.0736
   Panzeri S, 1996, NETWORK-COMP NEURAL, V7, P87, DOI [10.1088/0954-898X/7/1/006, 10.1080/0954898X.1996.11978656]
   Panzeri S, 2001, NEURAL COMPUT, V13, P1311, DOI 10.1162/08997660152002870
   Panzeri S, 2001, NEURON, V29, P769, DOI 10.1016/S0896-6273(01)00251-3
   Petersen RS, 2001, NEURON, V32, P503, DOI 10.1016/S0896-6273(01)00481-0
   Petersen RS, 2000, J NEUROSCI, V20, P6135, DOI 10.1523/JNEUROSCI.20-16-06135.2000
   Rees G, 2000, NAT NEUROSCI, V3, P716, DOI 10.1038/76673
   Reich DS, 2001, SCIENCE, V294, P2566, DOI 10.1126/science.1065839
   RICE FL, 1995, BARREL CORTEX RODENT
   Segev I, 2000, SCIENCE, V290, P744, DOI 10.1126/science.290.5492.744
   Shadlen MN, 1996, J NEUROSCI, V16, P1486
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   SIMONS DJ, 1978, J NEUROPHYSIOL, V41, P798
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   Wandell BA, 1999, ANNU REV NEUROSCI, V22, P145, DOI 10.1146/annurev.neuro.22.1.145
   WOOLSEY TA, 1970, BRAIN RES, V17, P205, DOI 10.1016/0006-8993(70)90079-X
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 30
TC 41
Z9 41
U1 0
U2 3
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
EI 1460-2199
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 45
EP 52
DI 10.1093/cercor/13.1.45
PG 8
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500007
PM 12466214
OA Bronze
DA 2019-06-15
ER

PT J
AU Cuijpers, RH
   van Schie, HT
   Koppen, M
   Erlhagen, W
   Bekkering, H
AF Cuijpers, Raymond H.
   van Schie, Hein T.
   Koppen, Mathieu
   Erlhagen, Wolfram
   Bekkering, Harold
TI Goals and means in action observation: A computational approach
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE action observation; action planning; action goals; goal inference;
   decision making; perception and action; human agents
ID GRASPING MOVEMENTS; MOTOR CONTROL; IMITATION; COORDINATION;
   ORGANIZATION; MODULATION; OTHERS; ROBOTS; MODELS; TASK
AB Many of our daily activities are supported by behavioural goals that guide the selection of actions, which allow us to reach these goals effectively. Goals are considered to be important for action observation since they allow the observer to copy the goal of the action without the need to use the exact same means. The importance of being able to use different action rneans becomes evident when the observer and observed actor have different bodies (robots and humans) or bodily measurements (parents and children), or when the environments of actor and observer differ substantially (when an obstacle is present or absent in either environment). A selective focus on the action goals instead of the action means furthermore circumvents the need to consider the vantage point of the actor. which is consistent with recent findings that people prefer to represent the actions of others from their own individual perspective. In this paper, we use a computational approach to investigate how knowledge about action goals and means are used in action observation. We hypothesise that in action observation human agents are primarily interested in identifying the goals of the observed actor's behaviour. Behavioural cues (e.g. the way an object is grasped) may help to disambiguate the goal of the actor (e.g. whether a cup is grasped for drinking or handing it over). Recent advances in cognitive neuroscience are cited in support of the model's architecture. (c) 2006 Elsevier Ltd. All rights reserved.
C1 Radboud Univ Nijmegen, Nijmegen Inst Cognit & Informat, NL-6500 HE Nijmegen, Netherlands.
   Univ Minho, Dept Math Sci & Technol, P-4800058 Guimaraes, Portugal.
RP Cuijpers, RH (reprint author), Radboud Univ Nijmegen, Nijmegen Inst Cognit & Informat, POB 9104, NL-6500 HE Nijmegen, Netherlands.
EM r.cuijpers@nici.ru.nl
RI Bekkering, Harold/A-6357-2009; van Schie, Hein/D-2281-2010
OI Erlhagen, Wolfram/0000-0002-8150-3587
CR Bekkering H, 2000, Q J EXP PSYCHOL-A, V53, P153, DOI 10.1080/027249800390718
   Brass M, 2001, ACTA PSYCHOL, V106, P3, DOI 10.1016/S0001-6918(00)00024-X
   Breazeal C, 2005, ARTIF LIFE, V11, P31, DOI 10.1162/1064546053278955
   Burstedt MKO, 1997, EXP BRAIN RES, V117, P67, DOI 10.1007/s002210050200
   CALINON S, 2005, P 2005 IEEE INT C RO
   Cuijpers RH, 2004, J NEUROPHYSIOL, V91, P2598, DOI 10.1152/jn.00644.2003
   Desmurget M, 1998, NEUROSCI BIOBEHAV R, V22, P761, DOI 10.1016/S0149-7634(98)00004-9
   ERLHAGEN W, 2005, IN PRESS P INT C ART
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Flanagan JR, 2003, NATURE, V424, P769, DOI 10.1038/nature01861
   Fogassi L, 2005, SCIENCE, V308, P662, DOI 10.1126/science.1106138
   Frith CD, 1999, SCIENCE, V286, P1692, DOI 10.1126/science.286.5445.1692
   Gallese V, 1998, TRENDS COGN SCI, V2, P493, DOI 10.1016/S1364-6613(98)01262-5
   Gangitano M, 2004, EUR J NEUROSCI, V20, P2193, DOI 10.1111/j.1460-9568.2004.03655.x
   Goldman A. I., 1992, MIND LANG, V7, P104, DOI DOI 10.1111/J.1468-0017.1992.TB00200.X
   Grea H, 2000, EXP BRAIN RES, V134, P155, DOI 10.1007/s002210000427
   Grezes J, 2004, NEUROIMAGE, V21, P744, DOI 10.1016/j.neuroimage.2003.10.014
   Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528
   Iacoboni M, 2005, PLOS BIOL, V3, P529, DOI 10.1371/journal.pbio.0030079
   Knoblich G, 2003, J EXP PSYCHOL LEARN, V29, P1006, DOI 10.1037/0278-7393.29.5.1006
   Mehta B, 2002, J NEUROPHYSIOL, V88, P942, DOI 10.1152/jn.00804.2001
   Oztop E, 2005, COGNITIVE BRAIN RES, V22, P129, DOI 10.1016/j.cogbrainres.2004.08.004
   Paine RW, 2004, NEURAL NETWORKS, V17, P1291, DOI 10.1016/j.neunet.2004.08.005
   Rizzolatti G, 2001, NAT REV NEUROSCI, V2, P661, DOI 10.1038/35090060
   Rosenbaum DA, 2001, PSYCHOL REV, V108, P709, DOI 10.1037//0033-295X.108.4.709
   Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3
   Schaal S., 2003, 2 INT S AD MOT AN MA
   SCHAAL S, 2004, SPRINGER TRACTS ADV
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Thoroughman KA, 2000, NATURE, V407, P742, DOI 10.1038/35037588
   Triesch J, 2003, J VISION, V3, P86, DOI 10.1167/3.1.9
   Trommershauser J, 2005, J NEUROSCI, V25, P7169, DOI 10.1523/JNEUROSCI.1906-05.2005
   van Schie HT, 2004, NAT NEUROSCI, V7, P549, DOI 10.1038/nn1239
   Wilson M, 2005, PSYCHOL BULL, V131, P460, DOI 10.1037/0033-2909.131.3.460
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
   Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238
   YEUTIELI Y, 2005, J NEUROPHYSIOL, V94, P1443
   YEUTIELI Y, 2005, J NEUROPHYSIOL, V94, P1459
NR 38
TC 39
Z9 39
U1 0
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 311
EP 322
DI 10.1016/j.neunet.2006.02.004
PG 12
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600006
PM 16618535
OA Green Published
DA 2019-06-15
ER

PT J
AU Scott, C
   Nowak, RD
AF Scott, C
   Nowak, RD
TI Minimax-optimal classification with dyadic decision trees
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT 18th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2004
CL Vancouver, CANADA
DE complexity regularization; decision trees; feature rejection;
   generalization error bounds; manifold learning; minimax optimality;
   pruning; rates of convergence; recursive dyadic partitions; statistical
   learning theory
ID NONPARAMETRIC FUNCTION ESTIMATION; WAVELET SHRINKAGE; DISCRIMINATION;
   CONVERGENCE; ADAPTATION; RATES
AB Decision trees are among the most popular types of classifiers, with interpretability and ease of implementation being among their chief attributes. Despite the widespread use of decision trees, theoretical analysis of their performance has only begun to emerge in recent years. In this paper, it is shown that a new family of decision trees, dyadic decision trees (DDTs), attain nearly optimal (in a minimax sense) rates of convergence for a broad range of classification problems. Furthermore, DDTs are surprisingly adaptive in three important respects: they automatically 1) adapt to favorable conditions near the Bayes decision boundary; 2) focus on data distributed on lower dimensional manifolds; and 3) reject irrelevant features. DDTs are constructed by penalized empirical risk minimization using a new data-dependent penalty and may be computed exactly with computational complexity that is nearly linear in the training sample size. DDTs comprise the first classifiers known to achieve nearly optimal rates for the diverse class of distributions studied here while also being practical and implementable. This is also the first study (of which we are aware) to consider rates for adaptation to intrinsic data dimension and relevant features.
C1 Rice Univ, Dept Stat, Houston, TX 77005 USA.
   Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA.
RP Scott, C (reprint author), Rice Univ, Dept Stat, Houston, TX 77005 USA.
EM cscott@rice.edu; nowak@engr.wisc.edu
CR AUDIBERT JY, 2004, THESIS U PARIS 6 PAR
   BARRON AR, 1991, NATO ADV SCI I C-MAT, V335, P561
   Bartlett Peter L., 2003, J AM STAT ASS
   Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812
   Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808
   BERKMAN N, 1995, 9520 TR
   BLANCARD G, 1995, J AM STAT ASSOC, V90, P1200
   Blanchard G, 2004, J MACH LEARN RES, V4, P861, DOI 10.1162/1532443041424319
   Blanchard G, 2004, LECT NOTES COMPUT SC, V3120, P378, DOI 10.1007/978-3-540-27819-1_26
   BLANCHARD G, 2005, OPTIMAL DYADIC DECIS
   BOUCHERON S, 2004, THEORY CLASSIFICATIO
   Breiman L., 1984, CLASSIFICATION REGRE
   Candes EJ, 2001, J APPROX THEORY, V113, P59, DOI 10.1006/jath.2001.3624
   CHERNOFF H, 1952, ANN MATH STAT, V23, P493, DOI 10.1214/aoms/1177729330
   Cohen A, 2001, APPL COMPUT HARMON A, V11, P192, DOI 10.1006/acha.2001.0336
   Cover T. M., 1991, ELEMENTS INFORM THEO
   DeVore R. A., 1998, Acta Numerica, V7, P51, DOI 10.1017/S0962492900002816
   Devroye L., 1996, PROBABILISTIC THEORY
   Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261
   Donoho DL, 1997, ANN STAT, V25, P1870, DOI 10.1214/aos/1069362377
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301
   Dudley R. M., 1974, Journal of Approximation Theory, V10, P227, DOI 10.1016/0021-9045(74)90120-8
   Esposito F, 1997, IEEE T PATTERN ANAL, V19, P476, DOI 10.1109/34.589207
   Falconer K, 1990, FRACTAL GEOMETRY MAT
   GEY S, 2002, MSRI P NONLINEA ESTI
   GOLEA M, 1998, ADV NEURAL INFORM PR, V10
   GORDON L, 1978, ANN STAT, V6, P515, DOI 10.1214/aos/1176344197
   HAGERUP T, 1990, INFORM PROCESS LETT, V33, P305, DOI 10.1016/0020-0190(90)90214-I
   HUBER C, 1997, FESTSCHRIFT LUCIEN C, P245
   Johnstone IM, 1999, PHILOS T R SOC A, V357, P2475, DOI 10.1098/rsta.1999.0443
   Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543
   Kearns M, 1998, P 15 INT C MACH LEAR, P269
   Kolaczyk ED, 2005, BIOMETRIKA, V92, P119, DOI 10.1093/biomet/92.1.119
   Kolaczyk ED, 2004, ANN STAT, V32, P500
   Korostelev A., 1993, MINIMAX THEORY IMAGE
   Lugosi G, 1996, IEEE T INFORM THEORY, V42, P48, DOI 10.1109/18.481777
   Mallat S., 1998, WAVELET TOUR SIGNAL
   Mammen E, 1999, ANN STAT, V27, P1808
   Mansour Y, 1997, P 14 INT C MACH LEAR, P195
   Mansour Y., 2000, P 13 ANN C COMP LEAR, P69
   MARRON JS, 1983, ANN STAT, V11, P1142, DOI 10.1214/aos/1176346328
   Murthy SK, 1998, DATA MIN KNOWL DISC, V2, P345, DOI 10.1023/A:1009744630224
   Nobel AB, 2002, IEEE T INFORM THEORY, V48, P2362, DOI 10.1109/TIT.2002.800482
   OKAMOTO M, 1958, ANN I STAT MATH, V10, P29
   Quinlan R., 1993, C4 5 PROGRAMS MACHIN
   SCOTT C, 2003, ADV NEURAL INFORM PR, V15
   SCOTT C, 2004, ADV NEURAL INFORM PR, V16
   SCOVEL JC, 2003, LAUR039117
   Tsybakov AB, 2005, ANN STAT, V33, P1203, DOI 10.1214/009053604000001066
   Tsybakov AB, 2004, ANN STAT, V32, P135
   VANERVAART AW, 1996, WEAK CONVERGENCE EMP
   Vapnik V. N., 1982, ESTIMATION DEPENDENC
   WU Q, UNPUB J COMPLEXITY
   Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   ZHOU DX, 2004, UNPUB ADV COMPUT MAT
NR 57
TC 39
Z9 40
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9448
EI 1557-9654
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD APR
PY 2006
VL 52
IS 4
BP 1335
EP 1353
DI 10.1109/TIT.2006.871056
PG 19
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 031ON
UT WOS:000236714000004
DA 2019-06-15
ER

PT J
AU Johnson, R
   Zhang, T
AF Johnson, Ric
   Zhang, Tong
TI Graph-based semi-supervised learning and spectral kernel design
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE graph-based semi-supervised learning; kernel design; transductive
   learning
ID BOUNDS
AB In this paper, we consider a framework for semi-supervised learning using spectral decomposition-based unsupervised kernel design. We relate this approach to previously proposed semi-supervised learning methods on graphs. We examine various theoretical properties of such methods. In particular, we present learning bounds and derive optimal kernel representation by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can improve the predictive performance. Empirical examples are included to illustrate the main consequences of our analysis.
C1 [Johnson, Ric] RJ Res Consulting, Tarrytown, NY 10591 USA.
   [Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA.
RP Johnson, R (reprint author), RJ Res Consulting, Tarrytown, NY 10591 USA.
EM riejohnson@gmail.com; tzhang@stat.rutgers.edu
CR ALTUN Y, 2006, ADV NEURL INFORM PRO, P33
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   BELKIN M, 2004, TR200406 U CHICAGO
   BELKIN M, 2005, P 18 ANN C LEARN THE, P486
   Chapelle O., 2003, ADV NEURAL INFORM PR, V15, P585
   GINE E, 2005, P 4 INT C HIGH DIMEN, P238
   HEIN M, 2006, P 19 ANN C LEARN THE, P50
   HEIN M, 2005, P 18 C LEARN THEOR C, P470
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   NY AY, 2001, ADV NEURAL INFORM PR, P849
   Rockafellar R T, 1970, CONVEX ANAL
   Szummer M., 2002, ADV NEURAL INFORM PR, P945
   Wahba G., 1990, SPLINE MODELS OBSERV
   Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008
   Zhang T, 2003, NEURAL COMPUT, V15, P1397, DOI 10.1162/089976603321780326
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, P 20 INT C MACH LEAR, P912, DOI DOI 10.1109/18.850663
   ZHU X, 2005, ADV NEURAL INFORM PR, P1641
   Zhu X., 2003, CMUCS03175
NR 20
TC 37
Z9 42
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 0018-9448
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD JAN
PY 2008
VL 54
IS 1
BP 275
EP 288
DI 10.1109/TIT.2007.911294
PG 14
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 249VA
UT WOS:000252256900018
DA 2019-06-15
ER

PT J
AU Hoffman, MW
   Grimes, DB
   Shon, AP
   Rao, RPN
AF Hoffman, Matthew W.
   Grimes, David B.
   Shon, Aaron P.
   Rao, Rajesh P. N.
TI A probabilistic model of gaze imitation and shared attention
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE imitation learning; shared attention; gaze tracking; human-robot
   interaction
ID VISUAL-ATTENTION; MIRROR NEURONS; CORTEX; CONSEQUENCES
AB An important component of language acquisition and cognitive learning is gaze imitation. Infants as young as one year of age can follow the gaze of an adult to determine the object the adult is focusing on. The ability to follow gaze is a precursor to shared attention, wherein two or more agents simultaneously focus their attention on a single object in the environment. Shared attention is a necessary skill for many complex, natural forms of learning, including learning based on imitation. This paper presents a probabilistic model of gaze imitation and shared attention that is inspired by Meltzoff and Moore's AIM model for imitation in infants. Our model combines a probabilistic algorithm for estimating gaze vectors with bottom-up saliency maps of visual scenes to produce maximum a posteriori (MAP) estimates of objects being looked at by an observed instructor. We test our model using a robotic system involving a pan-tilt camera head and show that combining saliency maps with gaze estimates leads to greater accuracy than using gaze alone. We additionally show that the system can learn instructor-specific probability distributions over objects, leading to increasing gaze accuracy over successive interactions with the instructor. Our results provide further support for probabilistic models of imitation and suggest new ways of implementing robotic systems that can interact with humans over an extended period of time. (c) 2006 Published by Elsevier Ltd.
C1 Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
RP Hoffman, MW (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
EM mhoffman@cs.washington.edu; grimes@cs.washington.edu;
   aaron@cs.washington.edu; rao@cs.washington.edu
CR ALISSANDRAKIS A, 2003, P 2 INT S IM AN ART, P79
   ALISSANDRAKIS A, 2000, P LEARN DO THINGS AA, P1
   Billard A., 2000, Proceedings of the Fourth International Conference on Autonomous Agents, P373, DOI 10.1145/336595.337544
   BILLARD A, 2004, ROBOTICS AUTONOMOUS, V47, P2
   Blakemore SJ, 1998, J NEUROSCI, V18, P7511
   Blakemore SJ, 2001, NEUROREPORT, V12, P1879, DOI 10.1097/00001756-200107030-00023
   Booth MCA, 1998, CEREB CORTEX, V8, P510, DOI 10.1093/cercor/8.6.510
   Breazeal C, 2005, ARTIF LIFE, V11, P31, DOI 10.1162/1064546053278955
   BREAZEAL C, 1998, P 1998 SIM AD BEH WO, P25
   BREAZEAL C, 2001, IMITATION ANIMALS AR
   BREAZEAL C, 1999, P AISB 99 S IM AN AR, P96
   Brooks R, 2002, DEV PSYCHOL, V38, P958, DOI 10.1037//0012-1649.38.6.958
   Buccino G, 2001, EUR J NEUROSCI, V13, P400, DOI 10.1046/j.1460-9568.2001.01385.x
   Byrne RW, 1998, BEHAV BRAIN SCI, V21, P667, DOI 10.1017/S0140525X98001745
   CALINON S, 2005, P IEEE INT C ROB AUT
   CALINON S, 2006, P IEEE INT C ROB AUT
   CALINON S, 2005, P INT C MACH LEARN I
   Carp F. M., 1988, TRANSPORTATION AGING, V2, P1
   DEMIRIS J, 1997, P 6 IEEE INT WORKSH
   DEMIRIS Y, 2005, P ICRA WORKSH ROB PR
   Dempster A. P., 1977, J ROYAL STAT SOC B, V39
   DIPELLEGRINO G, 1992, EXP BRAIN RES, V91, P176, DOI 10.1007/BF00230027
   FASEL I, 2002, P ICDL, V2
   Fong T.W., 2002, ROBOTICS AUTONOMOUS, V42, P142
   Haruno M, 2001, NEURAL COMPUT, V13, P2201, DOI 10.1162/089976601750541778
   Iacoboni Marco, 2005, PERSPECTIVES IMITATI, V1
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   JANSEN B, 2005, P WORKSH SOC MECH RO
   Johnson M., 2005, P AISB 2005 S IM AN, P69
   Johnson-Frey SH, 2003, NEURON, V39, P1053, DOI 10.1016/S0896-6273(03)00524-5
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Krebs J.R., 1991, P105
   LEMPERS JD, 1979, J GENET PSYCHOL, V135, P93, DOI 10.1080/00221325.1979.10533420
   LUNGARELLA M, 2003, EPIROB 03, P81
   MELTZOFF AN, 1977, SCIENCE, V198, P75, DOI 10.1126/science.198.4312.75
   Meltzoff AN, 1997, EARLY DEV PARENTING, V6, P179, DOI 10.1002/(SICI)1099-0917(199709/12)6:3/4<179::AID-EDP157>3.0.CO;2-R
   MELTZOFF AN, 2005, PERSPECTIVES IMITATI, P55
   Miall RC, 2003, NEUROREPORT, V14, P2135, DOI 10.1097/01.wnr.0000098751.87269.77
   Moore C, 1997, INFANT BEHAV DEV, V20, P83, DOI 10.1016/S0163-6383(97)90063-1
   NAGAI Y, 2003, P 2 INT S AD MOT AN
   Nehaniv C.L., 2000, INTERDISCIPLINARY AP
   PRICE B, 2003, THESIS U BRIT COLUMB
   RAO R, 2004, IMITATION SOCIAL LEA
   RAO RPN, 2003, P ART INT SIM BEH
   Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230
   Rizzolatti G, 2000, INT J PSYCHOL, V35, P205
   SCASSELLATI B, 1999, LECT NOTES COMPUTER, V1562, P176
   Viola Paul, 2001, INT J COMPUTER VISIO
   VISALBERGHY E, 1990, DO MONKEYS APE LANGU, P247
   WU Y, 2000, AFGR00, P183
   YAMANE S, 1988, EXP BRAIN RES, V73, P209, DOI 10.1007/BF00279674
NR 51
TC 36
Z9 36
U1 0
U2 3
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 299
EP 310
DI 10.1016/j.neunet.2006.02.008
PG 12
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600005
PM 16716907
DA 2019-06-15
ER

PT S
AU Goodfellow, IJ
   Pouget-Abadie, J
   Mirza, M
   Xu, B
   Warde-Farley, D
   Ozair, S
   Courville, A
   Bengio, Y
AF Goodfellow, Ian J.
   Pouget-Abadie, Jean
   Mirza, Mehdi
   Xu, Bing
   Warde-Farley, David
   Ozair, Sherjil
   Courville, Aaron
   Bengio, Yoshua
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Generative Adversarial Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.
C1 [Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.
RP Goodfellow, IJ (reprint author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.
FU CIFAR; Canada Research Chairs; Google Fellowship in Deep Learning
FX We would like to acknowledge Patrice Marcotte, Olivier Delalleau,
   Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful
   discussions. Yann Dauphin shared his Parzen window evaluation code with
   us. We would like to thank the developers of Pylearn2 [11] and Theano
   [6, 1], particularly Frederic Bastien who rushed a Theano feature
   specifically to benefit this project. Arnaud Bergeron provided
   much-needed support with LATEX typesetting. We would also like to thank
   CIFAR, and Canada Research Chairs for funding, and Compute Canada, and
   Calcul Quebec for providing computational resources. Ian Goodfellow is
   supported by the 2013 Google Fellowship in Deep Learning. Finally, we
   would like to thank Les Trois Brasseurs for stimulating our creativity.
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y., 2014, ICML 14
   Bengio Y., 2009, LEARNING DEEP ARCHIT
   Bengio Y., 2014, P 30 INT C MACH LEAR
   Bengio Y., 2013, ICML 13
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Breuleux O, 2011, NEURAL COMPUT, V23, P2058, DOI 10.1162/NECO_a_00158
   Glorot X., 2011, AISTATS 2011
   Goodfellow I. J., 2013, NIPS 2013
   Goodfellow I. J., 2013, ICML 2013
   Goodfellow I. J., 2013, ARXIV13084214
   Gregor K., 2014, ICML 2014
   Gutmann Michael, 2010, P 13 INT C ART INT S
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   HINTON GE, 2012, TECHNICAL REPORT
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kingma D. P., 2014, P INT C LEARN REPR I
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS 2012
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mnih Andriy, 2014, ARXIV14020030
   Rezende D. J, 2014, ARXIV14014082
   Rifai S., 2012, ICML 12
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Susskind J., 2010, 2010001 UTML TR
   Szegedy C., 2014, ABS13126199 ICLR
   Tu Z, 2007, PROC CVPR IEEE, P500
NR 29
TC 34
Z9 34
U1 52
U2 52
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101094
DA 2019-06-15
ER

PT J
AU Ormoneit, D
   Black, MJ
   Hastie, T
   Kjellstrom, H
AF Ormoneit, D
   Black, MJ
   Hastie, T
   Kjellstrom, H
TI Representing cyclic human motion using functional analysis
SO IMAGE AND VISION COMPUTING
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, CO
DE human motion; functional data analysis; missing data; singular value
   decomposition; principal component analysis; motion capture; tracking
ID TEMPORAL TEMPLATES; IMAGE SEQUENCES; GAIT ANALYSIS; RECOGNITION;
   TRACKING; CAPTURE; CAMERA; MODELS; VIDEO
AB We present a robust automatic method for modeling cyclic 3D human motion such as walking using motion-capture data. The pose of the body is represented by a time-series of joint angles which are automatically segmented into a sequence of motion cycles. The mean and the principal components of these cycles are computed using a new algorithm that enforces smooth transitions between the cycles by operating in the Fourier domain. Key to this method is its ability to automatically deal with noise and missing data. A learned walking model is then exploited for Bayesian tracking of 3D human motion. (C) 2005 Elsevier B.V. All rights reserved.
C1 Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   Marshall Wace LLP, London WC2N 6HT, England.
   Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   Swedish Def Res Agcy, Dept IR Syst, SE-16490 Stockholm, Sweden.
RP Black, MJ (reprint author), Brown Univ, Dept Comp Sci, 115 Waterman St,Box 1910, Providence, RI 02912 USA.
EM d.ormoneit@mwam.com; black@cs.brown.edu; hastie@stat.stanford.edu;
   hedvig@nada.kth.se
CR ALLMEN M, 1990, INT C PATT REC, P365
   Amaya K, 1996, PROC GRAPH INTERF, P222
   ARIKAN O, 2002, SIGGRAPH, P438
   BESSE P, 1986, PSYCHOMETRIKA, V51, P285, DOI 10.1007/BF02293986
   BLACK MJ, 1998, LNCS SERIES, V140, P909
   BOBICK A, 1996, INT C PATTERN REC, V1, P307
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bobick AF, 2001, P IEEE COMP VIS PATT, V1, P423
   Bowden R., 2000, IEEE WORKSH HUM MOD
   Brand M, 2000, COMP GRAPH, P183
   Brand M., 1999, INT C COMP VIS, V2, P1237
   Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581
   Bregler C, 1997, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.1997.609382
   CAMPBELL LW, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P624, DOI 10.1109/ICCV.1995.466880
   CHAM T, 1999, P CVPR, V1, P239
   Collins R. T., 2002, P IEEE INT C AUT FAC, P351
   Cunado D, 2003, COMPUT VIS IMAGE UND, V90, P1, DOI 10.1010/SI077-3142(03)00008-0
   Cutler R., 2002, P IEEE C FAC GEST RE, P267
   De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986
   Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c
   Faraway JJ, 1997, TECHNOMETRICS, V39, P254, DOI 10.2307/1271130
   FLOGG DC, 1983, IMAGE VISION COMPUT, V1, P5
   Gleicher M, 1999, COMPUT GRAPHICS-US, V33, P51
   GONCALVES L, 1998, IEEE INT C AUT FAC G, P234
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   HALL P, 1999, NONPARAMETRIC ESTIMA
   HASTIE T, 2000, IMPUTING MISSING DAT
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Huang PS, 1999, IEE P-VIS IMAGE SIGN, V146, P93, DOI 10.1049/ip-vis:19990187
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Isard M., 1996, EUR C COMP VIS, P343
   JU SX, 1996, INT C AUT FAC GEST R, P38
   Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865
   KOVAR L, 2002, SIGGRAPH, P491
   Lee Jehee, 2002, INTERACTIVE CONTROL, P491
   Little J. J., 1998, VIDERE, V1, P1
   MOLINA L, 2000, IEEE WORKSH HUM MOT, P137
   Murase H, 1996, PATTERN RECOGN LETT, V17, P155, DOI 10.1016/0167-8655(95)00109-3
   Murray M P, 1967, Am J Phys Med, V46, P290
   Nixon MS, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P139, DOI 10.1109/AFGR.2004.1301521
   OLSHEN RA, 1989, ANN STAT, V17, P1419, DOI 10.1214/aos/1176347372
   Ormoneit D, 2001, ADV NEUR IN, V13, P894
   PAVOLVIC V, 1999, ICCV, V1, P94
   PULLEN K, 2000, IEEE COMPUTER ANIMAT, P36
   PULLEN K, 2002, SIGGRAPH 02, P501
   Ramsey J. O., 1997, FUNCTIONAL DATA ANAL
   RICE JA, 1991, J ROY STAT SOC B MET, V53, P233
   ROHR K, 1994, CVGIP-IMAG UNDERSTAN, V59, P94, DOI 10.1006/cviu.1994.1006
   ROHR K, 1997, MOTION BASED RECOGNI, P171
   SEITZ SM, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P970, DOI 10.1109/CVPR.1994.323936
   SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651
   SHUM HY, 2002, P ACM SIGGRAPH, P465
   SIDENBLADH H, 2000, INT C AUT FAC GEST R, P368
   SIDENBLADH H, 2000, LNCS SERIES, V1813, P702
   SIDENBLADH H, 2001, THESIS KTH STOCKHOLM
   TANAWONGSUWAN R, 2001, P IEEE C COMP VIS PA, V2, P726
   Troje NF, 2002, J VISION, V2, P371, DOI 10.1167/2.5.2
   Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520
   UNUMA M, 1995, SIGGRAPH 95, P91
   Urtasun R., 2004, EUR C COMP VIS, P92
   URTASUN R, 2005, P IEEE COMP VIS PATT, V2, P932
   Wachter S, 1999, COMPUT VIS IMAGE UND, V74, P174, DOI 10.1006/cviu.1999.0758
   Winter D. A., 1991, BIOMECHANICS MOTOR C
   WREN CR, 1999, TR451 MIT
   Yacoob Y, 1999, COMPUT VIS IMAGE UND, V73, P232, DOI 10.1006/cviu.1998.0726
   Yacoob Y, 2000, INT J COMPUT VISION, V36, P5, DOI 10.1023/A:1008173322902
NR 66
TC 32
Z9 33
U1 0
U2 6
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD DEC 12
PY 2005
VL 23
IS 14
BP 1264
EP 1276
DI 10.1016/j.imavis.2005.09.004
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA 997KJ
UT WOS:000234243800003
DA 2019-06-15
ER

PT S
AU Ren, SQ
   He, KM
   Girshick, R
   Sun, J
AF Ren, Shaoqing
   He, Kaiming
   Girshick, Ross
   Sun, Jian
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_ rcnn.
C1 [Ren, Shaoqing; He, Kaiming; Girshick, Ross; Sun, Jian] Microsoft Res, Redmond, WA 98052 USA.
RP Ren, SQ (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM v-shren@microsoft.com; kahe@microsoft.com; rbg@microsoft.com;
   jiansun@microsoft.com
CR Chavali N., 2015, ARXIV150505836
   Dai J., 2015, CVPR
   Erhan  D., 2014, CVPR
   Everingham  M., 2007, PASCAL VISUAL OBJECT
   Girshick R., 2015, ARXIV150408083
   Girshick R., 2014, CVPR
   He K., 2014, ECCV
   Hosang J., 2014, BMVC
   Hosang J., 2015, ARXIV150205082
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2012, NIPS
   LeCun Y., 1989, NEURAL COMPUTATION
   Lenc K., 2015, ARXIV150606981
   Long  J., 2015, CVPR
   Nair V., 2010, ICML
   Ren S., 2015, ARXIV150406066
   Russakovsky O, 2014, ARXIV14090575
   Sermanet  P., 2014, ICLR
   Simonyan Karen, 2015, ICLR
   Szegedy C., 2015, ARXIV14121441V2
   Szegedy C., 2013, NIPS
   Uijlings J., 2013, IJCV
   Zeiler  M.D., 2014, ECCV
   Zitnick C. L., 2014, ECCV
NR 24
TC 29
Z9 29
U1 21
U2 22
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100006
DA 2019-06-15
ER

PT J
AU Mendelson, S
AF Mendelson, S
TI On the performance of kernel classes
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
AB We present sharp bounds on the localized Rademacher averages of the unit ball in a reproducing kernel Hilbert space in terms of the eigenvalues of the integral operator associated with the kernel. We use this result to estimate the performance of the empirical minimization algorithm when the base class is the unit ball of the reproducing kernel Hilbert space.
C1 Australian Natl Univ, RSISE, Canberra, ACT 0200, Australia.
RP Mendelson, S (reprint author), Australian Natl Univ, RSISE, Canberra, ACT 0200, Australia.
EM SHAHAR.MENDELSON@ANU.EDU.AU
OI Mendelson, Shahar/0000-0002-5673-7576
CR BARTLETT PL, 2003, EMPIRICAL RISK MINIM
   BARTLETT PL, 2003, LOCALIZED RADEMACHER
   BOUSQUET O, 2002, THESIS ECOLE POLYTEC
   Cucker F, 2002, B AM MATH SOC, V39, P1
   CUCKER P, 2003, FDN COMPUTATIONAL MA, V2, P413
   LUGOSI G, 2003, IN PRESS ANN STAT
   Massart P, 2000, ANN PROBAB, V28, P863, DOI 10.1214/aop/1019160263
   Mendelson S, 2002, IEEE T INFORM THEORY, V48, P1977, DOI 10.1109/TIT.2002.1013137
   MENDELSON S, 2003, IN PRESS ANN PROBABI
   Milman VD, 1986, LECT NOTES MATH, V1200
   Pisier  G., 1989, VOLUME CONVEX BODIES
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   ZHANG T, 2003, ADV NEURAL INFORMATI, V15, P454
NR 13
TC 28
Z9 28
U1 0
U2 3
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 759
EP 771
DI 10.1162/1532443041424337
PG 13
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800002
DA 2019-06-15
ER

PT S
AU Dornhege, G
   Blankertz, B
   Curi, G
   Muller, KR
AF Dornhege, G
   Blankertz, B
   Curi, G
   Muller, KR
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Increase information transfer rates in BCI by CSP extension to
   multi-class
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID SINGLE-TRIAL EEG; DIAGONALIZATION; CLASSIFICATION; COMMUNICATION;
   MOVEMENT
AB Brain-Computer Interfaces (BCI) are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intentions into a control signal for devices like computers or neuroprostheses. If this can be done bypassing the usual human output pathways like peripheral nerves and muscles it can ultimately become a valuable tool for paralyzed patients. Most activity in BCI research is devoted to finding suitable features and algorithms to increase information transfer rates (ITRs). The present paper studies the implications of using more classes, e.g., left vs. right hand vs. foot, for operating a BCI. We contribute by (1) a theoretical study showing under some mild assumptions that it is practically not useful to employ more than three or four classes, (2) two extensions of the common spatial pattern (CSP) algorithm, one interestingly based on simultaneous diagonalization, and (3) controlled EEG experiments that underline our theoretical findings and show excellent improved ITRs.
C1 Fraunhofer FIRSTIDA, D-12489 Berlin, Germany.
RP Dornhege, G (reprint author), Fraunhofer FIRSTIDA, Kekulestr 7, D-12489 Berlin, Germany.
OI Blankertz, Benjamin/0000-0002-2437-4846
CR Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133
   Birbaumer N, 1999, NATURE, V398, P297, DOI 10.1038/18581
   Blanco A, 2002, TAPPI J, V1, P14
   Blankertz B, 2003, IEEE T NEUR SYS REH, V11, P127, DOI 10.1109/TNSRE.2003.814456
   Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546
   DORNHEGE B, 2003, P 1 INT IEEE EMBS C, P591
   DORNHEGE G, 2003, ADV NEURAL INF P SYS, V15
   Muller-Gerking J, 1999, CLIN NEUROPHYSIOL, V110, P787, DOI 10.1016/S1388-2457(98)00038-8
   OBERMAIER C, 2001, IEEE T NEUR SYS REH, V9, P283
   PARRA L, 2002, IN PRESS NEUROIMAGE
   Penny WD, 2000, IEEE T REHABIL ENG, V8, P214, DOI 10.1109/86.847820
   Peters BO, 2001, IEEE T BIO-MED ENG, V48, P111, DOI 10.1109/10.900270
   Pham DT, 2001, SIAM J MATRIX ANAL A, V22, P1136, DOI 10.1137/S089547980035689X
   Ramoser H, 2000, IEEE T REHABIL ENG, V8, P441, DOI 10.1109/86.895946
   TREJO L, 2003, IN PRESS IEEE T NEUR
   Wolpaw JR, 2000, IEEE T REHABIL ENG, V8, P164, DOI 10.1109/TRE.2000.847807
   Wolpaw JR, 2000, IEEE T REHABIL ENG, V8, P222, DOI 10.1109/86.847823
   Wolpaw JR, 2002, CLIN NEUROPHYSIOL, V113, P767, DOI 10.1016/S1388-2457(02)00057-3
   Ziehe A, 2003, P INT C IND COMP AN, P469
NR 19
TC 28
Z9 29
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 733
EP 740
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500092
DA 2019-06-15
ER

PT S
AU Merolla, P
   Boahen, K
AF Merolla, P
   Boahen, K
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A recurrent model of orientation maps with simple and complex cells
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID VISUAL-CORTEX; SELECTIVITY
AB We describe a neuromorphic chip that utilizes transistor heterogeneity, introduced by the fabrication process, to generate orientation maps similar to those imaged in vivo. Our model consists of a recurrent network of excitatory and inhibitory cells in parallel with a push-pull stage. Similar to a previous model the recurrent network displays hotspots of activity that give rise to visual feature maps. Unlike previous work, however, the map for orientation does not depend on the sign of contrast. Instead, sign-independent cells driven by both ON and OFF channels anchor the map, while push-pull interactions give rise to sign-preserving cells. These two groups of orientation-selective cells are similar to complex and simple cells observed in VI.
C1 Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA.
RP Merolla, P (reprint author), Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA.
CR BLASDEL GG, 1992, J NEUROSCI, V12, P3139
   BOAHEN K, 1992, NIPS91
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Crair MC, 1998, SCIENCE, V279, P566, DOI 10.1126/science.279.5350.566
   Culurciello E, 2003, IEEE J SOLID-ST CIRC, V38, P281, DOI 10.1109/JSSC.2002.807412
   Ernst UA, 2001, NAT NEUROSCI, V4, P431, DOI 10.1038/86089
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Ringach DL, 2002, J NEUROSCI, V22, P5639
   ZAGHLOUL K, 2002, NEUROSCIENCE
NR 9
TC 26
Z9 26
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 995
EP 1002
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500124
DA 2019-06-15
ER

PT S
AU Bottou, U
   Le Cun, Y
AF Bottou, U
   Le Cun, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Large scale online learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.
C1 NEC Corp Ltd, Labs Amer, Princeton, NJ 08540 USA.
RP Bottou, U (reprint author), NEC Corp Ltd, Labs Amer, Princeton, NJ 08540 USA.
CR Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bottou L., 2002, HDB BRAIN THEORY NEU
   Bottou L, 1998, LECT NOTES COMPUTER, V1524
   Bottou L., 1998, ONLINE LEARNING NEUR, P9
   BOTTOU L, 2003, IN PRESS APPL STOCHA
   Chambers J. M., 1992, STAT MODELS S
   DENNIS JR, 1983, NUMERICAL METHODS UN
   Murata N, 1999, SIGNAL PROCESS, V74, P3, DOI 10.1016/S0165-1684(98)00206-0
   Tsypkin  Y., 1973, FDN THEORY LEARNING
   Vapnik V. N., 1974, THEORY PATTERN RECOG
NR 11
TC 25
Z9 25
U1 0
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 217
EP 224
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500028
DA 2019-06-15
ER

PT S
AU Moreno, PJ
   Ho, PP
   Vasconcelos, N
AF Moreno, PJ
   Ho, PP
   Vasconcelos, N
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A Kullback-Leibler divergence based kernel for SVM classification in
   multimedia applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Over the last years significant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classifiers such as SVM's. In this paper we suggest an alternative procedure to the Fisher kernel for systematically finding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identification/verification and image classification tasks and show that these new kernels have the best performance in speaker verification and mostly outperform the Fisher kernel based SVM's and the generative classifiers in speaker identification and image classification.
C1 Hewlett Packard Corp, Cambridge Res Lab, Cambridge, MA 02142 USA.
RP Moreno, PJ (reprint author), Hewlett Packard Corp, Cambridge Res Lab, Cambridge, MA 02142 USA.
CR BIMBOT F, 1995, SPEECH COMMUN, V17, P177, DOI 10.1016/0167-6393(95)00013-E
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   CHEN K, 2003, PATTRN RECOGNITION, P329
   Jaakkola T., 1999, P INT C INT SYST MOL
   MORENO PJ, 2000, ICASSP
   Smith N., 2001, CUEDFINFENGTR387
   STERN RM, 1997, DARPA SPEECH REC WOR
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VASCONCELOS N, 2000, IEEE INT C PATT REC
   WAN V, 2000, IEEE P
NR 10
TC 25
Z9 27
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1385
EP 1392
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500172
DA 2019-06-15
ER

PT J
AU Sauser, EL
   Billard, AG
AF Sauser, EL
   Billard, AG
TI Parallel and distributed neural models of the ideomotor principle: An
   investigation of imitative cortical pathways
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT Neural Information Processing Systems Conference (NIPS 05)
CY DEC, 2005
CL Vancouver, CANADA
DE neural mechanisms of imitation; ideomotor principle; neural field
   computation
ID TEMPORAL CORTEX; PARIETAL CORTEX; MACAQUE MONKEY; MOVEMENT;
   COMPATIBILITY; DIRECTION; REPRESENTATION; ORIENTATION; INFORMATION;
   MECHANISMS
AB Humans' capacity to imitate has been extensively investigated through a wide-range of behavioral and developmental studies. Yet, despite the huge amount of phenomenological evidence gathered, we are still unable to relate this behavioral data to any specific neural substrate. In this paper, we investigate how principles from psychology can be the result of neural computations and therefore attempt to bridge the gap between monkey neurophysiology and human behavioral data, and hence between these two complementary disciplines. Specifically, we address the principle of ideomotor compatibility, by which 'observing the movements of others influences the quality of one's own performance' and develop two neural models which account for a set of related behavioral studies [Brass, M., Bekkering, H., Wohlschlager, A., & Prinz, W. (2000). Compatibility between observed and executed finger movements: comparing symbolic, spatial and imitative cues. Brain and Cognition 44, 124-143]. We show that the ideomotor effect could be the result of two distinct cognitive pathways, which can be modeled by means of biologically plausible neural architectures. Furthermore, we propose a novel behavioral experiment to confirm or refute either of the two model pathways. (c) 2006 Elsevier Ltd. All rights reserved.
C1 Ecole Polytech Fed Lausanne, LASA Lab, CH-1015 Lausanne, Switzerland.
RP Sauser, EL (reprint author), Ecole Polytech Fed Lausanne, LASA Lab, Stn 9, CH-1015 Lausanne, Switzerland.
EM eric.sauser@epfl.ch
CR AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259
   Andersen RA, 1997, ANNU REV NEUROSCI, V20, P303, DOI 10.1146/annurev.neuro.20.1.303
   Arbib M, 2003, NEURAL NETWORKS, V16, P1237, DOI 10.1016/j.neunet.2003.08.002
   ARBIB M, 2000, NEURAL NETWORKS, V13, P953
   Ashbridge E, 2000, COGN NEUROPSYCHOL, V17, P13, DOI 10.1080/026432900380463
   BILLARD A, 2001, ROBOTICS AUTONOMOUS, V941, P1
   BILLARD A, 2002, HDB BRAIN THEORY NEU, P566
   Brass M, 2000, BRAIN COGNITION, V44, P124, DOI 10.1006/brcg.2000.1225
   Brass M, 2001, ACTA PSYCHOL, V106, P3, DOI 10.1016/S0001-6918(00)00024-X
   Cisek P, 2005, NEURON, V45, P801, DOI 10.1016/j.neuron.2005.01.027
   Decety J, 2003, TRENDS COGN SCI, V7, P527, DOI 10.1016/j.tics.2003.10.004
   DEMIRIS Y, 2002, IMITATION ANIMALS AR, P327
   Erlhagen W, 2002, PSYCHOL REV, V109, P545, DOI 10.1037//0033-295X.109.3.545
   Franconeri SL, 2005, PERCEPT PSYCHOPHYS, V67, P962, DOI 10.3758/BF03193623
   GREENWALD AG, 1970, PSYCHOL REV, V77, P73, DOI 10.1037/h0028689
   HASBROUCQ T, 1991, J EXPT PSYCHOL HUMAN, V17, P266
   HEDGE A, 1975, ACTA PSYCHOL, V39, P427, DOI 10.1016/0001-6918(75)90041-4
   Heyes C, 2005, COGNITIVE BRAIN RES, V22, P233, DOI 10.1016/j.cogbrainres2004.09.009
   Hick WE, 1952, Q J EXP PSYCHOL, V4, P11, DOI 10.1080/17470215208416600
   Itti L., 2001, NATURE REV NEUOSCIEN, V2, P194
   Jellema T, 2004, CEREB CORTEX, V14, P781, DOI 10.1093/cercor/bhh038
   Kilner JM, 2003, CURR BIOL, V13, P522, DOI 10.1016/S0960-9822(03)00165-9
   KOPECZ K, 1995, BIOL CYBERN, V73, P49, DOI 10.1007/BF00199055
   KORNBLUM S, 1994, PSYCHOL RES-PSYCH FO, V56, P130, DOI 10.1007/BF00419699
   LACOBONI M, 1999, SCIENCE, V286, P2526
   Meltzoff AN, 1997, EARLY DEV PARENTING, V6, P179, DOI 10.1002/(SICI)1099-0917(199709/12)6:3/4<179::AID-EDP157>3.0.CO;2-R
   OZTOP E, 2006, NEURAL NETWORKS, V19
   PERRETT DI, 1989, IMAGES UNDERSTANDING, P94
   Pouget A, 2003, ANNU REV NEUROSCI, V26, P381, DOI 10.1146/annurev.neuro.26.041002.131112
   Proctor RW, 2003, ACTA PSYCHOL, V112, P259, DOI 10.1016/S0001-6918(02)00125-7
   Rizzolatti G, 2001, NAT REV NEUROSCI, V2, P661, DOI 10.1038/35090060
   Salinas E, 1996, P NATL ACAD SCI USA, V93, P11956, DOI 10.1073/pnas.93.21.11956
   Sauser EL, 2005, NEUROCOMPUTING, V64, P5, DOI 10.1016/j.neucom.2004.11.019
   SCHADLEN MN, 2001, J NEUROPHYSIOL, V86, P1916
   SCHWARTZ AB, 1988, J NEUROSCI, V8, P2913
   SIMON JR, 1990, ACTA PSYCHOL, V73, P159, DOI 10.1016/0001-6918(90)90077-S
   SIMON JR, 1981, ACTA PSYCHOL, V47, P63, DOI 10.1016/0001-6918(81)90039-1
   Wilimzig C., 2005, P 27 ANN COGN SCI SO, P2359
   Wohlschlager A, 2003, PHILOS T R SOC B, V358, P501, DOI 10.1098/rstb.2002.1257
   Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238
   Zhang HZH, 1999, COGNITIVE PSYCHOL, V38, P386, DOI 10.1006/cogp.1998.0703
   Zhang K, 1996, J NEUROSCI, V16, P2112
NR 42
TC 24
Z9 24
U1 0
U2 2
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 285
EP 298
DI 10.1016/j.neunet.2006.02.003
PG 14
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600004
PM 16624521
OA Green Published
DA 2019-06-15
ER

PT J
AU Almeida, LB
AF Almeida, LB
TI Linear and nonlinear ICA based on mutual information - the MISEP method
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 2002 Conference on Neural Information Processing Systems
CY 2002
CL VANCOUVER, CANADA
DE ICA; blind source separation; nonlinear ICA; nonlinear BSS; mutual
   information
ID INDEPENDENT COMPONENT ANALYSIS; BLIND SOURCE SEPARATION; ALGORITHM;
   INFOMAX
AB MISEP is a method for linear and nonlinear ICA, that is able to handle a large variety of situations. It is an extension of the well known INFOMAX method, in two directions: (1) handling of nonlinear mixtures, and (2) learning the nonlinearities to be used at the outputs. The method can therefore separate linear and nonlinear mixtures of components with a wide range of statistical distributions. This paper presents the basis of the MISEP method, as well as experimental results obtained with it. New results show the applicability of the method to mixtures of up to 10 sources, and suggest that its performance scales relatively well with the dimensionality of the problem. The results also show that, although the nonlinear blind source separation problem is ill-posed, the use of regularization allows the problem to be solved when the mixture is not too strongly nonlinear. (C) 2003 Published by Elsevier B.V.
C1 INESC ID, P-1000029 Lisbon, Portugal.
RP Almeida, LB (reprint author), INESC ID, R Alves Redol,9, P-1000029 Lisbon, Portugal.
EM luis.almeida@inesc-id.pt
RI Almeida, Luis/A-7651-2012
OI Almeida, Luis/0000-0002-1324-0068
CR ALMEIDA LB, IN PRESS J MACH LEAR
   ALMEIDA LB, 2002, P 2002 INT JOINT C N
   ALMEIDA LB, 2003, P INT WORKSH IND COM, P113
   ALMEIDA LB, P 2001 INT JOINT C N
   ALMEIDA LB, 2000, P 2 INT WORKSH IND C, P169
   ALMEIDA LB, 2000, P S 2000 AD SYST SIG
   Almeida LB, 1997, HDB NEURAL COMPUTATI
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Bell AJ, 2003, P 4 INT S IND COMP A, P921
   BUREL G, 1992, NEURAL NETWORKS, V5, P937, DOI 10.1016/S0893-6080(05)80090-5
   Cardoso JF, 1997, IEEE SIGNAL PROC LET, V4, P112, DOI 10.1109/97.566704
   CICHOCKI A, 2002, ADAPTIVE SIGNAL IMAG
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   DARMOIS G., 1953, REV I INT STAT, V21, P2, DOI 10.2307/1401511
   DECO G, 1995, NEURAL NETWORKS, V8, P525, DOI 10.1016/0893-6080(94)00108-X
   Harmeling S., 2001, P INT WORKSH IND COM
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719
   LEE TW, 1999, P AM I PHYS OCT
   Marques G., 1999, P 1 INT WORKSH IND C, P277
   MARQUES GC, 1996, P INT C NEUR NETW WA, P453
   Martinez D, 2003, IEEE T NEURAL NETWOR, V14, P228, DOI 10.1109/TNN.2002.806624
   PALMIERI F, 1999, P 1 INT WORKSH IND C, P93
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   TALEB A, 1999, P 1 INT WORKSH IND C, P155
   TALEB A, 1997, P ICANN 97 LAUS SWIT
   VALPOLA H, 2000, P 2 INT WORKSH IND C, P251
   Yang HH, 1998, SIGNAL PROCESS, V64, P291, DOI 10.1016/S0165-1684(97)00196-5
NR 29
TC 24
Z9 27
U1 0
U2 4
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 231
EP 245
DI 10.1016/j.sigpro.2003.10.008
PG 15
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600003
DA 2019-06-15
ER

PT S
AU Lavrenko, V
   Manmatha, R
   Jeon, J
AF Lavrenko, V
   Manmatha, R
   Jeon, J
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A model for learning the semantics of pictures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.
C1 Univ Massachusetts, Dept Comp Sci, Ctr Intelligent Informat Retrieval, Amherst, MA 01003 USA.
RP Lavrenko, V (reprint author), Univ Massachusetts, Dept Comp Sci, Ctr Intelligent Informat Retrieval, Amherst, MA 01003 USA.
CR Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   BLEI D, 2003, COMMUNICATION
   Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   DUYGULU P, 2002, 7 EUR C COMP VIS, P97
   Jeon J., 2003, P 26 ANN INT ACM SIG, P119, DOI DOI 10.1145/860435.860459
   Lavrenko V., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P175
   LVRENKO V, 2001, P 24 INT ACM SIGIR C, P120
   Mori Y., 1999, MISRM 99 1 INT WORKS
   Ponte J. M., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P275, DOI 10.1145/290941.291008
   SCNEIDERMAN H, P IEEE CVPR 2000, P1746
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
NR 11
TC 23
Z9 28
U1 0
U2 4
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 553
EP 560
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500070
DA 2019-06-15
ER

PT J
AU Valpola, H
   Harva, M
   Karhunen, J
AF Valpola, H
   Harva, M
   Karhunen, J
TI Hierarchical models of variance sources
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE variance modelling; variational Bayesian leaming; factor analysis;
   hierarchical model; blind source separation
AB In many models, variances are assumed to be constant although this assumption is often unrealistic in practice. Joint modelling of means and variances is difficult in many learning approaches, because it can lead into infinite probability densities. We show that a Bayesian variational technique which is sensitive to probability mass instead of density is able to jointly model both variances and means. We consider a model structure where a Gaussian variable, called variance node, controls the variance of another Gaussian variable. Variance nodes make it possible to build hierarchical models for both variances and means. We report experiments with artificial data which demonstrate the ability of the learning algorithm to find variance sources explaining and characterizing well the variances in the multidimensional data. Experiments with biomedical MEG data show that variance sources are present in real-world signals. (C) 2003 Elsevier B.V. All rights reserved.
C1 Aalto Univ, Neural Networks Res Ctr, FIN-02015 Espoo, Finland.
RP Karhunen, J (reprint author), Aalto Univ, Neural Networks Res Ctr, POB 5400, FIN-02015 Espoo, Finland.
EM juha.karhunen@hut.fi
CR Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458
   Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215
   BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1
   Cardoso JF, 1998, INT CONF ACOUST SPEE, P1941, DOI 10.1109/ICASSP.1998.681443
   CHAN K, 2001, P INT C IND COMP AN, P492
   CHOUDREY R, 2000, P IEEE WORKSH NEUR N, P435
   GHYSELS E, 1996, STAT METHODS FINANCE, P119
   Girolami M, 2001, NEURAL COMPUT, V13, P2517, DOI 10.1162/089976601753196003
   Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306
   Honkela A, 2003, NEURAL PROCESS LETT, V17, P191, DOI 10.1023/A:1023655202546
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   HYVARINEN A, 2003, UNPUB SIGNAL PROCESS
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   Ilin A, 2003, P 4 INT S IND COMP A, P915
   Jordan M., 1999, LEARNING GRAPHICAL M
   Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050
   Kohonen T, 1997, NEURAL COMPUT, V9, P1321, DOI 10.1162/neco.1997.9.6.1321
   Lappalainen H, 2000, PERSP NEURAL COMP, P75
   Lathauwer L. D., 1995, P IEEE SP ATHOS WORK, P134
   Miskin J, 2000, PERSP NEURAL COMP, P123
   Parra L, 2001, ADV NEUR IN, V13, P786
   Ueda N, 2000, NEURAL COMPUT, V12, P2109, DOI 10.1162/089976600300015088
   Valpola H, 2002, NEURAL COMPUT, V14, P2647, DOI 10.1162/089976602760408017
   VALPOLA H, 2003, P 4 INT S IND COMP A, P83
   VALPOLA H, 2001, P 3 INT C IND COMP A, P710
   VALPOLA H, 2003, P 4 INT S IND COMP A, P257
   Vigario R, 2000, IEEE T BIO-MED ENG, V47, P589, DOI 10.1109/10.841330
NR 27
TC 22
Z9 22
U1 0
U2 2
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1879-2677
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 267
EP 282
DI 10.1016/j.sigpro.2003.10.014
PG 16
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600006
DA 2019-06-15
ER

PT S
AU Likhachev, M
   Gordon, G
   Thrun, S
AF Likhachev, M
   Gordon, G
   Thrun, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI ARA*: Anytime A* with provable bounds on sub-optimality
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID SEARCH
AB In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they find a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by finding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it finds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is significantly more efficient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover.
C1 Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Likhachev, M (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
CR Bonet B, 2001, ARTIF INTELL, V129, P5, DOI 10.1016/S0004-3702(01)00108-4
   DEAN TL, 1988, P NAT C ART INT AAAI
   HAEHNEL D, 2003, COMMUNICATION
   KOENIG S, 2002, ADV NEURAL INFORMATI, V14
   KORF RE, 1993, ARTIF INTELL, V62, P41, DOI 10.1016/0004-3702(93)90045-D
   Likhachev M., 2003, CMUCS03148
   Pearl Judea, 1984, HEURISTICS INTELLIGE
   Zhou R., 2002, P NAT C ART INT AAAI
   ZILBERSTEIN S, 1995, IMPRECISE APPROXIMAT
NR 9
TC 22
Z9 23
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 767
EP 774
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500096
DA 2019-06-15
ER

PT J
AU Demiris, Y
   Simmons, G
AF Demiris, Yiannis
   Simmons, Gavin
TI Perceiving the unusual: Temporal properties of hierarchical motor
   representations for action perception
SO NEURAL NETWORKS
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE imitation; minimum variance; hierarchical structures; action
   recognition; corticospinal excitability
ID TORQUE-CHANGE MODEL; PREMOTOR CORTEX; ARM MOVEMENTS; MIRROR; MODULATION;
   IMITATION; COORDINATION; ARCHITECTURE; NEURONS; MONKEY
AB Recent computational approaches to action imitation have advocated the use of hierarchical representations in the perception and imitation of demonstrated actions. Hierarchical representations present several advantages, with the main one being their ability to process information at multiple levels of detail. However, the nature of the hierarchies in these approaches has remained relatively unsophisticated, and their relation with biological evidence has not been investigated in detail, in particular with respect to the timing of movements. Following recent neuroscience work on the modulation of the premotor mirror neuron activity during the observation of unpredictable grasping movements, we present here an implementation of our HAMMER architecture using the minimum variance model for implementing reaching and grasping movements that have biologically plausible trajectories. Subsequently, we evaluate the performance of our model in matching the temporal dynamics of the modulation of cortical excitability during the passive observation of normal and unpredictable movements of human demonstrators. (c) 2006 Elsevier Ltd. All rights reserved.
C1 Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, BioART, Intelligent Syst & Networks Grp, London SW7 2BT, England.
RP Demiris, Y (reprint author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, BioART, Intelligent Syst & Networks Grp, S Kensington Campus, London SW7 2BT, England.
EM y.demiris@imperial.ac.uk
OI Demiris, Yiannis/0000-0003-4917-3343
CR Alissandrakis A, 2002, IEEE T SYST MAN CY A, V32, P482, DOI 10.1109/TSMCA.2002.804820
   BILLARD A, 2000, CYBERNET SYST, V32, P155
   DEMIRIS J, 1999, THESIS U EDINBURGH S
   Demiris Y, 2003, CONNECT SCI, V15, P231, DOI 10.1080/09540090310001655129
   DEMIRIS Y, 2002, IMITATION ANIMALS AR, P327
   DEMIRIS Y, 2002, P INT C NEUR INF PRO, P111
   DEMIRIS Y, 2006, IN PRESS ROBOTICS AU
   Essen D.C.V., 1983, TRENDS NEUROSCI, V6, P370
   Fagg AH, 1998, NEURAL NETWORKS, V11, P1277, DOI 10.1016/S0893-6080(98)00047-1
   Ferrari PF, 2005, J COGNITIVE NEUROSCI, V17, P212, DOI 10.1162/0898929053124910
   FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392
   FLASH T, 1985, J NEUROSCI, V5, P1688
   Fuster JM, 2004, TRENDS COGN SCI, V8, P143, DOI 10.1016/j.tics.2004.02.004
   Gangitano M, 2004, EUR J NEUROSCI, V20, P2193, DOI 10.1111/j.1460-9568.2004.03655.x
   Gangitano M, 2001, NEUROREPORT, V12, P1489, DOI 10.1097/00001756-200105250-00038
   Gergely G, 2003, CONNECT SCI, V15, P191, DOI 10.1080/09540090310001684604
   Grezes J, 2003, NEUROIMAGE, V18, P928, DOI 10.1016/S1053-8119(03)00042-9
   Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   JEANNEROD M, 1984, J MOTOR BEHAV, V16, P235
   Jeannerod M., 1981, ATTENTION PERFORM, P153
   Johnson M., 2005, P AISB 2005 S IM AN, P69
   Johnson M., 2004, P TAROS ESS, P63
   Johnson M., 2005, P TAROS, P119
   Karniel A, 2002, NEURAL NETWORKS, V15, P305, DOI 10.1016/S0893-6080(02)00020-5
   KHADHOURI B, 2005, P IJCAI 05, P1458
   Kilner JM, 2004, NAT NEUROSCI, V7, P1299, DOI 10.1038/nn1355
   Koechlin E, 2003, SCIENCE, V302, P1181, DOI 10.1126/science.1088545
   KUNIYOSHI Y, 1994, IEEE T ROBOTIC AUTOM, V10, P799, DOI 10.1109/70.338535
   MORASSO P, 1981, EXP BRAIN RES, V42, P223
   Nakano E, 1999, J NEUROPHYSIOL, V81, P2140
   Narendra KS, 1997, IEEE T AUTOMAT CONTR, V42, P171, DOI 10.1109/9.554398
   Nehaniv CL, 2002, FROM ANIM ANIMAT, P41
   Oztop E, 2002, BIOL CYBERN, V87, P116, DOI 10.1007/s00422-002-0318-1
   RIZZOLATTI G, 1988, EXP BRAIN RES, V71, P491, DOI 10.1007/BF00248742
   Rizzolatti G, 1996, COGNITIVE BRAIN RES, V3, P131, DOI 10.1016/0926-6410(95)00038-0
   ROSENBAUM DA, 1983, J EXP PSYCHOL HUMAN, V9, P86, DOI 10.1037/0096-1523.9.1.86
   Rowe J, 2002, NEUROIMAGE, V17, P988, DOI 10.1006/nimg.2002.1156
   Schaal S, 2003, PHILOS T R SOC B, V358, P537, DOI 10.1098/rstb.2002.1258
   Simmons G, 2005, J ROBOTIC SYST, V22, P677, DOI 10.1002/rob.20092
   SIMMONS G, 2004, P 2004 4 IEEE RAS RS, V1, P215
   Smeets JBJ, 1999, MOTOR CONTROL, V3, P237, DOI 10.1123/mcj.3.3.237
   Sommerville JA, 2005, COGNITION, V96, pB1, DOI 10.1016/j.cognition.2004.07.004
   Todorov E, 2002, NAT NEUROSCI, V5, P1226, DOI 10.1038/nn963
   Umilta MA, 2001, NEURON, V31, P155, DOI 10.1016/S0896-6273(01)00337-3
   UNO Y, 1989, BIOL CYBERN, V61, P89
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
   Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238
NR 48
TC 20
Z9 20
U1 1
U2 5
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2006
VL 19
IS 3
BP 272
EP 284
DI 10.1016/j.neunet.2006.02.005
PG 13
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA 053HQ
UT WOS:000238296600003
PM 16595173
DA 2019-06-15
ER

PT J
AU Hyvarinen, A
   Hurri, J
AF Hyvarinen, A
   Hurri, J
TI Blind separation of sources that have spatiotemporal variance
   dependencies
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE independent component analysis; blind source separation; dependent
   component analysis; higher-order cumulants
ID INDEPENDENT COMPONENT ANALYSIS; NATURAL IMAGE SEQUENCES; COHERENCE;
   ALGORITHM; SIGNALS
AB In blind source separation methods, the sources are typically assumed to be independent. Some methods are also able to separate dependent sources by estimating or assuming a parametric model for their dependencies. Here, we propose a method that separates dependent sources without a parametric model of their dependency structure. This is possible by introducing some general assumptions on the structure of the dependencies: the sources are dependent only through their variances (general activity levels), and the variances of the sources have temporal correlations. The method can be called double-blind because of this additional blind aspect: We do not need to estimate (or assume) a parametric model of the dependencies, which is in stark contrast to most previous methods. (C) 2003 Elsevier B.V. All rights reserved.
C1 Univ Helsinki, Dept Comp Sci, Helsinki Inst Informat Technol, Basic Res Unit, FIN-00014 Helsinki, Finland.
   Aalto Univ, Neural Networks Res Ctr, FIN-02150 Espoo, Finland.
RP Hyvarinen, A (reprint author), Univ Helsinki, Dept Comp Sci, Helsinki Inst Informat Technol, Basic Res Unit, POB 26, FIN-00014 Helsinki, Finland.
EM aapo.hyvarinen@helsinki.fi
RI Hyvarinen, Aapo/E-9006-2012
CR BACH FR, 2002, P 18 C UAI 2002
   Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307
   Cardoso J.F., 1998, P IEEE INT C AC SPEE
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   DELFOSSE N, 1995, SIGNAL PROCESS, V45, P59, DOI 10.1016/0165-1684(95)00042-C
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Horn R. A., 1985, MATRIX ANAL
   Hurri J, 2003, NETWORK-COMP NEURAL, V14, P527, DOI 10.1088/0954-898X/14/3/308
   Hurri J, 2003, NEURAL COMPUT, V15, P663, DOI 10.1162/089976603321192121
   Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   Hyvarinen A, 2003, J OPT SOC AM A, V20, P1237, DOI 10.1364/JOSAA.20.001237
   Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992
   Hyvarinen A, 2001, IEEE T NEURAL NETWOR, V12, P1471, DOI 10.1109/72.963782
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   LANG T, 1991, IEEE T CIRCUITS SYST, V38, P499, DOI 10.1109/31.76486
   MATSUOKA K, 1995, NEURAL NETWORKS, V8, P411, DOI 10.1016/0893-6080(94)00083-X
   MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634
   Pham D.T, 2000, P INT WORKSH IND COM, pHelsinki
   Valpola H., 2003, SIGNAL PROCESSING, V84
   Vigario R, 1998, ADV NEUR IN, V10, P229
   Wainwright MJ, 2001, APPL COMPUT HARMON A, V11, P89, DOI 10.1006/acha.2000.0350
NR 23
TC 20
Z9 22
U1 0
U2 2
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1879-2677
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 247
EP 254
DI 10.1016/j.sigpro.2003.10.010
PG 8
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600004
DA 2019-06-15
ER

PT S
AU Sutskever, I
   Vinyals, O
   Le, QV
AF Sutskever, Ilya
   Vinyals, Oriol
   Le, Quoc, V
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Sequence to Sequence Learning with Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.
C1 [Sutskever, Ilya; Vinyals, Oriol; Le, Quoc, V] Google, Mountain View, CA 94043 USA.
RP Sutskever, I (reprint author), Google, Mountain View, CA 94043 USA.
EM ilyasu@google.com; vinyals@google.com; qvl@google.com
CR Auli M., 2013, EMNLP
   Bahdanau D., 2014, ARXIV14090473
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Ciresan D., 2012, CVPR
   Dahl  G.E., 2012, IEEE T AUDIO SPEECH
   Devlin J., 2014, ACL
   Durrani Nadir, 2014, WMT
   Graves A, 2013, ARXIV13080850
   Graves Alex, 2006, ICML
   Hermann Karl Moritz, 2014, ICLR
   Hinton  G., 2012, IEEE SIGNAL PROCESSI
   Hochreiter S., 1997, LSTM CAN SOLVE HARD
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hochreiter S., 1991, THESIS
   Kalchbrenner  N, 2013, EMNLP
   Krizhevsky A., 2012, NIPS
   Le Q. V., 2012, ICML
   Lecun Y., 1998, P IEEE
   Mikolov T., 2012, THESIS
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Papineni K., 2002, ACL
   Pascanu R, 2012, ARXIV E PRINTS, V1211, P5063
   Pouget-Abadie J., 2014, ARXIV14091257
   Razborov A., 1992, P 3 SCAND WORKSH ALG
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sundermeyer M., 2010, INTERSPEECH 2010
   Werbos Paul J., 1990, P IEEE
NR 30
TC 19
Z9 19
U1 9
U2 9
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101017
DA 2019-06-15
ER

PT S
AU Courville, AC
   Daw, ND
   Gordon, GJ
   Touretzky, DS
AF Courville, AC
   Daw, ND
   Gordon, GJ
   Touretzky, DS
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Model uncertainty in classical conditioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments. Traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery; uncertainty over the model structure is not considered. We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes. According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justified by additional experience.
C1 Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Courville, AC (reprint author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
CR Courville AC, 2002, ADV NEUR IN, V14, P3
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   Iba Y, 2001, INT J MOD PHYS C, V12, P623, DOI 10.1142/S0129183101001912
   Kakade S, 2002, PSYCHOL REV, V109, P533, DOI 10.1037//0033-295X.109.3.533
   MACKAY DJC, 1991, ADV NEURAL INFORMATI, V4
   PEARCE JM, 1994, PSYCHOL REV, V101, P587, DOI 10.1037/0033-295X.101.4.587
   Rescorla R. A., 1972, CLASSICAL CONDITIONI
   Sutton R. S., 1990, LEARNING COMPUTATION, P497
   Tenenbaum JB, 2001, ADV NEUR IN, V13, P59
   WILLIAMS DA, 1988, LEARN MOTIV, V19, P345, DOI 10.1016/0023-9690(88)90045-8
   YIN H, 1994, J EXP PSYCHOL ANIM B, V20, P419, DOI 10.1037//0097-7403.20.4.419
NR 11
TC 19
Z9 19
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 977
EP 984
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500122
DA 2019-06-15
ER

PT S
AU Li, HF
   Jiang, T
   Zhang, KH
AF Li, HF
   Jiang, T
   Zhang, KH
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Efficient and robust feature extraction by maximum margin criterion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix S-w. Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efficient and stable.
C1 Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA.
RP Li, HF (reprint author), Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA.
CR Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fukunaga K., 1990, INTRO STAT PATTERN R
   HONG ZQ, 1991, PATTERN RECOGN, V24, P317, DOI 10.1016/0031-3203(91)90074-F
   LIU K, 1992, PATTERN RECOGN, V25, P731, DOI 10.1016/0031-3203(92)90136-7
   Loog M, 2001, IEEE T PATTERN ANAL, V23, P762, DOI 10.1109/34.935849
   Mika S., 1999, NEURAL NETWORKS SIGN, VX, P41, DOI DOI 10.1109/NNSP.1999.788121
   Samaria F., 1994, P 2 IEEE WORKSH APPL
   Stewart G. W., 1973, INTRO MATRIX COMPUTA
   TIAN Q, 1986, OPT ENG, V25, P834, DOI 10.1117/12.7973916
   Vapnik V. N., 1998, STAT LEARNING THEORY
NR 11
TC 19
Z9 19
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 97
EP 104
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500013
DA 2019-06-15
ER

PT J
AU Blanchard, G
   Lugosi, G
   Vayatis, N
AF Blanchard, G
   Lugosi, G
   Vayatis, N
TI On the rate of convergence of regularized boosting classifiers
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE classification; boosting; consistency; rates of convergence; decision
   stumps
ID MINIMAX NONPARAMETRIC CLASSIFICATION; APPROXIMATION; BOUNDS
AB A regularized boosting method is introduced, for which regularization is obtained through a penalization function. It is shown through oracle inequalities that this method is model adaptive. The rate of convergence of the probability of misclassification is investigated. It is shown that for quite a large class of distributions, the probability of error converges to the Bayes risk at a rate faster than n(-(V+2)/(4(V+1))) where V is the VC dimension of the "base" class whose elements are combined by boosting methods to obtain an aggregated classifier. The dimension-independent nature of the rates may partially explain the good behavior of these methods in practical problems. Under Tsybakov's noise condition the rate of convergence is even faster. We investigate the conditions necessary to obtain such rates for different base classes. The special case of boosting using decision stumps is studied in detail. We characterize the class of classifiers realizable by aggregating decision stumps. It is shown that some versions of boosting work especially well in high-dimensional logistic additive models. It appears that adding a limited labelling noise to the training data may in certain cases improve the convergence, as has been also suggested by other authors.
C1 Univ Paris 11, CNRS, Math Lab, F-91405 Orsay, France.
   Pompeu Fabra Univ, Dept Econ, Barcelona 08005, Spain.
   Univ Paris 06, Lab Probabilities Modeles Aleatoires, F-75252 Paris 05, France.
RP Blanchard, G (reprint author), Univ Paris 11, CNRS, Math Lab, Batiment 425, F-91405 Orsay, France.
EM BLANCHAR@MATH.U-PSUD.FR; LUGOSI@UPK.ES; VAYATIS@CCR.JUSSIEU.FR
OI Lugosi, Gabor/0000-0003-1614-5901
CR Barron A. R., 1992, YAL WORKSH AD LEARN
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   BARTLETT P, 2002, P 15 ANN C COMP LEAR, P44
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   BARTLETT PL, 2003, UNPUB CONVEXITY CLAS
   BENNETT K, 2002, MACH LEARN, V48, P193
   Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720
   BLANCHARD G, 2003, UNPUB STAT PERFORMAN
   BOUSQUET O, 2003, IN PRESS ANN I STAT
   Breiman L, 1998, ANN STAT, V26, P801
   BREIMAN L, 2000, 577 UC BERK STAT DEP
   BUHLMANN P, 2003, IN PRESS J AM STAT A
   Collins M., 2000, P 13 ANN C COMP LEAR
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Devroye L., 1996, PROBABILISTIC THEORY
   Donahue MJ, 1997, CONSTR APPROX, V13, P187
   DUDLEY RM, 1978, ANN PROBAB, V6, P899, DOI 10.1214/aop/1176995384
   Duffy N, 2000, ADV NEUR IN, V12, P258
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   GIROSI F, 1993, ARTIFICIAL NEURAL NE, P169
   Hastie T. J., 1990, GENERALIZED ADDITIVE
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   JIANG W, 2003, IN PRESS ANN STAT
   Koltchinskii V., 2002, ANN STAT, V30
   Ledoux M., 1991, PROBABILITY BANACH S
   LUGOSI G, 2003, UNPUB NOTE RICHNESS
   LUGOSI G, 2003, IN PRESS ANN STAT
   Maiorov V, 1999, J APPROX THEORY, V99, P95, DOI 10.1006/jath.1998.3305
   MANNOR S, 2002, P 15 ANN C COMP LEAR
   MANNOR S, 2001, ADV NEURAL INFORMATI, V13
   Mason L., 1999, ADV LARGE MARGIN CLA, P221
   Massart Pascal, 2000, ANN FAC SCI TOULOUSE, V9, P245
   Meir R, 2002, LECT NOTES ARTIF INT, V2600, P118
   Meir R, 2000, IEEE T NEURAL NETWOR, V11, P323, DOI 10.1109/72.839004
   Mendelson S, 2002, IEEE T INFORM THEORY, V48, P1977, DOI 10.1109/TIT.2002.1013137
   NEDELEC E, 2003, UNPUB RISK BOUNDS ST
   Pinkus A., 1999, Acta Numerica, V8, P143, DOI 10.1017/S0962492900002919
   Pollard D., 1984, CONVERGENCE STOCHAST
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1007/BF00116037
   Schapire RE, 1998, ANN STAT, V26, P1651
   SONTAG ED, 1992, IEEE T NEURAL NETWOR, V3, P981, DOI 10.1109/72.165599
   TSYBAKOV AB, 2003, IN PRESS ANN STAT
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   Yang YH, 1999, IEEE T INFORM THEORY, V45, P2285
   Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271
   ZHANG T, 2003, IN PRESS ANN STAT
NR 48
TC 18
Z9 18
U1 0
U2 1
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 861
EP 894
DI 10.1162/1532443041424319
PG 34
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800006
DA 2019-06-15
ER

PT S
AU Marlin, B
AF Marlin, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Modeling user rating profiles for collaborative filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB In this paper we present a generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP). The generative process which underlies URP is designed to produce complete user rating profiles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each.
C1 Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.
RP Marlin, B (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.
CR Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boutilier C., 2003, P 19 ANN C UNC ART I, P98
   Buntine W., 2002, P EUR C MACH LEARN
   Claypool M, 1999, P ACM SIGIR WORKSH R
   Girolami Mark, 2003, P 26 ANN INT ACM SIG, P433
   Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209
   HOFMANN T, 2001, P EUR C MACH LEARN
   MINKA T, 2003, UNPUB ESTIMATING DIR
   Resnick P, 1994, P ACM C COMP SUPP CO, P175, DOI DOI 10.1145/192844.192905
NR 9
TC 17
Z9 19
U1 0
U2 5
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 627
EP 634
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500079
DA 2019-06-15
ER

PT S
AU Philipona, D
AF Philipona, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Perception of the structure of the physical world using unknown
   multimodal sensors and effectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in? Taking the case of space for example, is there a way for this algorithm to realize that its body is in a three dimensional world? Is it possible for this algorithm to discover how to move in a straight line? And more basically: do these questions make any sense at all given that the algorithm only has access to the very high-dimensional data consisting of its sensory inputs and motor outputs?
   We demonstrate in this article how these questions can be given a positive answer. We show that it is possible to make an algorithm that, by analyzing the law that links its motor outputs to its sensory inputs, discovers information about the structure of the world regardless of the devices constituting the body it is linked to. We present results from simulations demonstrating a way to issue motor orders resulting in "fundamental" movements of the body as regards the structure of the physical world.
C1 Sony CSL, F-75005 Paris, France.
RP Philipona, D (reprint author), Sony CSL, 6 Rue Amyot, F-75005 Paris, France.
CR Bourbaki  N., 1971, VARIETES DIFFERENTIE
   Masson T., 2001, GEOMETRIE DIFFERENTI
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24
   Philipona D., 2003, NEURAL COMPUTATION, V15
NR 4
TC 16
Z9 17
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 945
EP 952
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500118
DA 2019-06-15
ER

PT S
AU Taskar, B
   Wong, MF
   Abbeel, P
   Koller, D
AF Taskar, B
   Wong, MF
   Abbeel, P
   Koller, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Link prediction in relational data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Many real-world domains are relational in nature, consisting of a set of objects related to each other in complex ways. This paper focuses on predicting the existence and the type of links between entities in such domains. We apply the relational Markov network framework of Taskar et al. to define a joint probabilistic model over the entire link graph - entity attributes and links. The application of the RMN algorithm to this task requires the definition of probabilistic patterns over subgraph structures. We apply this method to two new relational datasets, one involving university webpages, and the other a social network. We show that the collective classification approach of RMNs, and the introduction of subgraph patterns over link labels, provide significant improvements in accuracy over flat classification, which attempts to predict each link in isolation.
C1 Stanford Univ, Stanford, CA 94305 USA.
RP Taskar, B (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM btaskar@cs.stanford.edu; mingfai.wong@cs.stanford.edu;
   abbeel@cs.stanford.edu; koller@cs.stanford.edu
CR ADAMIC L, 2002, SOCIAL NETWORK CAUGH
   Craven M., 1998, P AAAI
   DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021
   Egghe L, 1990, INTRO INFORMETRICS
   Getoor L., 2001, P ICML
   GETOOR L, 2001, IJCAI WORKSH TEXT LE
   GHANI R, 2001, P ICML
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Koller D, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P580
   Lavrac N., 1994, INDUCTIVE LOGIC PROG
   Neville J., 2000, AAAI WORKSH LEARN ST
   Pearl J, 1988, PROBABILISTIC REASON
   TASKAR B, 2002, P UAI
   TASKAR B, 2001, P 17 INT JOINT C ART, P870
   Wasserman S, 1996, PSYCHOMETRIKA, V61, P401, DOI 10.1007/BF02294547
   Winograd Terry, 1998, PAGERANK CITATION RA
   YEDIDIA I, 2000, P NIPS
NR 17
TC 16
Z9 16
U1 0
U2 5
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 659
EP 666
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500083
DA 2019-06-15
ER

PT S
AU Wipf, D
   Palmer, J
   Rao, B
AF Wipf, D
   Palmer, J
   Rao, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Perspectives on sparse Bayesian learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a specific approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice.
C1 Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92092 USA.
RP Wipf, D (reprint author), Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92092 USA.
CR Bishop Christopher M, 2000, P 16 C UNC ART INT, P46
   Duda R, 2001, PATTERN CLASSIFICATI
   Faul AC, 2002, ADV NEUR IN, V14, P383
   Girolami M, 2001, NEURAL COMPUT, V13, P2517, DOI 10.1162/089976601753196003
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
NR 7
TC 16
Z9 16
U1 1
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 249
EP 256
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500032
DA 2019-06-15
ER

PT S
AU Campbell, WM
   Campbell, JP
   Reynolds, DA
   Jones, DA
   Leek, TR
AF Campbell, WM
   Campbell, JP
   Reynolds, DA
   Jones, DA
   Leek, TR
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Phonetic speaker recognition with support vector machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID VERIFICATION
AB A recent area of significant progress in speaker recognition is the use of high level features-idiolect, phonetic relations, prosody, discourse structure, etc. A speaker not only has a distinctive acoustic sound but uses language in a characteristic manner. Large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns, word patterns, etc. of an individual. We propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker. To this end, we explore techniques for text categorization applied to the problem. We derive a new kernel based upon a linearization of likelihood ratio scoring. We introduce a new phone-based SVM speaker recognition approach that halves the error rate of conventional phone-based approaches.
C1 MIT, Lincoln Lab, Lexington, MA 02420 USA.
RP Campbell, WM (reprint author), MIT, Lincoln Lab, Lexington, MA 02420 USA.
CR ADAMI A, 2003, P INT C AC SPEECH SI, P788
   ANDREWS WD, 2002, P INT C AC SPEECH SI, P149
   CAMPBELL WM, 2002, P ICASSP, P161
   Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142
   Doddington G., 2001, P EUR, P2521
   Joachims T., 2002, LEARNING CLASSIFY TE
   KLUSACEK D, 2003, P INT C AC SPEECH SI, P804
   *LING DAT CONS, SWITCHB 2 CORP
   MARTIN A, 1997, P EUROSPEECH, P1895
   PRZYBOCKI M, NIST YEAR 2003 SPEAK
   Quatieri TF, 2000, IEEE T SPEECH AUDI P, V8, P567, DOI 10.1109/89.861376
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Schmidt-Nielsen A, 2000, DIGIT SIGNAL PROCESS, V10, P249, DOI 10.1006/dspr.1999.0356
   Zissman MA, 1996, IEEE T SPEECH AUDI P, V4, P31, DOI 10.1109/TSA.1996.481450
NR 15
TC 15
Z9 17
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1377
EP 1384
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500171
DA 2019-06-15
ER

PT S
AU Dekel, O
   Manning, CD
   Singer, Y
AF Dekel, O
   Manning, CD
   Singer, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Log-linear models for label ranking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Label ranking is the task of inferring a total order over a predefined set of labels for each given instance. We present a general framework for batch learning of label ranking functions from supervised data. We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent. This enables us to accommodate a variety of ranking problems. In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels. Special cases of our setting are multilabel categorization and hierarchical classification. We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration. The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus.
C1 Hebrew Univ Jerusalem, Jerusalem, Israel.
RP Dekel, O (reprint author), Hebrew Univ Jerusalem, Jerusalem, Israel.
CR COLLINS M, 2002, 30 ANN M ACL
   COLLINS M, 2002, MACH LEARN, V47, P253
   Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188
   Crammer K., 2001, NIPS, V14
   DEKEL O, 2003, COLT, V16
   Elisseeff Andre, 2001, NIPS, V14
   FREUND Y, 1998, MACH LEARN P 15 INT
   LEBANON G, 2001, NIPS, V14
   LEBANON G, 2002, NIPS, V15
   SCHAPIRE RE, 2000, MACHINE LEARNING, V32
   SHASHUA A, 2000, NIPS, V15
   TOUTANOVA K, 2002, P 6 C NAT LANG LEARN
NR 12
TC 15
Z9 15
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 497
EP 504
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500063
DA 2019-06-15
ER

PT S
AU Crammer, K
   Kandola, J
   Singer, Y
AF Crammer, K
   Kandola, J
   Singer, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Online classification on a budget
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions. In this paper we describe and analyze a simple approach for an on-the-fly reduction of the number of past examples used for prediction. Experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm, accesses each training example multiple times.
C1 Hebrew Univ Jerusalem, IL-91904 Jerusalem, Israel.
RP Crammer, K (reprint author), Hebrew Univ Jerusalem, IL-91904 Jerusalem, Israel.
CR Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951
   Gentile C., 2001, J MACHINE LEARNING R, V2, P213
   Li Y, 2002, MACH LEARN, V46, P361, DOI 10.1023/A:1012435301888
   MEZAR DM, 1987, J PHYS A, V20, P745
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Vapnik V. N., 1998, STAT LEARNING THEORY
NR 7
TC 14
Z9 15
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 225
EP 232
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500029
DA 2019-06-15
ER

PT S
AU Littlewort, GC
   Bartlett, MS
   Fasel, IR
   Chenu, J
   Kanda, T
   Ishiguro, H
   Movellan, JR
AF Littlewort, GC
   Bartlett, MS
   Fasel, IR
   Chenu, J
   Kanda, T
   Ishiguro, H
   Movellan, JR
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Towards social robots: Automatic evaluation of human-robot interaction
   by face detection and expression classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID OBJECT RECOGNITION
AB Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a time scale of less than a second. In this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [13, 2]. The expression recognizer employs a combination of AdaBoost and SVM's. The generalization performance to new subjects for a 7-way forced choice was over 90% correct on two publicly available datasets. The outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system was deployed and evaluated for measuring spontaneous facial expressions in the field in an application for automatic assessment of human-robot interaction.
C1 Univ Calif San Diego, Inst Neural Computat, San Diego, CA 92103 USA.
RP Littlewort, GC (reprint author), Univ Calif San Diego, Inst Neural Computat, San Diego, CA 92103 USA.
RI Kanda, Takayuki/I-5843-2016
OI Kanda, Takayuki/0000-0002-9546-5825
CR Ekman P., 1976, HIL0984 UCSF
   Fasel I., 2002, P INT C ART NEUR NET
   Freund Y., 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.4236/IIM.2010.26047
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Ishiguro H, 2001, IND ROBOT, V28, P498, DOI 10.1108/01439910110410051
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lyons M. J., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P202, DOI 10.1109/AFGR.2000.840635
   PADGETT C, 1997, ADV NEURAL INFORMATI, V9
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586
   SUNG KK, 1994, AIM1521
   VIOLA P, 2001, 200101 CAMBR RES LAB
NR 13
TC 14
Z9 16
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1563
EP 1570
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500194
DA 2019-06-15
ER

PT S
AU Ng, AY
   Kim, HJ
   Jordan, MI
   Sastry, S
AF Ng, AY
   Kim, HJ
   Jordan, MI
   Sastry, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Autonomous helicopter flight via reinforcement learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition.
C1 Stanford Univ, Stanford, CA 94305 USA.
RP Ng, AY (reprint author), Stanford Univ, Stanford, CA 94305 USA.
CR ATKESON CG, 1997, AI REV, V11
   BAGNELL J, 2001, INT C ROB AUT IEEE
   BALAS GJ, 1995, MU ANAL SYNTHESIS TO
   Cleveland WS, 1979, J AM STAT ASS, V74
   Franklin G. F., 1995, FEEDBACK CONTROL DYN
   KIEFER J, 1952, ANN MATH STAT, V23, P462, DOI 10.1214/aoms/1177729392
   Leishman J. G., 2000, PRINCIPLES HELICOPTE
   Ng A.Y., 2000, P 16 C UNC ART INT
   NG AY, 1999, P 16 INT C MACH LEAR, P278
   NG AY, 2003, THESIS U CALIFORNIA
   ROBBINS H, 1951, ANN MATH STAT, V22, P40
   SHIM H, 2000, THESIS UC BERKELEY
NR 12
TC 14
Z9 14
U1 0
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 799
EP 806
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500100
DA 2019-06-15
ER

PT S
AU Eigen, D
   Puhrsch, C
   Fergus, R
AF Eigen, David
   Puhrsch, Christian
   Fergus, Rob
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Depth Map Prediction from a Single Image using a Multi-Scale Deep
   Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.
C1 [Eigen, David; Puhrsch, Christian; Fergus, Rob] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
RP Eigen, D (reprint author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
EM deigen@cs.nyu.edu; cpuhrsch@nyu.edu; fergus@cs.nyu.edu
FU ONR [N00014-13-1-0646]; NSF [1116923, 1149633]; Microsoft Research
FX The authors are grateful for support from ONR #N00014-13-1-0646, NSF
   #1116923, #1149633 and Microsoft Research.
CR Deng J., 2009, CVPR
   Fouhey D. F., 2013, ICCV
   Geiger  A., 2013, INT J ROBOTICS RES I
   Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Karsch K., 2014, TPAMI
   Konda K., 2013, ARXIV13123429V2
   Krizhevsky A., 2012, NIPS
   Ladicky M. P. Lubor, 2014, CVPR
   Levin A., 2007, SIGGRAPH
   Liu C., 2008, SIFT FLOW DENSE CORR
   Memisevic R., 2011, NIPS WORKSH DEEP LAR
   Michels  J., 2005, P 22 INT C MACH LEAR, P593
   Saxena A., 2005, NIPS
   Saxena A., 2008, TPAMI, P2
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Silberman  N., 2012, ECCV
   Sinz FH, 2004, LECT NOTES COMPUT SC, V3175, P245
   Snavely N., 2006, PHOTO TOURISM EXPLOR
   Yamaguchi K., 2012, ARXIV12041393V1
   Zisserman A., 2004, MULTIPLE VIEW GEOMET
NR 21
TC 13
Z9 13
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102100
DA 2019-06-15
ER

PT J
AU Takimoto, E
   Warmuth, MK
AF Takimoto, E
   Warmuth, MK
TI Path kernels and multiplicative updates
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE kernels; multiplicative updates; on-line algorithms; series parallel
   digraphs
ID DECISION TREE; WELL; ALGORITHMS
AB Kernels are typically applied to linear algorithms whose weight vector is a linear combination of the feature vectors of the examples. On-line versions of these algorithms are sometimes called "additive updates" because they add a multiple of the last feature vector to the current weight vector.
   In this paper we have found a way to use special convolution kernels to efficiently implement,'multiplicative" updates. The kernels are defined by a directed graph. Each edge contributes an input. The inputs along a path form a product feature and all such products build the feature vector associated with the inputs. We also have a set of probabilities on the edges so that the outflow from each vertex is one. We then discuss multiplicative updates on these graphs where the prediction is essentially a kernel computation and the update contributes a factor to each edge. After adding the factors to the edges, the total outflow out of each vertex is not one any more. However some clever algorithms re-normalize the weights on the paths so that the total outflow out of each vertex is one again. Finally, we show that if the digraph is built from a regular expressions, then this can be used for speeding up the kernel and re-normalization computations.
   We reformulate a large number of multiplicative update algorithms using path kernels and characterize the applicability of our method. The examples include efficient algorithms for learning disjunctions and a recent algorithm that predicts as well as the best pruning of a series parallel digraphs.
C1 Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan.
   Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
RP Takimoto, E (reprint author), Tohoku Univ, Grad Sch Informat Sci, Sendai, Miyagi 9808579, Japan.
EM T2@ECEl.T0H0KU.AC.JP; MANFRED@CSE.UCSC.EDU
CR AIELLO W, 2003, 14 ACM SIAM S DISCR, P771
   Awerbuch B, 2001, J COMPUT SYST SCI, V62, P385, DOI 10.1006/jcss.1999.1662
   Bylander T., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P184, DOI 10.1145/267460.267495
   CORTES C, 2002, ADV NEURAL INFORMATI, V15
   CRISTIANINI N, 1999, 6 EUR S ART NEUR NET, P189
   CRISTIANINI N, 2000, INTRO SUPPORT VECOTR
   DESANTIS A, 1988, P 29 ANN IEEE S FDN, P110
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gentile C., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P1, DOI 10.1145/307400.307405
   GENTILE C, 1998, ADV NEURAL INFORMATI, V11, P225
   HAUSLER D, 1999, UCSCCRL9910
   Helmbold DP, 2002, THEOR COMPUT SCI, V284, P109, DOI 10.1016/S0304-3975(01)00081-0
   Helmbold DP, 1997, MACH LEARN, V27, P51, DOI 10.1023/A:1007396710653
   KHARDON R, 2001, ADV NEURAL INFORMATI, V14, P423
   Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Kivinen J, 1999, LECT NOTES ARTIF INT, V1572, P153
   LEONARDI S, 1998, LECT NOTES COMPUT SC, V1442, P242
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Maass W, 1998, INFORM COMPUT, V141, P66, DOI 10.1006/inco.1997.2686
   MOHRI M, 1998, 98121910TM AT T LAB
   SHU F, 2003, 16 ANN C LEARN THEOR, P188
   Singer Y, 1997, ADV NEUR IN, V9, P641
   Takimoto E, 2002, THEOR COMPUT SCI, V288, P217, DOI 10.1016/S0304-3975(01)00401-7
   Takimoto E, 2001, THEOR COMPUT SCI, V261, P179, DOI 10.1016/S0304-3975(00)00138-9
   VALDES J, 1982, SIAM J COMPUT, V11, P298, DOI 10.1137/0211023
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
   WATKINS C, 1999, CSDTR9811 U LOND ROY
NR 29
TC 13
Z9 14
U1 0
U2 1
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 773
EP 818
DI 10.1162/1532443041424328
PG 46
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800003
DA 2019-06-15
ER

PT J
AU Harmeling, S
   Meinecke, F
   Muller, KR
AF Harmeling, S
   Meinecke, F
   Muller, KR
TI Injecting noise for analysing the stability of ICA components
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 2002 Conference on Neural Information Processing Systems
CY 2002
CL VANCOUVER, CANADA
DE ICA; noise injection; stability
ID BLIND SIGNAL SEPARATION; ALGORITHM
AB Usually, noise is considered to be destructive. We present a new method that constructively injects noise to assess the reliability and the grouping structure of empirical ICA component estimates. Our method can be viewed as a Monte-Carlo-style approximation of the curvature of some performance measure at the solution. Simulations show that the true root-mean-squared angle distances between the real sources and the source estimates can be approximated well by our method. In a toy experiment, we see that we are also able to reveal the underlying grouping structure of the extracted ICA components. Furthermore, an experiment with fetal ECG data demonstrates that our approach is useful for exploratory data analysis of real-world data. (C) 2003 Elsevier B.V. All rights reserved.
C1 Fraunhofer First IDA, D-12489 Berlin, Germany.
   Univ Potsdam, Dept Comp Sci, D-14482 Potsdam, Germany.
RP Harmeling, S (reprint author), Fraunhofer First IDA, Kekulestr 7, D-12489 Berlin, Germany.
EM harmeli@first.fhg.de
RI Muller, Klaus/C-3196-2013
OI Mueller, Klaus-Robert/0000-0002-3861-7685
CR Amari S, 1996, ADV NEUR IN, V8, P757
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054
   Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250
   CARDOSO JF, P ICA 2001 WORKSH
   CARDOSO JF, 2001, P ICA 2001 WORKSH
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703
   Efron B., 1994, INTRO BOOTSTRAP
   Golub G. H., 1996, MATRIX COMPUTATIONS
   HARMELING S, 2003, P INT WORKSH IND COM
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   LATHAUWER LD, 1995, P HOS 95 AIG SPAIN
   Magnus J.R., 1999, WILEY SERIES PROBABI
   Meinecke F, 2002, IEEE T BIO-MED ENG, V49, P1514, DOI 10.1109/TBME.2002.805480
   MEINECKE F, 2002, ADV NEURAL INFORMATI, V14
   Pham DT, 2001, SIGNAL PROCESS, V81, P855, DOI 10.1016/S0165-1684(00)00260-7
   Ziehe A., 1998, P INT C ART NEUR NET, P675
NR 21
TC 13
Z9 13
U1 0
U2 4
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 255
EP 266
DI 10.1016/j.sigpro.2003.10.009
PG 12
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600005
DA 2019-06-15
ER

PT S
AU Eichhorn, J
   Tolias, A
   Zien, A
   Kuss, M
   Rasmussen, CE
   Weston, J
   Logothetis, N
   Schlkopf, B
AF Eichhorn, J
   Tolias, A
   Zien, A
   Kuss, M
   Rasmussen, CE
   Weston, J
   Logothetis, N
   Schlkopf, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Prediction on spike data using kernel algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Eichhorn, J (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
RI Scholkopf, Bernhard/A-7570-2013
OI Scholkopf, Bernhard/0000-0002-8177-0925; Rasmussen, Carl
   Edward/0000-0001-8899-7850
CR FOLDIAK P, 1993, COMPUTATIONAL NEURAL
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Sanger TD, 1996, J NEUROPHYSIOL, V76, P2790
   Scholkopf B., 2002, LEARNING KERNELS
   SHPIGELMAN L, 2003, ADV NEURAL INFORMATI, V15
   TOLIAS AS, 2002, SOC NEUR ABST 28
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310
   WESTON J, 2003, ADV NEURAL INFORMATI, V15
   WILLIAMS CKJ, 1996, ADV NEURAL INFORMATI, V8
   Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017
NR 12
TC 13
Z9 13
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1367
EP 1374
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500170
DA 2019-06-15
ER

PT S
AU Roth, V
   Lange, T
AF Roth, V
   Lange, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Feature selection in clustering problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID DISCRIMINANT-ANALYSIS; LASSO
AB A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features.
C1 ETH, Inst Computat Sci, CH-8092 Zurich, Switzerland.
RP Roth, V (reprint author), ETH, Inst Computat Sci, Hirschengraben 84, CH-8092 Zurich, Switzerland.
RI Roth, Volker/Q-4025-2017
OI Roth, Volker/0000-0003-0991-0273
CR Ben-Dor A., 2001, P 5 ANN INT C COMP M, P31, DOI DOI 10.1145/369133.369167
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   FIGUEIREDO M, 2001, CVPR2001, P35
   HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989
   HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456
   HASTIE T, 1996, J R STAT SOC B, V58, P158
   Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806
   LANGE T, 2003, IN PRESS ADV NEURAL, V15
   LAW MH, 2003, IN PRESS ADV NEURAL, V15
   MEINECKE F, 2002, ADV NEURAL INFORMATI, V14
   Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657
   ROTH V, 2003, IN PRESS ADV NEURAL, V15
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   VONHEYDEBRECK A, 2001, BIOINFORMATICS, V17
NR 14
TC 13
Z9 13
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 473
EP 480
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500060
DA 2019-06-15
ER

PT S
AU Yosinski, J
   Clune, J
   Bengio, Y
   Lipson, H
AF Yosinski, Jason
   Clune, Jeff
   Bengio, Yoshua
   Lipson, Hod
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI How transferable are features in deep neural networks ?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Many deep neural networks trained on natural images exhibit a curious phenomenon in common on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.
C1 [Yosinski, Jason] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   [Clune, Jeff] Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USA.
   [Bengio, Yoshua] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada.
   [Lipson, Hod] Cornell Univ, Dept Mech & Aerosp Engn, Ithaca, NY 14853 USA.
RP Yosinski, J (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
FU NASA Space Technology Research Fellowship; DARPA [W911NF-12-1-0449];
   NSERC; CIFAR; Ubisoft
FX The authors would like to thank Kyunghyun Cho and Thomas Fuchs for
   helpful discussions, Joost Huizinga, Anh Nguyen, and Roby Velez for
   editing, as well as funding from the NASA Space Technology Research
   Fellowship (JY), DARPA project W911NF-12-1-0449, NSERC, Ubisoft, and
   CIFAR (YB is a CIFAR Fellow).
CR Bengio Y., 2011, JMLR W CP P AISTATS
   Bengio Y., 2011, JMLR W CP P UNS TRAN
   Caruana R., 1995, LEARNING MANY RELATE, P657
   Deng J., 2009, CVPR09
   Donahue J., 2013, ARXIV13101531
   Fei-Fei  L., 2004, P IEEE C COMP VIS PA, P178, DOI DOI 10.1109/CVPR.2004.109
   Girshick  R.B., 2013, ARXIV13112524
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V25
   Le Q. V., 2011, ADV NEURAL INFORM PR, V24, P1017
   Lee H, 2009, CONVOLUTIONAL DEEP B
   Sermanet P., 2014, INT C LEARN REPR ICL
   Zeiler M. D., 2013, ARXIV13112901
NR 14
TC 12
Z9 12
U1 11
U2 11
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101018
DA 2019-06-15
ER

PT J
AU Roberts, S
   Roussos, E
   Choudrey, R
AF Roberts, S
   Roussos, E
   Choudrey, R
TI Hierarchy, priors and wavelets: structure and signal modelling using ICA
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 2002 Conference on Neural Information Processing Systems
CY 2002
CL VANCOUVER, CANADA
DE independent component analysis; latent variable models; variational
   Bayes; wavelet analysis; unsupervised partitioning; data representation
ID INDEPENDENT COMPONENT ANALYSIS; MAXIMUM-LIKELIHOOD; SEPARATION
AB In many data analysis problems, it is useful to consider the data as generated from a set of unknown (latent) generators or sources. The observations we make of a system are then taken to be related to these sources through some unknown function. Furthermore, the (unknown) number of underlying latent sources may be less than the number of observations. Recent developments in independent component analysis (ICA) have shown that such data decomposition may be achieved in a mathematically elegant manner. In this paper, we extend the general ICA paradigm to include a very flexible source model, prior constraints and conditioning on sets of intermediate variables so that ICA forms one part of a hierarchical system. We show that such an approach allows for efficient unsupervised data partitioning and for sparse coding of signals using a hybrid wavelet-ICA model. (C) 2003 Elsevier B.V. All rights reserved.
C1 Univ Oxford, Pattern Anal & Machine Learning Res Grp, Oxford OX1 3PJ, England.
RP Roberts, S (reprint author), Univ Oxford, Pattern Anal & Machine Learning Res Grp, Parks Rd, Oxford OX1 3PJ, England.
EM stephen.roberts@some.ox.ac.uk
CR AEBERHARD S, 1994, PATTERN RECOGN, V27, P1065, DOI 10.1016/0031-3203(94)90145-7
   Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Bishop C M, 1995, NEURAL NETWORKS PATT
   Cardoso JF, 1997, IEEE SIGNAL PROC LET, V4, P112, DOI 10.1109/97.566704
   Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250
   CHOUDREY R, 2000, P NEUR NETW SIGN PRO
   Choudrey R. A., 2001, P ICA 2001 SAN DIEG
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Everson R, 1999, NEURAL COMPUT, V11, P1957, DOI 10.1162/089976699300016043
   GUROLAMI M, 2001, INDEPENDENT COMPONEN, pCH10
   Harmeling S., 2001, P INT WORKSH IND COM, P102
   Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   ILIN A, 2003, P INT WORKSH IND COM
   Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310
   Jolliffe LT, 1986, PRINCIPAL COMPONENT
   Jordan M., 1999, LEARNING GRAPHICAL M
   LAPPALAINEN H, 1999, P ICA 99 AUSS FRANC
   MACKAY DJC, MAXIMUM LIKELIHOOD C
   MACKAY DJC, 1995, P 3 ANN S NEUR NETW, P191
   MISKIN J, 2001, INDEPENDENT COMPONEN, pCH8
   Neal R., 1999, LEARNING GRAPHICAL M, P355
   PEARL J, 1988, PROBALISTIC REASONIN
   PEARLMUTTER BA, 1996, 1996 INT C NEUR INF
   Roberts S., 2001, INDEPENDENT COMPONEN
   Roberts SJ, 1998, IEE P-VIS IMAGE SIGN, V145, P149, DOI 10.1049/ip-vis:19981928
   Roberts SJ, 2001, IEEE T PATTERN ANAL, V23, P909, DOI 10.1109/34.946994
   Roberts SJ, 2000, PATTERN RECOGN, V33, P833, DOI 10.1016/S0031-3203(99)00086-2
NR 30
TC 12
Z9 12
U1 0
U2 3
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 283
EP 297
DI 10.1016/j.sigpro.2003.10.012
PG 15
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600007
DA 2019-06-15
ER

PT S
AU Raina, R
   Shen, YR
   Ng, AY
   McCallum, A
AF Raina, R
   Shen, YR
   Ng, AY
   McCallum, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Classification with hybrid generative/discriminative models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Although discriminatively trained classifiers are usually more accurate when labeled training data is abundant, previous work has shown that when training data is limited, generative classifiers can out-perform them. This paper describes a hybrid model in which a high-dimensional subset of the parameters are trained to maximize generative likelihood, and another, small, subset of parameters are discriminatively trained to maximize conditional likelihood. We give a sample complexity bound showing that in order to fit the discriminative parameters well, the number of training examples required depends only on the logarithm of the number of feature occurrences and feature set size. Experimental results show that hybrid models can provide lower test error and can produce better accuracy/coverage curves than either their purely generative or purely discriminative counterparts. We also discuss several advantages of hybrid models, and advocate further work in this area.
C1 Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Raina, R (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
CR BENNETT PN, 2003, P SIGIR 03 26 ACM IN
   DEVROYE LP, 1979, IEEE T INFORMATION T, V5
   JAAKKOLA T, 1998, ADV NEURAL INFORMATI, V11
   JEBARA T, 1998, ADV NEURAL INFORMATI, V11
   JURAFSKY D, 2000, SPEECH LANGUAGE POCE
   KEARNS M, 1997, COMPUTATIONAL LEARNI
   LAFFERTY J, 1999, IJCAI 99 WORKSH MACH
   LANG K, 1997, P 9 EUR C MACH LEARN
   LEWIS DD, 1994, P SIGIR 94 17 ACM IN
   McCallum  A., 1998, AAAI 98 WORKSH LEARN
   Ng A. Y, 2001, NIPS, V14
   PLATT J. C., 1999, ADV LARGE MARGIN CLA
   Raina R., 2003, CLASSIFICATION HYBRI
   Vapnik V. N., 1998, STAT LEARNING THEORY
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   ZADROZNY B, 2001, ICML 01
NR 16
TC 12
Z9 12
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 545
EP 552
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500069
DA 2019-06-15
ER

PT S
AU Wu, JX
   Rehg, JM
   Mullin, MD
AF Wu, JX
   Rehg, JM
   Mullin, MD
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning a rare event detection cascade by direct feature selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than non-targets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classifiers of equivalent quality. This faster method could be used for more demanding classification tasks, such as on-line learning.
C1 Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
RP Wu, JX (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM wujx@cc.gatech.edu; rehg@cc.gatech.edu; mdmullin@cc.gatech.edu
RI Wu, Jianxin/A-3700-2011; Wu, Jianxin/B-8539-2012
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   CARMICHAEL O, 2002, BRIT MACH VIS C SEPT, V1, P103
   FAN W, 2000, P 11 ECML
   HEISELE B, 2001, P CVPR, V2, P18
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   KARAKOULAS GJ, 1999, NIPS, V11, P253
   Keren D, 2001, IEEE T PATTERN ANAL, V23, P747, DOI 10.1109/34.935848
   LAZEBNIK S, 2003, P CVPR
   LEUNG TK, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P637, DOI 10.1109/ICCV.1995.466878
   LI SZ, 2002, NIPS, V15
   Lienhart R., 2002, EMPIRICAL ANAL DETEC
   Romdhani S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P695, DOI 10.1109/ICCV.2001.937694
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Schatten G, 1998, J LAW MED ETHICS, V26, P29, DOI 10.1111/j.1748-720X.1998.tb01903.x
   Schneiderman H., 2000, IEEE C COMP VIS PATT
   Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648
   Viola P, 2001, PROC CVPR IEEE, P511
   VIOLA P, 2002, NIPS, V14
   Webb A.R., 1999, STAT PATTERN RECOGNI
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
NR 20
TC 12
Z9 14
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1523
EP 1530
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500189
DA 2019-06-15
ER

PT S
AU Jaderberg, M
   Simonyan, K
   Zisserman, A
   Kavukcuoglu, K
AF Jaderberg, Max
   Simonyan, Karen
   Zisserman, Andrew
   Kavukcuoglu, Koray
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Spatial Transformer Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.
C1 [Jaderberg, Max; Simonyan, Karen; Zisserman, Andrew; Kavukcuoglu, Koray] Google DeepMind, London, England.
RP Jaderberg, M (reprint author), Google DeepMind, London, England.
EM jaderberg@google.com; simonyan@google.com; zisserman@google.com;
   korayk@google.com
CR Ba  Jimmy, 2015, ICLR
   Branson S., 2014, BMVC
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Cimpoi M., 2015, CVPR
   Cohen T. S., 2015, ICLR
   Erhan  D., 2014, CVPR
   Frey B. J., 2001, NIPS
   Gens R., 2014, NIPS
   Girshick R., 2014, CVPR
   Goodfellow I. J., 2013, ARXIV13126082
   Gregor K., 2015, ICML
   Hinton G., 2012, CORRABS12070580
   Hinton G. E., 1981, IJCAI
   Hinton G. E., 2011, ICANN
   Ioffe S., 2015, ICML
   Jaderberg M., 2014, NIPS DLW
   Kanazawa A., 2014, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lenc K., 2015, CVPR
   Lin T.-Y., 2015, ARXIV150407889
   Netzer Y., 2011, NIPS DLW
   Russakovsky O, 2014, ARXIV14090575
   Sermanet P., 2014, ARXIV14127054
   Simon M., 2015, ARXIV150408289
   Sohn K., 2012, ARXIV12066418
   Stollenga M. F., 2014, NIPS
   Tieleman T., 2014, THESIS
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Xu K, 2015, ICML
   Zhang Xiangyu, 2014, ARXIV14114229
NR 30
TC 11
Z9 11
U1 5
U2 5
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102003
DA 2019-06-15
ER

PT S
AU Rabinovich, A
   Agarwal, S
   Laris, C
   Price, JH
   Belongie, S
AF Rabinovich, A
   Agarwal, S
   Laris, C
   Price, JH
   Belongie, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Unsupervised color decomposition of histologically stained tissue
   samples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the first automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results.
C1 Univ Calif San Diego, Dept Bioengn, San Diego, CA 92103 USA.
RP Rabinovich, A (reprint author), Univ Calif San Diego, Dept Bioengn, San Diego, CA 92103 USA.
CR Bravo-Zanoguera M, 1998, REV SCI INSTRUM, V69, P3966, DOI 10.1063/1.1149207
   Cardoso J.-F., 1993, IEE P F
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hyvarinen A., 2001, INDEPENDENT COMPONEN
   Krajewski S, 1999, P NATL ACAD SCI USA, V96, P5752, DOI 10.1073/pnas.96.10.5752
   KRAJEWSKI S, 1994, AM J PATHOL, V145, P1323
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Marr D., 1983, VISION COMPUTATIONAL
   Parra L, 2000, ADV NEUR IN, V12, P942
   Price JH, 1996, CYTOMETRY, V25, P303, DOI 10.1002/(SICI)1097-0320(19961201)25:4<303::AID-CYTO1>3.0.CO;2-E
   PRICE JM, 1990, THESIS U CALIFORNIA
   Ruifrok AC, 2001, ANAL QUANT CYTOL, V23, P291
   SHI J, 1994, P IEEE C COMP VIS PA, P593, DOI DOI 10.1109/CVPR.1994.323794
   WORDINGER RJ, 1985, MANUAL IMMUNOPEROXID
NR 14
TC 11
Z9 11
U1 2
U2 6
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 667
EP 673
PG 7
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500084
DA 2019-06-15
ER

PT S
AU Schwaighofer, A
   Grigoras, M
   Tresp, V
   Hoffmann, C
AF Schwaighofer, A
   Grigoras, M
   Tresp, V
   Hoffmann, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI GPPS: A Gaussian process positioning system for cellular networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user's position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user's position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network.
C1 Siemens Corp Technol Informat & Commun, D-81730 Munich, Germany.
RP Schwaighofer, A (reprint author), Siemens Corp Technol Informat & Commun, D-81730 Munich, Germany.
CR Bahl P., 2000, MSRTR200012
   Castro P., 2001, P 3 INT C UB COMP UB
   HAMPRECHT FA, 2002, EXPT DESIGN COMBINAT
   HASHEMI H, 1993, P IEEE, V81, P943, DOI 10.1109/5.231342
   Ladd A. M., 2002, P 8 ACM INT C MOB CO
   Rasmussen C., 1996, THESIS U TORONTO
   Roos T., 2002, International Journal of Wireless Information Networks, V9, P155, DOI 10.1023/A:1016003126882
   SCHWAIGHOFER A, 2003, THESIS GRAZ U TECHNO
   Stein M. L., 1999, INTERPOLATION SPATIA
NR 9
TC 11
Z9 11
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 579
EP 586
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500073
DA 2019-06-15
ER

PT S
AU Gatys, LA
   Ecker, AS
   Bethge, M
AF Gatys, Leon A.
   Ecker, Alexander S.
   Bethge, Matthias
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Texture Synthesis Using Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID IMAGE; STATISTICS
AB Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.
C1 [Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.
   [Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias] Bernstein Ctr Computat Neurosci, Tubingen, Germany.
   [Gatys, Leon A.] Univ Tubingen, Grad Sch Neural Informat Proc, Tubingen, Germany.
   [Ecker, Alexander S.; Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Ecker, Alexander S.] Baylor Coll Med, Houston, TX 77030 USA.
RP Gatys, LA (reprint author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.
EM leon.gatys@bethgelab.org
FU German National Academic Foundation; Bernstein Center for Computational
   Neuroscience [FKZ 01GQ1002]; German Excellency Initiative through the
   Centre for Integrative Neuroscience Tubingen [EXC307]
FX This work was funded by the German National Academic Foundation
   (L.A.G.), the Bernstein Center for Computational Neuroscience (FKZ
   01GQ1002) and the German Excellency Initiative through the Centre for
   Integrative Neuroscience Tubingen (EXC307)(M.B., A.S.E, L.A.G.)
CR Balas B, 2009, J VISION, V9, DOI [10.1167/9.12.13, 10.1167/9.2.16]
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Cimpoi M, 2014, ARXIV14116836CS
   Denton E., 2014, NIPS
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341
   Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   HE K., 2014, ARXIV14064729
   Heeger D. J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P229
   Jaderberg M., 2014, BMVC
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Julesz B., 1962, IRE T INFORM THEORY, V8
   Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lebedev V., 2014, ARXIV14126553
   LeCun Yann A., 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P9, DOI 10.1007/978-3-642-35289-8_3
   Movshon A. J., 2015, COLD SPRING HARB S Q
   Okazawa G, 2015, P NATL ACAD SCI USA, V112, pE351, DOI 10.1073/pnas.1415146112
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14
   Russakovsky O., 2014, ARXIV14090575CS
   SIMONCELLI EP, 1995, IM PROC INT C, V3, P3444
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wei L.-Y., 2009, EUROGRAPHICS 2009 ST, P93
   Wei LY, 2000, COMP GRAPH, P479
   Yamins D. L. K., 2014, P NATL ACAD SCI USA
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 30
TC 10
Z9 10
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100001
DA 2019-06-15
ER

PT S
AU Hennig, MH
   Wogotter, F
AF Hennig, MH
   Wogotter, F
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Eye micro-movements improve stimulus detection beyond the Nyquist limit
   in the peripheral retina
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID RECEPTIVE-FIELDS; GANGLION-CELLS; MORPHOLOGY; VISION; MONKEY; CAT
AB Even under perfect fixation the human eye is under steady motion (tremor, microsaccades, slow drift). The "dynamic" theory of vision [ 1, 2] states that eye-movements can improve hyperacuity. According to this theory, eye movements are thought to create variable spatial excitation patterns on the photoreceptor grid, which will allow for better spatiotemporal summation at later stages. We reexamine this theory using a realistic model of the vertebrate retina by comparing responses of a resting and a moving eye. The performance of simulated ganglion cells in a hyperacuity task is evaluated by ideal observer analysis. We find that in the central retina eye-micromovements have no effect on the performance. Here optical blurring limits vernier acuity. In the retinal periphery however, eye-micromovements clearly improve performance. Based on ROC analysis, our predictions are quantitatively testable in electrophysiological and psychophysical experiments.
C1 Univ Stirling, Stirling FK9 4LR, Scotland.
RP Hennig, MH (reprint author), Univ Stirling, Stirling FK9 4LR, Scotland.
CR Averill HL, 1925, J COMP PSYCHOL, V5, P147, DOI 10.1037/h0072373
   CRONER LJ, 1995, VISION RES, V35, P7, DOI 10.1016/0042-6989(94)E0066-T
   DACEY DM, 1992, P NATL ACAD SCI USA, V89, P9666, DOI 10.1073/pnas.89.20.9666
   EIZENMAN M, 1985, VISION RES, V25, P1635
   Goodchild AK, 1996, J COMP NEUROL, V366, P55, DOI 10.1002/(SICI)1096-9861(19960226)366:1<55::AID-CNE5>3.0.CO;2-J
   Hennig MH, 2002, J NEUROSCI, V22, P8726
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Landolt O, 2001, AUTON ROBOT, V11, P233, DOI 10.1023/A:1012482822516
   Marshall W. H., 1942, BIOL S VISUAL MECHAN, P117
   PACKER O, 1992, VISION RES, V32, P1931, DOI 10.1016/0042-6989(92)90052-K
   RODIECK RW, 1965, J NEUROPHYSIOL, V28, P833, DOI 10.1152/jn.1965.28.5.833
   SCHNAPF JL, 1990, J PHYSIOL-LONDON, V427, P681, DOI 10.1113/jphysiol.1990.sp018193
   Schneeweis DM, 1999, J NEUROSCI, V19, P1203
   Sjostrand J, 1999, VISION RES, V39, P2987, DOI 10.1016/S0042-6989(99)00030-9
   Steinman R M, 1990, Rev Oculomot Res, V4, P115
   Thibos LN, 1996, VISION RES, V36, P249, DOI 10.1016/0042-6989(95)00109-D
   THIBOS LN, 1987, VISION RES, V27, P2193, DOI 10.1016/0042-6989(87)90134-9
   Watson A. B., 1986, HDB PERCEPTION HUMAN, VI
   Westheimer G., 1986, HDB PERCEPTION HUMAN, V1
NR 19
TC 10
Z9 10
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1475
EP 1482
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500183
DA 2019-06-15
ER

PT S
AU Neill, DB
   Moore, AW
AF Neill, DB
   Moore, AW
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A fast multi-resolution method for detection of significant spatial
   disease clusters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Given an N x N grid of squares, where each square has a count and an underlying population, our goal is to find the square region with the highest density, and to calculate its significance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan. statistic D-K to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N-3) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in optimal O(N-2) time, in practice resulting in significant (10-200x) speedups.
C1 Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Neill, DB (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM neill@cs.cmu.edu; awm@cs.cmu.edu
CR Agrawal R., 1998, P ACM SIGMOD INT C M, P94
   DENG K, 1995, P 12 INT JOINT C ART, P1233
   Goil S., 1999, CPDCTR9906010 NW U
   Kulldorff M, 1999, STAT IND TECHNOL, P303
   Kulldorff M, 1997, COMMUN STAT-THEOR M, V26, P1481, DOI 10.1080/03610929708831995
   KULLDORFF M, 1995, STAT MED, V14, P799, DOI 10.1002/sim.4780140809
   OPENSHAW S, 1988, LANCET, V1, P272
   Preparata F., 1985, COMPUTATIONAL GEOMET
   Samet H., 1990, DESIGN ANAL SPATIAL
   WALLER LA, 1994, WILEY S PRO, P3
   WANG W, 1997, P 23 C VER LARG DAT, P186
NR 11
TC 10
Z9 10
U1 0
U2 2
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 651
EP 658
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500082
DA 2019-06-15
ER

PT S
AU Pickup, LC
   Roberts, SJ
   Zisserman, A
AF Pickup, LC
   Roberts, SJ
   Zisserman, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A sampled texture prior for image super-resolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID RESOLUTION
AB Super-resolution aims to produce a high-resolution image from a set of one or more low-resolution images by recovering or inventing plausible high-frequency image content. Typical approaches try to reconstruct a high-resolution image using the sub-pixel displacements of several low-resolution images, usually regularized by a generic smoothness prior over the high-resolution image space. Other methods use training data to learn low-to-high-resolution matches, and have been highly successful even in the single-input-image case. Here we present a domain-specific image prior in the form of a p.d.f. based upon sampled images, and show that for certain types of super-resolution problems, this sample-based prior gives a significant improvement over other common multiple-image super-resolution techniques.
C1 Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England.
RP Pickup, LC (reprint author), Univ Oxford, Dept Engn Sci, Robot Res Grp, Parks Rd, Oxford OX1 3PJ, England.
EM elle@robots.ox.ac.uk; sjrob@robots.ox.ac.uk; az@robots.ox.ac.uk
CR Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Capel D. P., 2001, THESIS U OXFORD
   Cheeseman P, 1996, FUND THEOR, V62, P293
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   FITZGIBBON A, 2003, P INT C COMP VIS OCT
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915
   STORKEY AJ, 2003, ADV NEURAL INFORMATI, V15, P1295
   TIPPING ME, 2003, ADV NEURAL INFORMATI, V15, P1279
NR 12
TC 10
Z9 11
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1587
EP 1594
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500197
DA 2019-06-15
ER

PT S
AU Wang, Z
   Simoncelli, EP
AF Wang, Z
   Simoncelli, EP
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Local phase coherence and the perception of blur
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NATURAL SCENES; STATISTICS; ENERGY; CELLS; HYPERACUITY; INFORMATION;
   RESPONSES; SIGNALS; VISION; IMAGES
AB Humans are able to detect blurring of visual images, but the mechanism by which they do so is not clear. A traditional view is that a blurred image looks "unnatural" because of the reduction in energy (either globally or locally) at high frequencies. In this paper, we propose that the disruption of local phase can provide an alternative explanation for blur perception. We show that precisely localized features such as step edges result in strong local phase coherence structures across scale and space in the complex wavelet transform domain, and blurring causes loss of such phase coherence. We propose a technique for coarse-to-fine phase prediction of wavelet coefficients, and observe that (1) such predictions are highly effective in natural images, (2) phase coherence increases with the strength of image features, and (3) blurring disrupts the phase coherence relationship in images. We thus lay the groundwork for a new theory of perceptual blur estimation, as well as a variety of algorithms for restoration and manipulation of photographic images.
C1 NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA.
RP Wang, Z (reprint author), NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   BERGEN JR, 1988, NATURE, V333, P363, DOI 10.1038/333363a0
   Daugman J, 2001, INT J COMPUT VISION, V45, P25, DOI 10.1023/A:1012365806338
   Field DJ, 1997, VISION RES, V37, P3367, DOI 10.1016/S0042-6989(97)00181-8
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772
   FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M
   GEISLER WS, 1984, J OPT SOC AM A, V1, P775, DOI 10.1364/JOSAA.1.000775
   Graham N., 1989, VISUAL PATTERN ANAL
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P427, DOI 10.1017/S095252380001124X
   Kovesi P, 2000, PSYCHOL RES-PSYCH FO, V64, P136, DOI 10.1007/s004260000024
   KRETZMER ER, 1952, AT&T TECH J, V31, P751, DOI 10.1002/j.1538-7305.1952.tb01404.x
   MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073
   MORRONE MC, 1987, PATTERN RECOGN LETT, V6, P303, DOI 10.1016/0167-8655(87)90013-4
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   Perona P., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P52, DOI 10.1109/ICCV.1990.139492
   POLLEN DA, 1981, SCIENCE, V212, P1409, DOI 10.1126/science.7233231
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Ruderman D.L., 1994, NETWORK COMPUTATION, V5, P517, DOI DOI 10.1088/0954-898X/5/4/006
   RUDERMAN DL, 1992, NEURAL COMPUT, V4, P682, DOI 10.1162/neco.1992.4.5.682
   Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526
   Simoncelli E, 1997, P 31 AS C SIGN SYST, P673
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   TADMOR Y, 1994, VISION RES, V34, P541, DOI 10.1016/0042-6989(94)90167-8
   Thomson MGA, 1999, NETWORK-COMP NEURAL, V10, P123, DOI 10.1088/0954-898X/10/2/302
   Venkatesh S., 1989, INT C IM PROC SING, P553
   Webster MA, 2002, NAT NEUROSCI, V5, P839, DOI 10.1038/nn906
   WESTHEIMER G, 1977, VISION RES, V17, P941, DOI 10.1016/0042-6989(77)90069-4
   Wiener N., 1958, NONLINEAR PROBLEMS R
NR 30
TC 10
Z9 10
U1 1
U2 4
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1435
EP 1442
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500178
DA 2019-06-15
ER

PT S
AU Zhou, BL
   Lapedriza, A
   Xiao, JX
   Totralba, A
   Oliva, A
AF Zhou, Bolei
   Lapedriza, Agata
   Xiao, Jianxiong
   Totralba, Antonio
   Oliva, Aude
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Deep Features for Scene Recognition using Places Database
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.
C1 [Zhou, Bolei; Lapedriza, Agata; Totralba, Antonio; Oliva, Aude] MIT, Cambridge, MA 02139 USA.
   [Xiao, Jianxiong] Princeton Univ, Princeton, NJ 08544 USA.
   [Lapedriza, Agata] Univ Oberta Catalunya, Barcelona, Spain.
RP Zhou, BL (reprint author), MIT, Cambridge, MA 02139 USA.
FU National Science Foundation [1016862]; ONR MURI [N000141010933]; MIT Big
   Data Initiative at CSAIL; Google; Xerox Award; Intel; Intelligence
   Advanced Research Projects Activity (IARPA) via Air Force Research
   Laboratory [FA8650-12-C-7211];  [TIN2012-38187-C03-02]
FX Thanks to Aditya Khosla for valuable discussions. This work is supported
   by the National Science Foundation under Grant No. 1016862 to A.O, ONR
   MURI N000141010933 to A.T, as well as MIT Big Data Initiative at CSAIL,
   Google and Xerox Awards, a hardware donation from NVIDIA Corporation, to
   A.O and A.T., Intel and Google awards to J.X, and grant
   TIN2012-38187-C03-02 to A.L. This work is also supported by the
   Intelligence Advanced Research Projects Activity (IARPA) via Air Force
   Research Laboratory, contract FA8650-12-C-7211 to A.T. The U.S.
   Government is authorized to reproduce and distribute reprints for
   Governmental purposes notwithstanding any copyright annotation thereon.
   Disclaimer: The views and conclusions contained herein are those of the
   authors and should not be interpreted as necessarily representing the
   official policies or endorsements, either expressed or implied, of
   IARPA, AFRL, or the U.S. Government.
CR Agrawal P., 2014, P ECCV
   Bengio Y., FDN TRENDS MACHINE L
   Deng J., 2009, P CVPR
   Doersch C., 2013, ADV NEURAL INFORM PR
   Donahue J, 2014, DECAF DEEP CONVOLUTI
   Fan R.E., 2008, LIBLINEAR LIB LARGE
   Fei-Fei L., 2007, COMPUTER VISION IMAG
   Griffin G., 2007, CALTECH 256 OBJECT C
   Heip C., 1998, OCEANIS
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Konkle T., 2010, PSYCH SCI
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lazebnik S., 2006, P CVPR
   LeCun Y., 1989, NEURAL COMPUTATION
   Li L. J., 2007, P ICCV
   Oliva A., 2013, NEW VISUAL NEUROSCIE
   Oliva A., 2001, INT J COMPUTER VISIO
   Patterson G, 2012, P CVPR
   Quattoni A., 2009, P CVPR
   Razavian A- S., 2014, CORR, V1403, P6382
   Sanchez J. V. J., 2013, INT J COMPUTER VISIO
   Simpson EH, 1949, NATURE
   Torralba A., 2011, P CVPR
   Xiao J., 2010, P CVPR
   Yao B., 2011, P ICCV
NR 25
TC 9
Z9 9
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101020
DA 2019-06-15
ER

PT S
AU De Bie, T
   Cristianini, N
AF De Bie, T
   Cristianini, N
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Convex methods for transduction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB The 2-class transduction problem, as formulated by Vapnik [1], involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3].
   In this paper we present a relaxation of this task based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated.
C1 Katholieke Univ Leuven, SISTA, ESAT, SCD, B-3001 Heverlee, Belgium.
RP De Bie, T (reprint author), Katholieke Univ Leuven, SISTA, ESAT, SCD, Kasteelpk Arenberg 10, B-3001 Heverlee, Belgium.
RI De Bie, Tijl/B-2920-2013
OI De Bie, Tijl/0000-0002-2692-7504
CR BENNETT K, 1999, ADV NEURAL INFORMATI, V11
   CHAPELLE O, 2003, ADV NEURAL INFORMATI, V15
   CRISTIANINI N, 2003, UNPUB OPTIMIZING KER
   HELMBERG C, 2000, ZR0034 TU BERL ZIB K
   Horn R. A., 1985, MATRIX ANAL
   JOCHIMS T, 1999, P INT C MACH LEARN I
   KAMVAR S, 2003, P INT JOINT C ART IN
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Poggio T., 2001, P C UNC GEOM COMP
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Vapnik V. N., 1998, STAT LEARNING THEORY
NR 11
TC 9
Z9 10
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 73
EP 80
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500010
DA 2019-06-15
ER

PT S
AU Hughes, NP
   Tarassenko, L
   Roberts, SJ
AF Hughes, NP
   Tarassenko, L
   Roberts, SJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Markov models for automated ECG interval analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling.
C1 Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England.
RP Hughes, NP (reprint author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
EM nph@robots.ox.ac.uk; lionel@robots.ox.ac.uk; sjrob@robots.ox.ac.uk
CR Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   GRAJA S, 2003, WISP 2003 IEEE INT S, P105
   Jane R, 1997, COMPUT CARDIOL, V24, P295, DOI 10.1109/CIC.1997.647889
   Koski A, 1996, ARTIF INTELL MED, V8, P453, DOI 10.1016/S0933-3657(96)00352-1
   Levinson S. E., 1986, Computer Speech and Language, V1, P29, DOI 10.1016/S0885-2308(86)80009-2
   Mallat S., 1999, WAVELET TOUR SIGNAL
   MRUPHY KP, 2002, HIDDEN SEMIMARKOV MO
   Pratt CM, 1996, AM HEART J, V131, P472, DOI 10.1016/S0002-8703(96)90525-6
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
NR 9
TC 9
Z9 9
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 611
EP 618
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500077
DA 2019-06-15
ER

PT S
AU Kemp, C
   Griffiths, TL
   Stromsten, S
   Tenenbaum, JB
AF Kemp, C
   Griffiths, TL
   Stromsten, S
   Tenenbaum, JB
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Semi-supervised learning with trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples. We test our approach on eight real-world datasets.
C1 MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
RP Kemp, C (reprint author), MIT, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
CR BELKIN M, 2003, IN PRESS MACHINE LEA
   BLUM A, 2001, ICML, V18
   CHAPELLE O, 2003, NIPS, V15
   HAUSSLER D, 1994, MACHINE LEARNING, V14
   JOW H, 2002, MOL BIOL EVOL, V19, P1951
   KEMP C, 2003, P25 ANN C COGN SCI S
   Mitchell T. M., 1997, MACHINE LEARNING
   NEAL R, 2001, 0108 U TOR
   SHIH L, 2003, UNPUB LEARNING CLASS
   SZUMMER M, 2002, NIPS, V14
   VERT JP, 2002, BIOINFORMATICS, V1, P1
   ZHU X, 2003, ICML, V20
NR 12
TC 9
Z9 9
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 257
EP 264
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500033
DA 2019-06-15
ER

PT S
AU Murphy, K
   Torralba, A
   Freeman, WT
AF Murphy, K
   Torralba, A
   Freeman, WT
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Using the forest to see the trees: A graphical model relating features,
   objects, and scenes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID CONTEXT; SYSTEM
AB Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random field for jointly solving the tasks of object detection and scene classification.
C1 MIT, AI Lab, Cambridge, MA 02139 USA.
RP Murphy, K (reprint author), MIT, AI Lab, Cambridge, MA 02139 USA.
CR Agarwal S., 2002, P EUR C COMP VIS
   Biederman I., 1981, PERCEPTUAL ORG, P213
   Boutell M., 2003, MULTILABEL SEMANTIC
   DAVON D, 1977, COGNITIVE PSYCHOL, V9, P353
   Duygulu P., 2002, P EUR C COMP VIS
   Fergus R., 2003, P IEEE C COMP VIS PA
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   FINK M, 2003, ADV NEURAL INFO P SY
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   HARALICK RM, 1983, IEEE T PATTERN ANAL, V5, P417, DOI 10.1109/TPAMI.1983.4767411
   KUMAR S, 2003, IEEE C COMP VIS PATT
   Lafferty J., 2001, INT C MACH LEARN
   LIENHART R, 2003, DAGM 25 PATT REC S
   Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689
   PLATT J. C., 1999, ADV LARGE MARGIN CLA
   Schapire R., 2001, MSRI WORKSH NONL EST
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Singhal A., 2003, P IEEE C COMP VIS PA
   STRAT TM, 1991, IEEE T PATTERN ANAL, V13, P1050, DOI 10.1109/34.99238
   TORRALBA A, 2003, INT C COMP VIS
   Torralba A., 2001, 028 MIT AI LAB
   TORRALBA A, 2003, INT J COMPUT VISION, V53, P153
   Viola P, 2001, P IEEE C COMP VIS PA
NR 24
TC 9
Z9 9
U1 0
U2 4
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1499
EP 1506
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500186
DA 2019-06-15
ER

PT S
AU Ren, MY
   Kiros, R
   Zemel, RS
AF Ren, Mengye
   Kiros, Ryan
   Zemel, Richard S.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Exploring Models and Data for Image Question Answering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.
C1 [Ren, Mengye; Kiros, Ryan; Zemel, Richard S.] Univ Toronto, Toronto, ON, Canada.
   [Zemel, Richard S.] Canadian Inst Adv Res, Quebec City, PQ, Canada.
RP Ren, MY (reprint author), Univ Toronto, Toronto, ON, Canada.
EM mren@cs.toronto.edu; rkiros@cs.toronto.edu; zemel@cs.toronto.edu
CR Antol S., 2015, CORR
   Bird S., 2006, ACL
   Chen X, 2015, CORR, V1504, P325
   Chen X., 2014, CORR
   Chomsky N., 1973, CONDITIONS TRANSFORM
   Devlin Jacob, 2015, CORR
   Donahue J., 2014, CVPR
   Fang H, 2015, CVPR
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Frome A., 2013, NIPS
   Gao H., 2015, CORR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Karpathy A., 2013, NIPS
   Kiros R., 2015, TACL
   Klein Benjamin, 2015, CVPR
   Klein D., 2003, ACL
   Lebret R., 2015, ICML
   Lin T.-Y., 2014, ECCV
   Ma L., 2015, CORR
   Malinowski M, 2014, NIPS
   Malinowski M., 2014, NIPS WORKSH LEARN SE
   Malinowski M., 2015, CORR
   Mao J., 2014, NIPS DEEP LEARN WORK
   Mikolov T., 2013, ICLR
   Ordonez V., 2011, NIPS
   Russakovsky Olga, 2015, IJCV
   Silberman  N., 2012, ECCV
   Simonyan Karen, 2015, ICLR
   Vinyals O, 2015, CVPR
   Wu Z., 1994, ACL
   Xu K, 2015, ICML
NR 32
TC 8
Z9 8
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100008
DA 2019-06-15
ER

PT S
AU Bagnell, JA
   Kakade, S
   Ng, AY
   Schneider, J
AF Bagnell, JA
   Kakade, S
   Ng, AY
   Schneider, J
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Policy search by dynamic programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We consider the policy search approach to reinforcement learning. We show that if a "baseline distribution" is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a finite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem.
C1 Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Bagnell, JA (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
CR AMALDI E, 1998, THEORETICAL COMP SCI
   ATKESON C, 2003, NIPS, V15
   KAKADE S, 2002, P 19 INT C MACH LEAR
   Kakade Sham, 2003, THESIS U COLL LONDON
   KEARNS M, 1999, NIPS, V12
   LITTMAN M, 1994, P 3 C SIM AD BEH
   Ng A.Y., 2000, P 16 C UNC ART INT
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 8
TC 8
Z9 8
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 831
EP 838
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500104
DA 2019-06-15
ER

PT S
AU Graepel, T
   Herbrich, R
AF Graepel, T
   Herbrich, R
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Invariant pattern recognition by semidefinite programming machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Knowledge about, local invariances with respect to given pattern transformations can greatly improve the accuracy of classification. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classifiers under known transformations based on semidefinite programming. We present a new learning algorithm the Semidefinite Programming Machine (SDPM)-which is able to find a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we rise a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and find improvements over known methods.
C1 Microsoft Res Ltd, Cambridge, England.
RP Graepel, T (reprint author), Microsoft Res Ltd, Cambridge, England.
CR Chapelle O, 2002, ADV NEUR IN, V14, P609
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   GRAEPEL T, 2004, ADV NEURAL INFORMATI, V16
   NEMIROVSKI A, 2002, LECT NOTES C O R E S
   Nesterov Y., 2000, HIGH PERFORMANCE OPT, P405, DOI DOI 10.1007/978-1-4757-3216-0_17
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Scholkopf B., 1997, SUPPORT VECTOR LEARN
   SIMARD P, 1998, NEUROL NETWORKS TRIC
   Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003
NR 9
TC 8
Z9 8
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 33
EP 40
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500005
DA 2019-06-15
ER

PT S
AU Mizutani, E
   Demmel, JW
AF Mizutani, E
   Demmel, JW
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Iterative scaled trust-region learning in Krylov subspaces via
   Pearlmutter's implicit sparse Hessian-vector multiply
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that, an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classification problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated "overdetermined" nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter's implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs.
C1 Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
RP Mizutani, E (reprint author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Conn A. R., 2000, TRUST REGION METHODS
   Demmel JW, 1997, APPL NUMERICAL LINEA
   DENNIS JE, 1981, ACM T MATH SOFTWARE, V7, P3
   Hastie T., 2002, ELEMENTS STAT LEARNI
   Mizutani E, 2003, NEURAL NETWORKS, V16, P745, DOI 10.1016/S0893-6080(03)00085-6
   MIZUTANI E, 2002, P IEEE INT JOINT C N, V3, P2399
   MIZUTANI E, 2001, P INNS IEEE INT JOIN, V1, P347
   MORE JJ, 1983, SIAM J SCI STAT COMP, V4, P553, DOI 10.1137/0904038
   Noether DL, 2001, CHEM ENG NEWS, V79, P3, DOI 10.1021/cen-v079n026.p003
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Schwenk H, 2000, NEURAL COMPUT, V12, P1869, DOI 10.1162/089976600300015178
   STEIHAUG T, 1983, SIAM J NUMER ANAL, V20, P626, DOI 10.1137/0720042
NR 13
TC 8
Z9 8
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 209
EP 216
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500027
DA 2019-06-15
ER

PT S
AU Theocharous, G
   Kaelbling, LP
AF Theocharous, G
   Kaelbling, LP
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Approximate planning in POMDPs with macro-actions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MARKOV DECISION-PROCESSES
AB Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efficiently.
C1 MIT, AI Lab, Cambridge, MA 02139 USA.
RP Theocharous, G (reprint author), MIT, AI Lab, 200 Technol Sq, Cambridge, MA 02139 USA.
CR Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678
   LOVEJOY WS, 1991, OPER RES, V39, P162, DOI 10.1287/opre.39.1.162
   MADANI O, 1999, P 16 NAT C ART INT, P409
   MAHADEVAN S, 1998, MACHINE LEARNING J, V31, P239
   Ng A. Y., 1999, P 16 INT C MACH LEAR
   PAPADIMITRIOU C, 1987, MATH OPERATION RES, V12
   Pineau J., 2003, INT JOINT C ART INT
   PUTERMAN ML, 1994, MARKOV DECISION PROC
   ROY N, 2003, ADV NEURAL INFORMATI
   Russell S J, 2003, ARTIFICIAL INTELLIGE
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   ZHOU R, 2001, P 1M INT C ART INT I
NR 12
TC 8
Z9 8
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 775
EP 782
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500097
DA 2019-06-15
ER

PT S
AU Yanover, C
   Weiss, Y
AF Yanover, C
   Weiss, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Finding the M most probable configurations using loopy belief
   propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ALGORITHM
AB Loopy belief propagation (BP) has been successfully used in a number of difficult graphical models to find the most probable configuration of the hidden variables. In applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top M. While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of finding the M best configurations when exact inference is impossible.
   We start by developing a new exact inference algorithm for calculating the best configurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs, calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables.
C1 Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.
RP Yanover, C (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.
RI Yanover, Chen/A-3754-2012
OI Yanover, Chen/0000-0003-3663-4286
CR CANO A, 2000, J INTELLIGENT SYSTEM, V15, P1010
   COWELL R, 1998, LEARNING GRAPHICAL M
   Dawid A. P., 1992, Statistics and Computing, V2, P25, DOI 10.1007/BF01890546
   DECHTER R, 1997, UNCERTAINTY ARTIFIC
   DOUCET A, 2000, P UAI 2000
   FREY BJ, 2001, ADV NEURAL INFORMATI, V14
   Jaakkola TS, 1999, J ARTIF INTELL RES, V10, P291, DOI 10.1613/jair.583
   Leach AR, 1998, PROTEINS, V33, P227, DOI 10.1002/(SICI)1097-0134(19981101)33:2<227::AID-PROT7>3.0.CO;2-F
   LEVIN A, 2002, P NIPS 2002
   Murphy K. P., 2001, COMPUTING SCI STAT, V33
   Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483
   Pearl J., 1998, PROBABILISTIC REASON
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   WAINWRIGHT MJ, 2002, P2554 MIT LIDS
   WAINWRIGHT MJ, 2002, P NIPS 2002
   Weiss Y., 2001, IEEE T INFORMATION T, V47, P723
   YANOVER C, 2002, P NIPS 2002
   Yedidia J., 2003, EXPLORING ARTIFICIAL
NR 18
TC 8
Z9 8
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 289
EP 296
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500037
DA 2019-06-15
ER

PT S
AU Andrews, S
AF Andrews, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Multiple instance learning via disjunctive programming boosting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ALGORITHMS
AB Learning from ambiguous training data is highly relevant in many applications. We present a new learning algorithm for classification problems where labels are associated with sets of pattern instead of individual patterns. This encompasses multiple instance learning as a special case. Our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non-convex problem.
C1 Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
RP Andrews, S (reprint author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
CR ANDREWS S, 2003, ADV NEURAL INFORMATI, V15
   BALAS E, 1985, SIAM J ALGEBRA DISCR, V6, P466, DOI 10.1137/0606047
   DEMIREZ A, 2000, APPL ALGORITHMS COMP
   Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Gartner T., 2002, P 19 INT C MACH LEAR
   Grove AJ, 1998, P 15 NAT C ART INT
   Joachims T., 1999, P 16 INT C MACH LEAR, P200
   Lee S, 2000, COMPUT CHEM ENG, V24, P2125, DOI 10.1016/S0098-1354(00)00581-0
   MARON O, 1998, P 15 INT C MACH LEAR, P341
   RAMON J, 2000, P ICML 2000 WORKSH A
   Ratsch G, 2002, IEEE T PATTERN ANAL, V24, P1184, DOI 10.1109/TPAMI.2002.1033211
   RATSCH G, 1998, NCTR1998021 U LOND D
   ZHANG Q, 2002, ADV NEURAL INFORMATI, V14
NR 14
TC 7
Z9 7
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 65
EP 72
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500009
DA 2019-06-15
ER

PT S
AU Fischer, B
   Roth, V
   Buhmann, JM
AF Fischer, B
   Roth, V
   Buhmann, JM
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Clustering with the connectivity kernel
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Clustering aims at extracting hidden structure in dataset. While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a much harder problem. In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones. In a second step, these new objects are clustered by optimizing a compactness-based criterion. The advantages of the method over related approaches are threefold: (i) robustness properties of compactness-based criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects; (ii) the transformed distances induce a Mercer kernel which allows us to formulate a polynomial approximation scheme to the generally NP-hard clustering problem; (iii) the new method does not contain free kernel parameters in contrast to methods like spectral clustering or mean-shift clustering.
C1 Swiss Fed Inst Technol, Inst Computat Sci, CH-8092 Zurich, Switzerland.
RP Fischer, B (reprint author), Swiss Fed Inst Technol, Inst Computat Sci, CH-8092 Zurich, Switzerland.
RI Roth, Volker/Q-4025-2017; Fischer, Bernd/E-7461-2011
OI Roth, Volker/0000-0003-0991-0273; Fischer, Bernd/0000-0001-9437-2099
CR Brucker P, 1977, OPTIMIZATION OPERATI, P45
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P281, DOI 10.1109/TPAMI.2003.1177159
   DEZA M, 1994, J COMPUT APPL MATH, V55, P191, DOI 10.1016/0377-0427(94)90020-5
   Drineas P, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P291
   Duda R, 2001, PATTERN CLASSIFICATI
   Fischer B, 2003, IEEE T PATTERN ANAL, V25, P513, DOI 10.1109/TPAMI.2003.1190577
   Inaba M., 1994, 10 ANN S COMP GEOM, P332
   Jain A.K., 1988, ALGORITHMS CLUSTERIN
   NG AY, 2002, NIPS, V14, P849
   Ostrovsky R, 2002, J ACM, V49, P139, DOI 10.1145/506147.506149
   PUZICHA J, 2000, PATTERN RECOGNITION
   ROTH V, 2003, IN PRESS NIPS, V15
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Young G, 1938, PSYCHOMETRIKA, V3, P19, DOI 10.1007/BF02287916
NR 15
TC 7
Z9 8
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 89
EP 96
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500012
DA 2019-06-15
ER

PT S
AU Garcez, ASD
   Lamba, LC
AF Garcez, ASD
   Lamba, LC
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Reasoning about time and knowledge in neural-symbolic learning systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NETWORKS; LOGIC; SEMANTICS
AB We show that temporal logic and combinations of temporal logics and modal logics of knowledge can be effectively represented in artificial neural networks. We present a Translation Algorithm from temporal rules to neural networks, and show that the networks compute a fixed-point semantics of the rules. We also apply the translation to the muddy children puzzle, which has been used as a testbed for distributed multi-agent systems. We provide a complete solution to the puzzle with the use of simple neural networks, capable of reasoning about time and of knowledge acquisition through inductive learning.
C1 City Univ London, Dept Comp, London EC1V 0HB, England.
RP Garcez, ASD (reprint author), City Univ London, Dept Comp, London EC1V 0HB, England.
CR Cloete L., 2000, KNOWLEDGE BASED NEUR
   Fagin R, 1995, REASONING KNOWLEDGE
   Garcez ASA, 1999, APPL INTELL, V11, P59, DOI 10.1023/A:1008328630915
   Garcez ASD, 2001, ARTIF INTELL, V125, P155, DOI 10.1016/S0004-3702(00)00077-1
   GARCEZ ASD, 2002, PERSPECTIVES NEURAL
   GARCEZ ASD, 2002, P IEEE INT C NEUR IN, P1992
   GARCEZ ASD, 2003, IN PRESS P 16 INT FL
   GARCEZ ASD, 2002, 20026 DEP COMP IMP C
   HALPERN JY, 1989, J COMPUT SYST SCI, V38, P195, DOI 10.1016/0022-0000(89)90039-1
   HALPERN JY, 2003, IN PRESS SIAM J COMP
   Hoelldobler S., 1994, P ECAI94 WORKSH COMB, P68
   Holldobler S, 1999, APPL INTELL, V11, P45, DOI 10.1023/A:1008376514077
   HOLLDOBLER S, 1993, THESIS TH DARMSTADT
   Huth  M., 2000, LOGIC COMPUTER SCI M
   Lloyd John W., 1987, FDN LOGIC PROGRAMMIN
   PAZZANI M, 1992, MACH LEARN, V9, P57, DOI 10.1007/BF00993254
   Rao AS, 1998, J LOGIC COMPUT, V8, P293, DOI 10.1093/logcom/8.3.293
   TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   VANEMDEN MH, 1976, J ACM, V23, P733, DOI 10.1145/321978.321991
NR 20
TC 7
Z9 7
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 921
EP 928
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500115
DA 2019-06-15
ER

PT S
AU Girolami, M
   Kaban, A
AF Girolami, M
   Kaban, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Simplicial mixtures of Markov chains: Distributed modelling of dynamic
   user profiles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a linear-time distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be 'explained' by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing low-complexity and intuitively interpretable representations.
C1 Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
RP Girolami, M (reprint author), Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM girolami@dcs.gla.ac.uk; a.kaban@cs.bham.ac.uk
CR Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   CADEZ I, IN PRESS J DATA MINI
   FRYDMAN H, 1984, J AM STAT ASSOC, V79, P632, DOI 10.2307/2288410
   Girolami Mark, 2003, P 26 ANN INT ACM SIG, P433
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Lappalainen H, 2000, PERSP NEURAL COMP, P75
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Minka T., 2002, P 18 C UNC ART INT
   Ronning G., 1989, J STAT COMPUT SIM, V32, P215, DOI DOI 10.1080/00949658908811178
   ROSS DA, 2003, ADV NEURAL INFORMATI, P15
NR 10
TC 7
Z9 7
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 9
EP 16
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500002
DA 2019-06-15
ER

PT S
AU Griffiths, TL
   Tenenbaum, JB
AF Griffiths, TL
   Tenenbaum, JB
BE Thrun, S
   Saul, K
   Scholkopf, B
TI From algorithmic to subjective randomness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID AUTOMATA; JUDGMENT; GRAMMARS
AB We explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise. We present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format, and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.
C1 MIT, Cambridge, MA 02139 USA.
RP Griffiths, TL (reprint author), MIT, Cambridge, MA 02139 USA.
EM gruffydd@mit.edu; jbt@mit.edu
CR AHO AV, 1968, J ACM, V15, P647, DOI 10.1145/321479.321488
   CHAITIN GJ, 1969, J ACM, V16, P145, DOI 10.1145/321495.321506
   CHERUBINI A, 1991, THEOR COMPUT SCI, V85, P171, DOI 10.1016/0304-3975(91)90053-5
   CHOMSKY N, 1956, IRE T INFORM THEOR, V2, P113
   Falk R, 1997, PSYCHOL REV, V104, P301, DOI 10.1037//0033-295X.104.2.301
   GINSBURG S, 1967, J ACM, V14, P172, DOI 10.1145/321371.321385
   GRIFFITHS TL, 2003, P 25 ANN C COGN SCI
   KAHNEMAN D, 1972, COGNITIVE PSYCHOL, V3, P430, DOI 10.1016/0010-0285(72)90016-3
   Kolmogorov A.N., 1965, Problems of Information Transmission, V1, P1
   SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2
NR 10
TC 7
Z9 7
U1 0
U2 2
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 953
EP 960
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500119
DA 2019-06-15
ER

PT S
AU Ihler, AT
   Sudderth, EB
   Freeman, WT
   Willsky, AS
AF Ihler, AT
   Sudderth, EB
   Freeman, WT
   Willsky, AS
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Efficient multiscale sampling from products of Gaussian mixtures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The first is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter c of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating significant improvements over existing methods.
C1 MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
RP Ihler, AT (reprint author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
EM ihler@mit.edu; esuddert@mit.edu; billf@ai.mit.edu; willsky@mit.edu
CR DENG K, 1995, IJCAI
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   GRAY AG, 2003, JSM
   HINTON GE, 2000, 2000004 GATSB COMP N
   Isard M., 2003, CVPR
   Liu JS, 2000, BIOMETRIKA, V87, P353, DOI 10.1093/biomet/87.2.353
   Silverman B. W., 1986, DENSITY ESTIMATION S
   STRAIN J, 1991, SIAM J SCI STAT COMP, V12, P1131, DOI 10.1137/0912059
   SUDDERTH EB, 2003, NONPARAMETRIC BELIEF
   THRUN S, 1999, ICML, P415
NR 11
TC 7
Z9 7
U1 0
U2 3
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1
EP 8
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500001
DA 2019-06-15
ER

PT S
AU Kearns, M
   Ortiz, LE
AF Kearns, M
   Ortiz, LE
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Algorithms for interdependent security games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
C1 Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
RP Kearns, M (reprint author), Univ Penn, Dept Comp & Informat Sci, 200 S 33Rd St, Philadelphia, PA 19104 USA.
CR Garey M. R, 1979, COMPUTERS INTRACTABI
   Heal G., 2003, YOU ONLY DIE ONCE MA
   KEARNS M, 2002, P C UNC ART INT
   Kearns M. J., 2001, P 17 C UNC ART INT, P253
   KUNREUTHER H, 2003, IN PRESS J RISK UNCE
   Schelling Thomas, 1978, MICROMOTIVES MACROBE
NR 6
TC 7
Z9 7
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 561
EP 568
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500071
DA 2019-06-15
ER

PT S
AU Parkes, DC
   Singh, S
AF Parkes, DC
   Singh, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI An MDP-based approach to online mechanism design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically. Agents can choose to misrepresent their arrival and departure times, in addition to information about their value for different outcomes. We consider the problem of maximizing the total long-term value of the system despite the self-interest of agents. The online MD problem induces a Markov Decision Process (MDP), which when solved can be used to implement optimal policies in a truth-revealing Bayesian-Nash equilibrium.
C1 Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA.
RP Parkes, DC (reprint author), Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA.
RI Bi, Fan/R-9511-2017
OI Bi, Fan/0000-0003-1844-9685
CR AWERBUCH B, 2003, P ACM S THEOR COMP S
   BLUM A, 2003, P 14 ANN ACM SIAM S
   FRIEDMAN E, 2003, 4 ACM C EL COMM EC 0, P240
   Jackson M.O., 2000, ENCY LIFE SUPPORT SY
   LAVI R, 2000, P 2 ACM C EL COMM EC
   PUTERMAN ML, 1994, MARKOV DECISION PROC
NR 6
TC 7
Z9 7
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 791
EP 798
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500099
DA 2019-06-15
ER

PT S
AU Sprague, N
   Ballard, D
AF Sprague, N
   Ballard, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Eye movements for reward maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Recent eye tracking studies in natural tasks suggest that there is a tight link between eye movements and goal directed motor actions. However, most existing models of human eye movements provide a bottom up account that relates visual attention to attributes of the visual scene. The purpose of this paper is to introduce a new model of human eye movements that directly ties eye movements to the ongoing demands of behavior. The basic idea is that eye movements serve to reduce uncertainty about environmental variables that are task relevant. A value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made. If there are several candidate eye movements, the one with the highest expected value is chosen. The model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment. Simulations show our protocol is superior to a simple round robin scheduling mechanism.
C1 Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
RP Sprague, N (reprint author), Univ Rochester, Dept Comp Sci, 601 Elmwood Ave, Rochester, NY 14627 USA.
CR BALLARD D, 2002, J VISION, V2, pA568
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   Cassandra A. R., 1998, THESIS BROWN U
   Humphrys M., 1996, P 4 INT C SIM AD BEH
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Kalman RE, 1960, T ASME D, V82, P35, DOI DOI 10.1115/1.3662552
   Karlsson J., 1997, THESIS U ROCHESTER
   LAND MF, 1994, NATURE, V377
   MALONEY L, IN PRESS J VISION
   SEARA JF, 2003, P 3 IEEE RAS INT C H
   SHINODA H, 2001, VISION RES, V41
   SPRAGUE N, 2004, 829 U ROCH COMP SCI
   Sprague N., 2003, INT JOINT C ART INT
   Sutton R., 1988, REINFORCEMENT LEARNI
   SUTTON RS, 1996, ADV NEURAL INFORMATI, V8
   WAELTI P, 2001, NATURE, V412
NR 17
TC 7
Z9 7
U1 1
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1467
EP 1474
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500182
DA 2019-06-15
ER

PT S
AU Tesauro, G
AF Tesauro, G
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Extending Q-Learning to general adaptive multi-agent systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Recent multi-agent extensions of Q-Learning require knowledge of other agents' payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed "Hyper-Q" Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents' strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented.
C1 IBM Thomas J Watson Res Ctr, Hawthorne, NY 10532 USA.
RP Tesauro, G (reprint author), IBM Thomas J Watson Res Ctr, 19 Skyline Dr, Hawthorne, NY 10532 USA.
EM tesauro@watson.ibm.com
CR Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2
   Bowling M., 2000, P 17 INT C MACH LEAR, P89
   CHANG YH, 2002, P NIPS 2001
   HONG SJ, 2002, RC22393 IBM RES
   HU J, 1998, P 15 INT C MACH LEAR, P242
   Kearns M. J., 2002, P UAI 02, P259
   LITTMAN ML, 2001, P ICML 01
   LITTMAN ML, 1994, P 11 INT C MACH LEAR, P157
   MUNOS R, 1997, P INT JOINT C ART IN, P826
   Singh S. P., 2000, P 16 C UNC ART INT, P541
   Smart W.D., 2000, P 17 INT C MACH LEAR, P903
   Uther WTB, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P769
   Watkins CJCH, 1989, THESIS CAMBRIDGE U
   Weibull J., 1995, EVOLUTIONARY GAME TH
NR 14
TC 7
Z9 7
U1 2
U2 3
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 871
EP 878
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500109
DA 2019-06-15
ER

PT S
AU Vovk, V
   Shafer, G
   Nouretdinov, I
AF Vovk, V
   Shafer, G
   Nouretdinov, I
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Self-calibrating probability forecasting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB In the problem of probability forecasting the learner's goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object's label. An on-line algorithm for probability forecasting is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural non-asymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability forecasting algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for "multiprobability forecasting" (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class "Venn probability machines". Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.
C1 Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England.
RP Vovk, V (reprint author), Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England.
EM vovk@cs.rhul.ac.uk; gshafer@andromeda.rutgers.edu; ilia@cs.rhul.ac.uk
CR Berger J. O., 1987, STAT SCI, V2, P317, DOI [10.1214/ss/1177013238, DOI 10.1214/SS/1177013238]
   Dawid A. P., 1986, ENCY STATISTICAL SCI, V7, P210
   Devroye L., 1996, PROBABILISTIC THEORY
   KILINC BE, 2001, PROBABILITY THEORY P, P97
   Littlestone N., 1986, RELATING DATA COMPRE
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   SAUNDERS C, 1999, P 16 INT JOINT C ART, P722
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Vovk V, 2002, ANN IEEE SYMP FOUND, P187, DOI 10.1109/SFCS.2002.1181895
   Vovk V, 2003, LECT NOTES ARTIF INT, V2777, P358, DOI 10.1007/978-3-540-45167-9_27
   Vovk V, 2003, P 20 INT C MACH LEAR, V12, P768
   VOVK V, IN PRESS ALGORITHMIC
   VOVK V, 1999, P 16 INT C MACH LEAR, P444
   Vovk Vladimir, 2001, PROBABILITY FINANCE
NR 14
TC 7
Z9 7
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1133
EP 1140
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500141
DA 2019-06-15
ER

PT S
AU Wang, XR
   Hutchinson, R
   Mitchell, TM
AF Wang, XR
   Hutchinson, R
   Mitchell, TM
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Training fMRI classifiers to discriminate cognitive states across
   multiple subjects
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classifiers constitute "virtual sensors" of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classifiers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classifiers that can be applied across multiple human subjects, including subjects who were not involved in training the classifier. We describe the design of several machine learning approaches to training multiple-subject classifiers, and report experimental results demonstrating the success of these methods in learning cross-subject classifiers for two different fMRI data sets.
C1 Carnegie Mellon Univ, Ctr Automated Learning & Discovery, Pittsburgh, PA 15213 USA.
RP Wang, XR (reprint author), Carnegie Mellon Univ, Ctr Automated Learning & Discovery, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
CR BURGES C, 1998, J DATA MINING KNOWLE, V2, P121
   Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736
   Keller TA, 2001, ANN CONV PSYCH SOC O
   MASON R, 2003, IN PRESS J EXPT PSYC
   Mitchell T. M., 1997, MACHINE LEARNING
   MITCHELL TM, 2003, IN PRESS MACHINE LEA
   MITCHELL TM, 2003, IN PRESS AM MED INF
   PEREIRA F, 2001, PKDD 2001 FREIB GERM
   Talairach J, 1988, COPLANAR STEREOTAXIC
   Wagner AD, 1998, SCIENCE, V281, P1188, DOI 10.1126/science.281.5380.1188
   2001, NIPS 2001 BRAIN COMP
NR 11
TC 7
Z9 7
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 709
EP 716
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500089
DA 2019-06-15
ER

PT S
AU Ding, BL
   Kulkarni, J
   Yekhanin, S
AF Ding, Bolin
   Kulkarni, Janardhan
   Yekhanin, Sergey
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Collecting Telemetry Data Privately
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.
C1 [Ding, Bolin; Kulkarni, Janardhan; Yekhanin, Sergey] Microsoft Res, Redmond, WA 98052 USA.
RP Ding, BL (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM bolind@microsoft.com; jakul@microsoft.com; yekhanin@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Agrawal S, 2005, PROC INT CONF DATA, P193
   Bansal N, 2008, SIAM J COMPUT, V38, P1157, DOI 10.1137/060674417
   Bassily R., 2017, NIPS
   Bassily R., 2015, P 47 ANN ACM S THEOR, P127, DOI DOI 10.1145/2746539.2746632
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Ding Bolin, 2017, COLLECTING TELEMETRY
   Duchi J. C., 2016, ABS160402390 CORR
   DUCHI J. C., 2013, ADV NEURAL INFORM PR, P1529
   Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Erlingsson Ulfar, 2014, P 2014 ACM SIGSAC C, P1054, DOI DOI 10.1145/2660267.2660348
   Evfimievski A., 2003, P 22 ACM SIGMOD SIGA, P211, DOI DOI 10.1145/773153.773174
   Fanti Giulia, 2016, Proceedings on Privacy Enhancing Technologies, V2016, P41, DOI 10.1515/popets-2016-0015
   Goemans MX, 2002, SIAM J DISCRETE MATH, V15, P165, DOI 10.1137/S089548019936223X
   HSU J, 2012, INT C AUT LANG PROGR, V7391, P461
   Kairouz P., 2016, ICML
   Tang J., 2017, ARXIV170902753
   WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137
   Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651
NR 20
TC 6
Z9 6
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403062
DA 2019-06-15
ER

PT S
AU Gao, HY
   Mao, JH
   Zhou, J
   Huang, ZH
   Wang, L
   Xu, W
AF Gao, Haoyuan
   Mao, Junhua
   Zhou, Jie
   Huang, Zhiheng
   Wang, Lei
   Xu, Wei
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Are You Talking to a Machine? Dataset and Methods for Multilingual Image
   Question Answering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html.
C1 [Gao, Haoyuan; Zhou, Jie; Huang, Zhiheng; Wang, Lei; Xu, Wei] Baidu Res, Sunnyvale, CA 94089 USA.
   [Mao, Junhua] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
RP Gao, HY (reprint author), Baidu Res, Sunnyvale, CA 94089 USA.
EM gaohaoyuan@baidu.com; mjhustc@ucla.edu; zhoujie01@baidu.com;
   huangzhiheng@baidu.com; wanglei22@baidu.com; wei.xu@baidu.com
CR Antol S., 2015, ARXIV150500468
   Bigham J. P., 2010, P 23 ANN ACM S US IN, P333, DOI DOI 10.1145/1866029.1866080
   Chen L. C., 2015, ICLR
   Chen X, 2015, CVPR
   Cho K, 2014, ARXIV14061078
   Donahue J., 2015, CVPR
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Fang H, 2015, CVPR
   Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112
   Girshick R., 2014, CVPR
   Grubinger M., 2006, INT WORKSH ONTOIMAGE, P13
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kalchbrenner N., 2013, P 2013 C EMP METH NA, P1700
   Karpathy A., 2015, CVPR
   Kiros R., 2015, TACL
   Klein B., 2014, ARXIV14117399
   Krizhevsky A., 2012, NIPS
   Lavie A., 2007, WORKSH STAT MACH TRA, P228
   Lebret R., 2014, ARXIV14128419
   LeCun Yann A., 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P9, DOI 10.1007/978-3-642-35289-8_3
   Lin T.-Y., 2014, ARXIV14050312
   Malinowski M, 2014, ADV NEURAL INFORM PR, P1682
   Malinowski Mateusz, 2015, ARXIV150501121
   Mao J., 2015, ARXIV150406692
   Mao J, 2015, ICLR
   Mao J., 2014, NIPS DEEPLEARNING WO
   Mikolov T., 2014, ARXIV14127753
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Ren M., 2015, ARXIV150502074
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Simonyan Karen, 2015, ICLR
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Szegedy C, 2014, ARXIV14094842
   Tu K, 2014, IEEE MULTIMEDIA, V21, P42, DOI 10.1109/MMUL.2014.29
   Turing A. M., 1950, MIND, V59, P433, DOI DOI 10.1093/MIND/LIX.236.433
   Vedantam Ramakrishna, 2015, CVPR
   Vinyals O, 2015, CVPR
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Xu K, 2015, ARXIV150203044
   Young P., 2014, ACL, P479
   Zhu J., 2014, NEURAL INFORM PROCES, P1125
NR 44
TC 6
Z9 6
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100009
DA 2019-06-15
ER

PT S
AU Defazio, A
   Bach, F
   Lacoste-Julien, S
AF Defazio, Aaron
   Bach, Francis
   Lacoste-Julien, Simon
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly
   Convex Composite Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.
C1 [Defazio, Aaron] Australian Natl Univ, Ambiata, Canberra, ACT, Australia.
   [Bach, Francis; Lacoste-Julien, Simon] Ecole Normale Super, Sierra Project Team, INRIA, Paris, France.
RP Defazio, A (reprint author), Australian Natl Univ, Ambiata, Canberra, ACT, Australia.
FU MSR-Inria Joint Centre; European Research Council [239993]
FX The first author completed this work while under funding from NICTA.
   This work was partially supported by the MSR-Inria Joint Centre and a
   grant by the European Research Council (SIERRA project 239993).
CR Combettes P. L., 2011, PROXIMAL SPLITTING M
   Defazio Aaron, 2014, THESIS
   Defazio Aaron, 2014, P 31 INT C MACH LEAR
   Greensmith E, 2004, J MACH LEARN RES, V5, P1471
   Johnson R., 2013, NIPS
   Konecny J., 2013, ARXIV13121666
   Mairal Julien, 2014, TECHNICAL REPORT
   Nesterov Yu., 1998, INTRO LECT CONVEX PR
   Schmidt Mark, 2013, HAL0086005 INRIA
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev- Shwartz Shai, 2013, TECHNICAL REPORT
   Suzuki Taiji, 2014, P 31 INT C MACH LEAR
   Tseng P, 2014, J OPTIMIZ THEORY APP, V160, P832, DOI 10.1007/s10957-013-0409-2
   Xiao Lin, 2014, TECHNICAL REPORT
NR 14
TC 6
Z9 6
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100037
DA 2019-06-15
ER

PT J
AU Trainor, L
   Sonnadara, R
   Wiklund, K
   Bondy, J
   Gupta, S
   Becker, S
   Bruce, IC
   Haykin, S
AF Trainor, L
   Sonnadara, R
   Wiklund, K
   Bondy, J
   Gupta, S
   Becker, S
   Bruce, IC
   Haykin, S
TI Development of a flexible, realistic hearing in noise test environment
   (R-HINT-E)
SO SIGNAL PROCESSING
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE sound; impulse response; hearing in noise; speech perception
ID SPEECH-RECEPTION THRESHOLD; ENERGETIC MASKING; INTELLIGIBILITY;
   AMPLIFICATION; COMPRESSION; BACKGROUNDS; RATIO
AB Through the use of DSP chips and multiple microphones, hearing aids now offer the possibility of performing signal-to-noise enhancement. Evaluating different algorithms before they are instantiated on a hearing aid is essential. However, commercially available tests of hearing in noise do not allow for speech perception evaluation with a variety of signals, noise types, signal and noise locations, and reverberation. Here we present a flexible realistic hearing in noise testing environment (R-HINT-E) that involves (1) measuring the impulse responses at microphones placed in the ears of a human head and torso model (KEMAR) from many different locations in real rooms of various dimensions and with various reverberation characteristics, (2) creating a corpus of sentences based on the hearing in noise test recorded in quiet from a variety of talkers, (3) creating "soundscapes" representing the input to the cars (or array of microphones in a hearing aid) by convolving specific sentences or noises with the impulse responses for specific locations in a room, and (4) using psychophysical procedures for measuring reception thresholds for speech under a variety of noise conditions. Preliminary evaluation based on the engineering signal-to-error ratio and on human perceptual tests indicates that the convolved sounds closely match real recordings from the same location in the room. R-HINT-E should be invaluable for the evaluation of hearing aid algorithms, as well as more general signal separation algorithms such as independent components analysis. (C) 2003 Elsevier B.V. All rights reserved.
C1 McMaster Univ, Dept Psychol, Hamilton, ON L8S 4K1, Canada.
   McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON, Canada.
RP Trainor, L (reprint author), McMaster Univ, Dept Psychol, 1280 Main W St, Hamilton, ON L8S 4K1, Canada.
EM ljt@mcmaster.ca
RI Bruce, Ian/A-1232-2008
OI Bruce, Ian/0000-0002-5169-4538; Trainor, Laurel/0000-0003-3397-2079;
   Becker, Suzanna/0000-0002-2645-070X
CR Arbogast TL, 2002, J ACOUST SOC AM, V112, P2086, DOI 10.1121/1.1510141
   BECKER S, 2002, WORKSH NEUR INF COD
   Bolia RS, 2000, J ACOUST SOC AM, V107, P1065, DOI 10.1121/1.428288
   BONDY J, 2003, UNPUB SIGNAL PROCESS
   Bregman A. S., 1990, AUDITORY SCENE ANAL
   BRONKHORST AW, 1989, J ACOUST SOC AM, V86, P1374, DOI 10.1121/1.398697
   BRUCE IC, 2002, INT C HEAR AID RES L
   Brungart DS, 2001, J ACOUST SOC AM, V109, P1101, DOI 10.1121/1.1345696
   CHABRIES DM, 1995, P IEEE INT C AC SPEE, P3527
   COLOMES C, 1995, P AES C FLOR IT
   DUQUESNOY AJ, 1983, J ACOUST SOC AM, V74, P739, DOI 10.1121/1.389859
   FESTEN JM, 1990, J ACOUST SOC AM, V88, P1725, DOI 10.1121/1.400247
   Fletcher H, 1933, J ACOUST SOC AM, V5, P82, DOI 10.1121/1.1915637
   GREENBERG JE, 1993, J ACOUST SOC AM, V94, P3009, DOI 10.1121/1.407334
   Gribonval R., 2003, P 4 INT S IND COMP A, P763
   HYGGE S, 1992, J SPEECH HEAR RES, V35, P208, DOI 10.1044/jshr.3501.208
   KALIKOW DN, 1977, J ACOUST SOC AM, V61, P1337, DOI 10.1121/1.381436
   Kates JM, 1996, J ACOUST SOC AM, V99, P3138, DOI 10.1121/1.414798
   Kochkin SMV, 2000, HEAR J, V53, P34, DOI DOI 10.1097/00025572-200002000-00004
   LOCKWOOD ME, 1999, J ACOUST SOC AM, V106, pA2278
   Moore B. C. J., 1995, PERCEPTUAL CONSEQUEN
   Moore BCJ, 1999, J ACOUST SOC AM, V105, P400, DOI 10.1121/1.424571
   NILSSON MJ, 1994, J ACOUST SOC AM, V95, P1985
   PLOMP R, 1994, EAR HEARING, V15, P2, DOI 10.1097/00003446-199402000-00002
   PLOMP R, 1986, J SPEECH HEAR RES, V29, P146, DOI 10.1044/jshr.2902.146
   SCHOBBEN D, 1999, P INT WORKSH IND COM, P261
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
   WESTNER A, 1999, P 106 AUD ENG SOC AE
NR 28
TC 6
Z9 6
U1 0
U2 4
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1872-7557
J9 SIGNAL PROCESS
JI Signal Process.
PD FEB
PY 2004
VL 84
IS 2
BP 299
EP 309
DI 10.1016/j.sigpro.2003.10.013
PG 11
WC Engineering, Electrical & Electronic
SC Engineering
GA 763JW
UT WOS:000188082600008
DA 2019-06-15
ER

PT S
AU Hauskrecht, M
   Kveton, B
AF Hauskrecht, M
   Kveton, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Linear program approximations for factored continuous-state Markov
   decision processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with finite state spaces. In this work we show that ALP solutions are not limited only to MDPs with finite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors.
C1 Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.
RP Hauskrecht, M (reprint author), Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.
CR Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Bertsekas D.P., 1995, DYNAMIC PROGRAMMING
   BERTSEKAS DP, 1995, NEURAL COMPUT, V7, P270, DOI 10.1162/neco.1995.7.2.270
   CHOW CS, 1991, IEEE T AUTOMAT CONTR, V36, P898, DOI 10.1109/9.133184
   Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x
   DEFARIAS DP, 2001, UNPUB MATH OPERATION
   DEFARIAS DP, 2003, OPERATIONS RE, V51, P6
   GUESTRIN C, 2001, P 17 INT JOINT C ART, P673
   Koller D, 1999, P 16 INT JOINT C ART, P1332
   KVETON B, 2004, IN PRESS 14 INT C AU
   POUPART P, 2002, P 18 NAT C AI, P292
   PUTERMAN ML, 1994, MARKOV DECISION PROC
   Rust J, 1997, ECONOMETRICA, V65, P487, DOI 10.2307/2171751
   SCHUURMANS D, 2002, ADV NEURAL INFORMATI, V14
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   TRICK M, 1993, LINEAR PROGRAMMING A
   VANROY B, 1998, THESIS MIT
NR 17
TC 6
Z9 6
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 895
EP 902
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500112
DA 2019-06-15
ER

PT S
AU Heskes, T
   Zoeter, O
   Wiegerinck, W
AF Heskes, T
   Zoeter, O
   Wiegerinck, W
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Approximate expectation maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID PROPAGATION
AB We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach.
C1 Univ Nijmegen, SNN, NL-6525 EZ Nijmegen, Netherlands.
RP Heskes, T (reprint author), Univ Nijmegen, SNN, Geert Grooteplein 21, NL-6525 EZ Nijmegen, Netherlands.
RI Heskes, Tom/A-1443-2010
OI Heskes, Tom/0000-0002-3398-5235
CR DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Frey BJ, 2001, ADV NEUR IN, V13, P486
   HESKES T, 2003, UNCERTAINTY ARTIFICI, P313
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Minka Thomas, 2002, P 18 C UNC ART INT, P352
   Murphy K.P., 2001, UAI 01, P378
   Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   SALAKHUTDINOV R, 2003, INT C MACH LEARN, P664
   TEH Y, 2003, AISTATS 2003
   WAINWRIGHT M, 2003, AISTATS 2003
   Yedidia Jonathan S., 2002, CONSTRUCTING FREE EN
   YEH Y, 2002, NIPS, V14
   Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674
NR 14
TC 6
Z9 6
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 353
EP 360
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500045
DA 2019-06-15
ER

PT S
AU Kim, W
   Navarro, DJ
   Pitt, MA
   Myung, IJ
AF Kim, W
   Navarro, DJ
   Pitt, MA
   Myung, IJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI An MCMC-based method of comparing connectionist models in cognitive
   science
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID PROBABILITY-DISTRIBUTIONS; INFORMATION
AB Despite the popularity of connectionist models in cognitive science, their performance can often be difficult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed finds these patterns efficiently. We demonstrate the approach using two localist network models of speech perception.
C1 Ohio State Univ, Dept Psychol, Columbus, OH 43210 USA.
RP Navarro, DJ (reprint author), Ohio State Univ, Dept Psychol, 1827 Neil Ave Mall, Columbus, OH 43210 USA.
OI Navarro, Danielle/0000-0001-7648-6578
CR Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349
   Gilks WR, 1995, MARKOV CHAIN MONTE C
   MCCLELLAND JL, 1986, COGNITIVE PSYCHOL, V18, P1, DOI 10.1016/0010-0285(86)90015-0
   Myung IJ, 2000, P NATL ACAD SCI USA, V97, P11170, DOI 10.1073/pnas.170283897
   Norris D, 2000, BEHAV BRAIN SCI, V23, P299, DOI 10.1017/S0140525X00003241
   Rissanen J, 2001, IEEE T INFORM THEORY, V47, P1712, DOI 10.1109/18.930912
   Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776
NR 7
TC 6
Z9 6
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 937
EP 944
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500117
DA 2019-06-15
ER

PT S
AU Quinlan, MJ
   Chalup, SK
   Middleton, RH
AF Quinlan, MJ
   Chalup, SK
   Middleton, RH
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Application of SVMs for colour classification and collision detection
   with AIBO robots
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID SUPPORT
AB This article addresses the issues of colour classification and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classification with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse fitting for colour classification and the statistical approach used for collision detection.
C1 Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia.
RP Quinlan, MJ (reprint author), Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia.
RI Middleton, Richard H/A-2950-2013; Chalup, Stephan/O-8914-2019
OI Middleton, Richard H/0000-0001-9885-8803; Chalup,
   Stephan/0000-0002-7886-3653
CR Boser B. E., 1992, P 5 ANN WORKSH COMP, P144, DOI DOI 10.1145/130385.130401
   BUNTING J, 2003, RETURN NUBOTS 2003 N
   Chang Chih-Chung, 2001, LIBSVM LIB SUPPORT V
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   QUINLAN MJ, 2003, ROB 2003 S
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Shapiro L. G., 2001, COMPUTER VISION
   Vapnik VN, 1995, NATURE STAT LEARNING
   OPEN R SDK
NR 10
TC 6
Z9 6
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 635
EP 642
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500080
DA 2019-06-15
ER

PT S
AU Verbeek, JJ
   Roweis, ST
   Vlassis, N
AF Verbeek, JJ
   Roweis, ST
   Vlassis, N
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Non-linear CCA and PCA by alignment of local models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID DIMENSIONALITY REDUCTION
AB We propose a non-linear Canonical Correlation Analysis (CCA) method which works by coordinating or aligning mixtures of linear models. In the same way that CCA extends the idea of PCA, our work extends recent methods for non-linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed, each lying on a different high dimensional manifold. We also show that a special case of our method, when applied to only a single manifold, reduces to the Laplacian Eigenmaps algorithm. As with previous alignment schemes, once the mixture models have been estimated, all of the parameters of our model can be estimated in closed form without local optima in the learning. Experimental results illustrate the viability of the approach as a non-linear extension of CCA.
C1 Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands.
RP Verbeek, JJ (reprint author), Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands.
CR BELKIN M, 2002, ADV NEURAL INFORMATI, V14
   Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953
   BRAND M, 2003, ADV NEURAL INFORMATI, V15
   BREGLER C, 1994, ADV NEURAL INFORMATI, V6
   HAM JH, 2003, ICML 03 WORKSH CONT
   Kohonen T, 2001, SELF ORG MAPS
   Peter M, 2002, SEMIN REPROD MED, V20, P249, DOI 10.1055/s-2002-35389
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   ROWEIS ST, 2002, ADV NEURAL INFORMATI, V14
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   TEH YW, 2003, ADV NEURAL INFORMATI, V15
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
NR 12
TC 6
Z9 6
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 297
EP 304
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500038
DA 2019-06-15
ER

PT J
AU Freeman, RD
AF Freeman, RD
TI Cortical columns: A multi-parameter examination
SO CEREBRAL CORTEX
LA English
DT Article; Proceedings Paper
CT 14th Annual Neural Information Processing Systems Conference (NIPS)
CY NOV 27-DEC 02, 2000
CL DENVER, COLORADO
ID MONKEY STRIATE CORTEX; CAT VISUAL-CORTEX; ORIENTATION SELECTIVITY;
   OCULAR DOMINANCE; ORGANIZATION; DIRECTION; AREA-17
AB Columnar structure in the cerebral cortex has been demonstrated in numerous studies. However, in the visual system, it is not clear from imaging, basic physiological and anatomical approaches how multiple stimulus parameters are related within columns. We have analyzed recordings from pairs of neurons in the striate cortex of the cat using various spatial and temporal parameters. We find that most parameters are clustered within inferred columns with the exception of spatial phase. Diversity of phase could be useful for serial processing in central visual pathways.
C1 Univ Calif Berkeley, Helen Wills Neurosci Inst, Grp Vis Sci, Berkeley, CA 94720 USA.
   Univ Calif Berkeley, Helen Wills Neurosci Inst, Sch Optometry, Berkeley, CA 94720 USA.
RP Freeman, RD (reprint author), Univ Calif Berkeley, Helen Wills Neurosci Inst, Grp Vis Sci, 360 Minor Hall, Berkeley, CA 94720 USA.
FU NEI NIH HHS [EY3176, EY01175]
CR BLASDEL GG, 1992, J NEUROSCI, V12, P3139
   BLASDEL GG, 1992, J NEUROSCI, V12, P3115
   BONHOEFFER T, 1995, EUR J NEUROSCI, V7, P1973, DOI 10.1111/j.1460-9568.1995.tb00720.x
   DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1091
   DeAngelis GC, 1999, J NEUROSCI, V19, P4046
   HUBEL DH, 1974, J COMP NEUROL, V158, P267, DOI 10.1002/cne.901580304
   HUBEL DH, 1977, PHILOS T ROY SOC B, V278, P377, DOI 10.1098/rstb.1977.0050
   Hubener M, 1997, J NEUROSCI, V17, P9270
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   PAYNE BR, 1980, SCIENCE, V207, P1097, DOI 10.1126/science.7355278
   Shmuel A, 1996, J NEUROSCI, V16, P6945
   TOLHURST DJ, 1981, EXP BRAIN RES, V44, P340
NR 12
TC 6
Z9 6
U1 0
U2 0
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 1047-3211
J9 CEREB CORTEX
JI Cereb. Cortex
PD JAN
PY 2003
VL 13
IS 1
BP 70
EP 72
DI 10.1093/cercor/13.1.70
PG 3
WC Neurosciences
SC Neurosciences & Neurology
GA 622VJ
UT WOS:000179667500010
PM 12466217
OA Bronze
DA 2019-06-15
ER

PT S
AU Sabour, S
   Frosst, N
   Hinton, GE
AF Sabour, Sara
   Frosst, Nicholas
   Hinton, Geoffrey E.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dynamic Routing Between Capsules
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.
C1 [Sabour, Sara; Frosst, Nicholas; Hinton, Geoffrey E.] Google Brain, Toronto, ON, Canada.
RP Sabour, S (reprint author), Google Brain, Toronto, ON, Canada.
EM sasabour@google.com; frosst@google.com; geoffhinton@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, ARXIV160304467
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755
   Chang Jia-Ren, 2015, ARXIV151102583
   Ciresan D. C., 2011, ARXIV11020183
   Goodfellow I. J., 2013, ARXIV13126082
   Greff  K., 2016, ADV NEURAL INFORM PR, P4484
   Hinton GE, 2000, ADV NEUR IN, V12, P463
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hinton Geoffrey E, 1981, INT JOINT C ART INT, V2
   HINTON GF, 1981, P 7 INT JOINT C ART, V2, P683
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   LeCun Y., 2004, COMP VIS PATT REC 20, V2, P104
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Talgar CP, 2004, J VISION, V4, P22, DOI [10.1167/4.1.3, 10.1167/4.12.12]
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Zeiler MD, 2013, ARXIV201313013557
NR 19
TC 5
Z9 5
U1 13
U2 13
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403089
DA 2019-06-15
ER

PT S
AU Han, S
   Pool, J
   Tran, J
   Dally, WJ
AF Han, Song
   Pool, Jeff
   Tran, John
   Dally, William J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning both Weights and Connections for Efficient Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.
C1 [Han, Song; Dally, William J.] Stanford Univ, Stanford, CA 94305 USA.
   [Pool, Jeff; Tran, John; Dally, William J.] NVIDIA, Santa Clara, CA USA.
RP Han, S (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM songhan@stanford.edu; jpool@nvidia.com; johntran@nvidia.com;
   dally@stanford.edu
CR BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chen W., 2015, ARXIV150404788
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Collins M. D., 2014, ARXIV14121442
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Gong Y., 2014, ARXIV14126115
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Han S., 2015, ARXIV151000149
   HANSON SJ, 1989, ADV NEURAL INFORMATI, V1, P177
   Hassibi B., 1993, ADV NEURAL INFORMATI, V5, P164
   Horowitz M., ENERGY TABLE 45NM PR
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin Min, 2013, ARXIV14094842
   RAUSCHECKER JP, 1984, HUM NEUROBIOL, V3, P109
   Shi QF, 2009, J MACH LEARN RES, V10, P2615
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srinivas S, 2015, ARXIV150706149
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, ARXIV14094842
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA
   Walsh CA, 2013, NATURE, V502, P172, DOI 10.1038/502172a
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Yang Zichao, 2014, ARXIV14127149
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
NR 30
TC 5
Z9 5
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101044
DA 2019-06-15
ER

PT S
AU Zhang, X
   Zhao, JB
   Yann, LC
AF Zhang, Xiang
   Zhao, Junbo
   Yann Lecun
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Character-level Convolutional Networks for Text Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.
C1 [Zhang, Xiang; Zhao, Junbo; Yann Lecun] NYU, Courant Inst Math Sci, 719 Broadway,12th Floor, New York, NY 10003 USA.
RP Zhang, X (reprint author), NYU, Courant Inst Math Sci, 719 Broadway,12th Floor, New York, NY 10003 USA.
EM xiang@cs.nyu.edu; junbo.zhao@cs.nyu.edu; yann@cs.nyu.edu
FU NVIDIA Corporation; Amazon.com Inc
FX We gratefully acknowledge the support of NVIDIA Corporation with the
   donation of 2 Tesla K40 GPUs used for this research. We gratefully
   acknowledge the support of Amazon. com Inc for an AWS in Education
   Research grant used for this research.
CR Bottou L., 1989, P EUR, V2, P537
   Boureau Y.L., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dos Santos C, 2014, COLING, P69
   Fellbaum C., 2005, ENCY LANGUAGE LINGUI, P665
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Greff K., 2015, CORR
   Hinton G. E, 2012, ARXIV12070580
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P137
   Johnson R., 2014, CORR
   Kanaris I, 2007, INT J ARTIF INTELL T, V16, P1047, DOI 10.1142/S0218213007003692
   Kim Y, 2014, P 2014 C EMP METH NA, P1746, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lehmann J., 2014, SEMANTIC WEB J
   Lev G, 2015, LECT NOTES COMPUT SC, V9103, P35, DOI 10.1007/978-3-319-19581-0_3
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   McAuley J., 2013, P 7 ACM C REC SYST, V13, P165, DOI DOI 10.1145/2507157.2507163
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Polyak B. T., 1964, COMP MATH MATH PHYS, V4, P1, DOI DOI 10.1016/0041-5553(64)90137-5
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Santos C. D., 2014, P 31 INT C MACH LEAR, P1818
   Shen Yelong, 2014, P 23 ACM INT C C INF, P101, DOI DOI 10.1145/2661829.2661935
   SPARCKJONES K, 1972, J DOC, V28, P11, DOI 10.1108/eb026526
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wang C., 2008, P 17 INT C WORLD WID, P457
NR 32
TC 5
Z9 5
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101042
DA 2019-06-15
ER

PT S
AU Mahmood, AR
   Van Hasselt, H
   Sutton, RS
AF Mahmood, A. Rupam
   Van Hasselt, Hado
   Sutton, Richard S.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Weighted importance sampling for off-policy learning with linear
   function approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID COVARIATE SHIFT
AB Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, weighted importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).
C1 [Mahmood, A. Rupam; Van Hasselt, Hado; Sutton, Richard S.] Univ Alberta, Reinforcement Learning & Artificial Intelligence, Edmonton, AB T6G 1S2, Canada.
RP Mahmood, AR (reprint author), Univ Alberta, Reinforcement Learning & Artificial Intelligence, Edmonton, AB T6G 1S2, Canada.
EM ashique@cs.ualberta.ca; vanhasse@cs.ualberta.ca; sutton@cs.ualberta.ca
FU National Science and Engineering Research Council; Alberta Innovates
   Centre for Machine Learning; Alberta Innovates Technology Futures
FX This work was supported by grants from Alberta Innovates Technology
   Futures, National Science and Engineering Research Council, and Alberta
   Innovates Centre for Machine Learning.
CR ANDRADOTTIR S, 1995, OPER RES, V43, P509, DOI 10.1287/opre.43.3.509
   Bertsekas DP, 2009, J COMPUT APPL MATH, V227, P27, DOI 10.1016/j.cam.2008.07.037
   Boyan JA, 1999, MACHINE LEARNING, PROCEEDINGS, P49
   Casella G, 1998, J COMPUT GRAPH STAT, V7, P139, DOI 10.2307/1390810
   Dann C, 2014, J MACH LEARN RES, V15, P809
   Geist M, 2014, J MACH LEARN RES, V15, P289
   Hachiya H, 2012, NEUROCOMPUTING, V80, P93, DOI 10.1016/j.neucom.2011.09.016
   Hachiya H, 2009, NEURAL NETWORKS, V22, P1399, DOI 10.1016/j.neunet.2009.01.002
   Hesterberg Timothy Classen, 1988, THESIS
   KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263
   Koller D., 2009, PROBABILISTIC GRAPHI
   Liu J, 2001, MONTE CARLO STRATEGI
   Maei H., 2011, THESIS
   MAEI HR, 2010, P 3 C ART GEN INT AG, P91
   Precup D., 2001, P 18 INT C MACH LEAR
   Precup D., 2000, INT C MACH LEARN, P759
   Robert C. P., 2004, MONTE CARLO STAT MET
   RUBINSTEIN R., 1981, SIMULATION MONTE CAR
   Shelton C R, 2001, THESIS
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Sutton R. S., 2014, P 31 INT C MACH LEAR
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Yu H., 2010, P 27 INT C MACH LEAR, V27, P1207
NR 23
TC 5
Z9 5
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100028
DA 2019-06-15
ER

PT S
AU Simonyan, K
   Zisserman, A
AF Simonyan, Karen
   Zisserman, Andrew
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Two-Stream Convolutional Networks for Action Recognition in Videos
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.
   Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multiframe dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.
C1 [Simonyan, Karen; Zisserman, Andrew] Univ Oxford, Visual Geometry Grp, Oxford, England.
RP Simonyan, K (reprint author), Univ Oxford, Visual Geometry Grp, Oxford, England.
EM karen@robots.ox.ac.uk; az@robots.ox.ac.uk
FU ERC grant VisRec [228180]; NVIDIA Corporation
FX This work was supported by ERC grant VisRec no. 228180. We gratefully
   acknowledge the support of NVIDIA Corporation with the donation of the
   GPUs used for this research.
CR Berg A., 2010, LARGE SCALE VISUAL R
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25
   Chatfield  K., 2014, P BMVC
   Chen B., 2010, NIPS DEEP LEARN UNS
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Karpathy A., 2014, P CVPR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I., 2008, P CVPR
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Peng X., 2014, ABS14054506 CORR
   PENG XJ, 2014, P EUR C COMPUT VIS, V8693, P581
   Perronnin F., 2010, P ECCV
   Simonyan  Karen, 2013, NIPS
   Soomro Khurram, 2012, ABS12120402 CORR
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Ullah Muhammad Muneeb, 2009, P BRIT MACH VIS C, P1
   Wang H., 2013, ICCV WORKSH ACT REC
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Zeiler Matthew D., 2013, ABS13112901 CORR
NR 31
TC 5
Z9 5
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101024
DA 2019-06-15
ER

PT S
AU Achan, K
   Roweis, ST
   Frey, BJ
AF Achan, K
   Roweis, ST
   Frey, BJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Probabilistic inference of speech signals from phaseless spectrograms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Many techniques for complex speech processing such as denoising and deconvolution, time/frequency warping, multiple speaker separation, and multiple microphone analysis operate on sequences of short-time power spectra (spectrograrns), a representation which is often well-suited to these tasks. However, a significant problem with algorithms that manipulate spectrograms is that the output spectrogram does not include a phase component, which is needed to create a time-domain signal that has good perceptual quality. Here we describe a generative model of time-domain speech signals and their spectrograms, and show how an efficient optimizer can be used to find the maximum a posteriori speech signal, given the spectrogram. In contrast to techniques that alternate between estimating the phase and a spectrally-consistent signal, our technique directly infers the speech signal, thus jointly optimizing the phase and a spectrally-consistent signal. We compare our technique with a standard method using signal-to-noise ratios, but we also provide audio files on the web for the purpose of demonstrating the improvement in perceptual quality that our technique offers.
C1 Univ Toronto, Machine Learning Grp, Toronto, ON, Canada.
RP Achan, K (reprint author), Univ Toronto, Machine Learning Grp, Toronto, ON, Canada.
CR BESAG J, 1986, J ROY STAT SOC B MET, V48, P259
   Fletcher R., 1987, PRACTICAL METHODS OP
   GRIFFIN DW, 1984, IEEE T ACOUSTIC SPEE, V32
   Jordan M. I., 1998, LEARNING GRAPHICAL M
   KSCHISCHANG FR, 2001, IEEE T INFORMATION T, V47
   Neal R. M., 1993, PROBABILISTIC INFERE
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   Roucos S., 1985, P IEEE INT C AC SPEE, P493
   SAUL LK, 2003, ADV NEURAL INFORMATI, V15
   WAN EA, 1998, P INT C AC SPEECH PR
NR 10
TC 5
Z9 5
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1393
EP 1400
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500173
DA 2019-06-15
ER

PT S
AU Barber, D
   Agakov, F
AF Barber, D
   Agakov, F
BE Thrun, S
   Saul, K
   Scholkopf, B
TI The IM algorithm: A variational approach to information maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA.
C1 Univ Edinburgh, Inst Adapt & Neural Computac, Edinburgh EH1 2QL, Midlothian, Scotland.
RP Barber, D (reprint author), Univ Edinburgh, Inst Adapt & Neural Computac, Edinburgh EH1 2QL, Midlothian, Scotland.
CR BARBER D, 2001, ADV MEAN FIELD METHO
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   BECKER S, 1992, THESIS U TORONTO
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115
   JAAKKOLA T, 1997, P NATO ASI LEARNING
   LINSKER R, 1993, ADV NEURAL INFORMATI, V5
   MIKA S, 1999, ADV NEURAL INFORMATI, V11
   Neal R. M., 1998, LEARNING GRAPHICAL M
   SAAD D, 2001, ADV MEAN FIELD METHO
   Tanaka T, 2001, ADV NEUR IN, V13, P315
   Torkkola K., 2000, P 17 INT C MACH LEAR
   WAINWRIGHT M, 2002, UNCERTAINTY ARTIFICI
NR 13
TC 5
Z9 5
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 201
EP 208
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500026
DA 2019-06-15
ER

PT S
AU Minka, T
   Qi, Y
AF Minka, T
   Qi, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Tree-structured approximations by expectation propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a "message" to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms.
C1 Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
RP Minka, T (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
CR CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   FREY BJ, 2000, NIPS, V13
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   HESKES T, 2002, P UAI
   Jensen F. V., 1990, Computational Statistics Quarterly, V5, P269
   KAPPEN HJ, 2001, NIPS 14
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   MINKA TP, 2001, THESIS LIT
   Murphy K. P., 2001, COMPUTING SCI STAT, V33
   TEH YW, 2001, NIPS, V14
   WAINWRIGHT M, 2001, NIPS, V14
   WAINWRIGHT MJ, 2002, P UAI
   WELLING M, 2000, UAI
   WIEGERINCK W, 2000, P UAI
   YEDIDIA J, 2000, NIPS, V13
   Yedidia Jonathan S., 2002, CONSTRUCTING FREE EN
   YUILLE A, 2002, IN PRESS NEURAL COMP
NR 17
TC 5
Z9 5
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 193
EP 200
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500025
DA 2019-06-15
ER

PT S
AU Pillow, JW
   Paninski, L
   Simoncelli, EP
AF Pillow, JW
   Paninski, L
   Simoncelli, EP
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Maximum likelihood estimation of a Stochastic integrate-and-fire neural
   model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID WHITE-NOISE ANALYSIS; SPIKE; NEURONS; ADAPTATION; RESPONSES; TIME
AB Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a noisy, leaky, integrate-and-fire mechanism with a spike-dependent after-current. This model is a biophysically plausible alternative to models with Poisson (memory-less) spiking, and has been shown to effectively reproduce various spiking statistics of neurons in vivo. However, the problem of estimating the model from extracellular spike train data has not been examined in depth. We formulate the problem in terms of maximum likelihood estimation, and show that the computational problem of maximizing the likelihood is tractable. Our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum with reasonable speed. We demonstrate the effectiveness of our estimator with numerical simulations.
C1 NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10012 USA.
RP Pillow, JW (reprint author), NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10012 USA.
EM pillow@cns.nyu.edu; liam@cns.nyu.edu; eero@cns.nyu.edu
CR Arcas BAY, 2003, NEURAL COMPUT, V15, P1789, DOI 10.1162/08997660360675044
   Berry MJ, 1998, J NEUROSCI, V18, P2200
   Brown EN, 2002, NEURAL COMPUT, V14, P325, DOI 10.1162/08997660252741149
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   GENZ A, 1992, J COMPUTATIONAL GRAP, V1, P141, DOI DOI 10.2307/1390838
   Gerstner W., 2002, SPIKING NEURON MODEL
   Karlin S., 1981, 2 COURSE STOCHASTIC
   Keat J, 2001, NEURON, V30, P803, DOI 10.1016/S0896-6273(01)00322-1
   Knight BW, 2000, NEURAL COMPUT, V12, P1045, DOI 10.1162/089976600300015493
   Levin JE, 1996, NATURE, V380, P165, DOI 10.1038/380165a0
   Paninski L, 2003, NETWORK-COMP NEURAL, V14, P437, DOI 10.1088/0954-898X/14/3/304
   Paninski L, 2003, NEUROCOMPUTING, V52-4, P877, DOI 10.1016/S0925-2312(02)00819-6
   PANINSKI L, 2004, UNPUB MAXIMUM LIKELI
   Pillow JW, 2003, NEUROCOMPUTING, V52-4, P109, DOI 10.1016/S0925-2312(02)00822-6
   Reich DS, 1998, J NEUROSCI, V18, P10090
   Rudd ME, 1997, NEURAL COMPUT, V9, P1047, DOI 10.1162/neco.1997.9.5.1047
   Victor JD, 2000, BRAIN RES, V886, P33, DOI 10.1016/S0006-8993(00)02751-7
   Yu YG, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.011901
NR 18
TC 5
Z9 5
U1 0
U2 2
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1311
EP 1318
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500163
DA 2019-06-15
ER

PT S
AU Poupart, P
   Boutiller, C
AF Poupart, P
   Boutiller, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Bounded finite state controllers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MARKOV DECISION-PROCESSES
AB We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic finite state controllers, combining several advantages of gradient ascent (efficiency, search through restricted controller space) and policy iteration (less vulnerability to local optima).
C1 Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.
RP Poupart, P (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.
CR Aberdeen Douglas, 2002, P 19 INT C MACH LEAR, P3
   Boutilier C, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1168
   BRAZIUNAS D, 2003, THESIS U TORONTO TOR
   CASSANDRA A, 1997, P 13 ANN C UNC ART I, P54
   CHENG HT, 1988, THESIS U BRIT COLUMB
   FENG Z, 2001, P ECP 01 TOL SPAIN
   Hansen E. A., 1998, P 14 C UNC ART INT, P211
   Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   MEULEAU N, 1999, P 15 C UNC ART INT, P427
   Meuleau N., 1999, P 15 C UNC ART INT, P417
   PINEAU J, 2003, P IJCAI 03 AC MEX
   POUPART P, 2002, P NIPS 02 VANC CAN, P1547
   Zhang NL, 2001, J ARTIF INTELL RES, V14, P29, DOI 10.1613/jair.761
NR 14
TC 5
Z9 5
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 823
EP 830
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500103
DA 2019-06-15
ER

PT S
AU Ricks, B
   Ventura, D
AF Ricks, B
   Ventura, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Training a quantum neural network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ASSOCIATIVE MEMORY
AB Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms.
C1 Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA.
RP Ricks, B (reprint author), Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA.
CR Altaisky M. V., 2001, QUANTUM NEURAL NETWO
   Behrman E. C., 2002, QUANTUM NEURAL NETWO
   Blake C., 1998, UCI REPOSITORY MACHI
   Boyer M, 1996, P 4 WORKSH PHYS COMP, P36
   EHRMAN EC, 1996, P 4 WORKSH PHYS COMP, P22
   EZHOV A, 2000, FUTURE DIRECTIONS IN
   Ezhov AA, 2000, INFORM SCIENCES, V128, P271, DOI 10.1016/S0020-0255(00)00057-8
   FUJITA Y, 2002, QUANTUM GAUGED NEURA
   Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866
   Grover LK, 1997, PHYS REV LETT, V79, P325, DOI 10.1103/PhysRevLett.79.325
   Gupta S, 2001, J COMPUT SYST SCI, V63, P355, DOI 10.1006/jcss.2001.1769
   Jozsa R, 1998, GEOMETRIC UNIVERSE, P369
   NARAYANAN A, 2000, INFORMATION SCI, V124, P231
   Nielsen M. A., 2000, QUANTUM COMPUTATION
   SHAFEE F, 2002, NEURAL NETWORKS C NO
   Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI 10.1137/S0097539795293172
   Vedral V, 1996, PHYS REV A, V54, P147, DOI 10.1103/PhysRevA.54.147
   Vedral V, 1997, PHYS REV LETT, V78, P2275, DOI 10.1103/PhysRevLett.78.2275
   Ventura D, 2000, INFORM SCIENCES, V124, P273, DOI 10.1016/S0020-0255(99)00101-2
   ZARNDT F, 1995, THESIS BRIGHAM YOUNG
NR 20
TC 5
Z9 5
U1 1
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1019
EP 1026
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500127
DA 2019-06-15
ER

PT S
AU Samejima, K
   Doya, K
   Ueda, Y
   Kimura, M
AF Samejima, K
   Doya, K
   Ueda, Y
   Kimura, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Estimating internal variables and parameters of a learning agent by a
   particle filter
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB When we model a higher order functions, such as learning and memory, we face a difficulty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle filter for estimating internal parameters and metaparameters of a reinforcement learning model. We verified the effectiveness of the method using both artificial data and real animal behavioral data.
C1 JST, CRST, ATR Computat Neurosci Labs, Dept Computat Neurobiol, Kyoto 6190288, Japan.
RP Samejima, K (reprint author), JST, CRST, ATR Computat Neurosci Labs, Dept Computat Neurobiol, Keihan Sci City, Kyoto 6190288, Japan.
EM samejima@atr.jp; doya@atr.jp; yasu@basic.kpu-m.ac.jp;
   mkimura@basic-kpu-m.ac.jp
CR Doucet A, 2001, STAT ENG IN, P3
   ODOHERTY JP, 2003, NEURON, V28, P329
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   UEDA Y, 2002, REWARD VALUE DEPENDE
NR 5
TC 5
Z9 5
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1335
EP 1342
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500166
DA 2019-06-15
ER

PT S
AU Torresani, L
   Hertzmann, A
   Bregler, C
AF Torresani, L
   Hertzmann, A
   Bregler, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning non-rigid 3D shape from 2D motion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID MISSING DATA
AB This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly fills-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data.
C1 Stanford Univ, Stanford, CA 94305 USA.
RP Torresani, L (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ltorresa@cs.stanford.edu; hertzman@dgp.toronto.edu;
   chris.bregler@nyu.edu
CR Blake A., 1998, ACTIVE CONTOURS
   Blanz V., 1999, P 26 ANN C COMP GRAP, P187, DOI DOI 10.1145/311535.311556
   BRAND M, 2001, P CVPR 2001
   BREGLER A, 2000, P CVPR 2000
   Cootes T. E, 2001, P SPIE MED IMAGING
   Ghahramani Z, 1996, CRGTR961 U TOR
   GRUBER A, 2003, P NIPS 2003
   Jacobs DW, 2001, COMPUT VIS IMAGE UND, V82, P57, DOI 10.1006/cviu.2001.0906
   SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
   SOATTO S, 2002, P ECCV 2002 MAY
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   TORRESANI L, 2001, P CVPR
NR 13
TC 5
Z9 5
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1555
EP 1562
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500193
DA 2019-06-15
ER

PT S
AU Toussaint, M
AF Toussaint, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning a world model and planning with a self-organizing, dynamic
   neural system
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We present a connectionist architecture that can learn a model of the relations between perceptions and actions and use this model for behavior planning. State representations are learned with a growing self-organizing layer which is directly coupled to a perception and a motor layer. Knowledge about possible state transitions is encoded in the lateral connectivity. Motor signals modulate this lateral connectivity and a dynamic field on the layer organizes a planning process. All mechanisms are local and adaptation is based on Hebbian ideas. The model is continuous in the action, perception, and time domain.
C1 Ruhr Univ Bochum, Inst Neuroinformat, D-44780 Bochum, Germany.
RP Toussaint, M (reprint author), Ruhr Univ Bochum, Inst Neuroinformat, ND 04, D-44780 Bochum, Germany.
CR ABBOTT LF, 1991, NETWORK-COMP NEURAL, V2, P245, DOI 10.1088/0954-898X/2/3/002
   AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   BISHOP CM, 1997, P IEEE 5 INT C ART N
   CARPENTER GA, 1992, IEEE T NEURAL NETWOR, V3, P698, DOI 10.1109/72.159059
   Dayan P, 2001, THEORETICAL NEUROSCI
   Fritzke B., 1995, Advances in Neural Information Processing Systems 7, P625
   GRUSH R, 2003, IN PRESS BEHAV BRAIN
   Hesslow G, 2002, TRENDS COGN SCI, V6, P242, DOI 10.1016/S1364-6613(02)01913-7
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Kohonen T., 1995, SELF ORG MAPS
   KROSE B, 1994, P INT C INT ROB SYST
   MAJORS M, 1997, CUEDFINENGTR286
   MEULEAU N, 1998, MACH LEARN, V35, P117
   Phillips WA, 1997, BEHAV BRAIN SCI, V20, P657, DOI 10.1017/S0140525X9700160X
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V2, P7
   SCHMIDHUBER J, 1991, FKI14991 TU MUN
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   von der Malsburg C, 1973, Kybernetik, V14, P85
   Wiemer JC, 2003, NEURAL COMPUT, V15, P1143, DOI 10.1162/089976603765202695
   Zimmer UR, 1996, NEUROCOMPUTING, V13, P247, DOI 10.1016/0925-2312(95)00097-6
NR 21
TC 5
Z9 5
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 929
EP 936
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500116
DA 2019-06-15
ER

PT S
AU Srivastava, RK
   Greff, K
   Schmidhuber, J
AF Srivastava, Rupesh Kumar
   Greff, Klaus
   Schmidhuber, Juergen
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Training Very Deep Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.
C1 [Srivastava, Rupesh Kumar; Greff, Klaus; Schmidhuber, Juergen] USI, SUPSI, Swiss AI Lab IDSIA, Lugano, Switzerland.
RP Srivastava, RK (reprint author), USI, SUPSI, Swiss AI Lab IDSIA, Lugano, Switzerland.
EM rupesh@idsia.ch; klaus@idsia.ch; juergen@idsia.ch
FU EU project NASCENCE [FP7-ICT-317662]
FX We thank NVIDIA Corporation for their donation of GPUs and acknowledge
   funding from the EU project NASCENCE (FP7-ICT-317662). We are grateful
   to Sepp Hochreiter and Thomas Unterthiner for helpful comments and Jan
   Koutnik for help in conducting experiments.
CR Bianchini M, 2014, IEEE T NEURAL NETWOR
   Ciresan D. C., 2012, IEEE C COMP VIS PATT
   Ciresan DC, 2011, IJCAI
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Gers FA, 1999, IEE CONF PUBL, P850, DOI 10.1049/cp:19991218
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow Ian J., 2013, ARXIV13024389CSSTAT
   Graham B., 2014, ARXIV14096070
   Graves A, 2013, ARXIV13080850
   Hastad J., 1991, Computational Complexity, V1, P113, DOI 10.1007/BF01272517
   Hastad Johan, 1987, COMPUTATIONAL LIMITA
   He K., 2015, ARXIV150201852CS
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1996, FRONTIERS ARTIFICIAL, P65
   Hochreiter S., 1991, THESIS
   Jia Y., 2014, ARXIV14085093
   Kalchbrenner N., 2015, ARXIV150701526CS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lee Chen-Yu, 2015, DEEPLY SUPERVISED NE, P562
   Lin M, 2014, ARXIV13124400
   Martens James, 2014, ARXIV14117717CSSTAT
   Martin Joyce A, 2012, NCHS Data Brief, P1
   Masci Jonathan, 2015, INT C LEARN REPR
   Montufar G F, 2014, ADV NEURAL INFORM PR
   Raiko T., 2012, P INT C ART INT STAT, V22, P924
   Romero Adriana, 2014, ARXIV14126550CS
   Saxe Andrew M., 2013, ARXIV13126120CONDMAT
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Srivastava RK, 2015, ARXIV150500387CS
   Stollenga M. F., 2014, NIPS
   Sussillo David, 2014, ARXIV14126558CSSTAT
   Sutskever I., 2013, ICML, P1139
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tobias Springenberg Jost, 2014, ARXIV14126806CS
   Yu  D., 2013, ARXIV13013605
NR 38
TC 4
Z9 4
U1 4
U2 4
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101109
DA 2019-06-15
ER

PT S
AU Dosovitskiy, A
   Springenberg, JT
   Riedmiller, M
   Brox, T
AF Dosovitskiy, Alexey
   Springenberg, Jost Tobias
   Riedmiller, Martin
   Brox, Thomas
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Discriminative Unsupervised Feature Learning with Convolutional Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).
C1 [Dosovitskiy, Alexey; Springenberg, Jost Tobias; Riedmiller, Martin; Brox, Thomas] Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.
RP Dosovitskiy, A (reprint author), Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.
EM dosovits@cs.uni-freiburg.de; springj@cs.uni-freiburg.de;
   riedmiller@cs.uni-freiburg.de; brox@cs.uni-freiburg.de
FU ERC Starting Grant VideoLearn [279401]; BrainLinks-BrainTools Cluster of
   Excellence - German Research Foundation (DFG) [EXC 1086]
FX We acknowledge funding by the ERC Starting Grant VideoLearn (279401);
   the work was also partly supported by the BrainLinks-BrainTools Cluster
   of Excellence funded by the German Research Foundation (DFG, grant
   number EXC 1086).
CR AHMED A, 2008, EUR C COMP VIS, V5304, P69, DOI DOI 10.1007/978-3-540-88690-76
   AMINI MR, 2002, ECAI, V77, P390
   Bo L., 2012, ISER
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Boureau Y., 2011, ICCV 11
   Cho  K., 2013, ICML JMLR WORKSH C P
   Coates A, 2011, AISTATS
   Coates A., 2011, ADV NEURAL INFORM PR, V24, P2528
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Donahue J., 2014, ICML
   DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600
   Fei-Fei  L., 2004, CVPR WGMBV
   Fischer P., 2014, ARXIV14055769V1CSCV
   Girshick R., 2014, CVPR
   Grandvalet  Y., 2006, SEMISUPERVISED LEARN, P151
   He K., 2014, ECCV
   Hinton G. E., 2012, ARXIVCS12070580V3
   Hui K. Y., 2013, ICML
   Jia Y., 2014, ARXIV14085093
   Kavukcuoglu  K., 2010, NIPS
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lin M., 2014, ICLR
   Rifai  S., 2011, NIPS
   Simard  P., 1992, NIPS
   Sohn  K., 2012, ICML
   Swersky  Kevin, 2013, NIPS
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Wager  S., 2013, NIPS
   Zeiler  M.D., 2014, ECCV
   Zou W, 2012, NIPS, V25, P3212
NR 32
TC 4
Z9 4
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102109
DA 2019-06-15
ER

PT S
AU Tompson, J
   Jain, A
   LeCun, Y
   Bregler, C
AF Tompson, Jonathan
   Jain, Arjun
   LeCun, Yann
   Bregler, Christoph
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Joint Training of a Convolutional Network and a Graphical Model for
   Human Pose Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.
C1 [Tompson, Jonathan; Jain, Arjun; LeCun, Yann; Bregler, Christoph] NYU, New York, NY 10003 USA.
RP Tompson, J (reprint author), NYU, New York, NY 10003 USA.
EM tompson@cs.nyu.edu; ajain@cs.nyu.edu; yann@cs.nyu.edu;
   bregler@cs.nyu.edu
FU Office of Naval Research ONR [N000141210327]
FX The authors would like to thank Mykhaylo Andriluka for his support. This
   research was funded in part by the Office of Naval Research ONR Award
   N000141210327.
CR Andriluka M., 2009, CVPR
   Bergtholdt M., 2010, IJCV
   Bourdev L., 2009, ICCV
   Bourlard H., 1995, EUROSPEECH
   Buehler P., 2009, CVPR
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Dalal  N., 2005, CVPR
   Dantone M., CVPR 13
   Eichner M., 2009, BMVC
   Felzenszwalb P. F., 2008, CVPR
   Giusti A., 2013, CORR
   Gkioxari G., CVPR 13
   Grauman K., 2003, ICCV
   Heitz G., 2008, CASCADED CLASSIFICAT
   Jain A., 2014, ICLR
   Johnson S., 2010, BMVC
   Johnson S., CVPR 11
   Lowe David G, 1999, ICCV
   Mathieu M., 2013, CORR
   Mori G., 2002, ECCV
   Morin F., 2005, P 10 INT WORKSH ART
   Ning F., 2005, IEEE TIP
   Pishchulin L., ICCV 13
   Pishchulin L., CVPR 13
   Ramanan D., 2005, CVPR
   Ross S., 2011, CVPR
   Sapp B., 2013, CVPR
   Sermanet  P., 2014, ICLR
   Tompson J., 2014, TOG
   Toshev A., 2014, CVPR
   Yang Yi, CVPR 11
NR 31
TC 4
Z9 4
U1 8
U2 8
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103024
DA 2019-06-15
ER

PT J
AU Cesa-Bianchi, N
   Gentile, C
AF Cesa-Bianchi, Nicolb
   Gentile, Claudio
TI Improved risk tail bounds for on-line algorithms
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT 19th Annual Conference on Neural Information Processing Systems (NIPS
   05)
CY DEC, 2005
CL Vancouver, CANADA
DE martingales; on-line learning; risk bounds; statistical learning theory
ID CLASSIFICATION; PROBABILITIES
AB Tight bounds are derived on the risk of models in the ensemble generated by incremental training of an arbitrary learning algorithm. The result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments, and improves on previous bounds published by the same authors.
C1 [Cesa-Bianchi, Nicolb] Univ Milan, Dipartimento Sci Informaz, I-20135 Milan, Italy.
   [Gentile, Claudio] Univ Insubria, Dipartimento Informat & Comunicaz, I-21100 Varese, Italy.
RP Cesa-Bianchi, N (reprint author), Univ Milan, Dipartimento Sci Informaz, I-20135 Milan, Italy.
EM cesa-bianchi@dsi.unimi.it; claudio.gentile@uninsubria.it
RI Cesa-Bianchi, Nicolo/C-3721-2013
OI Cesa-Bianchi, Nicolo/0000-0001-8477-4748
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Devroye L., 1996, PROBABILISTIC THEORY
   FREEDMAN DA, 1975, ANN PROBAB, V3, P100, DOI 10.1214/aop/1176996452
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Littlestone N., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P269
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Vapnik V. N., 1999, NATURE STAT LEARNING
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Ying YM, 2006, IEEE T INFORM THEORY, V52, P4775, DOI 10.1109/TIT.2006.883632
   ZHANG T, 2005, P 18 ANN C LEARN THE, P173
NR 13
TC 4
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 0018-9448
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD JAN
PY 2008
VL 54
IS 1
BP 386
EP 390
DI 10.1109/TIT.2007.911292
PG 5
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 249VA
UT WOS:000252256900026
DA 2019-06-15
ER

PT J
AU McAllester, D
   Ortiz, L
AF McAllester, D
   Ortiz, L
TI Concentration inequalities for the missing mass and for histogram rule
   error
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
AB This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding's inequality, the Angluin-Valiant bound, Bernstein's inequality, Bennett's inequality, or McDiarmid's theorem. The concentration inequality for histogram rule error is motivated by the desire to construct a new class of bounds on the generalization error of decision trees.
C1 Toyota Technol Inst, Chicago, IL 60637 USA.
   Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
RP McAllester, D (reprint author), Toyota Technol Inst, 1427 E 60th St, Chicago, IL 60637 USA.
EM MCALLESTER@TTI-C.ORG; LEORTIZ@LINC.CIS.UPENN.EDU
CR ANGLUIN D, 1979, J COMPUT SYST SCI, V18, P155, DOI 10.1016/0022-0000(79)90045-X
   CHERNOFF H, 1952, ANN MATH STAT, V23, P493, DOI 10.1214/aoms/1177729330
   Church K. W., 1991, Computer Speech and Language, V5, P19, DOI 10.1016/0885-2308(91)90016-J
   Dubhashi D, 1998, RANDOM STRUCT ALGOR, V13, P99, DOI 10.1002/(SICI)1098-2418(199809)13:2<99::AID-RSA1>3.0.CO;2-M
   GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344
   Goodman J., 1998, TR1098 HARV U
   HAGERUP T, 1990, INFORM PROCESS LETT, V33, P305, DOI 10.1016/0020-0190(90)90214-I
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125
   KEARNS M, 1998, P 14 C UNC ART INT U, P311
   KEARNS M, 1998, P 15 INT C MACH LEAR
   KUTIN S, 2002, THESIS U CHICAGO
   Langford J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P209, DOI 10.1145/307400.307441
   Lugosi G, 1996, ANN STAT, V24, P687
   McAllester A. D., 2000, P 13 ANN C COMP LEAR, P1
   MCALLESTER D, 2000, P 13 ANN C COMP LEAR, P69
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   MCDIARMID C., 1998, PROBABILISTIC METHOD, V16, P195
   Ortiz LE, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P378
NR 19
TC 4
Z9 4
U1 0
U2 0
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 895
EP 911
DI 10.1162/1532443041424292
PG 17
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800007
DA 2019-06-15
ER

PT J
AU Meir, R
   Zhang, T
AF Meir, R
   Zhang, T
TI Generalization error bounds for Bayesian mixture algorithms
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
ID MODEL SELECTION; CONVERGENCE
AB Bayesian approaches to learning and estimation have played a significant role in the Statistics literature over many years. While they are often provably optimal in a frequentist setting, and lead to excellent performance in practical applications, there have not been many precise characterizations of their performance for finite sample sizes under general conditions. In this paper we consider the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space. Similarly to what is observed in practice, our results demonstrate that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects. Our results, while being data-dependent in nature, are insensitive to the underlying model assumptions, and apply whether or not these hold. At a technical level, the approach applies to unbounded functions, constrained only by certain moment conditions. Finally, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Boosting and Bagging.
C1 Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
   IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Meir, R (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM RMEIR@EE.TECHNION.AC.IL; TZHANG@WATSON.IBM.COM
CR Apostol T., 1957, MATH ANAL
   BARTLETT P, 2002, LNAI, V2375
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812
   BOUCHERON S, 2003, IN PRESS ANN PROBABI, V2
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   BOYD S, 2002, CONVEX OPTIMIZATION
   DESYATNIKOV I, 2003, UNPUB DATA DEPENDENT
   Herbrich R, 2001, ADV NEUR IN, V13, P224
   JAAKKOLA TS, 1999, ADV NEURAL INFORMATI, V11
   KOLTCHINKSII V, 2002, ANN STAT, V30
   Langford J., 2001, P 18 INT C MACH LEAR, P290
   Ledoux M., 1991, PROBABILITY BANACH S
   LEHMANN EL, 1998, THEORY POINT ESTIMAT
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   LUGOSI G, 2002, LECT NOTES ARTIF INT, V2375, P303
   MANNOR S, 2002, LECT NOTES COMPUT SC, V2375, P319
   MANNOR S, 2003, 420 CCIT DEP EL ENG
   McAleese J, 1999, AEROSPACE AM, V37, P3
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   Robert C., 2001, BAYESIAN CHOICE DECI
   Rockafellar R T, 1970, CONVEX ANAL
   Seeger Matthias, 2002, JMLR, V3, P233
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   van der Vaart A. W., 1996, WEAK CONVERGENCE EMP
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271
   Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713
   Zhang T, 2002, MACH LEARN, V46, P91, DOI 10.1023/A:1012498226479
   ZHANG T, 2003, IN PRESS ANN STAT
   ZHANG T, 2001, ADV NEURAL INFORMATI, V15
NR 32
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 839
EP 860
DI 10.1162/1532443041424300
PG 22
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800005
DA 2019-06-15
ER

PT S
AU Bondy, J
   Bruce, IC
   Becker, S
   Raykin, S
AF Bondy, J
   Bruce, IC
   Becker, S
   Raykin, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Predicting speech intelligibility from a population of neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ARTICULATION INDEX; HEARING
AB A major issue in evaluating speech enhancement and hearing compensation algorithms is to come up with a suitable metric that predicts intelligibility as judged by a human listener. Previous methods such as the widely used Speech Transmission Index (STI) fail to account for masking effects that arise from the highly nonlinear cochlear transfer function. We therefore propose a Neural Articulation Index (NAI) that estimates speech intelligibility from the instantaneous neural spike rate over time, produced when a signal is processed by an auditory neural model. By using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely matches the modeled distortion in the instantaneous spike rates of the auditory nerve. In highly rippled frequency transfer conditions the NAI's prediction error is 8% versus the STI's prediction error of 10.8%.
C1 McMaster Univ, Dept Elect Engn, Hamilton, ON, Canada.
RP Bondy, J (reprint author), McMaster Univ, Dept Elect Engn, Hamilton, ON, Canada.
RI Bruce, Ian/A-1232-2008
OI Bruce, Ian/0000-0002-5169-4538; Becker, Suzanna/0000-0002-2645-070X
CR American National Standards Institute, 1997, S351997 ANSI
   Bruce IC, 2003, J ACOUST SOC AM, V113, P369, DOI 10.1121/1.1519544
   FLETCHER H, 1950, J ACOUST SOC AM, V22, P89, DOI 10.1121/1.1906605
   FRENCH NR, 1947, J ACOUST SOC AM, V19, P90, DOI 10.1121/1.1916407
   HOUTGAST T, 1973, ACUSTICA, V28, P66
   HOUTGAST T, 1991, P EUR 91 GEN, P285
   KRYTER KD, 1962, J ACOUST SOC AM, V34, P1689, DOI 10.1121/1.1909094
   KRYTER KD, 1962, J ACOUST SOC AM, V34, P1698, DOI 10.1121/1.1909096
   Sachs MB, 2002, ANN BIOMED ENG, V30, P157, DOI 10.1114/1.1458592
   STEENEKEN HJM, 1980, J ACOUST SOC AM, V67, P318, DOI 10.1121/1.384464
   STEENEKEN HJM, 1992, THESIS U AMSTERDAM
   van Schijndel NH, 2001, J ACOUST SOC AM, V110, P529, DOI 10.1121/1.1378345
   VANSON RJJ, 2001, EUROSPEECH 200U
NR 13
TC 4
Z9 4
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1409
EP 1416
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500175
DA 2019-06-15
ER

PT S
AU Kumar, S
   Hebert, M
AF Kumar, S
   Hebert, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Discriminative fields for modeling spatial dependencies in natural
   images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID SEGMENTATION
AB In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments.
C1 Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Kumar, S (reprint author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
EM skumar@ri.cmu.edu; hebert@ri.cmu.edu
CR Cheng H, 2001, IEEE T IMAGE PROCESS, V10, P511, DOI 10.1109/83.913586
   COLLINS M, 2002, P C EMP METH NAT LAN
   Feng XJ, 2002, IEEE T PATTERN ANAL, V24, P467, DOI 10.1109/34.993555
   FIGUEIREDO MAT, 2001, ADV NEURAL INFORMATI
   GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   KOLMOGOROV V, 2002, P EUR C COMP VIS, V3, P65
   Kumar S., 2003, P IEEE INT C COMP VI
   Kumar S., 2003, IEEE INT C COMP VIS, V2, P1150
   Lafferty J., 2001, P INT C MACH LEARN
   LI S, 2001, MARKOV RANDOM FIELD
   MacKay DJC, 1996, FUND THEOR, V62, P221
   MCCULLAGH P, 1987, GENERALISED LINEAR M
   Rubinstein Y. D., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P49
   WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117
   Wilson R, 2003, IEEE T PATTERN ANAL, V25, P42, DOI 10.1109/TPAMI.2003.1159945
NR 16
TC 4
Z9 5
U1 0
U2 4
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1531
EP 1538
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500190
DA 2019-06-15
ER

PT S
AU Ralaivola, L
   d'Alche-Buc, F
AF Ralaivola, L
   d'Alche-Buc, F
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Dynamical modeling with kernels for nonlinear time series prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions.
C1 Univ Paris 06, Lab Informat Paris 6, F-75015 Paris, France.
RP Ralaivola, L (reprint author), Univ Paris 06, Lab Informat Paris 6, 8 Rue Capitaine Scott, F-75015 Paris, France.
EM liva.ralaivola@lip6.fr; florence.dalche@lip6.fr
CR BOSER B, 1992, P 5 AN WORKSH COMP L, V5
   Dorffner G., 1996, Neural Network World, V6, P447
   Ghahramani Z., 1999, ADV NEURAL INFORMATI, P599
   Julier S.J., 1997, INT S AER DEF SENS S
   Kalman RE, 1960, T ASME D, V82, P35, DOI DOI 10.1115/1.3662552
   MIKA S, 1999, NIPS
   MUKHERJEE S, 1997, P IEEE NNSP 97
   Muller K.-R., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P999
   RALAVIOLA L, 2003, ACT 19 S GRETSI TRAI
   RALAVIOLA L, 2003, THSIS U PARIS 6 FRAN
   ROSTI A, 2001, CUEDFINFENGTR420
   ROWEIS S, 1997, NEURAL COMPUT, V11, P305
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Smola A. J., 1998, NEUROCOLT2
   Vapnik V. N., 1998, STAT LEARNING THEORY
NR 15
TC 4
Z9 4
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 129
EP 136
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500017
DA 2019-06-15
ER

PT S
AU Ramanan, D
   Forsyth, DA
AF Ramanan, D
   Forsyth, DA
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Automatic annotation of everyday movements
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MOTION
AB This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view. The system does not require a fixed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed - one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate.
C1 Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
RP Ramanan, D (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
CR Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   ARIKAN O, 2003, P ACM SIGGRAPH
   Arikan O, 2002, P ACM SIGGRAPH
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Bobick AF, 1997, PHILOS T ROY SOC B, V352, P1257, DOI 10.1098/rstb.1997.0108
   CAMPBELL LW, 1995, ICCV, P624
   Chang C. - C., 2000, LIBSVM INTRO BENCHMA
   FELZENSCHWALB P, 2000, P CVPR
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   HODGINS JK, 1997, SIGGRAPH 97
   Jordan M., 1999, LEARNING GRAPHICAL M
   LEVENTON ME, 1998, TR9806 MERL
   Ramanan D., 2003, P CVPR, P1298
   RAMANAN D, 2003, UCBCSD031262
NR 14
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1547
EP 1554
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500192
DA 2019-06-15
ER

PT S
AU Rosenberg, C
   Minka, T
   Ladsariya, A
AF Rosenberg, C
   Minka, T
   Ladsariya, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Bayesian color constancy with non-Gaussian models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We present a Bayesian approach to color constancy which utilizes a non-Gaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reflectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.
C1 Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Rosenberg, C (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
CR Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P985, DOI 10.1109/TIP.2002.802529
   Barnard K, 2002, COLOR RES APPL, V27, P147, DOI 10.1002/col.10049
   Barnard K, 2000, P 6 EUR C COMP VIS, P275
   Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393
   BUCHSBAUM G, 1980, J FRANKLIN I, V10, P1
   Finlayson G. D., 1999, P 7 IEEE INT C COMP, V2, P835
   FUNT B, 1996, P IS T SID 4 COL IM, P58
   FUNT B, 1999, P SPIE ELECT IMAGING, V4, P3644
   Funt B., 1998, P 5 EUR C COMP VIS, P445
   TRUSSELL HJ, 1991, P INT C AC SPEECH SI, P2513
NR 10
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1595
EP 1602
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500198
DA 2019-06-15
ER

PT S
AU Smola, AJ
   Vishwanathan, SVN
   Eskin, E
AF Smola, AJ
   Vishwanathan, SVN
   Eskin, E
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Laplace propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka's Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases.
C1 Australian Natl Univ, Machine Learning Grp, Canberra, ACT 0200, Australia.
RP Smola, AJ (reprint author), Australian Natl Univ, Machine Learning Grp, Canberra, ACT 0200, Australia.
EM smola@axiom.anu.edu.au; vishy@axiom.anu.edu.au; eeskin@cs.huji.ac.il
CR Blake C., 1998, UCI REPOSITORY MACHI
   COLLOBERT R, 2002, ADV NEURAL INFORMATI
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Joachims Thorsten, 1999, ADV KERNEL METHODS S, P169
   Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105
   Minka T. P., 2001, THESIS MIT MEDIA LAB
   Pearl Judea, 1984, HEURISTICS INTELLIGE
   Platt J., 1998, MSRTR9814
   Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908
NR 9
TC 4
Z9 4
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 441
EP 448
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500056
DA 2019-06-15
ER

PT S
AU Steinwart, I
AF Steinwart, I
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Sparseness of support vector machines - Some asymptotically sharp bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NETWORKS
AB The decision functions constructed by support vector machines (SVM's) usually depend only on a subset of the training set-the so-called support vectors. We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM's. In particular, we show for the Gaussian RBF kernel that the fraction of support vectors tends to twice the Bayes risk for the L1-SVM, to the probability of noise for the L2-SVM, and to 1 for the LS-SVM.
C1 Los Alamos Natl Lab, Modeling Algorithms & Informat Grp, Los Alamos, NM 87545 USA.
RP Steinwart, I (reprint author), Los Alamos Natl Lab, Modeling Algorithms & Informat Grp, CCS-3,Mail Stop B256, Los Alamos, NM 87545 USA.
CR ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Devroye  L., 1997, PROBABILISTIC THEORY
   GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219
   Kowalczyk A, 2001, ADV NEUR IN, V13, P252
   Range R. M., 1986, HOLOMORPHIC FUNCTION
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Steinwart I., 2001, J MACHINE LEARNING R, V2, P67
   STEINWART I, IN PRESS IEEE T INFO
   STEINWART I, 2003, J MACHINE LEARNING R, V4, P1071
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
NR 12
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1069
EP 1076
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500133
DA 2019-06-15
ER

PT S
AU Thomas, PJ
   Spencer, DJ
   Hampton, SK
   Park, P
   Zurkus, JP
AF Thomas, PJ
   Spencer, DJ
   Hampton, SK
   Park, P
   Zurkus, JP
BE Thrun, S
   Saul, K
   Scholkopf, B
TI The diffusion mediated biochemical signal relay channel
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID INFORMATION CAPACITY
AB Biochemical signal-transduction networks are the biological information-processing systems by which individual cells, from neurons to amoebae, perceive and respond to their chemical environments. We introduce a simplified model of a single biochemical relay and analyse its capacity as a communications channel. A diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell. This receptor-ligand interaction creates a nonlinear communications channel with non-Gaussian noise. We model this channel numerically and study its response to input signals of different frequencies in order to estimate its channel capacity. Stochastic effects introduced in both the diffusion process and the receptor-ligand interaction give the channel low-pass characteristics. We estimate the channel capacity using a water-filling formula adapted from the additive white-noise Gaussian channel.
C1 Salk Inst Biol Studies, Computat Neurobiol Lab, La Jolla, CA 92037 USA.
RP Thomas, PJ (reprint author), Salk Inst Biol Studies, Computat Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.
CR COVER TM, 1991, ELEMENTS INFORMATION
   Detwiler PB, 2000, BIOPHYS J, V79, P2801, DOI 10.1016/S0006-3495(00)76519-2
   FREY MR, 1991, IEEE T INFORM THEORY, V37, P244, DOI 10.1109/18.75239
   Getz WM, 2001, CHEM SENSES, V26, P95, DOI 10.1093/chemse/26.2.95
   Mitra PP, 2001, NATURE, V411, P1027, DOI 10.1038/35082518
   Rappel WJ, 2002, BIOPHYS J, V83, P1361, DOI 10.1016/S0006-3495(02)73906-4
   Ueda M, 2001, SCIENCE, V294, P864, DOI 10.1126/science.1063951
   Uteshev VV, 1997, BIOPHYS J, V72, P1127, DOI 10.1016/S0006-3495(97)78761-7
NR 8
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1263
EP 1270
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500157
DA 2019-06-15
ER

PT S
AU Weston, J
   Leslie, C
AF Weston, J
   Leslie, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Semi-supervised protein classification using cluster kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID SEQUENCES; DATABASE
AB A key issue in supervised protein classification is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data - examples with known 3D structures, organized into structural classes - while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efficiency.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Weston, J (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
CR Altschul SF, 1997, NUCLEIC ACIDS RES, V25, P3389, DOI 10.1093/nar/25.17.3389
   ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1016/S0022-2836(05)80360-2
   CHAPELLE O, 2002, NEURAL INFORMATION P, V15
   JAAKKOLA T, 2000, J COMPUTATIONAL BIOL
   JOACHIMS T, 1999, P ICML
   KROGH A, 1994, J MOL BIOL, V235, P1501, DOI 10.1006/jmbi.1994.1104
   LESLIE C, 2002, NEURAL INFORMATION P, V15
   LIAO C, 2002, P RECOMB
   MURZIN AG, 1995, J MOL BIOL, V247, P536, DOI 10.1006/jmbi.1995.0159
   NG A, 2001, NEURAL PROCESSING IN, V14
   Park J, 1998, J MOL BIOL, V284, P1201, DOI 10.1006/jmbi.1998.2221
   SEEGER M, 2001, LEARNING LABELED UNL
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   SZUMMER M, 2001, NEURAL INFORMATION P, V14
   Zhu  X., 2002, LEARNING LABELED UNL
NR 15
TC 4
Z9 4
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 595
EP 602
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500075
DA 2019-06-15
ER

PT S
AU Dai, JF
   Li, Y
   He, KM
   Sun, J
AF Dai, Jifeng
   Li, Yi
   He, Kaiming
   Sun, Jian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI R-FCN: Object Detection via Region-based Fully Convolutional Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [7, 19] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [10], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.
C1 [Dai, Jifeng; Li, Yi; He, Kaiming; Sun, Jian] Microsoft Res, Redmond, WA 98052 USA.
   [Li, Yi] Tsinghua Univ, Beijing, Peoples R China.
RP Dai, JF (reprint author), Microsoft Res, Redmond, WA 98052 USA.
CR Bell Sean, 2016, CVPR
   Chen L. C., 2015, ICLR
   Dai J., 2016, ARXIV160308678
   Erhan  D., 2014, CVPR
   Everingham M., 2010, IJCV
   Gidaris S., 2015, ICCV
   Girshick R., 2015, ICCV
   Girshick R., 2014, CVPR
   He K., 2014, ECCV
   He K., 2016, CVPR
   Krizhevsky A., 2012, NIPS
   LeCun Y., 1989, NEURAL COMPUTATION
   Lenc K., 2015, BMVC
   Lin T.-Y., 2014, ECCV
   Liu W., 2015, ARXIV151202325V2
   Long  J., 2015, CVPR
   Mallat S., 1999, WAVELET TOUR SIGNAL
   Redmon J., 2016, CVPR
   Ren S., 2015, ARXIV150406066
   Ren S., 2015, NIPS
   Russakovsky Olga, 2015, IJCV
   Sermanet  P., 2014, ICLR
   Shrivastava A., 2016, CVPR
   Simonyan Karen, 2015, ICLR
   Szegedy C., 2016, CVPR
   Szegedy C., 2015, CVPR
   Szegedy C., 2013, NIPS
   Uijlings J., 2013, IJCV
   Zitnick C. L., 2014, ECCV
NR 29
TC 3
Z9 3
U1 15
U2 15
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703101
DA 2019-06-15
ER

PT S
AU Novikov, A
   Podoprikhin, D
   Osokin, A
   Vetrov, D
AF Novikov, Alexander
   Podoprikhin, Dmitry
   Osokin, Anton
   Vetrov, Dmitry
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Tensorizing Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.
C1 [Novikov, Alexander; Podoprikhin, Dmitry; Vetrov, Dmitry] Skolkovo Inst Sci & Technol, Moscow, Russia.
   [Osokin, Anton] INRIA, SIERRA Project Team, Paris, France.
   [Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   [Novikov, Alexander] Russian Acad Sci, Inst Numer Math, Moscow, Russia.
RP Novikov, A (reprint author), Skolkovo Inst Sci & Technol, Moscow, Russia.
EM novikov@bayesgroup.ru; podoprikhin.dmitry@gmail.com;
   anton.osokin@inria.fr; vetrovd@yandex.ru
RI Osokin, Anton/D-7398-2012
OI Osokin, Anton/0000-0002-8807-5132
FU RFBR [15-31-20596 (mol-a-ved)]; Microsoft: Moscow State University Joint
   Research Center [RPD 1053945]; MSR-INRIA Joint Center; Russian Science
   Foundation [14-11-00659]
FX We would like to thank Ivan Oseledets for valuable discussions. A.
   Novikov, D. Podoprikhin, D. Vetrov were supported by RFBR project No.
   15-31-20596 (mol-a-ved) and by Microsoft: Moscow State University Joint
   Research Center (RPD 1053945). A. Osokin was supported by the MSR-INRIA
   Joint Center. The results of the tensor toolbox application (in Sec. 6)
   are supported by Russian Science Foundation No. 14-11-00659.
CR Asanovi K., 1991, TECH REP
   Ba J, 2014, ADV NEURAL INFORM PR, V1, P2654
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chen  W., 2015, INT C MACH LEARN, P2285
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Gilboa E., 2012, 12094120 ARXIV
   Gong Y., 2014, 14126115 ARXIV
   Goodfellow Ian, 2013, JMLR W CP, P1319
   Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebedev  Vadim, 2014, INT C LEARN REPR ICL
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Novikov A., 2014, INT C MACH LEARN, P811
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Simonyan K, 2015, INT C LEARN REPR ICL
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vedaldi A., P ACM INT C MULT
   Xue J, 2013, INTERSPEECH, P2364
   Yang Z., 2014, 14127149 ARXIV
   Zhang Z., 2014, CLOUD BASED DESIGN M, P63
NR 27
TC 3
Z9 3
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101047
DA 2019-06-15
ER

PT S
AU Sukhbaatar, S
   Szlam, A
   Weston, J
   Fergus, R
AF Sukhbaatar, Sainbayar
   Szlam, Arthur
   Weston, Jason
   Fergus, Rob
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI End-To-End Memory Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.
C1 [Sukhbaatar, Sainbayar] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
   [Szlam, Arthur; Weston, Jason; Fergus, Rob] Facebook AI Res, New York, NY USA.
RP Sukhbaatar, S (reprint author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.
EM sainbar@cs.nyu.edu; aszlam@fb.com; jase@fb.com; robfergus@fb.com
CR Atkeson CG, 1995, NEUROCOMPUTING, V9, P243, DOI 10.1016/0925-2312(95)00033-6
   Bahdanau  D., 2015, INT C LEARN REPR ICL
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Chung J., 2014, CORR
   Das S., 1992, P 14 ANN C COGN SCI
   Goodman J., 2001, CORR
   Graves A., 2014, ARXIV14105401
   Graves A., 2013, 13080850 ARXIV
   Gregor Karol, 2015, CORR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Joulin  Armand, 2015, NIPS
   Koutnik J., 2014, ICML
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Mikolov T., 2014, 14127753 ARXIV
   Mikolov T., 2012, THESIS
   Mozer M. C., 1993, NIPS, P863
   Peng B., 2015, 150805508 ARXIV
   POLLACK JB, 1991, MACH LEARN, V7, P227, DOI 10.1007/BF00114845
   Steinbuch K., 1963, IEEE Transactions on Electronic Computers, VEC-12, P846, DOI 10.1109/PGEC.1963.263565
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Taylor W.K., 1959, Proceedings of the Institution of Electrical Engineers. B. Radio and Electronic Engineering, V106, P198
   Weston J., 2015, 150205698 ARXIV
   Weston J., 2015, INT C LEARN REPR ICL
   Xu K., 2015, 150203044 ARXIV
   Zaremba W, 2014, ARXIV14092329
NR 25
TC 3
Z9 3
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101105
DA 2019-06-15
ER

PT S
AU Dai, B
   Xie, B
   He, N
   Liang, YY
   Raj, A
   Balcan, MF
   Song, L
AF Dai, Bo
   Xie, Bo
   He, Niao
   Liang, Yingyu
   Raj, Anant
   Balcan, Maria-Florina
   Song, Le
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Scalable Kernel Methods via Doubly Stochastic Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper, we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient-one using random training points and another using random features associated with the kernel-and performing descent steps with this noisy functional gradient. Our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization bound of O(1/root t). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.
C1 [Dai, Bo; Xie, Bo; He, Niao; Raj, Anant; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Liang, Yingyu] Princeton Univ, Princeton, NJ 08544 USA.
   [Balcan, Maria-Florina] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Dai, B (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM bodai@gatech.edu; bxie33@gatech.edu; nhe6@gatech.edu;
   yingyul@cs.princeton.edu; araj34@gatech.edu; ninamf@cs.cmu.edu;
   lsong@cc.gatech.edu
FU NSF [CCF-0953192, CCF-1451177, CCF-1101283, CCF-1422910, IIS-1116886];
   ONR [N00014-09-1-0751]; AFOSR [FA9550-09-1-0538]; NSF/NIH BIGDATA
   [1R01GM108341]; NSF CAREER [IIS-1350983]; Raytheon Faculty Fellowship
FX M.B. is suppoerted in part by NSF CCF-0953192, CCF-1451177, CCF-1101283,
   and CCF-1422910, ONR N00014-09-1-0751, and AFOSR FA9550-09-1-0538. L.S.
   is supported in part by NSF IIS-1116886, NSF/NIH BIGDATA 1R01GM108341,
   NSF CAREER IIS-1350983, and a Raytheon Faculty Fellowship.
CR Cortes C., 2010, AISTATS
   Cotter A., 2013, ICML
   Dang Cong D., 2013, TECHNICAL REPORT
   DEVINATZ A, 1953, T AM MATH SOC, V74, P56, DOI 10.2307/1990848
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Hein M., 2004, 127 MAX PLANCK I BIO
   Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   Le  Q., 2013, ICML
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Lopez-Paz D., 2014, ICML
   Montavon G., 2012, NIPS
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Pham N., 2013, KDD
   Platt J., 1998, MSRTR9814
   Rahimi Ali, 2008, NIPS
   Rahimi Ali, 2009, NIPS
   Rakhlin A., 2012, P 29 INT C MACH LEAR, P449
   Ratliff N., 2007, IJCAI
   Scholkopf B., 2002, LEARNING KERNELS
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz Shai, 2007, ICML
   Smola A. J., 2000, ICML
   Wendland  H., 2005, SCATTERED DATA APPRO
   Williams Christopher K. I., 2000, NIPS
NR 30
TC 3
Z9 3
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100017
DA 2019-06-15
ER

PT S
AU Gens, R
   Domingos, P
AF Gens, Robert
   Domingos, Pedro
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Deep Symmetry Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.
C1 [Gens, Robert; Domingos, Pedro] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
RP Gens, R (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
EM rcg@cs.washington.edu; pedrod@cs.washington.edu
FU ARO [W911NF-08-1-0242]; ONR [N00014-13-1-0720, N00014-12-1-0312]; AFRL
   [FA8750-13-2-0019]
FX This research was partly funded by ARO grant W911NF-08-1-0242, ONR
   grants N00014-13-1-0720 and N00014-12-1-0312, and AFRL contract
   FA8750-13-2-0019. The views and conclusions contained in this document
   are those of the authors and should not be interpreted as necessarily
   representing the official policies, either expressed or implied, of ARO,
   ONR, AFRL, or the United States Government.
CR ABUMOSTAFA YS, 1993, NEURAL COMPUT, V5, P278, DOI 10.1162/neco.1993.5.2.278
   Anselmi F., 2013, 13114158 ARXIV
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bergstra J., 2010, P PYTH SCI COMP C
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Ciresan D., 2012, P IEEE C COMP VIS PA
   Diaconis P, 1988, GROUP REPRESENTATION
   Drost B., 2010, P IEEE C COMP VIS PA
   Felzenszwalb P., 2008, P IEEE C COMP VIS PA
   Hinton G. E., 2011, P 21 INT C ART NEUR
   Huang F., 2004, P IEEE C COMP VIS PA
   Kondor IR, 2008, GROUP THEORETICAL ME
   Kriznar A, 2012, ACTA ARTIS ACADEMICA 2012: KNOWLEDGE AND EXPERIENCE IN THE FINE ART, P25
   Kulesza A., 2012, 12076083 ARXIV
   Larochelle H., 2007, P 24 INT C MACH LEAR
   Lee T, 2011, IMAGE VISION COMPUT, V29, P639, DOI 10.1016/j.imavis.2011.08.003
   Lowe D. G., 1999, P IEEE C COMP VIS PA
   Lu F, 1997, J INTELL ROBOT SYST, V18, P249, DOI 10.1023/A:1007957421070
   Miller W, 1972, SYMMETRY GROUPS THEI
   Niepert M., 2012, P 28 C UNC ART INT
   Simard P., 1992, ADV NEURAL INFORM PR, V5
   Szegedy Christian, 2014, INT C LEARN REPR
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
NR 24
TC 3
Z9 3
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101095
DA 2019-06-15
ER

PT S
AU Borodin, A
   El-Yaniv, R
   Gogan, V
AF Borodin, A
   El-Yaniv, R
   Gogan, V
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Can we learn to beat the best stock
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID INDIVIDUAL SEQUENCES; COMPRESSION; PORTFOLIOS
AB A novel algorithm for actively trading stocks is presented. While traditional universal algorithms (and technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can "beat the market" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.
C1 Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
RP Borodin, A (reprint author), Univ Toronto, Dept Comp Sci, 100 Coll St, Toronto, ON, Canada.
CR Blum A, 1998, MACH LEARN, V30, P23, DOI 10.1023/A:1007402410823
   Borodin A., 1998, ONLINE COMPUTATION C
   BROCK W, 1992, J FINANC, V47, P1731, DOI 10.2307/2328994
   CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179
   CHOU A, 1995, P 6 ANN ACM SIAM S D
   COVER T, 1991, ELEMENT INFORMATION
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Cover TM, 1996, IEEE T INFORM THEORY, V42, P348, DOI 10.1109/18.485708
   COVER TM, 1986, ADV APPL MATH, V7, P170, DOI 10.1016/0196-8858(86)90029-1
   FEDER M, 1991, IEEE T INFORM THEORY, V37, P1459, DOI 10.1109/18.133269
   Helmbold DP, 1998, MATH FINANC, V8, P325, DOI 10.1111/1467-9965.00058
   LANGDON GG, 1983, IEEE T INFORM THEORY, V29, P284, DOI 10.1109/TIT.1983.1056645
   Lo A.W., 1999, NONRANDOM WALK WALL
   LUGOSI G, 2001, LECT PREDICTION INDI
   Markowitz H. M., 1959, PORTFOLIO SELECTION
   Raghavan P., 1991, DIMACS SERIES DISCRE, P79
   RISSANEN J, 1983, IEEE T INFORM THEORY, V29, P656, DOI 10.1109/TIT.1983.1056741
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
NR 18
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 345
EP 352
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500044
DA 2019-06-15
ER

PT S
AU Bousquet, O
   Chapelle, O
   Hein, M
AF Bousquet, O
   Chapelle, O
   Hein, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Measure based regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID DIMENSIONALITY REDUCTION
AB We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We, also propose practical implementations.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Bousquet, O (reprint author), Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   BELKIN M, 2003, IN PRESS MACHINE LEA
   GIROSI F, 1993, 1430 MIT
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3
   LALOUDOUANA D, 2002, ADV NEURAL INFORMATI, V15
   NG AY, 2001, ADV NEURAL INFORMATI, V14
   Scholkopf B., 2002, LEARNING KERNELS
   Smola AJ, 1998, ALGORITHMICA, V22, P211, DOI 10.1007/PL00013831
   SZUMMER M, 2002, ADV NEURAL INFORMATI, V15
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   VINCENT P, 2003, SNOWB LEARN WORKSH
   VONLUXBURG U, 2003, P 16 ANN C COMP LEAR
NR 13
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1221
EP 1228
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500152
DA 2019-06-15
ER

PT S
AU Chigirev, D
   Bialek, W
AF Chigirev, D
   Bialek, W
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Optimal manifold representation of data: An information theoretic
   approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NONLINEAR DIMENSIONALITY REDUCTION; STATISTICAL VARIABLES; PRINCIPAL
   COMPONENTS; COMPLEX
AB We introduce an information theoretic method for nonparametric, nonlinear dimensionality reduction, based on the infinite cluster limit of rate distortion theory. By constraining the information available to manifold coordinates, a natural probabilistic map emerges that assigns original data to corresponding points on a lower dimensional manifold. With only the information-distortion trade off as a parameter, our method determines the shape of the manifold, its dimensionality, the probabilistic map and the prior that provide optimal description of the data.
C1 Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
RP Chigirev, D (reprint author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953
   BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855
   BRAND M, 2003, ADV NEURAL INFORMATI, V15
   BREGLER C, 1995, ADV NEURAL INFORMATI, V7
   GRASSBERGER P, 1983, PHYS REV LETT, V50, P346, DOI 10.1103/PhysRevLett.50.346
   HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P498, DOI 10.1037/h0070888
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
NR 13
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 161
EP 168
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500021
DA 2019-06-15
ER

PT S
AU de Sa, VR
AF de Sa, VR
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Sensory modality segregation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the "visual input". We explain this finding in terms of the statistical structure in sensory inputs.
C1 Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.
RP de Sa, VR (reprint author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.
CR Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   de Sa Virginia R, 1994, ADV NEURAL INFORM PR, P112
   de Sa VR, 1998, NEURAL COMPUT, V10, P1097, DOI 10.1162/089976698300017368
   DESA V, 1997, PSYCHOL LEARN MOTIV, V36, P309
   DODWELL PC, 1990, PSYCHOL REV
   DURGIN FH, P ANN M PSYCH SOC
   KOHONEN T, 1990, IJCNN INT JOINT C NE, V1
   MCCOLLOUGH C, 1965, SCIENCE, V149, P1115, DOI 10.1126/science.149.3688.1115
   Muslea Ion, 2002, P 19 INT C MACH LEAR, V2, P435
   POLANA R, 1994, THESIS U ROCHESTER
NR 10
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 913
EP 920
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500114
DA 2019-06-15
ER

PT S
AU Fink, M
   Perona, P
AF Fink, M
   Perona, P
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Mutual boosting for contextual inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID PERCEPTION; OBJECTS
AB Mutual Boosting is a method aimed at incorporating contextual information to augment object detection. When multiple detectors of objects and parts are trained in parallel using AdaBoost [1], object detectors might use the remaining intermediate detectors to enrich the weak learner set. This method generalizes the efficient features suggested by Viola and Jones [2] thus enabling information inference between parts and objects in a compositional hierarchy. In our experiments eye-, nose-, mouth- and face detectors are trained using the Mutual Boosting framework. Results show that the method outperforms applications overlooking contextual information. We suggest that achieving contextual integration is a step toward human-like detection capabilities.
C1 Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.
RP Fink, M (reprint author), Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.
EM fink@huji.ac.il; perona@vision.caltech.edu
CR BARNARD K, 2001, INT C COMP VIS, V2, P408
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X
   Biederman I., 1981, PERCEPTUAL ORG, P213
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3
   Oliva A, 2002, LECT NOTES COMPUT SC, V2525, P263
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   TANAKA K, 1991, J NEUROPHYSIOL, V66, P170
   VIOLA VP, 2001, IEEE ICCV WORKSH STA
   WEBER M, 2000, ECCV, P18
NR 11
TC 3
Z9 3
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1515
EP 1522
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500188
DA 2019-06-15
ER

PT S
AU Li, YQ
   Cichocki, A
   Amari, SI
   Shishkin, S
   Cao, JT
   Gu, FJ
AF Li, YQ
   Cichocki, A
   Amari, SI
   Shishkin, S
   Cao, JT
   Gu, FJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Sparse representation and its applications in blind source separation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB In this paper, sparse representation (factorization) of a data matrix is first discussed. An overcomplete basis matrix is estimated by using the K-means method. We have proved that for the estimated overcomplete basis matrix, the sparse solution (coefficient matrix) with minimum l(1)-norm is unique with probability of one, which can be obtained using a linear programming algorithm. The comparisons of the l(1)-norm solution and the l(0)-norm solution are also presented, which can be used in recoverability analysis of blind source separation (BSS). Next, we apply the sparse rnatrix factorization approach to BSS in the overcomplete case. Generally, if the sources are not sufficiently sparse, we perform blind separation in the time-frequency domain after preprocessing the observed data using the wavelet packets transformation. Third, an EEG experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance. Two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis, and their periods of significant phase synchronization are found which are related to tasks. Finally, concluding remarks review the approach and state areas that require further study.
C1 RIKEN, Brain Sci Inst, Saitama, Saitama 3510198, Japan.
RP Li, YQ (reprint author), RIKEN, Brain Sci Inst, Saitama, Saitama 3510198, Japan.
RI Shishkin, Sergei/A-3461-2013
OI Shishkin, Sergei/0000-0002-3257-1022
CR Chen QW, 2001, HIGH PRESSURE RES, V20, P1, DOI 10.1080/08957950108206146
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Le Van Quyen M, 2001, J NEUROSCI METH, V111, P83, DOI 10.1016/S0165-0270(01)00372-7
   Lee TW, 1999, IEEE SIGNAL PROC LET, V6, P87, DOI 10.1109/97.752062
   Makeig S, 2002, SCIENCE, V295, P690, DOI 10.1126/science.1066168
   Olshausen BA, 2001, ADV NEUR IN, V13, P887
   ZIBULEVSKY M, 2000, INDEPENDENT COMPONEN
NR 7
TC 3
Z9 3
U1 0
U2 4
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 241
EP 248
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500031
DA 2019-06-15
ER

PT S
AU Miyawaki, Y
   Okada, M
AF Miyawaki, Y
   Okada, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Mechanism of neural interference by transcranial magnetic stimulation:
   network or single neuron?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID HUMAN VISUAL-CORTEX; ORIENTATION SELECTIVITY; NEOCORTICAL NEURONS; COIL
   STIMULATION; MODEL; INHIBITION; PERCEPTION
AB This paper proposes neural mechanisms of transcranial magnetic stimulation (TMS). TMS can stimulate the brain non-invasively through a brief magnetic pulse delivered by a coil placed on the scalp, interfering with specific cortical functions with a high temporal resolution. Due to these advantages, TMS has been a popular experimental tool in various neuroscience fields. However, the neural mechanisms underlying TMS-induced interference are still unknown; a theoretical basis for TMS has not been developed. This paper provides computational evidence that inhibitory interactions in a neural population, not an isolated single neuron, play a critical role in yielding the neural interference induced by TMS.
C1 RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
RP Miyawaki, Y (reprint author), RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
CR Amassian V. E., 2002, HDB TRANSCRANIAL MAG, P323
   AMASSIAN VE, 1989, ELECTROEN CLIN NEURO, V74, P458, DOI 10.1016/0168-5597(89)90036-1
   AMASSIAN VE, 1993, BRAIN RES, V605, P317, DOI 10.1016/0006-8993(93)91758-K
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   INGHILLERI M, 1993, J PHYSIOL-LONDON, V466, P521
   Kamitani Y, 1999, NAT NEUROSCI, V2, P767, DOI 10.1038/11245
   Kamitani Y, 2001, NEUROCOMPUTING, V38, P697, DOI 10.1016/S0925-2312(01)00447-7
   Kammer T, 1998, NEUROPSYCHOLOGIA, V36, P1161, DOI 10.1016/S0028-3932(98)00003-7
   Mainen ZF, 1996, NATURE, V382, P363, DOI 10.1038/382363a0
   NAGARAJAN SS, 1993, IEEE T BIO-MED ENG, V40, P1175, DOI 10.1109/10.245636
   PRIORI A, 1995, ELECTROMYOGR MOTOR C, V97, P69, DOI 10.1016/0924-980X(94)00224-U
   Ray PG, 1998, J CLIN NEUROPHYSIOL, V15, P351, DOI 10.1097/00004691-199807000-00007
   ROTH BJ, 1990, IEEE T BIO-MED ENG, V37, P588, DOI 10.1109/10.55662
   SOMERS DC, 1995, J NEUROSCI, V15, P5448
   Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1
   Walsh V, 2000, NAT REV NEUROSCI, V1, P73, DOI 10.1038/35036239
   Ziemann U, 1996, J PHYSIOL-LONDON, V496, P873, DOI 10.1113/jphysiol.1996.sp021734
NR 17
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1295
EP 1302
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500161
DA 2019-06-15
ER

PT S
AU Moallemi, CC
   Van Roy, B
AF Moallemi, CC
   Van Roy, B
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Distributed optimization in adaptive networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID POLICY-GRADIENT ESTIMATION/; INFINITE-HORIZON
AB We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overflow in a sensor network.
   Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective.
C1 Stanford Univ, Stanford, CA 94305 USA.
RP Moallemi, CC (reprint author), Stanford Univ, Stanford, CA 94305 USA.
CR Bartlett PL, 2002, J COMPUT SYST SCI, V64, P133, DOI 10.1006/jcss.2001.1793
   Bartlett PL, 2000, IEEE DECIS CONTR P, P124, DOI 10.1109/CDC.2000.912744
   Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Baxter J, 2001, J ARTIF INTELL RES, V15, P351, DOI 10.1613/jair.807
   Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345
   KUSHNER HJ, 1997, STOCHASTIC APPROXIM
   Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687
   MARBACH P, 1998, IEEE C DEC CONTR
   MOALLEMI CC, APPENDIX NIPS SUBMIS
NR 9
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 887
EP 894
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500111
DA 2019-06-15
ER

PT S
AU Shi, RZ
   Horiuchi, T
AF Shi, RZ
   Horiuchi, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A summating, exponentially-decaying CMOS synapse for spiking neural
   systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID DEPRESSION; SILICON; FILTERS
AB Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modification. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5 mum CMOS process.
C1 Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
RP Shi, RZ (reprint author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
CR Boahen KA, 1997, ANALOG INTEGR CIRC S, V13, P53, DOI 10.1023/A:1008215524165
   Boegerhausen M, 2003, NEURAL COMPUT, V15, P331, DOI 10.1162/089976603762552942
   CHEELY M, 2003, IN PRESS EURASIP J
   DEISS SR, 1999, PULSED NEURAL NETWOR, P157
   Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Frey DR, 1996, IEEE T CIRCUITS-I, V43, P34, DOI 10.1109/81.481459
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hafliger P, 1997, ADV NEUR IN, V9, P692
   INDIVERI G, 2002, ADV NEURAL INFORMATI, V15
   Lazzaro J., 1994, SILICON IMPLEMENTATI, P153
   Liu S.-C., 2002, ANALOG VLSI CIRCUITS
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   MCEWAN A, 2000, P ICSC S INT SYST AP
   Mead C., 1989, ANALOG VLSI NEURAL S
   Mortara A., 1998, NEUROMORPHIC SYSTEMS, P217
   RALL W, 1967, J NEUROPHYSIOL, V30, P1138
   Rasche C, 2001, BIOL CYBERN, V84, P57, DOI 10.1007/s004220170004
   SEEVINCK E, 1990, ELECTRON LETT, V26, P2046, DOI 10.1049/el:19901319
   Tsividis Y, 1997, IEEE T CIRCUITS-II, V44, P65, DOI 10.1109/82.554425
   VITTOZ E, 1977, IEEE J SOLID-ST CIRC, V12, P224, DOI 10.1109/JSSC.1977.1050882
NR 21
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1003
EP 1010
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500125
DA 2019-06-15
ER

PT S
AU Sigal, L
   Isard, M
   Sigelman, BH
   Black, MJ
AF Sigal, L
   Isard, M
   Sigelman, BH
   Black, MJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Attractive people: Assembling loose-limbed models using non-parametric
   belief propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuous-valued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter.
C1 Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
RP Sigal, L (reprint author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
CR BREGLER C, 1998, CVPR, P8
   Burl M. C., 1998, ECCV, P628
   COUGHLAN J, 2002, ECCV, V3, P453
   DEUTSCHER J, 1999, ICCV, P1144
   DEUTSCHER J, 2001, CVPR, P669
   DEUTSCHER J, 2002, ECCV, P175
   Doucet A, 2001, STAT ENG IN, P3
   FELZENSZWALB P, 2000, CVPR, V2, P66
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   GAO J, 2003, CMURITR0305
   Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708
   Isard Michael, 2003, CVPR, V1, P613
   Jordan M. I., 2001, GRAPHICAL MODELS FDN
   JU SX, 1996, INT C AUT FAC GEST R, P38
   MACCORMICK I, 2000, ECCV, P3
   PAVOLVIC V, 1999, ICCV, P94
   RAMANAN D, 2003, CVPR, V2, P467
   Sidenbladh H., 2000, EUR C COMP VIS, V2, P702
   SIDENBLADH H, 2001, IEEE INT C COMP VIS, V2, P709
   SIGELMAN B, 2003, CS0308 BROWN U DEP C
   SMINCHISESCU C, 2001, CVPR, V1, P447
   Sudderth E. B., 2003, CVPR, V1, pI
   WU Y, 2003, ICCV, P1094
   YEDIDIA J, 2000, ADV NEURAL INFORM PR, P689
   Yu S. X., 2003, ADV NEURAL INFO P SY, V15, P1407
NR 25
TC 3
Z9 3
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1539
EP 1546
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500191
DA 2019-06-15
ER

PT S
AU Still, S
   Bialek, W
   Bottou, L
AF Still, S
   Bialek, W
   Bottou, L
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Geometric clustering using the information bottleneck method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB We argue that K-means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach. If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate. But if we treat the equations that define the optimal solution as an iterative algorithm, then a set of "smooth" initial conditions selects solutions with the desired geometrical properties. In addition to conceptual unification, we argue that this approach can be more efficient and robust than classic algorithms.
C1 Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
RP Still, S (reprint author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
EM susanna@princeton.edu; wbialek@princeton.edu; leon@bottou.org
CR Bialek W, 1996, PHYS REV LETT, V77, P4693, DOI 10.1103/PhysRevLett.77.4693
   BIALEK W, 2001, PHYS BIOMOLECULES CE, V75, P485
   Blatt M, 1996, PHYS REV LETT, V76, P3251, DOI 10.1103/PhysRevLett.76.3251
   Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131
   Gordon A. D., 1999, CLASSIFICATION
   HALL P, 1988, BIOMETRIKA, V75, P705
   Horn D, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.018702
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788
   ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945
   Shannon C. E., 1963, MATH THEORY COMMUNIC
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Smyth P, 2000, STAT COMPUT, V10, P63, DOI 10.1023/A:1008940618127
   STILL S, 2003, UNPUB
   TISHBY N, 1999, P 37 ANN ALL C
NR 15
TC 3
Z9 3
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1165
EP 1172
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500145
DA 2019-06-15
ER

PT S
AU Klindt, DA
   Ecker, AS
   Euler, T
   Bethge, M
AF Klindt, David A.
   Ecker, Alexander S.
   Euler, Thomas
   Bethge, Matthias
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural system identification for large populations separating "what" and
   "where"
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECEPTIVE-FIELDS; SPATIAL STRUCTURE; RESPONSES; NEURONS; MODELS
AB Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of "what" and "where". Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations - a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.
C1 [Klindt, David A.; Ecker, Alexander S.; Euler, Thomas; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.
   [Klindt, David A.; Ecker, Alexander S.; Euler, Thomas; Bethge, Matthias] Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany.
   [Klindt, David A.; Euler, Thomas] Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany.
   [Ecker, Alexander S.; Bethge, Matthias] Univ Tubingen, Inst Theoret Phys, Tubingen, Germany.
   [Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Ecker, Alexander S.; Bethge, Matthias] Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA.
RP Klindt, DA (reprint author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.; Klindt, DA (reprint author), Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany.; Klindt, DA (reprint author), Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany.
EM klindt.david@gmail.com; alexander.ecker@uni-tuebingen.de;
   thomas.euler@cin.uni-tuebingen.de; matthias.bethge@bethgelab.org
FU German Research Foundation (DFG) through Collaborative Research Center
   [CRC 1233]; DFG [EC 479/1-1]; European Union [674901]; German Excellency
   Initiative through the Centre for Integrative Neuroscience Tubingen
   [EXC307]; Intelligence Advanced Research Projects Activity (IARPA) via
   Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]
FX This work was supported by the German Research Foundation (DFG) through
   Collaborative Research Center (CRC 1233) "Robust Vision" as well as DFG
   grant EC 479/1-1; the European Union's Horizon 2020 research and
   innovation programme under the Marie Sklodowska-Curie grant agreement No
   674901; the German Excellency Initiative through the Centre for
   Integrative Neuroscience Tubingen (EXC307). The research was also
   supported by Intelligence Advanced Research Projects Activity (IARPA)
   via Department of Interior/Interior Business Center (DoI/IBC) contract
   number D16PC00003. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon. Disclaimer: The views and conclusions
   contained herein are those of the authors and should not be interpreted
   as necessarily representing the official policies or endorsements,
   either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Antolik J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004927
   Baden T, 2016, NATURE, V529, P345, DOI 10.1038/nature16468
   Batty Eleanor, 2017, 5 INT C LEARN REPR
   Benjamin AS, 2017, BIORXIV
   Cadena S. A, 2017, BIORXIV, DOI [10.1101/201764, DOI 10.1101/201764]
   Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005
   Franke K, 2017, NATURE, V542, P439, DOI 10.1038/nature21394
   Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009
   Heitman Alexander, 2016, BIORXIV, P45336
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Ioffe S, 2015, ARXIV150203167CS
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1187
   Khaligh-Razavi S-M, 2014, BIORXIV, P9936, DOI [10.1101/009936, DOI 10.1101/009936]
   Kindel William F., 2017, ARXIV170606208CSQBIO
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lau B, 2002, P NATL ACAD SCI USA, V99, P8974, DOI 10.1073/pnas.122173499
   LEHKY SR, 1992, J NEUROSCI, V12, P3568
   McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143
   McIntosh Lane T., 2017, ARXIV170201825QBIOST
   Prenger R, 2004, NEURAL NETWORKS, V17, P663, DOI 10.1016/j.neunet.2004.03.008
   Real Esteban, 2017, CURRENT BIOL
   Rowekamp RJ, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15739
   Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Touryan J, 2005, NEURON, V45, P781, DOI 10.1016/j.neuron.2005.01.029
   Vintch B, 2015, J NEUROSCI, V35, P14829, DOI 10.1523/JNEUROSCI.2815-13.2015
   Weber Alison I., 2016, ARXIV160207389QBIO
   Willmore B, 2008, NEURAL COMPUT, V20, P1537, DOI 10.1162/neco.2007.05-07-513
   Wu MCK, 2006, ANNU REV NEUROSCI, V29, P477, DOI 10.1146/annurev.neuro.29.051605.113024
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
NR 31
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403056
DA 2019-06-15
ER

PT S
AU Mescheder, L
   Nowozin, S
   Geiger, A
AF Mescheder, Lars
   Nowozin, Sebastian
   Geiger, Andreas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Numerics of GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.
C1 [Mescheder, Lars; Geiger, Andreas] MPI Tubingen, Autonomous Vis Grp, Tubingen, Germany.
   [Nowozin, Sebastian] Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA USA.
RP Mescheder, L (reprint author), MPI Tubingen, Autonomous Vis Grp, Tubingen, Germany.
EM lars.mescheder@tuebingen.mpg.de; sebastian.nowozin@microsoft.com;
   andreas.geiger@tuebingen.mpg.de
FU Microsoft Research through its PhD Scholarship Programme
FX This work was supported by Microsoft Research through its PhD
   Scholarship Programme.
CR Abadi  M., 2016, ABS160304467 CORR
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Arjovsky M., 2017, ABS170104862 CORR
   Arjovsky  M., 2017, ABS170107875 CORR
   Arora S., 2017, P 34 INT C MACH LEAR, V70, P224
   Bertsekas DP, 2014, CONSTRAINED OPTIMIZA
   Butcher J. C., 2016, NUMERICAL METHODS OR
   Donahue Jeff, 2016, ABS160509782 CORR
   Dumoulin Vincent, 2016, ABS160600704 CORR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, ABS170400028 CORR
   Isola P., 2016, ABS161107004 CORR
   Ledig  C., 2016, ABS160904802 CORR, V2, P3
   Mescheder L, 2017, P 34 INT C MACH LEAR, P2391
   Metz L., 2016, ABS161102163 CORR
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Odena A., 2017, P 34 INT C MACH LEAR, P2642
   Pascanu Razvan, 2013, ABS13013584 CORR
   Paszke Adam, 2017, PYTORCH
   Pfau David, 2016, ABS161001945 CORR
   Radford  A., 2015, ABS151106434 CORR
   Ratliff LJ, 2013, ANN ALLERTON CONF, P917, DOI 10.1109/Allerton.2013.6736623
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Sonderby Casper Kaae, 2016, ABS161004490 CORR
   Tieleman T., 2012, LECT 6 5 RMSPROP DIV
   Tzeng  E., 2017, ABS170205464 CORR
   Yeh R. A., 2016, ABS160707539 CORR
NR 27
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401083
DA 2019-06-15
ER

PT S
AU Russell, C
   Kusner, MJ
   Loftus, JR
   Silva, R
AF Russell, Chris
   Kusner, Matt J.
   Loftus, Joshua R.
   Silva, Ricardo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI When Worlds Collide: Integrating Different Counterfactual Assumptions in
   Fairness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RISK
AB Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal "world" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.
C1 [Russell, Chris; Kusner, Matt J.; Loftus, Joshua R.; Silva, Ricardo] Alan Turing Inst, London, England.
   [Russell, Chris] Univ Surrey, Guildford, Surrey, England.
   [Kusner, Matt J.] Univ Warwick, Coventry, W Midlands, England.
   [Loftus, Joshua R.] NYU, New York, NY 10003 USA.
   [Silva, Ricardo] UCL, London, England.
RP Russell, C (reprint author), Alan Turing Inst, London, England.; Russell, C (reprint author), Univ Surrey, Guildford, Surrey, England.
EM crussell@turing.ac.uk; mkusner@turing.ac.uk; loftus@nyu.edu;
   ricardo@stats.ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Alan Turing Institute under the EPSRC [EP/N510129/1]; EPSRC Platform
   Grant [EP/P022529/1]
FX This work was supported by The Alan Turing Institute under the EPSRC
   grant EP/N510129/1. CR acknowledges additional support under the EPSRC
   Platform Grant EP/P022529/1.
CR Angwin J., 2016, MACHINE BIAS
   [Anonymous], 2016, COMPAS RISK SCALES D
   Berk R., 2017, ARXIV170309207
   Brennan T, 2009, CRIM JUSTICE BEHAV, V36, P21, DOI 10.1177/0093854808326545
   Dawid AP, 2000, J AM STAT ASSOC, V95, P407, DOI 10.2307/2669377
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Khandani AE, 2010, J BANK FINANC, V34, P2767, DOI 10.1016/j.jbankfin.2010.06.001
   Kirkpatrick K, 2017, COMMUN ACM, V60, P21, DOI 10.1145/3022181
   Kleinberg Jon, 2016, ARXIV160905807
   Kutnowski M, 2017, J COMMUNITY SAFETY W, V2, P13
   Larson Jeff, 2016, PROPUBLICA 5 2016
   Lopez-Paz David, 2016, ARXIV160703300
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Pearsall B., 2010, NIJ J, V266, P16
   Richardson T. S., 2013, 128 U WASH CTR STAT
   Russell C, 2017, ADV NEUR IN, V30
   Upchurch Paul, 2016, ARXIV161105507
   Wightman Linda F, 1998, LSAC RES REPORT SERI
   Zafar Muhammad Bilal, 2016, ARXIV161008452
NR 20
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406047
DA 2019-06-15
ER

PT S
AU Tolstikhin, I
   Gelly, S
   Bousquet, O
   Simon-Gabriel, CJ
   Scholkopf, B
AF Tolstikhin, Ilya
   Gelly, Sylvain
   Bousquet, Olivier
   Simon-Gabriel, Carl-Johann
   Schoelkopf, Bernhard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI AdaGAN: Boosting Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.
C1 [Tolstikhin, Ilya; Simon-Gabriel, Carl-Johann; Schoelkopf, Bernhard] MPI Intelligent Syst, Tubingen, Germany.
   [Gelly, Sylvain; Bousquet, Olivier] Google Brain, Zurich, Switzerland.
RP Tolstikhin, I (reprint author), MPI Intelligent Syst, Tubingen, Germany.
EM ilya@tue.mpg.de; sylvaingelly@google.com; obousquet@google.com;
   cjsimon@tue.mpg.de; bs@tue.mpg.de
RI Jeong, Yongwook/N-7413-2016
CR Arjovsky M., 2017, ARXIV170107875
   Barron A, 1997, BIOMETRICS, V53, P603
   Che T., 2016, ARXIV161202136
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grover Aditya, 2016, ICLR 2017 C UNPUB
   Hein M., 2005, P 10 INT WORKSH ART, P136
   Kingma Diederik P, 2014, ICLR
   Liese F, 2008, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-73194-0_1
   Metz L., 2017, ARXIV161102163
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Nowozin S., 2016, ADV NEURAL INFORM PR
   Radford A., 2016, ICLR
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Rosset S, 2002, NIPS, P641
   Tu Z, 2007, PROC CVPR IEEE, P500
   Wang Yaxing, 2016, ARXIV161200991
   Welling Max, 2002, ADV NEURAL INFORM PR, P665
NR 19
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405049
DA 2019-06-15
ER

PT S
AU Wang, G
   Giannakis, GB
   Saad, Y
   Chen, J
AF Wang, Gang
   Giannakis, Georgios B.
   Saad, Yousef
   Chen, Jie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Solving Most Systems of Random Quadratic Equations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PHASE RETRIEVAL; RECOVERY
AB This paper deals with finding an n-dimensional solution x to a system of quadratic equations y(i) = vertical bar < a(i), x >vertical bar(2), 1 <= i <= m, which in general is known to be NP-hard. We put forth a novel procedure, that starts with a weighted maximal correlation initialization obtainable with a few power iterations, followed by successive refinements based on iteratively reweighted gradient-type iterations. The novel techniques distinguish themselves from prior works by the inclusion of a fresh (re) weighting regularization. For certain random measurement models, the proposed procedure returns the true solution x with high probability in time proportional to reading the data {(a(i); y(i))}1 <= i <= m, provided that the number m of equations is some constant c > 0 times the number n of unknowns, that is, m >= cn. Empirically, the upshots of this contribution are: i) perfect signal recovery in the high-dimensional regime given only an information-theoretic limit number of equations; and, ii) (near-) optimal statistical accuracy in the presence of additive noise. Extensive numerical tests using both synthetic data and real images corroborate its improved signal recovery performance and computational efficiency relative to state-of-the-art approaches.
C1 [Wang, Gang; Chen, Jie] Beijing Inst Technol, Key Lab Intell Contr & Decis Complex Syst, Beijing, Peoples R China.
   [Wang, Gang; Giannakis, Georgios B.] Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.
   [Wang, Gang; Giannakis, Georgios B.] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
   [Saad, Yousef] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Wang, G (reprint author), Beijing Inst Technol, Key Lab Intell Contr & Decis Complex Syst, Beijing, Peoples R China.; Wang, G (reprint author), Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.; Wang, G (reprint author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
EM gangwang@umn.edu; georgios@umn.edu; saad@umn.edu; chenjie@bit.edu.cn
RI Wang, Gang/I-9061-2019
OI Wang, Gang/0000-0002-7266-2412
FU NSF [1500713, 1514056, 1505970]; National Natural Science Foundation of
   China [U1509215, 61621063]; Program for Changjiang Scholars and
   Innovative Research Team in University [IRT1208]
FX G. Wang and G. B. Giannakis were partially supported by NSF grants
   1500713 and 1514056. Y. Saad was partially supported by NSF grant
   1505970. J. Chen was partially supported by the National Natural Science
   Foundation of China grants U1509215, 61621063, and the Program for
   Changjiang Scholars and Innovative Research Team in University
   (IRT1208).
CR Balan R, 2006, APPL COMPUT HARMON A, V20, P345, DOI 10.1016/j.acha.2005.07.001
   Ben-Tal  A., 2001, LECT MODERN CONVEX O, V2
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen J., 2017, ARXIV170406256
   Chen Y., 2015, ADV NEURAL INFORM PR, V2, P739
   Conca A, 2015, APPL COMPUT HARMON A, V38, P346, DOI 10.1016/j.acha.2014.06.005
   Duchi J. C., 2017, ARXIV170502356
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   GERCHBERG RW, 1972, OPTIK, V35, P237
   Goldstein T., 2016, ARXIV161007531V1
   Hand P., 2016, ARXIV161103935
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Lu Y. M., 2017, ARXIV170206435
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   PARDALOS P. M., 1991, J GLOBAL OPTIM, V1, P15, DOI DOI 10.1007/BF00120662
   Pereyra G., 2017, ARXIV170106548
   Rice J. R., 1992, NUMERICAL METHODS SO
   Saad Y., 2011, NUMERICAL METHODS LA
   Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673
   Soltanolkotabi M, 2017, ARXIV170206175
   Sun J., 2017, FDN COMPUT MATH
   Waldspurger I., 2016, AXIV160903088
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wang G., 2016, P ADV NEUR INF PROC, P568
   Wang G, 2017, IEEE T SIGNAL PROCES, V65, P1961, DOI 10.1109/TSP.2017.2652392
   Wang LG, 2017, IEEE INT SYMP INFO
   Yi X., 2014, P 31 INT C MACH LEAR, P613
   Zhang HS, 2017, J MACH LEARN RES, V18
NR 30
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401087
DA 2019-06-15
ER

PT S
AU Benozzo, D
   Olivetti, E
   Avesani, P
AF Benozzo, Danilo
   Olivetti, Emanuele
   Avesani, Paolo
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Classification-Based Causality Detection in Time Series
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID BRAIN CONNECTIVITY; NEURAL SYSTEMS
AB Brain effective connectivity aims to detect causal interactions between distinct brain units and it can be studied through the analysis of magneto/electroencephalography (M/EEG) signals. Methods to evaluate effective connectivity belong to the large body of literature related to detecting causal interactions between multivariate autoregressive (MAR) data, a field of signal processing. Here, we reformulate the problem of causality detection as a supervised learning task and we propose a classification-based approach for it. Our solution takes advantage of the MAR model by generating a labeled data set that contains trials of multivariate signals for each possible configuration of causal interactions. Through the definition of a proper feature space, a classifier is trained to identify the causality structure within each trial. As evidence of the efficacy of the proposed method, we report both the cross-validated results and the details of our submission to the causality detection competition of Biomag2014, where the method reached the 2nd place.
C1 [Benozzo, Danilo; Olivetti, Emanuele; Avesani, Paolo] Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.
   [Benozzo, Danilo; Olivetti, Emanuele; Avesani, Paolo] Univ Trento, Ctr Mind & Brain Sci CIMeC, Trento, Italy.
RP Benozzo, D (reprint author), Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.; Benozzo, D (reprint author), Univ Trento, Ctr Mind & Brain Sci CIMeC, Trento, Italy.
EM benozzo@fbk.eu; olivetti@fbk.eu; avesani@fbk.eu
CR Baccala L. A., 1998, Applied Signal Processing, V5, P40, DOI 10.1007/s005290050005
   Baccala LA, 2001, BIOL CYBERN, V84, P463, DOI 10.1007/PL00007990
   Brookes MJ, 2012, NEUROIMAGE, V63, P910, DOI 10.1016/j.neuroimage.2012.03.048
   BUTLER SR, 1974, ELECTROEN CLIN NEURO, V36, P481, DOI 10.1016/0013-4694(74)90205-3
   Faes L, 2012, COMPUT MATH METHOD M, DOI 10.1155/2012/140513
   Freiwald WA, 1999, J NEUROSCI METH, V94, P105, DOI 10.1016/S0165-0270(99)00129-6
   Friston KJ, 2011, BRAIN CONNECT, V1, P13, DOI 10.1089/brain.2011.0008
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Hlavackova-Schindler K, 2007, PHYS REP, V441, P1, DOI 10.1016/j.physrep.2006.12.004
   Horwitz B, 2003, NEUROIMAGE, V19, P466, DOI 10.1016/S1053-8119(03)00112-5
   Kaminski M, 2001, BIOL CYBERN, V85, P145, DOI 10.1007/s004220000235
   KAMINSKI MJ, 1991, BIOL CYBERN, V65, P203, DOI 10.1007/BF00198091
   Papana A, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.036207
   Pereda E, 2005, PROG NEUROBIOL, V77, P1, DOI 10.1016/j.pneurobio.2005.10.003
   Sakkalis V, 2011, COMPUT BIOL MED, V41, P1110, DOI 10.1016/j.compbiomed.2011.06.020
   Winterhalder M, 2005, SIGNAL PROCESS, V85, P2137, DOI 10.1016/j.sigpro.2005.07.011
NR 16
TC 2
Z9 2
U1 2
U2 3
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 85
EP 93
DI 10.1007/978-3-319-45174-9_9
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400009
DA 2019-06-15
ER

PT S
AU Herlau, T
   Schmidt, MN
   Morup, M
AF Herlau, Tue
   Schmidt, Mikkel N.
   Morup, Morten
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Completely random measures for modelling block-structured sparse
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Statistical methods for network data often parameterize the edge-probability by attributing latent traits such as block structure to the vertices and assume exchangeability in the sense of the Aldous-Hoover representation theorem. These assumptions are however incompatible with traits found in real-world networks such as a power-law degree-distribution. Recently, Caron & Fox (2014) proposed the use of a different notion of exchangeability after Kallenberg (2005) and obtained a network model which permits edge-inhomogeneity, such as a power-law degree-distribution whilst retaining desirable statistical properties. However, this model does not capture latent vertex traits such as block-structure. In this work we re-introduce the use of block-structure for network models obeying Kallenberg's notion of exchangeability and thereby obtain a collapsed model which both admits the inference of block-structure and edge inhomogeneity. We derive a simple expression for the likelihood and an efficient sampling method. The obtained model is not significantly more difficult to implement than existing approaches to block-modelling and performs well on real network datasets.
C1 [Herlau, Tue; Schmidt, Mikkel N.; Morup, Morten] Tech Univ Denmark, DTU Compute, Richard Petersens Plads 31, DK-2800 Lyngby, Denmark.
RP Herlau, T (reprint author), Tech Univ Denmark, DTU Compute, Richard Petersens Plads 31, DK-2800 Lyngby, Denmark.
EM tuhe@dtu.dk; mns@dtu.dk; mmor@dtu.dk
OI Schmidt, Mikkel Norgaard/0000-0001-6927-8869; Morup,
   Morten/0000-0003-4985-4368
FU Lundbeck Foundation [R105-9813]
FX This project was funded by the Lundbeck Foundation (grant nr.
   R105-9813).
CR ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Caron F., 2014, ARXIV14011137
   Chen C., 2013, P 30 INT C MACH LEAR, P969
   Devroye L, 2014, STAT METHOD APPL-GER, V23, P307, DOI 10.1007/s10260-014-0260-0
   Herlau T, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.032819
   Hoover Douglas N., 1979, PREPRINT, P2
   HOUGAARD P, 1986, BIOMETRIKA, V73, P387, DOI 10.1093/biomet/73.2.387
   James Lancelot F, 2002, MATH0205093 ARXIV
   Kallenberg Olaf, 2005, APPL PROBABILITY, V10
   Kemp C., 2006, AAAI, V3, P5
   KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59
   Kriegel Hans-Peter, 2006, P 22 INT C UNC ART I
   Newman MEJ, 2001, PHYS REV E, V64
   Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607
   Pitman J, 2003, INST MATH S, V40, P1
   Pitman Jim, 2006, COMBINATORIAL STOCHA
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Teh Yee Whye, 2014, ARXIV14074211
   Veitch Victor, 2015, CLASS RANDOM GRAPHS
   WHITE HC, 1976, AM J SOCIOL, V81, P730, DOI 10.1086/226141
   Zolotarev Vladimir Mikhailovich, 1964, T MATEMATICHESKOGO I, V71, P46
NR 22
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704046
DA 2019-06-15
ER

PT S
AU Chwialkowski, K
   Ramdas, A
   Sejdinovic, D
   Gretton, A
AF Chwialkowski, Kacper
   Ramdas, Aaditya
   Sejdinovic, Dino
   Gretton, Arthur
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast Two-Sample Testing with Analytic Representations of Probability
   Measures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID KERNEL HILBERT-SPACES; STATISTICS
AB We propose a class of nonparametric two-sample tests with a cost linear in the sample size. Two tests are given, both based on an ensemble of distances between analytic functions representing each of the distributions. The first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel Hilbert space. Analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies. The new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distancebased tests. Experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests. This performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.
C1 [Chwialkowski, Kacper; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London, England.
   [Ramdas, Aaditya] Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA USA.
   [Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England.
RP Chwialkowski, K (reprint author), UCL, Gatsby Computat Neurosci Unit, London, England.
EM kacper.chwialkowski@gmail.com; aramdas@cs.berkeley.edu;
   dino.sejdinovic@gmail.com; arthur.gretton@gmail.com
CR Anderson T. W, 2003, INTRO MULTIVARIATE S
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Baringhaus L, 2004, J MULTIVARIATE ANAL, V88, P190, DOI 10.1016/S0047-259X(03)00079-4
   Berlinet A., 2004, REPRODUCING KERNEL H, V3
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   DAVIDSON KR, 1983, AM MATH MON, V90, P391, DOI 10.2307/2975578
   Epps T. W., 1986, J STATISTICAL COMPUT, V26, P177, DOI DOI 10.1080/00949658608810963
   Fernandez VA, 2008, COMPUT STAT DATA AN, V52, P3730, DOI 10.1016/j.csda.2007.12.013
   Gretton A, 2009, NIPS
   Gretton A., 2012, NIPS
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Harchaoui Zaid, 2008, NIPS
   HEATHCOTE CR, 1977, BIOMETRIKA, V64, P255, DOI 10.2307/2335691
   HEATHCOTE CR, 1972, AUST J STAT, V14, P172, DOI 10.1111/j.1467-842X.1972.tb00355.x
   Ho HC, 2006, SCAND J STAT, V33, P861, DOI 10.1111/j.1467-9469.2006.00516.x
   Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979
   Le Q., 2013, JMLR W CP, P244
   Lichman M., 2013, UCI MACHINE LEARNING
   Lloyd J. R., 2014, TECHNICAL REPORT
   Pevny T, 2008, LECT NOTES COMPUT SC, V5284, P251
   Rahimi A., 2007, NIPS
   Ramdas A., 2015, AAAI
   Reddi S., 2015, AISTATS
   Rudin W., 1987, REAL COMPLEX ANAL
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I, 2006, IEEE T INFORM THEORY, V52, P4635, DOI 10.1109/TIT.2006.881713
   Steinwart I, 2008, INFORM SCI STAT, P1
   Sun HW, 2008, J FOURIER ANAL APPL, V14, P89, DOI 10.1007/s00041-007-9003-z
   Szekely GJ, 2003, TECHNICAL REPORT
   Zaremba W., 2013, NIPS
   Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732
   Zinger A., 1992, J MATH SCI, V59, P914
NR 34
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100053
DA 2019-06-15
ER

PT S
AU Clevert, DA
   Mayr, A
   Unterthiner, T
   Hochreiter, S
AF Clevert, Djork-Arne
   Mayr, Andreas
   Unterthiner, Thomas
   Hochreiter, Sepp
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Rectified Factor Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NEURAL-NETWORKS; OPTIMIZATION
AB We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.
   RFN package for GPU/CPU is available at http://www.bioinf.jku.at/software/rfn.
C1 [Clevert, Djork-Arne; Mayr, Andreas; Unterthiner, Thomas; Hochreiter, Sepp] Johannes Kepler Univ Linz, Inst Bioinformat, Linz, Austria.
RP Clevert, DA (reprint author), Johannes Kepler Univ Linz, Inst Bioinformat, Linz, Austria.
EM okko@bioinf.jku.at; mayr@bioinf.jku.at; unterthiner@bioinf.jku.at;
   hochreit@bioinf.jku.at
CR Abadie J., 1969, OPTIMIZATION
   Ben-Tal A., 2001, INTERIOR POINT POLYN, P377
   Bengio Y., 2007, P ADV NEUR INF PROC, P153
   BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194
   BERTSEKAS DP, 1982, SIAM J CONTROL OPTIM, V20, P221, DOI 10.1137/0320018
   Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Gunawardana A, 2005, J MACH LEARN RES, V6, P2049
   Harva M, 2007, SIGNAL PROCESS, V87, P509, DOI 10.1016/j.sigpro.2006.06.006
   Haug E. J., 1979, APPL OPTIMAL DESIGN
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hochreiter S, 2006, BIOINFORMATICS, V22, P943, DOI 10.1093/bioinformatics/btl033
   Hochreiter S, 2013, NUCLEIC ACIDS RES, V41, DOI 10.1093/nar/gkt1013
   Hochreiter S, 2010, BIOINFORMATICS, V26, P1520, DOI 10.1093/bioinformatics/btq227
   Huang F., 2004, P IEEE C COMP VIS PA
   Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483
   Kelley CT, 1999, ITERATIVE METHODS OP
   Krizhevsky A., 2009, THESIS
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Palmer A., 2006, ADV NEURAL INFORM PR, V18, P1059
   ROSEN JB, 1961, J SOC IND APPL MATH, V9, P514, DOI 10.1137/0109044
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Srebro N, 2004, THESIS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Verbist B, 2015, DRUG DISCOV TODAY, V20, P505, DOI 10.1016/j.drudis.2014.12.014
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Zangwill W.I., 1969, NONLINEAR PROGRAMMIN
NR 30
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103001
DA 2019-06-15
ER

PT S
AU Denton, E
   Chintala, S
   Szlam, A
   Fergus, R
AF Denton, Emily
   Chintala, Soumith
   Szlam, Arthur
   Fergus, Rob
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Generative Image Models using a Laplacian Pyramid of Adversarial
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID FIELDS
AB In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.
C1 [Denton, Emily] NYU, Dept Comp Sci, Courant Inst, New York, NY 10003 USA.
   [Chintala, Soumith; Szlam, Arthur; Fergus, Rob] Facebook AI Res, New York, NY USA.
RP Denton, E (reprint author), NYU, Dept Comp Sci, Courant Inst, New York, NY 10003 USA.
FU NSERC Fellowship; FAIR Infrastructure team
FX We would like to thank the anonymous reviewers for their insightful and
   constructive comments. We also thank Andrew Tulloch, Wojciech Zaremba
   and the FAIR Infrastructure team for useful discussions and support.
   Emily Denton was supported by an NSERC Fellowship.
CR BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Coates A, 2011, AISTATS
   De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton E., DEEP GENERATIVE IMAG
   Dosovitskiy A., 2014, ARXIV14115928
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Gauthier J., 2014, CLASS PROJECT STANFO, V2014
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor Karol, 2015, CORR
   Hays J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276382, 10.1145/1239451.1239455]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ioffe S, 2015, ARXIV150203167V3
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2009, THESIS
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Osindero S, 2008, ADV NEURAL INFORM PR, V20, P1121
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Ranzato MA, 2010, INT C ART INT STAT, V9, P621
   Ranzato M, 2013, IEEE T PATTERN ANAL, V35, P2206, DOI 10.1109/TPAMI.2013.29
   Rezende D. J, 2014, ARXIV14014082
   Roth S, 2005, PROC CVPR IEEE, P860
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   Sohl-Dickstein J., 2015, CORR
   Theis L., 2015, GENERATIVE IMAGE MOD
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Zhang Y., 2015, CVPR WORKSH
   Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420
   Zoran D., 2011, ICCV
NR 34
TC 2
Z9 2
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101033
DA 2019-06-15
ER

PT S
AU Duvenaudt, D
   Maclaurin, D
   Aguilera-Iparraguirre, J
   Gomez-Bombarelli, R
   Hirzel, T
   Aspuru-Guzik, A
   Adams, RP
AF Duvenaudt, David
   Maclaurin, Dougal
   Aguilera-Iparraguirre, Jorge
   Gomez-Bombarelli, Rafael
   Hirzel, Timothy
   Aspuru-Guzik, Alan
   Adams, Ryan P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convolutional Networks on Graphs for Learning Molecular Fingerprints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID AQUEOUS SOLUBILITY; NEURAL-NETWORK
AB We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.
C1 [Duvenaudt, David; Maclaurin, Dougal; Aguilera-Iparraguirre, Jorge; Gomez-Bombarelli, Rafael; Hirzel, Timothy; Aspuru-Guzik, Alan; Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA.
RP Duvenaudt, D (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
FU Samsung Advanced Institute of Technology; NSF [IIS-1421780]
FX We thank Edward Pyzer-Knapp, Jennifer Wei, and Samsung Advanced
   Institute of Technology for their support. This work was partially
   funded by NSF IIS-1421780.
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Blunsom P., 2014, P 52 ANN M ASS COMP
   Bruna J., 2013, ABS13126203 CORR
   Dahl G. E., 2014, ARXIV14061231
   Delaney JS, 2004, J CHEM INF COMP SCI, V44, P1000, DOI 10.1021/ci034243x
   Gamo FJ, 2010, NATURE, V465, P305, DOI 10.1038/nature09107
   Glen RC, 2006, IDRUGS, V9, P199
   Graves A., 2014, ARXIV14105401
   Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s
   Hershey J. R., 2014, ARXIV14092574
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   LeCun Y., 1995, HDB BRAIN THEORY NEU, V3361
   Lusci A, 2013, J CHEM INF MODEL, V53, P1563, DOI 10.1021/ci400187y
   Micheli A, 2009, IEEE T NEURAL NETWOR, V20, P498, DOI 10.1109/TNN.2008.2010350
   MORGAN HL, 1965, J CHEM DOC, V5, P107, DOI 10.1021/c160017a018
   Oliphant TE, 2007, COMPUT SCI ENG, V9, P10, DOI 10.1109/MCSE.2007.58
   Ramsundar B., 2015, ARXIV150202072
   Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Tai Kai Sheng, 2015, ARXIV150300075
   Tox2l Challenge, 2014, TOX21 CHALL
   Unterthiner T., 2015, ARXIV150301445
   Unterthiner Thomas, 2014, ADV NEURAL INFORM PR
   Wan Li, 2013, INT C MACH LEARN
   WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005
NR 29
TC 2
Z9 2
U1 4
U2 4
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102102
DA 2019-06-15
ER

PT S
AU Feurer, M
   Springenberg, JT
   Klein, A
   Blum, M
   Eggensperger, K
   Hutter, F
AF Feurer, Matthias
   Springenberg, Jost Tobias
   Klein, Aaron
   Blum, Manuel
   Eggensperger, Katharina
   Hutter, Frank
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Efficient and Robust Automated Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.
C1 [Feurer, Matthias; Springenberg, Jost Tobias; Klein, Aaron; Blum, Manuel; Eggensperger, Katharina; Hutter, Frank] Univ Freiburg, Dept Comp Sci, Freiburg, Germany.
RP Feurer, M (reprint author), Univ Freiburg, Dept Comp Sci, Freiburg, Germany.
EM feurerm@cs.uni-freiburg.de; springj@cs.uni-freiburg.de;
   kleinaa@cs.uni-freiburg.de; mblum@cs.uni-freiburg.de;
   eggenspk@cs.uni-freiburg.de; fh@cs.uni-freiburg.de
FU German Research Foundation (DFG) under Priority Programme Autonomous
   Learning (SPP 1527) [HU 1900/3-1]; German Research Foundation (DFG)
   under Emmy Noether grant [HU 1900/2-1]; German Research Foundation (DFG)
   under BrainLinks-BrainTools Cluster of Excellence [EXC 1086]
FX This work was supported by the German Research Foundation (DFG), under
   Priority Programme Autonomous Learning (SPP 1527, grant HU 1900/3-1),
   under Emmy Noether grant HU 1900/2-1, and under the
   BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086).
CR [Anonymous], 2014, P ICML 13
   Bardenet R., P ICML 13, P199
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Brazdil P., 2009, METALEARNING APPL DA
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brochu E., 2010, ABS10122599 CORR
   Caruana R., 2004, P 21 INT C MACH LEAR, P18, DOI DOI 10.1145/1015330.1015432
   Caruana R, 2006, IEEE DATA MINING, P828
   Eggensperger K., 2013, NIPS WORKSH BAYES OP
   Feurer M., 2015, P 29 AAAI C ART INT, P1128
   Gomes TAF, 2012, NEUROCOMPUTING, V75, P3, DOI 10.1016/j.neucom.2011.07.005
   Guyon I., 2015, P IJCNN 15
   Guyon I, 2010, J MACH LEARN RES, V11, P61
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI [DOI 10.1145/1656274.1656278, 10.1145/1656274.1656278]
   Hamerly G, 2004, ADV NEUR IN, V16, P281
   Hutter F., 2011, P LION 11, P507
   Kalousis A., 2002, THESIS
   Komer B., 2014, ICML WORKSH AUTOML
   Lacoste Alexandre, 2014, P 31 INT C MACH LEAR, P611
   Michie D., 1994, MACHINE LEARNING NEU
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pfahringer B., 2000, P 17 INT C MACH LEAR, P743
   Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7
   Snoek J., 2012, ADV NEURAL INFORM PR, P2960
   Thornton C., 2013, P 19 ACM SIGKDD INT, P847, DOI [10.1145/2487575.2487629, DOI 10.1145/2487575.2487629]
   Vanschoren J., 2013, ACM SIGKDD EXPLORATI, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Yogatama D., 2014, JMLR P, P1077
NR 28
TC 2
Z9 2
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102021
DA 2019-06-15
ER

PT S
AU Kim, B
   Shah, J
   Doshi-Velez, F
AF Kim, Been
   Shah, Julie
   Doshi-Velez, Finale
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Mind the Gap: A Generative Approach to Interpretable Feature Selection
   and Extraction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection. By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.
C1 [Kim, Been; Shah, Julie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA.
RP Kim, B (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM beenkim@csail.mit.edu; julie_a_shah@csail.mit.edu;
   finale@seas.harvard.edu
CR Agrawal R., 1998, SIGMOD Record, V27, P94
   Alelyani S., 2013, DATA CLUSTERING ALGO, V29
   [Anonymous], 2012, JAMA-J AM MED ASSOC, V307, P1533
   Batmanghelich Nematollah Kayhan, 2014, CORR
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Dash M, 2000, LECT NOTES ARTIF INT, V1805, P110
   De'ath G, 2000, ECOLOGY, V81, P3178, DOI 10.2307/177409
   Doshi-Velez F, 2014, PEDIATRICS, V133, pE54, DOI 10.1542/peds.2013-0819
   Dy JG, 2004, J MACH LEARN RES, V5, P845
   Eisenstein Jacob, 2011, ICML
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fan W., 2012, ACML, P113
   Freitas A., 2014, ACM SIGKDD EXPLORATI
   Guan Y., 2011, P 28 INT C MACH LEAR, P1073
   Guerif S., 2008, FSDM
   Kemp C., 2008, PNAS
   Khoat Than, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P490, DOI 10.1007/978-3-642-33460-3_37
   Kim B., 2014, NIPS
   Kulesza A., 2010, NIPS
   Kulesza A., 2012, THESIS
   Lichman M., 2013, UCI MACHINE LEARNING
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/0033-295X.101.2.343
   Mitra P, 2002, IEEE T PATTERN ANAL, V24, P301, DOI 10.1109/34.990133
   Tsuda K., 2003, NIPS
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Williamson S., 2010, ICML
   Yu G., 2010, P 16 ACM SIGKDD INT, P763, DOI DOI 10.1145/1835804.1835901
   Zhang J, 1998, JAMA-J AM MED ASSOC, V280, P1690, DOI 10.1001/jama.280.19.1690
   Zhu J., 2009, ICML, P1257
   Zou H., 2004, J COMPUT GRAPH STAT, V15, P2006
   Zou J., 2013, NIPS
   Zou J. Y., 2012, NIPS
NR 32
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102105
DA 2019-06-15
ER

PT S
AU Kulkarni, TD
   Whitney, WF
   Kohli, P
   Tenenbaum, JB
AF Kulkarni, Tejas D.
   Whitney, William F.
   Kohli, Pushmeet
   Tenenbaum, Joshua B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Deep Convolutional Inverse Graphics Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [10]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative tests of the model's efficacy at learning a 3D rendering engine for varied object classes including faces and chairs.
C1 [Kulkarni, Tejas D.; Whitney, William F.; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA.
   [Kohli, Pushmeet] Microsoft Res, Cambridge, England.
RP Kulkarni, TD (reprint author), MIT, Cambridge, MA 02139 USA.
EM tejask@mit.edu; wwhitney@mit.edu; pkohli@microsoft.com; jbt@mit.edu
FU MIT Center for Brains, Minds, and Machines (CBMM)
FX We thank Thomas Vetter for access to the Basel face model. We are
   grateful for support from the MIT Center for Brains, Minds, and Machines
   (CBMM). We also thank Geoffrey Hinton and Ilker Yildrim for helpful
   feedback and discussions.
CR Aubry M., 2014, CVPR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cohen T., 2014, ARXIV14024437
   Desjardins G., 2012, ARXIV12105474
   Dosovitskiy A., 2015, ARXIV14115928
   Goodfellow I., 2009, ADV NEURAL INFORM PR, P646
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Kingma D.P., 2013, ARXIV13126114
   Kulkarni T. D., 2014, ARXIV14071339
   Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068
   LeCun Y., 1995, HDB BRAIN THEORY NEU, V3361
   Lee H, 2009, P ANN INT C MACH LEA, V26, P609, DOI DOI 10.1145/1553374.1553453
   Mansinghka V., 2013, ADV NEURAL INFORM PR, P1520
   Mottaghi R., 2014, IEEE C COMP VIS PATT
   Paysan P., 2009, 3D FACE MODEL POSE I
   Ranzato M., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Tang Y., 2012, INT C MACH LEARN, V1206, P6445
   Tieleman T., 2012, LECT 6 5 RMSPROP COU
   Tieleman T., 2014, THESIS
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
NR 23
TC 2
Z9 2
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101110
DA 2019-06-15
ER

PT S
AU Ma, YA
   Chen, TQ
   Fox, EB
AF Ma, Yi-An
   Chen, Tianqi
   Fox, Emily B.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Complete Recipe for Stochastic Gradient MCMC
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LANGEVIN
AB Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers-including stochastic gradient versions-based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.
C1 [Ma, Yi-An; Chen, Tianqi; Fox, Emily B.] Univ Washington, Seattle, WA 98195 USA.
RP Ma, YA (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM yianma@u.washington.edu; tqchen@cs.washington.edu;
   ebfox@stat.washington.edu
FU ONR [N00014-10-1-0746]; NSF CAREER Award [IIS-1350133]; TerraSwarm
   Research Center - MARCO; TerraSwarm Research Center - DARPA
FX This work was supported in part by ONR Grant N00014-10-1-0746, NSF
   CAREER Award IIS-1350133, and the TerraSwarm Research Center sponsored
   by MARCO and DARPA. We also thank Mr. Lei Wu for helping with the proof
   of Theorem 2 and Professors Ping Ao and Hong Qian for many discussions.
CR Ahn S., 2014, P 31 INT C MACH LEAR
   Ahn  Sungjin, 2012, P 29 INT C MACH LEAR
   Bardenet R., 2014, P 30 INT C MACH LEAR
   Betancourt M., 2015, P 31 INT C MACH LEAR
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen T., 2014, P 31 INT C MACH LEAR
   Ding N, 2014, ADV NEUR IN, V27
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Feller W, 1950, INTRO PROBABILITY TH
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Korattikara A., 2014, P 30 INT C MACH LEAR
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Patterson S., 2013, ADV NEURAL INFORM PR, V26
   Risken H., 1996, FOKKER PLANCK EQUATI
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shi JH, 2012, J STAT PHYS, V148, P579, DOI 10.1007/s10955-012-0532-8
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Xifara T, 2014, STAT PROBABIL LETT, V91, P14, DOI 10.1016/j.spl.2014.04.002
   Yin L, 2006, J PHYS A-MATH GEN, V39, P8593, DOI 10.1088/0305-4470/39/27/003
   Zwanzig R., 2001, NONEQUILIBRIUM STAT
NR 20
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102040
DA 2019-06-15
ER

PT S
AU Maystre, L
   Grossglauser, M
AF Maystre, Lucas
   Grossglauser, Matthias
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Fast and Accurate Inference of Plackett-Luce Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We show that the maximum-likelihood (ML) estimate of models derived from Luce's choice axiom (e.g., the Plackett-Luce model) can be expressed as the stationary distribution of a Markov chain. This conveys insight into several recently proposed spectral inference algorithms. We take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the Plackett-Luce model. With a simple adaptation, this algorithm can be used iteratively, producing a sequence of estimates that converges to the ML estimate. The ML version runs faster than competing approaches on a benchmark of five datasets. Our algorithms are easy to implement, making them relevant for practitioners at large.
C1 [Maystre, Lucas; Grossglauser, Matthias] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Maystre, L (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM lucas.maystre@epfl.ch; matthias.grossglauser@epfl.ch
CR BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Caron F, 2012, J COMPUT GRAPH STAT, V21, P174, DOI 10.1080/10618600.2012.638220
   Dwork C., 2001, P 10 INT C WORLD WID
   DYKSTRA O, 1960, BIOMETRICS, V16, P176, DOI 10.2307/2527550
   Elo AE, 1978, RATING CHESS PLAYERS
   FORD JR L. R., 1957, AM MATH MONTHLY, V64, P28, DOI DOI 10.2307/2308513
   Guiver J., 2009, P 26 INT C MACH LEAR
   Hajek B, 2014, ADV NEUR IN, V27
   Hastie T, 1998, ANN STAT, V26, P451
   Hunter DR, 2004, ANN STAT, V32, P384
   Kamishima T, 2009, STUD COMPUT INTELL, V165, P261
   Kumar R., 2015, P 8 ACM INT C WEB SE, P359
   Levin D. A., 2008, MARKOV CHAINS MIXING
   Luce R. D., 1959, INDIVIDUAL CHOICE BE
   McFadden D, 1974, FRONTIERS ECONOMETRI, P105, DOI DOI 10.1108/EB028592
   Negahban S., 2012, ADV NEURAL INFORM PR, V25
   Page L, 1998, TECHNICAL REPORT
   PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193
   Rajkumar A., 2014, P 31 INT C MACH LEAR
   RAO PV, 1967, J AM STAT ASSOC, V62, P194, DOI 10.2307/2282923
   Saaty T. L., 1980, ANAL HIERARCHY PROCE
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26
   Thurstone L. L., 1927, J ABNORMAL SOCIAL PS, V21, P384, DOI DOI 10.1037/H0065439
   Zermelo E, 1929, MATH Z, V29, P436, DOI 10.1007/BF01180541
NR 24
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100049
DA 2019-06-15
ER

PT S
AU Neu, G
AF Neu, Gergely
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Explore no more: Improved high-probability regret bounds for
   non-stochastic bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS
AB This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample arms from the uniform distribution at least Omega(root T) times over T rounds, which can adversely affect performance if many of the arms are suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.
C1 [Neu, Gergely] INRIA Lille Nord Europe, SequeL Team, Villeneuve Dascq, France.
RP Neu, G (reprint author), Pompeu Fabra Univ, Dept Informat & Commun Technol, Barcelona, Spain.
EM gergely.neu@gmail.com
FU INRIA; French Ministry of Higher Education and Research; FUI project
   Hermes
FX This work was supported by INRIA, the French Ministry of Higher
   Education and Research, and by FUI project Hermes. The author wishes to
   thank Haipeng Luo for catching a bug in an earlier version of the paper,
   and the anonymous reviewers for their helpful suggestions.
CR Alon N., 2012, NIPS 25, P1610
   Alon N., 2014, ARXIV14098428
   Audibert JY, 2010, J MACH LEARN RES, V11, P2785
   Audibert Jean-Yves, 2009, P 22 ANN C LEARN THE
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Bartlett P.L., 2008, P 21 ANN C LEARN THE, P335
   Beygelzimer A, 2011, P 14 INT C ART INT S, V15, P19
   Bubeck S., 2012, MINIMAX POLICIES ONL
   Bubeck S, 2012, REGRET ANAL STOCHAST
   Cesa-Bianchi N., 2012, NIPS P, V25, P989
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   FREEDMAN DA, 1975, ANN PROBAB, V3, P100, DOI 10.1214/aop/1176996452
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hannan J., 1957, ANN MATH STUD, V3, P97
   Hazan E, 2011, J MACH LEARN RES, V12, P1287
   Hazan Elad, 2014, J MACHINE LEARNING R, V35, P408
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Kocak T., 2014, ADV NEURAL INFORM PR, P613
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Mannor S., 2011, NEURAL INFORM PROCES
   McMahan H. B., 2009, COLT
   Neu  G., 2015, COLT, V40, P1360
   Rakhlin Alexander, 2013, COLT, P993
   Seldin Y., 2012, P WORKSH ON LIN TRAD, V2
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
NR 26
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100100
DA 2019-06-15
ER

PT S
AU Ren, JSJ
   Xu, L
   Yan, Q
   Sun, WX
AF Ren, Jimmy S. J.
   Xu, Li
   Yan, Qiong
   Sun, Wenxiu
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Shepard Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Deep learning has recently been introduced to the field of low-level computer vision and image processing. Promising results have been obtained in a number of tasks including super-resolution, inpainting, deconvolution, filtering, etc. However, previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators. We found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation (TVI). In this paper, we draw on Shepard interpolation and design Shepard Convolutional Neural Networks (ShCNN) which efficiently realizes endto- end trainable TVI operators in the network. We show that by adding only a few feature maps in the new Shepard layers, the network is able to achieve stronger results than a much deeper architecture. Superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive.
C1 [Ren, Jimmy S. J.; Xu, Li; Yan, Qiong; Sun, Wenxiu] SenseTime Grp Ltd, Hong Kong, Hong Kong, Peoples R China.
RP Ren, JSJ (reprint author), SenseTime Grp Ltd, Hong Kong, Hong Kong, Peoples R China.
EM rensijie@sensetime.com; xuli@sensetime.com; yanqiong@sensetime.com;
   sunwenxiu@sensetime.com
CR Bevilacqua M, 2012, BMVC
   Burger H. C., 2012, CVPR
   Chang H., 2004, CVPR
   Dong C., 2014, ECCV
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Eigen D, 2013, ICCV
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y., 1998, P IEEE
   Shepard D., 1968, 23 ACM NAT C
   Sun Y., 2015, ARXIV150200873
   Szegedy C., 2015, CVPR
   Timofte R., 2014, ACCV
   Timofte R., 2013, ICCV
   Xie J, 2012, NIPS
   Xu  L., 2014, NIPS
   Xu L., 2015, ICML
   Zeyde R, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
NR 17
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101034
DA 2019-06-15
ER

PT S
AU Sculley, D
   Holt, G
   Golovin, D
   Davydov, E
   Phillips, T
   Ebner, D
   Chaudhary, V
   Young, M
   Crespo, JF
   Dennison, D
AF Sculley, D.
   Holt, Gary
   Golovin, Daniel
   Davydov, Eugene
   Phillips, Todd
   Ebner, Dietmar
   Chaudhary, Vinay
   Young, Michael
   Crespo, Jean-Francois
   Dennison, Dan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Hidden Technical Debt in Machine Learning Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.
C1 [Sculley, D.; Holt, Gary; Golovin, Daniel; Davydov, Eugene; Phillips, Todd; Ebner, Dietmar; Chaudhary, Vinay; Young, Michael; Crespo, Jean-Francois; Dennison, Dan] Google Inc, Mountain View, CA 94043 USA.
RP Sculley, D (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM dsculley@google.com; gholt@google.com; dgg@google.com;
   edavydov@google.com; toddphillips@google.com; ebner@google.com;
   vchaudhary@google.com; mwyoung@google.com; jfcrespo@google.com;
   dennison@google.com
CR Ananthanarayanan R., 2013, SIGMOD, P577, DOI DOI 10.1145/2463676.2465272
   Anonymous A., SE4ML SOFTW ENG MACH
   Bottou L., 2013, J MACHINE LEARNING R, V14
   Brown W., 1998, ANTIPATTERNS REFACTO
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   Dalessandro B., 2014, P 20 ACM SIGKDD INT, P1573
   Fowler M., 1999, REFACTORING IMPROVIN
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Li M, 2014, P 11 USENIX S OP SYS, P583
   Lin J., 2013, ACM SIGKDD EXPLORATI, V14, P6
   McMahan H. B., 2013, 19 ACM SIGKDD INT C
   Morgenthaler J. D., 2012, P 3 INT WORKSH MAN T
   Sculley D., 2011, P 17 ACM SIGKDD INT
   Securities and E. Commission, 2013, SEC CHARG KNIGHT CAP
   Spector A., 2012, COMMUNICATIONS ACM, V55
   Zheng A., SE4ML SOFTW ENG MACH
NR 16
TC 2
Z9 2
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100024
DA 2019-06-15
ER

PT S
AU Shi, XJ
   Chen, ZR
   Wang, H
   Yeung, DY
   Wong, WK
   Woo, WC
AF Shi, Xingjian
   Chen, Zhourong
   Wang, Hao
   Yeung, Dit-Yan
   Wong, Wai-kin
   Woo, Wang-chun
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convolutional LSTM Network: A Machine Learning Approach for
   Precipitation Nowcasting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.
C1 [Shi, Xingjian; Chen, Zhourong; Wang, Hao; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Wong, Wai-kin; Woo, Wang-chun] Hong Kong Observ, Hong Kong, Hong Kong, Peoples R China.
RP Shi, XJ (reprint author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM xshiab@cse.ust.hk; zchenbb@cse.ust.hk; hwangaz@cse.ust.hk;
   dyyeung@cse.ust.hk; wkwong@hko.gov.hk; wcwoo@hko.gov.hk
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y., 2015, DEEP LEARNING
   Bergstra J, 2010, P PYTH SCI COMP C SC, P3
   Bridson R., 2008, FLUID SIMULATION COM
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25
   Cheung P, 2012, 3 WMO INT S NOWC VER, P6
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Donahue J., 2015, CVPR
   Douglas R. H., 1990, RES ACTION COMMUNITY, P61
   Germann U, 2002, MON WEATHER REV, V130, P2859, DOI 10.1175/1520-0493(2002)130<2859:SDOTPO>2.0.CO;2
   Graves A, 2013, ARXIV13080850
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Karpathy A., 2015, CVPR
   Klein Benjamin, 2015, CVPR
   Li PW., 2000, SWIRLS AN EVOLVING N
   Long  J., 2015, CVPR
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Ranzato M, 2014, ARXIV14126604
   Reyniers M, 2008, QUANTITATIVE PRECIPI
   Sakaino H, 2013, IEEE T GEOSCI REMOTE, V51, P3023, DOI 10.1109/TGRS.2012.2212201
   Srivastava N, 2015, ICML
   Sun JZ, 2014, B AM METEOROL SOC, V95, P409, DOI 10.1175/BAMS-D-11-00263.1
   Sutskever I., 2014, P 27 INT C NEUR INF, V3104, P3112, DOI DOI 10.1021/acs.analchem.7b05329
   Tieleman T., 2012, COURSERA COURSE NEUR, V4
   Woo W. C., 2014, 27 C SEV LOC STORMS
   Xu K, 2015, ICML
NR 26
TC 2
Z9 2
U1 9
U2 9
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102103
DA 2019-06-15
ER

PT S
AU Theis, L
   Bethge, M
AF Theis, Lucas
   Bethge, Matthias
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Generative Image Modeling Using Spatial LSTMs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.
C1 [Theis, Lucas; Bethge, Matthias] Univ Tubingen, D-72076 Tubingen, Germany.
RP Theis, L (reprint author), Univ Tubingen, D-72076 Tubingen, Germany.
EM lucas@bethgelab.org; matthias@bethgelab.org
FU German Research Foundation (DFG) [1527, BE 3848/2-1]
FX The authors would like to thank Aaron van den Oord for insightful
   discussions and Wieland Brendel, Christian Behrens, and Matthias
   Kummerer for helpful input on this paper. This study was financially
   supported by the German Research Foundation (DFG; priority program 1527,
   BE 3848/2-1).
CR Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Brodatz P, 1966, TEXTURES PHOTOGRAPHI
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Denton E, 2015, ADV NEUR IN, V28
   Domke J., 2008, CVPR
   Donahue J., 2014, ICML 31
   Gerhard H. E., 2015, BIOL INSPIRED COMPUT
   Goodfellow IJ, 2014, ADV NEUR IN, V27
   Graves A., 2009, ADV NEURAL INFORM PR, V22
   Gregor Karol, 2015, P 32 INT C MACH LEAR
   Gregor Karol, 2014, P 31 INT C MACH LEAR
   Heess N., 2009, BMCV
   Hinton G., 2006, NEURAL COMP
   Hochreiter  S, 1997, NEURAL COMPUTATION, V9
   Hosseini R., 2010, VIS RES
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   Jia Y., 2014, ARXIV14085093
   Kingma DP, 2014, ADV NEUR IN, V27
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V25
   Larochelle H., 2011, P 14 INT C ART INT S
   Lee H., 2009, ICML 26
   Li Y., 2015, ICML 32
   Martin  D., 2001, ICCV
   Matheron G., 1968, TECHNICAL REPORT
   Mumford D., 2001, INT J COMPUTER VISIO
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Ngiam J., 2011, ICML 28
   Osindero S., 2008, ADV NEURAL INFORM PR, V20
   Ranzato M. A., 2015, ARXIV14126604V2
   Ranzato M, 2011, PROC CVPR IEEE
   Robinson A. J., 1987, TECHNICAL REPORT
   Roth S., 2009, INT J COMPUTER VISIO, V82
   Simonyan  K., 2015, INT C LEARN REPR
   Sohl-Dickstein J., 2015, ICML 32
   Srivastava N., 2014, JMLR
   Srivastava N., 2015, P 32 INT C MACH LEAR
   Sundermeyer M., 2010, INTERSPEECH
   Sutskever I, 2014, ADV NEUR IN, V27
   Theis L., 2012, ADV NEURAL INFORM PR, V25
   Theis L., 2011, JMLR
   Theis L, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039857
   Tierney L., 1994, ANN STAT
   Uria B., 2014, ICML 31
   Uria B., 2013, ADV NEURAL INFORM PR, V26
   Van den Oord A, 2014, ADV NEUR IN, V27
   van den Oord A, 2014, J MACH LEARN RES, V15, P2061
   van Hateren J. H., 1998, P ROYAL SOC B, V265
   Zoran D., 2012, NIPS
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 50
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100005
DA 2019-06-15
ER

PT S
AU Dauphin, YN
   Pascanu, R
   Gulcehre, C
   Cho, K
   Ganguli, S
   Bengio, Y
AF Dauphin, Yann N.
   Pascanu, Razvan
   Gulcehre, Caglar
   Cho, Kyunghyun
   Ganguli, Surya
   Bengio, Yoshua
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Identifying and attacking the saddle point problem in high-dimensional
   non-convex optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID GRADIENT DESCENT
AB A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.
C1 [Dauphin, Yann N.; Pascanu, Razvan; Gulcehre, Caglar; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
   [Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA.
RP Dauphin, YN (reprint author), Univ Montreal, Montreal, PQ, Canada.
EM dauphiya@iro.umontreal.ca; r.pascanu@gmail.com;
   gulcehrc@iro.umontreal.ca; kyunghyun.cho@umontreal.ca;
   sganguli@standford.edu; yoshua.bengio@umontreal.ca
FU CIFAR; Canada Research Chairs; DeepMind Google Fellowship; Burroughs
   Wellcome Foundation; Sloan Foundation
FX We would like to thank the developers of Theano (Bergstra et al., 2010;
   Bastien et al., 2012). We would also like to thank CIFAR, and Canada
   Research Chairs for funding, and Compute Canada, and Calcul Quebec for
   providing computational resources. Razvan Pascanu is supported by a
   DeepMind Google Fellowship. Surya Ganguli thanks the Burroughs Wellcome
   and Sloan Foundations for support.
CR BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Bastien F., 2012, THEANO NEW FEATURES
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bray AJ, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.150201
   Callahan J., 2010, UNDERGRADUATE TEXTS
   Fyodorov YV, 2007, J STAT PHYS, V129, P1081, DOI 10.1007/s10955-007-9386-x
   Inoue M, 2003, J PHYS SOC JPN, V72, P805, DOI 10.1143/JPSJ.72.805
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Mizutani E., 2010, ADV NEURAL INFORM PR, P1669
   Murray W., 2010, TECHNICAL REPORT
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Parisi G., 2007, ARXIV07060094
   Pascanu R., 2014, 14054604 ARXIV
   Pascanu R., 2014, ICML
   Pascanu Razvan, 2014, INT C LEARN REPR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rattray M, 1998, PHYS REV LETT, V81, P5461, DOI 10.1103/PhysRevLett.81.5461
   SAAD D, 1995, PHYS REV E, V52, P4225, DOI 10.1103/PhysRevE.52.4225
   Saxe  A.M., 2014, INT C LEARN REPR
   Sohl- Dickstein J., 2014, ICML
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Vinyals O., 2012, AISTATS
   WIGNER EP, 1958, ANN MATH, V67, P325, DOI 10.2307/1970008
NR 25
TC 2
Z9 2
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102047
DA 2019-06-15
ER

PT S
AU Guo, XX
   Singh, S
   Lee, H
   Lewis, R
   Wang, XS
AF Guo, Xiaoxiao
   Singh, Satinder
   Lee, Honglak
   Lewis, Richard
   Wang, Xiaoshi
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo
   Tree Search Planning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.
C1 [Guo, Xiaoxiao; Singh, Satinder; Lee, Honglak; Wang, Xiaoshi] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
   [Lewis, Richard] Univ Michigan, Dept Psychol, Ann Arbor, MI 48109 USA.
RP Guo, XX (reprint author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
EM guoxiao@umich.edu; baveja@umich.edu; honglak@umich.edu; rickl@umich.edu;
   xiaoshiw@umich.edu
FU NSF [IIS-1148668]
FX This work was supported in part by NSF grant IIS-1148668. Any opinions,
   findings, conclusions, or recommendations expressed here are those of
   the authors and do not necessarily reflect the views of the sponsors.
CR Bellemare M. G., 2012, ADV NEURAL INF PROCE, P2222
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Ciregan D., 2012, PROC CVPR IEEE, P3642, DOI [10.1109/CVPR.2012.6248110, DOI 10.1109/CVPR.2012.6248110]
   Erhan D, 2009, TECHNICAL REPORT
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hausknecht M, 2012, PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P217, DOI 10.1145/2330163.2330195
   Karpathy A., 2014, IEEE C COMP VIS PATT
   Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2009, P ANN INT C MACH LEA, V26, P609, DOI DOI 10.1145/1553374.1553453
   Mnih V., 2013, DEEP LEARN NEUR INF
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Ross Stephane, 2011, P 14 INT C ART INT S
   Schmidhuber J., 2014, NEURAL NETWORKS
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
NR 20
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101092
DA 2019-06-15
ER

PT S
AU Kingma, DP
   Rezende, DJ
   Mohamed, S
   Welling, M
AF Kingma, Diederik P.
   Rezende, Danilo J.
   Mohamed, Shakir
   Welling, Max
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Semi-supervised Learning with Deep Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The ever-increasing size of modem data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.
C1 [Kingma, Diederik P.; Welling, Max] Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.
   [Rezende, Danilo J.; Mohamed, Shakir] Google Deepmind, London, England.
RP Kingma, DP (reprint author), Univ Amsterdam, Machine Learning Grp, Amsterdam, Netherlands.
EM D.P.Kingma@uva.nl; dlanilor@google.com; shakir@google.com;
   M.Welling@uva.nl
CR Adams R. P., 2009, P INT C MACH LEARN I
   Blum A., 2004, P INT C MACH LEARN I
   Dayan P., 2000, HDB BRAIN THEORY NEU, V44
   Dietterich T., 1995, CS9501101 ARXIV
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fergus R., 2009, ADV NEURAL INFORM PR
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kemp C., 2003, ADV NEURAL INFORM PR
   Kingma D. P., 2014, P INT C LEARN REPR I
   Li P., 2009, P ESANN2009, P11
   Liang P, 2005, THESIS
   Liu Y., 2013, P INT
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Pal C., 2005, ADV NEURAL INFORM PR
   Pitelis Nikolaos, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P565, DOI 10.1007/978-3-662-44851-9_36
   Ranzato M., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Rezende D. J., 2014, JMLR W CP, V32
   Rifai S., 2011, ADV NEURAL INFORM PR, P2294
   Rosenberg Chuck, 2005, P 7 IEEE WORKSH APPL
   Shi M, 2011, BIOINFORMATICS, V27, P3017, DOI 10.1093/bioinformatics/btr502
   Stuhlmuller A., 2013, ADV NEURAL INFORM PR, P3048
   Tang Y., 2013, ADV NEURAL INFORM PR, P530
   Wang Y., 2009, ADV NEURAL INFORM PR, P2008, DOI DOI 10.1097/EDE.0B013E318231D67A
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Zhu X, 2006, TECHNICAL REPORT
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 26
TC 2
Z9 2
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101023
DA 2019-06-15
ER

PT S
AU Lieder, F
   Plunkett, D
   Hamrick, JB
   Russell, SJ
   Hay, NJ
   Griffiths, TL
AF Lieder, Falk
   Plunkett, Dillon
   Hamrick, Jessica B.
   Russell, Stuart J.
   Hay, Nicholas J.
   Griffiths, Thomas L.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Algorithm selection by rational metareasoning as a model of human
   strategy selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.
C1 [Lieder, Falk] Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA.
   [Plunkett, Dillon; Hamrick, Jessica B.; Griffiths, Thomas L.] Univ Calif Berkeley, Dept Psychol, Berkeley, CA USA.
   [Russell, Stuart J.; Hay, Nicholas J.] Univ Calif Berkeley, EECS Dept, Berkeley, CA USA.
RP Lieder, F (reprint author), Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA.
EM falk.lieder@berkeley.edu; dillonplunkett@berkeley.edu;
   jhamrick@berkeley.edu; russell@cs.berkeley.edu
OI Hamrick, Jessica/0000-0002-3860-0429
FU ONR MURI [N00014-13-1-0341]
FX This work was supported by ONR MURI N00014-13-1-0341.
CR Erev I, 2005, PSYCHOL REV, V112, P912, DOI 10.1037/0033-295X.112.4.912
   Gigerenzer G., 2002, BOUNDED RATIONALITY
   Gonzalez C, 2011, PSYCHOL REV, V118, P523, DOI 10.1037/a0024558
   Guo H., 2003, THESIS
   Harada D., 1998, NIPS 98 WORKSH ABSTR
   Jaakkola T., 1997, 6 INT WORKSH ART INT
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Kotthoff L., 2014, AI MAGAZINE
   Lagoudakis M. G., 2001, P 2001 AAAI FALL S S
   Marewski JN, 2014, WIRES COGN SCI, V5, P39, DOI 10.1002/wcs.1265
   PAYNE JW, 1988, J EXP PSYCHOL LEARN, V14, P534, DOI 10.1037//0278-7393.14.3.534
   Penny WD, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059655
   Rice J. R., 1976, Advances in computers, vol.15, P65, DOI 10.1016/S0065-2458(08)60520-3
   Rieskamp J, 2006, J EXP PSYCHOL GEN, V135, P207, DOI 10.1037/0096-3445.135.2.207
   RUSSELL S, 1991, ARTIF INTELL, V49, P361, DOI 10.1016/0004-3702(91)90015-C
   Shrager J, 1998, PSYCHOL SCI, V9, P405, DOI 10.1111/1467-9280.00076
   Siegler RS, 1999, TRENDS COGN SCI, V3, P430, DOI 10.1016/S1364-6613(99)01372-8
   Smith-Miles KA, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456656
NR 18
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103003
DA 2019-06-15
ER

PT S
AU Mnih, V
   Heess, N
   Graves, A
   Kavukcuoglu, K
AF Mnih, Volodymyr
   Heess, Nicolas
   Graves, Alex
   Kavukcuoglu, Koray
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Recurrent Models of Visual Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID EYE-MOVEMENTS
AB Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.
C1 [Mnih, Volodymyr; Heess, Nicolas; Graves, Alex; Kavukcuoglu, Koray] Google DeepMind, London, England.
RP Mnih, V (reprint author), Google DeepMind, London, England.
EM vmnih@google.com; heess@google.com; gravesa@google.com;
   korayk@google.com
CR Alexe B, 2010, CVPR
   Alexe Bogdan, 2012, NIPS
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Butko N., 2009, CVPR
   Butko NJ, 2008, INT C DEVEL LEARN, P139, DOI 10.1109/DEVLRN.2008.4640819
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Felzenszwalb P. F., 2010, CVPR
   Girshick R. B., 2013, CORR, V1311, P2524
   Hayhoe M, 2005, TRENDS COGN SCI, V9, P188, DOI 10.1016/j.tics.2005.02.009
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lampert C., 2008, CVPR
   Larochelle H., 2010, NIPS
   Mathe S., 2013, NIPS
   Paletta Lucas, 2005, CVPR
   Ranzato M., 2014, ARXIV E PRINTS
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Stanley Kenneth O., 2004, GECCO
   SUTTON RS, 2000, NIPS, V12, P1057
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   van de Sande K.E.A., 2011, ICCV
   Viola P., 2001, CVPR
   Wierstra D., 2007, ICANN
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 26
TC 2
Z9 2
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102103
DA 2019-06-15
ER

PT S
AU Naesseth, CA
   Lindsten, F
   Schott, TB
AF Naesseth, Christian A.
   Lindsten, Fredrik
   Schott, Thomas B.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Sequential Monte Carlo for Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SIMULATION METHODS; PARTICLE; INFERENCE
AB We propose a new framework for how to use sequential Monte Carlo (SMC) algorithms for inference in probabilistic graphical models (PGM). Via a sequential decomposition of the PGM we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces. By targeting these auxiliary distributions using SMC we are able to approximate the full joint distribution defined by the PGM. One of the key merits of the SMC sampler is that it provides an unbiased estimate of the partition function of the model. We also show how it can be used within a particle Markov chain Monte Carlo framework in order to construct high-dimensional block-sampling algorithms for general PGMs.
C1 [Naesseth, Christian A.] Linkoping Univ, Div Automat Control, Linkoping, Sweden.
   [Lindsten, Fredrik] Univ Cambridge, Dept Engn, Cambridge, England.
   [Schott, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.
RP Naesseth, CA (reprint author), Linkoping Univ, Div Automat Control, Linkoping, Sweden.
EM chran60@isy.liu.se; fsm12@cam.ac.uk; thomas.schon@it.uu.se
FU project: Learning of complex dynamical systems - Swedish Research
   Council [637-2014-466]; project: Probabilistic modeling of dynamical
   systems - Swedish Research Council [621-2013-5524]
FX We would like to thank Iain Murray for his kind and very prompt help in
   providing the data for the LDA example. This work was supported by the
   projects: Learning of complex dynamical systems (Contract number:
   637-2014-466) and Probabilistic modeling of dynamical systems (Contract
   number: 621-2013-5524), both funded by the Swedish Research Council.
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bouchard-Cote A, 2012, SYST BIOL, V61, P579, DOI 10.1093/sysbio/syr131
   Briers M., 2005, P 8 INT C INF FUS PH
   Buntine W, 2009, LECT NOTES ARTIF INT, V5828, P51, DOI 10.1007/978-3-642-05224-8_6
   Carbonetto P., 2007, ADV NEURAL INFORM PR, V19
   Del Moral P, 2004, PROB APPL S
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Doucet A., 2011, OXFORD HDB NONLINEAR
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Everitt RG, 2012, J COMPUT GRAPH STAT, V21, P940, DOI 10.1080/10618600.2012.687493
   Fearnhead P, 2003, J R STAT SOC B, V65, P887, DOI 10.1111/1467-9868.00421
   Frank A., 2009, ADV NEURAL INFORM PR, P826
   Hamze F., 2004, P 20 C UNC ART INT U
   Hamze F., 2005, ADV NEURAL INFORM PR
   Ihler A. T., 2009, P INT C ART INT STAT
   Isard M., 2003, P C COMP VIS PATT RE
   Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026
   Kosterlitz JM, 1973, J PHYS C SOLID STATE, V6, P1181, DOI 10.1088/0022-3719/6/7/010
   Lindsten F, 2014, J MACH LEARN RES, V15, P2145
   Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045
   Naesseth C. A., 2014, P IEEE INF THEOR WOR
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Pitt MK, 2012, J ECONOMETRICS, V171, P134, DOI 10.1016/j.jeconom.2012.06.004
   Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179
   Robert C. P., 2004, MONTE CARLO STAT MET
   Scott G. S., 2009, P 16 INT C ART INT S, P1105
   Sudderth E. B., 2003, P C COMP VIS PATT RE
   Sudderth EB, 2010, COMMUN ACM, V53, P95, DOI 10.1145/1831407.1831431
   Tomita Y, 2002, PHYS REV B, V65, DOI 10.1103/PhysRevB.65.184405
   Wallach Hanna M., 2009, P 26 ANN INT C MACH, P1105, DOI DOI 10.1145/1553374.1553515
NR 31
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103021
DA 2019-06-15
ER

PT S
AU Needell, D
   Srebro, N
   Ward, R
AF Needell, Deanna
   Srebro, Nathan
   Ward, Rachel
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Stochastic Gradient Descent, Weighted Sampling, and the Randomized
   Kaczmarz algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We improve a recent guarantee of Bach and Moulines on the linear convergence of SOD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SOD can improve convergence also in other scenarios. Our results are based on a connection between SOD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.
C1 [Needell, Deanna] Claremont Mckenna Coll, Dept Math Sci, Claremont, CA 91711 USA.
   [Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
   [Srebro, Nathan] Technion, Dept Comp Sci, Haifa, Israel.
   [Ward, Rachel] Univ Texas Austin, Dept Math, Austin, TX 78712 USA.
RP Needell, D (reprint author), Claremont Mckenna Coll, Dept Math Sci, Claremont, CA 91711 USA.
EM dneedell@cmc.edu; nati@ttic.edu; rward@math.utexas.edu
CR BACH F., 2012, ADV NEURAL INF PROCE, V25, P2672
   Bach F., 2011, ADV NEURAL INFORM PR
   CENSOR Y, 1983, NUMER MATH, V41, P83, DOI 10.1007/BF01396307
   Foygel R., 2011, 24 ANN C LEARN THEOR
   HANKE M, 1990, LINEAR ALGEBRA APPL, V130, P83, DOI 10.1016/0024-3795(90)90207-S
   Herman GT, 2009, ADV PATTERN RECOGNIT, P1, DOI 10.1007/978-1-84628-723-7
   HOUNDFIELD GN, 1973, BRIT J RADIOL, V46, P1016, DOI 10.1259/0007-1285-46-552-1016
   Kaczmarz S., 1937, B ACAD POLON SCI L A, V35, P335
   NATTERER F., 2001, CLASSICS APPL MATH, V32, DOI [10.1137/1.9780898719284, DOI 10.1137/1.9780898719284]
   Needell D, 2013, J FOURIER ANAL APPL, V19, P256, DOI 10.1007/s00041-012-9248-z
   Needell D, 2010, BIT, V50, P395, DOI 10.1007/s10543-010-0265-5
   Nemirovski A., 2005, EFFICIENT METHODS CO
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Srebro N., 2010, ADV NEURAL INFORM PR
   Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4
   TANABE K, 1971, NUMER MATH, V17, P203, DOI 10.1007/BF01436376
   Whitney M. T., 1967, SIAM J NUMER ANAL, V4, P109
   Zhao P., 2014, STOCHASTIC OPT UNPUB
NR 22
TC 2
Z9 2
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101026
DA 2019-06-15
ER

PT S
AU Su, WJ
   Boyd, S
   Candes, EJ
AF Su, Weijie
   Boyd, Stephen
   Candes, Emmanuel J.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Differential Equation for Modeling Nesterov's Accelerated Gradient
   Method: Theory and Insights
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SYSTEMS
AB We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.
C1 [Su, Weijie; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Boyd, Stephen] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA.
RP Su, WJ (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
EM wjsu@stanford.edu; boyd@stanford.edu; candes@stanford.edu
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Becker S, 2011, SIAM J IMAGING SCI, V4, P1, DOI 10.1137/090756855
   Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5
   Bloch A., 1994, HAMILTONIAN GRADIENT, V3
   BRANIN FH, 1972, IBM J RES DEV, V16, P504, DOI 10.1147/rd.165.0504
   BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054
   Hauser R, 2005, SIAM J OPTIMIZ, V15, P915, DOI 10.1137/S1052623403432633
   Helmke U., 1996, P IEEE, V84, P907
   Leader J.J, 2004, NUMERICAL ANAL SCI C
   Monteiro R., 2012, ADAPTIVE ACCELERATED
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y, 2007, CORE DISCUSSION PAPE
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Yurii, 2004, APPL OPTIMIZATION, V87, pxviii
   O'Donoghue B., 2013, FDN COMPUT MATH
   Ou Y.-G., 2014, INT J COMPUT MATH, P1
   Rockafellar R. T., 1997, PRINCETON LANDMARKS
   Schropp J, 2000, NUMER FUNC ANAL OPT, V21, P537, DOI 10.1080/01630560008816971
   Tseng P., 2008, SIAM J UNPUB
   Tseng P, 2010, MATH PROGRAM, V125, P263, DOI 10.1007/s10107-010-0394-2
NR 20
TC 2
Z9 2
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100101
DA 2019-06-15
ER

PT S
AU Bartlett, PL
   Jordan, MI
   McAuliffe, JD
AF Bartlett, PL
   Jordan, MI
   McAuliffe, JD
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Large margin classifiers: convex loss, low noise, and convergence rates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Many classification algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function-that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a finite-dimensional base class.
C1 Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.
RP Bartlett, PL (reprint author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.
CR BARLETT PL, 2003, 638 UC BERK DEP STAT
   Boyd S., 2003, CONVEX OPTIMIZATION
   JIANG W, 2003, IN PRESS ANN STAT
   Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118, DOI 10.1109/18.556601
   Lin Y., 2001, 1044R U WISC DEP STA
   LUGOSI G, 2003, IN PRESS ANN STAT
   MANNOR S, 2002, P ANN C COMP LEARN T, P319
   Mendelson S, 2002, IEEE T INFORM THEORY, V48, P1977, DOI 10.1109/TIT.2002.1013137
   STENWART I, 2002, 0203 U JEN DEP MACH
   TSYBAKOV A, 2001, PMA682 U PAR 6
   ZHANG T, 2003, IN PRESS ANN STAT
NR 11
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1173
EP 1180
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500146
DA 2019-06-15
ER

PT S
AU Beardsley, SA
   Vaina, LM
AF Beardsley, SA
   Vaina, LM
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A functional architecture for motion pattern processing in MSTd
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID EXTRASTRIATE AREA MST; OPTIC FLOW STIMULI; DIRECTION DISCRIMINATION;
   MACAQUE MONKEY; NEURONS; SENSITIVITY; INTEGRATION; MECHANISMS; FIELD;
   MODEL
AB Psychophysical studies suggest the existence of specialized detectors for component motion patterns (radial, circular, and spiral), that are consistent with the visual motion properties of cells in the dorsal medial superior temporal area (MSTd) of non-human primates. Here we use a biologically constrained model of visual motion processing in MSTd, in conjunction with psychophysical performance on two motion pattern tasks, to elucidate the computational mechanisms associated with the processing of widefield motion patterns encountered during self-motion. In both tasks discrimination thresholds varied significantly with the type of motion pattern presented, suggesting perceptual correlates to the preferred motion bias reported in MSTd. Through the model we demonstrate that while independently responding motion pattern units are capable of encoding information relevant to the visual motion tasks, equivalent psychophysical performance can only be achieved using interconnected neural populations that systematically inhibit non-responsive units. These results suggest the cyclic trends in psychophysical performance may be mediated, in part, by recurrent connections within motion pattern responsive areas whose structure is a function of the similarity in preferred motion patterns and receptive field locations between units.
C1 Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA.
RP Beardsley, SA (reprint author), Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA.
CR Beardsley SA, 2001, J COMPUT NEUROSCI, V10, P255, DOI 10.1023/A:1011264014799
   Burr DC, 1998, VISION RES, V38, P1731, DOI 10.1016/S0042-6989(97)00346-5
   CELEBRINI S, 1995, J NEUROPHYSIOL, V73, P437
   CELEBRINI S, 1994, J NEUROSCI, V14, P4109
   DUFFY CJ, 1991, J NEUROPHYSIOL, V65, P1346
   DUFFY CJ, 1995, J NEUROSCI, V15, P5192
   GILBERT CD, 1992, NEURON, V9, P1, DOI 10.1016/0896-6273(92)90215-Y
   GRAZIANO MSA, 1994, J NEUROSCI, V14, P54
   Koechlin E, 1999, BIOL CYBERN, V80, P25, DOI 10.1007/s004220050502
   Malach R, 1997, CEREB CORTEX, V7, P386, DOI 10.1093/cercor/7.4.386
   Matthews N, 1999, VISION RES, V39, P2205, DOI 10.1016/S0042-6989(98)00300-9
   Meese TS, 2002, VISION RES, V42, P1073, DOI 10.1016/S0042-6989(02)00058-5
   STEMMLER M, 1995, SCIENCE, V269, P1877, DOI 10.1126/science.7569930
   TANAKA K, 1989, J NEUROPHYSIOL, V62, P626
NR 14
TC 2
Z9 2
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1451
EP 1458
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500180
DA 2019-06-15
ER

PT S
AU Bonin, V
   Mante, V
   Carandini, M
AF Bonin, V
   Mante, V
   Carandini, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Nonlinear processing in LGN neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID LATERAL GENICULATE-NUCLEUS; RETINAL GANGLION-CELLS; CAT STRIATE CORTEX;
   CONTRAST; SELECTIVITY; INHIBITION; LENGTH
AB According to a widely held view, neurons in lateral geniculate nucleus (LGN) operate on visual stimuli in a linear fashion. There is ample evidence, however, that LGN responses are not entirely linear. To account for nonlinearities we propose a model that synthesizes more than 30 years of research in the field. Model neurons have a linear receptive field, and a nonlinear, divisive suppressive field. The suppressive field computes local root-mean-square contrast. To test this model we recorded responses from LGN of anesthetized paralyzed cats. We estimate model parameters from a basic set of measurements and show that the model can accurately predict responses to novel stimuli. The model might serve as the new standard model of LGN responses. It specifies how visual processing in LGN involves both linear filtering and divisive gain control.
C1 Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA.
RP Bonin, V (reprint author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.
CR BONDS AB, 1989, VISUAL NEUROSCI, V2, P41, DOI 10.1017/S0952523800004314
   BONIN V, 2002, ABSTR VIEW IT PLANN
   Cai DQ, 1997, J NEUROPHYSIOL, V78, P1045
   Cavanaugh JR, 2002, J NEUROPHYSIOL, V88, P2547, DOI 10.1152/jn.00693.2001
   CLELAND BG, 1983, J NEUROSCI, V3, P108
   Dan Y, 1996, J NEUROSCI, V16, P3351
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   Freeman TCB, 2002, NEURON, V35, P759, DOI 10.1016/S0896-6273(02)00819-X
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   HUBEL DH, 1961, J PHYSIOL-LONDON, V155, P385, DOI 10.1113/jphysiol.1961.sp006635
   JONES HE, 1991, J PHYSIOL-LONDON, V444, P329, DOI 10.1113/jphysiol.1991.sp018881
   LEVICK WR, 1972, INVEST OPHTH VISUAL, V11, P302
   MANTE V, 2002, ABSTR VIEW IT PLANN
   RODIECK R. W., 1965, VISION RES, V5, P583, DOI 10.1016/0042-6989(65)90033-7
   Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526
   SCLAR G, 1990, VISION RES, V30, P1, DOI 10.1016/0042-6989(90)90123-3
   SHAPLEY RM, 1978, J PHYSIOL-LONDON, V285, P275, DOI 10.1113/jphysiol.1978.sp012571
   Solomon SG, 2002, J NEUROSCI, V22, P338, DOI 10.1523/JNEUROSCI.22-01-00338.2002
NR 18
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1443
EP 1450
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500179
DA 2019-06-15
ER

PT S
AU Chudova, D
   Hart, C
   Mjolsness, E
   Smyth, P
AF Chudova, D
   Hart, C
   Mjolsness, E
   Smyth, P
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Gene expression clustering with functional mixture models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We propose a functional mixture model for simultaneous clustering and alignment of sets of curves measured on a discrete time grid. The model is specifically tailored to gene expression time course data. Each functional cluster center is a nonlinear combination of solutions of a simple linear differential equation that describes the change of individual mRNA levels when the synthesis and decay rates are constant. The mixture of continuous time parametric functional forms allows one to (a) account for the heterogeneity in the observed profiles, (b) align the profiles in time by estimating real-valued time shifts, (c) capture the synthesis and decay of mRNA in the course of an experiment, and (d) regularize noisy profiles by enforcing smoothness in the mean curves. We derive an EM algorithm for estimating the parameters of the model, and apply the proposed approach to the set of cycling genes in yeast. The experiments show consistent improvement in predictive power and within cluster variance compared to regular Gaussian mixtures.
C1 Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
RP Chudova, D (reprint author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
CR BARJOSEPH Z, 2002, 6 ANN INT C RES COMP, P39
   Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8
   Chudova D., 2003, P 9 ACM SIGKDD INT C, P79
   DESARBO WS, 1988, J CLASSIF, V5, P249, DOI 10.1007/BF01897167
   Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863
   Gaffney SJ, 2003, P 9 INT WORKSH ART I
   Gibson M., 2001, COMPUTATIONAL METHOD
   James GM, 2003, J AM STAT ASSOC, V98, P397, DOI 10.1198/016214503000189
   Mestl T, 1996, PHYSICA D, V98, P33, DOI 10.1016/0167-2789(96)00086-3
   Ramsay J. O., 1997, FUNCTIONAL DATA ANAL
   Yeung KY, 2001, BIOINFORMATICS, V17, P977, DOI 10.1093/bioinformatics/17.10.977
NR 11
TC 2
Z9 2
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 683
EP 690
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500086
DA 2019-06-15
ER

PT S
AU Crammer, K
   Dekel, O
   Shalev-Shwartz, S
   Singer, Y
AF Crammer, K
   Dekel, O
   Shalev-Shwartz, S
   Singer, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Online passive-aggressive algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID RELATIVE LOSS BOUNDS
AB We present a unified view for online classification, regression, and uniclass problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss.
C1 Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.
RP Crammer, K (reprint author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.
CR BAUSCHKE HH, 1996, SIAM REV
   Censor Y. A., 1997, PARALLEL OPTIMIZATIO
   Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951
   Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062
   GENTILE C, NIPS 98
   Gentile C., 2001, J MACHINE LEARNING R, V2, P213
   Helmbold DP, 1999, IEEE T NEURAL NETWOR, V10, P1291, DOI 10.1109/72.809075
   HELMBOLD DP, COLT 95
   HERBSTER M, COLT 01
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Kivinen J, 2001, MACH LEARN, V45, P301, DOI 10.1023/A:1017938623079
   KIVINEN J, NIPS 02
   KLASNER N, COLT 95
   Li Y, 2002, MACH LEARN, V46, P361, DOI 10.1023/A:1012435301888
   Vapnik V. N., 1998, STAT LEARNING THEORY
   XING E, NIPS 03
   ZINKEVICH M, ICML 03
NR 17
TC 2
Z9 2
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1229
EP 1236
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500153
DA 2019-06-15
ER

PT S
AU Felzenszwalb, PF
   Huttenlocher, DP
   Kleinberg, JM
AF Felzenszwalb, PF
   Huttenlocher, DP
   Kleinberg, JM
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Fast algorithms for large-state-space HMMs with applications to web
   usage analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID FILTERS
AB In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artificially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show ail application to traffic analysis at a high-volume Web site.
C1 MIT, AI Lab, Cambridge, MA 02139 USA.
RP Felzenszwalb, PF (reprint author), MIT, AI Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
CR AIZEN J, IN PRESS J NATL ACAD
   BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0
   FLEZENSZWALB P, 2000, P IEEE COMP VIS PATT, P66
   GIL J, 1993, IEEE T PATTERN ANAL, V15, P504, DOI 10.1109/34.211471
   KARZANOV A, 1992, CYBERNETICS SYSTEM A
   KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375
   Pearl J, 1988, PROBABILISTIC REASON
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SCOTT SL, 2003, IN PRESS BAYESIAN ST, V7
   WELLS WM, 1986, IEEE T PATTERN ANAL, V8, P234, DOI 10.1109/TPAMI.1986.4767776
NR 10
TC 2
Z9 2
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 409
EP 416
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500052
DA 2019-06-15
ER

PT S
AU Ferguson, D
   Morris, A
   Hahnel, D
   Baker, C
   Omohundro, Z
   Reverte, C
   Thayer, S
   Whittaker, C
   Whittaker, W
   Burgard, W
   Thrun, S
AF Ferguson, D
   Morris, A
   Hahnel, D
   Baker, C
   Omohundro, Z
   Reverte, C
   Thayer, S
   Whittaker, C
   Whittaker, W
   Burgard, W
   Thrun, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI An autonomous robotic system for mapping abandoned mines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random fields. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy.
C1 Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Ferguson, D (reprint author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RI Burgard, Wolfram/N-2381-2019
OI Burgard, Wolfram/0000-0002-5680-6500
CR BAKER C, FSR 03
   BESL P, 1992, PAMI, V14
   BOSSE M, ICRA 03
   ELIAZAR A, IJCAI 03
   GUPTA A, 1997, T PARALLEL DISTRIB S, V8
   GUTMANN JS, CIRA 00
   HAHNEL D, UNPUB IROS 03
   HAHNEL D, IROS 0I
   HAHNEL D, 2003, 11 INT S ROB RES SIE
   Latombe JC, 1991, ROBOT MOTION PLANNIN
   Lu F, 1997, AUTONOMOUS ROBOTS, V4
   MONTEMERLO M, IJCAI 03
   MURPHY K, NIPS 99
   MURPHY KP, UAI 99
   Press WH, 1988, NUMERICAL RECIPES C
   SIMMONS R, AAAI 00
   Wainwright M.J., 2002, THESIS MIT
NR 17
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 587
EP 594
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500074
DA 2019-06-15
ER

PT S
AU Gentile, C
AF Gentile, C
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Fast feature selection from microarray expression data via
   multiplicative large margin algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID CLASSIFICATION; CANCER; PATTERNS
AB New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classification of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with five known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks.
C1 Univ Insubria, DICOM, I-21100 Varese, Italy.
RP Gentile, C (reprint author), Univ Insubria, DICOM, Via Mazzini 5, I-21100 Varese, Italy.
CR Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501
   Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745
   Ben-Dor A, 2000, J COMPUT BIOL, V7, P559, DOI 10.1089/106652700750050943
   Bradley P., 1998, P 15 INT C MACH LEAR, P82
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248
   Fodor SPA, 1997, SCIENCE, V277, P393, DOI 10.1126/science.277.5324.393
   GENTILE C, 2001, IN PRESS MACHINE LEA
   Gentile C., 2001, J MACHINE LEARNING R, V2, P213
   Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531
   Grove AJ, 2001, MACH LEARN, V43, P173, DOI 10.1023/A:1010844028087
   Gruvberger S, 2001, CANCER RES, V61, P5979
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   MANGASARIAN O, 1997, DATA MIN KNOWL DISC, V42, P183
   SINGH D, 2002, CANC CELL, V1
   TIBSHIRANI R, 1995, J ROY STAT SOC B MET, V1, P267
   WESTON J, 2000, P NIPS, V13
   WESTON J, 2002, IN PRESS JMLR
   XING E, 2001, P 18 ICML
NR 22
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 121
EP 128
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500016
DA 2019-06-15
ER

PT S
AU Graf, ABA
   Wichmann, FA
AF Graf, ABA
   Wichmann, FA
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Insights from machine learning applied to human visual classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID HUMAN FACES
AB We attempt to understand visual classification in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classification task. Human subjects classified the faces and their gender judgment, reaction time and confidence rating were recorded. Several hyperplane learning algorithms were used on the same classification task using the Principal Components of the texture and shape representation of the faces. The classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used. For classification, the brain needs more processing for stimuli close to that hyperplane than for those further away.
C1 Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.
RP Graf, ABA (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.
EM arnulf.graf@tuebingen.mpg.de; felix.wichmann@tuebingen.mpg.de
CR Blanz V., 1999, P 26 ANN C COMP GRAP, P187, DOI DOI 10.1145/311535.311556
   BROMLEY J, 1991, 1135991081916TM AT T
   Duda R, 2001, PATTERN CLASSIFICATI
   Graf ABA, 2002, LECT NOTES COMPUT SC, V2525, P491
   Graf ABA, 2003, IEEE T NEURAL NETWOR, V14, P597, DOI 10.1109/TNN.2003.811708
   REED SK, 1972, COGNITIVE PSYCHOL, V3, P382, DOI 10.1016/0010-0285(72)90014-X
   SCHOLKOPF B, 1998, NEURAL COMPUT, V10, P299
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Turk M., 1991, J COGNITIVE NEUROSCI, V3
   Vapnik VN, 1995, NATURE STAT LEARNING
   Wickens T, 2002, ELEMENTARY SIGNAL DE
NR 12
TC 2
Z9 2
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 905
EP 912
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500113
DA 2019-06-15
ER

PT S
AU Gruber, AJ
   Dayan, P
   Gutkin, BS
   Solla, SA
AF Gruber, AJ
   Dayan, P
   Gutkin, BS
   Solla, SA
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Dopamine modulation in a basal ganglio-cortical network implements
   saliency-based gating of working memory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID PREFRONTAL CORTEX; MODEL; RECEPTORS; REWARD
AB Dopamine exerts two classes of effect on the sustained neural activity in prefrontal cortex that underlies working memory. Direct release in the cortex increases the contrast of prefrontal neurons, enhancing the robustness of storage. Release of dopamine in the striatum is associated with salient stimuli and makes medium spiny neurons bistable; this modulation of the output of spiny neurons affects prefrontal cortex so as to indirectly gate access to working memory and additionally damp sensitivity to noise. Existing models have treated dopamine in one or other structure, or have addressed basal ganglia gating of working memory exclusive of dopamine effects. In this paper we combine these mechanisms and explore their joint effect. We model a memory-guided saccade task to illustrate how dopamine's actions lead to working memory that is selective for salient input and has increased robustness to distraction.
C1 Northwestern Univ, Chicago, IL 60611 USA.
RP Gruber, AJ (reprint author), Northwestern Univ, Chicago, IL 60611 USA.
EM a-gruber1@northwestern.edu; dayan@gatsby.ucl.ac.uk;
   boris@gatsby.ucl.ac.uk; sollal@northwestern.edu
RI Gutkin, Boris/A-7420-2014
CR Braver T S, 1999, Prog Brain Res, V121, P327
   Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320
   Camperi M, 1998, J COMPUT NEUROSCI, V5, P383, DOI 10.1023/A:1008837311948
   Cohen J, 2002, CURR OPIN NEUROBIOL, V12, P223, DOI 10.1016/S0959-4388(02)00314-8
   Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910
   Durstewitz D, 2000, J NEUROPHYSIOL, V83, P1733
   Frank MJ, 2001, COGN AFFECT BEHAV NE, V1, P137, DOI 10.3758/CABN.1.2.137
   FUNAHASHI S, 1989, J NEUROPHYSIOL, V255, P556
   Fuster J. M., 1995, MEMORY CEREBRAL CORT
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   GRUBER AJ, 2003, NIPS, V15
   Kawagoe R, 1998, NAT NEUROSCI, V1, P411
   O'Reilly RC, 2002, CEREB CORTEX, V12, P246, DOI 10.1093/cercor/12.3.246
   Reynolds JNJ, 2000, NEUROSCIENCE, V99, P199, DOI 10.1016/S0306-4522(00)00273-6
   SAWAGUCHI T, 1991, SCIENCE, V251, P947, DOI 10.1126/science.1825731
   SCHULTZ W, 1993, J NEUROSCI, V13, P900
   SERVANSCHREIBER D, 1990, SCIENCE, V249, P892, DOI 10.1126/science.2392679
   WILLIAMS GV, 1995, NATURE, V376, P572, DOI 10.1038/376572a0
NR 18
TC 2
Z9 2
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1271
EP 1278
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500158
DA 2019-06-15
ER

PT S
AU Kelly, R
   Lee, TS
AF Kelly, R
   Lee, TS
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Decoding V1 neuronal activity using particle filtering with volterra
   kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID HIPPOCAMPAL PLACE CELLS; RECONSTRUCTION
AB Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle filtering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to filter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufficient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels.
C1 Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
RP Kelly, R (reprint author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
CR BROCKWELL AE, 2003, UNPUB J NEUROPHYSIOL
   Brown EN, 1998, J NEUROSCI, V18, P7411
   EDEN UT, 2002, P COMP NEUR M CNS 02
   GAO Y, 2002, PROBABILISTIC INFERE, P213
   GEORGOPOULOS AP, 1989, SCIENCE, V243, P234, DOI 10.1126/science.2911737
   Rieke Fred, 1997, SPIKES EXPLORING NEU
   ROMERO R, 2002, NEUROCOMPUTING, V52, P135
   Stanley GB, 1999, J NEUROSCI, V19, P8036
   Stanley GB, 2002, NEURAL COMPUT, V14, P2925, DOI 10.1162/089976602760805340
   Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017
NR 10
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1359
EP 1366
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500169
DA 2019-06-15
ER

PT S
AU Monteleoni, C
   Jaakkola, T
AF Monteleoni, C
   Jaakkola, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Online learning of non-stationary sequences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID PREDICTION
AB We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts. On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics. We demonstrate the new algorithm in the context of wireless networks.
C1 MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
RP Monteleoni, C (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, 200 Technol Sq, Cambridge, MA 02139 USA.
EM cmontel@ai.mit.edu; tommi@ai.mit.edu
CR Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   BLUM A, 1999, IEEE S FDN COMP SCI, P450
   Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Haussler D, 1998, IEEE T INFORM THEORY, V44, P1906, DOI 10.1109/18.705569
   HELMBOLD DP, 1996, INT C MACH LEARN, P243
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   *IEEE, 1999, 80211 IEEE
   KRASHINSKY R, 2002, MOB 2002 ATL GA SEPT
   KRASHINSKY R, 2002, MOBICOM 2002
   KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331
   LITTLESTONE N, 1989, IEEE S FDN COMP SCI, P256
   Steinbach C., 2002, THESIS MIT
   Vovk V, 1999, MACH LEARN, V35, P247, DOI 10.1023/A:1007595032382
NR 14
TC 2
Z9 2
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1093
EP 1100
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500136
DA 2019-06-15
ER

PT S
AU Morris, QD
   Frey, BJ
   Paige, CJ
AF Morris, QD
   Frey, BJ
   Paige, CJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Denoising and untangling graphs using degree priors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB This paper addresses the problem of untangling hidden graphs from a set of noisy detections of undirected edges. We present a model of the generation of the observed graph that includes degree-based structure priors on the hidden graphs. Exact inference in the model is intractable; we present an efficient approximate inference algorithm to compute edge appearance posteriors. We evaluate our model and algorithm on a biological graph inference problem.
C1 Univ Toronto, Toronto, ON M5S 3G4, Canada.
RP Morris, QD (reprint author), Univ Toronto, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM quaid@psi.utoronto.ca; frey@psi.utoronto.ca; paige@uhnres.utoronto.ca
CR Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   FALOUTSOS M, 1999, COMPUTER COMMUNICATI, V29
   Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414
   FREY BJ, 1998, P 35 ALL C COMM CONT
   FREY BJ, 2003, PSI200302 U TOR
   FREY BJ, 2002, 2001 C ADV NEUR INF, V14
   FREY BJ, 1996, P 1996 ALL C COMM CO
   GOLDBERG DS, 2003, P NAT AC SCI
   Gomez Shawn M, 2002, Pac Symp Biocomput, P413
   Jeong H, 2000, NATURE, V407, P651, DOI 10.1038/35036627
   JORDAN MI, 2004, UNPUB INTRO LEARNING
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   MacKay DJC, 1996, ELECTRON LETT, V32, P1645, DOI 10.1049/el:19961141
   Mezard M, 2002, SCIENCE, V297, P812, DOI 10.1126/science.1073287
   Murphy K., 1999, UNCERTAINTY ARTIFICI
   Pearl J, 1988, PROBABILISTIC REASON
   Rzhetsky A, 2001, BIOINFORMATICS, V17, P988, DOI 10.1093/bioinformatics/17.10.988
   Saito R, 2003, BIOINFORMATICS, V19, P756, DOI 10.1093/bioinformatics/btg070
   VONMERING C, 2002, NATURE
NR 19
TC 2
Z9 2
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 385
EP 392
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500049
DA 2019-06-15
ER

PT S
AU Opper, M
   Winther, O
AF Opper, M
   Winther, O
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Variational linear response
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations.
C1 Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.
RP Opper, M (reprint author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.
CR ATTIAS H, 2000, ADV NEURAL INFORMATI, V12
   Bishop Christopher M., 2003, ARTIFICIAL INTELLIGE
   BISHOP CM, 2002, ADV NEURAL INFORMATI, V15
   Ghahramani Z, 2001, ADV NEUR IN, V13, P507
   Hojen-Sorensen PADFR, 2002, NEURAL COMPUT, V14, P889, DOI 10.1162/089976602317319009
   Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386
   MacKay DJC, 2003, INFORMATION THEORY I
   MISKIN JW, 2000, ADV INDEPENDENT COMP
   Opper M, 2001, PHYS REV E, V64, part. no., DOI 10.1103/PhysRevE.64.056131
   Opper  M., 2001, ADV MEAN FIELD METHO
   Parisi  G., 1988, STAT FIELD THEORY
   WELLING M, 2003, ARTIFICIAL INTELLIGE
   WELLING M, 2003, PROPAGATION RULES LI
NR 13
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1157
EP 1164
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500144
DA 2019-06-15
ER

PT S
AU Rosset, S
   Zhu, J
   Hastie, T
AF Rosset, S
   Zhu, J
   Hastie, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Margin maximizing loss functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Margin maximizing properties play an important role in the analysis of classification models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a sufficient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss, We also generalize it to multi-class classification problems, and present margin maximizing multiclass versions of logistic regression and support vector machines.
C1 IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Rosset, S (reprint author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM srosset@us.ibm.com; jizhu@umich.edu; hastie@stat.stanford.edu
CR BARTLETT PL, 2003, CONVEXITY CLASSIFICA
   Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106
   Freund Y, 1995, P 2 EUR C COMP LEARN
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   GROVE AJ, 1998, P 15 NAT C AI
   Hastie T, 2001, ELEMENTS STAT LEARNI
   Mangasarian OL, 1999, OPER RES LETT, V24, P15, DOI 10.1016/S0167-6377(98)00049-2
   ROSSET R, 2003, BOOSTING REGULARIZED
   Schapire RE, 1998, ANN STAT, V26, P1651
   Vapnik VN, 1995, NATURE STAT LEARNING
   Weston J., 1998, CSDTR9804 U LOND
NR 11
TC 2
Z9 2
U1 0
U2 2
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1237
EP 1244
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500154
DA 2019-06-15
ER

PT S
AU Sahani, M
AF Sahani, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A biologically plausible algorithm for reinforcement-shaped
   representational learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB Significant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a significant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generative-modelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way.
C1 Univ Calif San Francisco, WM Keck Fdn Ctr Integrat Neurosci, San Francisco, CA 94143 USA.
RP Sahani, M (reprint author), Univ Calif San Francisco, WM Keck Fdn Ctr Integrat Neurosci, San Francisco, CA 94143 USA.
EM maneesh@phy.ucsf.edu
CR Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Everitt B. S., 1984, INTRO LATENT VARIABL
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Press W. H., 1993, NUMERICAL RECIPES C
   Sutton R. S., 1998, REINFORCEMENT LEARNI
NR 7
TC 2
Z9 2
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1287
EP 1294
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500160
DA 2019-06-15
ER

PT S
AU Shental, N
   Zomet, A
AF Shental, N
   Zomet, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Pairwise clustering and graphical models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID IMAGE SEGMENTATION
AB Significant progress in clustering has been achieved by algorithms that are based on pairwise affinities between the datapoints. In particular, spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efficient eigenvector calculations. However, spectral methods lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data.
   In this paper we use the previously proposed typical cut framework for pairwise clustering. We show an equivalence between calculating the typical cut and inference in an undirected graphical model. We show that for clustering problems with hundreds of datapoints exact inference may still be possible. For more complicated datasets, we show that loopy belief propagation (BP) and generalized belief propagation (GBP) can give excellent results on challenging clustering problems. We also use graphical models to derive a learning algorithm for affinity matrices based on labeled data.
C1 Hebrew Univ Jerusalem, Ctr Neural Computac, IL-91904 Jerusalem, Israel.
RP Shental, N (reprint author), Hebrew Univ Jerusalem, Ctr Neural Computac, IL-91904 Jerusalem, Israel.
RI Hertz, Tomer/S-5744-2016
OI Hertz, Tomer/0000-0002-0561-1578
CR Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805
   Gdalyahu Y, 2001, IEEE T PATTERN ANAL, V23, P1053, DOI 10.1109/34.954598
   Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806
   MEILA M, 2001, ADV NEURAL INFORMATI, V14
   MINKA T, 2003, ADV NEURAL INFORMATI, V16
   NG AY, 2001, ADV NEURAL INFORMATI, V14
   SHENTAL N, 2003, 9 INT C COMP VIS
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   WANG JS, 1990, PHYSICA A, V167, P565, DOI 10.1016/0378-4371(90)90275-W
   Yedidia J., 2003, EXPLORING ARTIFICIAL
NR 10
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 185
EP 192
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500024
DA 2019-06-15
ER

PT S
AU Tenore, F
   Etienne-Cummings, R
   Lewis, MA
AF Tenore, F
   Etienne-Cummings, R
   Lewis, MA
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Entrainment of silicon central pattern generators for legged locomotory
   control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ADAPTATION
AB A second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine was constructed. This chip builds on the basic results achieved on a previous chip. Various improvements - most importantly the larger number of more sophisticated silicon neurons - were implemented to enhance its capabilities and to move toward a significantly more versatile device. The new neurons are all interconnected and synaptic weights on each neuron can be easily programmed. This allows on-chip creation of neural networks used to control a robotic leg. The enhanced versatility thus obtained allows us to get closer to a self-contained locomotion controller for walking robots.
C1 Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.
RP Tenore, F (reprint author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.
RI Etienne-Cummings, Ralph/A-3227-2010; Tenore, Francesco/H-2146-2013
OI Tenore, Francesco/0000-0002-1197-9643
CR ARSHAVSKY YI, 1983, TRENDS NEUROSCI, V6, P417, DOI 10.1016/0166-2236(83)90191-1
   COHEN AH, 1999, AUTONOMOUS ROBOTS SP, V7, P225
   FORSSBERG H, 1975, BRAIN RES, V85, P103, DOI 10.1016/0006-8993(75)91013-6
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Lewis MA, 2002, AUTON ROBOT, V12, P301, DOI 10.1023/A:1015221832567
   Lewis MA, 2003, BIOL CYBERN, V88, P137, DOI 10.1007/s00422-002-0365-7
   LEWIS MA, 2000, P INT C ROB AUT SAN
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   RASCHE C, 1998, NEUROMORPHIC SYSTEMS
   Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396
NR 10
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1043
EP 1050
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500130
DA 2019-06-15
ER

PT S
AU Wainwright, MJ
   Jordan, MI
AF Wainwright, MJ
   Jordan, MI
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Semidefinite relaxations for approximate inference on graphs with cycles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We present a new method for calculating approximate marginals for probability distributions defined by graphs with cycles, based on a Gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved by efficient interior point methods [8]. As with the Bethe approximation and its generalizations [12], the optimizing arguments of this problem can be taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches, our variational problem is strictly convex and so has a unique global optimum. An additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function. In experimental trials, the performance of the log-determinant relaxation is comparable to or better than the sum-product algorithm, and by a substantial margin for certain problem classes. Finally, the zero-temperature limit of our log-determinant relaxation recovers a class of well-known semidefinite relaxations for integer programming [e.g., 3].
C1 Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Wainwright, MJ (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
CR COWELL RG, 1999, STAT ENG INFORMATION
   Deza M. M., 1997, GEOMETRY CUTS METRIC
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Jordan M., 1999, LEARNING GRAPHICAL M
   Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802
   McEliece R. J., 2002, MATH THEORY SYSTEMS
   Rockafellar G., 1970, CONVEX ANAL
   Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430
   WAINWRIGHT M, 2003, UCBCSD31226
   Wainwright M. J., 2003, 649 UC BERK DEP STAT
   WAINWRIGHT MJ, 2002, UNCERTAINTY ARTIFICI, V18, P536
   YEDIDIA JS, 2002, TR200122 MITS EL RES
NR 12
TC 2
Z9 2
U1 2
U2 3
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 369
EP 376
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500047
DA 2019-06-15
ER

PT S
AU Werfel, J
   Xie, XH
   Seung, HS
AF Werfel, J
   Xie, XH
   Seung, HS
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning curves for stochastic gradient descent in linear feedforward
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NEURAL NETWORKS
AB Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures. in which they will be effective.
C1 MIT, Dept EECS, Cambridge, MA 02139 USA.
RP Werfel, J (reprint author), MIT, Dept EECS, Cambridge, MA 02139 USA.
CR BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248
   BARTLETT P, 1999, HEBBIAN SYNAPTIC MOD
   Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671
   CAUWENBERGHS G, 1993, ADV NEURAL INFORMATI, V5, P244
   FIETE I, COMMUNICATION
   FLOWER B, 1993, ADV NEURAL INFORMATI, V5, P212
   JABRI M, 1992, IEEE T NEURAL NETWOR, V3, P154, DOI 10.1109/72.105429
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 9
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1197
EP 1204
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500149
DA 2019-06-15
ER

PT S
AU Yagi, M
   Yamasaki, H
   Shibata, T
AF Yagi, M
   Yamasaki, H
   Shibata, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A mixed-signal VLSI for real-time generation of edge-based image vectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB A mixed-signal image filtering VLSI has been developed aiming at real-time generation of edge-based image vectors for robust image recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key processing parameter in vector generation. As a result, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-mum double-polysilicon three-metal-layer CMOS technology and the concept was verified by the fabricated chip. The chip generates a 64-dimension feature vector from a 64 x 64-pixel gray scale image every 80 musec. This is about 10(4) times faster than the software computation, making a real-time image recognition system feasible.
C1 Univ Tokyo, Dept Elect Engn, Bunkyo Ku, Tokyo 1138656, Japan.
RP Yagi, M (reprint author), Univ Tokyo, Dept Elect Engn, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.
CR CHAKRABARTTY S, 2003, ICASSP 2003
   Chen YT, 1999, IEEE ENG MED BIOL, V18, P25, DOI 10.1109/51.740961
   LEE CL, 1992, IEE PROC-G, V139, P63, DOI 10.1049/ip-g-2.1992.0012
   Liu C., 2002, IEEE T IMAGE PROCESS, V11
   Ogawa M., 2002, S VLSI CIRC, P244
   POTLAPALLI H, 1998, IEEE T IND ELECT, V45
   Yagi M, 2003, IEEE T NEURAL NETWOR, V14, P1144, DOI 10.1109/TNN.2003.819038
   YAGI M, 2002, P 11 EUR SIGN PROC C, V1, P103
   YAGI M, 2002, P ISCAS 2002
   YAGI M, 2003, P 13 SCAND C IM AN S, P534
   YAGI M, 2000, P 10 EUR SIGN PROC C, P729
   YAMASAKI T, ADV NEURAL INFORMATI, V14, P1131
NR 12
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1035
EP 1042
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500129
DA 2019-06-15
ER

PT S
AU Yuille, A
   Fang, F
   Schrater, P
   Kersten, D
AF Yuille, A
   Fang, F
   Schrater, P
   Kersten, D
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Human and ideal observers for detecting image curves
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID NATURAL IMAGES; INTEGRATION
AB This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers' performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.
C1 Univ Calif Los Angeles, Dept Stat & Psychol, Los Angeles, CA 90024 USA.
RP Yuille, A (reprint author), Univ Calif Los Angeles, Dept Stat & Psychol, Los Angeles, CA 90024 USA.
CR BRADY M, 1999, THESIS U MINNESOTA
   Elder JH, 2002, J VISION, V2, DOI 10.1167/2.4.5
   FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q
   Field DJ, 2000, SPATIAL VISION, V13, P51, DOI 10.1163/156856800741018
   Geisler WS, 2001, VISION RES, V41, P711, DOI 10.1016/S0042-6989(00)00277-7
   Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006
   Hess R, 1999, TRENDS COGN SCI, V3, P480, DOI 10.1016/S1364-6613(99)01410-2
   KOVACS I, 1993, P NATL ACAD SCI USA, V90, P7495, DOI 10.1073/pnas.90.16.7495
   LEE AB, 2000, INT J COMPUTER V OCT
   REN X, 2002, P ECCV
   RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814
   Sigman M, 2001, P NATL ACAD SCI USA, V98, P1935, DOI 10.1073/pnas.031571498
   Simoncelli E. P., 2000, NIPS, P855
   Yuille AL, 2001, INT J COMPUT VISION, V41, P9, DOI 10.1023/A:1011156931605
   YUILLE AL, 2000, IEEE PAMI, V22
   Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110
NR 16
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1459
EP 1466
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500181
DA 2019-06-15
ER

PT S
AU Zhang, T
AF Zhang, T
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Learning bounds for a generalized family of Bayesian posterior
   distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID CONSISTENCY; CONVERGENCE; RATES
AB In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simplifies and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist "bad" prior structures even at places far away from the true distribution.
C1 IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
RP Zhang, T (reprint author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
CR Barron A, 1999, ANN STAT, V27, P536
   DIACONIS P, 1986, ANN STAT, V14, P1, DOI 10.1214/aos/1176349830
   Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   Meir R., 2003, J MACHINE LEARNING R, V4, P839
   Robert CP, 1994, BAYESIAN CHOICE DECI
   Seeger Matthias, 2002, JMLR, V3, P233
   Shen XT, 2001, ANN STAT, V29, P687
NR 8
TC 2
Z9 2
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1149
EP 1156
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500143
DA 2019-06-15
ER

PT S
AU Kandasamy, K
   Neiswanger, W
   Schneider, J
   Poczos, B
   Xing, EP
AF Kandasamy, Kirthevasan
   Neiswanger, Willie
   Schneider, Jeff
   Poczos, Barnabas
   Xing, Eric P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Architecture Search with Bayesian Optimisation and Optimal
   Transport
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.
C1 [Kandasamy, Kirthevasan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   Petuum Inc, Pittsburgh, PA USA.
RP Kandasamy, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kandasamy@cs.cmu.edu; willie@cs.cmu.edu; schneide@cs.cmu.edu;
   bapoczos@cs.cmu.edu; epxing@cs.cmu.edu
FU DOE grant [DESC0011114]; NSF [IIS1563887]; Darpa D3M program; Facebook
   fellowship; Siebel scholarship
FX We would like to thank Guru Guruganesh and Dougal Sutherland for the
   insightful discussions. This research is partly funded by DOE grant
   DESC0011114, NSF grant IIS1563887, and the Darpa D3M program. KK is
   supported by a Facebook fellowship and a Siebel scholarship.
CR Baker B., 2016, ARXIV161102167
   Bergstra  James, 2013, MAKING SCI MODEL SEA
   Brochu E., 2010, TUTORIAL BAYESIAN OP
   Buza K, 2014, STUD CLASS DATA ANAL, P145, DOI 10.1007/978-3-319-01595-8_16
   Coraddu A, 2016, P I MECH ENG M-J ENG, V230, P136, DOI 10.1177/1475090214540874
   Cortes C., 2016, ARXIV160701097
   Fernandes  Kelwin, 2015, PORT C ART INT
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Gao XB, 2010, PATTERN ANAL APPL, V13, P113, DOI 10.1007/s10044-008-0141-y
   Ginsbourger  David, 2011, ERCIM
   Graf F, 2011, LECT NOTES COMPUT SC, V6892, P607, DOI 10.1007/978-3-642-23629-7_74
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang G, 2017, CVPR
   Hutter  Frank, 2011, LION
   Jenatton  Rodolphe, 2017, INT C MACH LEARN
   Jiang Nan, 2016, ARXIV161009512
   Kandasamy  Kirthevasan, 2017, ARXIV170306240
   Kandasamy  Kirthevasan, 2016, ADV NEURAL INFORM PR, P1777
   Kandasamy  Kirthevasan, 2016, ADV NEURAL INFORM PR, P992
   Kandasamy  Kirthevasan, 2016, ARXIV160306288
   Kitano H., 1990, Complex Systems, V4, P461
   Klein A., 2016, ARXIV160507079
   Kondor R., 2002, ICML, V2002, P315
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Liu C., 2017, ARXIV171200559
   Liu H, 2017, ARXIV171100436
   Mendoza H., 2016, WORKSH AUT MACH LEAR, P58
   Messmer BT, 1998, IEEE T PATTERN ANAL, V20, P493, DOI 10.1109/34.682179
   Miikkulainen R, 2017, ARXIV170300548
   Mockus J. B., 1991, J OPTIMIZATION THEOR
   Negrinho R., 2017, ARXIV170408792
   Peyre  Gabriel, COMPUTATIONAL OPTIMA
   Rana  PS, 2013, PHYSICOCHEMICAL PROP
   Rasmussen  C., 2006, ADAPTATIVE COMPUTATI
   Real E., 2017, ARXIV170301041
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Snoek J, 2012, ADV NEURAL INFORM PR
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Sutherland Dougal J, 2015, THESIS
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Swersky  Kevin, 2014, ARXIV14094011
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Torres-Sospedra J, 2014, INT C INDOOR POSIT, P261, DOI 10.1109/IPIN.2014.7275492
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Wallis WD, 2001, PATTERN RECOGN LETT, V22, P701, DOI 10.1016/S0167-8655(01)00022-8
   Wilson A., 2015, INT C MACH LEARN, P1775
   Yuille A., 2017, ARXIV170301513
   Zhong  Zhao, 2017, ARXIV170805552
   Zoph B., 2016, ARXIV161101578
   Zoph B., 2017, ARXIV170707012
NR 54
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302006
DA 2019-06-15
ER

PT S
AU Aghasi, A
   Abdi, A
   Nguyen, N
   Romberg, J
AF Aghasi, Alireza
   Abdi, Afshin
   Nguyen, Nam
   Romberg, Justin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Net-Trim: Convex Pruning of Deep Neural Networks with Performance
   Guarantee
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length N as inputs, we show that if the network response can be described using a maximum number of s non-zero weights per node, these weights can be learned from O (s log N) samples.
C1 [Aghasi, Alireza] Georgia State Univ, Inst Insight, Atlanta, GA 30303 USA.
   [Aghasi, Alireza; Nguyen, Nam] IBM TJ Watson, Yorktown Hts, NY 10598 USA.
   [Abdi, Afshin; Romberg, Justin] Georgia Tech, Dept ECE, Atlanta, GA USA.
RP Aghasi, A (reprint author), Georgia State Univ, Inst Insight, Atlanta, GA 30303 USA.; Aghasi, A (reprint author), IBM TJ Watson, Yorktown Hts, NY 10598 USA.
EM aaghasi@gsu.edu; abdi@gatech.edu; nnguyen@us.ibm.com;
   jrom@ece.gatech.edu
CR Arora S., 2014, P 31 INT C MACH LEAR
   Aslan O., 2014, P ADV NEUR INF PROC, P3275
   Bach F., 2014, TECHNICAL REPORT
   Bengio Y., 2005, ADV NEURAL INFORM PR, P123
   Chen  W., 2015, INT C MACH LEARN, P2285
   Choromanska A., 2015, P 18 INT C ART INT S
   Ghadimi E, 2015, IEEE T AUTOMAT CONTR, V60, P644, DOI 10.1109/TAC.2014.2354892
   GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219
   Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Kawaguchi K., 2016, PREPRINT
   NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wan L., 2016, P 33 INT C MACH LEAR
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403024
DA 2019-06-15
ER

PT S
AU Agrawal, S
   Jia, R
AF Agrawal, Shipra
   Jia, Randy
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Optimistic posterior sampling for reinforcement learning: worst-case
   regret bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of (O) over tilde (D root SAT) for any communicating MDP with S states, A actions and diameter D, when T >= S-5 A. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T. This result improves over the best previously known upper bound of (O) over tilde (DS root AT) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of Omega(root DSAT) for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.
C1 [Agrawal, Shipra; Jia, Randy] Columbia Univ, New York, NY 10027 USA.
RP Agrawal, S (reprint author), Columbia Univ, New York, NY 10027 USA.
EM sa3305@columbia.edu; rqj2000@columbia.edu
CR Abbasi-Yadkori Yasin, 2014, ARXIV14063926
   Abramowitz M, 1964, HDB MATH FUNCTIONS F, V55
   Agrawal S, 2013, P 16 INT C ART INT S, P99
   Agrawal S., 2012, P 25 ANN C LEARN THE
   Agrawal Shipra, 2013, P 30 INT C MACH LEAR
   Asmuth J., 2009, P 25 C UNC ART INT U, P19
   Azar Mohammad Gheshlaghi, 2017, ARXIV170305449
   Bartlett Peter L, 2009, P 25 C UNC ART INT U, P35
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Bubeck S, 2013, ADV NEURAL INFORM PR, P638
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Dann C, 2015, P 28 INT C NEUR INF, V28, P2818
   Fonteneau Raphael, 2013, NIPS 2013 WORKSH BAY
   Grinstead CM, 2012, INTRO PROBABILITY
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Kakade S. M., 2003, THESIS
   Kaufmann Emilie, 2012, INT C ALG LEARN THEO
   Kearns M, 1999, ADV NEUR IN, V11, P996
   Kleinberg R, 2008, ACM S THEORY COMPUT, P681
   Osband I., 2016, ARXIV160700215
   Osband I., 2013, ADV NEURAL INFORM PR, P3003
   Osband Ian, 2014, ARXIV14020635
   Puterman M. L., 2014, MARKOV DECISION PROC
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Russo Daniel, 2015, J MACHINE LEARNING R
   Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334
   Shevtsova IG, 2010, DOKL MATH, V82, P862, DOI 10.1134/S1064562410060062
   Strehl A. L., 2005, P 22 INT C MACH LEAR, P856, DOI DOI 10.1145/1102351.1102459
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Tewari A., 2008, P ADV NEUR INF PROC, P1505
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 33
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401022
DA 2019-06-15
ER

PT S
AU Benaim, S
   Wolf, L
AF Benaim, Sagie
   Wolf, Lior
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI One-Sided Unsupervised Domain Mapping
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In unsupervised domain mapping, the learner is given two unmatched datasets A and B. The goal is to learn a mapping G(AB) that translates a sample in A to the analog sample in B. Recent approaches have shown that when learning simultaneously both G(AB) and the inverse mapping G(BA), convincing mappings are obtained. In this work, we present a method of learning G(AB) without learning G(BA). This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https : //github. com/sagiebenaim/DistanceGAN.
C1 [Benaim, Sagie; Wolf, Lior] Tel Aviv Univ, Blavatn Sch Comp Sci, Tel Aviv, Israel.
   [Wolf, Lior] Facebook AI Res, New York, NY USA.
RP Benaim, S (reprint author), Tel Aviv Univ, Blavatn Sch Comp Sci, Tel Aviv, Israel.
RI Jeong, Yongwook/N-7413-2016
FU European Research Council (ERC) under the European Union [ERC CoG
   725974]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant ERC CoG 725974). The authors would like to thank
   Laurens van der Maaten and Ross Girshick for insightful discussions.
CR Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Bousmalis Konstantinos, 2017, CVPR
   Fidler S., 2012, NIPS
   Gatys L. A., 2016, CVPR
   Goodfellow I., 2014, NIPS
   Hoffman J., 2016, FCNS WILD PIXEL LEVE, V12
   Ioffe S., 2015, ICML
   Isola  Phillip, 2017, CVPR
   Johnson J., 2016, ECCV
   Kim T, 2017, ARXIV170305192
   LeCun Y, 2010, MNIST HANDWRITTEN DI
   Liu M.-Y., 2016, ADV NEURAL INFORM PR, P469
   Liu  Z., 2015, P INT C COMP VIS ICC
   Mao Xudong, 2016, ARXIV161104076
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Parkhi O. M., 2015, BRIT MACH VIS C
   Paysan Pascal, 2009, AVSS
   Radford  A., 2015, ARXIV151106434
   Ronneberger O., 2015, MICCAI
   Shrivastava A., 2016, ARXIV161207828
   Sutskever Ilya, 2016, ICLR WORKSH
   Taigman Y, 2017, INT C LEARN REPR ICL
   Ulyanov D., 2016, ICML
   Van Belle Werner, 2006, CORRELATION INPRODUC
   Xia Y., 2016, ARXIV161100179
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yi Z., 2017, ARXIV170402510
   Zhu J Y, 2017, ARXIV170310593
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400072
DA 2019-06-15
ER

PT S
AU Bui, TD
   Nguyen, CV
   Turner, RE
AF Bui, Thang D.
   Nguyen, Cuong, V
   Turner, Richard E.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Streaming Sparse Gaussian Process Approximations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseu do-input locations. The proposed framework is assessed using synthetic and real-world datasets.
C1 [Bui, Thang D.; Nguyen, Cuong, V; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England.
RP Bui, TD (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM tdb40@cam.ac.uk; vcn22@cam.ac.uk; ret26@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Google European Doctoral Fellowship; EPSRC [EP/M0269571, EP/L000776/1];
   Google
FX The authors would like to thank Mark Rowland, John Bradshaw, and
   Yingzhen Li for insightful comments and discussion. Thang D. Bui is
   supported by the Google European Doctoral Fellowship. Cuong V. Nguyen is
   supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by
   Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.
CR Bauer M., 2016, ADV NEURAL INFORM PR
   Broderick T., 2013, ADV NEURAL INFORM PR
   Bui T. D., 2014, ADV NEURAL INFORM PR
   Bui T. D., 2017, J MACHINE LEARNING R
   Bui T. D., 2016, INT C MACH LEARN ICM
   Cheng C.-A., 2016, ADV NEURAL INFORM PR
   Csato L., 2002, NEURAL COMPUTATION
   Csato L., 2002, THESIS
   Dezfouli A., 2015, ADV NEURAL INFORM PR
   Garofolo J., 1993, TIMIT ACOUSTIC PHONE
   Ghahramani Z., 2000, NIPS WORKSH ONL LEAR
   Hensman J., 2013, C UNC ART INT UAI
   Hensman J., 2015, INT C ART INT STAT A
   Hernandez-Lobato D., 2016, INT C ART INT STAT A
   Keogh E. J., 1999, INT C SCI STAT DAT M
   Matthews A. G., 2017, J MACHINE LEARNING R
   Matthews A. G. D. G., 2016, INT C ART INT STAT A
   Minka T., 2004, TECH REP
   Opper M., 1999, ON LINE LEARNING NEU
   Quinonero-Candela J., 2005, J MACHINE LEARNING R
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sato M.-A., 2001, NEURAL COMPUTATION
   Snelson E., 2006, ADV NEURAL INFORM PR
   Titsias M. K., 2009, INT C ART INT STAT A
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403036
DA 2019-06-15
ER

PT S
AU Chen, R
   Lucier, B
   Singer, Y
   Syrgkanis, V
AF Chen, Robert
   Lucier, Brendan
   Singer, Yaron
   Syrgkanis, Vasilis
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust Optimization for Non-Convex Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to stochastic optimization: given an oracle that returns alpha-approximate solutions for distributions over objectives, we compute a distribution over solutions that is alpha-approximate in the worst case. We show that derandomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks. We apply our results to robust neural network training and submodular optimization. We evaluate our approach experimentally on corrupted character classification and robust influence maximization in networks.
C1 [Chen, Robert; Singer, Yaron] Harvard Univ, Comp Sci, Cambridge, MA 02138 USA.
   [Lucier, Brendan; Syrgkanis, Vasilis] Microsoft Res New England, Cambridge, MA USA.
RP Chen, R (reprint author), Harvard Univ, Comp Sci, Cambridge, MA 02138 USA.
RI Jeong, Yongwook/N-7413-2016
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374
   Chatterjee Sabyasachi, 2016, ADV NEURAL INFORM PR, P3423
   Chen W, 2016, P 22 ACM SIGKDD INT, P795, DOI DOI 10.1145/ 2939672.2939745
   Hazan Elad, 2016, P 33 INT C MACH LEAR, P1833
   Hazan Elad, 2015, ADV NEURAL INFORM PR, P1594
   He X, 2016, P 22 ACM SIGKDD INT, P885, DOI DOI 10.1145/2939672.2939760
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Krause Andreas, 2011, P INT JOINT C ART IN, P2133
   Krause Andreas, 2007, ADV NEURAL INFORM PR, P777
   LESKOVEC J., STANFORD NETWORK ANA
   Lowalekar M, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1395
   Namkoong Hongseok, 2016, ADV NEURAL INFORM PR, P2208
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Shalev-Shwartz Shai, 2016, P 33 INT C MACH LEAR, P793
   Steinhardt Jacob, 2015, P 28 C LEARN THEOR C, P1564
   YishayMansour Aviad Rubinstein, 2015, P 26 ANN ACM SIAM S, P449
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P699
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404075
DA 2019-06-15
ER

PT S
AU Chen, YP
   Li, JN
   Xiao, HX
   Jin, XJ
   Yan, SC
   Feng, JS
AF Chen, Yunpeng
   Li, Jianan
   Xiao, Huaxin
   Jin, Xiaojie
   Yan, Shuicheng
   Feng, Jiashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dual Path Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 x 4 d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.
C1 [Chen, Yunpeng; Li, Jianan; Xiao, Huaxin; Jin, Xiaojie; Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore.
   [Li, Jianan] Beijing Inst Technol, Beijing, Peoples R China.
   [Xiao, Huaxin] Natl Univ Def Technol, Changsha, Hunan, Peoples R China.
   [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China.
RP Chen, YP (reprint author), Natl Univ Singapore, Singapore, Singapore.
FU National University of Singapore [R-263-000-C08-133]; Ministry of
   Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS
   grant [R-263-000-C67-646]
FX The work of Jiashi Feng was partially supported by National University
   of Singapore startup grant R-263-000-C08-133, Ministry of Education of
   Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant
   R-263-000-C67-646.
CR Chen L.-C., 2016, ARXIV160600915
   Chen Tianqi, 2015, ARXIV151201274
   Chen Yunpeng, 2017, ARXIV170302180
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   He K., 2017, ARXIV170306870
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang G, 2016, ARXIV160806993
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lee C.-Y., 2016, P 19 INT C ART INT S, P464
   Liao Qianli, 2016, ARXIV160403640
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pleiss G., 2017, ARXIV170706990
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Soltani  R., 2016, ARXIV160500064
   Szegedy C., 2016, ARXIV160207261
   Xie S., 2016, ARXIV161105431
   Zagoruyko S, 2016, ARXIV160507146
   Zhang  X., 2016, ARXIV161105725
   Zhou  B., 2016, ARXIV161002055
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404052
DA 2019-06-15
ER

PT S
AU Dahlgaard, S
   Knudsen, MBT
   Thorup, M
AF Dahlgaard, Soren
   Knudsen, Mathias Baek Tejs
   Thorup, Mikkel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Practical Hash Functions for Similarity Estimation and Dimensionality
   Reduction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However, the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used, without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input.The question is if they can be trusted in the real world where they may be faced with more structured input.
   In this paper we focus on two prominent applications of hashing, namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of which have found numerous applications, i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM.
   We consider the recent mixed tabulation hash function of Dahlgaard et al. [FOCS' 15] which was proved theoretically to perform like a truly random hash function in many applications, including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are not too dense. Our main contribution, however, is an experimental comparison of different hashing schemes when used inside FH, OPH, and LSH.
   We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme (ax + b) mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data, but here we demonstrate that in the above applications, it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3, which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However, mixed tabulation was 40% faster than MurmurHash3, and it has the proven guarantee of good performance (like fully random) on all possible input making it more reliable.
C1 [Dahlgaard, Soren; Knudsen, Mathias Baek Tejs; Thorup, Mikkel] Univ Copenhagen, SupWiz, Copenhagen, Denmark.
   [Thorup, Mikkel] Univ Copenhagen, Copenhagen, Denmark.
RP Dahlgaard, S (reprint author), Univ Copenhagen, SupWiz, Copenhagen, Denmark.
EM s.dahlgaard@supwiz.com; m.knudsen@supwiz.com; mthorup@di.ku.dk
RI Jeong, Yongwook/N-7413-2016
FU Mikkel Thorup's Advanced Grant from the Danish Council for Independent
   Research [DFF-0602-02499B]; DABAI project; FNU project AlgoDisc
FX The authors gratefully acknowledge support from Mikkel Thorup's Advanced
   Grant DFF-0602-02499B from the Danish Council for Independent Research
   as well as the DABAI project. Mathias Baek Tejs Knudsen gratefully
   acknowledges support from the FNU project AlgoDisc.
CR Andoni A., 2014, SODA, P1018
   Andoni A., 2015, ADV NEURAL INFORM PR, P1225
   Andoni  A., 2015, P 47 ANN ACM S THEOR, P793
   Andoni A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P47
   [Anonymous], 2012, BREAKING MURMUR HASH
   Appleby Austin, 2016, MURMURHASH3
   Aumasson J.-P., 2013, LNCS, V7954, P119, DOI DOI 10.1007/978-3-642-38980-1
   Broder A. Z., 1997, P COMPR COMPL SEQ, P21, DOI DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7
   CARTER JL, 1979, J COMPUT SYST SCI, V18, P143, DOI 10.1016/0022-0000(79)90044-8
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Christiani T, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P31
   Dahlgaard Soren, 2013, 2013 IEEE International Conference on Big Data, P28, DOI 10.1109/BigData.2013.6691730
   Dahlgaard S, 2015, ANN IEEE SYMP FOUND, P1292, DOI 10.1109/FOCS.2015.83
   Dasgupta A, 2010, ACM S THEORY COMPUT, P341
   Dietzfelbinger M, 1997, J ALGORITHM, V25, P19, DOI 10.1006/jagm.1997.0873
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Kennedy Christopher, 2016, ABS160206922 CORR
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li P., 2012, P 26 ADV NEUR INF PR, P3122
   Li Ping, 2011, ADV NEURAL INFORM PR, P2672
   Li Ping, 2012, ABS12052958 CORR
   Manku Gurmeet Singh, 2007, P 16 INT C WORLD WID, P141, DOI [DOI 10.1145/1242572.1242592, 10.1145/1242572]
   Mitzenmacher M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P746
   NISAN N, 1992, COMBINATORICA, V12, P449, DOI 10.1007/BF01305237
   Patrascu M, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2716317
   Pike G., 2011, INTRO CITYHASH
   Shakhna-rovich G., 2008, IEEE T NEURAL NETWOR, V19, P377
   Shrivastava  A., 2014, P 31 INT C MACH LEAR, P557
   Shrivastava Anshumali, 2014, P 30 C UNC ART INT U, P732
   Terasawa K, 2007, LECT NOTES COMPUT SC, V4619, P27
   Thorup M, 2012, SIAM J COMPUT, V41, P293, DOI 10.1137/100800774
   Thorup Mikkel, 2013, P 45 ACM S THEOR COM
   Tong Simon, 2010, LESSONS LEARNED DEV
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
NR 37
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406066
DA 2019-06-15
ER

PT S
AU Daskalakis, C
   Dikkala, N
   Kamath, G
AF Daskalakis, Constantinos
   Dikkala, Nishanth
   Kamath, Gautam
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Concentration of Multilinear Functions of the Ising Model with
   Applications to Network Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SYSTEMS
AB We prove near-tight concentration of measure for polynomial functions of the Ising model under high temperature. For any degree d, we show that a degreed polynomial of a n-spin Ising model exhibits exponential tails that scale as exp(-r(2/d)) at radius r = (Omega) over tilde (d) (n(d/2)). Our concentration radius is optimal up to logarithmic factors for constant d, improving known results by polynomial factors in the number of spins. We demonstrate the efficacy of polynomial functions as statistics for testing the strength of interactions in social networks in both synthetic and real world data.
C1 [Daskalakis, Constantinos; Dikkala, Nishanth; Kamath, Gautam] MIT, EECS, Cambridge, MA 02139 USA.
   [Daskalakis, Constantinos; Dikkala, Nishanth; Kamath, Gautam] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Daskalakis, C (reprint author), MIT, EECS, Cambridge, MA 02139 USA.; Daskalakis, C (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM costis@csail.mit.edu; nishanthd@csail.mit.edu; g@csail.mit.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1617730, CCF-1650733]; ONR [N00014-12-1-0999]
FX Research was supported by NSF CCF-1617730, CCF-1650733, and ONR
   N00014-12-1-0999. Part of this work was done while GK was an intern at
   Microsoft Research New England.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Bhattacharya Bhaswar B., 2016, INFERENCE ISING MODE
   Bhattacharya Bhaswar B., 2016, ARXIV150807530
   Bresler G., 2015, P 47 ANN ACM S THEOR, P771
   Bresler G., 2014, NIPS, P2852
   Bresler Guy, 2016, ARXIV160406749
   Cantador I., 2011, P 5 ACM C REC SYST, P387, DOI DOI 10.1145/2043932.2044016
   Chatterjee S, 2007, ANN STAT, V35, P1931, DOI 10.1214/009053607000000109
   Chatterjee Sourav, 2005, THESIS
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Csiszar I, 2006, ANN STAT, V34, P123, DOI 10.1214/009053605000000912
   Daskalakis C, 2011, PROBAB THEORY REL, V149, P149, DOI 10.1007/s00440-009-0246-2
   Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S
   del Campo Abraham Martin, 2016, SCANDINAVIAN J STAT
   Deser Stanley, 1968, STAT PHYS PHASE TRAN
   Easley D., 2010, NETWORKS CROWDS MARK
   Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493
   Felsenstein J., 2004, INFERRING PHYLOGENIE
   FREEDMAN DA, 1975, ANN PROBAB, V3, P100, DOI 10.1214/aop/1176996452
   Geman S., 1986, P INT C MATH, P1496
   Gheissari Reza, 2017, ARXIV170600121
   Hamilton Linus, 2017, ADV NEURAL INFORM PR
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577
   Jalali A., 2011, NIPS, V24, P1935
   Klivans Adam, 2017, P 58 ANN IEEE S FDN
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Mackey L, 2014, ANN PROBAB, V42, P906, DOI 10.1214/13-AOP892
   Montanari A, 2010, P NATL ACAD SCI USA, V107, P20196, DOI 10.1073/pnas.1004098107
   Onsager L, 1944, PHYS REV, V65, P117, DOI 10.1103/PhysRev.65.117
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   SHERRINGTON D, 1975, PHYS REV LETT, V35, P1792, DOI 10.1103/PhysRevLett.35.1792
   STROOCK DW, 1992, COMMUN MATH PHYS, V149, P175, DOI 10.1007/BF02096629
   Vuffray M, 2016, ADV NEURAL INFORM PR, P2595
NR 36
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400002
DA 2019-06-15
ER

PT S
AU Gallagher, NM
   Ulrich, K
   Talbot, A
   Dzirasa, K
   Carin, L
   Carlson, DE
AF Gallagher, Neil M.
   Ulrich, Kyle
   Talbot, Austin
   Dzirasa, Kafui
   Carin, Lawrence
   Carlson, David E.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Cross-Spectral Factor Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In neuropsychiatric disorders such as schizophrenia or depression, there is often a disruption in the way that regions of the brain synchronize with one another. To facilitate understanding of network-level synchronization between brain regions, we introduce a novel model of multisite low-frequency neural recordings, such as local field potentials (LFPs) and electroencephalograms (EEGs). The proposed model, named Cross-Spectral Factor Analysis (CSFA), breaks the observed signal into factors defined by unique spatio-spectral properties. These properties are granted to the factors via a Gaussian process formulation in a multiple kernel learning framework. In this way, the LFP signals can be mapped to a lower dimensional space in a way that retains information of relevance to neuroscientists. Critically, the factors are interpretable. The proposed approach empirically allows similar performance in classifying mouse genotype and behavioral context when compared to commonly used approaches that lack the interpretability of CSFA. We also introduce a semi-supervised approach, termed discriminative CSFA (dCSFA). CSFA and dCSFA provide useful tools for understanding neural dynamics, particularly by aiding in the design of causal follow-up experiments.
C1 [Gallagher, Neil M.; Dzirasa, Kafui] Duke Univ, Dept Neurobiol, Durham, NC 27706 USA.
   [Ulrich, Kyle; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
   [Talbot, Austin] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.
   [Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA.
RP Gallagher, NM (reprint author), Duke Univ, Dept Neurobiol, Durham, NC 27706 USA.
EM neil.gallagher@duke.edu; austin.talbot@duke.edu; kafui.dzirasa@duke.edu;
   lcarin@duke.edu; david.carlson@duke.edu
RI Jeong, Yongwook/N-7413-2016
OI Carlson, David/0000-0003-1005-6385
FU DARPA HIST program; National Institutes of Health [R01MH099192-05S2];
   W.M. Keck Foundation
FX In working on this project L.C. received funding from the DARPA HIST
   program; K.D., L.C., and D.C. received funding from the National
   Institutes of Health by grant R01MH099192-05S2; K.D received funding
   from the W.M. Keck Foundation.
CR Abelaira H. M., 2013, REV BRASILEIRA PSIQU
   Akil H., 2010, SCIENCE
   Alvarez Mauricio A, 2012, FDN TRENDS MACHINE L
   Banerjee S., 2014, HIERARCHICAL MODELIN
   Bastos AM, 2016, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00175
   Beal M.J., 2003, THESIS
   Bishop C. M, 2006, MACHINE LEARNING
   Blei D. M., 2003, J MACHINE LEARNING R
   Buzsaki G., 2012, NATURE REV NEUROSCIE
   Carlson D., 2017, BIOL PSYCHIAT
   Caruana Rich, 1997, MACHINE LEARNING
   Chen B., 2013, IEEE T PATTERN ANAL
   Cho Y, 2009, ADV NEURAL INFORM PR
   Cunningham J.P., 2014, NATURE NEUROSCIENCE
   Deisseroth K., 2011, NATURE METHODS
   Eaton W. W., 2008, EPIDEMIOLOGIC REV
   Gonen M., 2011, J MACHINE LEARNING R
   Harris A. Z., 2015, ANN REV NEUROSCIENCE
   Harris K. D., 2011, NATURE REV NEUROSCIE
   Hultman R., 2016, NEURON
   Iacoviello D., 2015, COMPUTER METHODS PRO
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kwak C., 2002, NURSING RES
   Lisman J. E., 2013, NEURON
   Mairal J., 2014, ADV NEURAL INFORM PR
   Miesenbock G., 2004, CURRENT OPINION NEUR
   Moran M. D., 2003, OIKOS
   Nestler E. J., 2010, NATURE NEUROSCIENCE
   Oppenheim AV, 1999, DISCRETE TIME SIGNAL
   Raina R., 2004, ADV NEURAL INFORM PR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Teh Y. W., 2005, AISTATS
   Uhlhaas P. J., 2008, SCHIZOPHR B
   Ulrich K. R., 2015, ADV NEURAL INFORM PR
   van Enkhuizen J., 2013, BEHAV BRAIN RES
   Wang HE, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00405
   Welch P., 1967, IEEE T AUDIO ELECTRO
   Wilson A. G., 2014, ADV NEURAL INFORM PR
   Wilson Andrew Gordon, 2013, P 30 INT C MACH LEAR
   Zhou M., 2009, ADV NEURAL INFORM PR
NR 40
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406087
DA 2019-06-15
ER

PT S
AU Gunasekar, S
   Woodworth, B
   Bhojanapalli, S
   Neyshabur, B
   Srebro, N
AF Gunasekar, Suriya
   Woodworth, Blake
   Bhojanapalli, Srinadh
   Neyshabur, Behnam
   Srebro, Nathan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Implicit Regularization in Matrix Factorization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RANK; NORM
AB We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.
C1 [Gunasekar, Suriya; Woodworth, Blake; Bhojanapalli, Srinadh; Neyshabur, Behnam; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA.
RP Gunasekar, S (reprint author), TTI Chicago, Chicago, IL 60637 USA.
EM suriya@ttic.edu; blake@ttic.edu; srinadh@ttic.edu; behnam@ttic.edu;
   nati@ttic.edu
RI Jeong, Yongwook/N-7413-2016
CR Amit Y., 2007, P INT C MACH LEARN, V24, P17, DOI DOI 10.1145/1273496.1273499
   Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Bhojanapalli Srinadh, 2016, ADV NEURAL INFORM PR
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Foygel R., 2011, COLT, V19, P315
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Jones E., 2001, SCIPY OPEN SOURCE SC
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Keshavan R. H., 2012, THESIS
   Keskar Nitish Shirish, 2016, INT C LEARN REPR
   Lee Jason D., 2016, 29 ANN C LEARN THEOR
   Neyshabur Behnam, 2017, ARXIV170503071
   Neyshabur Behnam, 2015, INT C LEARN REPR
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Srebro Nathan, 2005, ADV NEURAL INFORM PR, P1321
   Zhang Chiyuan, 2017, INT C LEARN REPR
NR 18
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406022
DA 2019-06-15
ER

PT S
AU Gulrajani, I
   Ahmed, F
   Arjovsky, M
   Dumoulin, V
   Courville, A
AF Ishaan Gulrajani
   Ahmed, Faruk
   Arjovsky, Martin
   Dumoulin, Vincent
   Courville, Aaron
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Improved Training of Wasserstein GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.
C1 [Ishaan Gulrajani; Ahmed, Faruk; Dumoulin, Vincent; Courville, Aaron] Montreal Inst Learning Algorithms, Montreal, PQ, Canada.
   [Arjovsky, Martin] Courant Inst Math Sci, New York, NY USA.
RP Gulrajani, I (reprint author), Montreal Inst Learning Algorithms, Montreal, PQ, Canada.
EM igul222@gmail.com; faruk.ahmed@umontreal.ca; ma4371@nyu.edu;
   vincent.dumoulin@umontreal.ca; aaron.couryille@umontreal.ca
CR Arjovsky M., 2017, ARXIV170107875
   Arjovsky M., 2017, PRINCIPLED METHODS T
   Ba J. L., 2016, ARXIV160706450
   Berthelot D., 2017, ARXIV170310717
   Che T., 2017, ARXIV170207983
   Chelba C., 2013, ARXIV13123005
   Dai Zihang, 2017, ARXIV170201691
   Dumoulin V., 2017, ADVERSARIALLY LEARNE
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hjelm RD, 2017, ARXIV170208431
   Huang  X., 2016, ARXIV161204357
   Jang Eric, 2016, ARXIV161101144
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Li J., 2017, ARXIV170106547
   Liang Xiaodan, 2017, ARXIV170307022
   Maddison Chris J, 2016, ARXIV161100712
   Mao Xudong, 2016, ARXIV161104076
   Metz L., 2016, ARXIV161102163
   Odena A., 2016, ARXIV161009585
   Poole B., 2016, ARXIV161202780
   Radford A., 2015, ARXIV151106434
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang D., 2016, ARXIV161101722
   Warde-Farley D., 2017, IMPROVING GENERATIVE
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu Yuhuai, 2016, ARXIV161104273
   Yang Z., 2017, ARXIV170304887
   Yu F., 2015, ARXIV150603365
   Yu L, 2016, ARXIV160905473
NR 31
TC 1
Z9 1
U1 7
U2 7
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405082
DA 2019-06-15
ER

PT S
AU Jin, WG
   Coley, CW
   Barzilay, R
   Jaakkola, T
AF Jin, Wengong
   Coley, Connor W.
   Barzilay, Regina
   Jaakkola, Tommi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center - the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.
C1 [Jin, Wengong; Barzilay, Regina; Jaakkola, Tommi] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
   [Coley, Connor W.] MIT, Dept Chem Engn, Cambridge, MA 02139 USA.
RP Jin, WG (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
EM wengong@csail.mit.edu; regina@csail.mit.edu; tommi@csail.mit.edu;
   ccoley@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA Make-It program [ARO W911NF-16-2-0023]
FX We thank Tim Jamison, Darsh Shah, Karthik Narasimhan and the reviewers
   for their helpful comments. We also thank members of the MIT Department
   of Chemistry and Department of Chemical Engineering who participated in
   the human benchmarking study. This work was supported by the DARPA
   Make-It program under contract ARO W911NF-16-2-0023.
CR Chen JH, 2009, J CHEM INF MODEL, V49, P2034, DOI 10.1021/ci900157k
   Christ CD, 2012, J CHEM INF MODEL, V52, P1745, DOI 10.1021/ci300116p
   Coley Connor W, 2017, ACS CENTRAL SCI
   Dai H., 2016, ARXIV160305629
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Gilmer J., 2017, ARXIV170401212
   Hartenfeller M, 2011, J CHEM INF MODEL, V51, P3093, DOI 10.1021/ci200379p
   Kayala MA, 2011, J CHEM INF MODEL, V51, P2209, DOI 10.1021/ci200207y
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Kingma D. P., 2015, INT C LEARN REPR
   Law J, 2009, J CHEM INF MODEL, V49, P593, DOI 10.1021/ci800228y
   Lei Tao, 2017, P 34 INT C MACH LEAR
   Lowe D. M., 2014, PATENT REACTION EXTR
   Segler Marwin HS, 2017, CHEM EUROPEAN J
   Szymkuc S, 2016, ANGEW CHEM INT EDIT, V55, P5904, DOI 10.1002/anie.201506101
   Todd MH, 2005, CHEM SOC REV, V34, P247, DOI 10.1039/b104620a
   Warr WA, 2014, MOL INFORM, V33, P469, DOI 10.1002/minf.201400052
   Wei JN, 2016, ACS CENTRAL SCI, V2, P725, DOI 10.1021/acscentsci.6b00219
NR 18
TC 1
Z9 1
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402064
DA 2019-06-15
ER

PT S
AU Kilbertus, N
   Rojas-Carulla, M
   Parascandolo, G
   Hardt, M
   Janzing, D
   Scholkopf, B
AF Kilbertus, Niki
   Rojas-Carulla, Mateo
   Parascandolo, Giambattista
   Hardt, Moritz
   Janzing, Dominik
   Scholkopf, Bernhard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Avoiding Discrimination through Causal Reasoning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively.
   Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about our model of the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.
C1 [Kilbertus, Niki; Rojas-Carulla, Mateo; Parascandolo, Giambattista; Janzing, Dominik; Scholkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
   [Kilbertus, Niki; Rojas-Carulla, Mateo] Univ Cambridge, Cambridge, England.
   [Parascandolo, Giambattista] Max Planck ETH Ctr Learning Syst, Tubingen, Germany.
   [Hardt, Moritz] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Kilbertus, N (reprint author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.; Kilbertus, N (reprint author), Univ Cambridge, Cambridge, England.
EM nkilbertus@tue.mpg.de; mrojas@tue.mpg.de; gparascandolo@tue.mpg.de;
   hardt@berkeley.edu; janzing@tue.mpg.de; bs@tue.mpg.de
RI Jeong, Yongwook/N-7413-2016
CR Angrist Joshua, 2001, TECH REP
   Berk R., 2017, ARXIV170309207V1
   BICKEL PJ, 1975, SCIENCE, V187, P398, DOI 10.1126/science.187.4175.398
   Bonchi Francesco, 2017, ARXIV151000552V3
   Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x
   Chouldechova Alexandra, 2016, ARXIV161007524V1
   Cornia Nicholas, 2014, P WORKSH CAUS INF UA, P35
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Edwards Harrison, 2015, ARXIV151105897V3
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Friedler Sorelle A., 2016, ARXIV160907236V1
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Kamiran F, 2013, KNOWL INF SYST, V35, P613, DOI 10.1007/s10115-012-0584-8
   Kleinberg Jon, 2016, ARXIV160905807V1
   Kusner Matt J., 2017, ARXIV170306856V1
   Nabi R., 2017, ARXIV170510378V1
   Pearl J., 2009, CAUSALITY
   Qureshi Bilal, 2016, ARXIV160803735
   ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41
   VanderWeele TJ, 2014, EPIDEMIOLOGY, V25, P473, DOI 10.1097/EDE.0000000000000105
   Zafar M. B., 2017, ARTIF INTELL, P962
   Zafar MB, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1171, DOI 10.1145/3038912.3052660
   Zemel R., 2013, JMLR P, P325
   Zhang LF, 2017, INTERNATIONAL SYMPOSIUM 2017: SOCIAL SCIENCE MANAGEMENT AND INNOVATION, P1
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400063
DA 2019-06-15
ER

PT S
AU Li, CL
   Chang, WC
   Cheng, Y
   Yang, YM
   Poczos, B
AF Li, Chun-Liang
   Chang, Wei-Cheng
   Cheng, Yu
   Yang, Yiming
   Poczos, Barnabas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI MMD GAN: Towards Deeper Understanding of Moment Matching Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD).' Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak* topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD GAN significantly outperforms GMMN, and is competitive with other representative GAN works.
C1 [Li, Chun-Liang; Chang, Wei-Cheng; Yang, Yiming; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Cheng, Yu] IBM Res, AI Fdn, Yorktown Hts, NY USA.
RP Li, CL (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM chunlia@cs.cmu.edu; wchang2@cs.cmu.edu; chengyu@us.ibm.com;
   yiming@cs.cmu.edu; bapoczos@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation (NSF) [IIS-1546329, IIS-1563887]
FX We thank the reviewers for their helpful comments. This work is
   supported in part by the National Science Foundation (NSF) under grants
   IIS-1546329 and IIS-1563887.
CR Arjovsky  M., 2017, ICML
   Arora S., 2017, ARXIV170300573
   Bellemare M. G., 2017, ARXIV170510743
   Berthelot D., 2017, ARXIV170310717
   Dumoulin V., 2017, ICLR
   Dziugaite G. K., 2015, UAI
   Fukumizu Kenji, 2009, NIPS
   Goodfellow I., 2014, NIPS
   Gretton A., 2012, JMLR
   Gretton A., 2012, NIPS
   Gretton Arthur, 2017, NOTES CRAMER GAN
   Gulrajani Ishaan, 2017, ARXIV170400028
   Herbrich R., 1999, SUPPORT VECTOR LEARN
   Kingma D. P., 2013, ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y., 1998, P IEEE
   Li Y., 2015, ICML
   Liu Ziwei, 2015, CVPR
   Makhzani A., 2015, ARXIV151105644
   Mroueh Youssef, 2017, 170208398 ARXIV
   Muandet Krikamol, 2016, ARXIV160509522
   Nowozin Sebastian, 2016, NIPS
   Radford A., 2016, ICLR
   Salakhutdinov R., 2009, AISTATS
   Salimans T., 2016, NIPS
   Sriperumbudur Bharath K., 2010, JMLR
   Sutherland D., 2017, ICLR
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Ulyanov Dmitry, 2017, ARXIV170402304
   Warde-Farley David, 2017, ICLR
   Wasserman  L., 2013, ALL STAT CONCISE COU
   Wilson A. G., 2016, AISTATS
   Xu K, 2015, ICML
   Yu F., 2015, ARXIV150603365
   Zellinger  W., 2017, ARXIV170208811
   Zhai Shuangfei, 2016, CORR
   Zhao J., 2017, ICLR
NR 37
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402025
DA 2019-06-15
ER

PT S
AU Liu, JZ
   Coull, B
AF Liu, Jeremiah Zhe
   Coull, Brent
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This work constructs a hypothesis test for detecting whether an data-generating function h : R-p -> R belongs to a specific reproducing kernel Hilbert space H-0, where the structure of H-0 is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results reveal interesting connections between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlight unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.
C1 [Liu, Jeremiah Zhe; Coull, Brent] Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA.
RP Liu, JZ (reprint author), Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA.
EM zhl112@mail.harvard.edu; bcoull@hsph.harvard.edu
RI Jeong, Yongwook/N-7413-2016
CR ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Bodenham DA, 2016, STAT COMPUT, V26, P917, DOI 10.1007/s11222-015-9583-4
   Cortes C., 2012, ARXIV12023712CSSTAT
   Cortes C., 2010, TWO STAGE LEARNING K
   Davies A., 2014, ARXIV14024293CSS
   Elisseeff A., 2002, LEARNING THEORY PRAC
   Evgeniou T, 2004, MACH LEARN, V55, P71, DOI 10.1023/B:MACH.0000019805.88351.60
   Evgeniou T., 2000, P 17 INT C MACH LEAR, P271
   Gu C., 2013, SMOOTHING SPLINE ANO
   HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Lin XH, 1997, BIOMETRIKA, V84, P309, DOI 10.1093/biomet/84.2.309
   Liu D, 2007, BIOMETRICS, V63, P1079, DOI 10.1111/j.1541-0420.2007.00799.x
   Lukic MN, 2001, T AM MATH SOC, V353, P3945, DOI 10.1090/S0002-9947-01-02852-5
   Maity A, 2011, BIOMETRICS, V67, P1271, DOI 10.1111/j.1541-0420.2011.01598.x
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Snoek J., 2012, ARXIV12062944CSSTAT
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wilson A. G., 2015, ARXIV151102222CSSTAT
   Zhang DW, 2003, BIOSTATISTICS, V4, P57, DOI 10.1093/biostatistics/4.1.57
NR 20
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400076
DA 2019-06-15
ER

PT S
AU Liu, MY
   Breuel, T
   Kautz, J
AF Liu, Ming-Yu
   Breuel, Thomas
   Kautz, Jan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised Image-to-Image Translation Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.
C1 [Liu, Ming-Yu; Breuel, Thomas; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA.
RP Liu, MY (reprint author), NVIDIA, Santa Clara, CA 95051 USA.
EM mingyul@nvidia.com; tbreuel@nvidia.com; jkautz@nvidia.com
RI Jeong, Yongwook/N-7413-2016
CR Arjovsky M, 2017, ARXIV170107875
   Cordts M., 2015, C COMP VIS PATT REC
   Denton E., 2015, ADV NEURAL INFORM PR
   Fernando B., 2013, INT C COMP VIS
   Ganin Yaroslav, 2016, J MACHINE LEARNING R
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   He K, 2016, COMPUTER VISION PATT
   Isola Phillip, 2017, C COMP VIS PATT REC
   Johnson J., 2016, EUR C COMP VIS
   Kim T., 2017, INT C MACH LEARN
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma D. P., 2016, ADV NEURAL INFORM PR
   Kingma Diederik, 2014, INT C LEARN REPR
   Larsen A. B. L., 2016, INT C MACH LEARN
   Ledig C., 2017, C COMP VIS PATT REC
   Lindvall T., 2002, LECT COUPLING METHOD
   Liu M.-Y., 2016, ADV NEURAL INFORM PR
   Liu Z., 2015, INT C COMP VIS
   Maaloe L., 2016, INT C MACH LEARN
   Netzer  Yuval, 2011, ADV NEUR INF PROC SY
   Radford A., 2016, INT C LEARN REPR
   Rezende D. J., 2014, INT C MACH LEARN
   Ros G., 2016, C COMP VIS PATT REC
   Salimans  T., 2016, ADV NEURAL INFORM PR
   Shrivastava A., 2017, C COMP VIS PATT REC
   Taigman  Y., 2017, INT C LEARN REPR
   van den Oord A, 2016, ADV NEURAL INFORM PR
   Yan X., 2016, EUR C COMP VIS
   Zhu J. Y., 2017, INT C COMP VIS
NR 29
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400067
DA 2019-06-15
ER

PT S
AU Liu, Q
AF Liu, Qiang
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Stein Variational Gradient Descent as Gradient Flow
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.
C1 [Liu, Qiang] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
RP Liu, Q (reprint author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
EM qiang.liu@dartmouth.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CRII [1565796]
FX This work is supported in part by NSF CRII 1565796. We thank Lester
   Mackey and the anonymous reviewers for their comments.
CR Berlinet A., 2011, REPRODUCING KERNEL H
   BRAUN W, 1977, COMMUN MATH PHYS, V56, P101, DOI 10.1007/BF01611497
   Chen Y., 2010, C UNC ART INT UAI
   Dai B., 2016, 19 INT C ART INT STA
   DelMoral P, 2013, MONOGR STAT APPL PRO, V126, P1
   Dick J, 2013, ACTA NUMER, V22, P133, DOI 10.1017/S0962492913000044
   Gorham Jackson, 2017, INT C MACH LEARN ICM
   Gretton Arthur, 2016, INT C MACH LEARN ICM
   Han J., 2017, UNCERTAINTY ARTIFICI
   Liu  Q., 2016, ADV NEURAL INFORM PR
   Liu Q., 2016, INT C MACH LEARN ICM
   OATES C. J., 2016, ARXIV160303220
   Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185
   Otto F., 2001, THE GEOMETRY OF DISS
   Spohn H., 2012, LARGE SCALE DYNAMICS
   Stein C, 1986, LECT NOTES MONOGRAPH, V7, P164
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Vlasov A. A., 1938, SOV PHYS JETP, V8, P291
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403018
DA 2019-06-15
ER

PT S
AU Lowe, R
   Wu, Y
   Tamar, A
   Harb, J
   Abbeel, P
   Mordatch, I
AF Lowe, Ryan
   Wu, Yi
   Tamar, Aviv
   Harb, Jean
   Abbeel, Pieter
   Mordatch, Igor
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.
C1 [Lowe, Ryan; Harb, Jean] McGill Univ, Montreal, PQ H3A 2T5, Canada.
   [Lowe, Ryan; Harb, Jean; Abbeel, Pieter; Mordatch, Igor] OpenAI, San Francisco, CA 94110 USA.
   [Wu, Yi; Tamar, Aviv; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Lowe, R (reprint author), McGill Univ, Montreal, PQ H3A 2T5, Canada.; Lowe, R; Mordatch, I (reprint author), OpenAI, San Francisco, CA 94110 USA.; Wu, Y (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM ryan.lowe@cs.mcgillca; jxwuyi@gmail.com; mordatch@openai.com
RI Jeong, Yongwook/N-7413-2016
FU Vanier CGS Scholarship; Samsung Advanced Institute of Technology
FX The authors would like to thank Jacob Andreas, Smitha Milli, Jack Clark,
   Jakob Foerster, and others at OpenAI and UC Berkeley for interesting
   discussions related to this paper, as well as Jakub Pachocki, Yura
   Burda, and Joelle Pineau for comments on the paper draft. We thank
   Tambet Matiisen for providing the code base that was used for some early
   experiments associated with this paper. Ryan Lowe is supported in part
   by a Vanier CGS Scholarship and the Samsung Advanced Institute of
   Technology. Finally, we'd like to thank OpenAI for fostering an engaging
   and productive research environment.
CR Abadi M., 2016, ARXIV161006918
   Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P106
   Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Chalkiadakis G., 2003, P 2 INT JOINT C AUT, P709, DOI 10.1145/860575.860689
   Dayan P., 1993, ADV NEURAL INFORMATI, V5, P271
   Foerster J. N., 2017, ABS170208887 CORR
   Foerster J. N., 2016, ABS160506676 CORR
   Foerster Jakob N., 2017, ARXIV170508926
   Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gupta J. K., 2017, COOPERATIVE MULTIAGE
   Jang Eric, 2016, ARXIV161101144
   Junling Hu, 1998, Proceedings of the Second International Conference on Autonomous Agents, P239
   Lauer M., 2000, P 17 INT C MACH LEAR, P535
   Lazaridou Angeliki, 2016, ARXIV161207182
   Leibo J. Z., 2017, ABS170203037 CORR
   Levine S., 2015, ARXIV150400702
   Littman M., 1994, P 11 INT C MACH LEAR, V157, P157
   Matignon L., 2012, AAAI
   Matignon L, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P64, DOI 10.1109/IROS.2007.4399095
   Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mordatch Igor, 2017, ARXIV170304908
   Omidshafiei S., 2017, ABS170306182 CORR
   Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2
   Peng P., 2017, ABS170310069 CORR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sukhbaatar S, 2016, ADV NEURAL INFORM PR, V29, P2244
   Sukhbaatar S., 2017, ARXIV170305407
   Sutton R., 2000, ADV NEURAL INFORM PR
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395
   Tan Ming, 1993, P 10 INT C MACH LEAR, P330
   Tesauro G, 2004, ADV NEUR IN, V16, P871
   Thomas P. S., 2011, P 28 INT C MACH LEAR, P137
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 36
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406044
DA 2019-06-15
ER

PT S
AU Lundberg, SM
   Lee, SI
AF Lundberg, Scott M.
   Lee, Su-In
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Unified Approach to Interpreting Model Predictions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.
C1 [Lundberg, Scott M.] Univ Washington, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA.
   [Lee, Su-In] Univ Washington, Dept Genome Sci, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA.
RP Lundberg, SM (reprint author), Univ Washington, Paul G Allen Sch Comp Sci, Seattle, WA 98105 USA.
EM slund1@cs.washington.edu; suinlee@cs.washington.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation (NSF) [DBI-135589]; NSF CAREER [DBI-155230];
   American Cancer Society [127332-RSG-15-097-01-TBG]; National Institute
   of Health (NIH) [AG049196]; NSF Graduate Research Fellowship
FX This work was supported by a National Science Foundation (NSF)
   DBI-135589, NSF CAREER DBI-155230, American Cancer Society
   127332-RSG-15-097-01-TBG, National Institute of Health (NIH) AG049196,
   and NSF Graduate Research Fellowship. We would like to thank Marco
   Ribeiro, Erik Strumbelj, Avanti Shrikumar, Yair Zick, the Lee Lab, and
   the NIPS reviewers for feedback that has significantly improved this
   work.
CR Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Charnes A, 1988, ECONOMETRICS PLANNIN, V11, P123
   Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42
   Lipovetsky S, 2001, APPL STOCH MODEL BUS, V17, P319, DOI 10.1002/asmb.446
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Shapley L. S., 1953, ANN MATH STUD, P307, DOI DOI 10.1515/9781400881970-018
   Shrikumar A., 2016, ARXIV160501713
   Shrikumar A., 2017, ARXIV170402685
   Strumbelj E, 2014, KNOWL INF SYST, V41, P647, DOI 10.1007/s10115-013-0679-x
   Young H. P., 1985, International Journal of Game Theory, V14, P65, DOI 10.1007/BF01769885
NR 10
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404081
DA 2019-06-15
ER

PT S
AU Mroueh, Y
   Sercu, T
AF Mroueh, Youssef
   Sercu, Tom
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fisher GAN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.
C1 [Mroueh, Youssef; Sercu, Tom] IBM TJ Watson Res Ctr, IBM Res AI, AI Fdn, Yorktown Hts, NY 10598 USA.
RP Mroueh, Y (reprint author), IBM TJ Watson Res Ctr, IBM Res AI, AI Fdn, Yorktown Hts, NY 10598 USA.
EM mroueh@us.ibm.com; tom.sercul@ibm.com
CR Arjovsky  M., 2017, ICML
   Arjovsky Martin, 2017, ICLR
   Ba J. L., 2016, ARXIV160706450
   Bartlett Peter L., 2005, ANN STAT
   Berthelot D., 2017, ARXIV170310717
   Dai Zihang, 2017, ARXIV170201691
   Dumoulin V., 2017, ICLR
   Dziugaite G. K., 2015, UAI
   Ekeland I., 1983, INFINITE DIMENSIONAL
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow I., 2014, NIPS
   Gretton A., 2012, JMLR
   Gulrajani Ishaan, 2017, ARXIV170400028
   Harchaoui Zaid, 2008, NIPS
   He K., 2015, ARXIV150201852
   Huang  X., 2016, ARXIV161204357
   Ioffe Sergey, 2015, P ICML
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2009, THESIS
   Li Y., 2015, ICML
   Liu  Z., 2015, ICCV
   Mao Xudong, 2016, ARXIV161104076
   Mohamed S., 2016, ARXIV161003483
   Mroueh Youssef, 2017, ICML
   Muller Alfred, 1997, ADV APPL PROBABILITY
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Nowozin Sebastian, 2016, NIPS
   Odena A., 2016, ARXIV161009585
   Radford A., 2015, ARXIV151106434
   Radford  A., 2015, ARXIV151106434
   Salimans T., 2016, NIPS
   Sonderby C. Kaae, 2017, ICLR
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Sriperumbudur Bharath K., 2009, INTEGRAL PROBABILITY
   Sriperumbudur Bharath K., 2012, ELECT J STAT
   Theis  Lucas, 2016, ICLR
   Tosi Alessandra, 2014, METRICS PROBABILISTI
   Wang D., 2016, ARXIV161101722
   Warde-Farley D, 2017, ICLR SUBMISSIONS, V8
   Yu F., 2015, ARXIV150603365
NR 40
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402055
DA 2019-06-15
ER

PT S
AU Nagarajan, V
   Kolter, JZ
AF Nagarajan, Vaishnavh
   Kolter, J. Zico
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gradient descent GAN optimization is locally stable
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization, i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which is able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.
C1 [Nagarajan, Vaishnavh; Kolter, J. Zico] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
RP Nagarajan, V (reprint author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
EM vaishnavh@cs.cmu.edu; zkolter@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   Arjovsky M., 2017, INT C LEARN REPR ICL
   Arora S., 2017, P 34 INT C MACH LEAR, V70, P224
   Borkar VS, 2000, SIAM J CONTROL OPTIM, V38, P447, DOI 10.1137/S0363012997331639
   Che Tong, 5 INT C LEARN REPR I
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani Ishaan, 2017, 31 ANN C NEUR INF PR
   Im D. J., 2016, ARXIV160205110
   Khalil HK, 1996, NONLINEAR SYSTEMS
   Kushner H., 2003, STOCHASTIC MODELLING, V35
   Ledig C., 2017, IEEE C COMP VIS PATT
   Magnus J. R., 1995, MATRIX DIFFERENTIAL
   Mathieu Michael, 2016, 4 INT C LEARN REPR I
   Mescheder L., 2017, 31 ANN C NEUR INF PR
   Metz Luke, 2017, 5 INT C LEARN REPR I
   Nguyen A, 2017, IEEE C COMP VIS PATT
   Poole B., 2016, ARXIV161202780
   Radford Alec, 2016, 4 INT C LEARN REPR I
   Roth K., 2017, 31 ANN C NEUR INF PR
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Wu J., 2016, ADV NEURAL INFORM PR, P82
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405065
DA 2019-06-15
ER

PT S
AU Nickel, M
   Kiela, D
AF Nickel, Maximilian
   Kiela, Douwe
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Poincare Embeddings for Learning Hierarchical Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space - or more precisely into an n-dimensional Poincare ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincare embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.
C1 [Nickel, Maximilian; Kiela, Douwe] Facebook AI Res, New York, NY 10003 USA.
RP Nickel, M (reprint author), Facebook AI Res, New York, NY 10003 USA.
EM maxn@fb.com; dkiela@fb.com
CR Adcock AB, 2013, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2013.77
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Asta Dena Marie, 2015, P 31 C UNC ART INT U, P102
   Boguna M, 2010, NAT COMMUN, V1, DOI 10.1038/ncomms1063
   Bojanowski P, 2016, ARXIV160704606
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Bouchard G., 2015, AAAI SPRING S KNOWL
   Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830
   Demeester T., 2016, P 2016 C EMP METH NA, P1389
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Firth J. R., 1957, STUDIES LINGUISTIC A
   Gromov M., 1987, MATH SCI RES I PUBL, V8, P75, DOI DOI 10.1007/978-1-4613-9586-7_3
   Grover Aditya, 2016, KDD, V2016, P855
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   Joulin A., 2016, ARXIV160701759
   Kiela Douwe, 2015, P ACL BEIJ, P119
   Kleinberg R, 2007, IEEE INFOCOM SER, P1902, DOI 10.1109/INFCOM.2007.221
   Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106
   Mikolov T., 2013, CORR, V1310, P4546
   Nickel M., 2014, ADV NEURAL INFORM PR, P1179
   Nickel M., 2016, P 30 AAAI C ART INT, P1955
   Nickel M., 2011, P 28 INT C MACH LEAR, P809
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Ontrup J, 2006, NEURAL NETWORKS, V19, P751, DOI 10.1016/j.neunet.2006.05.015
   Paccanaro A, 2001, IEEE T KNOWL DATA EN, V13, P232, DOI 10.1109/69.917563
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112
   Riedel S, 2013, P 2013 C N AM CHAPT, P74
   Steyvers M, 2005, COGNITIVE SCI, V29, P41, DOI 10.1207/s15516709cog2901_3
   Sun Ke, 2015, ADV NEURAL INFORM PR, V28, P100
   Trouillon  T., 2016, P 33 INT C MACH LEAR, P2071
   Vendrov I., 2015, ARXIV151106361
   Vilnis Luke, 2015, INT C LEARN REPR ICL
   Vulic Ivan, 2016, ARXIV160802117
   Weeds J., 2014, P COLING 2014 25 INT, P2249
   Zhang H., 2016, P ADV NEUR INF PROC, P4592
   Zipf GK., 1949, HUMAN BEHAV PRINCIPL
NR 40
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406040
DA 2019-06-15
ER

PT S
AU Pedregosa, F
   Leblond, R
   Lacoste-Julien, S
AF Pedregosa, Fabian
   Leblond, Remi
   Lacoste-Julien, Simon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze PROXASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.
C1 [Pedregosa, Fabian; Leblond, Remi] PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.
   [Lacoste-Julien, Simon] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada.
RP Pedregosa, F (reprint author), PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.
RI Jeong, Yongwook/N-7413-2016
FU Google Research Award; chaire Economie des nouvelles donnees; fonds AXA
   pour la recherche; data science joint research initiative
FX This work was partially supported by a Google Research Award. FP
   acknowledges support from the chaire Economie des nouvelles donnees with
   the data science joint research initiative with the fonds AXA pour la
   recherche.
CR Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Beck A., 2009, CONVEX OPTIMIZATION
   Davis Damek, 2016, ADV NEURAL INFORM PR, V29
   Defazio A., 2014, ADV NEURAL INFORM PR
   Gu Bin, 2016, ARXIV161009447V3
   Hsieh Cho-Jui, 2015, ICML
   Johnson R., 2013, ADV NEURAL INFORM PR
   Juan Yuchin, 2016, P 10 ACM C REC SYST
   Le Roux N., 2012, ADV NEURAL INFORM PR
   Liu Ji, 2015, SIAM J OPTIMIZATION
   Mania Horia, 2017, SIAM J OPTIMIZATION
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Yurii, 2013, MATH PROGRAMMING
   Niu Feng, 2011, ADV NEURAL INFORM PR
   Peng Zhimin, 2016, SIAM J SCI COMPUTING
   Qi Meng, 2017, AAAI
   Reddi Sashank J, 2015, ADV NEURAL INFORM PR
   Remi Leblond, 2017, P 20 INT C ART INT S
   Schmidt  M., 2016, MATH PROGRAMMING
   Shalev-Shwartz S., 2012, ARXIV12112717
   Shalev-Shwartz S., 2013, J MACHINE LEARNING R
   Xiao Lin, 2014, SIAM J OPTIMIZATION
   You Yang, 2016, ADV NEURAL INFORM PR
   Yu H.-F., 2010, KDD CUP
   Zhao Tuo, 2014, ADV NEURAL INFORM PR
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400006
DA 2019-06-15
ER

PT S
AU Qi, CR
   Yi, L
   Su, H
   Guibas, LJ
AF Qi, Charles R.
   Yi, Li
   Su, Hao
   Guibas, Leonidas J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI PointNet plus plus : Deep Hierarchical Feature Learning on Point Sets in
   a Metric Space
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.
C1 [Qi, Charles R.; Yi, Li; Su, Hao; Guibas, Leonidas J.] Stanford Univ, Stanford, CA 94305 USA.
RP Qi, CR (reprint author), Stanford Univ, Stanford, CA 94305 USA.
FU Samsung GRO; NSF [IIS-1528025, DMS-1546206]; ONR MURI [N00014-13-1-0341]
FX The authors would like to acknowledge the support of a Samsung GRO
   grant, NSF grants IIS-1528025 and DMS-1546206, and ONR MURI grant
   N00014-13-1-0341.
CR Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Belton D., 2006, INT ARCH PHOTOGRAM 5, V5, P44
   Bruna J., 2013, ABS13126203 CORR
   Chang A. X., 2015, ARXIV151203012CSGR
   Dai A., 2017, ARXIV170204405
   Demantke J., 2011, INT ARCH PHOTOGRAM 5, V38, pW12, DOI DOI 10.5194/ISPRSARCHIVES-XXXVIII-5-W12-97-2011
   Gressin A, 2013, ISPRS J PHOTOGRAMM, V79, P240, DOI 10.1016/j.isprsjprs.2013.02.019
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Kingma D., ARXIV14126980
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lian  Z., 2015, EUR WORKSH 3D OBJ RE
   Lin M., 2013, ARXIV13124400
   Luciano  L., 2017, PATTERN RECOGNITION
   Masci J., 2015, P IEEE INT C COMP VI, P37
   Meyer M., 2002, VISUALIZATION MATH, V3, P52
   Mitra NJ, 2004, INT J COMPUT GEOM AP, V14, P261, DOI 10.1142/S0218195904001470
   Occipital I., 2016, STRUCTURE SENSOR 3D
   Pauly M, 2006, ACM T GRAPHIC, V25, P177, DOI 10.1145/1138450.1138451
   Qi C. R., 2016, ARXIV161200593
   Qi C. R., 2016, P COMP VIS PATT REC
   Riegler G., 2016, CORR
   Rustamov RM, 2009, COMPUT GRAPH FORUM, V28, P1279, DOI 10.1111/j.1467-8659.2009.01505.x
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Su H., 2015, P ICCV
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Vinyals O., 2015, ARXIV151106391
   Wang P.-S., 2017, O CNN OCTREE BASED C
   Weinmann M, 2015, ISPRS J PHOTOGRAMM, V105, P286, DOI 10.1016/j.isprsjprs.2015.01.016
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801
   Yi L., 2016, ARXIV161200606
   Yi Li, 2016, SIGGRAPH ASIA
NR 33
TC 1
Z9 1
U1 14
U2 14
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405018
DA 2019-06-15
ER

PT S
AU Ratner, AJ
   Ehrenberg, HR
   Hussain, Z
   Dunnmon, J
   Re, C
AF Ratner, Alexander J.
   Ehrenberg, Henry R.
   Hussain, Zeshan
   Dunnmon, Jared
   Re, Christopher
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning to Compose Domain-Specific Transformations for Data
   Augmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TEXT CATEGORIZATION
AB Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.
C1 [Ratner, Alexander J.; Ehrenberg, Henry R.; Hussain, Zeshan; Dunnmon, Jared; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA.
RP Ratner, AJ (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ajratner@cs.stanford.edu; henryre@cs.stanford.edu;
   zeshanmh@cs.stanford.edu; jdunnmon@cs.stanford.edu;
   chrismre@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Defense Advanced Research Projects Agency (DARPA) SIMPLEX program
   [N66001-15-C-4043]; DARPA D3M program [FA8750-17-2-0095]; DARPA programs
   [FA8750-12-2-0335, FA8750-13-2-0039]; National Institute of Health (NIH)
   [U54EB020405]; Office of Naval Research (ONR) [N000141210041,
   N000141310129]; Moore Foundation; Okawa Research Grant; American Family
   Insurance; Intel; Stanford DAWN project: Intel; Microsoft; VMware;
   Teradata; DOE [108845]; DARPA [FA8750-17-2-0095]; Accenture; Toshiba
FX We would like to thank Daniel Selsam, Ioannis Mitliagkas, Christopher De
   Sa, William Hamilton, and Daniel Rubin for valuable feedback and
   conversations. We gratefully acknowledge the support of the Defense
   Advanced Research Projects Agency (DARPA) SIMPLEX program under No.
   N66001-15-C-4043, the DARPA D3M program under No. FA8750-17-2-0095,
   DARPA programs No. FA8750-12-2-0335 and FA8750-13-2-0039, DOE 108845,
   National Institute of Health (NIH) U54EB020405, the Office of Naval
   Research (ONR) under awards No. N000141210041 and No. N000141310129, the
   Moore Foundation, the Okawa Research Grant, American Family Insurance,
   Accenture, Toshiba, and Intel. This research was also supported in part
   by affiliate members and other supporters of the Stanford DAWN project:
   Intel, Microsoft, Teradata, and VMware. This material is based on
   research sponsored by DARPA under agreement number FA8750-17-2-0095. The
   U.S. Government is authorized to reproduce and distribute reprints for
   Governmental purposes notwithstanding any copyright notation thereon.
   Any opinions, findings, and conclusions or recommendations expressed in
   this material are those of the authors and do not necessarily reflect
   the views, policies, or endorsements, either expressed or implied, of
   DARPA, AFRL, NSF, NIH, ONR, or the U.S. Government.
CR Baluja S., 2017, ARXIV170309387
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Ciresan D. C., 2010, DEEP BIG SIMPLE NEUR, P80
   Clark K, 2013, J DIGIT IMAGING, V26, P1045, DOI 10.1007/s10278-013-9622-7
   DeVries T., 2017, ARXIV170205538
   Doddington GR, 2004, C LANG RES EV, V2, P1
   Dosovitskiy A., 2015, ARXIV150602753
   Fawzi A, 2016, IEEE IMAGE PROC, P3688, DOI 10.1109/ICIP.2016.7533048
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow IJ, 2014, ARXIV14126572
   GRAHAM Benjamin, 2014, ARXIV14126071
   Greensmith E, 2004, J MACH LEARN RES, V5, P1471
   Hauberg S, 2016, ARTIF INTELL, P342
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Heath M., 2000, P 5 INT WORKSH DIG M, P212
   Huang G, 2016, ARXIV160806993
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee R. Sawyer, 2016, CANC IMAGING ARCHIVE
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lu XH, 2006, J AM MED INFORM ASSN, V13, P526, DOI 10.1197/jamia.M2051
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Miyato  T., 2015, ARXIV150700677
   Radford A., 2015, ARXIV151106434
   Sajjadi M. S. M., 2016, CORR
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Schulman  J., 2015, ADV NEURAL INFORM PR, P3528
   Sixt L., 2016, ARXIV161101331
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Teo C.H., 2008, ADV NEURAL INFORM PR, V20, P1489
   Uhlich S., 2017, ICASSP
   Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049
NR 32
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403030
DA 2019-06-15
ER

PT S
AU Regier, J
   Jordan, MI
   McAuliffe, J
AF Regier, Jeffrey
   Jordan, Michael, I
   McAuliffe, Jon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fast Black-box Variational Inference through Stochastic Trust-Region
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the "reparameterization trick." At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information, but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI, demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI, demonstrating that our convergence theory can matter in practice.
EM jregier@cs.berkeley.edu; jordan@cs.berkeley.edu; jon@stat.berkeley.edu
RI Jeong, Yongwook/N-7413-2016
CR Blei  David, 2017, J AM STAT ASS
   Carpenter B, 2016, J STAT SOFTWARE, V20
   Cheng RQ, 2018, MULTIMED TOOLS APPL, V77, P20651, DOI 10.1007/s11042-017-5472-5
   Deng G, 2009, MATH PROGRAM, V117, P81, DOI 10.1007/s10107-007-0164-y
   EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462
   Fan K., 2015, ADV NEURAL INFORM PR
   Fike J. A, 2012, LECT NOTES COMPUTATI, P163, DOI DOI 10.1007/978-3-642-30023-3_15
   Gelman A., 2006, DATA ANAL USING REGR
   Gould NIM, 1999, SIAM J OPTIMIZ, V9, P504, DOI 10.1137/S1052623497322735
   Kingma Diederik, 2014, INT C LEARN REPR
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Lenders Felix, 2016, ARXIV161104718
   Lunn D., 2012, BUGS BOOK PRACTICAL
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   OpenBugs developers, ALL MULT LOG REGR
   OpenBugs developers, RATS NORM HIER MOD
   OpenBugs developers, SEEDS RAND EFF LOG R
   OpenBugs developers, DYES VAR COMP MOD
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Regier Jeffrey, 2016, ARXIV161103404
   Rezende D. J., 2014, INT C MACH LEARN
   Shashaani Sara, 2016, IEEE WINT SIM C
   Spall JC, 2005, INTRO STOCHASTIC SEA
   Titsias M. K., 2014, INT C MACH LEARN
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402044
DA 2019-06-15
ER

PT S
AU Santoro, A
   Raposo, D
   Barrett, DGT
   Malinowski, M
   Pascanu, R
   Battaglia, P
   Lillicrap, T
AF Santoro, Adam
   Raposo, David
   Barrett, David G. T.
   Malinowski, Mateusz
   Pascanu, Razvan
   Battaglia, Peter
   Lillicrap, Timothy
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A simple neural network module for relational reasoning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties.
C1 [Santoro, Adam; Raposo, David; Barrett, David G. T.; Malinowski, Mateusz; Pascanu, Razvan; Battaglia, Peter; Lillicrap, Timothy] DeepMind, London, England.
RP Santoro, A (reprint author), DeepMind, London, England.
EM adamsantoro@google.com; draposo@google.com; barrettdavid@google.com;
   mateuszm@google.com; razp@google.com; peterbattaglia@google.com;
   countzero@google.com
RI Jeong, Yongwook/N-7413-2016
CR Antol S., 2015, ICCV
   Battaglia Peter W., 2016, NIPS
   Garnelo M., 2016, ARXIV160905518
   Graves Alex, 2016, NATURE
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Henaff M., 2017, ICLR
   Johnson J., 2017, CVPR
   Johnson J., 2017, ARXIV170503633
   Kafle K., 2017, ARXIV170309684
   Kemp C, 2008, P NATL ACAD SCI USA, V105, P10687, DOI 10.1073/pnas.0802631105
   Lake B. M., 2016, ARXIV160400289
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Y., 2016, ICLR
   Malinowski M., 2016, ARXIV160502697
   Malinowski M, 2014, NIPS
   NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1207/s15516709cog0402_2
   Rae Jack, 2016, NIPS
   Raposo David, 2017, ARXIV170205068
   Ren M., 2015, NIPS
   Scarselli F., 2009, IEEE T NEURAL NETWOR
   Todorov Emanuel, 2012, IROS
   Weston J, 2015, ARXIV150205698
   Weston Jason, 2015, ICLR
   Yang Z, 2016, CVPR
NR 24
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405005
DA 2019-06-15
ER

PT S
AU Speiser, A
   Yan, JY
   Archer, E
   Buesing, L
   Turaga, SC
   Macke, JH
AF Speiser, Artur
   Yan, Jinyao
   Archer, Evan
   Buesing, Lars
   Turaga, Srinivas C.
   Macke, Jakob H.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fast amortized inference of neural activity from calcium imaging data
   with variational autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SELECTIVITY; BRAIN
AB Calcium imaging permits optical measurement of neural activity. Since intracellular calcium concentration is an indirect measurement of neural activity, computational tools are necessary to infer the true underlying spiking activity from fluorescence measurements. Bayesian model inversion can be used to solve this problem, but typically requires either computationally expensive MCMC sampling, or faster but approximate maximum-a-posteriori optimization. Here, we introduce a flexible algorithmic framework for fast, efficient and accurate extraction of neural spikes from imaging data. Using the framework of variational autoencoders, we propose to amortize inference by training a deep neural network to perform model inversion efficiently. The recognition network is trained to produce samples from the posterior distribution over spike trains. Once trained, performing inference amounts to a fast single forward pass through the network, without the need for iterative optimization or sampling. We show that amortization can be applied flexibly to a wide range of nonlinear generative models and significantly improves upon the state of the art in computation time, while achieving competitive accuracy. Our framework is also able to represent posterior distributions over spike-trains. We demonstrate the generality of our method by proposing the first probabilistic approach for separating backpropagating action potentials from putative synaptic inputs in calcium imaging of dendritic spines.
C1 [Speiser, Artur; Macke, Jakob H.] Research Ctr Caesar, Bonn, Germany.
   [Speiser, Artur] IMPRS Brain & Behav Bonn Florida, Bonn, Germany.
   [Yan, Jinyao; Turaga, Srinivas C.] HHMI Janelia Res Campus, Ashburn, VA USA.
   [Archer, Evan; Buesing, Lars] Columbia Univ, New York, NY 10027 USA.
   [Archer, Evan] Cogitai Inc, Anaheim, CA USA.
   [Buesing, Lars] DeepMind, London, England.
   [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany.
RP Speiser, A (reprint author), Research Ctr Caesar, Bonn, Germany.; Speiser, A (reprint author), IMPRS Brain & Behav Bonn Florida, Bonn, Germany.
EM artur.speiser@caesar.de; turagas@janelia.hhmi.org; jakob.macke@caesar.de
RI Jeong, Yongwook/N-7413-2016
FU German Research Foundation (DFG) [SFB 1089]; IMPRS for Brain & Behavior
   scholarship by the Max Planck Society
FX We thank T. W. Chen, K. Svoboda and the GENIE project at Janelia
   Research Campus for sharing their published GCaMP6 data, available at
   http://crcns.org.We also thank T. Deneux for sharing his results for
   comparison and comments on the manuscript and D. Greenberg, L. Paninski
   and A. Mnih for discussions. This work was supported by SFB 1089 of the
   German Research Foundation (DFG) to J.H. Macke. A. Speiser was funded by
   an IMPRS for Brain & Behavior scholarship by the Max Planck Society.
CR Ahrens MB, 2012, NATURE, V485, P471, DOI 10.1038/nature11057
   Apthorpe N. J., 2016, ADV NEURAL INFORM PR, V29, P3270
   Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344
   Breuleux Olivier, 2010, P 9 PYTH SCI C, P1
   Burda Y., 2015, ARXIV150900519
   Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354
   Cho K., 2014, ARXIV14091259
   Deneux T., 2016, NATURE COMMUNICATION, V7
   Friedrich J., 2016, FAST ACTIVE SET METH
   Ganmor E, 2016, ARXIV160100364
   Greenberg D., 2015, 2015 NEUR M PLANN
   Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011
   Kaae Sonderby C, 2016, ARXIV160202282
   Kerr JND, 2008, NAT REV NEUROSCI, V9, P195, DOI 10.1038/nrn2338
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Larochelle H., 2011, AISTATS, V1, P2
   Maaloe L., 2015, NIPS WORKSH ADV APPR
   Maas A. L., 2013, P ICML
   Mnih A., 2016, P 33 INT C MACH LEAR
   Mnih Andriy, 2014, ARXIV14020030
   Oord A. v. d., 2016, ARXIV160106759
   Pachitariu M., 2017, BIORXIV
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Pnevmatikakis E. A., 2014, ARXIV14092903
   Pnevmatikakis E. A., 2016, NEURON
   Pnevmatikakis EA, 2013, CONF REC ASILOMAR C, P349, DOI 10.1109/ACSSC.2013.6810293
   Rahmati V, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004736
   Rezende D. J, 2014, ARXIV14014082
   Smith SL, 2013, NATURE, V503, P115, DOI 10.1038/nature12600
   Theis L, 2016, NEURON, V90, P471, DOI 10.1016/j.neuron.2016.04.014
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   TSIEN RY, 1980, BIOCHEMISTRY-US, V19, P2396, DOI 10.1021/bi00552a018
   Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009
   Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404010
DA 2019-06-15
ER

PT S
AU Tang, HR
   Houthooft, R
   Foote, D
   Stooke, A
   Chen, X
   Duan, Y
   Schulman, J
   De Turck, F
   Abbeel, P
AF Tang, Haoran
   Houthooft, Rein
   Foote, Davis
   Stooke, Adam
   Chen, Xi
   Duan, Yan
   Schulman, John
   De Turck, Filip
   Abbeel, Pieter
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI #Exploration: A Study of Count-Based Exploration for Deep Reinforcement
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.
C1 [Tang, Haoran] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Foote, Davis; Stooke, Adam; Chen, Xi; Duan, Yan; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA USA.
   [Houthooft, Rein; De Turck, Filip] Univ Ghent, Imec, Dept Informat Technol, Ghent, Belgium.
   [Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA.
RP Tang, HR (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Houthooft, R (reprint author), Univ Ghent, Imec, Dept Informat Technol, Ghent, Belgium.; Houthooft, R (reprint author), OpenAI, San Francisco, CA 94110 USA.
EM hrtang@math.berkeley.edu; rein.houthooft@openai.com
FU ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei
   Fellowship; NSF [IIS-1619362]; ARC [FL110100281]; Fannie and John Hertz
   Foundation fellowship; Research Foundation - Flanders (FWO); ARC through
   ARC Centre of Excellence for Mathematical and Statistical Frontiers
FX We would like to thank our colleagues at Berkeley and OpenAI for
   insightful discussions. This research was funded in part by ONR through
   a PECASE award. Yan Duan was also supported by a Berkeley AI Research
   lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a
   Berkeley AI Research lab Fellowship. We gratefully acknowledge the
   support of the NSF through grant IIS-1619362 and of the ARC through a
   Laureate Fellowship (FL110100281) and through the ARC Centre of
   Excellence for Mathematical and Statistical Frontiers. Adam Stooke
   gratefully acknowledges funding from a Fannie and John Hertz Foundation
   fellowship. Rein Houthooft was supported by a Ph.D. Fellowship of the
   Research Foundation - Flanders (FWO).
CR Abbeel P, 2016, ADV NEURAL INFORM PR, V29, P1109
   Abel David, 2016, ARXIV160304119
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dalal N, 2005, PROC CVPR IEEE, P886
   Duan Y., 2016, INT C MACH LEARN, P1329
   Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049
   Gregor K., 2016, ADV NEURAL INFORM PR, P3549
   Guez A., 2014, NIPS, P451
   He K, 2015, DEEP RESIDUAL LEARNI
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   Kolter J. Z., 2009, P 26 ANN INT C MACH, P513
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lillicrap T P, 2015, ARXIV150902971
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mnih V., 2016, ARXIV160201783
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair A., 2015, ARXIV150704296
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Osband  Ian, 2016, P 33 INT C MACH LEAR, P2377
   Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007
   Pazis J., 2013, P 27 AAAI C ART INT
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Schulman J., 2015, P 32 INT C MACH LEAR
   Simonyan K, 2014, ARXIV14091556
   Stadie BC, 2015, ARXIV150700814
   Strehl A. L., 2005, P 22 INT C MACH LEAR, P856, DOI DOI 10.1145/1102351.1102459
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   van den Oord A., 2016, P 33 INT C MACH LEAR, P1747
   Van Hasselt H., 2016, P 30 AAAI C ART INT
   van Hasselt Hado, 2016, ARXIV160207714
   Wang Z., 2016, SER P MACHINE LEARNI, P1995
   Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5
NR 40
TC 1
Z9 1
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402078
DA 2019-06-15
ER

PT S
AU van der Wilk, M
   Rasmussen, CE
   Liensman, J
AF van der Wilk, Mark
   Rasmussen, Carl Edward
   Liensman, James
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convolutional Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBI kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.
C1 [van der Wilk, Mark; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge, England.
   [Liensman, James] Prowler Io, Cambridge, England.
RP van der Wilk, M (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM mv310@cam.ac.uk; cer54@cam.ac.uk; james@prowler.io
FU EPSRC [EP/J012300]; Qualcomm Innovation Fellowship
FX CER gratefully acknowledges support from EPSRC grant EP/J012300. MvdW is
   generously supported by a Qualcomm Innovation Fellowship.
CR Bauer M., 2016, ADV NEURAL INFORM PR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bui T. D., 2016, ARXIV160507066
   Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626
   Durrande N., 2012, ANN FACULTE SCI TOUL, V21, P481
   Duvenaud D., 2011, ADV NEURAL INFORM PR, P226
   Duvenaud DK, 2013, P INT C MACH LEARN I, V3, P1166
   Figueiras-vidal Anibal, 2009, ADV NEURAL INFORM PR, P1087
   Hensman J, 2016, ARXIV161106740
   Hensman J, 2015, P 18 INT C ART INT S, P351
   Hensman J., 2013, GAUSSIAN PROCESSES B, P282
   Hensman James, 2015, ADV NEURAL INFORM PR, P1639
   Hernandez-Lobato D., 2016, 19 INT C ART INT STA, V51, P168
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krauth Karl, 2016, AUTOGP EXPLORING CAP
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mairal Julien, 2014, ADV NEURAL INFORM PR, P2627
   Matthews  A, 2016, THESIS
   Matthews A. G. de G., 2016, J MACH LEARN RES, P231
   Matthews AGD, 2017, J MACH LEARN RES, V18, P1
   Neal RM, 1996, BAYESIAN LEARNING NE, V118
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Pandey G., 2014, P 31 INT C MACH LEAR, P1719
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seeger  M., 2003, P 9 INT WORKSH ART I
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Steinwart I, 2002, J MACH LEARN RES, V2, P67
   Titsias M, 2009, ARTIF INTELL, P567
   Villacampa-Calvo Carlos, 2017, P MACHINE LEARNING R, V70, P3550
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson A. G., 2016, ADV NEURAL INFORM PR, P2586
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402087
DA 2019-06-15
ER

PT S
AU Vaswani, A
   Shazeer, N
   Parmar, N
   Uszkoreit, J
   Jones, L
   Gomez, AN
   Kaiser, L
   Polosukhin, I
AF Vaswani, Ashish
   Shazeer, Noam
   Parmar, Niki
   Uszkoreit, Jakob
   Jones, Llion
   Gomez, Aidan N.
   Kaiser, Lukasz
   Polosukhin, Illia
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Attention Is All You Need
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.
C1 [Vaswani, Ashish; Shazeer, Noam; Kaiser, Lukasz; Polosukhin, Illia] Google Brain, Mountain View, CA 94043 USA.
   [Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Polosukhin, Illia] Google Res, Mountain View, CA USA.
   [Gomez, Aidan N.] Univ Toronto, Toronto, ON, Canada.
RP Vaswani, A (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM avaswani@google.com; noam@google.com; nikip@google.com; usz@google.com;
   llion@google.com; aidan@cs.toronto.edu; lukaszkaiser@google.com;
   illia.polosukhin@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Ba J. L., 2016, ARXIV160706450
   Bahdanau  Dzmitry, 2014, ABS14090473 CORR
   Britz Denny, 2017, ABS170303906 CORR
   Cheng J., 2016, ARXIV160106733
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chollet F, 2016, ARXIV161002357
   Chung J., 2014, CORR
   Gehring J., 2017, ARXIV170503122V2
   Graves A, 2013, ARXIV13080850
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Jozefowicz R., 2016, ARXIV160202410
   Kaiser Lukasz, 2016, INT C LEARN REPR ICL
   Kaiser Samy Bengio Lukasz, 2016, ADV NEURAL INFORM PR
   Kalchbrenner N., 2017, ARXIV161010099V2
   Kim Yoon, 2017, INT C LEARN REPR
   Kingma D. P., 2015, ICLR
   Kuchaiev O., 2017, ARXIV170310722
   Lin Z., 2017, ARXIV170303130
   Luong M.T., 2015, ARXIV150804025
   Parikh Ankur, 2016, EMPIRICAL METHODS NA
   Paulus R., 2017, ARXIV170504304
   Press O., 2016, ARXIV160805859
   Sennrich  R., 2015, ARXIV150807909
   Shazeer N., 2017, ARXIV170106538
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Szegedy C., 2015, ARXIV151200567
   Wu Y., 2016, ARXIV160908144
   Zhou Jie, 2016, ABS160604199 CORR
NR 32
TC 1
Z9 1
U1 16
U2 16
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406008
DA 2019-06-15
ER

PT S
AU Wen, W
   Xu, C
   Yan, F
   Wu, CP
   Wang, YD
   Chen, YR
   Li, H
AF Wen, Wei
   Xu, Cong
   Yan, Feng
   Wu, Chunpeng
   Wang, Yandan
   Chen, Yiran
   Li, Hai
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DESCENT
AB High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1, 0, 1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet doesn't incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available(1).
C1 [Wen, Wei; Wu, Chunpeng; Chen, Yiran; Li, Hai] Duke Univ, Durham, NC 27706 USA.
   [Xu, Cong] Hewlett Packard Labs, Palo Alto, CA USA.
   [Yan, Feng] Univ Nevada, Reno, NV 89557 USA.
   [Wang, Yandan] Univ Pittsburgh, Pittsburgh, PA 15260 USA.
RP Wen, W (reprint author), Duke Univ, Durham, NC 27706 USA.
EM wei.wen@duke.edu; cong.xu@hpe.com; fyan@unr.edu; chunpeng.wu@duke.edu;
   yaw46@pitt.edu; yiran.chen@duke.edu; hai.li@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1744082]; DOE [SC0017030]
FX This work was supported in part by NSF CCF-1744082 and DOE SC0017030.
   Any opinions, findings, conclusions or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   views of NSF, DOE, or their contractors. Thanks Ali Taylan Cemgil at
   Bogazici University for valuable suggestions on this work.
CR Abadi M, 2016, 160304467 ARXIV, V1603, P04467
   Aji Alham Fikri, 2017, 170405021 ARXIV
   Alistarh D., 2017, ADV NEURAL INFORM PR, V30, P1707
   Bottou L., 1998, ON LINE LEARNING NEU, V17, P142
   Bradley J. K., 2011, ARXIV11055379
   Chen T, 2015, 151201274 ARXIV
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Feng Yan, 2015, P KNOWL DISC DATA MI, P1355, DOI DOI 10.1145/2783258.2783270
   Garg R., 2009, P 26 ANN INT C MACH, P337
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   Han S., 2015, ARXIV151000149
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Keskar N. S., 2017, INT C LEARN REPR
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li M, 2014, P 11 USENIX S OP SYS, P583
   Li Maofei, 2017, THESIS
   Li Mu, 2014, ADV NEURAL INFORM PR, P19
   Lin  Zhouhan, 2015, ARXIV151003009
   loffe S, 2015, 150203167 ARXIV
   Moritz Philipp, 2015, 151106051 ARXIV
   Neelakantan Arvind, 2015, 151106807 ARXIV
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Ott J., 2016, ARXIV160806902
   Pan Xinghao, 2017, 170205800 ARXIV
   Park J, 2017, INT C LEARN REPR ICL
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Seide F, 2014, INTERSPEECH, P1058
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Suresh Ananda Theertha, 2016, ARXIV161100429
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy Christian, 2015, 14094842 ARXIV
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Wen W., 2017, ARXIV170905027
   Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
   Zhang W., 2016, P 25 INT JOINT C ART, P2350
   Zhou S., 2016, ARXIV160606160
   Zinkevich M., 2010, ADV NEURAL INFORM PR, V23, P2595
NR 44
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401053
DA 2019-06-15
ER

PT S
AU Xiao, S
   Farajtabar, M
   Ye, XJ
   Yang, JC
   Song, L
   Zha, HY
AF Xiao, Shuai
   Farajtabar, Mehrdad
   Ye, Xiaojing
   Yang, Junchi
   Song, Le
   Zha, Hongyuan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Wasserstein Learning of Deep Generative Point Process Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.
C1 [Xiao, Shuai] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Farajtabar, Mehrdad; Song, Le; Zha, Hongyuan] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Ye, Xiaojing] Georgia State Univ, Sch Math, Atlanta, GA 30303 USA.
   [Yang, Junchi] IBM Res China, Beijing, Peoples R China.
   [Song, Le] Ant Financial, Xihu, Peoples R China.
RP Xiao, S (reprint author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM benjaminforever@sjtu.edu.cn; mehrdad@gatech.edu; xye@gsu.edu;
   yanjc@cn.ibm.com; lsong@cc.gatech.edu; zha@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1639792, IIS-1218749, IIS-1717916, CMMI-1745382, CNS-1704701];
   NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSFC [61602176];
   Intel ISTC; NVIDIA; Amazon AWS; NSF CAREER [IIS-1350983]
FX This project was supported in part by NSF (IIS-1639792, IIS-1218749,
   IIS-1717916, CMMI-1745382), NIH BIGDATA 1R01GM108341, NSF CAREER
   IIS-1350983, NSF CNS-1704701, ONR N00014-15-1-2340, NSFC 61602176, Intel
   ISTC, NVIDIA and Amazon AWS.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Arjovsky M., 2017, ARXIV170107875
   Arjovsky  M., 2017, NIPS 2016 WORKSH ADV, V2016
   Bacry E, 2015, MARK MICROSTRUCT LIQ, V1, DOI 10.1142/S2382626615500057
   Cuturi M, 2017, ICML, P894
   Daley D. J., 2003, INTRO THEORY POINT P
   Decreusefond L, 2016, ANN PROBAB, V44, P2147, DOI 10.1214/15-AOP1020
   Du  N., 2016, KDD
   Farajtabar  M., 2016, NIPS
   Farajtabar  M., 2015, NIPS
   Farajtabar M., 2014, NIPS
   Ghosh Arnab, 2016, ARXIV160909444
   Goodfellow I., 2016, 170100160 ARXIV
   Goodfellow I., 2014, NIPS
   Gulrajani Ishaan, 2017, ARXIV170400028
   Hawkes A. G., 1971, BIOMETRIKA
   Huszar Ferenc, 2015, ARXIV151105101
   Isham V., 1979, Stochastic Processes & their Applications, V8, P335, DOI 10.1016/0304-4149(79)90008-5
   Kingman J, 1993, POISSON PROCESSES
   Lian W., 2015, ICML, P2030
   Linderman S. W., 2014, P 31 INT C MACH LEAR, P1413
   Mei H., 2017, NIPS
   Mogren O., 2016, ARXIV161109904
   Nguyen Anh, 2016, 161200005 ARXIV
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Radford A., 2015, ARXIV151106434
   Schuhmacher D, 2008, ADV APPL PROBAB, V40, P651, DOI 10.1239/aap/1222868180
   Theis L., 2015, ARXIV151101844
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Xiao Shuai, 2017, ARXIV170308524
   Xiao Shuai, 2017, AAAI
   Xu LZ, 2014, MANAGE SCI, V60, P1392, DOI 10.1287/mnsc.2014.1952
   Zhou D., 2005, P 22 INT C MACH LEAR, P1028
NR 33
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403031
DA 2019-06-15
ER

PT S
AU Yan, BW
   Yin, MZ
   Sarkar, P
AF Yan, Bowei
   Yin, Mingzhang
   Sarkar, Purnamrita
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convergence of Gradient EM on Multi-component Mixture of Gaussians
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MAXIMUM-LIKELIHOOD; ALGORITHM
AB In this paper, we study convergence properties of the gradient variant of Expectation-Maximization algorithm [11] for Gaussian Mixture Models for arbitrary number of clusters and mixing coefficients. We derive the convergence rate depending on the mixing coefficients, minimum and maximum pairwise distances between the true centers, dimensionality and number of components; and obtain a near-optimal local contraction radius. While there have been some recent notable works that derive local convergence rates for EM in the two symmetric mixture of Gaussians, in the more general case, the derivations need structurally different and non-trivial arguments. We use recent tools from learning theory and empirical processes to achieve our theoretical results.
C1 [Yan, Bowei; Yin, Mingzhang; Sarkar, Purnamrita] Univ Texas Austin, Austin, TX 78712 USA.
RP Yan, BW (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM boweiy@utexas.edu; mzyin@utexas.edu; purna.sarkar@austin.utexas.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [DMS 1713082]
FX PS was partially supported by NSF grant DMS 1713082.
CR Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4
   Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435
   Combes Richard, 2015, ARXIV151105240
   CONNIFFE D., 1987, J ROY STAT SOC D-STA, V36, P317
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   DASGUPTA S, 2000, P 16 C UNC ART INT, P152
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Friggstad Z, 2016, ANN IEEE SYMP FOUND, P365, DOI 10.1109/FOCS.2016.47
   Jin C, 2016, ADV NEURAL INFORM PR, P4116
   Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35
   LANGE K, 1995, J ROY STAT SOC B MET, V57, P425
   Ledoux M., 2013, PROBABILITY BANACH S
   Lu Yu, 2016, ARXIV161202099
   Matousek J, 2000, DISCRETE COMPUT GEOM, V24, P61, DOI 10.1007/s004540010019
   Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Mixon D. G., 2016, ARXIV160206612
   Mohri M., 2012, FDN MACHINE LEARNING
   Naim Iftekhar, 2012, ARXIV12066427
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jess.2003.11.008
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Xu J., 2016, ADV NEURAL INFORM PR, P2676
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
   Yan Bowei, 2016, ADV NEUR INF PROC SY, P3090
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407005
DA 2019-06-15
ER

PT S
AU Yeh, RA
   Xiong, J
   Hwu, WMW
   Do, MN
   Schwing, AG
AF Yeh, Raymond A.
   Xiong, Jinjun
   Hwu, Wen-mei W.
   Do, Minh N.
   Schwing, Alexander G.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Interpretable and Globally Optimal Prediction for Textual Grounding
   using Image Concepts
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn't rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively.
C1 [Yeh, Raymond A.; Hwu, Wen-mei W.; Do, Minh N.; Schwing, Alexander G.] Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA.
   [Xiong, Jinjun] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY USA.
RP Yeh, RA (reprint author), Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA.
EM yeh17@illinois.edu; jinjun@us.ibm.com; w-hwu@illinois.edu;
   minhdo@illinois.edu; aschwing@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1718221]; NVIDIA Corporation; IBM-ILLINOIS
   Center for Cognitive Computing Systems Research (C3SR) - a research
   collaboration as part of the IBM Cognitive Horizons Network
FX This material is based upon work supported in part by the National
   Science Foundation under Grant No. 1718221. This work is supported by
   NVIDIA Corporation with the donation of a GPU. This work is supported in
   part by IBM-ILLINOIS Center for Cognitive Computing Systems Research
   (C3SR) - a research collaboration as part of the IBM Cognitive Horizons
   Network.
CR Arandjelovic R., 2012, P BMVC
   Cao Z., 2017, P CVPR
   Chen K., 2017, P ICCV
   Chen L.-C., 2015, P ICLR
   Deselaers T., 2012, IJCV
   Donahue J., 2015, P CVPR
   Endo K., 2017, P IJCAI
   Everingham M., 2010, IJCV
   Frome A., 2013, P NIPS
   Gong Y., 2014, P ECCV
   Guadarrama S., 2014, P RSS
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hoi S. C., 2006, P CVPR
   Hu R., 2016, P CVPR
   Hu R., 2017, P CVPR
   Joachims T., 2009, MACHINE LEARNING
   Johnson J., 2015, P CVPR
   Karpathy A., 2014, P NIPS
   Karpathy A., 2015, P CVPR
   Kazemzadeh S., 2014, P EMNLP
   Kiros R., 2015, TACL
   Klein B., 2014, ARXIV14117399
   Kong C., 2014, P CVPR
   Krishnamurthy J., 2013, P TACL
   Lampert C. H., 2009, PAMI
   Lehmann A., 2011, IJCV
   Lin D., 2014, P CVPR
   Lin  T.-Y., 2014, P ECCV
   Mao J., 2015, P ICLR
   Mao J., 2016, P CVPR
   Matuszek C., 2012, P ICML
   Nagaraja V. K., 2016, P ECCV
   Oneata D., 2014, P ECCV
   Plummer B., 2015, P ICCV
   Plummer B. A., 2017, IJCV
   Plummer B. A., 2017, P ICCV
   Redmon J, 2017, CVPR
   Rohrbach A., 2016, P ECCV
   Sadeghi F., 2015, P CVPR
   Schwing A. G., 2012, P ECCV
   Sun Q., 2015, P NIPS
   Wang L., 2016, P CVPR
   Yan  J., 2014, P CVPR, P1
   Young P., 2014, P TACL
   Yu H., 2013, P ACL
NR 45
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401091
DA 2019-06-15
ER

PT S
AU Zhu, JY
   Zhang, R
   Pathak, D
   Darrell, T
   Efros, AA
   Wang, O
   Shechtman, E
AF Zhu, Jun-Yan
   Zhang, Richard
   Pathak, Deepak
   Darrell, Trevor
   Efros, Alexei A.
   Wang, Oliver
   Shechtman, Eli
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Toward Multimodal Image-to-Image Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.
C1 [Zhu, Jun-Yan; Zhang, Richard; Pathak, Deepak; Darrell, Trevor; Efros, Alexei A.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Wang, Oliver; Shechtman, Eli] Adobe Res, San Jose, CA USA.
RP Zhu, JY (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
RI Jeong, Yongwook/N-7413-2016
FU Adobe Inc.; DARPA; DoD MURI award [N000141110688]; NSF [IIS-1633310,
   IIS-1427425, IIS-1212798]; Facebook Graduate Fellowship; Adobe Research
   Fellowship; NVIDIA Graduate Fellowship; AFRL; Berkeley Artificial
   Intelligence Research (BAIR) Lab
FX We thank Phillip Isola and Tinghui Zhou for helpful discussions. This
   work was supported in part by Adobe Inc., DARPA, AFRL, DoD MURI award
   N000141110688, NSF awards IIS-1633310, IIS-1427425, IIS-1212798, the
   Berkeley Artificial Intelligence Research (BAIR) Lab, and hardware
   donations from NVIDIA. JYZ is supported by Facebook Graduate Fellowship,
   RZ by Adobe Research Fellowship, and DP by NVIDIA Graduate Fellowship.
CR Arjovsky Martin, 2017, ICLR
   Bansal A., 2017, ARXIV170805349
   Chen Qifeng, 2017, ICCV
   Chen Xi, 2016, NIPS
   Cordts M., 2016, CVPR
   Denton E. L., 2015, NIPS
   Dinh Laurent, 2017, ICLR
   Donahue Jeff, 2016, ICLR
   Dosovitskiy Alexey, 2016, NIPS
   Dumoulin V., 2016, ICLR
   Efros  A., 1999, ICCV
   GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265
   Ghosh Arnab, 2017, ARXIV170402906
   Goodfellow I., 2014, NIPS
   Goodfellow Ian, 2016, ARXIV170100160
   Guadarrama S., 2017, BMVC
   He K., 2016, CVPR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Iizuka S., 2016, ACM T GRAPHIC, V35, P4, DOI DOI 10.1145/2897824.2925974
   Isola  Phillip, 2017, CVPR
   Johnson J., 2016, ECCV
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2014, ONE WEIRD TRICK PARA
   Laffont P.-Y., 2014, SIGGRAPH
   Larsen Anders Boesen Lindbo, 2016, ICML
   Larsson G., 2016, ECCV
   Mao Xudong, 2017, ICCV
   Mathieu M., 2016, ICLR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Nguyen A., 2017, CVPR
   Oord A. V. D., 2016, NIPS
   Oord A. v. d., 2016, PMLR
   Pathak D., 2016, CVPR
   Radford A., 2016, ICLR
   Reed S., 2016, ICML
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Russakovsky Olga, 2015, IJCV
   Salimans T., 2016, ARXIV160603498
   Sangkloy P., 2017, CVPR
   Smolensky P., 1986, DTIC DOCUMENT
   Sohn K., 2015, NIPS
   Vincent P., 2008, ICML
   Walker  J., 2016, ECCV
   Xian  W., 2017, ARXIV170602823
   Xue  T., 2016, NIPS
   Yang C., 2017, CVPR
   Yu Aron, 2014, CVPR
   Zhang Han, 2017, ICCV
   Zhang R., 2018, ARXIV180103924
   Zhang R., 2017, SIGGRAPH
   Zhang Richard, 2016, ECCV
   Zhao J., 2017, ICLR
   Zhu J.-Y., 2017, ICCV
   Zhu Jun-Yan, 2016, ECCV
NR 55
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400045
DA 2019-06-15
ER

PT S
AU Agrawal, P
   Nair, A
   Abbeel, P
   Malik, J
   Levine, S
AF Agrawal, Pulkit
   Nair, Ashvin
   Abbeel, Pieter
   Malik, Jitendra
   Levine, Sergey
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning to Poke by Poking: Experiential Learning of Intuitive Physics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MANIPULATION; MODEL
AB We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.
C1 [Agrawal, Pulkit; Nair, Ashvin; Abbeel, Pieter; Malik, Jitendra; Levine, Sergey] Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94720 USA.
RP Agrawal, P (reprint author), Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94720 USA.
EM pulkitag@berkeley.edu; anair17@berkeley.edu; pabbeel@berkeley.edu;
   malik@berkeley.edu; svlevine@berkeley.edu
RI Agrawal, Pulkit/O-8574-2019
OI Agrawal, Pulkit/0000-0001-8463-9917
FU ONR MURI [N00014-14-1-0671]; ONR YIP; ARL
FX We thank Alyosha Efros for inspiration and fruitful discussions
   throughout this work. The title of this paper is partly influenced by
   the term "pokebot" that Alyosha has been using for several years. We
   thank Ruzena Bajcsy for access to Baxter robot and Shubham Tulsiani for
   helpful comments. This work was supported in part by ONR MURI
   N00014-14-1-0671, ONR YIP and by ARL through the MAST program. We are
   grateful to NVIDIA corporation for donating K40 GPUs and providing
   access to the NVIDIA PSG cluster.
CR Dogar MR, 2012, AUTON ROBOT, V33, P217, DOI 10.1007/s10514-012-9306-z
   Finn C., 2016, ICRA
   Fragkiadaki Katerina, 2016, ICLR
   Gopnik A., 1999, SCI CRIB MINDS BRAIN
   Hamrick J., 2011, P 33 ANN C COGN SCI, P1545
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Kietzmann Tim C, 2009, ICMLA
   Kolev S, 2015, IEEE-RAS INT C HUMAN, P1036, DOI 10.1109/HUMANOIDS.2015.7363481
   Kopicki M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5722, DOI 10.1109/ICRA.2011.5980295
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lange S., 2012, P INT JOINT C NEUR N, P1
   Lau Manfred, 2011, 2011 IEEE International Conference on Robotics and Automation, P3733
   LaValle S.M., 2006, PLANNING ALGORITHMS
   Lerer  A., 2016, ARXIV160301312
   Levine Sergey, 2016, JMLR
   Levine Sergey, 2016, LEARNING HAND EYE CO
   Lillicrap T. P., 2016, ICLR
   Mayne DQ, 2014, AUTOMATICA, V50, P2967, DOI 10.1016/j.automatica.2014.10.128
   MCCLOSKEY M, 1983, SCI AM, V248, P122, DOI 10.1038/scientificamerican0483-122
   Mericli T, 2015, AUTON ROBOT, V38, P317, DOI 10.1007/s10514-014-9414-z
   Michotte A, 1963, PERCEPTION CAUSALITY
   Mnih V., 2015, NATURE
   Mottaghi Roozbeh, 2016, CVPR
   Oh  J., 2015, NIPS
   Pinto L, 2016, LECT NOTES COMPUT SC, V9906, P3, DOI 10.1007/978-3-319-46475-6_1
   Pinto Lerrel, 2016, ICRA
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Vondrick  Carl, 2016, CVPR, P2
   Wahlstrom Niklas, 2015, CORR
   Watter M., 2015, ADV NEURAL INFORM PR, P2728
   WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931
   Wu J., 2015, ADV NEURAL INFORM PR, P127, DOI DOI 10.1007/978-3-319-26532-2_15
NR 32
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700077
DA 2019-06-15
ER

PT S
AU Nguyen, A
   Dosovitskiy, A
   Yosinski, J
   Brox, T
   Clune, J
AF Anh Nguyen
   Dosovitskiy, Alexey
   Yosinski, Jason
   Brox, Thomas
   Clune, Jeff
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Synthesizing the preferred inputs for neurons in neural networks via
   deep generator networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right-similar to why we study the human brain-and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).
EM anguyen8@uwyo.edu; dosovits@cs.uni-freiburg.de; jason@geometric.ai;
   brox@cs.uni-freiburg.de; jeffclune@uwyo.edu
FU NSF CAREER award [CAREER: 1453549]; NASA Space Technology Research
   Fellowship; NSF [1527232]; ERC Starting Grant VideoLearn [279401]
FX The authors would like to thank Yoshua Bengio for helpful discussions
   and Bolei Zhou for providing images for our study. Jeff Clune was
   supported by an NSF CAREER award (CAREER: 1453549) and a hardware
   donation from the NVIDIA Corporation. Jason Yosinski was supported by
   the NASA Space Technology Research Fellowship and NSF grant 1527232.
   Alexey Dosovitskiy and Thomas Brox acknowledge funding by the ERC
   Starting Grant VideoLearn (279401).
CR Alain G, 2014, J MACH LEARN RES, V15, P3563
   Bengio Y., 2015, DEEP LEARNING
   Deng J., 2009, CVPR
   Donahue J., 2015, COMPUTER VISION PATT
   Dosovitskiy A., 2016, CVPR
   Dosovitskiy A., 2015, COMPUTER VISION PATT
   Dosovitskiy Alexey, 2016, NIPS
   Erhan D., 2009, 4323 U MONTR DEPT IR
   Goodfellow I., 2014, NIPS
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Jia Y., 2014, ARXIV14085093
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Mahendran A., 2016, COMPUTER VISION PATT
   Mordvintsev A., GOOGLE RES BLOG
   Nguyen A., 2016, VIS DEEP LEARN WORKS
   Nguyen A., 2015, COMPUTER VISION PATT
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Radford A., 2016, ICLR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, CLR WORKSH
   Szegedy C., 2015, COMPUTER VISION PATT
   Theis  Lucas, 2016, ICLR
   Wei D., 2015, ARXIV150702379
   Yosinski J., 2015, DEEP LEARN WORKSH IC
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhou B., 2015, INT C LEARN REPR CLR
   Zhou B., 2014, ADV NEURAL INFORM PR
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704044
DA 2019-06-15
ER

PT S
AU Atwood, J
   Towsley, D
AF Atwood, James
   Towsley, Don
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Diffusion-Convolutional Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graphstructured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on a GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.
C1 [Atwood, James; Towsley, Don] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Atwood, J (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM jatwood@cs.umass.edu; towsley@cs.umass.edu
FU Army Research Office [W911NF-12-1-0385]; ARL [W911NF-09-2-0053]
FX We would like to thank Bruno Ribeiro, Pinar Yanardag, and David Belanger
   for their feedback on drafts of this paper. This work was supported in
   part by Army Research Office Contract W911NF-12-1-0385 and ARL
   Cooperative Agreement W911NF-09-2-0053. This work was also supported by
   NVIDIA through the donation of equipment used to perform experiments.
CR Bergstra J., 2010, P PYTH SCI COMP C SC
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Bruna  Joan, 2014, SPECTRAL NETWORKS LO
   Cohn  Trevor, 2006, ECML
   DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046
   Duchi  J., 2011, J MACHINE LEARNING R
   Duvenaud David K, 2015, NIPS
   Fouss F, 2012, NEURAL NETWORKS, V31, P53, DOI 10.1016/j.neunet.2012.03.001
   Henaff  M., 2015, DEEP CONVOLUTIONAL N
   Koller D., 2009, PROBABILISTIC GRAPHI
   Micheli  A, 2009, IEEE T NEURAL NETWOR
   Scarselli F., 2009, IEEE T NEURAL NETWOR
   Sen  P, 2007, TECHNICAL REPORT
   Sen P., 2008, AI MAGAZINE
   Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130
   Verbeek  Jakob, 2007, NIPS
   Wale  Nikil, 2007, KNOWL INF SYST, V14, P347
   Yanardag P., 2015, P 21 ACM SIGKDD INT, P1365, DOI DOI 10.1145/2783258.2783417
NR 18
TC 1
Z9 1
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701069
DA 2019-06-15
ER

PT S
AU Barbier, J
   Dia, M
   Macris, N
   Krzakala, F
   Lesieur, T
   Zdeborova, L
AF Barbier, Jean
   Dia, Mohamad
   Macris, Nicolas
   Krzakala, Florent
   Lesieur, Thibault
   Zdeborova, Lenka
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Mutual information for symmetric rank-one matrix estimation: A proof of
   the replica formula
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.
C1 [Barbier, Jean; Dia, Mohamad; Macris, Nicolas] Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, CH-1015 Lausanne, Switzerland.
   [Krzakala, Florent] Sorbonne Univ, PSL Univ & Ecole Normale Super, CNRS, Lab Phys Stat, F-75005 Paris, France.
   [Krzakala, Florent] Univ Paris 06, F-75005 Paris, France.
   [Lesieur, Thibault; Zdeborova, Lenka] Univ Paris Saclay, CNRS, CEA, Inst Phys Theor, F-91191 Gif Sur Yvette, France.
RP Barbier, J (reprint author), Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, CH-1015 Lausanne, Switzerland.
EM jean.barbier@epfl.ch; mohamad.dia@epfl.ch; nicolas.macris@epfl.ch;
   florent.krzakala@ens.fr; lesieur.thibault@gmail.com;
   lenka.zdeborova@gmail.com
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU SNSF [200021-156672]; ERC under the EU [307087-SPARCS]
FX J.B and M.D acknowledge funding from the SNSF (grant 200021-156672).
   Part of this research received funding from the ERC under the EU's 7th
   Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS).
   F.K and L.Z thank the Simons Institute for its hospitality.
CR Amini AA, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-6, P2454, DOI 10.1109/ISIT.2008.4595432
   Baik J, 2005, ANN PROBAB, V33, P1643, DOI 10.1214/009117905000000233
   Barbier J., 2016, CORR
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Berthet Q., 2013, ARXIV13040828
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Deshpande Y., 2015, ARXIV150708685
   Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y
   Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223
   Guerra F., 2005, MATH STAT PHYS, P243
   Guo D., 2005, IEEE T INF THEORY, V51
   Hassani S. H., 2010, IEEE INF THEOR WORKS
   Javanmard A, 2013, INF INFERENCE, V2, P115, DOI 10.1093/imaiai/iat004
   Johnstone I. M., 2012, J AM STAT ASS
   Korada SB, 2009, J STAT PHYS, V136, P205, DOI 10.1007/s10955-009-9781-6
   Krzakala F., 2016, ARXIV160308447
   Kudekar S., 2011, IEEE T INF TH, V57
   Lesieur T., 2015, ANN ALL C
   Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733
   Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056
   Yedla A, 2014, IEEE T INFORM THEORY, V60, P6943, DOI 10.1109/TIT.2014.2352296
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703016
DA 2019-06-15
ER

PT S
AU Bauer, M
   van der Wilk, M
   Rasmussen, CE
AF Bauer, Matthias
   van der Wilk, Mark
   Rasmussen, Carl Edward
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Understanding Probabilistic Sparse Gaussian Process Approximations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.
C1 [Bauer, Matthias; van der Wilk, Mark; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge, England.
   [Bauer, Matthias] Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Bauer, M (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.; Bauer, M (reprint author), Max Planck Inst Intelligent Syst, Tubingen, Germany.
EM msb55@cam.ac.uk; mv310@cam.ac.uk; cer54@cam.ac.uk
CR Bui T. D., 2016, UNIFYING FRAMEWORK S
   Csato  L., 2002, NEURAL COMPUTATION, V14
   Hensman  J., 2013, C UNC ART INT
   Hensman  J., 2015, P 18 INT C ART INT S
   Lazaro-Aredilla  M., 2009, ADV NEURAL INFORM PR
   Lazaro-Gredilla  M., 2010, J MACHINE LEARNING R, V11
   Matthews  A, 2016, THESIS
   Matthews A. G., 2016, P 19 INT C ART INT S
   Qi  Y., 2010, P 26 C UNC ART INT
   Quinonero-Candela  J., 2005, J MACHINE LEARNING R, V6
   Rahimi  A., 2009, ADV NEURAL INFORM PR
   Rasmussen C. E., 2001, ADV NEURAL INFORM PR, V13
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seeger  M., 2003, P 9 INT WORKSH ART I
   Smola A. J., 2001, ADV NEURAL INFORM PR, V13
   Snelson  E., 2006, NEURAL INFORM PROCES, V18
   Snelson EL, 2007, THESIS
   Titsias  M., 2009, TECH REP
   Titsias M. K., 2009, P 12 INT C ART INT S
   Turner R. E., 2011, BAYESIAN TIME SERIES
   Yang  Z., 2015, ARTIFICIAL INTELLIGE
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704002
DA 2019-06-15
ER

PT S
AU Belilovsky, E
   Varoquaux, G
   Blaschko, M
AF Belilovsky, Eugene
   Varoquaux, Gael
   Blaschko, Matthew
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Testing for Differences in Gaussian Graphical Models: Applications to
   Brain Connectivity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INVERSE COVARIANCE ESTIMATION; CONFIDENCE-INTERVALS; SELECTION; STATES
AB Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.
C1 [Belilovsky, Eugene] Univ Paris Saclay, St Aubin, France.
   [Belilovsky, Eugene; Varoquaux, Gael] INRIA, Villers Les Nancy, France.
   [Belilovsky, Eugene; Blaschko, Matthew] Katholieke Univ Leuven, Leuven, Belgium.
RP Belilovsky, E (reprint author), Univ Paris Saclay, St Aubin, France.; Belilovsky, E (reprint author), INRIA, Villers Les Nancy, France.; Belilovsky, E (reprint author), Katholieke Univ Leuven, Leuven, Belgium.
EM eugene.belilovsky@inria.fr; gael.varoquaux@inria.fr;
   matthew.blaschko@esat.kuleuven.be
FU Internal Funds KU Leuven; ERC [259112, FP7-MC-CIG 334380, DIGITEO
   2013-0788D - SOPRANO]; NiConnect [ANR-11-BINF-0004]
FX This work is partially funded by Internal Funds KU Leuven, ERC Grant
   259112, FP7-MC-CIG 334380, and DIGITEO 2013-0788D - SOPRANO, and
   ANR-11-BINF-0004 NiConnect.
CR Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Button KS, 2013, NAT REV NEUROSCI, V14, P365, DOI 10.1038/nrn3475
   Castellanos FX, 2013, NEUROIMAGE, V80, P527, DOI 10.1016/j.neuroimage.2013.04.083
   Chen  X., 2011, UAI
   Da Mota B, 2014, NEUROIMAGE, V89, P203, DOI 10.1016/j.neuroimage.2013.11.012
   Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033
   Dezeure R, 2015, STAT SCI, V30, P533, DOI 10.1214/15-STS527
   Di Martino A, 2014, MOL PSYCHIATR, V19, P659, DOI 10.1038/mp.2013.78
   Fazayeli  F., 2016, ICML
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   G'Sell Max Grazier, 2013, ARXIV13074765
   Ganguly  A., 2014, ARXIV14108766
   Honey CJ, 2009, P NATL ACAD SCI USA, V106, P2035, DOI 10.1073/pnas.0811168106
   Honorio  J., 2010, ICML
   Jankova J, 2015, ELECTRON J STAT, V9, P1205, DOI 10.1214/15-EJS1031
   Javanmard A, 2014, J MACH LEARN RES, V15, P2869
   Kelly C, 2012, TRENDS COGN SCI, V16, P181, DOI 10.1016/j.tics.2012.02.001
   Lindquist MA, 2008, STAT SCI, V23, P439, DOI 10.1214/09-STS282
   Lockhart R, 2014, ANN STAT, V42, P413, DOI 10.1214/13-AOS1175
   Markov NT, 2013, SCIENCE, V342, P578, DOI 10.1126/science.1238406
   MARSAGLIA G, 1964, J AM STAT ASSOC, V59, P1203, DOI 10.2307/2282635
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Mohan K., 2012, ADV NEURAL INFORM PR, V25, P620
   Narayan  M., 2015, BIORXIV027516
   Nichols TE, 2002, HUM BRAIN MAPP, V15, P1, DOI 10.1002/hbm.1058
   Richiardi J, 2011, NEUROIMAGE, V56, P616, DOI 10.1016/j.neuroimage.2010.05.081
   Shirer WR, 2012, CEREB CORTEX, V22, P158, DOI 10.1093/cercor/bhr099
   Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221
   Varoquaux  G., 2011, IPMI
   Varoquaux G, 2010, NIPS
   Varoquaux G, 2013, NEUROIMAGE, V80, P405, DOI 10.1016/j.neuroimage.2013.04.007
   Waldorp  L., 2014, STAT SURVEY
   Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009
NR 35
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704055
DA 2019-06-15
ER

PT S
AU Bertinetto, L
   Henriques, JF
   Valmadre, J
   Torr, PHS
   Vedaldi, A
AF Bertinetto, Luca
   Henriques, Joao F.
   Valmadre, Jack
   Torr, Philip H. S.
   Vedaldi, Andrea
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning feed-forward one-shot learners
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.
C1 [Bertinetto, Luca; Henriques, Joao F.; Valmadre, Jack; Torr, Philip H. S.; Vedaldi, Andrea] Univ Oxford, Oxford, England.
RP Bertinetto, L (reprint author), Univ Oxford, Oxford, England.
EM luca@robots.ox.ac.uk; joao@robots.ox.ac.uk; jvlmdr@robots.ox.ac.uk;
   philip.torr@eng.ox.ac.uk; vedaldi@robots.ox.ac.uk
FU Apical Ltd.; ERC [ERC-2012-AdG 321162-HELIOS, HELIOS-DFR00200,
   EP/L024683/1]
FX This research was supported by Apical Ltd. and ERC grants ERC-2012-AdG
   321162-HELIOS, HELIOS-DFR00200 and "Integrated and Detailed Image
   Understanding" (EP/L024683/1).
CR Bertinetto L, 2016, FULLY CONVOLUTIONAL
   Bromley J., 1993, INT J PATTERN RECOGN
   Danelljan M., 2014, BMVC
   Denil  Misha, 2013, NIPS
   Fan H., 2014, LEARNING DEEP FACE R
   He K., 2015, ICCV
   Hong Z., 2015, CVPR
   Ioffe S., 2015, BATCH NORMALIZATION
   Kingma D. P., 2013, AUTOENCODING VARIATI
   Koch G., 2016, ICML 2015 DEEP LEARN
   Kristan M., 2015, ICCV WORKSH
   Krizhevsky A., 2012, NIPS
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Malisiewicz T., 2011, ICCV
   Noh H., 2016, CVPR
   Parkhi Omkar M, 2015, BMVC
   Possegger H., 2015, CVPR
   Rezende D. J., 2016, ONE SHOT GEN DEEP GE
   Russakovsky Olga, 2015, IJCV
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Socher R., 2013, NIPS
   Vedaldi A., 2015, P ACM INT C MULT
   Wang N., 2015, TRANSFERRING RICH FE
   Zhang J., 2014, ECCV
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700032
DA 2019-06-15
ER

PT S
AU Bolukbasi, T
   Chang, KW
   Zou, J
   Saligrama, V
   Kalai, A
AF Bolukbasi, Tolga
   Chang, Kai-Wei
   Zou, James
   Saligrama, Venkatesh
   Kalai, Adam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
   Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.
C1 [Bolukbasi, Tolga; Saligrama, Venkatesh] Boston Univ, 8 St Marys St, Boston, MA 02215 USA.
   [Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam] Microsoft Res New England, 1 Mem Dr, Cambridge, MA USA.
RP Bolukbasi, T (reprint author), Boston Univ, 8 St Marys St, Boston, MA 02215 USA.
EM tolgab@bu.edu; kw@kwchang.net; jamesyzou@gmail.com; srv@bu.edu;
   adam.kalai@microsoft.com
FU NSF [CNS-1330008, CCF-1527618]; ONR [50202168]; NGA [HM1582-09-1-0037];
   DHS [2013-ST-061-ED0001]
FX This material is based upon work supported in part by NSF Grants
   CNS-1330008, CCF-1527618, by ONR Grant 50202168, NGA Grant
   HM1582-09-1-0037 and DHS 2013-ST-061-ED0001
CR Angwin J, 2016, MACHINE BIAS THERES
   Barocas S., 2014, BIG DATAS DISPARATE
   Beigman  E., 2009, ACL
   Datta  A., 2015, P PRIVACY ENHANCING
   Dwork  C., 2012, INN THEOR COMP SCI C
   Eisenstein  J., 2014, PLOS ONE, P1
   Faruqui  M., 2015, NAACL
   Feldman Michael, 2015, KDD
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Finkelstein  L., 2001, WWW
   Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037//0022-3514.74.6.1464
   Hansen  C., SNN AD INT S MACH LE
   Holmes J., 2008, HDB LANGUAGE GENDER, V25
   Irsoy  O., 2014, NIPS
   Jakobson  R., 1990, LANGUAGE
   Kay  M., 2015, HUMAN FACTORS COMPUT
   Lei  T., 2016, NAACL
   Levy  O., 2014, CONLL
   Mikolov T., 2013, P NAACL 2013, P746
   Mikolov  T., NIPS
   Mikolov T., 2013, ICLR
   Nalisnick  E., 2016, WWW
   Nosek BA, 2002, GROUP DYN-THEOR RES, V6, P101, DOI 10.1037//1089-2699.6.1.101
   Pedreschi D., 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959
   Pennington Jeffrey, 2014, EMNLP
   Ross K, 2011, MEDIA CULT SOC, V33, P1148, DOI 10.1177/0163443711418272
   RUBENSTEIN H, 1965, COMMUN ACM, V8, P627, DOI 10.1145/365628.365657
   Sapir  E., 1985, SELECTED WRITINGS E, V342
   Schmidt  B., 2015, REJECTING GENDER BIN
   Stanley J. P., 1977, PAPERS LANGUAGE VARI, P303
   Sweeney L., 2013, DISCRIMINATION ONLIN, V11, P10
   Torralba  A., 2012, CVPR
   Turney PD, 2012, J ARTIF INTELL RES, V44, P533, DOI 10.1613/jair.3640
   Wagner C, 2015, 9 INT AAAI C WEB SOC
   Yogatama  D., 2015, ICML
   Zliobaite  I., 2015, ARXIV151100148
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701085
DA 2019-06-15
ER

PT S
AU Cai, D
   Campbell, T
   Broderick, T
AF Cai, Diana
   Campbell, Trevor
   Broderick, Tamara
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Edge-exchangeable graphs and sparsity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ARRAYS
AB Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox [12], models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.
C1 [Cai, Diana] Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
   [Campbell, Trevor; Broderick, Tamara] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Cai, D (reprint author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
EM dcai@uchicago.edu; tdjc@mit.edu; tbroderick@csail.mit.edu
CR Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3
   Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168
   Borgs C., 2014, 14012906 ARXIV
   Borgs C., 2016, 160107134 ARXIV
   Borgs C., 2015, 14011137 ARXIV
   Broderick T., 2015, NIPS 2015 WORKSH BAY
   Broderick T., 2015, NIPS 2015 WORKSH NET
   Broderick T, 2012, BAYESIAN ANAL, V7, P439, DOI 10.1214/12-BA715
   Cai D., 2015, NIPS 2015 WORKSH NET
   Campbell T., 2016, 160300861 ARXIV
   Caron F., 2015, 14011137V3 ARXIV
   Crane H., 2015, 150908184 ARXIV
   Crane H., 2016, 160304571 ARXIV
   Crane H., 2015, 150908185 ARXIV
   Hoover D., 1979, RELATIONS PROBABILIT
   Kallenberg O., 2005, PROB APPL S
   KALLENBERG O, 1990, J THEORET PROBAB, V3, P81
   Lloyd J. R., 2012, NIPS 25, V25
   Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607
   Teh Y. W., 2009, NIPS 23, V23
   Veitch V., 2015, 151203099 ARXIV
   Williamson RC, 2016, J MACH LEARN RES, V17, P1
   Wolfe P. J., 2013, 13095936 ARXIV
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704110
DA 2019-06-15
ER

PT S
AU Chen, W
   Hu, W
   Li, F
   Li, J
   Liu, Y
   Lu, PY
AF Chen, Wei
   Hu, Wei
   Li, Fu
   Li, Jian
   Liu, Yu
   Lu, Pinyan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Combinatorial Multi-Armed Bandit with General Reward Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MAXIMIZING EXPECTED UTILITY; OPTIMIZATION
AB In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O (log T) distribution-dependent regret and (O) over tilde (root T) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first (O) over tilde (root T) bound on the (1-epsilon)-approximation regret of its online problem, for any epsilon > 0.
C1 [Chen, Wei] Microsoft Res, Redmond, WA 98052 USA.
   [Hu, Wei] Princeton Univ, Princeton, NJ 08544 USA.
   [Li, Fu] Univ Texas Austin, Austin, TX 78712 USA.
   [Li, Jian; Liu, Yu] Tsinghua Univ, Beijing, Peoples R China.
   [Lu, Pinyan] Shanghai Univ Finance & Econ, Shanghai, Peoples R China.
RP Chen, W (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM weic@microsoft.com; huwei@cs.princeton.edu;
   fuli.theory.research@gmail.com; lapordge@gmail.com; liuyujyyz@gmail.com;
   lu.pinyan@mail.shufe.edu.cn
FU National Natural Science Foundation of China [61433014]; National Basic
   Research Program of China [2015CB358700, 2011CBA00300, 2011CBA00301];
   National NSFC [61033001, 61361136003]
FX Wei Chen was supported in part by the National Natural Science
   Foundation of China (Grant No. 61433014). Jian Li and Yu Liu were
   supported in part by the National Basic Research Program of China grants
   2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants
   61033001, 61361136003. The authors would like to thank Tor Lattimore for
   referring to us the DKW inequality.
CR Audibert J.-Y., 2009, COLT, P217
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bhalgat Anand, 2014, Integer Programming and Combinatorial Optimization. 17th International Conference, IPCO 2014. Proceedings: LNCS 8494, P126, DOI 10.1007/978-3-319-07557-0_11
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen  Shouyuan, 2014, NIPS
   Chen W, 2016, J MACH LEARN RES, V17
   Combes  Richard, 2015, NIPS
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Fishburn P. C., 1982, FDN EXPECTED UTILITY
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Goel A., 2006, PODS, P203
   Goel A, 2010, ACM T ALGORITHMS, V7, DOI 10.1145/1868237.1868250
   Gopalan A., 2014, P 31 INT C MACH LEAR, P100
   Kveton B, 2015, P 18 INT C ART INT S, P535
   Kveton B., 2014, P 30 C UNC ART INT J, P420
   Kveton  Branislav, 2015, NIPS
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Li J., 2013, P 45 ANN ACM S THEOR, P971
   Li J, 2011, ANN IEEE SYMP FOUND, P797, DOI 10.1109/FOCS.2011.33
   Lin  Tian, 2015, NIPS
   Lin  Tian, 2014, P 31 INT C MACH LEAR
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Streeter  Matthew, 2008, NIPS
   Yu JJ, 2016, OPER RES LETT, V44, P180, DOI 10.1016/j.orl.2015.12.016
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704036
DA 2019-06-15
ER

PT S
AU Choy, CB
   Gwak, J
   Savarese, S
   Chandraker, M
AF Choy, Christopher B.
   Gwak, JunYoung
   Savarese, Silvio
   Chandraker, Manmohan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Universal Correspondence Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations. In contrast to previous CNN-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity. Our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with O(n) feed forward passes for n keypoints, instead of O(n(2)) for typical patch similarity methods. We propose a convolutional spatial transformer to mimic patch normalization in traditional features like SIFT, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations. Extensive experiments on KITTI, PASCAL, and CUB-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features.
C1 [Choy, Christopher B.; Gwak, JunYoung; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA.
   [Chandraker, Manmohan] NEC Labs Amer Inc, Princeton, NJ USA.
RP Choy, CB (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM chrischoy@ai.stanford.edu; jgwak@ai.stanford.edu; ssilvio@stanford.edu;
   manu@nec-labs.com
FU Korea Foundation of Advanced Studies; Toyota [122282]; ONR
   [N00014-13-1-0761]; MURI [WF911NF-15-1-0479]
FX This work was part of C. Choy's internship at NEC Labs. We acknowledge
   the support of Korea Foundation of Advanced Studies, Toyota Award
   #122282, ONR N00014-13-1-0761, and MURI WF911NF-15-1-0479.
CR Agrawal P, 2015, ICCV
   Alcantarilla P., 2012, ECCV
   Bay H., 2008, CVIU
   Bourdev L., 2009, ICCV
   Bromley  J., 1994, NIPS
   Butler D., 2012, ECCV
   Chopra S., 2005, CVPR, V1
   Dalal  N., 2005, CVPR
   Everingham M., PASCAL VISUAL OBJECT
   Garcia V., 2010, ICIP
   Girshick R., 2015, ARXIV E PRINTS
   Hadsell Raia, 2006, CVPR
   Jaderberg M., 2015, NIPS
   Jia Y., 2014, ARXIV14085093
   Kaiming H., 2014, ECCV
   Kanazawa A., 2014, DEEP LEARN REPR LEAR
   Kanazawa A., 2016, ARXIV E PRINTS
   Kim J., 2013, CVPR
   Liu C., 2011, IEEE T PATTERN ANAL, V33, P5, DOI DOI 10.1109/TPAMI.2010.147
   Long  J., 2015, CVPR
   Long J.L., 2014, NIPS
   Lowe D. G., 2004, IJCV
   Matas J., 2002, BMVC
   Menze Moritz, 2015, CVPR
   Revaud J., 2015, DEEPMATCHING HIERARC
   Schroff F., 2015, CVPR
   Song H. O., 2016, COMPUTER VISION PATT
   Tola E., 2010, PAMI
   Wang J, 2014, CVPR
   Welinder P, 2010, CNSTR2010001 CALTECH
   Yang H., 2014, CVPR
   Yang Y., 2013, PAMI
   Yi K. M., 2016, ECCV
   Zagoruyko S., 2015, CVPR
   Zbontar  J., 2015, CVPR
   Zhou T., 2015, CVPR
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704012
DA 2019-06-15
ER

PT S
AU Defferrard, M
   Bresson, X
   Vandergheynst, P
AF Defferrard, Michael
   Bresson, Xavier
   Vandergheynst, Pierre
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Convolutional Neural Networks on Graphs with Fast Localized Spectral
   Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CUTS
AB In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.
C1 [Defferrard, Michael; Bresson, Xavier; Vandergheynst, Pierre] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Defferrard, M (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM michael.defferrard@epfl.ch; xavier.bresson@epfl.ch;
   pierre.vandergheynst@epfl.ch
OI Defferrard, Michael/0000-0002-6028-9024
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Bawa M., 2005, P 14 INT C WORLD WID, P651, DOI DOI 10.1145/1060745.1060840
   Belkin M, 2008, J COMPUT SYST SCI, V74, P1289, DOI 10.1016/j.jcss.2007.08.006
   Bruna J., 2013, ABS13126203 CORR
   BUI TN, 1992, INFORM PROCESS LETT, V42, P153, DOI 10.1016/0020-0190(92)90140-Q
   Chung F. R., 1997, SPECTRAL GRAPH THEOR, V92
   Coates A., 2011, ADV NEURAL INFORM PR, V24, P2528
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Gavish M., 2010, P 27 INT C MACH LEAR, P367
   Gregor K., 2010, ARXIV10060448
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Henaff Mikael, 2015, ARXIV150605163
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Joachims  T., 1996, CMUCS96118
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Yujia, GATED GRAPH SEQUENCE
   Mallat S., 1999, WAVELET TOUR SIGNAL
   Masci J., 2015, P IEEE INT C COMP VI, P37
   Mikolov T., 2013, INT C LEARN REPR
   Pasdeloup B, 2015, EUR SIGNAL PR CONF, P1496, DOI 10.1109/EUSIPCO.2015.7362633
   Perraudin N., 2016, ARXIV160303030
   Ram I, 2011, IEEE T SIGNAL PROCES, V59, P4199, DOI 10.1109/TSP.2011.2158428
   Ron D, 2011, MULTISCALE MODEL SIM, V9, P407, DOI 10.1137/100791142
   Scarselli F., GRAPH NEURAL NETWORK, V20, P61
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shuman DI, 2016, IEEE T SIGNAL PROCES, V64, P2119, DOI 10.1109/TSP.2015.2512529
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Susnjara A., 2015, ARXIV150904537
   Tsitsvero M, 2015, EUR SIGNAL PR CONF, P1506, DOI 10.1109/EUSIPCO.2015.7362635
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
NR 34
TC 1
Z9 1
U1 4
U2 4
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700045
DA 2019-06-15
ER

PT S
AU Feldman, D
   Volkov, M
   Rus, D
AF Feldman, Dan
   Volkov, Mikhail
   Rus, Daniela
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dimensionality Reduction of Massive Sparse Datasets Using Coresets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION; ALGORITHMS
AB In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices. We show applications of our approach to computing the Principle Component Analysis (PCA) of any n x d matrix, using one pass over the stream of its rows. Our solution uses coresets: a scaled subset of the n rows that approximates their sum of squared distances to every k-dimensional affine subspace. An open theoretical problem has been to compute such a coreset that is independent of both n and d. An open practical problem has been to compute a non-trivial approximation to the PCA of very large but sparse databases such as the Wikipedia document-term matrix in a reasonable time. We answer both of these questions affirmatively. Our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream.
C1 [Feldman, Dan] Univ Haifa, Haifa, Israel.
   [Volkov, Mikhail; Rus, Daniela] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Feldman, D (reprint author), Univ Haifa, Haifa, Israel.
EM dannyf.post@gmail.com; mikhail@csail.mit.edu; rus@csail.mit.edu
FU Hon Hai/Foxconn Technology Group; NSFSaTC-BSF [CNC 1526815]; Singapore
   MIT Alliance on Research and Technology through the Future of Urban
   Mobility project; Toyota Research Institute (TRI); TRI
FX Support for this research has been provided by Hon Hai/Foxconn
   Technology Group and NSFSaTC-BSF CNC 1526815, and in part by the
   Singapore MIT Alliance on Research and Technology through the Future of
   Urban Mobility project and by Toyota Research Institute (TRI). TRI
   provided funds to assist the authors with their research but this
   article solely reflects the opinions and conclusions of its authors and
   not TRI or any other Toyota entity. We are grateful for this support.
CR Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097
   Arora S, 2006, LECT NOTES COMPUT SC, V4110, P272
   Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873
   Caratheodory C., 1911, REND CIRC MAT PALERM, V3, P193, DOI [DOI 10.1007/BF03014795, 10.1007/BF03014795]
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Clarkson KL, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1824777.1824783
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cohen Michael B., 2016, CORR
   Drineas P, 2011, INFORM PROCESS LETT, V111, P385, DOI 10.1016/j.ipl.2011.01.010
   Feldman D., 2010, P 41 ANN ACM S THE0R
   Feldman D., 2013, P ACM SIAM S DISCR A
   Halko NP, 2012, THESIS
   Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042
   Journee M, 2010, J MACH LEARN RES, V11, P517
   Lanczos C, 1950, ITERATION METHOD SOL
   Langberg M., 2010, P ACM SIAM S DISCR A
   Liberty E, 2007, P NATL ACAD SCI USA, V104, P20167, DOI 10.1073/pnas.0709640104
   Paige C. C., 1972, Journal of the Institute of Mathematics and Its Applications, V10, P373
   Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P159, DOI 10.1145/275487.275505
   Ruvrek R., 2011, GENSIMSTATISTICAL SE
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705010
DA 2019-06-15
ER

PT S
AU Gal, Y
   Ghahramani, Z
AF Gal, Yarin
   Ghahramani, Zoubin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Theoretically Grounded Application of Dropout in Recurrent Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.
C1 [Gal, Yarin; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England.
RP Gal, Y (reprint author), Univ Cambridge, Cambridge, England.
EM yg279@cam.ac.uk; zg201@cam.ac.uk
CR [Anonymous], 2015, KERAS
   Balan A. K., 2015, NIPS
   Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215
   Bayer  J., 2013, ARXIV13110701
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Bluche  Theodore, 2015, ICDAR
   Blundell C., 2015, ICML
   Cho  K., 2014, EMNLP
   Gal Y., 2015, ARXIV150602158
   Gal Y, 2015, ARXIV150602142
   Graves A., 2013, ICASSP
   Graves A., 2011, NIPS
   Gunther J, 2015, IEEE WORK APPL SIG
   Hernandez-Lobato J. M., 2015, ICML
   Hinton G. E, 2012, ARXIV12070580
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Hochreiter  S, 1997, NEURAL COMPUTATION, V9
   Kalchbrenner  N, 2013, EMNLP
   Kingma D. P., 2015, NIPS
   Kingma D. P., 2014, ARXIV14126980
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Neal R M, 1995, THESIS
   Pachitariu M., 2013, ARXIV13015650
   Pang  Bo, 2005, ACL
   Pham V., 2014, ICFHR
   Rezende D. J., 2014, ICML
   Srivastava N., 2014, JMLR
   Sundermeyer  M., 2012, INTERSPEECH
   Sutskever  I., 2014, NIPS
   Zaremba W, 2014, ARXIV14092329
NR 30
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701098
DA 2019-06-15
ER

PT S
AU Germain, P
   Bach, F
   Lacoste, A
   Lacoste-Julien, S
AF Germain, Pascal
   Bach, Francis
   Lacoste, Alexandre
   Lacoste-Julien, Simon
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI PAC-Bayesian Theory Meets Bayesian Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BOUNDS
AB We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.
C1 [Germain, Pascal; Bach, Francis; Lacoste-Julien, Simon] INRIA Paris, Ecole Normale Super, Paris, France.
   [Lacoste, Alexandre] Google, Mountain View, CA USA.
RP Germain, P (reprint author), INRIA Paris, Ecole Normale Super, Paris, France.
EM pascal.germain@inria.fr; francis.bach@inria.fr;
   alexandre.lacoste@inria.fr; simon.lacoste-julien@inria.fr
CR Alquier P, 2016, J MACH LEARN RES, V17
   Ambroladze  Amiran, 2006, NIPS
   Banerjee  Arindam, 2006, ICML, P81
   Begin Luc, 2014, INT C ART INT STAT A, P105
   Bishop C. M., 2006, INFORM SCI STAT
   Bissiri P. G., 2016, J ROYAL STAT SOC B
   Boucheron S., 2013, CONCENTRATION INEQUA
   C. Williamson  Robert, 1997, COLT
   Catoni  Olivier, 2007, PAC BAYESIAN SUPERVI, V56
   Dalalyan A, 2008, MACH LEARN, V72, P39, DOI 10.1007/s10994-008-5051-0
   Germain P., 2009, P 26 ANN INT C MACH, P353
   Germain Pascal, 2016, ICML, V48, P859
   Germain  Pascal, 2015, JMLR, V16
   Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541
   Grunwald  Peter, 2012, ALT
   Grunwald Peter D., 2016, ABS160500252 CORR
   Guyon I, 2010, J MACH LEARN RES, V11, P61
   Hazan T., 2013, P ADV NEUR INF PROC, P1887
   Jeffreys William H., 1992, AM SCI
   Lacoste  Alexandre, 2015, THESIS
   Langford  John, 2002, NIPS, P423
   Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415
   Maurer  Andreas, 2004, CSLG0411099 CORR
   McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   McAllester  David, 2011, P ADV NEUR INF PROC, P2205
   Noy  Asf, 2014, AISTATS
   Pentina  Anastasia, 2014, ICML
   Seeger M., 2003, THESIS
   Seeger Matthias, 2002, JMLR, V3, P233
   Seldin  Yevgeny, 2010, JMLR, V11
   Seldin  Yevgeny, 2012, UAI
   Seldin Yevgeny, 2011, NIPS, P1683
   Tolstikhin Ilya O., 2013, NIPS
   Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704093
DA 2019-06-15
ER

PT S
AU Gregor, K
   Besse, F
   Rezende, DJ
   Danihelka, I
   Wierstra, D
AF Gregor, Karol
   Besse, Frederic
   Rezende, Danilo Jimenez
   Danihelka, Ivo
   Wierstra, Daan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Towards Conceptual Compression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce convolutional DRAW, a homogeneous deep generative model achieving state-of-the-art performance in latent variable image modeling. The algorithm naturally stratifies information into higher and lower level details, creating abstract features and as such addressing one of the fundamentally desired properties of representation learning. Furthermore, the hierarchical ordering of its latents creates the opportunity to selectively store global information about an image, yielding a high quality 'conceptual compression' framework.
C1 [Gregor, Karol; Besse, Frederic; Rezende, Danilo Jimenez; Danihelka, Ivo; Wierstra, Daan] Google DeepMind, London, England.
RP Gregor, K (reprint author), Google DeepMind, London, England.
EM karolg@google.com; fbesse@google.com; danilor@google.com;
   danihelka@google.com; wierstra@google.com
CR Carlson T, 2013, J VISION, V13, DOI 10.1167/13.10.1
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor Karol, 2015, P 32 INT C MACH LEAR
   Gregor Karol, 2014, P 31 INT C MACH LEAR
   Gregor Karol, 2011, ARXIV11081169
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2014, P INT C LEARN REPR I
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Larochelle H., 2011, P 14 INT C ART INT S, P29
   Oord  A.v.d., 2016, ARXIV160106759
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
NR 16
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704066
DA 2019-06-15
ER

PT S
AU Hardt, M
   Price, E
   Srebro, N
AF Hardt, Moritz
   Price, Eric
   Srebro, Nathan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Equality of Opportunity in Supervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.
C1 [Hardt, Moritz] Google, Mountain View, CA 94043 USA.
   [Price, Eric] UT Austin, Austin, TX USA.
   [Srebro, Nathan] TTI Chicago, Chicago, IL USA.
   [Price, Eric] OpenAI, San Francisco, CA USA.
RP Hardt, M (reprint author), Google, Mountain View, CA 94043 USA.
EM m@mrtz.org; ecprice@cs.utexas.edu; nati@ttic.edu
CR [Anonymous], 2016, BIG DAT REP ALG SYST
   Barocas S., 2016, CALIFORNIA LAW REV, V104
   Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Kleinberg Jon M., 2016, ABS160905807 CORR
   Pedreshi Dino, 2008, P 14 ACM SIGKDD
   Podesta John, 2014, BIG DATA SEIZING OPP
   US Federal Reserve, 2007, C CRED SCOR ITS EFF
   Wasserman L., 2010, ALL STAT CONCISE COU
   Zafar Muhammad Bilal, 2015, ABS150705259 CORR
   Zliobaite Indre, 2015, ABS150505723 CORR
NR 11
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703010
DA 2019-06-15
ER

PT S
AU He, B
   De Sa, C
   Mitliagkas, I
   Re, C
AF He, Bryan
   De Sa, Christopher
   Mitliagkas, Ioannis
   Re, Christopher
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on
   How Much
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SYSTEMATIC SCAN
AB Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.
C1 [He, Bryan; De Sa, Christopher; Mitliagkas, Ioannis; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA.
RP He, B (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM bryanhe@stanford.edu; cdesa@stanford.edu; imit@stanford.edu;
   chrismre@stanford.edu
FU DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; NSF [IIS-1247701,
   CCF-1111943, CCF-1337375, IIS-1353606, DGE-114747]; DOE [108845]; ONR
   [N000141210041, N000141310129]; NIH [U54EB020405]; DARPA's SIMPLEX
   program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship;
   Moore Foundation; American Family Insurance; Google; Toshiba
FX The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF
   IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA
   FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129;
   NIH U54EB020405; NSF DGE-114747; DARPA's SIMPLEX program; Oracle;
   NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation;
   American Family Insurance; Google; and Toshiba. The views and
   conclusions expressed in this material are those of the authors and
   should not be interpreted as necessarily representing the official
   policies or endorsements, either expressed or implied, of DARPA, AFRL,
   NSF, ONR, NIH, or the U.S. Government.
CR Benjamini I, 2005, T AM MATH SOC, V357, P3013, DOI 10.1090/S0002-9947-05-03610-X
   Diaconis P, 2000, MICH MATH J, V48, P157
   Diaconis P, 2013, BERNOULLI, V19, P1294, DOI 10.3150/12-BEJSP09
   Dyer M, 2006, ANN APPL PROBAB, V16, P185, DOI 10.1214/105051605000000683
   Dyer M, 2008, COMB PROBAB COMPUT, V17, P761, DOI 10.1017/S0963548308009437
   Finkel Jenny Rose, 2005, P 43 ANN M ASS COMP
   GELFAND AE, 1990, J AM STAT ASSOC, V85, P398, DOI 10.2307/2289776
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Gotovos A., 2015, ADV NEURAL INFORM PR
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   GURBUZBALABAN M, 2015, ARXIV151008562
   Hayes T. P., 2006, 47 ANN IEEE S FDN CO
   Hellerstein JM, 2012, PROC VLDB ENDOW, V5, P1700, DOI 10.14778/2367502.2367510
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680
   McCallum A., 2009, ADV NEURAL INFORM PR
   Plummer M., 2003, P 3 INT WORKSH DISTR
   Recht B., 2012, P 25 ANN C LEARN THE
   Roberts G. O., 2015, INT J STAT PROB, V5, P51, DOI DOI 10.5539/IJSP.V5N1P51
   Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931
   Zhang C., 2013, P 2013 ACM SIGMOD IN
   Zhang C., 2015, ADV NEURAL INFORM PR
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
   Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705003
DA 2019-06-15
ER

PT S
AU Ho, J
   Ermon, S
AF Ho, Jonathan
   Ermon, Stefano
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generative Adversarial Imitation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.
C1 [Ho, Jonathan] OpenAI, San Francisco, CA 94110 USA.
   [Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA.
RP Ho, J (reprint author), OpenAI, San Francisco, CA 94110 USA.
EM hoj@openai.com; ermon@cs.stanford.edu
FU NSF Graduate Research Fellowship [DGE-114747]
FX We thank Jayesh K. Gupta, John Schulman, and the anonymous reviewers for
   assistance, advice, and critique. This work was supported by the
   SAIL-Toyota Center for AI Research and by a NSF Graduate Research
   Fellowship (grant no. DGE-114747).
CR Abbeel P., 2004, P 21 INT C MACH LEAR
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brockman G, 2016, ARXIV160601540
   Finn C., 2016, P 33 INT C MACH LEAR
   Geramifard A., 2015, JMLR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hiriart-Urruty Jean-Baptiste, 1996, CONVEX ANAL MINIMIZA, V305
   Ho J., 2016, P 33 INT C MACH LEAR
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Levine S., 2011, ADV NEURAL INFORM PR, P19
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Levine S., 2012, P 29 INT C MACH LEAR, P41
   Moore Andrew William, 1990, EFFICIENT MEMORY BAS
   Ng A., 2000, ICML
   Nguyen X, 2009, ANN STAT, V37, P876, DOI 10.1214/08-AOS595
   Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88
   Puterman M. L., 2014, MARKOV DECISION PROC
   Ratliff ND, 2009, AUTON ROBOT, V27, P25, DOI 10.1007/s10514-009-9121-3
   Ross S., 2011, J MACHINE LEARNING R, P627
   Ross Stephane, 2010, INT C ART INT STAT, P661
   Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman J., 2015, ARXIV150602438
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Syed U., 2007, ADV NEURAL INFORM PR, P1449
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Ziebart B. D., 2010, P 27 INT C MACH LEAR, P1255
   Ziebart B. D., 2008, AAAI AAAI 08
NR 30
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703027
DA 2019-06-15
ER

PT S
AU Horel, T
   Singer, Y
AF Horel, Thibaut
   Singer, Yaron
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Maximization of Approximately Submodular Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of maximizing a function that is approximately submodular under a cardinality constraint. Approximate submodularity implicitly appears in a wide range of applications as in many cases errors in evaluation of a submodular function break submodularity. Say that F is epsilon-approximately submodular if there exists a submodular function f such that (1 - epsilon)f (S) <= F(S) <= (1+epsilon) f (S) for all subsets S. We are interested in characterizing the query-complexity of maximizing F subject to a cardinality constraint k as a function of the error level epsilon > 0. We provide both lower and upper bounds: for epsilon > n(-1/2) we show an exponential query-complexity lower bound. In contrast, when epsilon < 1/k or under a stronger bounded curvature assumption, we give constant approximation algorithms.
C1 [Horel, Thibaut; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
RP Horel, T (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM thorel@seas.harvard.edu; yaron@seas.harvard.edu
CR Bach F., 2010, NIPS
   Badanidiyuru Ashwinkumar, 2012, P 23 ANN ACM SIAM S, P1025
   Balcan MF, 2011, ACM S THEORY COMPUT, P793
   Belloni Alexandre, 2015, P 28 C LEARN THEOR, P240
   Chen  Y., 2015, P 28 C LEARN THEOR, P338
   Das  A., 2012, NIPS
   Das  A., 2011, ICML
   Defazio  A., 2012, NIPS
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Gomes R., 2010, ICML
   Gomez Rodriguez  M., 2011, ACM TKDD, V5
   Guestrin C., 2005, INT C MACH LEARN ICM
   Guillory Andrew, 2011, ICML
   Hassidim  A., 2016, ABS160103095 CORR
   Hoi  S., 2006, ICML
   Iyer R, 2013, ADV NEURAL INFORM PR, V26, P2742
   Iyer R. K., 2013, ADV NEURAL INFORM PR, P2436
   Kempe D., 2003, KDD
   Krause  A., 2007, NAT C ART INT AAAI N
   Krause A, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1989734.1989736
   Krause Andreas, 2007, ICML
   Lin  H., 2011, ACL HLT
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Rodriguez M. G., 2012, ICML
   Singer Yaron, 2015, NIPS, P3186
   Singla  A., 2015, ARXIV151107211
   Song  H., 2014, ICML
   Tschiatschek S., 2014, NIPS
   Zheng  J., 2014, NIPS
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701093
DA 2019-06-15
ER

PT S
AU Hubara, I
   Courbariaux, M
   Soudry, D
   El-Yaniv, R
   Bengio, Y
AF Hubara, Itay
   Courbariaux, Matthieu
   Soudry, Daniel
   El-Yaniv, Ran
   Bengio, Yoshua
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Binarized Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.
C1 [Hubara, Itay] Technion Israel Inst Technol, Haifa, Israel.
   [Courbariaux, Matthieu; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
   [Soudry, Daniel] Columbia Univ, New York, NY 10027 USA.
RP Hubara, I (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM itayh@technion.ac.il; matthieu.courbariaux@gmail.com;
   daniel.soudry@gmail.com; rani@cs.technion.ac.il;
   yoshua.umontreal@gmail.com
FU NSERC; Canada Research Chairs; Compute Canada; CIFAR; IBM; Samsung;
   Israel Science Foundation [1890/14]
FX We would like to express our appreciation to Elad Hoffer, for his
   technical assistance and constructive comments. We thank our fellow MILA
   lab members who took the time to read the article and give us some
   feedback. We thank the developers of Torch, Collobert et al. (2011) a
   Lua based environment, and Theano (Bergstra et al., 2010; Bastien et
   al., 2012), a Python library which allowed us to easily develop a fast
   and optimized code for GPU. We also thank the developers of Pylearn2
   (Goodfellow et al., 2013) and Lasagne (Dieleman et al., 2015), two Deep
   Learning libraries built on the top of Theano. We thank Yuxin Wu for
   helping us compare our GPU kernels with cuBLAS. We are also grateful for
   funding from NSERC, the Canada Research Chairs, Compute Canada, and
   CIFAR. We are also grateful for funding from CIFAR, NSERC, IBM, Samsung.
   This research was also supported by The Israel Science Foundation (grant
   No. 1890/14).
CR Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101
   Bastien F., 2012, DEEP LEARN UNSUP FEA
   Beauchamp M., 2006, FPGA 06, P12, DOI DOI 10.1145/1117201.1117204
   Bengio Y., 2013, ARXIV13052982
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Cheng Z., 2015, ARXIV150303562
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Collobert  R., 2011, BIGLEARN
   Courbariaux M., 2014, ARXIV E PRINTS
   Courbariaux M., 2015, ARXIV E PRINTS
   Dieleman S, 2015, LASAGNE 1 RELEASE
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Glorot X., 2010, AISTATS 2010
   Gong Y., 2014, ARXIV14126115
   Goodfellow I. J., 2013, ARXIV13084214
   Govindu G., 2004, P 18 INT PAR DISTR P, P149
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Hinton G, 2012, NEURAL NETWORKS MACH
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hubara I., 2016, ARXIV160907061
   Hwang Kyuyeon, 2014, SIGN PROC SYST SIPS, P1, DOI DOI 10.1109/SIPS.2014.6986082
   Ioffe S., 2015, BATCH NORMALIZATION
   Kim M., 2016, ARXIV E PRINTS
   Kingma D. P., 2014, ARXIV14126980
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C.-Y., 2015, ARXIV150908985
   Lin Z., 2015, ARXIV E PRINTS
   Soudry D., 2014, NIPS 2014
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2014, ARXIV14094842
   Wan L., 2013, ICML 2013
NR 34
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704097
DA 2019-06-15
ER

PT S
AU Kawaguchi, K
AF Kawaguchi, Kenji
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Learning without Poor Local Minima
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.
C1 [Kawaguchi, Kenji] MIT, Cambridge, MA 02139 USA.
RP Kawaguchi, K (reprint author), MIT, Cambridge, MA 02139 USA.
EM kawaguch@mit.edu
FU NSF [1420927]; ONR [N00014-14-1-0486]; ARO [W911NF1410433]
FX The author would like to thank Prof. Leslie Kaelbling, Quynh Nguyen, Li
   Huan and Anirbit Mukherjee for their thoughtful comments on the paper.
   We gratefully acknowledge support from NSF grant 1420927, from ONR grant
   N00014-14-1-0486, and from ARO grant W911NF1410433.
CR BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Baldi P, 2012, NEURAL NETWORKS, V33, P136, DOI 10.1016/j.neunet.2012.04.011
   Baldi Pierre, 1989, ADV NEURAL INFORMATI, P65
   BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3
   Choromanska A., 2015, ARTIF INTELL, P192
   Choromanska A., 2015, P 28 C LEARN THEOR C, P1756
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Mhaskar Hrushikesh, 2016, 45 MIT CBMM
   MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948
   Rockafellar R. T., 2009, VARIATIONAL ANAL, V317
   Saxe  A.M., 2014, INT C LEARN REPR
   Zhang Fuzhen, 2006, SCHUR COMPLEMENT ITS, V4
NR 15
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700076
DA 2019-06-15
ER

PT S
AU Kazemi, SM
   Kimmig, A
   Van den Broeck, G
   Poole, D
AF Kazemi, Seyed Mehran
   Kimmig, Angelika
   Van den Broeck, Guy
   Poole, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI New Liftable Classes for First-Order Probabilistic Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes (SFO2)-F-2 and (SRU)-R-2 of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.
C1 [Kazemi, Seyed Mehran; Poole, David] Univ British Columbia, Vancouver, BC, Canada.
   [Kimmig, Angelika] Katholieke Univ Leuven, Leuven, Belgium.
   [Van den Broeck, Guy] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
RP Kazemi, SM (reprint author), Univ British Columbia, Vancouver, BC, Canada.
EM smkazemi@cs.ubc.ca; angelika.kimmig@cs.kuleuven.be; guyvdb@cs.ucla.edu;
   poole@cs.ubc.ca
OI Van den Broeck, Guy/0000-0003-3434-2503
FU Research Foundation Flanders (FWO); NSF [IIS-1633857]
FX AK is supported by the Research Foundation Flanders (FWO). GVdB is
   partially supported by NSF (#IIS-1633857).
CR Ahmadi Babak, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P585, DOI 10.1007/978-3-642-33460-3_43
   Ball W. W. Rouse, 1960, MATH RECREATIONS ESS, P45
   Beame P, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P313, DOI 10.1145/2745754.2745760
   Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319
   Bui Hung Hai, 2013, UAI, P132
   Choi Jaesik, 2011, AAAI
   Dalvi N, 2007, VLDB J, V16, P523, DOI 10.1007/s00778-006-0004-3
   De Raedt Luc, 2016, SYNTHESIS LECT ARTIF, V10, P1, DOI DOI 10.2200/S00692ED1V01Y201601AIM032
   De Raedt Luc, 2007, IJCAI, V7
   Getoor L, 2007, INTRO STAT RELATIONA
   Gogate V., 2011, UAI, P256
   Jaeger Manfred, 1997, UAI
   Jernite Yacine, 2015, ICML
   Jha Abhay, 2010, P 24 ANN C NEUR INF, P973
   Kazemi Seyed Mehran, 2016, ARXIV160604512
   Kazemi Seyed Mehran, 2016, KR
   Kersting K., 2009, P 25 C UNC ART INT, P277
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kopp Timothy, 2015, NIPS, P1315
   Milch Brian, 2008, AAAI, P1062
   Niepert Mathias, 2012, UAI
   Poole D., 2003, P 18 INT JOINT C ART, P985
   Poole David, 2011, ARXIV11074035CSAI
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Singla P., 2008, P 23 AAAI C ART INT, P1094
   Suciu Dan, 2011, SYNTHESIS LECT DATA, DOI 10.2200/S00362ED1V01Y201105DTM016
   Taghipour Nima, 2013, P 16 INT C ART INT S, P572
   Van Den Broeck G., 2011, IJCAI, V3, P2178
   Van den Broeck Guy, 2012, UAI
   Van den Broeck Guy, 2011, ADV NEURAL INFORM PR, P1386
   Van den Broeck Guy, 2014, KR
   Van Haaren Jan, 2015, MACH LEARN, P1
   Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17
   Venugopal D., 2014, ADV NEURAL INFORM PR, P2978
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705017
DA 2019-06-15
ER

PT S
AU Kia, SM
   Vega-Pons, S
   Olivetti, E
   Avesani, P
AF Kia, Seyed Mostafa
   Vega-Pons, Sandro
   Olivetti, Emanuele
   Avesani, Paolo
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Multi-Task Learning for Interpretation of Brain Decoding Models
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID FMRI; REGULARIZATION; SMOOTHNESS; PREDICTION; SELECTION; SPARSITY
AB Improving the interpretability of multivariate models is of primary interest for many neuroimaging studies. In this study, we present an application of multi-task learning (MTL) to enhance the interpretability of linear classifiers once applied to neuroimaging data. To attain our goal, we propose to divide the data into spatial fractions and define the temporal data of each spatial unit as a task in MTL paradigm. Our result on magnetoencephalography (MEG) data reveals preliminary evidence that, (1) dividing the brain recordings into spatial fractions based on spatial units of data and (2) considering each spatial fraction as a task, are two factors that provide more stability and consequently more interpretability for brain decoding models.
C1 [Kia, Seyed Mostafa] Univ Trento, Trento, Italy.
   [Kia, Seyed Mostafa; Vega-Pons, Sandro; Olivetti, Emanuele; Avesani, Paolo] Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.
   [Kia, Seyed Mostafa; Vega-Pons, Sandro; Olivetti, Emanuele; Avesani, Paolo] Univ Trento, CIMeC, Trento, Italy.
RP Kia, SM (reprint author), Univ Trento, Trento, Italy.; Kia, SM (reprint author), Bruno Kessler Fdn, NeuroInformat Lab NILab, Trento, Italy.; Kia, SM (reprint author), Univ Trento, CIMeC, Trento, Italy.
EM seyedmostafa.kia@unitn.it
RI Kia, Seyed Mostafa/B-6337-2018
OI Kia, Seyed Mostafa/0000-0002-7128-814X
CR Afshin-Pour B, 2011, HUM BRAIN MAPP, V32, P699, DOI 10.1002/hbm.21057
   Ben-David S., 2002, INT C KNOWL DISC DAT, P443
   Carroll MK, 2009, NEUROIMAGE, V44, P112, DOI 10.1016/j.neuroimage.2008.08.020
   Chen J., 2011, MALSAR MULTITASK LEA
   Chen X., 2010, ARXIV10053579
   Conroy BR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079271
   Cox DD, 2003, NEUROIMAGE, V19, P261, DOI 10.1016/S1053-8119(03)00049-1
   de Brecht M, 2012, NEUROIMAGE, V60, P1550, DOI 10.1016/j.neuroimage.2011.12.085
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   Gramfort A, 2013, INT WORKSHOP PATTERN, P17, DOI 10.1109/PRNI.2013.14
   Groppe DM, 2011, PSYCHOPHYSIOLOGY, V48, P1711, DOI 10.1111/j.1469-8986.2011.01273.x
   Grosenick L, 2013, NEUROIMAGE, V72, P304, DOI 10.1016/j.neuroimage.2012.12.062
   Haufe S., 2014, 2014 INT WORKSH PATT, P1
   Haufe S, 2014, NEUROIMAGE, V87, P96, DOI 10.1016/j.neuroimage.2013.10.067
   Henson RN, 2011, FRONT HUM NEUROSCI, V5, DOI 10.3389/fnhum.2011.00076
   Kia S.M., 2013, THESIS
   Maris E, 2007, J NEUROSCI METH, V164, P177, DOI 10.1016/j.jneumeth.2007.03.024
   Maris E, 2012, PSYCHOPHYSIOLOGY, V49, P549, DOI 10.1111/j.1469-8986.2011.01320.x
   Olivetti Emanuele, 2010, 2010 First Workshop on Brain Decoding: Pattern Recognition Challenges in Neuroimaging (WBD 2010), P40, DOI 10.1109/WBD.2010.9
   Olivetti E., 2014, 2014 INT WORKSH PATT
   Rasmussen PM, 2012, PATTERN RECOGN, V45, P2085, DOI 10.1016/j.patcog.2011.09.011
   Rish I, 2014, NEURAL INF PROCESS S, P1
   Strother S. C., 2014, STABILITY REPRODUCIB
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Valverde-Albacete FJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0084217
   van Gerven M, 2009, NEUROIMAGE, V46, P665, DOI 10.1016/j.neuroimage.2009.02.041
   Varoquaux G., 2012, ARXIV12066447
   Xing EP, 2014, NEURAL INF PROCESS S, P37
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 31
TC 1
Z9 1
U1 0
U2 7
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 3
EP 11
DI 10.1007/978-3-319-45174-9_1
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400001
DA 2019-06-15
ER

PT S
AU Kim, B
   Khanna, R
   Koyejo, O
AF Kim, Been
   Khanna, Rajiv
   Koyejo, Oluwasanmi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Examples are not Enough, Learn to Criticize! Criticism for
   Interpretability
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.
C1 [Kim, Been] Allen Inst AI, Seattle, WA 98103 USA.
   [Khanna, Rajiv] UT Austin, Austin, TX USA.
   [Koyejo, Oluwasanmi] UIUC, Champaign, IL USA.
RP Kim, B (reprint author), Allen Inst AI, Seattle, WA 98103 USA.
EM beenkim@csail.mit.edu; rajivak@utexas.edu; sanmi@illinois.edu
CR Aamodt A., 1994, AI COMMUNICATIONS
   Badanidiyuru A., 2014, KDD
   Bichindaritz I., 2006, AI IN MED
   Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495
   Caruana R., 2015, KDD
   Cohen M. S., 1996, HUMAN FACTORS
   Deng J., 2009, CVPR
   Feige U., 1998, JACM
   Gelman A., 2014, BAYESIAN DATA ANAL
   Gretton A., 2008, JMLR
   He  K., 2015, ARXIV151203385
   Hull J. J., 1994, TPAMI
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Kaufman Leonard, 1987, CLUSTERING MEANS MED
   Kim B., 2014, NIPS
   Koyejo O. O., 2014, NIPS
   Krause Andreas, 2008, JMLR
   Kuncheva LI, 1998, IEEE T SYST MAN CY C, V28, P160, DOI 10.1109/5326.661099
   Lin H., 2011, ACL
   Lloyd J. R., 2015, NIPS
   Mirzasoleiman Baharan, 2015, NIPS
   Nemhauser G., 1978, MATH PROGRAMMING
   Newell A., 1972, HUMAN PROBLEM SOLVIN
   Priebe C. E., 2003, J CLASSIFICATION
   Russakovsky Olga, 2015, IJCV
   Sharma D., 2015, ICML
   Simon I., 2007, ICCV, P6
   Varshney K. R., 2016, ARXIV160104126
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702047
DA 2019-06-15
ER

PT S
AU McNamee, D
   Wolpert, D
   Lengyel, M
AF McNamee, Daniel
   Wolpert, Daniel
   Lengyel, Mate
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient state-space modularization for planning: theory, behavioral
   and neural signatures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INFERENCE; SYSTEMS; MDPS
AB Even in state-spaces of modest size, planning is plagued by the "curse of dimensionality". This problem is particularly acute in human and animal cognition given the limited capacity of working memory, and the time pressures under which planning often occurs in the natural environment. Hierarchically organized modular representations have long been suggested to underlie the capacity of biological systems (1,2) to efficiently and flexibly plan in complex environments. However, the principles underlying efficient modularization remain obscure, making it difficult to identify its behavioral and neural signatures. Here, we develop a normative theory of efficient state-space representations which partitions an environment into distinct modules by minimizing the average (information theoretic) description length of planning within the environment, thereby optimally trading off the complexity of planning across and within modules. We show that such optimal representations provide a unifying account for a diverse range of hitherto unrelated phenomena at multiple levels of behavior and neural representation.
C1 [McNamee, Daniel; Wolpert, Daniel; Lengyel, Mate] Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge CB2 1PZ, England.
RP McNamee, D (reprint author), Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge CB2 1PZ, England.
EM d.mcnamee@eng.cam.ac.uk; wolpert@eng.cam.ac.uk; m.lengyel@eng.cam.ac.uk
CR Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349
   Barnes TD, 2005, NATURE, V437, P1158, DOI 10.1038/nature04053
   Bonasia K, 2016, HIPPOCAMPUS, V26, P9, DOI 10.1002/hipo.22539
   Boutilier C, 1999, J ARTIF INTELL RES, V11, P1
   Daw ND, 2005, NAT NEUROSCI, V8, P1704, DOI 10.1038/nn1560
   Dayan P, 1992, ADV NEURAL INFORM PR
   Foster D, 2002, MACH LEARN, P325
   Fujii N, 2003, SCIENCE, V301, P1246, DOI 10.1126/science.1086872
   Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638
   Gershman SJ, 2009, J NEUROSCI, V29, P13524, DOI 10.1523/JNEUROSCI.2469-09.2009
   Hauskrecht M, 1998, UNCERTAINTY ARTIFICI
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   Huys QJM, 2015, P NATL ACAD SCI USA, V112, P3098, DOI 10.1073/pnas.1414219112
   Javadi AH, 2016, NATURE COMMUNICATION
   Jin X, 2010, NATURE, V466, P457, DOI 10.1038/nature09263
   Kafsi M, 2013, IEEE T INFORM THEORY, V59, P5577, DOI 10.1109/TIT.2013.2262497
   Kemeny J. G., 1983, FINITE MARKOV CHAINS
   Kim KE, 2003, ARTIF INTELL, V147, P225, DOI 10.1016/S0004-3702(02)00377-6
   Lashley K. S., 1951, CEREBRAL MECH BEHAV, P112, DOI DOI 10.1093/RFS/HHQ153
   Lengyel M, 2007, ADV NEURAL INFORM PR
   Littman ML, 1998, J ARTIF INTELL RES, V9, P1
   MacKay D. J, 2003, INFORM THEORY INFERE
   Moore AW, 1999, IJCAI INT JOINT C AR, V2, P1318
   Otto AR, 2013, PSYCHOL SCI, V24, P751, DOI 10.1177/0956797612463080
   Parr R., 1997, ADV NEURAL INFORM PR
   Rissanen J., 2007, INFORM COMPLEXITY ST
   Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105
   Rothkopf CA, 2010, FRONT PSYCHOL, V1, DOI 10.3389/fpsyg.2010.00173
   Russo E, 2008, NEW J PHYS, V10, DOI 10.1088/1367-2630/10/1/015008
   Schapiro AC, 2013, NAT NEUROSCI, V16, P486, DOI 10.1038/nn.3331
   Simon H, 1971, HUMAN PROBLEM SOLVIN
   Simsek O, 2008, ADV NEURAL INFORM PR
   Singh SP, 1995, ADV NEURAL INFORM PR
   Smith KS, 2013, NEURON, V79, P361, DOI 10.1016/j.neuron.2013.05.038
   Solway A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003779
   Stachenfeld K, 2014, ADV NEURAL INFORM PR
   Stalnaker TA, 2010, FRONT INTEGR NEUROSC, V4, DOI 10.3389/fnint.2010.00012
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Todd MT, 2008, ADV NEURAL INFORM PR
NR 40
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702067
DA 2019-06-15
ER

PT S
AU Pachitariu, M
   Steinmetz, N
   Kadir, S
   Carandini, M
   Harris, K
AF Pachitariu, Marius
   Steinmetz, Nick
   Kadir, Shabnam
   Carandini, Matteo
   Harris, Kenneth
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast and accurate spike sorting of high-channel count probes with
   KiloSort
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.
C1 [Pachitariu, Marius; Steinmetz, Nick; Kadir, Shabnam; Carandini, Matteo; Harris, Kenneth] UCL, London, England.
RP Pachitariu, M (reprint author), UCL, London, England.
EM ucgtmpa@ucl.ac.uk
CR Coates A, 2011, P 14 INT C ART INT S, P215
   Einevoll GT, 2012, CURR OPIN NEUROBIOL, V22, P11, DOI 10.1016/j.conb.2011.10.001
   Ekanadham C, 2014, J NEUROSCI METH, V222, P47, DOI 10.1016/j.jneumeth.2013.10.001
   Franke F, 2015, J NEUROPHYSIOL, V114, P2535, DOI 10.1152/jn.00993.2014
   Harris KD, 2000, J NEUROPHYSIOL, V84, P401
   Hill DN, 2011, J NEUROSCI, V31, P8699, DOI 10.1523/JNEUROSCI.0971-11.2011
   Neto Joana P, 2016, BIORXIV
   Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123
   Quiroga RQ, 2012, CURR BIOL, V22, pR45, DOI 10.1016/j.cub.2011.11.005
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Rossant C, 2016, NAT NEUROSCI, V19, P634, DOI 10.1038/nn.4268
NR 11
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702073
DA 2019-06-15
ER

PT S
AU Papamakarios, G
   Murray, I
AF Papamakarios, George
   Murray, Iain
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast epsilon-free Inference of Simulation Models with Bayesian
   Conditional Density Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MONTE-CARLO; COMPUTATION
AB Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an epsilon-ball around the observed data, which is only correct in the limit epsilon -> 0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as epsilon -> 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.
C1 [Papamakarios, George; Murray, Iain] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Papamakarios, G (reprint author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
EM g.papamakarios@ed.ac.uk; i.murray@ed.ac.uk
FU Centre for Doctoral Training in Data Science - EPSRC [EP/L016427/1];
   University of Edinburgh; Microsoft Research
FX We thank Amos Storkey for useful comments. George Papamakarios is
   supported by the Centre for Doctoral Training in Data Science, funded by
   EPSRC (grant EP/L016427/1) and the University of Edinburgh, and by
   Microsoft Research through its PhD Scholarship Programme.
CR Beaumont MA, 2002, GENETICS, V162, P2025
   Beaumont MA, 2009, BIOMETRIKA, V96, P983, DOI 10.1093/biomet/asp052
   Bishop C. M, 1994, NCRG94004 AST U
   Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0
   Bonassi FV, 2015, BAYESIAN ANAL, V10, P171, DOI 10.1214/14-BA891
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Fan Y, 2013, STAT, V2, P34, DOI 10.1002/sta4.15
   GOURIEROUX C, 1993, J APPL ECONOM, V8, pS85, DOI 10.1002/jae.3950080507
   Gu Shixiang, 2015, ADV NEURAL INFORM PR, P2629
   Gutmann M. U., 2015, ABS150103291V3 ARXIV
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100
   Meeds E., 2015, P 31 C UNC ART INT U, P582
   Meeds E., 2014, P 30 C UNC ART INT, V30
   Meeds T., 2015, ADV NEURAL INFORM PR, V28, P2071
   Morris Q., 2001, P 17 C UAI, P370
   Nair V, 2008, LECT NOTES COMPUT SC, V5163, P971, DOI 10.1007/978-3-540-87536-9_99
   Paige B., 2016, P 33 INT C MACH LEAR
   Papamakarios G., 2015, PROBABILISTIC INTEGR
   Pritchard JK, 1999, MOL BIOL EVOL, V16, P1791, DOI 10.1093/oxfordjournals.molbev.a026091
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Schafer C. M., 2012, STAT CHALLENGES MODE, V5, P3
   Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319
   Wu J., 2015, ADV NEURAL INFORM PR, P127, DOI DOI 10.1007/978-3-319-26532-2_15
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700048
DA 2019-06-15
ER

PT S
AU Picheny, V
   Gramacy, RB
   Wild, S
   Le Digabel, S
AF Picheny, Victor
   Gramacy, Robert B.
   Wild, Stefan
   Le Digabel, Sebastien
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian optimization under mixed constraints with a slack-variable
   augmented Lagrangian
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show our new slack "ALBO" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples.
C1 [Picheny, Victor] Univ Toulouse, MIAT, INRA, Castanet Tolosan, France.
   [Gramacy, Robert B.] Virginia Tech, Blacksburg, VA USA.
   [Wild, Stefan] Argonne Natl Lab, 9700 S Cass Ave, Argonne, IL 60439 USA.
   [Le Digabel, Sebastien] Ecole Polytech Montreal, Montreal, PQ, Canada.
RP Picheny, V (reprint author), Univ Toulouse, MIAT, INRA, Castanet Tolosan, France.
EM victor.picheny@toulouse.inra.fr; rbg@vt.edu;
   sebastien.le-digabel@polymtl.ca
RI Le Digabel, Sebastien/A-7740-2010
OI Le Digabel, Sebastien/0000-0003-3148-5090
FU National Science Foundation [DMS-1521702]; U.S. Department of Energy,
   Office of Science, Office of Advanced Scientific Computing Research
   [DE-AC02-06CH11357]; Natural Sciences and Engineering Research Council
   of Canada [418250]
FX We are grateful to Mickael Binois for comments on early drafts. RBG is
   grateful for partial support from National Science Foundation grant
   DMS-1521702. The work of SMW is supported by the U.S. Department of
   Energy, Office of Science, Office of Advanced Scientific Computing
   Research under Contract No. DE-AC02-06CH11357. The work of SLD is
   supported by the Natural Sciences and Engineering Research Council of
   Canada grant 418250.
CR Audet C., 2000, AIAA USAF NASA ISSMO
   Bertsekas DP, 1982, CONSTRAINED OPTIMIZA
   Box GE, 1987, EMPIRICAL MODEL BUIL
   Boyle  P., 2007, THESIS
   Brochu E., 2010, ARXIV1012, V2599, P1
   Gardner J. R., 2014, P 31 INT C MACH LEAR, V32
   Gelbart M. A., 2014, UNCERTAINTY ARTIFICI
   Gramacy RB, 2016, J STAT SOFTW, V72, P1, DOI 10.18637/jss.v072.i01
   Gramacy RB, 2016, TECHNOMETRICS, V58, P1, DOI 10.1080/00401706.2015.1014065
   Hernandez-Lobato J., 2015, P 32 INT C MACH LEAR, V37
   Johnson  S.G., 2014, NLOPT NONLINEAR OPTI
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Le Digabel S, 2011, ACM T MATH SOFTWARE, V37, DOI 10.1145/1916461.1916468
   Mockus J., 1989, BAYESIAN APPROACH GL
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Parr JM, 2012, ENG OPTIMIZ, V44, P1147, DOI 10.1080/0305215X.2011.637556
   Picheny V., 2016, DICEOPTIM KRIGING BA
   Picheny V., 2014, P 17 INT C ART INT S, P787
   Picheny V, 2016, TECHNOMETRICS, V58, P17, DOI 10.1080/00401706.2015.1079246
   R Development Core Team, 2004, R LANG ENV STAT COMP
   Sasena M, 2002, THESIS
   Schonlau M., 1998, NEW DEV APPL EXPT DE, V34, P11, DOI DOI 10.1214/LNMS/1215456182
   Snoek Jasper, 2012, NEURAL INFORM PROCES
NR 23
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703075
DA 2019-06-15
ER

PT S
AU Richardson, E
   Herskovitz, R
   Ginsburg, B
   Zibulevsky, M
AF Richardson, Elad
   Herskovitz, Rom
   Ginsburg, Boris
   Zibulevsky, Michael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI SEBOOST - Boosting Stochastic Learning Using Subspace Optimization
   Techniques
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method, and has been adapted for the stochastic learning. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating significant improvement in performance. Video presentation is given in [15]
C1 [Richardson, Elad; Herskovitz, Rom; Zibulevsky, Michael] Technion Israel Inst Technol, Haifa, Israel.
   [Ginsburg, Boris] Nvidia INC, Santa Clara, CA USA.
RP Richardson, E (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM eladrich@cs.technion.ac.il; fornoch@gmail.com; boris.ginsburg@gmail.com;
   mzib@cs.technion.ac.il
FU European Research Council under European Unions Seventh Framework
   Program, ERC [320649]; Intel Collaborative Research Institute for
   Computational Intelligence (ICRI-CI)
FX The research leading to these results has received funding from the
   European Research Council under European Unions Seventh Framework
   Program, ERC Grant agreement no. 320649 and was supported by the Intel
   Collaborative Research Institute for Computational Intelligence
   (ICRI-CI).
CR Collobert R, 2011, BIGLEARN NIPS WORKSH
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elad M, 2007, APPL COMPUT HARMON A, V23, P346, DOI 10.1016/j.acha.2007.02.002
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He  K., 2015, ARXIV151203385
   Hestenes M. R., 1952, METHODS CONJUGATE GR, V49
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Narkiss G., 2005, SEQUENTIAL SUBSPACE
   Nemirovski Arkadi, 1982, IZVESTIA AN SSSR TRA, V2, P937
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
   Zibulevsky M, 2010, IEEE SIGNAL PROC MAG, V27, P76, DOI 10.1109/MSP.2010.936023
   Zibulevsky Michael, 2013, ARXIV14010159
   Zibulevsky Michael, SESOP SEQUENTIAL SUB
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700073
DA 2019-06-15
ER

PT S
AU Scanagatta, M
   Corani, G
   de Campos, CP
   Zaffalon, M
AF Scanagatta, Mauro
   Corani, Giorgio
   de Campos, Cassio P.
   Zaffalon, Marco
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.
C1 [Scanagatta, Mauro; Corani, Giorgio] USI, SUPSI, IDSIA, Lugano, Switzerland.
   [de Campos, Cassio P.] Queens Univ Belfast, Belfast, Antrim, North Ireland.
   [Zaffalon, Marco] IDSIA, Lugano, Switzerland.
RP Scanagatta, M (reprint author), USI, SUPSI, IDSIA, Lugano, Switzerland.
EM mauro@idsia.ch; giorgio@idsia.ch; c.decampos@qub.ac.uk;
   zaffalon@idsia.ch
RI Zaffalon, Marco/M-7035-2017
OI Zaffalon, Marco/0000-0001-8908-1502
FU Swiss NSF [200021_146606 / 1, IZKSZ2_162188]
FX Work partially supported by the Swiss NSF grants 200021_146606 / 1 and
   IZKSZ2_162188.
CR Berg J., 2014, AISTATS 14
   Cussens J., 2011, UNCERTAINTY ARTIFICI, P153
   Elidan G., 2009, ADV NEURAL INFORM PR, P417
   Korhonen J.H., 2013, JMLR W CP, V31, P370
   Kwisthout J.H.P., 2010, ECAI 10
   Nie S., 2016, AAAI 16
   Nie S., 2014, ADV NEUR IN, P2285
   Nie SQ, 2015, LECT NOTES ARTIF INT, V9161, P387, DOI 10.1007/978-3-319-20807-7_35
   Parviainen P, 2014, P 17 INT C ART INT S
   Patil H. P., 1986, J COMBINATORICS INFO, V11, P57
   Scanagatta M, 2015, ADV NEURAL INFORM PR, V28, P1855
   Teyssier M, 2012, ABS12071429 CORR
NR 12
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701089
DA 2019-06-15
ER

PT S
AU Sonderby, CK
   Raiko, T
   Maaloe, L
   Sonderby, SK
   Winther, O
AF Sonderby, Casper Kaae
   Raiko, Tapani
   Maaloe, Lars
   Sonderby, Soren Kaae
   Winther, Ole
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Ladder Variational Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.
C1 [Sonderby, Casper Kaae; Sonderby, Soren Kaae; Winther, Ole] Univ Copenhagen, Bioinformat Ctr, Dept Biol, Copenhagen, Denmark.
   [Raiko, Tapani] Aalto Univ, Dept Comp Sci, Espoo, Finland.
   [Maaloe, Lars; Winther, Ole] Tech Univ Denmark, Dept Appl Math & Comp Sci, Lyngby, Denmark.
RP Sonderby, CK (reprint author), Univ Copenhagen, Bioinformat Ctr, Dept Biol, Copenhagen, Denmark.
EM casperkaae@gmail.com; tapani.raiko@aalto.fi; larsma@dtu.dk;
   skaaesonderby@gmail.com; olwi@dtu.dk
OI Winther, Ole/0000-0002-1966-3205
CR Bornschein J., 2015, ARXIV150603877
   Bowman S. R., 2015, ARXIV151106349
   Burda Y., 2015, ARXIV150900519
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Dieleman S, 2015, LASAGNE 1 RELEASE
   Gregor K., 2015, ARXIV150204623
   Ioffe S., 2015, ARXIV150203167
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma D. P, 2014, ADV NEURAL INFORM PR
   Lake B. M., 2013, ADV NEURAL INFORM PR
   LeCun Yann, 2004, COMPUTER VISION PATT
   Maaloe L, 2016, P 33 INT C MACH LEAR
   Mackay D. J. C., 2001, LOCAL MINIMA SYMMETR
   Oord  A.v.d., 2016, ARXIV160106759
   Raiko T., 2014, ADV NEURAL INFORM PR
   Raiko T., 2007, J MACHINE LEARNING R, V8
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Rezende D. J, 2014, ARXIV14014082
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Theano Development Team, 2016, ABS160502688 ARXIV
   Tran Dustin, 2015, ARXIV151106499
   Valpola H., 2015, ARXIV14117783
   van den Broeke G., 2016, THESIS
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702022
DA 2019-06-15
ER

PT S
AU Teymur, O
   Zygalakis, K
   Calderhead, B
AF Teymur, Onur
   Zygalakis, Konstantinos
   Calderhead, Ben
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Probabilistic Linear Multistep Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature [1, 2, 3, 4]. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.
C1 [Teymur, Onur; Calderhead, Ben] Imperial Coll London, Dept Math, London, England.
   [Zygalakis, Konstantinos] Univ Edinburgh, Sch Math, Edinburgh, Midlothian, Scotland.
RP Teymur, O (reprint author), Imperial Coll London, Dept Math, London, England.
EM o@teymur.uk; k.zygalakis@ed.ac.uk; b.calderhead@imperial.ac.uk
FU Simons Foundation; Alan Turing Institute under the EPSRC [EP/N510129/1]
FX KZ was partially supported by a grant from the Simons Foundation and by
   the Alan Turing Institute under the EPSRC grant EP/N510129/1. Part of
   this work was done during the author's stay at the Newton Institute for
   the programme Stochastic Dynamical Systems in Biology: Numerical Methods
   and Applications.
CR Bilotta E, 2008, GALLERY CHUA ATTRACT
   Buckwar E, 2006, SIAM J NUMER ANAL, V44, P779, DOI 10.1137/040602857
   Butcher JC, 2008, NUMERICAL METHODS OR
   CHKREBTII O. A., 2016, BAYESIAN ANAL
   Chua L.O., 2007, SCHOLARPEDIA, V2, P1488
   CHUA LO, 1992, AEU-INT J ELECTRON C, V46, P250
   CONRAD P. R., 2016, STAT COMPUTING
   Diaconis P., 1988, STATISTICAL DECISION, V1, P163
   FORNBERG B, 1988, MATH COMPUT, V51, P699, DOI 10.2307/2008770
   Girolami M, 2008, THEOR COMPUT SCI, V408, P4, DOI 10.1016/j.tcs.2008.07.005
   Hairer E., 2008, SOLVING ORDINARY DIF
   Hennig P., 2014, P 17 INT C ART INT S, V33
   Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142
   Iserles A., 2008, 1 COURSE NUMERICAL A
   Kennedy MC, 2001, J ROY STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schober M., 2014, ADV NEURAL INFORM PR, P739
   SKILLING J, 1993, PHYSICS AND PROBABILITY, P207, DOI 10.1017/CBO9780511524448.020
NR 18
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702103
DA 2019-06-15
ER

PT S
AU Vaswani, N
   Guo, H
AF Vaswani, Namrata
   Guo, Han
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Correlated-PCA: Principal Components' Analysis when Data and Noise are
   Correlated
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as "data-dependent noise". We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.
C1 [Vaswani, Namrata; Guo, Han] Iowa State Univ, Ames, IA 50011 USA.
RP Vaswani, N (reprint author), Iowa State Univ, Ames, IA 50011 USA.
EM namrata@iastate.edu; hanguo@iastate.edu
CR Arora R., 2013, ADV NEURAL INFORM PR, V26, P1815
   Balsubramani A., 2013, ADV NEURAL INFORM PR, V26, P3174
   Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chandrasekaran V., 2011, SIAM J OPTIMIZATION, V21
   DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001
   Fazel M, 2002, THESIS
   Gillberg Jussi, 2016, J MACHINE LEARNING R
   Golub GH, 2000, J COMPUT APPL MATH, V123, P35, DOI 10.1016/S0377-0427(00)00413-1
   Hsu D., 2011, IEEE T INFO TH
   Karnin Z., 2015, P 28 ANN C COMP LEAR, P505
   Lin Z., 2009, TECH REP
   Lois B., 2015, IEEE INT S INF TH IS
   Mitliagkas I., 2013, P ADV NEUR INF PROC, V26, P2886
   Nadler B, 2008, ANN STAT, V36, P2791, DOI 10.1214/08-AOS618
   Netrapalli P., 2014, NEURAL INFO PROC SYS
   Netrapalli P., 2013, S THEOR COMP STOC
   Qiu C., 2010, ALL C COMM CONTR COM
   Qiu CL, 2014, IEEE T INFORM THEORY, V60, P5007, DOI 10.1109/TIT.2014.2331344
   Shamir O., 2014, ARXIV14092848
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Vaswani N., 2017, PCA DATA DEPENDENT N
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Zhan J., 2016, INT C ART INT STAT A
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705012
DA 2019-06-15
ER

PT S
AU Veit, A
   Wilber, M
   Belongie, S
AF Veit, Andreas
   Wilber, Michael
   Belongie, Serge
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Residual Networks Behave Like Ensembles of Relatively Shallow Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ARCHITECTURE
AB In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.
C1 [Veit, Andreas] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
   Cornell Univ, Cornell Tech, Ithaca, NY 14853 USA.
RP Veit, A (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
EM av443@cornell.edu; mjw285@cornell.edu; sjb344@cornell.edu
FU AOL through the Connected Experiences Laboratory; NSF Graduate Research
   Fellowship [NSF DGE-1144153]; Google Focused Research award
FX We would like to thank Sam Kwak and Theofanis Karaletsos for insightful
   feedback. We also thank the reviewers of NIPS 2016 for their very
   constructive and helpful feedback and for suggesting the paper title.
   This work is partly funded by AOL through the Connected Experiences
   Laboratory (Author 1), an NSF Graduate Research Fellowship award (NSF
   DGE-1144153, Author 2), and a Google Focused Research award (Author 3).
CR BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Deng Jia, 2009, C COMP VIS PATT REC
   DRUCKER H, 1994, NEURAL COMPUT, V6, P1289, DOI 10.1162/neco.1994.6.6.1289
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   He K., 2016, ARXIV160305027
   He  K., 2015, ARXIV151203385
   Hinton G. E, 2012, ARXIV12070580
   Hochreiter  Sepp, 1991, THESIS
   Huang G., 2016, ARXIV160309382
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Ioffe S, 2015, INT C MACH LEARN
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Malik  Jitendra, 1990, J OPTICAL SOC AM
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1007/BF00116037
   Serre T, 2007, P NATL ACAD SCI USA, V104, P6424, DOI 10.1073/pnas.0700622104
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava R. K., 2015, ARXIV150500387
   Szegedy C, 2013, ARXIV13126199
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yosinski J., 2014, ADV NEURAL INFORM PR
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
NR 23
TC 1
Z9 1
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704080
DA 2019-06-15
ER

PT S
AU Wang, G
   Giannakis, GB
AF Wang, Gang
   Giannakis, Georgios B.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Solving Random Systems of Quadratic Equations via Truncated Generalized
   Gradient Flow
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PHASE RETRIEVAL; RECOVERY
AB This paper puts forth a novel algorithm, termed truncated generalized gradient flow (TGGF), to solve for x is an element of R-n/C-n a system of m quadratic equations y(i) = vertical bar < a(i), x >vertical bar(2), i = 1, 2, . . ., m, which even for{a(i) is an element of R-n/C-n}(i=1)(m) random is known to be NP-hard in general. We prove that as soon as the number of equations m is on the order of the number of unknowns n, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data {(a(i), y(i))(i=1)(m). Specifically, TGGF proceeds in two stages. s1) A novel orthogonality-promoting initialization that is obtained with simple power iterations, and, s2) a refinement of the initial estimate by successive updates of scalable truncated generalized gradient iterations. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. Empirical results demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts, and, ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms.
C1 [Wang, Gang] Univ Minnesota, ECE Dept, Minneapolis, MN 55455 USA.
   [Wang, Gang] Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.
   [Wang, Gang; Giannakis, Georgios B.] Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
RP Wang, G (reprint author), Univ Minnesota, ECE Dept, Minneapolis, MN 55455 USA.; Wang, G (reprint author), Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.; Wang, G (reprint author), Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
EM gangwang@umn.edu; georgios@umn.edu
RI Wang, Gang/I-9061-2019
OI Wang, Gang/0000-0002-7266-2412
FU NSF [1500713, 1514056]
FX Work in this paper was supported in part by NSF grants 1500713 and
   1514056.
CR Balan R, 2006, APPL COMPUT HARMON A, V20, P345, DOI 10.1016/j.acha.2005.07.001
   Berberidis DK, 2015, INT CONF ACOUST SPEE, P5475, DOI 10.1109/ICASSP.2015.7179018
   Cai T, 2013, J MACH LEARN RES, V14, P1837
   Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen P., 2015, ARXIV151007379V2
   CHEN Y., 2016, COMM PURE APPL MATH
   CLARKE FH, 1975, T AM MATH SOC, V205, P247, DOI 10.2307/1997202
   Clarke FH, 1990, OPTIMIZATION NONSMOO, V5
   Conca A, 2015, APPL COMPUT HARMON A, V38, P346, DOI 10.1016/j.acha.2014.06.005
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   GERCHBERG RW, 1972, OPTIK, V35, P237
   HAUPTMAN HA, 1991, REP PROG PHYS, V54, P1427, DOI 10.1088/0034-4885/54/11/002
   MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948
   Netrapalli P, 2015, IEEE T SIGNAL PROCES, V63, P4814, DOI 10.1109/TSP.2015.2448516
   PARDALOS P. M., 1991, J GLOBAL OPTIM, V1, P15, DOI DOI 10.1007/BF00120662
   SAHINOGLOU H, 1991, IEEE T CIRCUITS SYST, V38, P954, DOI 10.1109/31.85639
   Shor N., 1985, MINIMIZATION METHODS
   Sun J., 2016, ARXIV160206664
   Vershynin  Roman, 2010, ARXIV10113027
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wang G, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P326, DOI 10.1109/GlobalSIP.2014.7032132
   Yeh LH, 2015, OPT EXPRESS, V23, P33214, DOI 10.1364/OE.23.033214
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700025
DA 2019-06-15
ER

PT S
AU Woodworth, B
   Srebro, N
AF Woodworth, Blake
   Srebro, Nathan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tight Complexity Bounds for Optimizing Composite Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.
C1 [Woodworth, Blake; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
RP Woodworth, B (reprint author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
EM blake@ttic.edu; nati@ttic.edu
CR Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2
   Agarwal A., 2014, ARXIV14100723
   Allen-Zhu Z, 2016, ARXIV160305953
   Allen-Zhu Zeyuan, 2016, ARXIV160305642
   Arjevani Y., 2015, NIPS ADV NEURAL INFO, P1747
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lan  Guanghui, 2015, ARXIV150702000
   Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Orabona Francesco, 2012, ARXIV12062372
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz Shai, 2016, STOCHASTIC OPTIMIZAT
   Shamir Ohad, 2013, ARXIV13127853
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   Zhang Yuchen, 2015, ARXIV150100263
NR 20
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700022
DA 2019-06-15
ER

PT S
AU Wu, J
   Frazier, PI
AF Wu, Jian
   Frazier, Peter, I
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Parallel Knowledge Gradient Method for Batch Bayesian Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural networks in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm - the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.
C1 [Wu, Jian; Frazier, Peter, I] Cornell Univ, Ithaca, NY 14853 USA.
RP Wu, J (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM jw926@cornell.edu; pf98@cornell.edu
FU NSF CAREER [CMMI-1254298]; NSF [CMMI-1536895, IIS-1247696]; AFOSR
   [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]
FX The authors were partially supported by NSF CAREER CMMI-1254298, NSF
   CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR
   FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.
CR Bingham D., 2015, OPTIMIZATION TEST PR
   Chevalier Clement, 2013, Learning and Intelligent Optimization. 7th International Conference, LION 7. Revised Selected Papers: LNCS 7997, P59, DOI 10.1007/978-3-642-44973-4_7
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314
   Gardner J.R., 2014, ICML, P937
   Gelbart M. A., 2014, P 30 C UNC ART INT, P250
   Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   LECUYER P, 1990, MANAGE SCI, V36, P1364, DOI 10.1287/mnsc.36.11.1364
   Marmin Sebastien, 2015, Machine Learning, Optimization, and Big Data. First International Workshop, MOD 2015. Revised Selected Papers: LNCS 9432, P37, DOI 10.1007/978-3-319-27926-8_4
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275
   Shah A., 2015, ADV NEURAL INFORM PR, P3312
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Snoek J., 2014, INT C MACH LEARN, P1674
   Snoek Jasper, 2015, SPEARMINT
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Torn A., 1989, GLOBAL OPTIMIZATION, V350
   Wang J., 2015, PARALLEL BAYESIAN GL
   Wang J., 2014, METRICS OPTIMIZATION
   Wang YF, 2015, SIAM J SCI COMPUT, V37, pB361, DOI 10.1137/140971117
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702054
DA 2019-06-15
ER

PT S
AU Xin, B
   Wang, YZ
   Gao, W
   Wang, BY
   Wipf, D
AF Xin, Bo
   Wang, Yizhou
   Gao, Wen
   Wang, Baoyuan
   Wipf, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Maximal Sparsity with Deep Networks?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIGNAL RECONSTRUCTION
AB The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer. Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights. It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available. While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy. In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations. In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal l(0)-norm representations in regimes where existing methods fail. The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene.
C1 [Xin, Bo; Wang, Yizhou; Gao, Wen] Peking Univ, Beijing, Peoples R China.
   [Xin, Bo; Wipf, David] Microsoft Res, Beijing, Peoples R China.
   [Wang, Baoyuan] Microsoft Res, Redmond, WA USA.
RP Xin, B (reprint author), Peking Univ, Beijing, Peoples R China.; Xin, B (reprint author), Microsoft Res, Beijing, Peoples R China.
EM boxin@microsoft.com; yizhou.wang@pku.edu.cn; wgao@pku.edu.cn;
   baoyuanw@microsoft.com; davidwip@microsoft.com
FU MOEMicrosoft Key Laboratory, Peking University;  [973-2015CB351800]; 
   [NSFC-61231010];  [NSFC-61527804];  [NSFC-61421062];  [NSFC-61210005]
FX This work was done while the first author was an intern at Microsoft
   Research, Beijing. It is also funded by 973-2015CB351800, NSFC-61231010,
   NSFC-61527804, NSFC-61421062, NSFC-61210005 and MOEMicrosoft Key
   Laboratory, Peking University.
CR Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Blumensath T., 2010, IEEE J SELECTED TOPI, V4
   Blumensath T., 2009, APPL COMPUTATIONAL H, V27
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Cotter S. F., 2002, IEEE T COMMUNICATION, V50
   Figueiredo M. A. T., 2002, NIPS
   Gregor K., 2010, ICML
   He K., 2016, CVPR
   Hershey J. R., 2014, ARXIV14092574V4
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Ikehata S., 2012, CVPR
   Ioffe S., 2015, ARXIV150203167
   Kamilov U., 2015, ARXIV151204754
   Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882
   Nair V., 2010, ICML
   Pati Y. C., 1993, 27 AS C SIGN SYST CO
   Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779
   Srivastava R. K., 2015, NIPS
   Tibshirani R., 1996, J ROYAL STAT SOC
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Wang Z., 2015, ARXIV150900153V2
   Woodham R. J., 1980, OPTICAL ENG, V19
   Wu L., 2010, P AS C COMP VIS
   Xin Bo, 2016, ARXIV160501636
NR 26
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702093
DA 2019-06-15
ER

PT S
AU Yang, Y
   Tarr, MJ
   Kass, RE
AF Yang, Ying
   Tarr, Michael J.
   Kass, Robert E.
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Estimating Learning Effects: A Short-Time Fourier Transform Regression
   Model for MEG Source Localization
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID FUSIFORM FACE AREA; INVERSE PROBLEM; BRAIN
AB Magnetoencephalography (MEG) has a high temporal resolution well-suited for studying perceptual learning. However, to identify where learning happens in the brain, one needs to apply source localization techniques to project MEG sensor data into brain space. Previous source localization methods, such as the short-time Fourier transform (STFT) method by Gramfort et al. [6] produced intriguing results, but they were not designed to incorporate trial-by-trial learning effects. Here we modify the approach in [6] to produce an STFT-based source localization method (STFT-R) that includes an additional regression of the STFT components on covariates such as the behavioral learning curve. We also exploit a hierarchical L-21 penalty to induce structured sparsity of STFT components and to emphasize signals from regions of interest (ROIs) that are selected according to prior knowledge. In reconstructing the ROI source signals from simulated data, STFT-R achieved smaller errors than a two-step method using the popular minimum-norm estimate (MNE), and in a real-world human learning experiment, STFT-R yielded more interpretable results about what time-frequency components of the ROI signals were correlated with learning.
C1 [Yang, Ying; Tarr, Michael J.; Kass, Robert E.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Yang, Y (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM yingyan1@andrew.cmu.edu; michaeltarr@cmu.edu; kass@stat.cmu.edu
FU NIDA NIH HHS [R90 DA023420]; NIMH NIH HHS [R01 MH064537]
CR Bach F., 2011, ABS11080775 CORR
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Dale AM, 2000, NEURON, V26, P55, DOI 10.1016/S0896-6273(00)81138-1
   Galka A, 2004, NEUROIMAGE, V23, P435, DOI 10.1016/j.neuroimage.2004.02.022
   Gauthier I, 2000, J COGNITIVE NEUROSCI, V12, P495, DOI 10.1162/089892900562165
   Gramfort A, 2013, NEUROIMAGE, V70, P410, DOI 10.1016/j.neuroimage.2012.12.051
   Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027
   HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413
   HAMALAINEN MS, 1994, MED BIOL ENG COMPUT, V32, P35, DOI 10.1007/BF02512476
   Henson RN, 2011, FRONT HUM NEUROSCI, V5, DOI 10.3389/fnhum.2011.00076
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Lamus C, 2012, NEUROIMAGE, V63, P894, DOI 10.1016/j.neuroimage.2011.11.020
   Mattout J, 2006, NEUROIMAGE, V30, P753, DOI 10.1016/j.neuroimage.2005.10.037
   Pascual-Marqui RD, 2002, METHOD FIND EXP CLIN, V24, P5
   Pitcher D, 2011, EXP BRAIN RES, V209, P481, DOI 10.1007/s00221-011-2579-1
   STINE RA, 1985, J AM STAT ASSOC, V80, P1026, DOI 10.2307/2288570
   Tanaka JW, 2006, J COGNITIVE NEUROSCI, V18, P1488, DOI 10.1162/jocn.2006.18.9.1488
   Xu YG, 2013, THESIS
NR 19
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 69
EP 82
DI 10.1007/978-3-319-45174-9_8
PG 14
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400008
PM 30246177
DA 2019-06-15
ER

PT S
AU Yi, XY
   Park, D
   Chen, YD
   Caramanis, C
AF Yi, Xinyang
   Park, Dohyung
   Chen, Yudong
   Caramanis, Constantine
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast Algorithms for Robust PCA via Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MATRIX; INCOHERENCE
AB We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with r denoting rank and d dimension, we reduce the complexity from O(r(2)d(2) log(1/kappa)) to O(rd(2) log(1/epsilon)) - a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than O(r(4)d log dlog(1/epsilon)). Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where r is small compared to d, it also allows for near-linear-in-d run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.
C1 [Yi, Xinyang; Park, Dohyung; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA.
   [Chen, Yudong] Cornell Univ, Ithaca, NY 14853 USA.
RP Yi, XY (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM yixy@utexas.edu; dhpark@utexas.edu; yudong.chen@cornell.edu;
   constantine@utexas.edu
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Balakrishnan S., 2014, ARXIV14082156
   Bhojanapalli S., 2015, P 26 ANN ACM SIAM S, P902
   Bhojanapalli Srinadh, 2015, ARXIV150903917
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chen Y., 2015, ADV NEURAL INFORM PR, V2, P739
   Chen Y., 2015, ARXIV150903025
   Chen YD, 2015, IEEE T INFORM THEORY, V61, P2909, DOI 10.1109/TIT.2015.2415195
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Gu Q., 2016, P 19 INT C ART INT S, P600
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Lin Zhouchen, 2013, ARXIV10095055V3
   Netrapalli P., 2014, P ADV NEUR INF PROC, P1107
   Sun J., 2015, ARXIV151006096
   Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25
   Tu S., 2015, ARXIV150703566
   Wang Zhaoran, 2015, Adv Neural Inf Process Syst, V28, P2512
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Yi X., 2015, ADV NEURAL INFORM PR, P1567
   Zhang Huishuai, 2016, ARXIV160303805
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
   Zheng Q., 2015, ADV NEURAL INFORM PR, P109
NR 30
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703081
DA 2019-06-15
ER

PT S
AU Borgs, C
   Chayes, JT
   Smith, A
AF Borgs, Christian
   Chayes, Jennifer T.
   Smith, Adam
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Private Graphon Estimation for Sparse Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID STOCHASTIC BLOCKMODELS; CONVERGENT SEQUENCES
AB We design algorithms for fitting a high-dimensional statistical model to a large, sparse network without revealing sensitive information of individual members. Given a sparse input graph G, our algorithms output a node-differentially private nonparametric block model approximation. By node-differentially private, we mean that our output hides the insertion or removal of a vertex and all its adjacent edges. If G is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon W, our model guarantees consistency: as the number of vertices tends to infinity, the output of our algorithm converges to W in an appropriate version of the L2 norm. In particular, this means we can estimate the sizes of all multi-way cuts in G.
   Our results hold as long as W is bounded, the average degree of G grows at least like the log of the number of vertices, and the number of blocks goes to infinity at an appropriate rate. We give explicit error bounds in terms of the parameters of the model; in several settings, our bounds improve on or match known nonprivate results.
C1 [Borgs, Christian; Chayes, Jennifer T.] Microsoft Res New England, Cambridge, MA 02142 USA.
   [Smith, Adam] Penn State Univ, University Pk, PA 16802 USA.
RP Borgs, C (reprint author), Microsoft Res New England, Cambridge, MA 02142 USA.
EM cborgs@microsoft.com; jchayes@microsoft.com; asmith@psu.edu
FU NSF [IIS-1447700]; Google Faculty Award
FX A.S. was supported by NSF award IIS-1447700 and a Google Faculty Award.
   Part of this work was done while visiting Boston University's Hariri
   Institute for Computation and Harvard University's Center for Research
   on Computation and Society.
CR Abbe E., 2015, RECOVERING COM UNPUB
   Abbe E, 2014, ARXIV14053267
   Abbe Emmanuel, 2015, ARXIV150300609
   Bickel PJ, 2011, ANN STAT, V39, P2280, DOI 10.1214/11-AOS904
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Blocki J., 2013, P 4 C INN THEOR COMP, P87
   Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168
   Borgs C, 2008, ADV MATH, V219, P1801, DOI 10.1016/j.aim.2008.07.008
   Borgs C, 2012, ANN MATH, V176, P151, DOI 10.4007/annals.2012.176.1.2
   Borgs C., 2014, ARXIV14080744
   Borgs C., 2014, ARXIV14012906
   Borgs C., 2006, ALGORITHMS COMBIN, V26, P315, DOI [DOI 10.1007/3-540-33700-8, 10.1007/3-540-33700-8_18]
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Chen S., 2013, SIGMOD, P653, DOI DOI 10.1145/2463676.2465304
   Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053
   Diaconis P, 2008, REND MAT APPL, V28, P33
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Gao C., 2014, ARXIV14105837
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26
   Klopp O., 2015, ARXIV150704118
   Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002
   Lu W., 2014, P 20 ACM SIGKDD INT, P921
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Raskhodnikova S., 2015, ARXIV150407912
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   WOLFE P. J., 2013, ARXIV13095936
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101087
DA 2019-06-15
ER

PT S
AU Chaturapruek, S
   Duchi, JC
   Re, C
AF Chaturapruek, Sorathan
   Duchi, John C.
   Re, Chris
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Asynchronous stochastic convex optimization: the noise is in the noise
   and SGD don't care
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID APPROXIMATION
AB We show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures. Roughly, the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony. We also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods. In short, we show that for many stochastic approximation problems, as FreddieMercury sings in Queen's Bohemian Rhapsody, "Nothing really matters."
C1 [Chaturapruek, Sorathan; Re, Chris] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Duchi, John C.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Duchi, John C.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Chaturapruek, S (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM sorathan@stanford.edu; jduchi@stanford.edu; chrismre@stanford.edu
CR Agarwal A., 2011, ADV NEURAL INFORM PR, V24
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Duchi J. C., 2013, ADV NEURAL INFORM PR, V26
   Duchi J. C., 2015, ASYNCHRONOUS STOCHAS
   Duchi J. C., 2015, ARXIV150800882MATHOC
   Ermoliev Y. M., 1969, KIBERNETIKA, V2, P72
   Juditsky A., 2011, STOCHASTIC SYSTEMS, V1, P17, DOI DOI 10.1214/10-SSY011
   Le Cam LM, 2000, ASYMPTOTICS STAT SOM
   Lehmann E. L., 1998, THEORY POINT ESTIMAT
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu J, 2014, P 31 INT C MACH LEAR
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Niu Feng, 2011, ADV NEURAL INFORM PR, V24
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Recht B., 2012, P 25 ANN C COMP LEAR
   Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Robbins H., 1971, OPTIMIZING METHODS S, P233
   van der Vaart AW, 1998, CAMBRIDGE SERIES STA
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103069
DA 2019-06-15
ER

PT S
AU Chen, PH
   Chen, J
   Yeshurun, Y
   Hasson, U
   Haxby, JV
   Ramadge, PJ
AF Chen, Po-Hsuan
   Chen, Janice
   Yeshurun, Yaara
   Hasson, Uri
   Haxby, James V.
   Ramadge, Peter J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Reduced-Dimension fMRI Shared Response Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID HUMAN BRAIN; SYSTEM
AB Multi-subject fMRI data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity. We develop a shared response model for aggregating multi-subject fMRI data that accounts for different functional topographies among anatomically aligned datasets. Our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest. Furthermore, by removing the identified shared response, it allows improved detection of group differences. The ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fMRI studies.
C1 [Chen, Po-Hsuan; Ramadge, Peter J.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
   [Chen, Janice; Yeshurun, Yaara; Hasson, Uri] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Chen, Janice; Yeshurun, Yaara; Hasson, Uri] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA.
   [Haxby, James V.] Dartmouth Coll, Dept Psychol & Brain Sci, Hanover, NH 03755 USA.
   [Haxby, James V.] Dartmouth Coll, Ctr Cognit Neurosci, Hanover, NH 03755 USA.
RP Chen, PH (reprint author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
CR Ahn JH, 2003, NEURAL COMPUT, V15, P57, DOI 10.1162/089976603321043694
   Ames D. L., 2014, J COGNITIVE NEUROSCI
   Brett M, 2002, NAT REV NEUROSCI, V3, P243, DOI 10.1038/nrn756
   Chen J., 2014, ABSTRACTS COGNITIVE
   Conroy B, 2009, ADV NEURAL INFORM PR
   Conroy B. R., 2013, NEUROIMAGE
   Debettencourt MT, 2015, NAT NEUROSCI, V18, P470, DOI 10.1038/nn.3940
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Fischl B, 1999, HUM BRAIN MAPP, V8, P272, DOI 10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4
   Griffiths TD, 2002, TRENDS NEUROSCI, V25, P348, DOI 10.1016/S0166-2236(02)02191-4
   Hanke M, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.3
   Haxby JV, 2011, NEURON, V72, P404, DOI 10.1016/j.neuron.2011.08.026
   Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736
   Horn R. A., 2012, MATRIX ANAL
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Huth A. G., 2015, ARXIV150403622
   Hyvarinen A., 2004, INDEPENDENT COMPONEN
   Lorbert A., 2012, ADV NEURAL INFORM P
   Manning JR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094914
   Margulies DS, 2009, P NATL ACAD SCI USA, V106, P20069, DOI 10.1073/pnas.0905314106
   Mazziotta J, 2001, PHILOS T R SOC B, V356, P1293, DOI 10.1098/rstb.2001.0915
   Michael AM, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00106
   Raichle M. E., 2015, ANN REV NEUROSCIENCE, V38
   Sabuncu MR, 2010, CEREB CORTEX, V20, P130, DOI 10.1093/cercor/bhp085
   Talairach J, 1988, COPLANAR STEREOTAXIC
   TOOTELL RBH, 1995, NATURE, V375, P139, DOI 10.1038/375139a0
   WATSON JDG, 1993, CEREB CORTEX, V3, P79, DOI 10.1093/cercor/3.2.79
   Xu H, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P229, DOI 10.1109/SSP.2012.6319668
   Yeshurun Y., 2014, SOC NEUR ABSTR
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102004
DA 2019-06-15
ER

PT S
AU Chorowski, J
   Bahdanau, D
   Serdyuk, D
   Cho, K
   Bengio, Y
AF Chorowski, Jan
   Bahdanau, Dzmitry
   Serdyuk, Dmitriy
   Cho, Kyunghyun
   Bengio, Yoshua
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Attention-Based Models for Speech Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.
C1 [Chorowski, Jan] Univ Wroclaw, Wroclaw, Poland.
   [Bahdanau, Dzmitry] Jacobs Univ Bremen, Bremen, Germany.
   [Serdyuk, Dmitriy; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
RP Chorowski, J (reprint author), Univ Wroclaw, Wroclaw, Poland.
EM jan.chorowski@ii.uni.wroc.pl
FU National Science Center (Poland) [Sonata 8 2014/15/D/ST6/04402]; CIFAR;
   NSERC; Calcul Quebec; Compute Canada; Canada Research Chairs
FX The authors would like to acknowledge the support of the following
   agencies for research funding and computing support: National Science
   Center (Poland) grant Sonata 8 2014/15/D/ST6/04402, NSERC, Calcul
   Quebec, Compute Canada, the Canada Research Chairs and CIFAR. D.
   Bahdanau also thanks Planet Intelligent Systems GmbH and Yandex.
CR Bahdanau D., 2015, ARXIV150600619CSSTAT
   Bahdanau Dzmitry, 2015, P 3 ICLR
   Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bergstra J, 2010, P SCIPY
   Cho  K., 2014, EMNLP
   Chopra S., 2014, ARXIV14103916
   Chorowski J., 2014, ARXIV14121602CSSTAT
   Gales M, 2007, FOUND TRENDS SIGNAL, V1, P195, DOI 10.1561/2000000004
   Garofolo J. S., 1993, DARPA TIMIT ACOUSTIC
   Goodfellow I. J., 2013, ARXIV13084214
   Graves A, 2013, ARXIV13080850
   Graves A., 2014, P 31 ICML
   Graves A., 2006, P 23 ICML 06
   Graves A., 2012, P 29 ICML
   Graves A., 2011, P 24 NIPS
   Graves A, 2014, ARXIV14105401
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gulcehre C., 2015, ARXIV150303535
   Hannun A., 2014, ARXIV14125567
   Hinton G. E, 2012, ARXIV12070580
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Lecun Y., 1998, P IEEE
   Mnih V., 2014, P 27 NIPS
   Povey D., 2011, P ASRU
   Sukhbaatar  Sainbayar, 2015, ARXIV150308895
   Sutskever I., 2014, P 27 NIPS
   Toth L., 2014, P ICASSP
   Xu K., 2015, P 32 ICML
   Zeiler M.D., 2012, ARXIV12125701
NR 30
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101106
DA 2019-06-15
ER

PT S
AU Chung, J
   Kastner, K
   Dinh, L
   Goel, K
   Courville, A
   Bengio, Y
AF Chung, Junyoung
   Kastner, Kyle
   Dinh, Laurent
   Goel, Kratarth
   Courville, Aaron
   Bengio, Yoshua
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Recurrent Latent Variable Model for Sequential Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) 1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.
C1 [Chung, Junyoung; Kastner, Kyle; Dinh, Laurent; Goel, Kratarth; Courville, Aaron; Bengio, Yoshua] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada.
RP Chung, J (reprint author), Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada.
EM junyoung.chung@umontreal.ca; kyle.kastner@umontreal.ca;
   laurent.dinh@umontreal.ca; kratarth.goel@umontreal.ca;
   aaron.courville@umontreal.ca; yoshua.bengio@umontreal.ca
FU CIFAR; Ubisoft; NSERC; Calcul Quebec; Canada Research Chairs; Compute
   Canada
FX The authors would like to thank the developers of Theano [1]. Also, the
   authors thank Kyunghyun Cho, Kelvin Xu and Sungjin Ahn for insightful
   comments and discussion. We acknowledge the support of the following
   agencies for research funding and computing support: Ubisoft, NSERC,
   Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bayer Justin, 2014, ARXIV14117610
   Bertrand A, 2008, INT CONF ACOUST SPEE, P4713, DOI 10.1109/ICASSP.2008.4518709
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Fabius Otto, 2014, ARXIV14126581
   Graves A, 2013, ARXIV13080850
   Gregor Karol, 2015, P 32 INT C MACH LEAR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   King S., 2013, 9 ANN BLIZZARD CHALL
   Kingma D., 2015, P INT C LEARN REPR I
   Kingma D. P., 2014, P INT C LEARN REPR I
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lewandowski N. B., 2012, P 29 INT C MACH LEAR, P1159
   Liwicki M, 2005, EIGHTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS 1 AND 2, PROCEEDINGS, P956, DOI 10.1109/ICDAR.2005.132
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pachitariu M., 2012, ADV NEURAL INFORM PR, V25, P1322
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Tokuda K, 2013, P IEEE, V101, P1234, DOI 10.1109/JPROC.2013.2251852
NR 18
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100021
DA 2019-06-15
ER

PT S
AU Dai, AM
   Le, QV
AF Dai, Andrew M.
   Le, Quoc V.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Semi-supervised Sequence Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.
C1 [Dai, Andrew M.; Le, Quoc V.] Google Inc, Mountain View, CA 94043 USA.
RP Dai, AM (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM adai@google.com; qvl@google.com
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Bengio Y., 2003, JMLR
   Cardoso-Cachopo A., 2015, DATASETS SINGLE LABE
   Chan W., 2015, ARXIV150801211
   Dauphin Y., 2013, NIPS
   Gers Felix A, 2000, NEURAL COMPUTATION
   Graves A., 2013, ARXIV
   Greff K., 2015, ICML
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hochreiter S., 2001, FIELD GUIDE DYNAMICA
   Jean S., 2014, ICML
   Johnson R., 2014, NAACL
   Kim Y, 2014, CONVOLUTIONAL NEURAL
   Kiros Ryan, 2015, NIPS
   Krizhevsky A., 2010, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   Lang K., 1995, ICML
   Larochelle H., 2012, JMLR
   Le Q. V., 2014, ICML
   Lehmann J., 2014, SEMANTIC WEB
   Luong T., 2014, ARXIV14108206
   Maas A. L., 2011, ACL
   McAuley J., 2013, P 7 ACM C REC SYST, V13, P165, DOI DOI 10.1145/2507157.2507163
   Mikolov Tomas, 2010, INTERSPEECH
   Ng J. Y. H, 2015, CVPR
   Pang  Bo, 2005, ACL
   Rumelhart D. E., 1986, NATURE
   Shang L., 2015, EMNLP
   Socher R., 2013, EMNLP
   Socher R., 2012, EMNLP
   Srivastava N, 2015, ICML
   Sutskever  I., 2014, NIPS
   Vinyals O., 2015, NIPS
   Vinyals O., 2014, CVPR
   Vinyals Oriol, 2015, ICML DEEP LEARN WORK
   Wang Sida, 2012, ACL
   Werbos PJ, 1974, THESIS
   Zaremba W, 2014, ARXIV14092329
   Zhang Xiang, 2015, NIPS
NR 39
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102097
DA 2019-06-15
ER

PT S
AU Dwork, C
   Feldman, V
   Hardt, M
   Pitassi, T
   Reingold, O
   Roth, A
AF Dwork, Cynthia
   Feldman, Vitaly
   Hardt, Moritz
   Pitassi, Toniann
   Reingold, Omer
   Roth, Aaron
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Generalization in Adaptive Data Analysis and Holdout Reuse
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID STABILITY
AB Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [7], where we focused on the problem of estimating expectations of adaptively chosen functions.
   In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment.
   We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [7] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.
C1 [Dwork, Cynthia] Microsoft Res, New York, NY 10011 USA.
   [Feldman, Vitaly] IBM Almaden Res Ctr, San Jose, CA USA.
   [Hardt, Moritz] Google Res, Mountain View, CA USA.
   [Pitassi, Toniann] Univ Toronto, Toronto, ON, Canada.
   [Reingold, Omer] Samsung Res Amer, Mountain View, CA USA.
   [Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA.
RP Dwork, C (reprint author), Microsoft Res, New York, NY 10011 USA.
CR Bassily Raef, 2015, CORR
   Blum Avrim, 2015, CORR
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cawley GC, 2010, J MACH LEARN RES, V11, P2079
   Do CB, 2007, ADV NEURAL INFORM PR, V20, P377
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork Cynthia, 2014, CORR
   Dwork Cynthia, 2015, CORR
   FREEDMAN DA, 1983, AM STAT, V37, P152, DOI 10.2307/2685877
   Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55
   Hastie T., 2009, SPRINGER SERIES STAT
   Langford John, 2005, CLEVER METHODS OVERF
   Nissim Kobbi, 2015, CORR
   Poggio T, 2004, NATURE, V428, P419, DOI 10.1038/nature02341
   Rao R.B., 2008, P 2008 SIAM INT C DA, P588, DOI DOI 10.1137/1.9781611972788.54
   Reunanen J., 2003, Journal of Machine Learning Research, V3, P1371, DOI 10.1162/153244303322753715
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Steinke Thomas, 2014, ARXIV14101228
   Taylor J, 2015, P NATL ACAD SCI USA, V112, P7629, DOI 10.1073/pnas.1507583112
   Wang Yu-Xiang, 2015, CORR
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103031
DA 2019-06-15
ER

PT S
AU Farajtabar, M
   Wang, YC
   Gomez-Rodriguez, M
   Li, S
   Zha, HY
   Song, L
AF Farajtabar, Mehrdad
   Wang, Yichen
   Gomez-Rodriguez, Manuel
   Li, Shuang
   Zha, Hongyuan
   Song, Le
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI COEVOLVE: A Joint Point Process Model for Information Diffusion and
   Network Co-evolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.
C1 [Farajtabar, Mehrdad; Wang, Yichen; Li, Shuang; Zha, Hongyuan; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Gomez-Rodriguez, Manuel] MPI Software Syst, Kaiserslautern, Germany.
RP Farajtabar, M (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM mehrdad@gatech.edu; yichen.wang@gatech.edu; manuelgr@mpi-sws.org;
   sli370@gatech.edu; zha@cc.gatech.edu; lsong@cc.gatech.edu
FU NSF/NIH [BIGDATA 1R01GM108341]; ONR [N00014-15-1-2340]; NSF
   [IIS-1218749, CAREER IIS-1350983]
FX The authors would like to thank Demetris Antoniades and Constantine
   Dovrolis for providing them with the dataset. The research was supported
   in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF
   IIS-1218749, NSF CAREER IIS-1350983.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Antoniades D., 2013, ARXIV13096001
   Backstrom L., 2012, WEBSCI
   Blundell C., 2012, NIPS
   Chakrabarti D., 2004, R MAT RECURSIVE MODE, P541
   Cheng Justin, 2014, WWW
   Du  N., 2013, NIPS
   Erdos P., 1960, PUBL MATH I HUNG, V5, P17, DOI DOI 10.2307/1999405
   Farajtabar M., 2014, NIPS
   Goel S., 2012, EC
   Gomez-Rodriguez M., 2010, KDD
   Gomez-Rodriguez  M., 2011, ICML
   GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469
   Gross T., 2008, ADAPTIVE COEVOLUTION
   Hunter  D., 2011, ICML
   Iwata  T., 2013, KDD
   Kempe D., 2003, KDD
   Kwak H, 2010, WWW
   Leskovec J., 2005, KDD
   Leskovec J., 2010, JMLR
   Leskovec Jure, 2008, KDD
   Linderman S., 2014, ICML
   Liniger T. J., 2009, THESIS
   Myers S. A., 2014, WWW
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Romero D. M., 2010, ICWSM
   Singer Philipp, 2012, Modeling and Mining Ubiquitous Social Media. International Workshops MSM 2011 and MUSE 2011. Revised Selected Papers, P40, DOI 10.1007/978-3-642-33684-3_3
   Strogatz SH, 1998, NATURE
   Ugander J., 2013, WWW
   Valera I., 2015, ICDM
   Vu D. Q., 2011, NIPS
   Weng L., 2013, KDD
   Zhou K., 2013, ICML
   Zhou K., 2013, AISTATS
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101014
DA 2019-06-15
ER

PT S
AU Gabrie, M
   Tramel, EW
   Krzakala, F
AF Gabrie, Marylou
   Tramel, Eric W.
   Krzakala, Florent
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer
   Free Energy
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LEARNING ALGORITHM
AB Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.
C1 [Gabrie, Marylou] Ecole Normale Super, CNRS, Lab Phys Stat, UMR 8550, F-75005 Paris, France.
   Univ Paris 06, F-75005 Paris, France.
RP Gabrie, M (reprint author), Ecole Normale Super, CNRS, Lab Phys Stat, UMR 8550, F-75005 Paris, France.
EM marylou.gabrie@lps.ens.fr; eric.tramel@lps.ens.fr;
   florent.krzakala@ens.fr
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU European Research Council under the European Union's 7th Framework
   Programme (FP/2007-2013/ERC Grant) [307087-SPARCS]
FX We would like to thank F. Caltagirone and A. Decelle for many insightful
   discussions. This research was funded by European Research Council under
   the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant
   Agreement 307087-SPARCS).
CR Bolthausen E, 2014, COMMUN MATH PHYS, V325, P333, DOI 10.1007/s00220-013-1862-3
   Coates A, 2011, P 14 INT C ART INT S, P215
   Cocco S, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.090601
   Cocco S, 2009, P NATL ACAD SCI USA, V106, P14058, DOI 10.1073/pnas.0906705106
   GALLAND CC, 1993, NETWORK-COMP NEURAL, V4, P355, DOI 10.1088/0954-898X/4/3/007
   Georges A., 1999, J PHYS A, V24, P2173
   Goodfellow I. J., 2013, 13013568 ARXIV
   Hinton G., 2010, MOMENTUM, V9, P1, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G.E., 2009, ADV NEURAL INFORM PR, V180, P1607
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 1989, NEURAL COMPUT, V1, P143, DOI 10.1162/neco.1989.1.1.143
   Kappen HJ, 1998, ADV NEUR IN, V10, P280
   Larochelle Hugo, 2008, P 25 INT C MACH LEAR, P536, DOI DOI 10.1145/1390156.1390224
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Marlin B.M., 2010, P 13 INT C ART INT S, P509
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Opper  M., 2001, ADV MEAN FIELD METHO
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peterson C., 1987, Complex Systems, V1, P995
   PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Smolensky P., 1986, PROCESSING PARALLEL, V1
   THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Welling M, 2002, LECT NOTES COMPUT SC, V2415, P351
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101048
DA 2019-06-15
ER

PT S
AU Ha, W
   Barber, RF
AF Ha, Wooseok
   Barber, Rina Foygel
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Robust PCA with compressed data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB The robust principal component analysis (RPCA) problem seeks to separate low-rank trends from sparse outliers within a data matrix, that is, to approximate a n x d matrix D as the sum of a low-rank matrix L and a sparse matrix S. We examine the robust principal component analysis (RPCA) problem under data compression, where the data Y is approximately given by (L+S).C, that is, a low-rank + sparse data matrix that has been compressed to size n x m (with m substantially smaller than the original dimension d) via multiplication with a compression matrix C. We give a convex program for recovering the sparse component S along with the compressed low-rank component L. C, along with upper bounds on the error of this reconstruction that scales naturally with the compression dimension m and coincides with existing results for the uncompressed setting m = d. Our results can also handle error introduced through additive noise or through missing data. The scaling of dimension, compression, and signal complexity in our theoretical results is verified empirically through simulations, and we also apply our method to a data set measuring chlorine concentration across a network of sensors to test its performance in practice.
C1 [Ha, Wooseok; Barber, Rina Foygel] Univ Chicago, Chicago, IL 60637 USA.
RP Ha, W (reprint author), Univ Chicago, Chicago, IL 60637 USA.
EM haywse@uchicago.edu; rina@uchicago.edu
CR Agarwal A, 2012, ANN STAT, V40, P1171, DOI 10.1214/12-AOS1000
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Foygel R., 2011, ADV NEURAL INFORM PR, P2133
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   He J., 2011, ARXIV11093827
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Maillard O., 2009, ADV NEURAL INFORM PR, V22, P1213
   Netrapalli P., 2014, P ADV NEUR INF PROC, P1107
   Wright J., 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1002/CPA.20132
   Xu H., 2010, ADV NEURAL INFORM PR, P2496
   Zhou SH, 2009, IEEE T INFORM THEORY, V55, P846, DOI 10.1109/TIT.2008.2009605
   Zhou T., 2011, INT C MACH LEARN ICM, P33
NR 13
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100073
DA 2019-06-15
ER

PT S
AU Hartline, J
   Syrgkanis, V
   Tardos, E
AF Hartline, Jason
   Syrgkanis, Vasilis
   Tardos, Eva
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI No-Regret Learning in Bayesian Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.
C1 [Hartline, Jason] Northwestern Univ, Evanston, IL 60208 USA.
   [Syrgkanis, Vasilis] Microsoft Res, New York, NY USA.
   [Tardos, Eva] Cornell Univ, Ithaca, NY USA.
RP Hartline, J (reprint author), Northwestern Univ, Evanston, IL 60208 USA.
EM hartline@northwestern.edu; vasy@microsoft.com; eva@cs.cornell.edu
CR Bergemann Dirk, 2011, COWLES FDN DISCUSSIO
   Blum A, 2008, ACM S THEORY COMPUT, P373
   Cai Yang, 2014, P 15 ACM C EC COMP, P895
   Caragiannis Ioannis, 2014, J EC THEORY
   de Keijzer B, 2013, LECT NOTES COMPUT SC, V8125, P385, DOI 10.1007/978-3-642-40450-4_33
   FORGES F, 1993, THEOR DECIS, V35, P277, DOI 10.1007/BF01075202
   Foster DP, 1998, BIOMETRIKA, V85, P379, DOI 10.1093/biomet/85.2.379
   Kaplan TR, 2012, ECON THEOR, V50, P269, DOI 10.1007/s00199-010-0563-9
   Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404
   Lucier B, 2010, PROC APPL MATH, V135, P537
   Roughgarden T, 2009, ACM S THEORY COMPUT, P513
   Syrgkanis V., 2013, P 45 ANN ACM S THEOR, P211, DOI DOI 10.1145/2488608.2488635
   Vetta A, 2002, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2002.1181966
NR 13
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103054
DA 2019-06-15
ER

PT S
AU Hensman, J
   Matthews, AGD
   Filippone, M
   Ghahramani, Z
AF Hensman, James
   Matthews, Alexander G. de G.
   Filippone, Maurizio
   Ghahramani, Zoubin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI MCMC for Variationally Sparse Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID CLASSIFICATION
AB Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper is available at github.com/sparseMCMC.
C1 [Hensman, James] Univ Lancaster, CHICAS, Lancaster, England.
   [Matthews, Alexander G. de G.; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England.
   [Filippone, Maurizio] EURECOM, Biot, France.
RP Hensman, J (reprint author), Univ Lancaster, CHICAS, Lancaster, England.
EM james.hensman@lancaster.ac.uk; am554@cam.ac.uk;
   maurizio.filippone@eurecom.fr; zoubin@cam.ac.uk
FU MRC fellowship; EPSRC [EP/I036575/1]; Google Focussed Research award
FX JH was funded by an MRC fellowship, AM and ZG by EPSRC grant
   EP/I036575/1 and a Google Focussed Research award.
CR Chai KMA, 2012, J MACH LEARN RES, V13, P1745
   Christensen OF, 2005, J R STAT SOC B, V67, P253, DOI 10.1111/j.1467-9868.2005.00500.x
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Filippone M, 2013, MACH LEARN, V93, P93, DOI 10.1007/s10994-013-5388-x
   Filippone M., 2015, ICML 2015
   Gal Y., 2014, NIPS
   Gibbs MN, 2000, IEEE T NEURAL NETWOR, V11, P1458, DOI 10.1109/72.883477
   Girolami M., 2006, NEURAL COMPUT, V18, P2005
   Hensman J., 2014, AISTATS, P351
   Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, V24, P280
   Khan E., 2012, ADV NEURAL INFORM PR, P3140
   Kim HC, 2006, IEEE T PATTERN ANAL, V28, P1948, DOI 10.1109/TPAMI.2006.238
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Lloyd C., 2015, ICML 2015
   Matthews A. G. D., 2015, 150407027 ARXIV
   Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115
   Murray I., 2010, AISTATS, V9
   Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732
   Nguyen T. V., 2014, ADV NEURAL INFORM PR, P1404
   Nickisch H, 2008, J MACH LEARN RES, V9, P2035
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488
   Sarkka S, 2013, BAYESIAN FILTERING S, V3
   Smith S. P., 1995, J COMPUT GRAPH STAT, V4, P134
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Solin A., 2014, 14015508 ARXIV
   Titsias M, 2009, ARTIF INTELL, P567
   Titsias Michalis K, 2011, BAYESIAN TIME SERIES
   Vanhatalo J., 2007, J MACHINE LEARNING R, P73
   Wang Z., 2013, ICML, P1462
   Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807
   Wilson A. G., 2014, ADV NEURAL INF PROCE, P3626
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102024
DA 2019-06-15
ER

PT S
AU Hofmann, T
   Lucchi, A
   Lacoste-Julien, S
   McWilliams, B
AF Hofmann, Thomas
   Lucchi, Aurelien
   Lacoste-Julien, Simon
   McWilliams, Brian
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Variance Reduced Stochastic Gradient Descent with Neighbors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its slow convergence can be a computational bottleneck. Variance reduction techniques such as SAG, SVRG and SAGA have been proposed to overcome this weakness, achieving linear convergence. However, these methods are either based on computations of full gradients at pivot points, or on keeping per data point corrections in memory. Therefore speed-ups relative to SGD may need a minimal number of epochs in order to materialize. This paper investigates algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points, which offers advantages in the transient optimization phase. As a side-product we provide a unified convergence analysis for a family of variance reduction algorithms, which we call memorization algorithms. We provide experimental results supporting our theory.
C1 [Hofmann, Thomas; Lucchi, Aurelien; McWilliams, Brian] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Lacoste-Julien, Simon] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.
RP Hofmann, T (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Dasgupta S, 2015, ALGORITHMICA, V72, P237, DOI 10.1007/s00453-014-9885-5
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Konecny J., 2013, ARXIV13121666
   Mark Schmidt, 2014, CONVERGENCE RATE STO
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt M., 2013, ARXIV13092388
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
NR 11
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102067
DA 2019-06-15
ER

PT S
AU Joulin, A
   Mikolov, T
AF Joulin, Armand
   Mikolov, Tomas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NETWORKS
AB Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.
C1 [Joulin, Armand; Mikolov, Tomas] Facebook AI Res, 770 Broadway, New York, NY 10003 USA.
RP Joulin, A (reprint author), Facebook AI Res, 770 Broadway, New York, NY 10003 USA.
EM ajoulin@fb.com; tmikolov@fb.com
CR Bengio Yoshua, 2007, LARGE SCALE KERNEL M
   Bishop C. M., 2006, PATTERN RECOGNITION
   Boden M., 2000, CONNECTION SCI
   Bottou L., 2010, COMPSTAT
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bridle J. S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P227
   Christiansen MH, 1999, COGNITIVE SCI, V23, P157, DOI 10.1016/S0364-0213(99)00003-8
   Chung  J., 2015, GATED FEEDBACK RECUR
   Ciresan D. C., 2011, HIGH PERFORMANCE NEU
   Crocker M. W., 1996, MECH SENTENCE PROCES
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Das S., 1992, ACCSS
   Das S., 1993, NIPS
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Fanty M., 1994, PARALLEL NATURAL LAN
   Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769
   Graves A, 2014, NEURAL TURING MACHIN
   Grunwald P., 1996, ACCSS
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Holldobler S., 1997, ADV ARTIFICIAL INTEL
   Krizhevsky A., 2012, NIPS
   LeCun Y., 1998, GRADIENT BASED LEARN
   Mikolov T., 2014, LEARNING LONGER MEMO
   Mikolov T., 2012, THESIS
   Minsky  Marvin, 1969, PERCEPTRONS
   Mozer M. C., 1993, NIPS
   POLLACK JB, 1991, MACH LEARN, V7, P227, DOI 10.1007/BF00114845
   Recht B., 2011, NIPS
   Rodriguez P., 1999, CONNECTION SCI
   Rumelhart David E, 1985, TECHNICAL REPORT
   Tabor W., 2000, EXPERT SYSTEMS
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Weston Jason, 2015, ICLR
   Wiles J., 1995, ACCSS
   Williams R. J., 1995, BACK PROPAGATION THE, P433
   Zaremba W., 2014, LEARNING TO EXECUTE
   ZENG Z, 1994, IEEE T NEURAL NETWOR, V5, P320, DOI 10.1109/72.279194
NR 37
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102006
DA 2019-06-15
ER

PT S
AU Kawaguchi, K
   Kaelbling, LP
   Lozano-Perez, T
AF Kawaguchi, Kenji
   Kaelbling, Leslie Pack
   Lozano-Perez, Tomas
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Bayesian Optimization with Exponential Convergence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID GLOBAL MAXIMUM; ALGORITHM
AB This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence [1] requires access to the delta-cover sampling, which was considered to be impractical [1, 2]. Our approach eliminates both requirements and achieves an exponential convergence rate.
C1 [Kawaguchi, Kenji; Kaelbling, Leslie Pack; Lozano-Perez, Tomas] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Kawaguchi, K (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM kawaguch@mit.edu; lpk@csail.mit.edu; tlp@csail.mit.edu
FU NSF [1420927]; ONR [N00014-14-1-0486]; ARO [W911NF1410433]; Funai
   Overseas Scholarship
FX The authors would like to thank Dr. Remi Munos for his thoughtful
   comments and suggestions. We gratefully acknowledge support from NSF
   grant 1420927, from ONR grant N00014-14-1-0486, and from ARO grant
   W911NF1410433. Kenji Kawaguchi was supported in part by the Funai
   Overseas Scholarship. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of our sponsors.
CR Bubeck S, 2011, LECT NOTES ARTIF INT, V6925, P144, DOI 10.1007/978-3-642-24412-4_14
   Carter RG, 2001, OPTIM ENG, V2, P139, DOI 10.1023/A:1013123110266
   De Freitas N., 2012, P 29 INT C MACH LEAR
   Dixon L. C. W., 1977, GLOBAL OPTIMA CONVEX
   Gardner J.R., 2014, ICML, P937
   JONES DR, 1993, J OPTIMIZ THEORY APP, V79, P157, DOI 10.1007/BF00941892
   Kandasamy K., 2015, ARXIV150301673
   Kvasov DE, 2003, NUMER MATH, V94, P93, DOI 10.1007/s00211-002-0419-8
   MAYNE DQ, 1984, J OPTIMIZ THEORY APP, V42, P19, DOI 10.1007/BF00934131
   McDonald DB, 2007, APPL MATH MODEL, V31, P2095, DOI 10.1016/j.apm.2006.08.008
   MLADINEO RH, 1986, MATH PROGRAM, V34, P188, DOI 10.1007/BF01580583
   Munos R., 2011, P ADV NEUR INF PROC
   Murphy K. P., 2012, MACHINE LEARNING PRO, P521
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   SHUBERT BO, 1972, SIAM J NUMER ANAL, V9, P379, DOI 10.1137/0709036
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Strehl AL, 2009, J MACH LEARN RES, V10, P2413
   STRONGIN RG, 1973, ENG CYBERN, V11, P549
   SURJANOVIC S, 2014, VIRTUAL LIB SIMULATI
   Walsh TJ, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P612
   Wang Z, 2013, P 23 INT JOINT C ART, P1778
   Wang ZX, 2014, C IND ELECT APPL, P1005, DOI 10.1109/ICIEA.2014.6931310
   Zwolak JW, 2005, IEE P SYST BIOL, V152, P81, DOI [10.1049/ip-syb:20045032, 10.1049/ip-sb:20045032]
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100083
DA 2019-06-15
ER

PT S
AU Korhonen, JH
   Parviainen, P
AF Korhonen, Janne H.
   Parviainen, Pekka
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Tractable Bayesian Network Structure Learning with Bounded Vertex Cover
   Number
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains NP-hard even for tree-width 2. In this paper, we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is W[1]-hard in parameter k. Furthermore, we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP), and show this is feasible in practice.
C1 [Korhonen, Janne H.] Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.
   [Parviainen, Pekka] Aalto Univ, Dept Comp Sci, HIIT, Espoo, Finland.
RP Korhonen, JH (reprint author), Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.
EM janne.h.korhonen@helsinki.fi; pekka.parviainen@aalto.fi
FU Academy of Finland (Finnish Centre of Excellence in Computational
   Inference Research COIN) [251170]
FX We thank James Cussens for fruitful discussions. This research was
   partially funded by the Academy of Finland (Finnish Centre of Excellence
   in Computational Inference Research COIN, 251170). The experiments were
   performed using computing resources within the Aalto University School
   of Science "Science-IT" project.
CR Bartlett Mark, 2013, 29 C UNC ART INT UAI
   Berg Jeremias, 2014, 17 INT C ART INT STA
   Chickering D.M, 1996, LEARNING DATA ARTIFI, P121, DOI DOI 10.1007/978-1-4612-2404-4_12
   Chickering DM, 2004, J MACH LEARN RES, V5, P1287
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D
   COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110
   Cussens James, 2011, 27 C UNC ART INT UAI
   Dasgupta Sanjoy, 1999, 15 C UNC ART INT UAI
   Downey R.G., 1999, MG COMP SCI, P530
   Downey Rodney G., 1994, FEASIBLE MATH, P219
   Elidan G, 2008, J MACH LEARN RES, V9, P2699
   Flum J., 2006, TEXT THEORET COMP S
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503
   Jaakkola Tommi, 2010, 13 INT C ART INT STA
   Korhonen Janne H., 2013, 16 INT C ART INT STA
   Kwisthout Johan H. P., 2010, 19 EUR C ART INT ECA
   Meek C, 2001, J ARTIF INTELL RES, V15, P383, DOI 10.1613/jair.914
   Nie Siqi, 2014, ADV NEURAL INFORM PR
   Niedermeier R., 2006, INVITATION FIXED PAR
   Ott Sascha, 2003, Genome Inform, V14, P124
   Parviainen Pekka, 2014, 17 INT C ART INT STA
   Silander Tomi, 2006, 22 C UNC ART INT UAI
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103041
DA 2019-06-15
ER

PT S
AU Kucukelbir, A
   Ranganath, R
   Gelman, A
   Blei, DM
AF Kucukelbir, Alp
   Ranganath, Rajesh
   Gelman, Andrew
   Blei, David M.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Automatic Variational Inference in Stan
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.
C1 [Kucukelbir, Alp; Gelman, Andrew; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA.
RP Kucukelbir, A (reprint author), Columbia Univ, New York, NY 10027 USA.
EM alp@cs.columbia.edu; rajeshr@cs.princeton.edu; gelman@stat.columbia.edu;
   david.blei@columbia.edu
FU NSF [IIS-0745520, IIS-1247664, IIS-1009542, SES-1424962]; ONR
   [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; NDSEG;
   Siebel Scholar Foundation; John Templeton Foundation; Sloan
   [G-2015-13987]; IES [DE R305D140059]; Facebook; Adobe; Amazon
FX We thank Dustin Tran, Bruno Jacobs, and the reviewers for their
   comments. This work is supported by NSF IIS-0745520, IIS-1247664,
   IIS-1009542, SES-1424962, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009,
   N66001-15-C-4032, Sloan G-2015-13987, IES DE R305D140059, NDSEG,
   Facebook, Adobe, Amazon, and the Siebel Scholar and John Templeton
   Foundations.
CR Bishop C. M., 2006, PATTERN RECOGNITION
   Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gelman A., 2006, DATA ANAL USING REGR
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Goodman N., 2008, P 24 C UNC ART INT, V8, P220
   Hardle W. K., 2012, APPL MULTIVARIATE ST
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2013, ARXIV13126114
   Mansinghka V, 2014, ARXIV14040099
   Olive D. J, 2014, STAT THEORY INFERENC
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Ranganath  R., 2014, AISTATS, P814
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Robert C. P., 1999, MONTE CARLO STAT MET
   Salimans Tim, 2014, ARXIV14011022
   Stan Development Team, 2015, STAN MOD LANG US GUI
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Villegas Mauricio, 2013, CLEF EV LABS WORKSH
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wingate D., 2013, ARXIV13011299
   Winn J, 2005, J MACH LEARN RES, V6, P661
   Wood F., 2014, AISTATS, P2
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101018
DA 2019-06-15
ER

PT S
AU Lian, XR
   Huang, YJ
   Li, YC
   Liu, J
AF Lian, Xiangru
   Huang, Yijun
   Li, Yuncheng
   Liu, Ji
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Asynchronous parallel implementations of stochastic gradient (SG) have been broadly used in solving deep neural network and received many successes in practice recently. However, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism. To fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of SG: one is over a computer network and the other is on a shared memory system. We establish an ergodic convergence rate O (1/root K) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by root K (K is the total number of iterations). Our results generalize and improve existing analysis for convex minimization.
C1 [Lian, Xiangru; Huang, Yijun; Li, Yuncheng; Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
RP Lian, XR (reprint author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
EM lianxiangru@gmail.com; huangyj0@gmail.com; raingomm@gmail.com;
   ji.liu.uwisc@gmail.com
FU NSF [CNS-1548078]; NEC fellowship; University of Rochester
FX This project is supported by the NSF grant CNS-1548078, the NEC
   fellowship, and the startup funding at University of Rochester. We thank
   Professor Daniel Gildea and Professor Sandhya Dwarkadas at University of
   Rochester, Professor Stephen J. Wright at University of
   Wisconsin-Madison, and anonymous (meta-) reviewers for their
   constructive comments and helpful advices.
CR Agarwal A., 2011, NIPS
   Avron H., 2014, IPDPS
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED, V23
   Dean J., 2012, NIPS
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Fercoq O., 2013, ARXIV13125799
   Feyzmahdavian H. R., 2015, ARXIV E PRINTS
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Hong M., 2014, ARXIV14126058
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, V1, P7
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li M., 2014, OSDI
   Li M., 2014, NIPS
   Li M., 2013, BIG LEARN NIPS WORKS
   Liu J., 2014, ARXIV14033862
   Liu J., 2014, ICML
   Liu J., 2014, ARXIV14014780
   Mania Horia, 2015, ARXIV150706970
   Marecek J., 2014, ARXIV14060238
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Niu F., 2011, NIPS
   Paine T., 2013, NIPS
   Petroni F., 2014, ACM C REC SYST
   Sridhar S., 2013, NIPS
   Tappenden Rachael, 2015, ARXIV150303033
   Tran K., 2015, ICML
   Yun H., 2013, ARXIV13120193
   Zhang R., 2014, ICML
   Zhang S., 2014, CORR
NR 31
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101011
DA 2019-06-15
ER

PT S
AU Liang, M
   Hu, XL
   Zhang, B
AF Liang, Ming
   Hu, Xiaolin
   Zhang, Bo
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Convolutional Neural Networks with Intra-layer Recurrent Connections for
   Scene Labeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BACKPROPAGATION
AB Scene labeling is a challenging computer vision task. It requires the use of both local discriminative features and global context information. We adopt a deep recurrent convolutional neural network (RCNN) for this task, which is originally proposed for object recognition. Different from traditional convolutional neural networks (CNN), this model has intra-layer recurrent connections in the convolutional layers. Therefore each convolutional layer becomes a two-dimensional recurrent neural network. The units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods. While recurrent iterations proceed, the region of context captured by each unit expands. In this way, feature extraction and context modulation are seamlessly integrated, which is different from typical methods that entail separate modules for the two steps. To further utilize the context, a multi-scale RCNN is proposed. Over two benchmark datasets, Standford Background and Sift Flow, the model outperforms many state-of-the-art models in accuracy and efficiency.
C1 [Liang, Ming; Hu, Xiaolin; Zhang, Bo] Tsinghua Univ, CBICR, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
RP Liang, M (reprint author), Tsinghua Univ, CBICR, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
EM liangm07@mails.tsinghua.edu.cn; xlhu@tsinghua.edu.cn;
   dcszb@tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2012CB316301,
   2013CB329403]; National Natural Science Foundation of China [61273023,
   91420201, 61332007]; Natural Science Foundation of Beijing [4132046]
FX We are grateful to the anonymous reviewers for their valuable comments.
   This work was supported in part by the National Basic Research Program
   (973 Program) of China under Grant 2012CB316301 and Grant 2013CB329403,
   in part by the National Natural Science Foundation of China under Grant
   61273023, Grant 91420201, and Grant 61332007, in part by the Natural
   Science Foundation of Beijing under Grant 4132046.
CR Chen L. C., 2015, ICLR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eigen D., 2014, ICLR
   Eigen D, 2012, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2012.6248004
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Grangier D., 2009, ICML DEEP LEARN WORK, V3
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lempitsky V.S., 2011, NIPS, p[24, 1485]
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Long  J., 2015, CVPR
   Mostajabi  M., 2015, CVPR
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Pinheiro P. H., 2014, ICML
   Sermanet  P., 2014, ICLR
   Sharma A., 2014, P ADV NEUR INF PROC, P2447
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Singh G, 2013, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR.2013.405
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zheng S., 2015, ICCV
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100002
DA 2019-06-15
ER

PT S
AU Linderman, SW
   Johnson, MJ
   Adams, RP
AF Linderman, Scott W.
   Johnson, Matthew J.
   Adams, Ryan P.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Dependent Multinomial Models Made Easy: Stick Breaking with the
   Polya-Gamma Augmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in Polya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.
C1 [Linderman, Scott W.; Johnson, Matthew J.; Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA.
   [Adams, Ryan P.] Twitter, Cambridge, MA 02138 USA.
RP Linderman, SW (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM swl@seas.harvard.edu; mattjj@csail.mit.edu; rpa@seas.harvard.edu
FU Siebel Scholarship; Center for Brains, Minds and Machines (CBMM) - NSF
   STC [CCF-1231216]; Harvard/MIT Joint Research Grants Program; NSF
   [IIS-1421780]; Alfred P. Sloan Foundation
FX S.W.L. is supported by a Siebel Scholarship and the Center for Brains,
   Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. M.J.J.
   is supported by the Harvard/MIT Joint Research Grants Program. R.P.A. is
   supported by NSF IIS-1421780 as well as the Alfred P. Sloan Foundation.
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Belanger David, 2015, P INT C MACH LEARN
   Belanger David, 2014, NIPS 2014 MOD ML NLP
   Blei D., 2006, ADV NEURAL INFORM PR, V18, P147
   Blei D. M., 2006, ICML, P113, DOI DOI 10.1145/1143844.1143859
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen Jianlei, 2013, ADV NEURAL INFORM PR, P2445
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Holmes CC, 2006, BAYESIAN ANAL, V1, P145, DOI 10.1214/06-BA105
   Khan Mohammad Emtiyaz, 2012, AISTATS, P610
   Lindsten Fredrik, 2012, ADV NEURAL INFORM PR, P2591
   Murray I., 2010, JMLR W CP, V9, P541
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wang X., 2008, ADV NEURAL INFORM PR, P1577
   Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100028
DA 2019-06-15
ER

PT S
AU Makhzani, A
   Frey, B
AF Makhzani, Alireza
   Frey, Brendan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Winner-Take-All Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB In this paper, we propose a winner-take-allmethod for learning hierarchical sparse representations in an unsupervised fashion. We first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units. We then propose the convolutional winner-take-all autoencoderwhich combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations. We describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions. We will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets, and achieve competitive classification performance.
C1 [Makhzani, Alireza; Frey, Brendan] Univ Toronto, Toronto, ON, Canada.
RP Makhzani, A (reprint author), Univ Toronto, Toronto, ON, Canada.
EM makhzani@psi.toronto.edu; frey@psi.toronto.edu
FU NVIDIA
FX We would like to thank Ruslan Salakhutdinov and Andrew Delong for the
   valuable comments. We also acknowledge the support of NVIDIA with the
   donation of the GPUs used for this research.
CR Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Coates A., 2011, INT C ART INT STAT
   Coates A., 2011, NIPS
   Dosovitskiy  A., 2014, ADV NEURAL INFORM PR, P766
   Goodfellow I, 2013, ICML
   Hinton G. E, 2012, ARXIV12070580
   Kavukcuoglu K., 2010, NIPS, V1, P5
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Krizhevsky Alex, 2010, CONVOLUTIONAL UNPUB
   Lee H, 2009, P ANN INT C MACH LEA, V26, P609, DOI DOI 10.1145/1553374.1553453
   Lin T.-H., 2014, P 31 INT C MACH LEAR, P1323
   Mairal Julien, 2014, ADV NEURAL INFORM PR, P2627
   Makhzani A., 2014, INT C LEARN REPR ICL
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Ng A., 2011, CS294A LECT NOTES, V72
   Ranzato M., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Zeiler M. D., 2012, ARXIV12070151
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 22
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101043
DA 2019-06-15
ER

PT S
AU Miller, A
   Wu, A
   Regier, J
   McAuliffe, J
   Lang, D
   Prabhat
   Schlegel, D
   Adams, R
AF Miller, Andrew
   Wu, Albert
   Regier, Jeffrey
   McAuliffe, Jon
   Lang, Dustin
   Prabhat
   Schlegel, David
   Adams, Ryan
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Gaussian Process Model of Quasar Spectral Energy Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We propose a method for combining two sources of astronomical data, spectroscopy and photometry, that carry information about sources of light (e.g., stars, galaxies, and quasars) at extremely different spectral resolutions. Our model treats the spectral energy distribution (SED) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations. We place a flexible, nonparametric prior over the SED of a light source that admits a physically interpretable decomposition, and allows us to tractably perform inference. We use our model to predict the distribution of the redshift of a quasar from five-band (low spectral resolution) photometric data, the so called "photo-z" problem. Our method shows that tools from machine learning and Bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties.
C1 [Miller, Andrew; Wu, Albert; Adams, Ryan] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Regier, Jeffrey; McAuliffe, Jon] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
   [Lang, Dustin] Carnegie Mellon Univ, McWilliams Ctr Cosmol, Pittsburgh, PA 15213 USA.
   [Prabhat; Schlegel, David] Lawrence Berkeley Natl Lab, Berkeley, CA USA.
RP Miller, A (reprint author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM acm@seas.harvard.edu; awu@college.harvard.edu; jeff@stat.berkeley.edu;
   jon@stat.berkeley.edu; dstn@cmu.edu; prabhat@lbl.gov;
   djschlegel@lbl.gov; rpa@seas.harvard.edu
FU Applied Mathematics Program within the Office of Science Advanced
   Scientific Computing Research of the U.S. Department of Energy
   [DE-AC02-05CH11231]
FX The authors would like to thank Matthew Hoffman and members of the HIPS
   lab for helpful discussions. This work is supported by the Applied
   Mathematics Program within the Office of Science Advanced Scientific
   Computing Research of the U.S. Department of Energy under contract No.
   DE-AC02-05CH11231. This work used resources of the National Energy
   Research Scientific Computing Center (NERSC). We would like to thank
   Tina Butler, Tina Declerck and Yushu Yao for their assistance.
CR Alam Shadab, 2015, ARXIV150100963
   Bovy J, 2012, ASTROPHYS J, V749, DOI 10.1088/0004-637X/749/1/41
   Brescia M, 2013, ASTROPHYS J, V772, DOI 10.1088/0004-637X/772/2/140
   Brooks S, 2011, CH CRC HANDB MOD STA, pXIX
   Christopher Martin D, 2005, ASTROPHYSICAL J LETT, V619
   Dawson KS, 2013, ASTRON J, V145, DOI 10.1088/0004-6256/145/1/10
   Gray RO, 2001, ASTRON J, V121, P2159, DOI 10.1086/319957
   HARRISON E, 1993, ASTROPHYS J, V403, P28, DOI 10.1086/172179
   Hogg David W, 1999, ASTROPH9905116 ARXIV
   Maclaurin Dougal, 2015, ICML WORKSH AUT MACH
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.2307/2006193
   Paris I, 2014, ASTRON ASTROPHYS, V563, DOI 10.1051/0004-6361/201322691
   Regier  Jeffrey, 2015, P 32 INT C MACH LEAR
   SDSSIII, 2013, MEAS FLUX MAGN
   Silk Joseph, 1998, ASTRONOMY ASTROPHYSI
   Stoughton C, 2002, ASTRON J, V123, P485, DOI 10.1086/324741
   Walcher J, 2011, ASTROPHYS SPACE SCI, V331, P1, DOI 10.1007/s10509-010-0458-z
   Weinberg David H, 2003, P 13 ANN ASTR C MAR, V666
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102108
DA 2019-06-15
ER

PT S
AU Mohamed, S
   Rezende, DJ
AF Mohamed, Shakir
   Rezende, Danilo J.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Variational Information Maximisation for Intrinsically Motivated
   Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID EMPOWERMENT
AB The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm - an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.
C1 [Mohamed, Shakir; Rezende, Danilo J.] Google DeepMind, London, England.
RP Mohamed, S (reprint author), Google DeepMind, London, England.
EM shakir@google.com; danilor@google.com
CR Andrew Y, 1999, ICML
   Barber D, 2004, ADV NEUR IN, V16, P201
   Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115
   Buhmann J. M., 2012, WORKSH UNS TRANSF LE
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gao S., 2014, ARXIV14112003
   Gretton A., 2003, ICASP, V4, pIV
   Itti L., 2005, ADV NEURAL INFORM PR, V19, P547
   Jaakkola TS, 1998, NATO ADV SCI I D-BEH, V89, P163
   Jung T, 2011, ADAPT BEHAV, V19, P16, DOI 10.1177/1059712310392389
   Klyubin AS, 2005, IEEE C EVOL COMPUTAT, P128
   Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358
   LeCun Y., 1995, HDB BRAIN THEORY NEU, V3361, P310
   Little DY, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00037
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nelson JD, 2005, PSYCHOL REV, V112, P979, DOI 10.1037/0033-295X.112.4.979
   Oudeyer P., 2008, INT C EP ROB
   Rubin J, 2012, INTEL SYST REF LIBR, V28, P57
   Salge C, 2014, EMERGENCE COMPLEX CO, V9, P67, DOI 10.1007/978-3-642-53734-9_4
   Salge C, 2014, ENTROPY-SWITZ, V16, P2789, DOI 10.3390/e16052789
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Singh S. P., 2005, NIPS
   Still S, 2012, THEOR BIOSCI, V131, P139, DOI 10.1007/s12064-011-0142-z
   Sutton R., 1998, INTRO REINFORCEMENT
   Tishby N., 1999, ALL C COMM CONTR COM
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Wissner-Gross AD, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.168702
   Yeung RW, 2008, INFORM TECH TRANS PR, P211
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100036
DA 2019-06-15
ER

PT S
AU Mroueh, Y
   Voinea, S
   Poggio, T
AF Mroueh, Youssef
   Voinea, Stephen
   Poggio, Tomaso
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning with Group Invariant Features: A Kernel Perspective
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.
C1 [Mroueh, Youssef] IBM Watson Grp, Armonk, NY 10504 USA.
   [Voinea, Stephen; Poggio, Tomaso] MIT, CBMM, Cambridge, MA 02139 USA.
RP Mroueh, Y (reprint author), IBM Watson Grp, Armonk, NY 10504 USA.
EM mroueh@us.ibm.com; voinea@mit.edu; tp@ai.mit.edu
FU Nuance Foundation Grant; Center for Brains, Minds and Machines (CBMM) -
   NSF STC award [CCF 1231216]
FX Stephen Voinea acknowledges the support of a Nuance Foundation Grant.
   This work was also supported in part by the Center for Brains, Minds and
   Machines (CBMM), funded by NSF STC award CCF 1231216.
CR Abu-Mostafa Y. S., 1990, Journal of Complexity, V6, P192, DOI 10.1016/0885-064X(90)90006-Y
   Anselmi F., 2013, CORR
   Bach F. R., 2015, CORR
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Benzeghiba M, 2007, SPEECH COMMUN, V49, P763, DOI 10.1016/j.specom.2007.02.006
   Bo L., 2010, NIPS
   Bruna J., 2012, CORR
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Haasdonk B., 2005, SCIA
   Hinton G., 2011, ICANN 11
   Johnson W. B., 1984, C MOD AN PROB
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mairal J., 2014, NIPS
   Niyogi P, 1998, P IEEE, V86, P2196, DOI 10.1109/5.726787
   Rahimi Ali, 2008, NIPS
   Rahimi Ali, 2008, P 46 ANN ALL C
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tacchetti A., 2013, CORR
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Voinea S., 2014, WORD LEVEL INVARIANT, V14, P3201
   Wahba G, 1990, SPLINE MODELS OBSERV, V59
   Walder C., 2007, NIPS
   Williams Christopher KI, 2001, NIPS
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101058
DA 2019-06-15
ER

PT S
AU Musco, C
   Musco, C
AF Musco, Cameron
   Musco, Christopher
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Randomized Block Krylov Methods for Stronger and Faster Approximate
   Singular Value Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LANCZOS METHOD; CONVERGENCE; EIGENVALUE; ALGORITHMS
AB Since being analyzed by Rokhlin, Szlam, and Tygert [1] and popularized by Halko, Martinsson, and Tropp [2], randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for any matrix, independently of singular value gaps. After (O) over tilde (1/epsilon) iterations, it gives a low-rank approximation within (1 + epsilon) of optimal for spectral norm error.
   We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just (O) over tilde (1/root epsilon) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice.
   Furthermore, while it is a simple accuracy benchmark, even (1 + epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.
C1 [Musco, Cameron; Musco, Christopher] MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Musco, C (reprint author), MIT, EECS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM cnmusco@mit.edu; cpmusco@mit.edu
CR Bauer F. L., 1957, Z ANGEW MATH PHYS, V8, P214, DOI [10.1007/BF01600502, DOI 10.1007/BF01600502]
   Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Cohen Michael B., 2015, P 47 ANN ACM S THEOR
   Cullum J., 1974, 1974 P IEEE C DEC CO, V13, P505
   DAVIS TA, 2011, ACM T MATH SOFTWARE, V38, P1
   Golub G. H., 1996, MATRIX COMPUTATIONS
   Golub G. H., 1977, MATH SOFTWARE, V3, P361
   GOLUB GH, 1981, ACM T MATH SOFTWARE, V7, P149, DOI 10.1145/355945.355946
   Gu Ming, 2014, ARXIV14082208
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Halko N, 2011, SIAM J SCI COMPUT, V33, P2580, DOI 10.1137/100804139
   Halko NP, 2012, THESIS
   Hall David, 2009, SCALANLP BREEZE
   IBM Reseach Division Skylark Team, 2014, LIBSK SKETCH BAS DIS
   Karnin Z., 2015, P 28 ANN C COMP LEAR, P505
   KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893
   Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727
   Li RC, 2015, NUMER MATH, V131, P83, DOI 10.1007/s00211-014-0681-6
   Liutkus Antoine, 2014, MATLAB CENTRAL FILE
   Martinsson Per-Gunnar, 2010, NIPS WORKSH LOW RANK
   Martinsson Per-Gunnar, 2006, 1361 YAL U
   Musco C., 2015, ARXIV150405477
   Okanohara Daisuke, 2010, REDSVD RANDOMIZED SV
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Rennie Jason, 2015, 20 NEWSGROUPS
   Rokhlin V, 2009, SIAM J MATRIX ANAL A, V31, P1100, DOI 10.1137/080736417
   SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059
   Saad Y, 2011, CLASS APPL MATH, V66, P1, DOI 10.1137/1.9781611970739
   Sarlos T, 2006, P 47 ANN IEEE S FDN
   Sou Kin Cheong, 2010, P 19 INT S MATH THEO
   Szlam A., 2014, ARXIV14123510
   Tulloch Andrew, 2014, FAST RANDOMIZED SING
   Witten Rafi, 2014, ALGORITHMICA, V31, P1
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100103
DA 2019-06-15
ER

PT S
AU Ndiaye, E
   Fercoq, O
   Gramfort, A
   Salmon, J
AF Ndiaye, Eugene
   Fercoq, Olivier
   Gramfort, Alexandre
   Salmon, Joseph
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI GAP Safe screening rules for sparse multi-task and multi-class models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID LASSO; REGRESSION; SELECTION
AB High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with l(1) and l(1)/l(2) norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.
C1 [Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph] Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.
RP Ndiaye, E (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.
EM eugene.ndiaye@telecom-paristech.fr; olivier.fercoq@telecom-paristech.fr;
   alexandre.gramfort@telecom-paristech.fr;
   joseph.salmon@telecom-paristech.fr
FU Chair Machine Learning for Big Data at Telecom ParisTech; Orange/Telecom
   ParisTech think tank phi-TAB; FMJH Program Gaspard Monge in optimization
   and operation research; EDF
FX We acknowledge the support from Chair Machine Learning for Big Data at
   Telecom ParisTech and from the Orange/Telecom ParisTech think tank
   phi-TAB. This work benefited from the support of the "FMJH Program
   Gaspard Monge in optimization and operation research", and from the
   support to this program from EDF.
CR Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Argyriou Andreas, 2006, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Blondel M, 2013, MACH LEARN, V93, P31, DOI 10.1007/s10994-013-5367-2
   Bonnefoy A., 2014, EUSIPCO
   Bonnefoy A, 2015, IEEE T SIGNAL PROCES, V63, P5121, DOI 10.1109/TSP.2015.2447503
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Efron B, 2004, ANN STAT, V32, P407
   El Ghaoui L, 2012, PAC J OPTIM, V8, P667
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fercoq  O., 2015, ICML, V37, P333
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Gramfort A, 2012, PHYS MED BIOL, V57, P1937, DOI 10.1088/0031-9155/57/7/1937
   Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA, V306
   Koh KM, 2007, J MACH LEARN RES, V8, P1519
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Tibshirani RJ, 2013, ELECTRON J STAT, V7, P1456, DOI 10.1214/13-EJS815
   Wang J., 2014, ADV NEURAL INFORM PR, V27, P1053
   Wang J., 2012, ARXIV12113966
   Xiang Z. J., 2014, ARXIV14054897
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100067
DA 2019-06-15
ER

PT S
AU Oh, J
   Guo, XX
   Lee, H
   Lewis, R
   Singh, S
AF Oh, Junhyuk
   Guo, Xiaoxiao
   Lee, Honglak
   Lewis, Richard
   Singh, Satinder
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Action-Conditional Video Prediction using Deep Networks in Atari Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.
C1 [Oh, Junhyuk; Guo, Xiaoxiao; Lee, Honglak; Lewis, Richard; Singh, Satinder] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Oh, J (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM junhyuk@umich.edu; guoxiao@umich.edu; honglak@umich.edu;
   rickl@umich.edu; baveja@umich.edu
FU NSF [IIS-1526059]; Bosch Research; ONR [N00014-13-1-0762]
FX This work was supported by NSF grant IIS-1526059, Bosch Research, and
   ONR grant N00014-13-1-0762. Any opinions, findings, conclusions, or
   recommendations expressed here are those of the authors and do not
   necessarily reflect the views of the sponsors.
CR Bellemare M.G., 2014, ICML
   Bellemare M.G., 2012, AAAI
   Bellemare M.G., 2013, ICML
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bengio Y., 2009, ICML
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Ciresan D., 2012, CVPR
   Dosovitskiy Alexey, 2015, CVPR
   Girshick R., 2014, CVPR
   Graves A, 2013, ARXIV13080850
   Guo  Xiaoxiao, 2014, NIPS
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jia Y., 2014, ACM MULTIMEDIA
   Karpathy A., 2014, CVPR
   Kocsis L., 2006, ECML
   Krizhevsky A., 2012, NIPS
   Lenz I., 2015, RSS
   Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53
   Michalski V., 2014, NIPS
   Mittelman R., 2014, ICML
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair V., 2010, ICML
   Reed S., 2014, ICML
   Rifai S., 2012, ECCV
   Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Srivastava N, 2015, ICML
   Sutskever I., 2009, NIPS
   Sutskever  I., 2014, NIPS
   Sutskever I., 2011, ICML
   Szegedy C, 2014, ARXIV14094842
   Taylor G., 2009, ICML
   Tieleman T., 2012, LECT 6 5 RMSPROP DIV
   Tran D., 2015, ICCV
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Yang Jimei, 2015, NIPS
NR 37
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102008
DA 2019-06-15
ER

PT S
AU Paul, S
   Magdon-Ismail, M
   Drineas, P
AF Paul, Saurabh
   Magdon-Ismail, Malik
   Drineas, Petros
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Column Selection via Adaptive Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID ALGORITHMS
AB Selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm. Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms. Our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.
C1 [Paul, Saurabh] Paypal Inc, Palo Alto, CA 94303 USA.
   [Magdon-Ismail, Malik; Drineas, Petros] Rensselaer Polytech Inst, CS Dept, Troy, NY 12181 USA.
RP Paul, S (reprint author), Paypal Inc, Palo Alto, CA 94303 USA.
EM saupaul@paypal.com; magdon@cs.rpi.edu; drinep@cs.rpi.edu
FU  [IIS-1447283];  [IIS-1319280]
FX Most of the work was done when SP was a graduate student at RPI. PD was
   supported by IIS-1447283 and IIS-1319280.
CR Boutsidis C., 2013, IEEE T INFORM THEORY, V59
   Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X
   Boutsidis C, 2014, INFORM PROCESS LETT, V114, P273, DOI 10.1016/j.ipl.2013.11.011
   Boutsidis C, 2011, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2011.21
   Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968
   Boutsidis Christos, 2011, P 25 ANN C NEUR INF
   CHAN TF, 1987, LINEAR ALGEBRA APPL, V88-9, P67, DOI 10.1016/0024-3795(87)90103-0
   CHAN TF, 1992, SIAM J SCI STAT COMP, V13, P727, DOI 10.1137/0913043
   Davidov D., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P250, DOI 10.1145/1008992.1009036
   Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681
   Deshpande A, 2006, LECT NOTES COMPUT SC, V4110, P292
   Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38
   Deshpande Amit, 2006, THEORY COMPUT, V2, P225
   Drineas P., 2002, P 34 ANN ACM S THEOR, P82
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023
   Drineas P, 2006, LECT NOTES COMPUT SC, V4110, P316
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Liberty E, 2007, P NATL ACAD SCI USA, V104, P20167, DOI 10.1073/pnas.0709640104
   Magdon-Ismail Malik, 2015, ARXIV150206626
   Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]
   Maung C., 2013, P ADV NEUR INF PROC, V26, P1628
   Papailiopoulos D., 2014, P 20 ACM SIGKDD INT, P997
   Paschou P, 2010, J MED GENET, V47, P835, DOI 10.1136/jmg.2010.078212
NR 26
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103049
DA 2019-06-15
ER

PT S
AU Ping, W
   Liu, Q
   Ihler, A
AF Ping, Wei
   Liu, Qiang
   Ihler, Alexander
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Decomposition Bounds for Marginal MAP
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.
C1 [Ping, Wei; Ihler, Alexander] UC Irvine, Comp Sci, Irvine, CA 92697 USA.
   [Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA.
RP Ping, W (reprint author), UC Irvine, Comp Sci, Irvine, CA 92697 USA.
EM wping@ics.uci.edu; qliu@cs.dartmouth.edu; ihler@ics.uci.edu
RI Ping, Wei/O-4470-2019
FU NSF [IIS-1065618, IIS-1254071]; United States Air Force under the DARPA
   PPAML program [FA8750-14-C-0011]
FX This work is sponsored in part by NSF grants IIS-1065618 and
   IIS-1254071. Alexander Ihler is also funded in part by the United States
   Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML
   program.
CR Dechter R., 2003, JACM
   Dechter R., 2013, SYNTHESIS LECT ARTIF
   Domke J., 2011, AAAI
   Doucet A., 2002, STAT COMPUTING
   Globerson A., 2008, NIPS
   Globerson A., 2007, AISTATS
   Hardy GH, 1952, INEQUALITIES
   Hazan T., 2012, UAI
   Hazan T., 2010, IEEE T INFORM THEORY
   Hazan T., 2008, UAI
   Ihler A., 2012, UAI
   Jancsary J., 2011, AISTATS
   Kiselev I., 2014, AAMAS
   Komodakis N., 2011, TPAMI
   Liu Q, 2014, THESIS
   Liu Q., 2013, JMLR
   Liu Q., 2011, ICML
   Marinescu R., 2014, UAI
   Maua D., 2012, ICML
   Meek C., 2011, BAYESIAN STAT
   Meltzer T., 2009, UAI
   Meshi O., 2011, ECML PKDD
   Meshi O., 2010, ICML
   Mooij J. M., 2010, JMLR
   Naradowsky J., 2012, EMNLP
   Nowozin S., 2011, FDN TRENDS COMPUTER
   Park J., 2003, UAI
   Park J., 2004, JAIR
   Ping W., 2014, ICML
   Ruozzi N., 2013, IEEE T INFORM THEORY
   Sontag D., 2009, AISTATS
   Sontag D., 2008, UAI
   Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219
   Wainwright M.J., 2005, IEEE T INFORM THEORY
   Weiss Y., 2007, UAI
   Werner T., 2007, TPAMI
   Yarkony J., 2010, CVPR
   Yuan C., 2004, UAI
   Yuan C., 2009, IJCAI
NR 39
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100068
DA 2019-06-15
ER

PT S
AU Rasmus, A
   Valpola, H
   Honkala, M
   Berglund, M
   Raiko, T
AF Rasmus, Antti
   Valpola, Harri
   Honkala, Mikko
   Berglund, Mathias
   Raiko, Tapani
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Semi-Supervised Learning with Ladder Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID NEURAL-NETWORKS
AB We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.
C1 [Rasmus, Antti; Valpola, Harri] Curious AI Co, Helsinki, Finland.
   [Honkala, Mikko] Nokia Labs, Espoo, Finland.
   Aalto Univ, Helsinki, Finland.
RP Rasmus, A (reprint author), Curious AI Co, Helsinki, Finland.
FU Academy of Finland
FX We have received comments and help from a number of colleagues who would
   all deserve to be mentioned but we wish to thank especially Yann LeCun,
   Diederik Kingma, Aaron Courville, Ian Goodfellow, Soren Sonderby, Jim
   Fan and Hugo Larochelle for their helpful comments and suggestions. The
   software for the simulations for this paper was based on Theano [32] and
   Blocks [33]. We also acknowledge the computational resources provided by
   the Aalto Science-IT project. The Academy of Finland has supported
   Tapani Raiko.
CR Bastien  Frederic, 2012, DEEP LEARN UNS FEAT
   Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Dosovitskiy  A., 2014, ADV NEURAL INFORM PR, P766
   Goodfellow I., 2012, INT C MACH LEARN ICM, P1439
   Goodfellow I., 2013, ADV NEURAL INFORM PR, V26, P548
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Goodfellow Ian J., 2013, P ICML 2013
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Lee  Dong-Hyun, 2013, WORKSH CHALL REPR LE
   MCLACHLAN GJ, 1975, J AM STAT ASSOC, V70, P365
   Merrienboer Bart van, 2015, ABS150600619 CORR
   Miyato  T., 2015, ARXIV150700677
   Pitelis Nikolaos, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P565, DOI 10.1007/978-3-662-44851-9_36
   Ranzato M., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Rasmus A., 2015, ARXIV150702672
   Rasmus A., 2015, ARXIV14127210
   Rifai S., 2011, ADV NEURAL INFORM PR, P2294
   Sarela J, 2005, J MACH LEARN RES, V6, P233
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suddarth Steven C, 1990, P 1990 EURASIP WORKS, P120, DOI DOI 10.1007/3-540-52255-7_33
   Szummer M, 2002, ADV NEUR IN, V14, P945
   Titterington D.M., 1985, WILEY SERIES PROBABI
   Valpola H., 2015, ADV INDEPENDENT COMP, P143
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhao Junbo, 2015, ARXIV150602351
NR 33
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102095
DA 2019-06-15
ER

PT S
AU Reddi, SJ
   Hefny, A
   Sra, S
   Poczos, B
   Smola, A
AF Reddi, Sashank J.
   Hefny, Ahmed
   Sra, Suvrit
   Poczos, Barnabas
   Smola, Alex
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI On Variance Reduction in Stochastic Gradient Descent and its
   Asynchronous Variants
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms-a crucial requirement for modern large-scale applications-have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.
C1 [Reddi, Sashank J.; Hefny, Ahmed; Poczos, Barnabas; Smola, Alex] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Sra, Suvrit] MIT, Cambridge, MA 02139 USA.
RP Reddi, SJ (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM sjakkamr@cs.cmu.edu; ahefny@cs.cmu.edu; suvrit@mit.edu;
   bapoczos@cs.cmu.edu; alex@smola.org
FU NSF [IIS-1409802]
FX SS was partially supported by NSF IIS-1409802.
CR Agarwal A., 2011, P ADV NEUR INF PROC, P873
   Agarwal A., 2014, ARXIV14100723
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   BERTSEKAS D. P., 2011, OPTIMIZATION MACHINE, V2010, P1
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Defazio A. J., 2014, ARXIV14072710
   Defazio Aaron, 2014, THESIS
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Gurbuzbalaban M, 2015, MATH PROGRAM, V151, P283, DOI 10.1007/s10107-015-0897-y
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Konecny J., 2013, ARXIV13121666
   Konecny J., 2015, ARXIV150404407
   Li Mu, 2014, ADV NEURAL INFORM PR, P19
   Liu J, 2014, IEEE INT CONF ROBOT, P469, DOI 10.1109/ICRA.2014.6906897
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Mairal J., 2013, ARXIV13053120
   Nedic A., 2001, STUDIES COMPUT MATH, V8, P381
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Reddi S., 2015, UAI 31
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S., 2013, ADV NEURAL INF PROCE, P378
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shamir O., 2014, P 52 ANN ALL C COMM
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zinkevich M., 2010, ADV NEURAL INFORM PR, V23, P2595
NR 30
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101080
DA 2019-06-15
ER

PT S
AU Rothenhausler, D
   Heinze, C
   Peters, J
   Meinshausen, N
AF Rothenhausler, Dominik
   Heinze, Christina
   Peters, Jonas
   Meinshausen, Nicolai
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI BACKSHIFT: Learning causal cyclic graphs from unknown shift
   interventions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MODELS
AB We propose a simple method to learn linear causal cyclic models in the presence of latent variables. The method relies on equilibrium data of the model recorded under a specific kind of interventions ("shift interventions"). The location and strength of these interventions do not have to be known and can be estimated from the data. Our method, called BACKSHIFT, only uses second moments of the data and performs simple joint matrix diagonalization, applied to differences between covariance matrices. We give a sufficient and necessary condition for identifiability of the system, which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings, one of which can be pure observational data. We demonstrate the performance on some simulated data and applications in flow cytometry and financial time series.
C1 [Rothenhausler, Dominik; Heinze, Christina; Meinshausen, Nicolai] Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland.
   [Peters, Jonas] Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Rothenhausler, D (reprint author), Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland.
EM rothenhaeusler@stat.math.ethz.ch; heinze@stat.math.ethz.ch;
   jonas.peters@tuebingen.mpg.de; meinshausen@stat.math.ethz.ch
RI Peters, Jan/D-5068-2009
OI Peters, Jan/0000-0002-5266-8091
CR Bollen K. A., 1989, STRUCTURAL EQUATIONS
   Burkard R. E., 2013, HDB COMBINATORIAL OP, P2741
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Eaton D., 2007, INT C ART INT STAT, P107
   Eberhardt F., 2010, J MACHINE LEARNING W, P185
   Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   Hyttinen A, 2012, J MACH LEARN RES, V13, P3387
   Jackson AL, 2003, NAT BIOTECHNOL, V21, P635, DOI 10.1038/nbt831
   Korb KB, 2004, LECT NOTES ARTIF INT, V3157, P322
   Kulkarni MM, 2006, NAT METHODS, V3, P833, DOI 10.1038/nmeth935
   Lacerda G., 2008, P 24 C UNC ART INT U, P366
   Lauritzen SL, 2002, J R STAT SOC B, V64, P321, DOI 10.1111/1467-9868.00340
   Maathuis MH, 2009, ANN STAT, V37, P3133, DOI 10.1214/09-AOS685
   Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x
   Mooij J. M., 2011, ADV NEURAL INFORM PR, V24, P639
   Mooij JM, 2013, P 29 ANN C UNC ART I, P431
   Pearl J, 2009, CAUSALITY MODELS REA
   Peters J., 2015, J ROYAL STAT SOC B
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Shimizu S, 2011, J MACH LEARN RES, V12, P1225
   Spirtes P., 2000, CAUSATION PREDICTION
   Tian J, 2001, P 17 C UNC ART INT, P512
   Ziehe A, 2004, J MACH LEARN RES, V5, P777
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100050
DA 2019-06-15
ER

PT S
AU Schiratti, JB
   Allassonniere, S
   Colliot, O
   Durrleman, S
AF Schiratti, Jean-Baptiste
   Allassonniere, Stephanie
   Colliot, Olivier
   Durrleman, Stanley
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Learning spatiotemporal trajectories from manifold-valued longitudinal
   data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID MAXIMUM-LIKELIHOOD; ALZHEIMERS-DISEASE; PROGRESSION; MODELS
AB We propose a Bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data, namely repeated measurements of the same objects or individuals at several points in time. The model allows to estimate a group-average trajectory in the space of measurements. Random variations of this trajectory result from spatiotemporal transformations, which allow changes in the direction of the trajectory and in the pace at which trajectories are followed. The use of the tools of Riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints, which lie therefore on a Riemannian manifold. Stochastic approximations of the Expectation-Maximization algorithm is used to estimate the model parameters in this highly non-linear setting. The method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of Alzheimer's disease. Experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease, thus validating the fact that it effectively estimated a normative scenario of disease progression. Random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals.
C1 [Schiratti, Jean-Baptiste; Colliot, Olivier; Durrleman, Stanley] UPMC Univ Paris 06, Sorbonne Univ,Inserm, INRIA Paris,Inst Cerveau & Moelle Epiniere,ICM, U1127,CNRS,UMR 7225,UMR S 1127,ARAMIS Lab, F-75013 Paris, France.
   [Schiratti, Jean-Baptiste; Allassonniere, Stephanie] Ecole Polytech, CMAP, Palaiseau, France.
RP Schiratti, JB (reprint author), UPMC Univ Paris 06, Sorbonne Univ,Inserm, INRIA Paris,Inst Cerveau & Moelle Epiniere,ICM, U1127,CNRS,UMR 7225,UMR S 1127,ARAMIS Lab, F-75013 Paris, France.; Schiratti, JB (reprint author), Ecole Polytech, CMAP, Palaiseau, France.
EM jean-baptiste.schiratti@cmap.polytechnique.fr;
   stephanie.allassonniere@polytechnique.edu; olivier.colliot@upmc.fr;
   stanley.durrleman@inria.fr
RI Jeong, Yongwook/N-7413-2016
CR Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229
   BRAAK H, 1995, NEUROBIOL AGING, V16, P271, DOI 10.1016/0197-4580(95)00021-6
   Delyon B, 1999, ANN STAT, V27, P94
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Diggle P, 2002, ANAL LONGITUDINAL DA
   Donohue MC, 2014, ALZHEIMERS DEMENT, V10, pS400, DOI 10.1016/j.jalz.2013.10.003
   Durrleman S, 2013, INT J COMPUT VISION, V103, P22, DOI 10.1007/s11263-012-0592-x
   Fonteijn HM, 2012, NEUROIMAGE, V60, P1880, DOI 10.1016/j.neuroimage.2012.01.062
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Hirsch MW, 2012, DIFFERENTIAL TOPOLOG
   Hyvarinen A., 2004, INDEPENDENT COMPONEN, V46
   Jack CR, 2010, LANCET NEUROL, V9, P119, DOI 10.1016/S1474-4422(09)70299-6
   Kuhn E, 2005, COMPUT STAT DATA AN, V49, P1020, DOI 10.1016/j.csda.2004.07.002
   LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876
   Singer J. D, 2003, APPL LONGITUDINAL DA
   Singh Nikhil, 2013, Inf Process Med Imaging, V23, P560, DOI 10.1007/978-3-642-38868-2_47
   Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100031
DA 2019-06-15
ER

PT S
AU Schulman, J
   Heess, N
   Weber, T
   Abbeel, P
AF Schulman, John
   Heess, Nicolas
   Weber, Theophane
   Abbeel, Pieter
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Gradient Estimation Using Stochastic Computation Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID POLICY-GRADIENT
AB In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs-directed acyclic graphs that include both deterministic functions and conditional probability distributions-and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.
C1 [Schulman, John; Heess, Nicolas; Weber, Theophane] Google DeepMind, London, England.
   [Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA.
RP Schulman, J (reprint author), Google DeepMind, London, England.
EM joschu@eecs.berkeley.edu; heess@google.com; theophane@google.com;
   pabbeel@eecs.berkeley.edu
CR Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Bengio Y., 2013, ARXIV13083432
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   Glasserman  P., 2003, MONTE CARLO METHODS, V53
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Greensmith E, 2004, J MACH LEARN RES, V5, P1471
   Gregor K., 2013, ARXIV13108499
   Griewank A, 2008, EVALUATING DERIVATIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2014, ARXIV14020480
   Kingma D. P., 2013, ARXIV13126114
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Mnih Andriy, 2014, ARXIV14020030
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   Munos R, 2006, J MACH LEARN RES, V7, P771
   Neal Radford M., 1990, LEARNING STOCHASTIC
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Pearl J., 2014, PROBABILISTIC REASON
   Ranganath R., 2013, ARXIV14010118
   Rezende D. J, 2014, ARXIV14014082
   Silver D., 2014, ICML
   SUTTON R.S., 1999, NIPS, V99, P1057
   Vlassis N, 2009, AUTON ROBOT, V27, P123, DOI 10.1007/s10514-009-9132-0
   Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wingate D., 2013, ARXIV13011299
   Wright S, 1999, NUMERICAL OPTIMIZATI, V2
   Zaremba Wojciech, 2015, ARXIV150500521
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102048
DA 2019-06-15
ER

PT S
AU Shah, A
   Ghahramani, Z
AF Shah, Amar
   Ghahramani, Zoubin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Parallel Predictive Entropy Search for Batch Global Optimization of
   Expensive Objective Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.
C1 [Shah, Amar] Univ Cambridge, Dept Engn, Cambridge, England.
   [Ghahramani, Zoubin] Univ Cambridge, Dept Engn, Cambridge, England.
RP Shah, A (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM as793@cam.ac.uk; zoubin@eng.cam.ac.uk
CR AHMAD IA, 1976, IEEE T INFORM THEORY, V22, P372, DOI 10.1109/TIT.1976.1055550
   Anderson B. S., 2000, ICML
   Azimi J., 2010, NIPS
   BACHE K., 2013, UCI MACHINE LEARNING
   Bochner S., 1959, LECT FOURIER INTEGRA
   Brochu E., 2009, TR200923 U BRIT COL
   Burrows EH, 2009, BIOTECHNOL PROGR, V25, P1009, DOI 10.1002/btpr.213
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Cunningham J. P., 2013, GAUSSIAN PROBABILITI
   Desautels T., 2012, ICML
   Ginsbourger D., 2011, DEALING ASYNCHRONICI
   Gutin G, 2002, DISCRETE APPL MATH, V117, P81, DOI 10.1016/S0166-218X(01)00195-0
   Hasbun J. E., 2008, CLASSICAL MECH MATLA
   Hennig P., 2012, JMLR
   Hernandez-Lobato Jose Miguel, 2014, NIPS
   Houlsby N., 2012, NIPS
   Lizotte D. J., 2008, THESIS
   Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590
   Minka T. P., 2001, THESIS
   Mockus J., 1989, BAYESIAN APPROACH GL
   Neal R M, 1995, THESIS
   Negoescu DM, 2011, INFORMS J COMPUT, V23, P346, DOI 10.1287/ijoc.1100.0417
   Rahimi A., 2007, NIPS
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seeger M., 2008, TECHNICAL REPORT
   Shah A., 2014, AISTATS
   Shekel J., 1971, INFORM SCI SYSTEMS
   Snoek J., 2015, ICML
   Snoek  Jasper, 2012, NIPS
   Srinivas N., 2010, ICML
   Wang GG, 2007, J MECH DESIGN, V129, P370, DOI 10.1115/1.2429697
   Westervelt E., 2007, CONTROL AUTOMATION S
   Ziemba W., 2006, STOCHASTIC OPTIMIZAT
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101064
DA 2019-06-15
ER

PT S
AU Sivakumar, V
   Banerjee, A
   Ravikumar, P
AF Sivakumar, Vidyashankar
   Banerjee, Arindam
   Ravikumar, Pradeep
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation
   with Sub-Exponential Designs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID RECOVERY; LASSO
AB We consider the problem of high-dimensional structured estimation with norm-regularized estimators, such as Lasso, when the design matrix and noise are drawn from sub-exponential distributions. Existing results only consider sub-Gaussian designs and noise, and both the sample complexity and non-asymptotic estimation error have been shown to depend on the Gaussian width of suitable sets. In contrast, for the sub-exponential setting, we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets, and the analysis holds for any norm. Further, using generic chaining, we show that the exponential width for any set will be at most root logp times the Gaussian width of the set, yielding Gaussian width based results even for the sub-exponential case. Further, for certain popular estimators, viz Lasso and Group Lasso, using a VC-dimension based analysis, we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.
C1 [Sivakumar, Vidyashankar; Banerjee, Arindam] Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
   [Ravikumar, Pradeep] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Sivakumar, V (reprint author), Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM sivakuma@cs.umn.edu; banerjee@cs.umn.edu; pradeepr@cs.utexas.edu
FU NSF [IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560,
   IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX This work was supported by NSF grants IIS-1447566, IIS-1447574,
   IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by
   NASA grant NNX12AQ39A.
CR Adamczak R, 2011, CONSTR APPROX, V34, P61, DOI 10.1007/s00365-010-9117-4
   Banerjee A., 2014, NIPS
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee S., 2014, NIPS
   Hsu D., 2014, ICML
   Koltchinskii V., 2013, ARXIV13123580
   Lecue G., 2014, ARXIV14012188
   Ledoux M., 1991, PROBABILITY BANACH S
   Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582
   Mendelson S, 2015, J ACM, V62, DOI 10.1145/2699439
   Mendelson S, 2012, J FUNCT ANAL, V262, P3775, DOI 10.1016/j.jfa.2012.01.027
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Oliveira Roberto Imbuzeiro, 2013, ARXIV13122903
   Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201
   Talagrand M., 2005, THE GENERIC CHAINING
   Tropp JA, 2015, APPL NUMER HARMON AN, P67, DOI 10.1007/978-3-319-19749-4_2
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102083
DA 2019-06-15
ER

PT S
AU Stollenga, MF
   Byeon, W
   Liwicki, M
   Schmidhuber, J
AF Stollenga, Marijn F.
   Byeon, Wonmin
   Liwicki, Marcus
   Schmidhuber, Juergen
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical
   Volumetric Image Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelise on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelise, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
C1 [Stollenga, Marijn F.; Byeon, Wonmin; Schmidhuber, Juergen] Ist Dalle Molle Sull Intelligenza Artificiale, Swiss AI Lab IDSIA, Lugano, Switzerland.
   [Stollenga, Marijn F.; Byeon, Wonmin; Schmidhuber, Juergen] SUPSI, Manno, Switzerland.
   [Stollenga, Marijn F.; Schmidhuber, Juergen] USI, Lugano, Switzerland.
   [Byeon, Wonmin; Liwicki, Marcus] Univ Kaiserslautern, Kaiserslautern, Germany.
   [Byeon, Wonmin] German Res Ctr Artificial Intelligence DFKI, Saarbrucken, Germany.
RP Stollenga, MF (reprint author), Ist Dalle Molle Sull Intelligenza Artificiale, Swiss AI Lab IDSIA, Lugano, Switzerland.
EM marijn@idsia.ch; wonmin.byeon@dfki.de
FU NASCENCE EU project [EU/FP7-ICT-317662]
FX We would like to thank Klaus Greff and Alessandro Giusti for their
   valuable discussions, and Jan Koutnik and Dan Ciresan for their useful
   feedback. We also thank the ISBI NEATBrain15 organisers [13] and the
   ISBI 2012 organisers, in particular Adrienne Mendrik and Ignacio
   Arganda-Carreras. Lastly we thank NVIDIA for generously providing us
   with hardware to perform our research. This research was funded by the
   NASCENCE EU project (EU/FP7-ICT-317662).
CR [Anonymous], 2012, IEEE INT S BIOM IM I
   Byeon W., 2015, CVPR
   Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502
   Chetlur  Sharan, 2014, ABS14100759 CORR
   Ciresan D., 2012, NIPS
   Ciresan  D., 2011, IJCNN
   Gers F. A., 1999, ICANN
   Graves A., 2007, ICANN
   Graves A., 2009, PAMI, V31
   Graves A, 2009, NIPS
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1991, THESIS
   Kass M., 1988, INT J COMPUTER VISIO
   Krizhevsky A., 2012, NIPS
   Liu T., 2014, J NEUROSCIENCE METHO
   Mendrik A. M., 2015, MRBRAINS CHALLENGE O
   Pham V., 2014, ICFHR
   Pizer S. M., 1987, COMPUT VISION GRAPH
   Sak H., 2014, P INTERSPEECH
   Srivastava N., 2014, J MACHINE LEARNING R
   Sutskever I., 2014, SEQUENCE SEQUENCE LE
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Wang L., 2015, NEUROIMAGE
   Weber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409626
   Zeiler M. D., 2013, ARXIV13112901CSCV NY
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913100010
DA 2019-06-15
ER

PT S
AU Sun, W
   Wang, ZR
   Liu, H
   Cheng, G
AF Sun, Wei
   Wang, Zhaoran
   Liu, Han
   Cheng, Guang
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Non-convex Statistical Optimization for Sparse Tensor Graphical Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID COVARIANCE ESTIMATION; SELECTION; DECOMPOSITIONS; CONVERGENCE; LASSO
AB We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.
C1 [Sun, Wei] Yahoo Labs, Sunnyvale, CA 94089 USA.
   [Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
   [Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.
RP Sun, W (reprint author), Yahoo Labs, Sunnyvale, CA 94089 USA.
EM sunweisurrey@yahoo-inc.com; zhaoran@princeton.edu; hanliu@princeton.edu;
   chengg@stat.purdue.edu
FU NSF CAREER Award [DMS1454377, DMS1151692]; NSF [IIS1408910, IIS1332109,
   DMS1418042]; NIH [R01MH102339, R01GM083084, R01HG06841]; Simons
   Fellowship in Mathematics; Indiana Clinical and Translational Sciences
   Institute; ONR [N00014-15-1-2331]
FX We would like to thank the anonymous reviewers for their helpful
   comments. Han Liu is grateful for the support of NSF CAREER Award
   DMS1454377, NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH
   R01GM083084, and NIH R01HG06841. Guang Cheng's research is sponsored by
   NSF CAREER Award DMS1151692, NSF DMS1418042, Simons Fellowship in
   Mathematics, ONR N00014-15-1-2331 and a grant from Indiana Clinical and
   Translational Sciences Institute.
CR Allen G. I., 2012, INT C ART INT STAT
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora S., 2015, ARXIV150300778
   Cai T., 2015, ANN STAT
   DAWID AP, 1981, BIOMETRIKA, V68, P265
   Fan JQ, 2009, ANN APPL STAT, V3, P521, DOI 10.1214/08-AOAS215
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Gupta A. K., 2000, MATRIX VARIATE DISTR
   He SY, 2014, J MULTIVARIATE ANAL, V128, P165, DOI 10.1016/j.jmva.2014.03.007
   Hoff PD, 2011, BAYESIAN ANAL, V6, P179, DOI 10.1214/11-BA606
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Ledoux M, 2011, CLASS MATH, P1
   Leng CL, 2012, J AM STAT ASSOC, V107, P1187, DOI 10.1080/01621459.2012.706133
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Rendle S., 2010, INT C WEB SEARCH DAT
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Sun J., 2015, ARXIV150406785
   Sun W., 2015, ARXIV150201425
   Sun W, 2013, J MACH LEARN RES, V14, P3419
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tsiligkaridis T, 2013, IEEE T SIGNAL PROCES, V61, P1743, DOI 10.1109/TSP.2013.2240157
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   Yin JX, 2012, J MULTIVARIATE ANAL, V107, P119, DOI 10.1016/j.jmva.2012.01.005
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
   Zahn JM, 2007, PLOS GENET, V3, P2326, DOI 10.1371/journal.pgen.0030201
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhao T, 2012, J MACH LEARN RES, V13, P1059
   Zhe S., 2015, 29 AAAI C ART INT
   Zhe S., 2015, INT C ART INT STAT
   Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187
NR 33
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913102068
DA 2019-06-15
ER

PT S
AU Tsiligkaridis, T
   Forsythe, KW
AF Tsiligkaridis, Theodoros
   Forsythe, Keith W.
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Adaptive Low-Complexity Sequential Inference for Dirichlet Process
   Mixture Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID BAYESIAN-INFERENCE
AB We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.
C1 [Tsiligkaridis, Theodoros; Forsythe, Keith W.] MIT, Lincoln Lab, Lexington, MA 02421 USA.
RP Tsiligkaridis, T (reprint author), MIT, Lincoln Lab, Lexington, MA 02421 USA.
EM ttsili@ll.mit.edu; forsythe@ll.mit.edu
CR ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871
   Batir N, 2008, ARCH MATH, V91, P554, DOI 10.1007/s00013-008-2856-9
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Daume H., 2007, C ART INT STAT
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   Fearnhead P, 2004, STAT COMPUT, V14, P11, DOI 10.1023/B:STCO.0000009418.04621.cd
   Kurihara K., 2006, ADV NEURAL INFORM PR
   Lin D., 2013, ADV NEURAL INFORM PR, V26, P395
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   NEAL RM, 1992, FUND THEOR, V50, P197
   Rasmussen CE, 2000, ADV NEUR IN, V12, P554
   Tsiligkaridis T, 2015, IEEE INT WORKS MACH
   Tzikas DG, 2008, IEEE SIGNAL PROC MAG, V25, P131, DOI 10.1109/MSP.2008.929620
   Wang LM, 2011, J COMPUT GRAPH STAT, V20, P196, DOI 10.1198/jcgs.2010.07081
NR 14
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103073
DA 2019-06-15
ER

PT S
AU Ulrich, K
   Carlson, DE
   Dzirasa, K
   Carin, L
AF Ulrich, Kyle
   Carlson, David E.
   Dzirasa, Kafui
   Carin, Lawrence
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI GP Kernels for Cross-Spectrum Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB Multi-output Gaussian processes provide a convenient framework for multi-task problems. An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels. Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework. In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel. This new, flexible kernel represents both the power and phase relationship between multiple observation channels. We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel. Results are presented for measured multi-region electrophysiological data.
C1 [Ulrich, Kyle; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
   [Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Durham, NC 27706 USA.
   [Carlson, David E.] Columbia Univ, Dept Stat, New York, NY 10027 USA.
RP Ulrich, K (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM kyle.ulrich@duke.edu; david.edwin.carlson@gmail.com;
   kafui.dzirasa@duke.edu; lcarin@duke.edu
OI Carlson, David/0000-0003-1005-6385
FU ARO; DARPA; ONR; DOE; NGA
FX The research reported here was funded in part by ARO, DARPA, DOE, NGA
   and ONR.
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Beal M. J., THESIS
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Dietrich CR, 1997, SIAM J SCI COMPUT, V18, P1088, DOI 10.1137/S1064827592240555
   Dzirasa K, 2006, J NEUROSCI, V26, P10577, DOI 10.1523/JNEUROSCI.1767-06.2006
   Dzirasa K, 2011, J NEUROSCI METH, V195, P36, DOI 10.1016/j.jneumeth.2010.11.014
   Gallager R. G., 2008, PRINCIPLES DIGITAL C, P229
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Goovaerts P, 1997, GEOSTATISTICS NATURA
   Gregoriou GG, 2009, SCIENCE, V324, P1207, DOI 10.1126/science.1171402
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Lloyd J. R., 2014, AAAI
   MacKay D. J. C., 1997, TECHNICAL REPORT
   Pfaff D., 2008, CONCEPTS MECH GEN CE
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sauseng P, 2008, NEUROSCI BIOBEHAV R, V32, P1001, DOI 10.1016/j.neubiorev.2008.03.014
   Seeger M., 2005, WORKSH ART INT STAT, P333
   Sweeney-Reed CM, 2014, ELIFE, V3, DOI 10.7554/eLife.05352
   Tucker MA, 2006, NEUROBIOL LEARN MEM, V86, P241, DOI 10.1016/j.nlm.2006.03.005
   Ulrich K., 2014, NIPS
   WELCH PD, 1967, IEEE T ACOUST SPEECH, VAU15, P70, DOI 10.1109/TAU.1967.1161901
   Wilson A., 2014, NIPS
   Wilson A. G., 2012, ICML
   Wilson A. G., 2013, ICML
   Wilson Andrew Gordon, 2014, THESIS
   Yang Z., 2015, AISTATS
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103004
DA 2019-06-15
ER

PT S
AU Vovk, V
   Petej, I
   Fedorova, V
AF Vovk, Vladimir
   Petej, Ivan
   Fedorova, Valentina
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Large-scale probabilistic predictors with and without guarantees of
   validity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.
C1 [Vovk, Vladimir; Petej, Ivan] Univ London, Dept Comp Sci, Royal Holloway, Egham, Surrey, England.
   [Fedorova, Valentina] Yandex, Moscow, Russia.
RP Vovk, V (reprint author), Univ London, Dept Comp Sci, Royal Holloway, Egham, Surrey, England.
EM volodya.vovk@gmail.com; ivan.petej@gmail.com; alushaf@gmail.com
FU EPSRC [EP/K033344/1]; AFOSR
FX We are grateful to the conference reviewers for numerous helpful
   comments and observations, to Vladimir Vapnik for sharing his ideas
   about exploiting synergy between different learning algorithms, and to
   participants of the conference Machine Learning: Prospects and
   Applications (October 2015, Berlin) for their questions and comments.
   The first author has been partially supported by EPSRC (grant
   EP/K033344/1) and AFOSR (grant "Semantic Completions"). The second and
   third authors are grateful to their home institutions for funding their
   trips to Montreal.
CR AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Barlow R, 1972, STAT INFERENCE ORDER
   Caruana R., 2006, ICML 2006 P 23 INT C, P161, DOI DOI 10.1145/1143844.1143865
   Cormen T H., 2009, INTRO ALGORITHMS
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10
   Jiang Xiaoqian, 2011, AMIA Jt Summits Transl Sci Proc, V2011, P16
   Lambrou A., 2012, ARTIFICIAL INTELLI 2, V382, P182, DOI DOI 10.1007/978-3-642-33412-2_19
   LEE CIC, 1983, ANN STAT, V11, P467, DOI 10.1214/aos/1176346153
   Murphy A. H., 1973, Journal of Applied Meteorology, V12, P595, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2
   MURRAY GD, 1983, BIOMETRIKA, V70, P490
   Platt JC, 2000, ADV NEUR IN, P61
   Vapnik Vladimir N., 2015, COMMUNICATION   1006
   Vovk V., 2005, ALGORITHMIC LEARNING
   Vovk V, 2015, LECT NOTES COMPUT SC, V9300, P307, DOI 10.1007/978-3-319-23534-9_20
   Vovk Vladimir, 2015, TECHNICAL REPORT
   Vovk Vladimir, 2014, PROCEEDINGS OF THE T, P829
   Zadrozny B., 2001, P 18 INT C MACH LEAR, P609
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913101065
DA 2019-06-15
ER

PT S
AU Waggoner, B
   Frongillo, R
   Abernethy, J
AF Waggoner, Bo
   Frongillo, Rafael
   Abernethy, Jacob
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI A Market Framework for Eliciting Private Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
ID DIFFERENTIAL PRIVACY
AB We propose a mechanism for purchasing information from a sequence of participants. The participants may simply hold data points they wish to sell, or may have more sophisticated information; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's future prediction on a test set. The mechanism, which draws on the principles of prediction markets, has a bounded budget and minimizes generalization error for Bregman divergence loss functions. We then show how to modify this mechanism to preserve the privacy of participants' information: At any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately aggregated.
C1 [Waggoner, Bo] Harvard SEAS, Cambridge, MA 02138 USA.
   [Frongillo, Rafael] Univ Colorado, Boulder, CO 80309 USA.
   [Abernethy, Jacob] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Waggoner, B (reprint author), Harvard SEAS, Cambridge, MA 02138 USA.
EM bwaggoner@fas.harvard.edu; raf@colorado.edu; jabernet@umich.edu
FU US National Science Foundation under CAREER [IIS-1453304, IIS-1421391]
FX J. Abernethy acknowledges the generous support of the US National
   Science Foundation under CAREER Grant IIS-1453304 and Grant IIS-1421391.
CR Abernethy J. D., 2011, ADV NEURAL INFORM PR, V24, P2600
   Abernethy Jacob, 2013, ACM T EC COMPUTATION, V1
   Abernethy Jacob, 2014, P 15 ACM C EC COMP, P395
   Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009
   Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626
   Chen Y, 2010, P 11 ACM C EL COMM, P189
   Chen  Y., 2007, 23 C UNC ART INT, P49
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   Dwork Cynthia, 2014, FDN TRENDS THEORETIC
   Hall R, 2013, J MACH LEARN RES, V14, P703
   Hanson R, 2003, INFORM SYST FRONT, V5, P107, DOI 10.1023/A:1022058209073
   Hanson R, 2002, ENTREPRENEURIAL EC B, P79
   Hanson R., 2007, J PREDICTION MARKETS, V1, P3
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   Othman Abraham, 2011, P 2 C AUCT MARK MECH, P19
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Storkey Amos J., 2011, P 14 INT C ART INT S, P716
   Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321
   Wolfers J., 2006, TECHNICAL REPORT
   Yiling Chen, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P72, DOI 10.1007/978-3-642-25510-6_7
   Zawadzki Erik, 2015, P 29 AAAI C ART INT
   Zhang L, 2013, P 30 INT C MACH LEAR, P621
   Zhang Lijun, 2012, AAAI
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103033
DA 2019-06-15
ER

PT S
AU Watter, M
   Springenberg, JT
   Boedecker, J
   Riedmiller, M
AF Watter, Manuel
   Springenberg, Jost Tobias
   Boedecker, Joschka
   Riedmiller, Martin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Embed to Control: A Locally Linear Latent Dynamics Model for Control
   from Raw Images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.
C1 [Watter, Manuel; Springenberg, Jost Tobias; Boedecker, Joschka] Univ Freiburg, Freiburg, Germany.
   [Riedmiller, Martin] Google DeepMind, London, England.
RP Watter, M (reprint author), Univ Freiburg, Freiburg, Germany.
EM watterm@cs.uni-freiburg.de; springj@cs.uni-freiburg.de;
   jboedeck@cs.uni-freiburg.de; riedmiller@google.com
FU DFG [SPP1597]; BrainLinks-BrainTools Cluster of Excellence [EXC 1086];
   State Graduate Funding Program of Baden-Wurttemberg
FX We thank A. Radford, L. Metz, and T. DeWolf for sharing code, as well as
   A. Dosovitskiy for useful discussions. This work was partly funded by a
   DFG grant within the priority program "Autonomous learning" (SPP1597)
   and the BrainLinks-BrainTools Cluster of Excellence (grant number EXC
   1086). M. Watter is funded through the State Graduate Funding Program of
   Baden-Wurttemberg.
CR Bayer Justin, 2014, NIPS 2014 WORKSH ADV
   Bohmer W., 2015, KI KUNSTLICHE INTELL
   Cohen T. S., 2015, ICLR
   Dinh L., 2015, ABS14108516 CORR
   Dosovitskiy A., 2015, P CVPR
   Gregor  K., 2015, P ICML
   Hinton G., 2011, P ICANN
   Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC
   Jordan M. I., 1999, MACHINE LEARNING
   Kingma  D., 2014, P ICLR
   Kingma D. P., 2015, P ICLR
   Lange S., 2010, P IJCNN
   Langford J., 2009, ICML
   Levine S., 2013, P NIPS
   Levine S, 2015, ABS150400702 CORR
   Li W., 2004, P ICINCO
   Matsubara T., 2014, UAI
   Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pan Y., 2014, P NIPS
   Rezende Danilo Jimenez, 2014, P ICML
   Stengel R. F., 1994, OPTIMAL CONTROL ESTI
   Sutton R., 1998, INTRO REINFORCEMENT
   Tanaka K, 1996, IEEE T FUZZY SYST, V4, P1, DOI 10.1109/91.481840
   Tassa Y., 2008, P NIPS
   Taylor G. W., 2010, P CVPR
   Todorov  E., 2005, ACC
   Toussaint M., 2009, P ICML
   van Hoof H., 2015, P AISTATS
   Wahlstrom N., 2015, ABS50202251 CORR
   West M., 1997, BAYESIAN FORECASTING
   Zeiler M. D., 2010, CVPR
NR 32
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103002
DA 2019-06-15
ER

PT S
AU Zhang, HS
   Zhou, Y
   Liang, YB
AF Zhang, Huishuai
   Zhou, Yi
   Liang, Yingbin
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Analysis of Robust PCA via Local Incoherence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB We investigate the robust PCA problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming Principal Component Pursuit (PCP). In contrast to previous studies that assume the support of the error matrix is generated by uniform Bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities. We characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by PCP is guaranteed. Such a refined analysis of robust PCA captures how robust each entry of the low rank matrix combats error corruption. In order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.
C1 [Zhang, Huishuai; Zhou, Yi; Liang, Yingbin] Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.
RP Zhang, HS (reprint author), Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.
EM hzhan23@syr.edu; yzhou35@syr.edu; yliang06@syr.edu
CR Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chen Y., 2013, ARXIV13062979
   Chen YD, 2015, IEEE T INFORM THEORY, V61, P2909, DOI 10.1109/TIT.2015.2415195
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Chen Yudong, 2012, ADV NEURAL INFORM PR, V25, P2204
   Ganesh A, 2010, IEEE INT SYMP INFO, P1513, DOI 10.1109/ISIT.2010.5513538
   Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   Li XD, 2013, CONSTR APPROX, V37, P73, DOI 10.1007/s00365-012-9176-9
   Lin Z., 2010, ARXIV10095055
   Oymak S., 2011, ARXIV11045186
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Salakhutdinov R., 2010, ADV NEURAL INFORM PR, P2056
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Vershynin  Roman, 2010, ARXIV10113027
NR 20
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103056
DA 2019-06-15
ER

PT S
AU Zoghi, M
   Karnin, Z
   Whiteson, S
   de Rijke, M
AF Zoghi, Masrour
   Karnin, Zohar
   Whiteson, Shimon
   de Rijke, Maarten
BE Cortes, C
   Lawrence, ND
   Lee, DD
   Sugiyama, M
   Garnett, R
TI Copeland Dueling Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 29th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 07-12, 2015
CL Montreal, CANADA
AB A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K-2 log T) without requiring such assumptions. Our results offer the best of both worlds: O(K log T) bounds without restrictive assumptions.
C1 [Zoghi, Masrour; de Rijke, Maarten] Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.
   [Karnin, Zohar] Yahoo Labs, New York, NY USA.
   [Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England.
RP Zoghi, M (reprint author), Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.
EM m.zoghi@uva.nl; zkarnin@yahoo-inc.com; shimon.whiteson@cs.ox.ac.uk;
   derijke@uva.nl
FU Amsterdam Data Science; Dutch national program COMMIT; Elsevier;
   European Community [312827]; ESF Research Network Program ELIAS; Royal
   Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project;
   Microsoft Research Ph.D. program; Netherlands eScience Center
   [027.012.105]; Netherlands Institute for Sound and Vision; Netherlands
   Organisation for Scientific Research (NWO) [727.011.005, 612.001.116,
   HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15]; Yahoo!
   Faculty Research and Engagement Program; Yandex
FX We would like to thank Nir Ailon and Ulle Endriss for helpful
   discussions. This research was supported by Amsterdam Data Science, the
   Dutch national program COMMIT, Elsevier, the European Community's
   Seventh Framework Programme (FP7/2007-2013) under grant agreement nr
   312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Royal
   Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project,
   the Microsoft Research Ph.D. program, the Netherlands eScience Center
   under project number 027.012.105, the Netherlands Institute for Sound
   and Vision, the Netherlands Organisation for Scientific Research (NWO)
   under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013,
   612.066.930, CI-14-25, SH-322-15, the Yahoo! Faculty Research and
   Engagement Program, and Yandex. All content represents the opinion of
   the authors, which is not necessarily shared or endorsed by their
   respective employers and/or sponsors.
CR Ailon N., 2014, ICML
   Altman A., 2008, JAIR
   Bartok Gabor, 2012, ICML
   Bubeck S., 2011, JMLR, V12
   Bull A. D., 2011, JMLR, V12
   Busa-Fekete R., 2013, ICML
   Busa-Fekete Robert, 2014, AAAI
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   deFreitas N., 2012, ICML
   Dudik M., 2015, COLT
   Hofmann K, 2013, INFORM RETRIEVAL, V16, P63, DOI 10.1007/s10791-012-9197-9
   Joachims  T., 2002, KDD
   Kleinberg R., 2008, STOC
   Kohavi R., 2013, KDD
   Li L., 2015, WSDM
   Manning C. D., 2008, INTRO INFORM RETRIEV
   Munos R., 2011, NIPS
   Negahban S., 2012, NIPS
   Piccolboni A., 2001, COLT
   Schulze M, 2011, SOC CHOICE WELFARE, V36, P267, DOI 10.1007/s00355-010-0475-4
   Schuth A., 2014, CIKM
   Srinivas N., 2010, ICML
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Urvoy T., 2013, ICML
   Valko M., 2013, ICML
   Yue Y., 2011, ICML
   Yue Y., 2012, J COMPUTER SYSTEM SC, V78
   Yue Y., 2009, ICML
   Zoghi M., 2015, WSDM
   Zoghi M., 2014, ICML
   Zoghi M., 2014, WSDM
   2010, PREFERENCE LEARNING, P1
NR 32
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2015
VL 28
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL4WJ
UT WOS:000450913103061
DA 2019-06-15
ER

PT S
AU Ammar, W
   Dyer, C
   Smith, NA
AF Ammar, Waleed
   Dyer, Chris
   Smith, Noah A.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Conditional Random Field Autoencoders for Unsupervised Structured
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate connections to traditional autoencoders, posterior regularization, and multi-view learning. We then show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training the proposed model can be substantially more efficient than a comparable feature-rich baseline.
C1 [Ammar, Waleed; Dyer, Chris; Smith, Noah A.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Ammar, W (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM wammar@cs.cmu.edu; cdyer@cs.cmu.edu; nasmith@cs.cmu.edu
FU U.S. Army Research Laboratory; U.S. Army Research Office
   [W911NF-10-1-0533]
FX We thank Brendan O'Connor, Dani Yogatama, Jeffrey Flanigan, Manaal
   Faruqui, Nathan Schneider, Phil Blunsom and the anonymous reviewers for
   helpful suggestions. We also thank Taylor Berg-Kirkpatrick for providing
   his implementation of the POS induction baseline, and Phil Blunsom for
   sharing POS induction evaluation scripts. This work was sponsored by the
   U.S. Army Research Laboratory and the U.S. Army Research Office under
   contract/grant number W911NF-10-1-0533. The statements made herein are
   solely the responsibility of the authors.
CR Bailey T. L., 1995, MACHINE LEARNING
   Bellare K., 2009, P UAI
   Berg- Kirkpatrick T., 2010, P NAACL
   Blunsom P., 2006, P P ACL
   Brown P. F, 1993, COMPUTATIONAL LINGUI
   Brown P. F., 1992, COMPUTATIONAL LINGUI
   Buchholz S., 2006, CONLL X
   Collobert Ronan, 2008, P ICML
   Daume III H., 2009, P ICML MONTR, P209
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Druck G., 2010, P ICML
   Duchi J., 2011, JMLR
   Dyer C., 2011, P ACL HLT
   Dyer C., 2010, P ACL
   Dyer Chris, 2013, P NAACL
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Gao Q., 2008, P ACL WORKSH
   Haghighi A., 2006, P NAACL HLT
   Jelinek F., 1997, STAT METHODS SPEECH
   Johnson M., 2007, P EMNLP
   Koehn P., 2003, P NAACL
   Koehn P., 2010, STAT MACHINE TRANSLA
   Lafferty J., 2001, P ICML
   Liang P, 2005, THESIS
   Lin C. - C., 2014, 1 WORKSH COMP APPR C
   Lukashin AV, 1998, NUCLEIC ACIDS RES, V26, P1107, DOI 10.1093/nar/26.4.1107
   Manning C. D., 2004, P ACL
   Merialdo B., 1994, COMP LING
   Minka T, 2005, MSRTR2005144
   Mnih A., 2012, P ICML
   Nivre J., 2007, P CONLL
   Och F. J., 2003, COMPUTATIONAL LINGUI
   Papineni  K., 2002, P ACL
   Petrov S., 2012, P LREC MAY
   Poon H., 2008, P EMNLP
   Reddy S., 2009, P NAM ENT WORKSH
   Rosenberg A., 2007, EMNLP CONLL
   Smith N. A., 2005, P ACL
   Socher R., 2010, NIPS WORKSH
   Stoyanov V., 2011, P AISTATS
   Suzuki J., 2008, P ACL
   Swier R., 2004, P EMNLP
   Vickrey D., 2010, P ICML
   Vincent P., 2008, P ICML
   Vogel Stephan, 1996, P COLING
   Weber M., 2000, UNSUPERVISED LEARNIN
   Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161
NR 47
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101015
DA 2019-06-15
ER

PT S
AU Archer, E
   Koster, U
   Pillow, J
   Macke, JH
AF Archer, Evan
   Koster, Urs
   Pillow, Jonathan
   Macke, Jakob H.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Low-dimensional models of neural population activity in sensory cortical
   circuits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SPACE; INFORMATION
AB Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations, for which inference scales linearly in both population size and recording duration. We test this model to multi-channel recordings from primary visual cortex and show that it accounts for neural tuning properties as well as cross-neural correlations.
C1 [Archer, Evan; Macke, Jakob H.] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Archer, Evan; Macke, Jakob H.] Bernstein Ctr Computat Neurosci, Tubingen, Germany.
   [Koster, Urs] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.
   [Pillow, Jonathan] Princeton Univ, Dept Psychol, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Archer, E (reprint author), Max Planck Inst Biol Cybernet, Tubingen, Germany.
EM evan.archer@tuebingen.mpg.de; urs@nervanasys.com; pillow@princeton.edu;
   jakob@tuebingen.mpg.de
FU German Federal Ministry of Education and Research (BMBF, Bernstein
   Center Tubingen) [FKZ: 01GQ1002]; Max Planck Society; National Eye
   Institute [EY019965]
FX We are thankful to Arnulf Graf and the co-authors of [26] for sharing
   the data used in Fig. 5, and to Memming Park for comments on the
   manuscript. JHM and EA were funded by the German Federal Ministry of
   Education and Research (BMBF; FKZ: 01GQ1002, Bernstein Center Tubingen)
   and the Max Planck Society, and UK by National Eye Institute grant
   #EY019965. Collaboration between EA, JP and JHM initiated at the 'MCN'
   Course at the Marine Biological Laboratory, Woods Hole.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fairhall A, 2014, CURR OPIN NEUROBIOL, V25, pIX, DOI 10.1016/j.conb.2014.02.001
   Ghahramani Z., 1996, U TORONTO TECH REPOR, V6
   Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Khan M. Emtiyaz, 2013, P ICML
   Koster U, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003684
   Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173
   Macke J. H., 2008, ADV NEURAL INFORM PR, V20, P969
   Macke J. H., 2012, ADV NEURAL INFO P SY, V24
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Mountcastle V., 1957, J NEUROPHYSIOL
   Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x
   Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2006, J VISION, V6, P414, DOI 10.1167/6.4.9
   Reich DS, 2001, SCIENCE, V294, P2566, DOI 10.1126/science.1065839
   Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021
   Sharpee TO, 2013, ANNU REV NEUROSCI, V36, P103, DOI 10.1146/annurev-neuro-062012-170253
   Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622
   Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620
   Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455
   Vidne M., 2011, J COMPUT NEUROSCI
   Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008
   ZammitMangion A, 2011, NEURAL COMPUT, V23, P1967, DOI 10.1162/NECO_a_00156
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100042
DA 2019-06-15
ER

PT S
AU Avron, H
   Nguyen, HL
   Woodruff, DP
AF Avron, Haim
   Nguyen, Huy L.
   Woodruff, David P.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Subspace Embeddings for the Polynomial Kernel
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first fast oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel without explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.
C1 [Avron, Haim] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Nguyen, Huy L.] Univ Calif Berkeley, Simons Inst, Berkeley, CA 94720 USA.
   [Woodruff, David P.] IBM Almaden Res Ctr, San Jose, CA 95120 USA.
RP Avron, H (reprint author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM haimav@us.ibm.com; hlnguyen@cs.princeton.edu; dpwoodru@us.ibm.com
CR Avron H., 2013, ADV NEURAL INFORM PR
   CARTER JL, 1979, J COMPUT SYST SCI, V18, P143, DOI 10.1016/0022-0000(79)90044-8
   Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6
   Clarkson K., 2009, P 41 ANN ACM S THEOR
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Hamid R., 2014, P 31 INT C MACH LEAR
   JOLLIFFE IT, 1982, APPL STAT-J ROY ST C, V31, P300, DOI 10.2307/2348005
   Kannan R., 2014, P 27 C LEARN THEOR C
   Kar  P., 2012, P 15 INT C ART INT S
   Le Q., 2013, P 30 INT C MACH LEAR
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Nelson J., 2013, 54 IEEE ANN S FDN CO
   Pagh R, 2013, ACM T COMPUT THEORY, V5, DOI 10.1145/2493252.2493254
   Patrascu M, 2012, J ACM, V59, DOI 10.1145/2220357.2220361
   Pham N., 2013, P 19 ACM SIGKDD INT, P239
   Rahimi A, 2007, ADV NEURAL INFORM PR
NR 17
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100019
DA 2019-06-15
ER

PT S
AU Ba, LJ
   Caruana, R
AF Ba, Lei Jimmy
   Caruana, Rich
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Do Deep Nets Really Need to be Deep?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.
C1 [Ba, Lei Jimmy] Univ Toronto, Toronto, ON, Canada.
   [Caruana, Rich] Microsoft Res, Redmond, WA USA.
RP Ba, LJ (reprint author), Univ Toronto, Toronto, ON, Canada.
EM jimmy@psi.utoronto.ca; rcaruana@microsoft.com
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Abdel-Hamid Ossama, 2013, INTERSPEECH, V2013
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dauphin YN, 2013, ARXIV13013583
   Eigen D., 2013, ARXIV13121847
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Goodfellow Ian, 2013, JMLR W CP, P1319
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Krizhevsky A., 2009, TECH REP
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Li Deng, 2013, ICASSP 2013
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Seide F., 2011, P INTERSPEECH, P437, DOI DOI 10.1.1.368.3047
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xue J., 2013, P INTERSPEECH
   Zeiler MD, 2013, ARXIV201313013557
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102045
DA 2019-06-15
ER

PT S
AU Bareinboim, E
   Pearl, J
AF Bareinboim, Elias
   Pearl, Judea
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Transportability from Multiple Environments with Limited Experiments:
   Completeness Results
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This paper addresses the problem of mz-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of mz-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the mz-transportability class.
C1 [Bareinboim, Elias; Pearl, Judea] Univ Calif Los Angeles, Comp Sci, Los Angeles, CA 90095 USA.
RP Bareinboim, E (reprint author), Univ Calif Los Angeles, Comp Sci, Los Angeles, CA 90095 USA.
EM eb@cs.ucla.edu; judea@cs.ucla.edu
CR Bareinboim E., 2013, P 16 INT C ART INT S, P135
   Bareinboim E, 2013, ADV NEURAL INFORM PR, V26, P136
   Bareinboim E, 2013, P 27 AAAI C ART INT, P95
   Bareinboim E, 2012, P 26 AAAI C ART INT, P698
   Bareinboim E, 2013, J CAUSAL INFERENCE, V1, P107, DOI 10.1515/jci-2012-0004
   Campbell D. T., 1963, EXPT QUASIEXPERIMENT
   Daume H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Hedges L. V., 1985, STAT METHODS METAANA
   Lee S., 2013, AAAI, P583
   Lee S., 2013, P 29 C UNC ART INT U, P361
   Manski C. F, 2007, IDENTIFICATION PREDI
   Morgan SL, 2007, ANAL METHOD SOC RES, P1, DOI 10.1017/CBO9780511804564
   Pearl  J., 2011, P 25 AAAI C ART INT, P247
   Pearl J., 2000, CAUSALITY MODELS REA
   Scholkopf B., 2012, P 29 INT C MACH LEAR, P1255
   Shadish W. R., 2002, EXPT QUASIEXPERIMENT
   Shpitser I, 2006, P 21 NAT C ART INT, V2, P1219
   Spirtes P., 2000, CAUSATION PREDICTION
   Storkey A, 2009, NEURAL INF PROCESS S, P3
   Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567
   Tian J., 2002, THESIS
   Zhang K., 2013, P 30 INT C MACH LEAR, V28
NR 22
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102097
DA 2019-06-15
ER

PT S
AU Besbes, O
   Gur, Y
   Zeevi, A
AF Besbes, Omar
   Gur, Yonatan
   Zeevi, Assaf
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID REGRET
AB In a multi-armed bandit (MAB) problem a gambler needs to choose at each round of play one of K arms, each characterized by an unknown reward distribution. Reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play T. To do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length T. This problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings. In this paper, we focus on a MAB formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability. We fully characterize the (regret) complexity of this class of MAB problems by establishing a direct link between the extent of allowable reward "variation" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic MAB frameworks.
C1 [Besbes, Omar; Zeevi, Assaf] Columbia Univ, New York, NY 10027 USA.
   [Gur, Yonatan] Stanford Univ, Stanford, CA 94305 USA.
RP Besbes, O (reprint author), Columbia Univ, New York, NY 10027 USA.
EM ob2105@columbia.edu; ygur@stanford.edu; assaf@gsb.columbia.edu
CR Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Awerbuch  B., 2004, P 36 ANN ACM S THEOR, P45
   Azar M. G., 2014, ARXIV14020562
   Bergemann D, 2005, RAND J ECON, V36, P719
   Bergemann D, 1996, ECONOMETRICA, V64, P1125, DOI 10.2307/2171959
   Berry D., 1985, BANDIT PROBLEMS SEQU
   Bertsimas D, 2000, OPER RES, V48, P80, DOI 10.1287/opre.48.1.80.12444
   Besbes O., 2014, WORKING PAPER
   Blackwell D., 1956, PAC J MATH, V6, P1, DOI DOI 10.2140/PJM.1956.6.1
   Caro F, 2007, MANAGE SCI, V53, P276, DOI 10.1287/mnsc.1060.0613
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Garivier A, 2011, LECT NOTES ARTIF INT, V6925, P174, DOI 10.1007/978-3-642-24412-4_16
   Gittins J., 1989, MULTIARMED BANDIT AL
   Gittins J. C., 1974, DYNAMIC ALLOCATION I
   GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148
   Guha S, 2007, ANN IEEE SYMP FOUND, P483, DOI 10.1109/FOCS.2007.23
   Hannan J., 1957, CONTRIBUTIONS THEORY, V3
   Hartland C., 2006, NIPS 2006 WORKSH ONL
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19
   Pandey S., 2007, SIAM INT C DATA MINI
   PAPADIMITRIOU CH, 1994, STRUCT COMPL TH CONF, P318, DOI 10.1109/SCT.1994.315792
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Slivkins A., 2008, COLT, P343
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   WHITTLE P, 1981, ANN PROBAB, V9, P284, DOI 10.1214/aop/1176994469
   Whittle P., 1988, J APPL PROB A, P287, DOI [10.2307/3214163, DOI 10.2307/3214163]
   ZELEN M, 1969, J AM STAT ASSOC, V64, P131, DOI 10.2307/2283724
NR 32
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101049
DA 2019-06-15
ER

PT S
AU Buesing, L
   Machado, TA
   Cunningham, JP
   Paninski, L
AF Buesing, Lars
   Machado, Timothy A.
   Cunningham, John P.
   Paninski, Liam
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Clustered factor analysis of multineuronal spike data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID CONNECTIVITY; MODEL
AB High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.
C1 [Buesing, Lars; Machado, Timothy A.; Cunningham, John P.; Paninski, Liam] Columbia Univ, Ctr Theoret Neurosci, Dept Stat, New York, NY 10027 USA.
   [Buesing, Lars; Machado, Timothy A.; Cunningham, John P.; Paninski, Liam] Columbia Univ, Grossman Ctr Stat Mind, New York, NY USA.
   [Machado, Timothy A.] Columbia Univ, Howard Hughes Med Inst, New York, NY 10032 USA.
   [Machado, Timothy A.] Columbia Univ, Dept Neurosci, New York, NY USA.
RP Buesing, L (reprint author), Columbia Univ, Ctr Theoret Neurosci, Dept Stat, New York, NY 10027 USA.
EM lars@stat.columbia.edu; cunningham@stat.columbia.edu;
   liam@stat.columbia.edu
FU Simons Foundation (SCGB) [325171, 325233]; Grossman Center at Columbia
   University; Gatsby Charitable Trust; ARO [MURI W911NF-12-1-0594]; ONR
   [vN00014-14-1-0243]; DARPA [W91NF-14-1-0269]; NSF CAREER award
FX This work was supported by Simons Foundation (SCGB#325171 and
   SCGB#325233), Grossman Center at Columbia University, and Gatsby
   Charitable Trust as well as grants MURI W911NF-12-1-0594 from the ARO,
   vN00014-14-1-0243 from the ONR, W91NF-14-1-0269 from DARPA and an NSF
   CAREER award (L.P.).
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Buesing Lars, 2012, NIPS, P1691
   Collins M., 2001, ADV NEURAL INFORM PR, V13, P23
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138
   Ghahramani Z, 1996, CRGTR961 U TOR
   Jones LM, 2007, P NATL ACAD SCI USA, V104, P18772, DOI 10.1073/pnas.0705546104
   Keshri S., 2013, ARXIV13093724
   Khan M. E., 2013, INT C MACH LEARN, P951
   Machado Timothy A., 2014, 79 COLD SPRING HARB
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Mishchenko Y, 2011, ANN APPL STAT, V5, P1229, DOI 10.1214/09-AOAS303
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Okatan M, 2005, NEURAL COMPUT, V17, P1927, DOI 10.1162/0899766054322973
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pnevmatikakis E. A., 2014, ARXIV E PRINTS
   Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622
   Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Yu Byron M., 2008, NIPS, P1881
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101010
DA 2019-06-15
ER

PT S
AU Chakrabarty, D
   Jain, P
   Kothari, P
AF Chakrabarty, Deeparnab
   Jain, Prateek
   Kothari, Pravesh
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Provable Submodular Minimization using Wolfe's Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB submodular function minimization ( SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time [10, 11]. However, these algorithms are typically not practical. In 1976, Wolfe [21] proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige [3] showed how Wolfe's algorithm can be used for SFM. For general submodular functions, this Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance.
   Despite its good practical performance, very little is known about Wolfe's minimum norm algorithm theoretically. To our knowledge, the only result is an exponential time analysis due to Wolfe [21] himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns an O(1/t)-approximate solution to the min-norm point on any polytope. We also prove a robust version of Fujishige's theorem which shows that an O(1/n(2)) approximate solution to the min-norm point on the base polytope implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for unconstrained submodular function minimization.
C1 [Chakrabarty, Deeparnab; Jain, Prateek; Kothari, Pravesh] Microsoft Res, 9 Lavelle Rd, Bangalore 560001, Karnataka, India.
   [Kothari, Pravesh] Univ Texas Austin, Austin, TX 78712 USA.
RP Chakrabarty, D (reprint author), Microsoft Res, 9 Lavelle Rd, Bangalore 560001, Karnataka, India.
CR Bach Francis R., 2010, ABS10104207 CORR
   Edmonds J., 1970, COMBINATORIAL STRUCT, P69
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   FUJISHIGE S, 1980, MATH OPER RES, V5, P186, DOI 10.1287/moor.5.2.186
   Fujishige S, 2011, PAC J OPTIM, V7, P3
   Fujishige Satoru, 1984, MATH PROGRAMMING STU
   Fujishige Satoru, 2006, MINIMUM NORM POINT A
   GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273
   Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096
   Iwata S, 2000, INT OFFSHORE POLAR E, P97, DOI 10.1145/335305.335317
   IWATA S, 2009, SODA 09, P1230
   Iyer R. K., 2013, ADV NEURAL INFORM PR, P2436
   Iyer Rishabh, 2013, ABS13112110 CORR
   Iyer RK, 2013, P 30 INT C MACH LEAR, V3, P855
   Jegelka  S., 2013, ADV NEURAL INFORM PR, P1313
   Jegelka S., 2011, ADV NEURAL INFORM PR, P460
   Kohli P, 2010, STUD COMPUT INTELL, V285, P51
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989
   Stobbe  P., 2010, P NIPS, P2208
   WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100100
DA 2019-06-15
ER

PT S
AU Chatterjee, S
   Chen, S
   Banerjee, A
AF Chatterjee, Soumyadeep
   Chen, Sheng
   Banerjee, Arindam
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Generalized Dantzig Selector: Application to the k-support norm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SHRINKAGE
AB We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.
C1 [Chatterjee, Soumyadeep; Chen, Sheng; Banerjee, Arindam] Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Chatterjee, S (reprint author), Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM chatter@cs.umn.edu; shengc@cs.umn.edu; banerjee@cs.umn.edu
FU NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711]; NASA [NNX12AQ39A]
FX The research was supported by NSF grants IIS-1447566, IIS-1422557,
   CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant
   NNX12AQ39A.
CR ARGYRIOU A., 2012, ADV NEURAL INFORM PR, V25, P1466
   Banerjee A., 2014, NIPS
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee Soumyadeep, 2014, ARXIV E PRINTS
   Gordon Y, 2007, J APPROX THEORY, V149, P59, DOI 10.1016/j.jat.2007.04.007
   James GM, 2009, BIOMETRIKA, V96, P323, DOI 10.1093/biomet/asp013
   James GM, 2009, J ROY STAT SOC B, V71, P127, DOI 10.1111/j.1467-9868.2008.00668.x
   Jiu J., 2010, J MACH LEARN RES, V9, P461
   Lu ZS, 2012, COMPUT STAT DATA AN, V56, P4037, DOI 10.1016/j.csda.2012.04.019
   McDonald Andrew M., 2014, ARXIV E PRINTS
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Rao N., 2012, P 15 INT C ART INT S, P942
   Rudelson M, 2008, COMMUN PUR APPL MATH, V61, P1025, DOI 10.1002/cpa.20227
   Stewart Ian, 2003, CHAPMAN HALL CRC MAT
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wang H., 2014, NIPS
   Wang Xiangfeng, 2012, SIAM J SCI COMPUTING, V34
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100074
DA 2019-06-15
ER

PT S
AU De Bock, J
   De Campos, CP
   Antonucci, A
AF De Bock, Jasper
   De Campos, Cassio P.
   Antonucci, Alessandro
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Global Sensitivity Analysis for MAP Inference in Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.
C1 [De Bock, Jasper] Univ Ghent, SYSTeMS, Ghent, Belgium.
   [De Campos, Cassio P.] Queens Univ, Belfast, Antrim, North Ireland.
   [Antonucci, Alessandro] IDSIA, Lugano, Switzerland.
RP De Bock, J (reprint author), Univ Ghent, SYSTeMS, Ghent, Belgium.
EM jasper.debock@ugent.be; c.decampos@qub.ac.uk; alessandro@idsia.ch
FU Research Foundation Flanders (FWO); Swiss NSF [200021_146606 / 1]
FX J. De Bock is a PhD Fellow of the Research Foundation Flanders (FWO) and
   he wishes to acknowledge its financial support. The work of C. P. de
   Campos has been mostly performed while he was with IDSIA and has been
   partially supported by the Swiss NSF grant 200021_146606 / 1.
CR Asuncion A., 2007, UCI MACHINE LEARNING
   Berger J., 1985, SPRINGER SERIES STAT
   BLACKMOND K, 1995, IEEE T SYST MAN CYB, V25, P901, DOI 10.1109/21.384252
   Castillo E, 1997, IEEE T SYST MAN CY A, V27, P412, DOI 10.1109/3468.594909
   Chan H, 2002, J ARTIF INTELL RES, V17, P265, DOI 10.1613/jair.967
   Chan H, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1300
   Chan Hei, 2006, P 22 C UNC ART INT, P63
   Chuan H., 2004, P 20 C UNC ART INT, P67
   Dechter R., 2012, P AAAI 2012
   Ekman P., 1978, FACIAL ACTION CODING
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Kjaerulff U., 2000, P 16 C UNC ART INT, P317
   Kwisthout J, 2011, INT J APPROX REASON, V52, P1452, DOI 10.1016/j.ijar.2011.08.003
   Kwisthout JHP, 2011, LECT NOTES COMPUT SC, V6543, P356, DOI 10.1007/978-3-642-18381-2_30
   Levi I, 1980, ENTERPRISE KNOWLEDGE
   Lucey P, 2010, P 3 INT WORKSH CVPR, P94
   Pradhan M, 1996, ARTIF INTELL, V85, P363, DOI 10.1016/0004-3702(96)00002-1
   Renooij S, 2008, INT J APPROX REASON, V49, P398, DOI 10.1016/j.ijar.2008.02.008
   Walley P., 1991, STAT REASONING IMPRE
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102033
DA 2019-06-15
ER

PT S
AU Ding, N
   Fang, YH
   Babbush, R
   Chen, CY
   Skeel, RD
   Neven, H
AF Ding, Nan
   Fang, Youhan
   Babbush, Ryan
   Chen, Changyou
   Skeel, Robert D.
   Neven, Hartmut
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Bayesian Sampling Using Stochastic Gradient Thermostats
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.
C1 [Ding, Nan; Babbush, Ryan; Neven, Hartmut] Google Inc, Mountain View, CA 94043 USA.
   [Fang, Youhan; Skeel, Robert D.] Purdue Univ, W Lafayette, IN 47907 USA.
   [Chen, Changyou] Duke Univ, Durham, NC 27706 USA.
RP Ding, N (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM dingnan@google.com; yfang@cs.purdue.edu; babbush@google.com;
   cchangyou@gmail.com; skeel@cs.purdue.edu; neven@google.com
CR Ahn S., 2012, P 29 INT C MACH LEAR, P1591
   Balan A. K., 2014, P 31 INT C MACH LEAR
   Bardenet R., 2014, P 31 INT C MACH LEAR
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen T., 2014, P 31 INT C MACH LEAR
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Fang YH, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4874000
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   HOROWITZ AM, 1991, PHYS LETT B, V268, P247, DOI 10.1016/0370-2693(91)90812-5
   Leimkuhler B., 2014, IMA J NUM ANAL
   Leimkuhler B., 2012, ARXIV12035428
   Leimkuhler B, 2009, ESAIM-MATH MODEL NUM, V43, P743, DOI 10.1051/m2an/2009023
   Maclaurin D., 2014, ARXIV14035693
   Mattingly J. C., 2014, SIAM J NUMER ANAL, V48, P552
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Neal R. M., 1901, ARXIV12061901
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Talay D., 1990, Stochastics and Stochastics Reports, V29, P13, DOI 10.1080/17442509008833606
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Tuckerman M E, 2010, STAT MECH THEORY MOL
   Welling M., 2011, P 28 INT C MACH LEAR
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103043
DA 2019-06-15
ER

PT S
AU Festa, D
   Hennequin, G
   Lengyel, M
AF Festa, Dylan
   Hennequin, Guillaume
   Lengyel, Mate
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Analog Memories in a Balanced Rate-Based Network of E-I Neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID GRADED-RESPONSE; SIMPLE CELLS; DYNAMICS; VARIABILITY; INHIBITION;
   PLASTICITY; EXCITATION; STABILITY
AB The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.
C1 [Festa, Dylan; Hennequin, Guillaume; Lengyel, Mate] Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Cambridge, England.
RP Festa, D (reprint author), Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Cambridge, England.
EM df325@cam.ac.uk; gjeh2@cam.ac.uk; m.lengyel@eng.cam.ac.uk
FU Wellcome Trust; European Union [269921]; Swiss National Science
   Foundation
FX This work was supported by the Wellcome Trust (GH, ML), the European
   Union Seventh Framework Programme (FP7/20072013) under grant agreement
   no. 269921 (Brain-ScaleS) (DF, ML), and the Swiss National Science
   Foundation (GH).
CR Ahmadian Y, 2013, NEURAL COMPUT, V25, P1994, DOI 10.1162/NECO_a_00472
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   Anderson JS, 2000, SCIENCE, V290, P1968, DOI 10.1126/science.290.5498.1968
   Battaglia FP, 1998, NEURAL COMPUT, V10, P431, DOI 10.1162/089976698300017827
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Deco G, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002395
   Finn IM, 2007, NEURON, V54, P137, DOI 10.1016/j.neuron.2007.02.029
   Goldberg JA, 2004, NEURON, V42, P489, DOI 10.1016/S0896-6273(04)00197-7
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Johnson S. G., NLOPT NONLINEAR OPTI
   Latham PE, 2004, NEURAL COMPUT, V16, P1385, DOI 10.1162/089976604323057434
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   Lengyel M, 2005, NAT NEUROSCI, V8, P1677, DOI 10.1038/nn1561
   Lengyel M, 2005, ADV NEURAL INFORM PR, V17, P769
   Litwin-Kumar A, 2012, NAT NEUROSCI, V15, P1498, DOI 10.1038/nn.3220
   Priebe NJ, 2005, NEURON, V45, P133, DOI 10.1016/j.neuron.2004.12.024
   Priebe NJ, 2006, NAT NEUROSCI, V9, P552, DOI 10.1038/nn1660
   Roudi Y, 2007, PLOS COMPUT BIOL, V3, P1679, DOI 10.1371/journal.pcbi.0030141.eor
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   TREVES A, 1991, NETWORK-COMP NEURAL, V2, P371, DOI 10.1088/0954-898X/2/4/004
   TREVES A, 1990, PHYS REV A, V42, P2418, DOI 10.1103/PhysRevA.42.2418
   Vanbiervliet J, 2009, SIAM J OPTIMIZ, V20, P156, DOI 10.1137/070704034
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101007
DA 2019-06-15
ER

PT S
AU Fragkiadaki, K
   Salas, M
   Arbelaez, P
   Malik, J
AF Fragkiadaki, Katerina
   Salas, Marta
   Arbelaez, Pablo
   Malik, Jitendra
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID THRESHOLDING ALGORITHM
AB Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation "leaking", optical flow "bleeding" etc. In this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and NRSfM for 3D reconstruction of objects in videos. For each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3D shape under different rank bounds of the trajectory matrix. We show that dense 3D shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera. We achieve competitive results on a public NRSfM benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches. We further test our approach on popular video segmentation datasets. To the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in Youtube or Hollywood movies, without object-specific priors.
C1 [Fragkiadaki, Katerina; Malik, Jitendra] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.
   [Salas, Marta] Univ Zaragoza, Zaragoza, Spain.
   [Arbelaez, Pablo] Univ Los Andes, Bogota, Colombia.
RP Fragkiadaki, K (reprint author), Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.
EM katef@berkeley.edu; msalasg@unizar.es; pa.arbelaez@uniandes.edu.co;
   malik@eecs.berkeley.edu
FU Direccion General de Investigacion of Spain [DPI2012-32168]; Ministerio
   de Educacion [FPU-AP2010-2906]
FX The authors would like to thank Philipos Modrohai for useful
   discussions. M.S. acknowledges funding from Direccion General de
   Investigacion of Spain under project DPI2012-32168 and the Ministerio de
   Educacion (scholarship FPU-AP2010-2906).
CR Akhter I., 2011, ACM T GRAPHICS
   Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201
   Andersen RA, 1998, TRENDS COGN SCI, V2, P222, DOI 10.1016/S1364-6613(98)01181-4
   BAO SY, 2013, COMP VIS PATT REC CV, P1264, DOI DOI 10.1109/CVPR.2013.167
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Brand M., 2005, CVPR
   Bregler C., 2000, CVPR
   Brox T., 2010, ECCV
   Burer S, 2005, MATH PROGRAM, V103, P427, DOI 10.1007/s10107-004-0564-1
   Cabral R., 2013, ICCV
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Dai Y., 2012, IJCV
   Galasso F., 2013, ICCV
   Garg R., 2013, DENSE VARIATIONAL RE
   Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7
   Gibson J.J., 1951, AM J PSYCHOL, V64, P622, DOI 10.2307/1419017
   Gotardo P.F.U., 2011, TPAMI, V33
   Lucas B. D., 1981, P 1981 DARPA IM UND
   Ma SQ, 2011, MATH PROGRAM, V128, P321, DOI 10.1007/s10107-009-0306-5
   Marques M, 2009, COMPUT VIS IMAGE UND, V113, P261, DOI 10.1016/j.cviu.2008.09.004
   Paladini M., 2012, IJCV, V96
   Park H. S., 2010, ECCV
   Russell C., 2014, ECCV
   Simon T., 2014, ECCV
   Sundaram N., 2010, ECCV
   Thompson WB, 1998, INT J COMPUT VISION, V30, P163, DOI 10.1023/A:1008026031844
   Toh K.C., 2010, PACIFIC J OPTIMIZATI
   Tomasi C., 1991, IJCV
   Torresani L., 2008, TPAMI, V30
   Torresani L., 2002, ECCV
   Torresani L., 2001, CVPR
   Xiao J., 2003, CMURITR0316
   Yu S.X., 2003, ICCV
NR 33
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101085
DA 2019-06-15
ER

PT S
AU Frigola, R
   Chen, YT
   Rasmussen, CE
AF Frigola, Roger
   Chen, Yutian
   Rasmussen, Carl E.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Variational Gaussian Process State-Space Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.
C1 [Frigola, Roger; Chen, Yutian; Rasmussen, Carl E.] Univ Cambridge, Dept Engn, Cambridge, England.
RP Frigola, R (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM rf342@cam.ac.uk; yc373@cam.ac.uk; cer54@cam.ac.uk
CR Bishop C. M., 2006, PATTERN RECOGNITION
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Brown EN, 1998, J NEUROSCI, V18, P7411
   Candela JQ, 2003, INT CONF ACOUST SPEE, P701
   Damianou A. C., 2011, ADV NEURAL INFORM PR, P2510
   Daunizeau J, 2009, PHYSICA D, V238, P2089, DOI 10.1016/j.physd.2009.08.002
   Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618
   Deisenroth MP, 2012, IEEE T AUTOMAT CONTR, V57, P1865, DOI 10.1109/TAC.2011.2179426
   Frigola Roger, 2013, ADV NEURAL INFORM PR, V26
   Ghahramani Z., 1999, ADV NEURAL INFORM PR, V11
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Lawrence Neil D., 2007, P 24 INT C MACH LEAR
   Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045
   Opper Manfred, 1998, ON LINE LEARNING NEU
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sarkka S., 2013, BAYESIAN FILTERING S
   Shumway RH, 2011, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-1-4419-7865-3
   Titsias M. K., 2009, P 12 INT C ART INT S
   Turner R., 2010, W CP, P868
   Valpola H, 2002, NEURAL COMPUT, V14, P2647, DOI 10.1162/089976602760408017
   Van Overschee P, 1996, SUBSPACE IDENTIFICAT
   Wang J.M., 2006, NIPS, V18, P1441
NR 24
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101046
DA 2019-06-15
ER

PT S
AU Hajek, B
   Oh, S
   Xu, JM
AF Hajek, Bruce
   Oh, Sewoong
   Xu, Jiaming
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Minimax-optimal Inference from Partial Rankings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cramer-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cramer-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.
C1 [Hajek, Bruce; Oh, Sewoong; Xu, Jiaming] UIUC, Champaign, IL 61820 USA.
RP Hajek, B (reprint author), UIUC, Champaign, IL 61820 USA.
EM b-hajek@illinois.edu; swoh@illinois.edu; jxu18@illinois.edu
CR Ben-Akiva M. E., 1985, DISCRETE CHOICE ANAL, V9
   Braverman  M., 2009, ARXIV09101191
   Duchi J. C., 2010, P ICML C HAIF ISR JU
   Gill R., 1995, BERNOULLI, V1, P59
   GUADAGNI PM, 1983, MARKET SCI, V2, P203, DOI DOI 10.1287/MKSC.2.3.203
   Guiver J., 2009, P 26 ANN INT C MACH, P377
   Hossein A. S., 2012, P 25 ANN C NEUR INF
   Hunter DR, 2004, ANN STAT, V32, P384
   Jagabathula S., 2008, NIPS, V2008
   Lozano J. A., 2012, PROBABILISTIC MODELI
   MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093
   Negahban S., 2012, ARXIV12091688
   Qin T., 2010, ADV NEURAL INFORM PR
   Rajkumar A., 2014, P INT C MACH LEARN
   SHAM PC, 1995, ANN HUM GENET, V59, P323, DOI 10.1111/j.1469-1809.1995.tb00751.x
   Simons G, 1999, ANN STAT, V27, P1041
   Soufiani H. A., 2013, ARXIV13096864
   Soufiani H. Azari, 2014, P INT C MACH LEARN
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101032
DA 2019-06-15
ER

PT S
AU Hernandez-Lobato, JM
   Hoffman, MW
   Ghahramani, Z
AF Hernandez-Lobato, Jose Miguel
   Hoffman, Matthew W.
   Ghahramani, Zoubin
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Predictive Entropy Search for Efficient Global Optimization of Black-box
   Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.
C1 [Hernandez-Lobato, Jose Miguel; Hoffman, Matthew W.; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England.
RP Hernandez-Lobato, JM (reprint author), Univ Cambridge, Cambridge, England.
EM jmh233@cam.ac.uk; mwh30@cam.ac.uk; zoubin@eng.cam.ac.uk
FU Rafael del Pino Foundation
FX J.M.H.L acknowledges support from the Rafael del Pino Foundation.
CR AHMAD IA, 1976, IEEE T INFORM THEORY, V22, P372, DOI 10.1109/TIT.1976.1055550
   Anderson B. S., 2000, ICML, P17
   BACHE K., 2013, UCI MACHINE LEARNING
   Bochner S., 1959, LECT FOURIER INTEGRA
   Brochu E., 2007, P ADV NEUR INF PROC, P409
   Brochu E., 2009, TR200923 UBC DEP COM
   Burrows EH, 2009, BIOTECHNOL PROGR, V25, P1009, DOI 10.1002/btpr.213
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Hennig P., 2012, J MACHINE LEARNING R, V13
   Hoffman M. W., 2011, UNCERTAINTY ARTIFICI, P327
   Houlsby N., 2012, ADV NEURAL INFORM PR, P2096
   Jondeau E, 2006, J INT MONEY FINANC, V25, P827, DOI 10.1016/j.jimonfin.2006.04.007
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Kushner H. J., 1964, J BASIC ENG, V86
   Lizotte D. J., 2008, THESIS
   Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590
   Minka T. P., 2001, THESIS
   Mockus J., 1978, GLOBAL OPTIMIZATION, V2
   Negoescu DM, 2011, INFORMS J COMPUT, V23, P346, DOI 10.1287/ijoc.1100.0417
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seeger MW, 2008, J MACH LEARN RES, V9, P759
   Snoek J., 2012, ADV NEURAL INFORM PR, P2960
   Solak E., 2003, ADV NEURAL INF PROCE, V13, P1057
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Vanhatalo J., 2012, ABS12065754 CORR
   Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2
   Wang Ziyu, 2013, ICML
   Westervelt E., 2007, CONTROL AUTOMATION S
NR 30
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100103
DA 2019-06-15
ER

PT S
AU Inouye, DI
   Ravikumar, P
   Dhillon, IS
AF Inouye, David I.
   Ravikumar, Pradeep
   Dhillon, Inderjit S.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Capturing Semantically Meaningful Word Dependencies with an Admixture of
   Poisson MRFs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model [1] and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. [1] is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models [2, 3, 4, 5] and measures of model fitness [6] provide strong support that explicitly modeling word dependencies-as in APM-could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because O(p(2)) parameters must be estimated where p is the number of words ([1] could only provide results for datasets with p = 200). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle p = 10(4) as an important step towards scaling to large datasets. In addition, Inouye et al. [1] only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word "brings to mind" another word [7]). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies.
C1 [Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Inouye, DI (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM dinouye@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu
FU NSF Graduate Research Fellowship [DGE-1110007]; ARO [W911NF-12-1-0390];
   NSF [CCF-1117055, IIS-1149803, IIS-1447574, DMS-1264033]
FX D. Inouye was supported by the NSF Graduate Research Fellowship via
   DGE-1110007. P. Ravikumar acknowledges support from ARO via
   W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, and DMS-1264033.
   I. Dhillon acknowledges support from NSF via CCF-1117055.
CR Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   Aletras N., 2013, P 10 INT C COMP SEM, P13
   Blei David, 2005, NIPS, P147
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd-graber J., 2006, P GLOB WORDNET C
   Chang J., 2009, NIPS, V31, P1
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hinton G., 2009, NIPS
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Hsieh C. J., 2011, NIPS, V24, P1
   Inouye D. I., 2014, INT C MACH LEARN ICM
   Lau J.H., 2011, P 49 ANN M ASS COMP, V1, P1536
   Lau J.H., 2012, P 13 C EUR CHAPT ASS, P591
   Lee D. D., 2000, ADV NEURAL INFORM PR, P556
   Lee J., 2013, J MACH LEARN RES, P388
   Magatti D., 2009, ISDA
   Mao X. -L., 2012, P 21 ACM INT C INF K, P2383
   McCallum Andrew Kachites, 2002, MALLET MACHINE LEARN
   Mimno D., 2011, P C EMP METH NAT LAN, P227
   Mimno D., 2011, P C EMP METH NAT LAN, P262
   Newman D., 2010, P 10 ANN JOINT C DIG, P215, DOI DOI 10.1145/1816123.1816156
   Nikolova S, 2009, ASSETS'09: PROCEEDINGS OF THE 11TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P171
   Reisinger J., 2010, P 27 INT C MACH LEAR, P903
   Stevens K., 2012, P 2012 JOINT C EMP M, V2012, P952
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Varin C, 2011, STAT SINICA, V21, P5
   Yang E, 2013, NIPS, P1718
   YANG E., 2012, ADV NEURAL INFO PROC, V25, P1367
   Yu HF, 2011, MACH LEARN, V85, P41, DOI 10.1007/s10994-010-5221-8
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100079
DA 2019-06-15
ER

PT S
AU Kocak, T
   Neu, G
   Valko, M
   Munos, R
AF Kocak, Tomas
   Neu, Gergely
   Valko, Michal
   Munos, Remi
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Efficient learning by implicit exploration in bandit problems with side
   observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.
C1 [Kocak, Tomas; Neu, Gergely; Valko, Michal; Munos, Remi] INRIA Lille Nord Europe, SequeL Team, Lille, France.
RP Kocak, T (reprint author), INRIA Lille Nord Europe, SequeL Team, Lille, France.
EM tomas.kocak@inria.fr; gergely.neu@inria.fr; michal.valko@inria.fr;
   remi.munos@inria.fr
FU French Ministry of Higher Education and Research; European Community
   [270327]; FUI project Hermes
FX The research presented in this paper was supported by French Ministry of
   Higher Education and Research, by European Community's Seventh Framework
   Programme (FP7/2007-2013) under grant agreement no270327 (CompLACS), and
   by FUI project Hermes.
CR Alon N., 2013, NEURAL INFORM PROCES
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Gyorfi L, 2007, IEEE T INFORM THEORY, V53, P1866, DOI 10.1109/TIT.2007.894660
   Hannan J., 1957, ANN MATH STUD, V3, P97
   Hutter M, 2004, LECT NOTES ARTIF INT, V3244, P279
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Koolen W. M., 2010, P 23 ANN C LEARN THE, P93
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Mannor S., 2011, NEURAL INFORM PROCES
   Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
NR 17
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102023
DA 2019-06-15
ER

PT S
AU Latimer, KW
   Chichilnisky, EJ
   Rieke, F
   Pillow, JW
AF Latimer, Kenneth W.
   Chichilnisky, E. J.
   Rieke, Fred
   Pillow, Jonathan W.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Inferring synaptic conductances from spike trains under a biophysically
   inspired point process model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FRAMEWORK; INPUT
AB A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite "push-pull" fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. The stimulus-dependence of both excitatory and inhibitory conductances can be well described by a linear-nonlinear cascade, with the filter driving inhibition exhibiting opposite sign and a slight delay relative to the filter driving excitation. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.
C1 [Latimer, Kenneth W.] Univ Texas Austin, Inst Neurosci, Austin, TX 78712 USA.
   [Chichilnisky, E. J.] Stanford Univ, Hansen Expt Phys Lab, Dept Neurosurg, Stanford, CA 94305 USA.
   [Rieke, Fred] Univ Washington, Howard Hughes Med Inst, Dept Physiol & Biophys, Seattle, WA 98195 USA.
   [Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Latimer, KW (reprint author), Univ Texas Austin, Inst Neurosci, Austin, TX 78712 USA.
EM latimerk@utexas.edu; ej@stanford.edu; rieke@u.washington.edu;
   pillow@princeton.edu
CR Ahrens MB, 2008, NETWORK-COMP NEURAL, V19, P35, DOI 10.1080/09548980701813936
   Butts DA, 2011, J NEUROSCI, V31, P11313, DOI 10.1523/JNEUROSCI.0434-11.2011
   Chander D, 2001, J NEUROSCI, V21, P9904, DOI 10.1523/JNEUROSCI.21-24-09904.2001
   GERSTNER W, 2001, HDB BIOL PHYS, V4, P469
   Gerwinn S., 2010, FRONTIERS COMPUTATIO
   Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834
   McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143
   Mensi S., 2011, ADV NEURAL INF PROCE, P1377
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0
   Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Plesser HE, 2000, NEURAL COMPUT, V12, P367, DOI 10.1162/089976600300015835
   Poo C, 2009, NEURON, V62, P850, DOI 10.1016/j.neuron.2009.05.022
   Stevenson IH, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002775
   Theis L., 2010, PLOS COMPUTATIONAL B
   Trong PK, 2008, NAT NEUROSCI, V11, P1343, DOI 10.1038/nn.2199
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Vintch B, 2012, NEURAL INFORM PROCES, V25
NR 20
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100041
DA 2019-06-15
ER

PT S
AU Lee, S
   Kim, JK
   Zheng, X
   Ho, Q
   Gibson, GA
   Xing, EP
AF Lee, Seunghak
   Kim, Jin Kyu
   Zheng, Xun
   Ho, Qirong
   Gibson, Garth A.
   Xing, Eric P.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI On Model Parallelization and Scheduling Strategies for Distributed
   Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.
C1 [Lee, Seunghak; Kim, Jin Kyu; Zheng, Xun; Gibson, Garth A.; Xing, Eric P.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Ho, Qirong] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
RP Lee, S (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM seunghak@cs.cmu.edu; jinkyuk@cs.cmu.edu; xunzheng@cs.cmu.edu;
   hoqirong@gmail.com; garth@cs.cmu.edu; epxing@cs.cmu.edu
FU NSF [IIS1447676, CNS-1042543 (PRObE [11])]; DARPA [FA87501220324]; Intel
   via the Intel Science and Technology Center for Cloud Computing
   (ISTC-CC)
FX This work was done under support from NSF IIS1447676, CNS-1042543 (PRObE
   [11]), DARPA FA87501220324, and support from Intel via the Intel Science
   and Technology Center for Cloud Computing (ISTC-CC).
CR Ahmed A., 2012, WSDM
   Bennett J., 2007, P KDD CUP WORKSH
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bradley J.K., 2011, ICML
   Dai W., 2014, AAAI
   Dean J., 2012, NIPS
   Dean J, 2008, COMMUN ACM, V51, P107, DOI 10.1145/1327452.1327492
   Fan JQ, 2009, J MACH LEARN RES, V10, P2013
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Gemulla R., 2011, SIGKDD
   Gibson G., 2013, USENIX LOGIN, V38
   Gonzalez J., 2011, AISTATS
   Gonzalez J. E., 2012, OSDI
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Ho Q., 2013, NIPS
   Lau Jey Han, 2013, ACM T SPEECH LANGUAG, V10, P10
   Le Q. V., 2012, ICML
   Li M., 2014, OSDI
   Low Y., 2012, VLDB
   Newman D, 2009, J MACH LEARN RES, V10, P1801
   Scherrer C., 2012, NIPS
   Wang Y., 2014, ARXIV14054402CSIR
   Wei J., 2013, ARXIV13127869STATML
   Yu Hsiang- Fu, 2012, ICDM
   Zaharia M., 2010, HOTCLOUD
   Zhou Y., 2008, AAIM
   Zinkevich M., 2009, NIPS
   Zinkevich M. A., 2010, NIPS
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103049
DA 2019-06-15
ER

PT S
AU Li, M
   Andersen, DG
   Smola, A
   Yu, K
AF Li, Mu
   Andersen, David G.
   Smola, Alexander
   Yu, Kai
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Communication Efficient Distributed Machine Learning with the Parameter
   Server
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l(1)-regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.
C1 [Li, Mu; Andersen, David G.; Smola, Alexander] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Li, Mu; Yu, Kai] Baidu, Beijing, Peoples R China.
   [Smola, Alexander] Google, Mountain View, CA USA.
RP Li, M (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM muli@cs.cmu.edu; dga@cs.cmu.edu; alex@smola.org; yukai@baidu.com
CR Agarwal A., 2012, IEEE CDC
   Ahmed A., 2012, WSDM
   Ahmed Amr, 2013, WWW
   Barroso L.A., 2009, SYNTHESIS LECT COMPU, V8, P1, DOI DOI 10.2200/S00193ED1V01Y200905CAC006
   Bradley J.K., 2011, ICML
   Byers J, 2003, LECT NOTES COMPUT SC, V2735, P80
   Canini K., 2012, SIBYL SYSTEM LARGE S
   Dean J., 2012, NIPS
   Dean Jeffrey, 2008, CACM
   Domo, 2014, DAT NEV SLEEPS 2 0
   Gunderson S. H., SNAPPY
   Ho Q., 2013, NIPS
   Joachims T., 1999, ADV KERNEL METHODS
   Langford J., 2009, NIPS
   Le Q. V., 2011, NIPS
   Li M., 2014, OSDI
   Li M., 2013, BIG LEARN NIPS WORKS
   Li  Mu, 2013, NIPS WORKSH OPT MACH
   Low Y., 2012, PVLDB
   Matsushima S., 2012, KDD
   Parikh N., 2013, FDN TRENDS OPTIMIZAT
   Petersen KB, 2008, MATRIX COOKBOOK
   Phanishayee A., 2012, MANAGEMENT BIG DATA
   Recht B., 2011, NIPS
   Richtarik P., 2012, MATH PROGRAMMING
   Rowstron A., 2001, DISTRIBUTED SYSTEMS
   Smola A. J., 2010, VLDB
   Sparks E., 2013, MLI API DISTRIBUTED
   Sra S., 2012, NIPS
   Stoica I., 2001, SIGCOMM COMPUTER COM
   Teflioudi C., 2012, ICDM
   Teo C. H., 2010, JMLR
   The Apache Software Foundation, 2009, APACHE HADOOP
   van Renesse R., 2004, OSDI
   Yuan G. X., 2010, JMLR
   Zaharia M., 2012, USENIX LOGIN
   Zinkevich M. A., 2010, NIPS
NR 37
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103048
DA 2019-06-15
ER

PT S
AU Li, N
   Jin, R
   Zhou, ZH
AF Li, Nan
   Jin, Rong
   Zhou, Zhi-Hua
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Top Rank Optimization in Linear Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID AREA
AB Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.
C1 [Li, Nan; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Jin, Rong] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
RP Li, N (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM lin@lamda.nju.edu.cn; rongjin@cse.msu.edu; zhouzh@lamda.nju.edu.cn
FU 973 Program [2014CB340501]; NSFC [61333014]; NSF [IIS-1251031]; ONR
   Award [N000141210431]
FX This research was supported by the 973 Program (2014CB340501), NSFC
   (61333014), NSF (IIS-1251031), and ONR Award (N000141210431).
CR Agarwal S, 2005, J MACH LEARN RES, V6, P393
   Agarwal S., 2011, SDM, P839
   Borges C., 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Boyd S., 2004, CONVEX OPTIMIZATION
   Boyd S., 2012, ADV NEURAL INFORM PR, P962
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Clemencon S, 2007, J MACH LEARN RES, V8, P2671
   Cormen T. H., 2001, INTRO ALGORITHMS
   CORTES C, 2004, NIPS, V16, P313
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933
   Gao W., 2013, P 30 INT C MACH LEAR, P906
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Herbrich R, 2000, ADV NEUR IN, P115
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Kanamori T, 2013, J MACH LEARN RES, V14, P1461
   Kotlowski W, 2011, P 28 INT C MACH LEAR, P1113
   Le Q. V., 2007, ABS07043359 CORR
   Li N., 2014, ABS14101462 CORR
   Li N, 2013, IEEE T PATTERN ANAL, V35, P1370, DOI 10.1109/TPAMI.2012.172
   Liu J., 2009, P 26 ANN INT C MACH, P657, DOI DOI 10.1145/1553374.1553459
   Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3
   Narasimhan H., 2013, KDD, P167
   Narasimhan H., 2013, P 30 INT C MACH LEAR, V28, P516
   Narasimhan H., 2013, ADV NEURAL INFORM PR, P2913
   Nemirovski A., 1994, LECT NOTES
   Nesterov Yurii E., 2003, INTRO LECT CONVEX OP
   Rakotomamonjy A., 2012, ICML
   Rendle S, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727
   Rudin C, 2009, J MACH LEARN RES, V10, P2193
   Shalev-Shwartz S, 2006, J MACH LEARN RES, V7, P1567
   Sun SL, 2010, J MACH LEARN RES, V11, P2423
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Usunier N., 2009, P 26 ANN INT C MACH, P1057
   Valizadegan H., 2009, ADV NEURAL INFORM PR, P1883
   Xu M., 2013, P 27 AAAI C ART INT, P998
   Yang T., 2012, NIPS, P485
   Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271
   Zhao Peilin, 2011, P 28 INT C MACH LEAR, P233
NR 42
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100001
DA 2019-06-15
ER

PT S
AU Lindsey, RV
   Khajah, M
   Mozer, MC
AF Lindsey, Robert, V
   Khajah, Mohammad
   Mozer, Michael C.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Automatic Discovery of Cognitive Skills to Improve the Prediction of
   Student Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB To master a discipline such as algebra or physics, students must acquire a set of cognitive skills. Traditionally, educators and domain experts use intuition to determine what these skills are and then select practice exercises to hone a particular skill. We propose a technique that uses student performance data to automatically discover the skills needed in a discipline. The technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice. Rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted Chinese restaurant process. We test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college. We obtain two surprising results. First, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills. Second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it. We discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.
C1 [Lindsey, Robert, V] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
   Univ Colorado, Inst Cognit Sci, Boulder, CO 80309 USA.
RP Lindsey, RV (reprint author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.
FU NSF [BCS-0339103, BCS-720375]; NSF Graduate Research Fellowship
FX This research was supported by NSF grants BCS-0339103 and BCS-720375 and
   by an NSF Graduate Research Fellowship to R. L.
CR Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   ATKINSON RC, 1972, J EXP PSYCHOL, V96, P124, DOI 10.1037/h0033475
   Barnes T., 2005, P 2005 AAAI ED DAT M
   Cen H, 2006, LECT NOTES COMPUT SC, V4053, P164
   Corbett A., 1997, HDB HUMAN COMPUTER I, P849, DOI DOI 10.1016/B978-044481862-1.50103-5
   CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253
   Gonzalez-Brenes J., 2012, P 5 INT C ED DAT MIN
   Gonzalez-Brenes J., 2013, P 6 INT C ED DAT MIN
   Hannah LA, 2011, J MACH LEARN RES, V12, P1923
   Ishwaran H, 2003, STAT SINICA, V13, P1211
   Khajah M., 2014, EDM 2014
   Koedinger K. R., 2010, HDB ED DATA MINING
   Koedinger KR, 2012, COGNITIVE SCI, V36, P757, DOI 10.1111/j.1551-6709.2012.01245.x
   Lan A. S., 2014, ACM SIGKDD C KNOWL D
   Lindsey RV, 2014, PSYCHOL SCI, V25, P639, DOI 10.1177/0956797613504302
   Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   Pardos Zachary A., 2012, Intelligent Tutoring Systems. Proceedings 11th International Conference (ITS 2012), P405, DOI 10.1007/978-3-642-30950-2_52
   Pardos ZA, 2011, LECT NOTES COMPUT SC, V6787, P243, DOI 10.1007/978-3-642-22362-4_21
   Rafferty A., 2011, P 15 INT C AI ED
   Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004
   Sohl-Dickstein J., 2013, NIPS WORKSH DAT DRIV
   Teh Y., 2011, ADV NEURAL INFORM PR
   Thai-Nghe N., 2011, ED RECOMMENDER SYSTE, P129
   Whitehill J., 2012, THESIS
NR 25
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103005
DA 2019-06-15
ER

PT S
AU Maddison, CJ
   Tarlow, D
   Minka, T
AF Maddison, Chris J.
   Tarlow, Daniel
   Minka, Tom
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A* Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem [1, 2, 3, 4]. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A. Sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A. search. We analyze the correctness and convergence time of A* Sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.
C1 [Maddison, Chris J.] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
   [Tarlow, Daniel; Minka, Tom] Microsoft Res, Redmond, WA USA.
RP Maddison, CJ (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
EM cmaddis@cs.toronto.edu; dtarlow@microsoft.com; minkal@microsoft.com
FU NSERC
FX This research was supported by NSERC. We thank James Martens and Radford
   Neal for helpful discussions, Elad Mezuman for help developing early
   ideas related to this work, and Roger Grosse for suggestions that
   greatly improved this work.
CR Chang M. M. J., 2008, PULMONARY EPITHELIUM, P1, DOI DOI 10.1109/ARSO.2008.4653593.
   Dymetman Marc, 2012, ARXIV12070742
   Ermon S., 2013, ADV NEURAL INFORM PR, P2085
   GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337
   Gumbel E., 1954, STAT THEORY EXTREME
   Hansen E., 2003, GLOBAL OPTIMIZATION, V264
   Hazan T, 2012, P 29 INT C MACH LEAR, P991
   Hazan T., 2013, NIPS, P1268
   Kroshko Dmitrey, 2014, FUNCDESIGNER
   Malmberg Hannes, 2013, THESIS
   Mansinghka V, 2009, 12 INT C ART INT STA, V5, P400
   Mateescu Robert Eugeniu, 2007, THESIS
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Mira A, 2001, J ROY STAT SOC B, V63, P593, DOI 10.1111/1467-9868.00301
   Mitha Faheem, 2003, THESIS
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   Papandreou G., 2010, ADV NEURAL INF PROCE, P1858
   Papandreou G, 2011, IEEE I CONF COMP VIS, P193, DOI 10.1109/ICCV.2011.6126242
   Propp JG, 1996, RANDOM STRUCT ALGOR, V9, P223
   Tarlow Daniel, 2012, AISTATS, P21
   YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8
NR 21
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102010
DA 2019-06-15
ER

PT S
AU Mairal, J
   Konius, P
   Harchaoui, Z
   Schmid, C
AF Mairal, Julien
   Konius, Piotr
   Harchaoui, Zaid
   Schmid, Cordelia
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Convolutional Kernel Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data.
   Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.
C1 [Mairal, Julien; Konius, Piotr; Harchaoui, Zaid; Schmid, Cordelia] Inria, Rennes, France.
RP Mairal, J (reprint author), Univ Grenoble Alpes, LEAR Team, Inria Grenoble, Lab Jean Kuntzmann,CNRS, Grenoble, France.
EM julien.mairal@inria.fr; piotr.konius@inria.fr; zaid.harchaoui@inria.fr;
   cordelia.schmid@inria.fr
FU ANR [MACARON ANR-14-CE23-0003-01]; MSR-Inria joint centre; European
   Research Council; CNRS-Mastodons program; LabEx PERSYVAL-Lab
   [ANR-11-LABX-0025]
FX This work was partially supported by grants from ANR (project MACARON
   ANR-14-CE23-0003-01), MSR-Inria joint centre, European Research Council
   (project ALLEGRO), CNRS-Mastodons program (project GARGANTUA), and the
   LabEx PERSYVAL-Lab (ANR-11-LABX-0025).
CR Bengio Y., 2009, FDN TRENDS MACH LEAR
   Bo L., 2009, ADV NIPS
   Bo L., 2011, P CVPR
   Bo L., 2013, EXPT ROBOTICS
   Bo L., 2010, ADV NIPS
   Bottou L, 2007, LARGE SCALE KERNEL M
   Bouvrie J. V., 2009, ADV NIPS
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Cho Y., 2010, NEURAL COMPUT, V22
   Ciresan D., 2012, P CVPR
   Coates A., 2011, ADV NIPS
   Coates Adam, 2011, P AISTATS
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Donahue J., 2013, ARXIV13101531
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gens R., 2012, ADV NIPS
   Goodfellow I., 2013, P ICML
   Jarrett K., 2009, P ICCV
   Krizhevsky A., 2009, TECH REP
   Krizhevsky A., 2012, ADV NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Rahimi A., 2007, ADV NIPS
   Ranzato M., 2007, P CVPR
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Sohn K., 2012, P ICML
   Swersky K., 2013, ADV NIPS
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wan L., 2013, P ICML
   Williams C., 2001, ADV NIPS
   Zeiler M. D., 2013, P ICLR
   Zeiler M. D., 2014, P ECCV
NR 33
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101019
DA 2019-06-15
ER

PT S
AU Malinowski, M
   Fritz, M
AF Malinowski, Mateusz
   Fritz, Mario
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Multi-World Approach to Question Answering about Real-World Scenes
   based on Uncertain Input
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.
C1 [Malinowski, Mateusz; Fritz, Mario] Max Planck Inst Informat, Saarbrucken, Germany.
RP Malinowski, M (reprint author), Max Planck Inst Informat, Saarbrucken, Germany.
EM mmalinow@mpi-inf.mpg.de; mfritz@mpi-inf.mpg.de
CR Fellbaum C., 1999, WORDNET
   Guadarrama S., 2013, IROS
   Guadarrama S., 2013, ICCV
   Gupta  S., 2013, CVPR
   Karpathy Andrej, 2014, NIPS
   Kong C., 2014, CVPR
   Krishnamurthy J., 2013, JOINTLY LEARNING PAR
   Kruijff G. J. M., 2007, IJARS
   Kwiatkowski T., 2010, EMNLP
   Lan T., 2012, ECCV
   Levit M., 2007, SYSTEMS MAN CYBERN B
   Liang P., 2013, COMPUTATIONAL LINGUI
   Manning C. D., 2008, INTRO INFORM RETRIEV
   Matuszek C., 2013, EXPT ROBOTICS
   Matuszek C, 2012, ICML
   Miller G. A., 1995, CACM
   Regier T., 2001, J EXPT PSYCHOL GEN
   Silberman  N., 2012, ECCV
   Tellex Stefanie, 2011, AAAI
   Tukey J. W., 1977, EXPLORATORY DATA ANA
   VanDeWeijer J., 2007, CVPR
   Vogel A., 2010, ACL
   Wick M., 2010, VLDB
   Wu Z., 1994, ACL
   Zadeh L. A., 1965, FUZZY SETS INFORM CO
   Zettlemoyer L. S., 2007, EMNLPCONLL2007
NR 26
TC 1
Z9 1
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101082
DA 2019-06-15
ER

PT S
AU McDonald, AM
   Ponti, M
   Stamos, D
AF McDonald, Andrew M.
   Ponti, Massimiliano
   Stamos, Dimitris
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Spectral k-Support Norm Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID THRESHOLDING ALGORITHM; SHRINKAGE; SELECTION
AB The k-support norm has successfully been applied to sparse vector prediction problems. We observe that it belongs to a wider class of norms, which we call the box-norms. Within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the k-support norm. We extend the norms from the vector to the matrix setting and we introduce the spectral k-support norm. We study its properties and show that it is closely related to the multitask learning cluster norm. We apply the norms to real and synthetic matrix completion datasets. Our findings indicate that spectral k-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.
C1 [McDonald, Andrew M.; Ponti, Massimiliano; Stamos, Dimitris] UCL, Dept Comp Sci, London, England.
RP McDonald, AM (reprint author), UCL, Dept Comp Sci, London, England.
EM a.mcdonald@cs.ucl.ac.uk; m.pontil@cs.ucl.ac.uk; d.stamos@cs.ucl.ac.uk
FU EPSRC [EP/H027203/1]
FX We would like to thank Andreas Maurer, Charles Micchelli and especially
   Andreas Argyriou for useful discussions. Part of this work was supported
   by EPSRC Grant EP/H027203/1.
CR Abernethy J, 2009, J MACH LEARN RES, V10, P803
   ARGYRIOU A., 2012, ADV NEURAL INFORM PR, V25, P1466
   Argyriou A., 2011, ABS11041436 CORR
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas D., 2003, CONVEX ANAL OPTIMIZA
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Combettes P., 2011, FIXED POINT ALGORITH
   Grandvalet Y., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks, P201
   Horn R. A., 1991, TOPICS MATRIX ANAL
   Jacob L., 2009, ADV NEURAL INFORM PR
   Jacob L., 2009, P 26 INT C MACH LEAR
   Jaggi Martin, 2010, P 27 INT C MACH LEAR
   LEWIS A. S., 1995, J CONVEX ANAL, V2, P173
   Li H, 2012, IEEE T NEUR NET LEAR, V23, P737, DOI 10.1109/TNNLS.2012.2188906
   Marshall A. W., 1979, INEQUALITIES THEORY
   Maurer A, 2012, J MACH LEARN RES, V13, P671
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Micchelli CA, 2005, J MACH LEARN RES, V6, P1099
   Micchelli CA, 2013, ADV COMPUT MATH, V38, P455, DOI 10.1007/s10444-011-9245-9
   Nesterov Y., 2007, CTR OPERATIONS RES E, V76
   Obozinski G., 2012, CORR
   Rockafellar R T, 1970, CONVEX ANAL
   Rudin W., 1991, FUNCTIONAL ANAL
   Shen ZW, 2011, SIAM J IMAGING SCI, V4, P573, DOI 10.1137/090779437
   Srebro N., 2005, ADV NEURAL INFORM PR, V17
   Szafranski M., 2007, ADV NEURAL INFORM PR, V21
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Von Neumann J., 1937, TOMSK U REV, VI
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 29
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100076
DA 2019-06-15
ER

PT S
AU McWilliams, B
   Krummenacher, G
   Lucic, M
   Buhmann, JM
AF McWilliams, Brian
   Krummenacher, Gabriel
   Lucic, Mario
   Buhmann, Joachim M.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Fast and Robust Least Squares Estimation in Corrupted Linear Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates.
   The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence - for which we also develop a randomized approximation - motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.
C1 [McWilliams, Brian; Krummenacher, Gabriel; Lucic, Mario; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP McWilliams, B (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM mcbrian@inf.ethz.ch; gabriel.krummenacher@inf.ethz.ch;
   lucic@inf.ethz.ch; jbuhmann@inf.ethz.ch
CR Ailon N, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1
   Belsley David A, 1981, REGRESSION DIAGNOSTI
   Boutsidis Christos, 2012, ARXIV12040062V4CSDS
   Chen Yudong, 2013, INT C MACH LEARN
   Chen Yudong, 2012, ARXIV12060823
   Dhillon P, 2013, ADV NEURAL INFORM PR
   Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682
   Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6
   Drineas Petros, 2011, ARXIV11093843V2CSDS
   Hsu D., 2012, ELECTRON COMMUN PROB, V17, P6
   Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018
   Ma Ping, 2014, P INT C MACH LEARN
   Mahoney Michael W, 2011, ARXIV11045557V3CSDS
   McWilliams Brian, 2012, Statistical Analysis and Data Mining, V5, P304, DOI 10.1002/sam.11144
   McWilliams B, 2014, DATA MIN KNOWL DISC, V28, P736, DOI 10.1007/s10618-013-0317-y
   Tropp Joel A, 2010, ARXIV10111595V4MATHN
   Vershynin  Roman, 2010, ARXIV10113027
   Welsch R. E, 1980, EVALUATION ECONOMETR, P153
NR 18
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101099
DA 2019-06-15
ER

PT S
AU Meier, F
   Hennig, P
   Schaal, S
AF Meier, Franziska
   Hennig, Philipp
   Schaal, Stefan
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Incremental Local Gaussian Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.
C1 [Meier, Franziska; Schaal, Stefan] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Hennig, Philipp; Schaal, Stefan] Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Meier, F (reprint author), Univ Southern Calif, Los Angeles, CA 90089 USA.
EM fmeier@usc.edu; phennig@tue.mpg.de; sschaal@usc.edu
CR Atkeson CG, 1997, ARTIF INTELL REV, V11, P75, DOI 10.1023/A:1006511328852
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Chalupka K, 2013, J MACH LEARN RES, V14, P333
   Csato L., 2002, NEURAL COMPUTATION
   D'Souza Aaron, 2004, ICML
   FAN JQ, 1995, J ROY STAT SOC B MET, V57, P371
   Gijsberts A, 2013, NEURAL NETWORKS, V41, P59, DOI 10.1016/j.neunet.2012.08.011
   Hastie Trevor, 1993, STAT SCI
   Hensman J., 2013, UAI
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Honkela A., 2003, 4 INT S IND COMP AN, P803
   Huber MF, 2014, PATTERN RECOGN LETT, V45, P85, DOI 10.1016/j.patrec.2014.03.004
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Luts Jan, 2013, REAL TIME SEMIPARAME
   Meier Franziska, 2014, P IEEE INT C INT ROB
   Moody J., 1988, P 1988 CONN MOD SUMM, P133
   Neal RM, 1996, BAYESIAN LEARNING NE, V118
   Nguyen-Tuong D., 2008, ADV NEURAL INFORM PR, V21, P1193
   Quinonero-Candela Joaquin, 2002, NIPS
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rahimi A., 2007, NIPS
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schaal S, 1998, NEURAL COMPUT, V10, P2047, DOI 10.1162/089976698300016963
   Schaal Stefan, 2009, TECHNICAL REPORT
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Snelson E., 2007, INT C ART INT STAT, P524
   Ting Jo- Anne, 2008, ADV NEURAL INFORM PR, V6, P7
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Titsias M, 2009, ARTIF INTELL, P567
   Vijayakumar S., 2000, P 17 INT C MACH LEAR, P1079
   Wainwright Martin J., 2008, FDN TRENDS MACHINE L
NR 31
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103045
DA 2019-06-15
ER

PT S
AU Montufar, G
   Pascanu, R
   Cho, K
   Bengio, Y
AF Montufar, Guido
   Pascanu, Razvan
   Cho, Kyunghyun
   Bengio, Yoshua
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI On the Number of Linear Regions of Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
DE Deep learning; neural network; input space partition; rectifier; maxout
AB We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.
C1 [Montufar, Guido] Max Planck Inst Math Sci, Leipzig, Germany.
   [Pascanu, Razvan; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada.
RP Montufar, G (reprint author), Max Planck Inst Math Sci, Leipzig, Germany.
EM montufar@mis.mpg.de; pascanur@iro.umontreal.ca;
   kyunghyun.cho@umontreal.ca; yoshua.bengio@umontreal.ca
CR Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Delalleau O., 2011, NIPS
   Glorot X., 2011, AISTATS
   Goodfellow Ian, 2013, JMLR W CP, P1319
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Krause O, 2013, P 30 INT C MACH LEAR, P419
   Krizhevsky A., 2012, NIPS
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   Montufar G., 2014, NEURAL COMPUTATION, V26
   Montufar G, 2011, NEURAL COMPUT, V23, P1306, DOI 10.1162/NECO_a_00113
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pascanu R, 2013, ARXIV13126098
   Pascanu Razvan, 2014, INT C LEARN REPR
   Stanley RP, 2004, LECT NOTES
   Susskind J., 2010, 2010001 UTML TR
   Zaslavsky T., 1975, MEMOIRS AM MATH SOC
   Zeiler M. D., 2013, ARXIV13112901
NR 20
TC 1
Z9 1
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101093
DA 2019-06-15
ER

PT S
AU Moon, KR
   Hero, AO
AF Moon, Kevin R.
   Hero, Alfred O., III
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Multivariate f-Divergence EstimationWith Confidence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID INFORMATION; FUNCTIONALS; DEPENDENCY; DENSITIES
AB The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several nonparametric divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O (1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.
C1 [Moon, Kevin R.; Hero, Alfred O., III] Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA.
RP Moon, KR (reprint author), Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA.
EM krmoon@umich.edu; hero@eecs.umich.edu
FU NSF [CCF-1217880]; NSF Graduate Research Fellowship [F031543]
FX This work was partially supported by NSF grant CCF-1217880 and a NSF
   Graduate Research Fellowship to the first author under Grant No.
   F031543.
CR BACHE K., 2013, UCI MACHINE LEARNING
   BERLINET A, 1995, STATISTICS, V26, P329, DOI 10.1080/02331889508802500
   Berlinet A., 1997, PUBLICATIONS I STAT, V41, P3
   BICKEL PJ, 1973, ANN STAT, V1, P1071, DOI 10.1214/aos/1176342558
   Blum J. R., 1958, CAN J MATH, V10, P222, DOI DOI 10.4153/CJM-1958-026-0
   Carter KM, 2010, IEEE T SIGNAL PROCES, V58, P650, DOI 10.1109/TSP.2009.2031722
   Chai B., 2009, ADV NEURAL INFORM PR, P270
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299
   Darbellay GA, 1999, IEEE T INFORM THEORY, V45, P1315, DOI 10.1109/18.761290
   Dhillon I. S., 2003, Journal of Machine Learning Research, V3, P1265, DOI 10.1162/153244303322753661
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355
   Krishnamurthy A., 2014, INT C MACH LEARN, V32
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lewi J., 2006, P 19 INT C NEUR INF, P857
   LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079
   Moon K. R., 2014, CORR
   Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Oliva J., 2013, P 30 INT C MACH LEAR, P1049
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Poczos B., 2011, INT C ART INT STAT, V15, P609
   RENYI A., 1961, P 4 BERK S MATH STAT, V1, P547, DOI DOI 10.1021/JP106846B
   Schneidman E., 2002, ADV NEURAL INFORM PR, P197
   Silva J, 2010, J STAT PLAN INFER, V140, P3180, DOI 10.1016/j.jspi.2010.04.011
   Singh S, 2014, INT C MACH LEARN, P333
   Sricharan K., 2012, THESIS
   Sricharan K., 2012, ADV NEURAL INF PROCE, V25, P575
   Sricharan K, 2013, IEEE T INFORM THEORY, V59, P4374, DOI 10.1109/TIT.2013.2251456
   Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549
   Le TK, 2013, J STAT PLAN INFER, V143, P2089, DOI 10.1016/j.jspi.2013.08.007
   Wang Q, 2005, IEEE T INFORM THEORY, V51, P3064, DOI 10.1109/TIT.2005.853314
   Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102051
DA 2019-06-15
ER

PT S
AU Muandet, K
   Sriperumbudur, B
   Scholkopf, B
AF Muandet, Krikamol
   Sriperumbudur, Bharath
   Schoelkopf, Bernhard
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Kernel Mean Estimation via Spectral Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing "better" estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.
C1 [Muandet, Krikamol; Schoelkopf, Bernhard] MPI IS, Tubingen, Germany.
   [Sriperumbudur, Bharath] PSU, Dept Stat, University Pk, PA USA.
RP Muandet, K (reprint author), MPI IS, Tubingen, Germany.
EM krikamol@tue.mpg.de; bks18@psu.edu; bs@tue.mpg.de
CR Baldassarre L, 2010, LECT NOTES ARTIF INT, V6321, P56, DOI 10.1007/978-3-642-15880-3_10
   Berlinet A, 2004, REPRODUCING KERNEL H
   De Vito E, 2005, J MACH LEARN RES, V6, P883
   Engl H. W., 1996, MATH ITS APPL, V375
   Fukumizu K, 2013, J MACH LEARN RES, V14, P3753
   GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751
   Gretton A, 2007, NIPS
   Grunewalder S., 2013, P 30 INT C MACH LEAR, V28, P1184
   Kim J, 2012, J MACH LEARN RES, V13, P2529
   Lepski OV, 1997, ANN STAT, V25, P929
   Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517
   Muandet K., 2013, P 29 C UNC ART INT, P449
   Muandet K., 2013, P 30 INT C MACH LEAR, P10
   Muandet K., 2014, P 31 INT C MACH LEAR, P10
   Muandet K., 2012, ADV NEURAL INFORM PR, V25, P10
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Song L., 2013, ADV NEURAL INFORM PR, P3228
   Song L., 2008, P 25 INT C MACH LEAR, P992
   Song L., 2009, ICML
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Steinwart I, 2008, INFORM SCI STAT, P1
   Vito E. D., 2006, SPECTRAL METHODS REG
NR 26
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100018
DA 2019-06-15
ER

PT S
AU Nishihara, R
   Jegelka, S
   Jordan, MI
AF Nishihara, Robert
   Jegelka, Stefanie
   Jordan, Michael, I
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI On the Convergence Rate of Decomposable Submodular Function Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID CYCLIC PROJECTIONS ALGORITHM; SUBSPACES; ANGLES
AB Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of "simple" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.
C1 [Nishihara, Robert; Jegelka, Stefanie; Jordan, Michael, I] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Nishihara, R (reprint author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM rkn@eecs.berkeley.edu; stefje@eecs.berkeley.edu;
   jordan@eecs.berkeley.edu
FU NSF CISE Expeditions Award [CCF-1139158]; LBNL Award [7076018]; DARPA
   XData Award [FA8750-12-2-0331]; Office of Naval Research
   [N00014-11-1-0688]; US ARL; US ARO [W911NF-11-1-0391]; NSF [DGE-1106400]
FX We would like to thank Madalina Persu for suggesting the use of
   Cheeger's inequality. This research is supported in part by NSF CISE
   Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award
   FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The
   Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera,
   EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel,
   Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and
   Yahoo!. This work is supported in part by the Office of Naval Research
   under grant number N00014-11-1-0688, the US ARL and the US ARO under
   grant number W911NF-11-1-0391, and the NSF under grant number
   DGE-1106400.
CR Bach F., 2011, ADV NEURAL INFORM PR
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Bauschke H.H., 1993, SET-VALUED ANAL, V1, P185, DOI DOI 10.1007/BF01027691
   Bauschke HH, 2014, J APPROX THEORY, V185, P63, DOI 10.1016/j.jat.2014.06.002
   BAUSCHKE HH, 1994, J APPROX THEORY, V79, P418, DOI 10.1006/jath.1994.1136
   Bauschke HH, 1996, SIAM REV, V38, P367, DOI 10.1137/S0036144593251710
   Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   BURKE JV, 1988, SIAM J NUMER ANAL, V25, P1197, DOI 10.1137/0725068
   Chung Fan R. K., 1997, SPECTRAL GRAPH THEOR
   DEUTSCH F, 1992, NATO ADV SCI I C-MAT, V356, P105
   DEUTSCH F, 1994, NUMER FUNC ANAL OPT, V15, P537, DOI 10.1080/01630569408816580
   Deutsch F., 2001, BEST APPROXIMATION I, V7
   Deutsch F, 2006, J APPROX THEORY, V142, P36, DOI 10.1016/j.jat.2006.02.005
   Diaconis P, 2010, ILLINOIS J MATH, V54, P963, DOI 10.1215/ijm/1336568522
   Edmonds J., 1970, SUBMODULAR FUNCTIONS, P69
   Fix A., 2013, INT C COMP VIS ICCV
   Fujishige S, 2011, PAC J OPTIM, V7, P3
   Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006
   GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273
   Gubin L., 1967, USSR COMP MATH MATH, V7, P1, DOI DOI 10.1016/0041-5553(67)90113-9
   Halperin I., 1962, ACTA SCI MATH SZEGED, V23, P96
   Hochbaum D., 2009, INT C COMP VIS ICCV
   Iwata S, 2003, SIAM J COMPUT, V32, P833, DOI 10.1137/S0097539701397813
   Jegelka S., 2011, ADV NEURAL INFORM PR
   Jegelka  S., 2013, ADV NEURAL INFORM PR, P1313
   Jenatton R., 2011, JMLR
   Knyazev AV, 2002, SIAM J SCI COMPUT, V23, P2008, DOI 10.1137/S1064827500377332
   Kohli  P., 2009, INT J COMPUTER VISIO, V82
   Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025
   Komodakis N., 2011, IEEE T PATTERN ANAL
   Lin  H., 2011, P INTERSPEECH
   MCCORMICK S. T., 2006, HDB DISCRETE OPTIMIZ, P321, DOI DOI 10.1016/S0927-0507(05)12007-6
   Narasimhan M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P981
   Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2
   Stobbe P., 2010, ADV NEURAL INFORM PR
   Tseng P, 1997, SIAM J OPTIMIZ, V7, P951, DOI 10.1137/S1052623495279797
   Vicente S., 2009, INT C COMP VIS ICCV
   Von Neumann J., 1950, FUNCTIONAL OPERATORS
NR 38
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100034
DA 2019-06-15
ER

PT S
AU Nitanda, A
AF Nitanda, Atsushi
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Stochastic Proximal Gradient Descent with Acceleration Techniques
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.
C1 [Nitanda, Atsushi] NTT DATA Math Syst Inc, Shinjuku Ku, 1F Shinanomachi Rengakan,35 Shinanomachi, Tokyo 1600016, Japan.
RP Nitanda, A (reprint author), NTT DATA Math Syst Inc, Shinjuku Ku, 1F Shinanomachi Rengakan,35 Shinanomachi, Tokyo 1600016, Japan.
EM nitanda@msi.co.jp
CR Agarwal A., 2011, P ADV NEUR INF PROC, P873
   BACH F., 2012, ADV NEURAL INF PROCE, V25, P2672
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2007, CORE DISCUSSION PAPE
   Shalev-Shwartz S., 2012, ARXIV12112717
   Shalev-Shwartz S., 2013, ADV NEURAL INF PROCE, P378
   Shalev-Shwartz S., 2014, P 31 INT C MACH LEAR, P64
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Xiao L., 2014, ARXIV14034699
NR 11
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103061
DA 2019-06-15
ER

PT S
AU O'Connor, L
   Feizi, S
AF O'Connor, Luke
   Feizi, Soheil
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Biclustering Using Message Passing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice. Moreover, Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations, we find that our method outperforms two of the best existing biclustering algorithms, ISA and LAS, when the planted clusters overlap. Applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.
C1 [O'Connor, Luke] Harvard Univ, Bioinformat & Integrat Genom, Cambridge, MA 02138 USA.
   [Feizi, Soheil] MIT, Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
RP O'Connor, L (reprint author), Harvard Univ, Bioinformat & Integrat Genom, Cambridge, MA 02138 USA.
EM loconnor@g.harvard.edu; sfeizi@mit.edu
FU Harvard Division of Medical Sciences
FX We would like to thank Professor Manolis Kellis and Professor Muriel
   Medard for their advice and support. We would like to thank the Harvard
   Division of Medical Sciences for supporting this project.
CR Bergmann S, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.031902
   Bisson Gilles, 2008, MACH LEARN APPL 2008
   Caldas Jose, 2008, MACH LEARN SIGN PROC
   Cheng Yizong, 2000, ISMB, V8
   Dao P, 2010, BIOINFORMATICS, V26, pi625, DOI 10.1093/bioinformatics/btq393
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dueck Delbert, 2008, RES COMPUTATIONAL MO
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Govaert G, 2008, COMPUT STAT DATA AN, V52, P3233, DOI 10.1016/j.csda.2007.09.007
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Li L, 2012, BMC BIOPHYS, V5, DOI 10.1186/2046-1682-5-9
   Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/nmeth.2016, 10.1038/NMETH.2016]
   Nadakuditi RR, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.188701
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Prelic A, 2006, BIOINFORMATICS, V22, P1122, DOI 10.1093/bioinformatics/btl060
   Shabalin AA, 2009, ANN APPL STAT, V3, P985, DOI 10.1214/09-AOAS239
   Tanay Amos, 2002, Bioinformatics, V18 Suppl 1, pS136
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103054
DA 2019-06-15
ER

PT S
AU Orabona, F
AF Orabona, Francesco
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Simultaneous Model Selection and Optimization through Parameter-free
   Stochastic Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ONLINE; REGULARIZATION; CLASSIFICATION; CLASSIFIERS
AB Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.
C1 [Orabona, Francesco] Yahoo Labs, New York, NY 10036 USA.
RP Orabona, F (reprint author), Yahoo Labs, New York, NY 10036 USA.
EM francesco@orabona.com
CR Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795
   Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Blanchard G, 2008, ANN STAT, V36, P489, DOI 10.1214/009053607000000839
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chang Chih-Chung, 2001, LIBSVM LIB SUPPORT V
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P297
   Chen DR, 2004, J MACH LEARN RES, V5, P1143
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Luo H., 2014, ADV NEURAL INFORM PR
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan H. B., 2014, COLT
   Mendelson S, 2010, ANN STAT, V38, P526, DOI 10.1214/09-AOS728
   Orabona F., 2014, ARXIV14063816
   Orabona Francesco, 2013, ADV NEURAL INFORM PR, P1806
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rosasco L., 2014, ARXIV14050042
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Smale S, 2006, FOUND COMPUT MATH, V6, P145, DOI 10.1007/s10208-004-0160-z
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart Ingo, 2009, COLT
   Tarres P., ARXIV11035538
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Yao YA, 2010, IEEE T INFORM THEORY, V56, P6470, DOI 10.1109/TIT.2010.2079010
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Ying YM, 2006, IEEE T INFORM THEORY, V52, P4775, DOI 10.1109/TIT.2006.883632
   Zhang T., 2004, P 21 INT C MACH LEAR, P919, DOI DOI 10.1145/1015330.1015332
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 36
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102064
DA 2019-06-15
ER

PT S
AU Pan, Y
   Theodorou, EA
AF Pan, Yunpeng
   Theodorou, Evangelos A.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Probabilistic Differential Dynamic Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.
C1 [Pan, Yunpeng; Theodorou, Evangelos A.] Georgia Inst Technol, Inst Robot & Intelligent Machines, Daniel Guggenheim Sch Aerosp Engn, Atlanta, GA 30332 USA.
RP Pan, Y (reprint author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Daniel Guggenheim Sch Aerosp Engn, Atlanta, GA 30332 USA.
EM ypan37@gatech.edu; evangelos.theodorou@ae.gatech.edu
FU National Science Foundation [NRI-1426945]
FX This work was partially supported by a National Science Foundation grant
   NRI-1426945.
CR Abbeel P., 2007, ADV NEURAL INFORM PR, V19, P1
   Candela J. Quinonero, 2003, IEEE INT C AC SPEECH
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Deisenroth M. P., 2014, IEEE TRANSSACTIONS P, V27, P75
   Deisenroth MP, 2009, NEUROCOMPUTING, V72, P1508, DOI 10.1016/j.neucom.2008.12.019
   Hemakumara P, 2013, IEEE T ROBOT, V29, P813, DOI 10.1109/TRO.2013.2258732
   Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC
   Levine  S., 2013, ADV NEURAL INFORM PR, V2013, P207
   Mitrovic D, 2010, STUD COMPUT INTELL, V264, P65
   Morimoto J., 2002, ADV NEURAL INFORM PR, P1539
   Nguyen-Tuong D., 2008, ADV NEURAL INFORM PR, V21, P1193
   Raiko T, 2009, NEUROCOMPUTING, V72, P3704, DOI 10.1016/j.neucom.2009.06.009
   Rasmussen CE, 2004, ADV NEUR IN, V16, P751
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Tassa Y., NIPS, P1465
   Theodorou E, 2010, P AMER CONTR CONF, P1125
   Todorov E, 2005, P AMER CONTR CONF, P300, DOI 10.1109/ACC.2005.1469949
   van den Berg J, 2012, INT J ROBOT RES, V31, P1263, DOI 10.1177/0278364912456319
   van Hasselt H. P, 2011, INSIGHTS REINFORCEME
   Zhong W, 2001, PROCEEDINGS OF THE 2001 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS (CCA'01), P896, DOI 10.1109/CCA.2001.973983
NR 22
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100027
DA 2019-06-15
ER

PT S
AU Park, D
   Caramanis, C
   Sanghavi, S
AF Park, Dohyung
   Caramanis, Constantine
   Sanghavi, Sujay
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Greedy Subspace Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FACE RECOGNITION
AB We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.
C1 [Park, Dohyung; Caramanis, Constantine; Sanghavi, Sujay] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
RP Park, D (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM dhpark@utexas.edu; constantine@utexas.edu; sanghavi@mail.utexas.edu
FU NSF [1302435, 0954059, 1017525, 1056028]; DTRA [HDTRA1-13-1-0024]; U.S.
   Department of Transportation through the Data-Supported Transportation
   Operations and Planning (D-STOP) Tier 1 University Transportation Center
FX The authors would like to acknowledge NSF grants 1302435, 0954059,
   1017525, 1056028 and DTRA grant HDTRA1-13-1-0024 for supporting this
   research. This research was also partially supported by the U.S.
   Department of Transportation through the Data-Supported Transportation
   Operations and Planning (D-STOP) Tier 1 University Transportation
   Center.
CR Bradley PS, 2000, J GLOBAL OPTIM, V16, P23, DOI 10.1023/A:1008324625522
   Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9
   Dyer EL, 2013, J MACH LEARN RES, V14, P2487
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Heckel R., 2014, ARXIV13074891V2
   Heckel R., 2013, IEEE INT C AC SPEECH
   Ho J., 2003, IEEE C COMP VIS PATT
   Inglot T, 2010, PROBAB MATH STAT-POL, V30, P339
   Kriegel HP, 2009, ACM T KNOWL DISCOV D, V3, DOI 10.1145/1497577.1497578
   Kunis S, 2008, FOUND COMPUT MATH, V8, P737, DOI 10.1007/s10208-007-9005-x
   LEDOUX M., 2005, CONCENTRATION MEASUR, V89
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Milman V, 1986, LECT NOTES MATH
   Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034
   Tron R., 2007, IEEE C COMP VIS PATT
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Tseng P, 2000, J OPTIMIZ THEORY APP, V105, P249, DOI 10.1023/A:1004678431677
   Vidal R, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P167
   Vidal R., 2003, IEEE C COMP VIS PATT
   Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang Y.-X., 2013, ADV NEURAL INFORM PR
   Yang A. Y., 2006, IEEE C COMP VIS PATT
   Zhang T, 2012, INT J COMPUT VISION, V100, P217, DOI 10.1007/s11263-012-0535-6
NR 26
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100087
DA 2019-06-15
ER

PT S
AU Patrini, G
   Nock, R
   Rivera, P
   Caetano, T
AF Patrini, Giorgio
   Nock, Richard
   Rivera, Paul
   Caetano, Tiberio
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI (Almost) No Label No Cry
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FRAMEWORK
AB In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to approximate to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.
C1 [Patrini, Giorgio; Nock, Richard; Rivera, Paul; Caetano, Tiberio] Australian Natl Univ, Sydney, NSW, Australia.
   [Patrini, Giorgio; Nock, Richard; Rivera, Paul] NICTA, Sydney, NSW, Australia.
   [Caetano, Tiberio] Univ New South Wales, Sydney, NSW, Australia.
   [Caetano, Tiberio] Ambiata, Sydney, NSW, Australia.
RP Patrini, G (reprint author), Australian Natl Univ, Sydney, NSW, Australia.
EM giorgio.patrini@anu.edu.au; richard.nock@anu.edu.au;
   paul.rivera@anu.edu.au; tiberio.caetano@anu.edu.au
FU Australian Government through the Department of Communications;
   Australian Research Council through the ICT Centre of Excellence Program
FX NICTA is funded by the Australian Government through the Department of
   Communications and the Australian Research Council through the ICT
   Centre of Excellence Program. The first author would like to acknowledge
   that part of this research was conducted during his internship at the
   Commonwealth Bank of Australia. We thank A. Menon and D. Garcia-Garcia
   for useful discussions.
CR Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13
   BACHE K., 2013, UCI MACHINE LEARNING
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Beygelzimer A., 2005, P 22 INT C MACH LEAR, P49
   Chen B. -C., 2006, P 22 INT C DAT ENG I, P3
   Chen S, 2009, INT CONF DAT MIN WOR, P356, DOI 10.1109/ICDMW.2009.33
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Fan K, 2014, NEUROCOMPUTING, V139, P47, DOI 10.1016/j.neucom.2013.09.057
   Graca J., 2007, ADV NEURAL INFORM PR, P569
   Hernandez-Gonzalez J, 2013, PATTERN RECOGN, V46, P3425, DOI 10.1016/j.patcog.2013.05.002
   Kearns M., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P459, DOI 10.1145/237814.237994
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Kueck H., 2005, P 21 C UNC ART INT, P332
   Lai K. -T., 2014, 11 CVPR
   Liang P., 2009, 26 ANN INT C MACH LE, P641
   Mann G. -S., 2008, 46 ACL
   Musicant DR, 2007, IEEE DATA MINING, P252, DOI 10.1109/ICDM.2007.50
   Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225
   Patrini G., 2014, NIPS 27
   Quadrianto N, 2009, J MACH LEARN RES, V10, P2349
   RUEPING S., 2010, P 27 INT C MACH LEAR, P911
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Stolpe M, 2011, LECT NOTES ARTIF INT, V6913, P349, DOI 10.1007/978-3-642-23808-6_23
   Wojtusiak J., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P84, DOI 10.1109/ICMLA.2011.154
   Yu F. X., 2013, P 30 INT C MACH LEAR, P504
   Yu F. -X., 2014, CORR
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102014
DA 2019-06-15
ER

PT S
AU Prasad, A
   Jegelka, S
   Batra, D
AF Prasad, Adarsh
   Jegelka, Stefanie
   Batra, Dhruv
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Submodular meets Structured: Finding Diverse Subsets in
   Exponentially-Large Structured Item Sets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, trade-offs, and show that our constructions lead to significantly better proposals.
C1 [Prasad, Adarsh] UT Austin, Austin, TX 78712 USA.
   [Jegelka, Stefanie] Univ Calif Berkeley, Berkeley, CA USA.
   [Batra, Dhruv] Virginia Tech, Blacksburg, VA USA.
RP Prasad, A (reprint author), UT Austin, Austin, TX 78712 USA.
EM adarsh@cs.utexas.edu; stefje@eecs.berkeley.edu; dbatra@vt.edu
FU National Science Foundation [IIS-1353694, IIS-1350553]; Army Research
   Office YIP Award [W911NF-14-1-0180]; Office of Naval Research Award
   [N00014-14-1-0679]
FX We thank Xiao Lin for his help. The majority of this work was done while
   AP was an intern at Virginia Tech. AP and DB were partially supported by
   the National Science Foundation under Grant No. IIS-1353694 and
   IIS-1350553, the Army Research Office YIP Award W911NF-14-1-0180, and
   the Office of Naval Research Award N00014-14-1-0679, awarded to DB. SJ
   was supported by gifts from Amazon Web Services, Google, SAP, The Thomas
   and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC,
   Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft,
   NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!.
CR Batra D., 2012, UAI
   Batra D., 2012, ECCV
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Buchbinder N., 2012, FOCS
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Chen C., 2013, AISTATS
   Chen C., 2014, NIPS
   Delong A, 2010, PROC CVPR IEEE, P2173, DOI 10.1109/CVPR.2010.5539897
   Dey D, 2012, ROBOTICS SCI SYSTEMS
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Feige U., 2007, FOCS
   Goundan P., 2009, REVISITING GRE UNPUB
   Guzman-Rivera A., 2012, P NIPS
   Guzman-Rivera A., 2014, AISTATS
   Jegelka S., 2011, CVPR
   Kempe D., 2003, ACM SIGKDD C KNOWL D
   Kohli P., 2013, CVPR
   Kohli P., 2010, CVPR
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krause A., 2013, ICML TUTORIAL
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Kulesza A., 2010, P NIPS
   Lafferty J. O., 2001, ICML
   LeCun Y., 2006, PREDICTING STRUCTURE
   Lin H., 2011, ACL
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Park D., 2011, ICCV
   Streeter  Matthew, 2008, NIPS
   Tarlow D., 2010, INT C ART INT STAT, P812
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yanover C., 2003, NIPS
NR 34
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100104
DA 2019-06-15
ER

PT S
AU Putzky, P
   Franzen, F
   Bassetto, G
   Macke, JH
AF Putzky, Patrick
   Franzen, Florian
   Bassetto, Giacomo
   Macke, Jakob H.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Bayesian model for identifying hierarchically organised states in
   neural population activity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales. To understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs. Here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity. Population states are modelled using a hidden Markov decision tree with state-dependent tuning parameters and a generalised linear observation model. We present a variational Bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings. On simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters. Using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model. Finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.
C1 [Putzky, Patrick; Franzen, Florian; Bassetto, Giacomo; Macke, Jakob H.] Max Planck Inst Biol Cybernet, Tubingen, Germany.
   [Putzky, Patrick; Franzen, Florian] Univ Tubingen, Grad Training Ctr Neurosci, Tubingen, Germany.
   [Putzky, Patrick; Franzen, Florian; Bassetto, Giacomo; Macke, Jakob H.] Bernstein Ctr Computat Neurosci, Tubingen, Germany.
RP Putzky, P (reprint author), Max Planck Inst Biol Cybernet, Tubingen, Germany.
EM patrick.putzky@gmail.com; florian.franzen@tuebingen.mpg.de;
   giacomo.bassetto@tuebingen.mpg.de; jakob@tuebingen.mpg.de
FU German Federal Ministry of Education and Research (BMBF) [FKZ:
   01GQ1002]; Max Planck Society
FX We are grateful to the authors of [2] for sharing their data
   (toliaslab.org/publications/ecker-et-al-2014/) and to Alexander Ecker,
   William McGhee, Marcel Nonnenmacher and David Janssen for comments on
   the manuscript. This work was funded by the German Federal Ministry of
   Education and Research (BMBF; FKZ: 01GQ1002, Bernstein Center Tubingen)
   and the Max Planck Society. Supplementary details and code are available
   at www.mackelab.org.
CR Aston-Jones G, 2005, ANNU REV NEUROSCI, V28, P403, DOI 10.1146/annurev.neuro.28.061604.135709
   Beal M. J., 2003, VARIATIONAL ALGORITH
   Bengio Y., 1995, Advances in Neural Information Processing Systems 7, P427
   Bezdudnaya T, 2006, NEURON, V49, P421, DOI 10.1016/j.neuron.2006.01.010
   Bishop C.M., 2003, P 19 C UNC ART INT, P57
   Bishop C. M., 2006, PATTERN RECOGNITION
   Chen Z., 2009, NEURAL COMPUTATION, V21
   Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006
   Escola S., 2011, NEURAL COMPUTATION, V23
   Gerwinn S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00012
   Harris K. D., 2011, NATURE REV NEUROSCIE, V12
   Jaakkola T. S., 1996, VARIATIONAL APPROACH
   Jones L. M., 2007, PNAS, V104
   Jordan M. I., 1994, NEURAL COMPUTATION, V6
   Jordan MI, 1997, ADV NEUR IN, V9, P501
   Kisley MA, 1999, J NEUROSCI, V19, P10451
   Lewi J., 2009, NEURAL COMPUTATION, V21
   Logothetis NK, 2012, NATURE, V491, P547, DOI 10.1038/nature11618
   MacKay D, 1994, ASHRAE T, V100, P1053
   Mackay D. J. C., 1997, TECH REP
   Macke JH, 2011, ADV NEURAL INFORM PR, V24
   Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0
   Shababo B, 2013, NIPS, P1304
   Steriade M, 2005, BRAIN CONTROL WAKEFU
   Zagha E, 2013, NEURON, V79, P567, DOI 10.1016/j.neuron.2013.06.008
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101009
DA 2019-06-15
ER

PT S
AU Saade, A
   Krzakala, F
   Zdeborova, L
AF Saade, Alaa
   Krzakala, Florent
   Zdeborova, Lenka
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Spectral Clustering of Graphs with the Bethe Hessian
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID STOCHASTIC BLOCKMODELS; COMMUNITY STRUCTURE
AB Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.
C1 [Saade, Alaa] Ecole Normale Super, CNRS UMR 8550, Lab Phys Stat, 24 Rue Lhomond, F-75005 Paris, France.
   [Krzakala, Florent] UPMC Univ Paris 06, Sorbonne Univ, Ecole Normale Super, Lab Phys Stat,CNRS UMR 8550, F-75005 Paris, France.
   [Zdeborova, Lenka] CEA Saclay, Inst Phys Theor, F-91191 Gif Sur Yvette, France.
   [Zdeborova, Lenka] CNRS URA 2306, F-91191 Gif Sur Yvette, France.
RP Saade, A (reprint author), Ecole Normale Super, CNRS UMR 8550, Lab Phys Stat, 24 Rue Lhomond, F-75005 Paris, France.
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU ERC under the European Union [307087-SPARCS]
FX This work has been supported in part by the ERC under the European
   Union's 7th Framework Programme Grant Agreement 307087-SPARCS
CR Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Bordenave C, 2010, RANDOM STRUCT ALGOR, V37, P332, DOI 10.1002/rsa.20313
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Decelle A, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.065701
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Lusseau D, 2003, BEHAV ECOL SOCIOBIOL, V54, P396, DOI 10.1007/s00265-003-0651-y
   Massoulie Laurent, 2013, ARXIV13113085
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Mooij Joris M, 2004, NIPS
   Mossel E, 2013, ARXIV13114115
   Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104
   Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015
   Rogers T, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.031116
   RUHE A, 1973, SIAM J NUMER ANAL, V10, P674, DOI 10.1137/0710059
   Saade A, 2014, EPL-EUROPHYS LETT, V107, DOI 10.1209/0295-5075/107/50005
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   WANG YJ, 1987, J AM STAT ASSOC, V82, P8, DOI 10.2307/2289119
   Watanabe Y., 2009, ADV NEURAL INF PROCE, P2017
   ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752
NR 23
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102081
DA 2019-06-15
ER

PT S
AU Schober, M
   Duvenaud, D
   Hennig, P
AF Schober, Michael
   Duvenaud, David
   Hennig, Philipp
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Probabilistic ODE Solvers with Runge-Kutta Means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.
C1 [Schober, Michael; Hennig, Philipp] MPI Intelligent Syst, Tubingen, Germany.
   [Duvenaud, David] Univ Cambridge, Dept Engn, Cambridge, England.
RP Schober, M (reprint author), MPI Intelligent Syst, Tubingen, Germany.
EM mschober@tue.mpg.de; dkd23@cam.ac.uk; phennig@tue.mpg.de
CR Butcher J.C., 1963, J AUST MATH SOC, V3, P185, DOI [10.1017/S1446788700027932, DOI 10.1017/S1446788700027932]
   Calderhead B., 2008, ADV NEURAL INFORM PR
   Ceschino F., 1963, PROBLEMES DIFFERENTI
   Chkrebtii O., 2013, 13062365 ARXIV
   Dondelinger F., 2013, JMLR WORKSHOP C P, P216
   Dormand J. R., 1980, J COMPUT APPL MATH, V6, P19, DOI [DOI 10.1016/0771-050X(80)90013-3, 10.1016/0771-050X(80)90013-3]
   Ghahramani  Z., 1996, CRGTR962 U TOTR DEP
   Graepel T., 2003, INT C MACH LEARN ICM
   Hairer E., 2012, PRINCETON COMPANION
   Hairer E., 1987, SOLVING ORDINARY DIF
   Hennig P., 2014, P 17 INT C ART INT S, V33
   Kutta W., 1901, Z MATH PHYS, V46, P435
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   RUNGE C., 1896, MATH ANN, V46, P167, DOI [DOI 10.1007/BF01446807, 10.1007/BF01446807]
   Sarkka S., 2013, BAYESIAN FILTERING S
   Schober M., 2014, MED IMAGE COMPUTING
   SHANKS EB, 1966, MATH COMPUT, V20, P21, DOI 10.2307/2004265
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
   Skilling J., 1991, MAXIMUM ENTROPY BAYE
   Wang Yali, 2014, INT C MACH LEARN ICM
   Wiener N., 1950, B AM MATH SOC, V56, P378
NR 21
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102012
DA 2019-06-15
ER

PT S
AU Semedo, JD
   Zandvakili, A
   Kohn, A
   Machens, CK
   Yu, BM
AF Semedo, Joao D.
   Zandvakili, Amin
   Kohn, Adam
   Machens, Christian K.
   Yu, Byron M.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Extracting Latent Structure From Multiple Interacting Neural Populations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID INFORMATION; DYNAMICS
AB Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how these latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.
C1 [Semedo, Joao D.; Yu, Byron M.] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
   [Semedo, Joao D.] Inst Super Tecn, Dept Elect & Comp Engn, Lisbon, Portugal.
   [Semedo, Joao D.; Machens, Christian K.] Champalimaud Ctr Unknown, Champalimaud Neurosci Programme, Lisbon, Portugal.
   [Zandvakili, Amin; Kohn, Adam] Albert Einstein Coll Med, Dominick Purpura Dept Neurosci, Bronx, NY 10467 USA.
   [Yu, Byron M.] Carnegie Mellon Univ, Dept Biomed Engn, Pittsburgh, PA 15213 USA.
RP Semedo, JD (reprint author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
EM jsemedo@cmu.edu; amin.zandvakili@einstein.yu.edu;
   adam.kohn@einstein.yu.edu; christian.machens@neuro.fchampalimaud.org;
   byronyu@cmu.edu
CR Ahrens MB, 2012, NATURE, V485, P471, DOI 10.1038/nature11057
   Bach F. R., 2005, PROBABILISTIC INTERP
   Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802
   Brendel W., 2011, ADV NEURAL INF PROCE, P2654
   Brian DO Anderson, 2012, OPTIMAL FILTERING
   Crowe DA, 2013, NAT NEUROSCI, V16, P1484, DOI 10.1038/nn.3509
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Everitt B. S., 1984, INTRO LATENT VARIABL
   Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402
   Fries P, 2005, TRENDS COGN SCI, V9, P474, DOI 10.1016/j.tics.2005.08.011
   Gregoriou GG, 2009, SCIENCE, V324, P1207, DOI 10.1126/science.1171402
   Jia XX, 2013, NEURON, V77, P762, DOI 10.1016/j.neuron.2012.12.036
   KAMINSKI MJ, 1991, BIOL CYBERN, V65, P203, DOI 10.1007/BF00198091
   Kim S, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001110
   Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Quinn CJ, 2011, J COMPUT NEUROSCI, V30, P17, DOI 10.1007/s10827-010-0247-2
   Saalmann YB, 2012, SCIENCE, V337, P753, DOI 10.1126/science.1223082
   Salazar RF, 2012, SCIENCE, V338, P1097, DOI 10.1126/science.1224000
   Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622
   Stopfer M, 2003, NEURON, V39, P991, DOI 10.1016/j.neuron.2003.08.011
   Vazquez Y, 2013, P NATL ACAD SCI USA, V110, pE2635, DOI 10.1073/pnas.1309728110
   Yu Byron M., 2008, NIPS, P1881
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103076
DA 2019-06-15
ER

PT S
AU Sharma, A
   Tuzel, O
   Liu, MY
AF Sharma, Abhishek
   Tuzel, Oncel
   Liu, Ming-Yu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Recursive Context Propagation Network for Semantic Scene Labeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256 x 256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.
C1 [Sharma, Abhishek] Univ Maryland, College Pk, MD 20742 USA.
   [Tuzel, Oncel; Liu, Ming-Yu] MERL, Cambridge, MA USA.
RP Sharma, A (reprint author), Univ Maryland, College Pk, MD 20742 USA.
EM bhokaal@cs.umd.edu; oncel@merl.com; mliu@merl.com
CR Daniel Munoz J., 2010, ECCV
   Farabet Clement, 2013, IEEE TPAMI
   Fergus R., 2012, IEEE CVPR
   Goller Christoph, 1995, INT C NEUR NETW
   Gould Stephen, 2009, IEEE ICCV
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Kumar M. Pawan, 2010, IEEE CVPR
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Lempitsky V., 2011, NIPS
   Lin T.-Y., 2014, ECCV
   Liu Ce, 2011, IEEE TPAMI, V33
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu Ming- Yu, 2011, IEEE CVPR
   Mottaghi Roozbeh, 2014, IEEE CVPR
   Mottaghi Roozbeh, 2013, IEEE CVPR
   Pinheiro P. H., 2014, ICML
   Polatkan Gungor, 2011, UAI, P609
   Singh Gautam, 2013, IEEE CVPR
   Socher R., 2011, ICML
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Tighe Joseph, 2013, IEEE CVPR
   Torralba A., 2003, IEEE CVPR
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
NR 23
TC 1
Z9 1
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100061
DA 2019-06-15
ER

PT S
AU Srikumar, V
   Manning, CD
AF Srikumar, Vivek
   Manning, Christopher D.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Learning Distributed Representations for Structured Output Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB In recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs. However, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning. In this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters. We extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches. We define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning. We show that our formulation outperforms structural SVM baselines in two tasks: multiclass document classification and part-of-speech tagging.
C1 [Srikumar, Vivek] Univ Utah, Salt Lake City, UT 84112 USA.
   [Manning, Christopher D.] Stanford Univ, Stanford, CA 94305 USA.
RP Srikumar, V (reprint author), Univ Utah, Salt Lake City, UT 84112 USA.
EM svivek@cs.utah.edu; manning@cs.stanford.edu
FU Defense Advanced Research Projects Agency (DARPA) Deep Exploration and
   Filtering of Text (DEFT) Program under Air Force Research Laboratory
   (AFRL) [FA8750-13-2-0040]
FX We thank the anonymous reviewers for their valuable comments. Stanford
   University gratefully acknowledges the support of the Defense Advanced
   Research Projects Agency (DARPA) Deep Exploration and Filtering of Text
   (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.
   FA8750-13-2-0040. Any opinions, findings, and conclusion or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the view of the DARPA, AFRL, or the US
   government.
CR Abernethy J, 2006, CS0611124 ARXIV
   Amit Y., 2007, INT C MACH LEARN
   Argyriou Andreas, 2007, ADV NEURAL INFORM PR
   Bengio Y, 2013, IEEE T PATTERN ANAL
   Brants T., 2000, C APPL NAT LANG PROC
   Cesa- Bianchi N., 2006, INT C MACH LEARN
   Coates A., 2011, INT C ART INT STAT
   Collins M., 2002, C EMP METH NAT LANG
   Collobert R., 2011, J MACHINE LEARNING R, V12
   Fazel M., 2004, P AM CONTR C, V4
   Hinton G. E., 1988, ANN C COGN SCI SOC
   Lafferty J., 2001, MACHINE LEARNING
   Lang K., 1995, INT C MACH LEARN
   Lei T., 2014, ANN M ASS COMP LING
   Marcus M., 1994, WORKSH HUM LANG TECH
   Mikolov T., 2013, ARXIV13013781
   Nivre J., 2007, CONLL SHAR TASK SESS
   Parikh N., 2013, FDN TRENDS OPTIM, P1
   Petrov  S., 2011, ARXIV11042086
   PLATE TA, 1995, IEEE T NEURAL NETWOR, V6, P623, DOI 10.1109/72.377968
   Smolensky P., 1990, ARTIFICIAL INTELLIGE, V46
   Socher R., 2012, EMPIRICAL METHODS NA
   SREBRO N., 2004, ADV NEURAL INFORM PR
   Toutanova K., 2003, C N AM CHAPT ASS COM
   Tsochantaridis Ioannis, 2005, J MACHINE LEARNING R
   Turian J., 2010, ANN M ASS COMP LING
   Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100102
DA 2019-06-15
ER

PT S
AU Stachenfeld, KL
   Botvinick, MM
   Gershman, SJ
AF Stachenfeld, Kimberly L.
   Botvinick, Matthew M.
   Gershman, Samuel J.
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Design Principles of the Hippocampal Cognitive Map
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID PLACE FIELDS; ENVIRONMENT; NAVIGATION; GOAL; REPRESENTATIONS; LOCATION;
   MODEL
AB Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.
C1 [Stachenfeld, Kimberly L.; Botvinick, Matthew M.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Stachenfeld, Kimberly L.; Botvinick, Matthew M.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA.
   [Gershman, Samuel J.] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
RP Stachenfeld, KL (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
EM kls4@princeton.edu; matthewb@princeton.edu; sjgershm@mit.edu
CR Alvernhe A, 2011, EUR J NEUROSCI, V33, P1696, DOI 10.1111/j.1460-9568.2011.07653.x
   Blair HT, 2007, J NEUROSCI, V27, P3211, DOI 10.1523/JNEUROSCI.4724-06.2007
   Botvinick MM, 2009, COGNITION, V113, P262, DOI 10.1016/j.cognition.2008.08.011
   Brandon MP, 2014, NEURON, V82, P789, DOI 10.1016/j.neuron.2014.04.013
   Daw ND, 2005, NAT NEUROSCI, V8, P1704, DOI 10.1038/nn1560
   DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613
   Derdikman D, 2009, NAT NEUROSCI, V12, P1325, DOI 10.1038/nn.2396
   Diuk C, 2013, J NEUROSCI, V33, P5797, DOI 10.1523/JNEUROSCI.5445-12.2013
   Fanselow MS, 2010, TRENDS COGN SCI, V14, P7, DOI 10.1016/j.tics.2009.10.008
   Fenton A., 2001, EUROPEAN J NEUROSCIE, V12, P3450
   Foster DJ, 2000, HIPPOCAMPUS, V10, P1, DOI 10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1
   Franzius M., 2007, PLOS COMPUTATIONAL B, V3, P3287
   Gallistel C. R., 1990, ORG LEARNING
   Gershman SJ, 2012, NEURAL COMPUT, V24, P1553, DOI 10.1162/NECO_a_00282
   Gustafson NJ, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002235
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   HIRTLE SC, 1985, MEM COGNITION, V13, P208, DOI 10.3758/BF03197683
   Hollup SA, 2001, J NEUROSCI, V21, P1635
   Howard MW, 2005, PSYCHOL REV, V112, P75, DOI 10.1037/0033-295X.112.1.75
   Howard MW, 2002, J MATH PSYCHOL, V46, P269, DOI 10.1006/jmps.2001.1388
   Kobayashi T, 2003, NEUROSCIENCE, V117, P1025, DOI 10.1016/S0306-4522(02)00700-5
   Konidaris  George, 2011, AAAI
   Krupic J, 2012, SCIENCE, V337, P853, DOI 10.1126/science.1222403
   Lenck-Santini PP, 2001, EUR J NEUROSCI, V13, P1055, DOI 10.1046/j.0953-816x.2001.01481.x
   Lenck-Santini PP, 2002, J NEUROSCI, V22, P9035
   Mahadevan S, 2009, FOUND TRENDS MACH LE, V1, P403, DOI 10.1561/2200000003
   McNaughton BL, 2006, NAT REV NEUROSCI, V7, P663, DOI 10.1038/nrn1932
   Mehta MR, 2000, NEURON, V25, P707, DOI 10.1016/S0896-6273(00)81072-7
   Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918
   Menache I, 2002, LECT NOTES ARTIF INT, V2430, P295
   Morgan LK, 2011, J NEUROSCI, V31, P1238, DOI 10.1523/JNEUROSCI.4667-10.2011
   Muller RU, 1996, J GEN PHYSIOL, V107, P663, DOI 10.1085/jgp.107.6.663
   MULLER RU, 1987, J NEUROSCI, V7, P1951
   O'Keefe J, 1978, HIPPOCAMPUS COGNITIV
   Reid AK, 1998, PSYCHOL REV, V105, P585, DOI 10.1037//0033-295X.105.3.585
   Ribas-Fernandes JJF, 2011, NEURON, V71, P370, DOI 10.1016/j.neuron.2011.05.042
   Schapiro A. C., 2013, NAT NEUROSCI, V16
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simsek O., 2005, P 22 INT C MACH LEAR, P816, DOI DOI 10.1145/1102351.1102454
   Skaggs WE, 1998, J NEUROSCI, V18, P8455
   SPEAKMAN A, 1990, EUR J NEUROSCI, V2, P544, DOI 10.1111/j.1460-9568.1990.tb00445.x
   Sprekeler H, 2011, NEURAL COMPUT, V23, P3287, DOI 10.1162/NECO_a_00214
   STEVENS A, 1978, COGNITIVE PSYCHOL, V10, P422, DOI 10.1016/0010-0285(78)90006-3
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626
   Wills TJ, 2010, SCIENCE, V328, P1573, DOI 10.1126/science.1188224
   Wiltgen BJ, 2006, J NEUROSCI, V26, P5484, DOI 10.1523/JNEUROSCI.2685-05.2006
NR 48
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101011
DA 2019-06-15
ER

PT S
AU Stollenga, MF
   Masci, J
   Gomez, F
   Schmidhuber, J
AF Stollenga, Marijn F.
   Masci, Jonathan
   Gomez, Faustino
   Schmidhuber, Juergen
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Deep Networks with Internal Selective Attention through Feedback
   Connections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID FEEDFORWARD
AB Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.
C1 [Stollenga, Marijn F.; Masci, Jonathan; Gomez, Faustino; Schmidhuber, Juergen] USI SUPSI, IDSIA, Manno Lugano, Switzerland.
RP Stollenga, MF (reprint author), USI SUPSI, IDSIA, Manno Lugano, Switzerland.
EM marijn@idsia.ch; jonathan@idsia.ch; tino@idsia.ch; juergen@idsia.ch
CR Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103
   Behnke S, 2005, NEURAL COMPUT APPL, V14, P97, DOI 10.1007/s00521-004-0444-x
   Behnke S., 2001, International Journal of Computational Intelligence and Applications, V1, P427, DOI 10.1142/S1469026801000342
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Bullier J, 2004, METH NE FRO NEUROSCI, P181
   Ciresan D., 2012, CVPR
   Ciresan D C, 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Dan C. C., 2012, NIPS, P2852
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fukushima K, 2003, LECT NOTES COMPUT SC, V2714, P393
   Fukushima Kunihiko, 1979, T IECE JAPAN A, V62, P658
   Gabor D., 1946, Journal of the Institution of Electrical Engineers. III. Radio and Communication Engineering, V93, P429
   Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019
   GLASMACHERS T, 2010, P 12 ANN C GEN EV CO, P393
   Goodfellow I, 2013, ICML
   Hinton G. E, 2012, ARXIV12070580
   Hupe JM, 1998, NATURE, V394, P784, DOI 10.1038/29537
   Itti L., 2007, VISUAL SALIENCE, V2, P3327
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Knoll B, 2012, IEEE DATA COMPR CONF, P377, DOI 10.1109/DCC.2012.44
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Krizhevsky A., 2009, THESIS
   Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X
   Lamme VAF, 2001, ACTA PSYCHOL, V107, P209, DOI 10.1016/S0001-6918(01)00020-8
   Larochelle H., 2010, IMAGE, V1, px2
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Min Lin, 2013, CORR
   O'Reilly RC, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00124
   OReilly RC, 1996, NEURAL COMPUT, V8, P895, DOI 10.1162/neco.1996.8.5.895
   RECHENBERG I, 1971, THESIS
   Schaul T, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P845
   Schmidhuber J., 2014, IDSIA0314 SWISS AI L
   Schmidhuber J., 1991, INT J NEURAL SYST, V2, P135, DOI DOI 10.1142/S012906579100011X
   Sermanet P, 2013, CVPR
   Srivastava R. K., 2013, NIPS
   Stollenga M. F., 2011, THESIS
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   VanRullen R, 2006, VISION RES, V46, P3017, DOI 10.1016/j.visres.2005.07.009
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Welinder P., 2010, CNSTR2010001 CAL I T
   Weng J., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P576, DOI 10.1109/IJCNN.1992.287150
   WHITEHEAD S, 1992, THESIS
   Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255
   Wyatte D, 2012, J COGNITIVE NEUROSCI, V24, P2248, DOI 10.1162/jocn_a_00282
   Wyatte D, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00182
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2013, ARXIV13112901CSCV
   Zeiler MD, 2013, ARXIV201313013557
NR 52
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100055
DA 2019-06-15
ER

PT S
AU Titsias, MK
   Yau, C
AF Titsias, Michalis K.
   Yau, Christopher
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.
C1 [Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece.
   [Yau, Christopher] Univ Oxford, Wellcome Trust Ctr Human Genet, Oxford, England.
RP Titsias, MK (reprint author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.
EM mtitsias@aueb.gr; cyau@well.ox.ac.uk
OI Yau, Christopher/0000-0001-7615-8523
FU UK Medical Research Council New Investigator Research Grant
   [MR/L001411/1]; Research Funding at AUEB for Excellence and
   Extroversion, Action 1
FX We thank the reviewers for insightful comments. MKT greatly acknowledges
   support from "Research Funding at AUEB for Excellence and Extroversion,
   Action 1: 2012-2014". CY acknowledges the support of a UK Medical
   Research Council New Investigator Research Grant (Ref No. MR/L001411/1).
   CY is also affiliated with the Department of Statistics, University of
   Oxford.
CR Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Griffiths T., 2005, NEURAL INFORM PROCES, P475
   Hyungsul Kim, 2011, SDM, V11, P747, DOI DOI 10.1137/1.9781611972818.64
   Kolter J. Z., 2011, SUSTKDD WORKSH DAT M
   Kolter J.Z., 2012, J MACHINE LEARNING R, P1472
   Li N, 2003, GENETICS, V165, P2213
   Marchini J, 2010, NAT REV GENET, V11, P499, DOI 10.1038/nrg2796
   MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Scott SL, 2002, J AM STAT ASSOC, V97, P409, DOI 10.1198/016214502760046961
   SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86
   Van Gael J., 2009, ADV NEURAL INFORM PR, V21
   Yau C, 2013, BIOINFORMATICS, V29, P2482, DOI 10.1093/bioinformatics/btt416
NR 14
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647102017
DA 2019-06-15
ER

PT S
AU Tootoonian, S
   Lengyel, M
AF Tootoonian, Sina
   Lengyel, Mate
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI A Dual Algorithm for Olfactory Computation in the Locust Brain
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ANTENNAL LOBE; ODOR REPRESENTATIONS
AB We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses in- dependent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.
C1 [Tootoonian, Sina; Lengyel, Mate] Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.
RP Tootoonian, S (reprint author), Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.
EM st582@eng.cam.ac.uk; m.lengyel@eng.cam.ac.uk
FU Wellcome Trust
FX This work was supported by the Wellcome Trust (ST, ML).
CR Boyd S., 2004, CONVEX OPTIMIZATION
   Caron SJC, 2013, NATURE, V497, P113, DOI 10.1038/nature12063
   Dayan P, 2005, THEORETICAL NEUROSCI
   Eisthen HL, 2002, BRAIN BEHAV EVOLUT, V59, P273, DOI 10.1159/000063564
   Foucart S, 2013, MATH INTRO COMPRESSI
   Jortner RA, COMMUNICATION
   Jortner RA, 2007, J NEUROSCI, V27, P1659, DOI 10.1523/JNEUROSCI.4171-06.2007
   Jouquand C, 2008, J AM SOC HORTIC SCI, V133, P859
   Leitch B, 1996, J COMP NEUROL, V372, P487
   Mazor O, 2005, NEURON, V48, P661, DOI 10.1016/j.neuron.2005.09.032
   Murthy M, 2008, NEURON, V59, P1009, DOI 10.1016/j.neuron.2008.07.040
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Ng M, 2002, NEURON, V36, P463, DOI 10.1016/S0896-6273(02)00975-3
   Papadopoulou M, 2011, SCIENCE, V332, P721, DOI 10.1126/science.1201835
   Perez-Orive J, 2002, SCIENCE, V297, P359, DOI 10.1126/science.1070502
   Shen K, 2013, NEURON, V80, P1246, DOI 10.1016/j.neuron.2013.08.026
   Stopfer M, 1999, NATURE, V402, P664, DOI 10.1038/45244
   Stopfer M, 2003, NEURON, V39, P991, DOI 10.1016/j.neuron.2003.08.011
NR 18
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101071
DA 2019-06-15
ER

PT S
AU Van den Oord, A
   Schrauwen, B
AF Van den Oord, Aaron
   Schrauwen, Benjamin
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Factoring Variations in Natural Images with Deep Gaussian Mixture Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.
C1 [Van den Oord, Aaron; Schrauwen, Benjamin] Univ Ghent, Elect & Informat Syst Dept ELIS, Ghent, Belgium.
RP Van den Oord, A (reprint author), Univ Ghent, Elect & Informat Syst Dept ELIS, Ghent, Belgium.
EM aaron.vandenoord@ugent.be; benjamin.schrauwen@ugent.be
CR Bengio Y., 2013, INT C MACH LEARN
   Bengio Y, 2006, INNOVATIONS MACHINE
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Byrd Richard H., 1995, SIAM J SCI COMPUTING
   Ghahramani Z., 1996, TECHNICAL REPORT
   Gregor Karol, 2013, INT C MACH LEARN
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Krizhevsky Alex, 2014, P INT C LEARN REPR
   Lake B. M., 2013, ADV NEURAL INFORM PR
   MARTIN D, 2001, P INT C COMP VIS
   Mikolov Tomas, 2013, P WORKS ICLR
   Pascanu Razvan, 2013, P INT C LEARN REPR
   Rezende D. J., 2014, INT C MACH LEARN
   Tang Yichuan, 2012, INT C MACH LEARN
   Torralba A., 2008, IEEE T PATTERN ANAL
   Uria Benigno, 2013, P INT C MACH LEARN
   Uria Benigno, 2013, ADV NEURAL INFORM PR
   van den Oord  Aaron, 2014, J MACHINE LEARNING R
   Zoran D., 2011, INT C COMP VIS
NR 19
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100006
DA 2019-06-15
ER

PT S
AU Vinayak, RK
   Oymak, S
   Hassibi, B
AF Vinayak, Ramya Korlakai
   Oymak, Samet
   Hassibi, Babak
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Graph Clustering With Missing Data : Convex Algorithms and Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID CLIQUE
AB We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain explicit bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.
C1 [Vinayak, Ramya Korlakai; Oymak, Samet; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
RP Vinayak, RK (reprint author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
EM ramya@caltech.edu; soymak@caltech.edu; hassibi@systems.caltech.edu
FU National Science Foundation [CCF-0729203, CNS-0932428, CIF-1018927];
   Office of Naval Research under the MURI [N00014-08-1-0747]; Qualcomm
   Inc.; Schlumberger Foundation Faculty for the Future Program Grant
FX The authors thank the anonymous reviewers for their insightful comments.
   This work was supported in part by the National Science Foundation under
   grants CCF-0729203, CNS-0932428 and CIF-1018927, by the Office of Naval
   Research under the MURI grant N00014-08-1-0747, and by a grant from
   Qualcomm Inc. The first author is also supported by the Schlumberger
   Foundation Faculty for the Future Program Grant.
CR Ailon Nir, 2013, CORR
   Ames BPW, 2014, MATH PROGRAM, V143, P299, DOI 10.1007/s10107-013-0733-1
   Ames BPW, 2011, MATH PROGRAM, V129, P69, DOI 10.1007/s10107-011-0459-x
   Ames Brendan P. W., 2013, ROBUST CONVEX RELAXA
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Candes EJ, 2006, FOUND COMPUT MATH, V6, P227, DOI 10.1007/s10208-004-0162-x
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chandrasekaran Venkat, 2012, CORR
   Chen TD, 2012, ADV NEURAL INF PROCE, V25, P2213
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2
   Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57
   Ester M., 1995, KDD-95 Proceedings. First International Conference on Knowledge Discovery and Data Mining, P94
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jalali A., 2011, P 28 INT C MACH LEAR, P1001
   Khosla A, 2011, IEEE C COMP VIS PATT
   Lin Z., 2010, MATH PROGRAMMING
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mishra N, 2007, LECT NOTES COMPUT SC, V4863, P56
   Oymak S., 2011, ARXIV11045186
   Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001
   Vinayak RK, 2014, INT CONF ACOUST SPEE
   Xu H., 2010, ADV NEURAL INFORM PR, P2496
   Xu XW, 1999, DATA MIN KNOWL DISC, V3, P263, DOI 10.1023/A:1009884809343
   Xu Y, 2002, BIOINFORMATICS, V18, P536, DOI 10.1093/bioinformatics/18.4.536
   Yang Q., 2005, 2005 IEEE Computational Systems Bioinformatics Conference Workshops and Poster Abstracts, P174
NR 30
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100088
DA 2019-06-15
ER

PT S
AU Wimalawarne, K
   Sugiyama, M
   Tomioka, R
AF Wimalawarne, Kishan
   Sugiyama, Masashi
   Tomioka, Ryota
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Multitask learning meets tensor factorization: task imputation via
   convex optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.
C1 [Wimalawarne, Kishan] Tokyo Inst Technol, Meguro Ku, Tokyo, Japan.
   [Sugiyama, Masashi] Univ Tokyo, Bunkyo Ku, Tokyo, Japan.
   [Tomioka, Ryota] TTI C, Chicago, IL USA.
RP Wimalawarne, K (reprint author), Tokyo Inst Technol, Meguro Ku, Tokyo, Japan.
EM kishan@sg.cs.titech.ac.jp; sugi@k.u-tokyo.ac.jp; tomioka@ttic.edu
FU JST CREST program
FX MS acknowledges support from the JST CREST program.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Argyriou A., 2008, ADV NEURAL INFORM PR, V20, P25
   Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730
   Foygel R., 2011, ARXIV11023923
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Liu Ji, 2009, P ICCV
   Maurer A., 2012, ARXIV12121496
   Romera-Paredes B., 2013, P 30 INT C MACH LEAR, V28, P1444
   Signoretto M., 2013, ARXIV13104977
   Signoretto M., 2010, 10186 ESATSISTA
   Srebro N., 2005, ADV NEURAL INFORM PR, P1329
   Tomioka R., 2013, P ADV NEUR INF PROC, P1331
   Tomioka R., 2011, ARXIV10100789
   Tomioka R, 2011, ADV NEURAL INFORM PR, V24, P972
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vargas-Govea B., 2011, P 3 WORKSH CONT AW R
NR 26
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647103079
DA 2019-06-15
ER

PT S
AU Wu, Y
   Lobato, JMH
   Ghahramani, Z
AF Wu, Yue
   Lobato, Jose Miguel Hernandez
   Ghahramani, Zoubin
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Gaussian Process Volatility Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID ASSET RETURNS; INFERENCE
AB The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.
C1 [Wu, Yue; Lobato, Jose Miguel Hernandez; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England.
RP Wu, Y (reprint author), Univ Cambridge, Cambridge, England.
EM wu5@post.harvard.edu; jmh233@cam.ac.uk; zoubin@eng.cam.ac.uk
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Bekaert G, 2000, REV FINANC STUD, V13, P1, DOI 10.1093/rfs/13.1.1
   BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1
   Brandt MW, 2006, J BUS ECON STAT, V24, P470, DOI 10.1198/07350010600000206
   CAMPBELL JY, 1992, J FINANC ECON, V31, P281, DOI 10.1016/0304-405X(92)90037-X
   Cont R, 2001, QUANT FINANC, V1, P223, DOI [10.1088/1469-7688/1/2/304, 10.1080/713665670]
   Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618
   Deisenroth Marc P., 2009, P 26 ANN INT C MACH, P225
   Dem J., 2006, J MACH LEARN RES, V7, P1
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Frigola R., 2013, ADV NEURAL INFORM PR, V26, P3156
   GLOSTEN LR, 1993, J FINANC, V48, P1779, DOI 10.2307/2329067
   HARVEY A, 1994, REV ECON STUD, V61, P247, DOI 10.2307/2297980
   HENTSCHEL L, 1995, J FINANC ECON, V39, P71, DOI 10.1016/0304-405X(94)00821-H
   Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050
   Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x
   Lazaro-Gredilla M., 2011, P 28 INT C MACH LEAR, P841
   Lindsten F., 2012, ADV NEURAL INFORM PR, V25, P2600
   Liu J., 1999, COMBINED PARAMETER S
   NELSON DB, 1991, ECONOMETRICA, V59, P347, DOI 10.2307/2938260
   Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179
   Poon SH, 2005, FINANC ANAL J, V61, P45, DOI 10.2469/faj.v61.n1.2683
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Turner R., 2010, W CP, P868
   Wilson A., 2010, ADV NEURAL INFORM PR, P2460
   Wu Y., 2013, P 31 INT C MACH LEAR, P558
NR 28
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647101047
DA 2019-06-15
ER

PT S
AU Yang, E
   Lozano, AC
   Ravikumar, P
AF Yang, Eunho
   Lozano, Aurelie C.
   Ravikumar, Pradeep
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Elementary Estimators for Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID SELECTION
AB We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE "breaks down" under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the l(1)-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.
C1 [Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Ravikumar, Pradeep] Univ Texas Austin, Austin, TX 78712 USA.
RP Yang, E (reprint author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM eunhyang@us.ibm.com; aclozano@us.ibm.com; pradeepr@cs.utexas.edu
FU ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574,
   DMS-1264033]
FX E.Y and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF
   via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033
CR Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341
   Friedman J., 2007, BIOSTATISTICS
   Hassner M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P538
   Hsieh C. J., 2011, NEUR INFO PROC SYS N, P24
   Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577
   Jalali A., 2011, INT C AI STAT AISTAT, V14
   Li L, 2010, MATH PROGRAM COMPUT, V2, P291, DOI 10.1007/s12532-010-0020-6
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Ripley B.D, 1981, SPATIAL STAT
   Rothman AJ, 2009, J AM STAT ASSOC, V104, P177, DOI 10.1198/jasa.2009.0101
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wainwright M. J., 2003, INT C AI STAT AISTAT
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   WOODS JW, 1978, IEEE T AUTOMAT CONTR, V23, P846, DOI 10.1109/TAC.1978.1101866
   Yang E., 2011, INT C MACH LEARN ICM, P28
   Yang E., 2014, INT C MACH LEARN ICM, P31
   Yedidia JS, 2001, ADV NEUR IN, V13, P689
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
NR 25
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100097
DA 2019-06-15
ER

PT S
AU Yerebakan, HZ
   Rajwa, B
   Dundar, M
AF Yerebakan, Halid Z.
   Rajwa, Bartek
   Dundar, Murat
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI The Infinite Mixture of Infinite Gaussian Mixtures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
AB Dirichlet process mixture of Gaussians (DPMG) has been used in the literature for clustering and density estimation problems. However, many real-world data exhibit cluster distributions that cannot be captured by a single Gaussian. Modeling such data sets by DPMG creates several extraneous clusters even when clusters are relatively well-defined. Herein, we present the infinite mixture of infinite Gaussian mixtures (I-2 GMM) for more flexible modeling of data sets with skewed and multi-modal cluster distributions. Instead of using a single Gaussian for each cluster as in the standard DPMG model, the generative model of I-2 GMM uses a single DPMG for each cluster. The individual DPMGs are linked together through centering of their base distributions at the atoms of a higher level DP prior. Inference is performed by a collapsed Gibbs sampler that also enables partial parallelization. Experimental results on several artificial and real-world data sets suggest the proposed I-2 GMM model can predict clusters more accurately than existing variational Bayes and Gibbs sampler versions of DPMG.
C1 [Yerebakan, Halid Z.; Dundar, Murat] IUPUI, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA.
   [Rajwa, Bartek] Purdue Univ, Bindley Biosci Ctr, W Lafayette, IN 47907 USA.
RP Yerebakan, HZ (reprint author), IUPUI, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA.
EM hzyereba@cs.iupui.edu; rajwa@cyto.purdue.edu; dundar@cs.iupui.edu
FU National Science Foundation (NSF) [IIS-1252648]; National Institute of
   Biomedical Imaging and Bioengineering (NIBIB) [5R21EB015707]; PhRMA
   Foundation
FX This research was sponsored by the National Science Foundation (NSF)
   under Grant Number IIS-1252648 (CAREER), by the National Institute of
   Biomedical Imaging and Bioengineering (NIBIB) under Grant Number
   5R21EB015707, and by the PhRMA Foundation (2012 Research Starter Grant
   in Informatics). The content is solely the responsibility of the authors
   and does not represent the official views of NSF, NIBIB or PhRMA.
CR Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]
   Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   BACHE K., 2013, UCI MACHINE LEARNING
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Cron AJ, 2011, AM STAT, V65, P16, DOI 10.1198/tast.2011.10170
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   FlowCAP, FLOW CYT CRIT ASS PO
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   Kim S, 2007, ADV NEURAL INFORM PR, V19, P697
   Kurihara K., 2002, ADV NEURAL INFORM PR, V19
   Pyne S, 2009, P NATL ACAD SCI USA, V106, P8519, DOI 10.1073/pnas.0903028106
   Rodriguez A, 2008, J AM STAT ASSOC, V103, P1131, DOI 10.1198/016214508000000553
   Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
NR 15
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100078
DA 2019-06-15
ER

PT S
AU Zhou, YB
   Porwal, U
   Zhang, C
   Ngo, H
   Nguyen, X
   Re, C
   Govindaraju, V
AF Zhou, Yingbo
   Porwal, Utkarsh
   Zhang, Ce
   Ngo, Hung
   Nguyen, XuanLong
   Re, Christopher
   Govindaraju, Venu
BE Ghahramani, Z
   Welling, M
   Cortes, C
   Lawrence, ND
   Weinberger, KQ
TI Parallel Feature Selection inspired by Group Testing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 28th Conference on Neural Information Processing Systems (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
ID INFORMATION; DIVERGENCE; FRAMEWORK; RELEVANCE
AB This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.
C1 [Zhou, Yingbo; Porwal, Utkarsh; Ngo, Hung; Govindaraju, Venu] SUNY Buffalo, CSE Dept, Buffalo, NY 14260 USA.
   [Zhang, Ce] Univ Wisconsin, CS Dept, Madison, WI USA.
   [Nguyen, XuanLong] Univ Michigan, EECS Dept, Ann Arbor, MI 48109 USA.
   [Re, Christopher] Stanford Univ, CS Dept, Stanford, CA 94305 USA.
RP Zhou, YB (reprint author), SUNY Buffalo, CSE Dept, Buffalo, NY 14260 USA.
EM yingbozh@buffalo.edu; utkarshp@buffalo.edu; czhang@cs.wisc.edu;
   hungngo@buffalo.edu; xuanlong@umich.edu; chrismre@cs.stanford.edu;
   govind@buffalo.edu
CR ALI SM, 1966, J ROY STAT SOC B, V28, P131
   Amaldi Edoardo, 1997, APPROXIMABILITY MINI
   Bekkerman R., 2003, Journal of Machine Learning Research, V3, P1183, DOI 10.1162/153244303322753625
   Brown G, 2012, J MACH LEARN RES, V13, P27
   Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299
   Du Ding-Zhu, 2000, SERIES APPL MATH, V12
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Gentleman R, 2005, STAT BIOL HEALTH, P189
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Jakulin A., 2005, THESIS
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Kuncheva L. I., 2007, ARTIF INTELL, P421
   LEWIS DD, 1992, SPEECH AND NATURAL LANGUAGE, P212
   Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68
   Meyer PE, 2006, LECT NOTES COMPUT SC, V3907, P91
   Mintz M., 2009, P JOINT C 47 ANN M A, P1003, DOI DOI 10.3115/1690219.1690287
   Ngo HQ, 2012, LEIBNIZ INT PR INFOR, V14, P230, DOI 10.4230/LIPIcs.STACS.2012.230
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nguyen X, 2009, ANN STAT, V37, P876, DOI 10.1214/08-AOS595
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Stoppiglia H., 2003, Journal of Machine Learning Research, V3, P1399, DOI 10.1162/153244303322753733
   Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190
   Tan M., 2010, P 27 INT C MACH LEAR, P1047
   Yang H. H., 1999, ADV NEURAL INFORM PR, P687
   Yu L, 2004, J MACH LEARN RES, V5, P1205
   Zhang C., 2012, P 50 ANN M ASS COMP, V1, P825
NR 27
TC 1
Z9 1
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2014
VL 27
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5SR
UT WOS:000452647100075
DA 2019-06-15
ER

PT J
AU Mesterharm, C
AF Mesterharm, C
TI Tracking linear-threshold concepts with winnow
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article; Proceedings Paper
CT 16th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC, 2002
CL VANCOUVER, CANADA
DE winnow; concept tracking; on-line learning
ID EXPERT; MARGIN
AB In this paper, we give a mistake-bound for learning arbitrary linear-threshold concepts that are allowed to change over time in the on-line model of learning. We use a variation of the Winnow algorithm and show that the bounds for learning shifting linear-threshold functions have many of the same advantages that the traditional Winnow algorithm has on fixed concepts. These benefits include a weak dependence on the number of irrelevant attributes, inexpensive runtime, and robust behavior against noise. In fact, we show that the bound for tracking Winnow has even better performance with respect to irrelevant attributes. Let X e [0, 1](n) be an instance of the learning problem. In the previous bounds, the number of mistakes depends on In n. In this paper, the shifting concept bound depends on max In (\\X\\(1)). We show that this behavior is a result of certain parameter choices in the tracking version of Winnow, and we show how to use related parameters to get a similar mistake bound for the traditional fixed concept version of Winnow.
C1 Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
RP Mesterharm, C (reprint author), Rutgers State Univ, Dept Comp Sci, 110 Frelinghuysen Rd, Piscataway, NJ 08854 USA.
EM MESTERHA@CS.RUTGERS.EDU
CR Auer P, 1998, MACH LEARN, V32, P127, DOI 10.1023/A:1007472513967
   BLUM A, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P157
   Blum A, 2000, MACH LEARN, V39, P35, DOI 10.1023/A:1007621832648
   CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179
   Gentile C, 1999, ADV NEUR IN, V11, P225
   GENTILE C, 2001, MACH LEARN, V2, P213
   Grove AJ, 2001, MACH LEARN, V43, P173, DOI 10.1023/A:1010844028087
   HELMBOLD DP, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P13
   Helmbold DP, 2000, MOBILE NETW APPL, V5, P285, DOI 10.1023/A:1019129116852
   Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Herbster M., 2001, P 14 ANN C COMP LEAR, P444
   Kivinen J, 2002, LECT NOTES ARTIF INT, V2533, P113
   Kuh A., 1991, ADV NEURAL INFORMATI, V3, P183
   LITTLESTONE N, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P147
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   LITTLESTONE N, 1989, THESIS U CALIFORNIA
   Minsky  Marvin, 1969, PERCEPTRONS
   Rosenblatt F., 1962, PRINCIPLES NEURODYNA
NR 20
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUL 1
PY 2004
VL 4
IS 5
BP 819
EP 838
DI 10.1162/1532443041424274
PG 20
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA 845JN
UT WOS:000223238800004
DA 2019-06-15
ER

PT S
AU Archer, C
   Leen, TK
   Baptista, A
AF Archer, C
   Leen, TK
   Baptista, A
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Parameterized novelty detection for environmental sensor monitoring
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB As part of an environmental observation and forecasting system, sensors deployed in the Columbia RIver Estuary (CORIE) gather information on physical dynamics and changes in estuary habitat. Of these, salinity sensors are particularly susceptible to bio-fouling, which gradually degrades sensor response and corrupts critical data. Automatic fault detectors have the capability to identify bio-fouling early and minimize data loss. Complicating the development of discriminatory classifiers is the scarcity of bio-fouling onset examples and the variability of the bio-fouling signature. To solve these problems, we take a novelty detection approach that incorporates a parameterized bio-fouling model. These detectors identify the occurrence of bio-fouling, and its onset time as reliably as human experts. Real-time detectors installed during the summer of 2001 produced no false alarms, yet detected all episodes of sensor degradation before the field staff scheduled these sensors for cleaning. From this initial deployment through February 2003, our bio-fouling detectors have essentially doubled the amount of useful data coming from the CORTE sensors.
C1 Oregon Hlth Sci Univ, OGI Sch Sci & Engn, Beaverton, OR 97006 USA.
RP Archer, C (reprint author), Oregon Hlth Sci Univ, OGI Sch Sci & Engn, 20000 NW Walker Rd, Beaverton, OR 97006 USA.
CR Archer C., 2003, WATER RESOURCES RES, V39
   BAPTISTA AM, 1999, EARTH SYSTEM MONITOR, V9
   BASSEVILLE M, 1988, AUTOMATICA, V24, P309, DOI 10.1016/0005-1098(88)90073-8
   *USA CORPS ENG, 2001, BIOL ASS COL RIV CHA
NR 4
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 619
EP 626
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500078
DA 2019-06-15
ER

PT S
AU Audibert, JY
   Bousquet, O
AF Audibert, JY
   Bousquet, O
BE Thrun, S
   Saul, K
   Scholkopf, B
TI PAC-Bayesian generic chaining
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB There exist many different generalization error bounds for classification. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classifiers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classifiers, and such priors also arise in the PAC-bayesian setting.
C1 Univ Paris 06, Lab Probabil & Modeles Aleatoires, F-75013 Paris, France.
RP Audibert, JY (reprint author), Univ Paris 06, Lab Probabil & Modeles Aleatoires, 175 Rue Chevaleret, F-75013 Paris, France.
CR AUDIBERT JY, 2003, UNPUB  DATA DEPENDEN
   BARTLETT P, 2003, LOCAL RADEMACHER COM
   Boucheron S, 2000, RANDOM STRUCT ALGOR, V16, P277, DOI 10.1002/(SICI)1098-2418(200005)16:3<277::AID-RSA4>3.0.CO;2-1
   CATONI O, 2003, LOCALIZED EMPIRICAL
   Devroye L., 2001, SPRINGER SERIES STAT
   Dudley R M, 1984, LECT NOTES MATH, V1097, P2
   Ledoux M., 1991, PROBABILITY BANACH S
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   McAllester D. A., 1999, P 12 ANN C COMP LEAR
   MCALLESTER DA, 2003, P COMP LEARN THEOR
   Talagrand M, 1996, ANN PROBAB, V24, P1049, DOI 10.1214/aop/1065725175
   TALAGRAND M, 1987, ANN PROBAB, V15, P837, DOI 10.1214/aop/1176992069
   Vapnik V. N., 1974, THEORY PATTERN RECOG
NR 13
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1125
EP 1132
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500140
DA 2019-06-15
ER

PT S
AU Aviel, Y
   Horn, D
   Abeles, M
AF Aviel, Y
   Horn, D
   Abeles, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI The doubly balanced network of spiking neurons: a memory model with high
   capacity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ASSOCIATIVE MEMORY; NEURAL NETWORKS; DYNAMICS
AB A balanced network leads to contradictory constraints on memory models, as exemplified in previous work on accommodation of synfire chains. Here we show that these constraints can be overcome by introducing a 'shadow' inhibitory pattern for each excitatory pattern of the model. This is interpreted as a doublebalance principle, whereby there exists both global balance between average excitatory and inhibitory currents and local balance between the currents carrying coherent activity at any given time frame. This principle can be applied to networks with Hebbian cell assemblies, leading to a high capacity of the associative memory. The number of possible patterns is limited by a combinatorial constraint that turns out to be P=0.06N within the specific model that we employ. This limit is reached by the Hebbian cell assembly network. To the best of our knowledge this is the first time that such high memory capacities are demonstrated in the asynchronous state of models of spiking neurons.
C1 Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel.
RP Aviel, Y (reprint author), Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel.
CR Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   Aviel Y, 2003, NEURAL COMPUT, V15, P1321, DOI 10.1162/089976603321780290
   AVIEL Y, 2003, MEMORY CAPACITY BALA
   Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004
   HERTZ JA, 1999, NEURONAL INFORMATION
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002
   Tuckwell H. C., 1988, INTRO THEORETICAL NE
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0
NR 12
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1247
EP 1254
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500155
DA 2019-06-15
ER

PT S
AU Bererton, C
   Gordon, G
   Thrun, S
   Khosla, P
AF Bererton, C
   Gordon, G
   Thrun, S
   Khosla, P
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Auction mechanism design for multi-robot coordination
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball.
C1 Carnegie Mellon Univ, Pittsburgh, PA 15217 USA.
RP Bererton, C (reprint author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15217 USA.
CR BENNEWITZ M, 2001, IEEE INT C ROB AUT I
   BURGARD W, 2000, P IEEE INT C ROB AUT
   CAO YU, 1997, AUTON ROBOT, V1, P1
   Carlos Guestrin and Geoffrey Gordon, 2002, UNCERTAINTY ARTIFICI, V18
   Chvatal V., 1983, LINEAR PROGRAMMING
   Dantzig G, 1963, LINEAR PROGRAMMING E
   DIAS MB, 2001, MARKET APPROACH MULT
   GERKEY BP, SOLD MARKET METHODS
   GOLDBERG D, 2000, IRIS00387 USC
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   KITANO H, 1998, P ROB 97 1 ROB WORLD
   Parker L. E., 1996, J ADV ROBOTICS, V10
   Rardin R. L., 1998, OPTIMIZATION OPERATI
   ROSENCRANTZ M, 2003, ACM AGENTS
   ROUMELIOTIS S, 2000, P DISTRIBUTED AUTONO, P179
   SALIDO J, 1997, P INT C SENS FUS DEC, P90
   Zlot R., 2002, MULTIROBOT EXPLORATI
NR 17
TC 1
Z9 1
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 879
EP 886
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500110
DA 2019-06-15
ER

PT S
AU Beygelzimer, A
   Rish, I
AF Beygelzimer, A
   Rish, I
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Approximability of probability distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID GRAPH
AB We consider the question of how well a given distribution can be approximated with probabilistic graphical models. We introduce a new parameter, effective treewidth, that captures the degree of approximability as a tradeoff between the accuracy and the complexity of approximation. We present a simple approach to analyzing achievable tradeoffs that exploits the threshold behavior of monotone graph properties, and provide experimental results that support the approach.
C1 IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
RP Beygelzimer, A (reprint author), IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
CR BARAK AB, 1984, SIAM J ALGEBRA DISCR, V5, P508, DOI 10.1137/0605049
   BEYGELZIMER A, 2002, P 8 INT C PRINC KNOW
   Bollobas B, 1997, SIAM J DISCRETE MATH, V10, P318, DOI 10.1137/S0895480194281215
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   COVER TM, 1991, ELEMENTS INFORMATION
   DECHTER R, 1998, LEARNING GRAPHICAL M
   Erdos P., 1961, B I INT STATIST TOKY, V38, P343
   Friedgut E, 1996, P AM MATH SOC, V124, P2993, DOI 10.1090/S0002-9939-96-03732-X
   Hoffgen K.-U., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P77, DOI 10.1145/168304.168314
   JENSEN FV, 1994, P 10 C UNC AI UAI
   NAOR J, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P213, DOI 10.1145/100216.100244
   Pearl J, 1988, PROBABILISTIC REASON
   Srebro N., 2001, P 17 C UNC ART INT U, P504
NR 13
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 377
EP 384
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500048
DA 2019-06-15
ER

PT S
AU Bofill-i-Petit, A
   Murray, AF
AF Bofill-i-Petit, A
   Murray, AF
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Synchrony detection by analogue VLSI neurons with bimodal STDP synapses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID TIMING-DEPENDENT PLASTICITY
AB We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with different types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection.
C1 Univ Edinburgh, Edinburgh EH9 3JL, Midlothian, Scotland.
RP Bofill-i-Petit, A (reprint author), Univ Edinburgh, Edinburgh EH9 3JL, Midlothian, Scotland.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464
   BOFILL A, 2002, ADV NEURAL INFORMATI, V14
   Bofill i Petit A., 2003, IEEE INT S CIRCUITS, VV, P817
   HALFIGER P, 1996, ADV NEURAL INFORMATI, V9, P692
   INDIVERI G, 2003, ADV NEURAL INFORMATI, V15
   Kepecs A, 2002, BIOL CYBERN, V87, P446, DOI 10.1007/s00422-002-0358-6
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Song S, 2000, NAT NEUROSCI, V3, P919
   van Rossum MCW, 2001, NEUROCOMPUTING, V38, P409, DOI 10.1016/S0925-2312(01)00360-5
   VOGELSTEIN RJ, 2003, ADV NEURAL INFORMATI, V15
   Zhang LI, 1998, NATURE, V395, P37
NR 11
TC 1
Z9 1
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1027
EP 1034
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500128
DA 2019-06-15
ER

PT S
AU Chang, YH
   Ho, A
   Kaelbling, LP
AF Chang, YH
   Ho, A
   Kaelbling, LP
BE Thrun, S
   Saul, K
   Scholkopf, B
TI All learning is local: Multi-agent learning in global reward games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent's limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy.
C1 MIT, CSAIL, Cambridge, MA 02139 USA.
RP Chang, YH (reprint author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
CR Auer Peter, 1995, P 36 ANN S FDN COMP
   CHANG Y, 2003, AIM2003025 MIT AI LA
   CHOI S, 1999, IJCAI WORKSH NEUR SY
   CLAUS C, 1998, P 15 AAAI
   KALMAN RE, 1960, T AM SOC MECH ENG J
   MCMAHAN H, 2003, P 20 ICML
   NG AY, 1999, P 16 ICML
   Sutton R.S., 1999, REINFORCEMENT LEARNI
   SZITA I, 2002, J MACHINE LEARNING R
   Wolpert D., 1999, NASAARCIC9963
NR 10
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 807
EP 814
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500101
DA 2019-06-15
ER

PT S
AU Derbeko, P
   El-Yaniv, R
   Meir, R
AF Derbeko, P
   El-Yaniv, R
   Meir, R
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Error bounds for transductive learning via compression and clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB This paper is concerned with transductive learning. Although transduction appears to be an easier task than induction, there have not been many provably useful algorithms and bounds for transduction. We present explicit error bounds for transduction and derive a general technique for devising bounds within this setting. The technique is applied to derive error bounds for compression schemes such as (transductive) SVMs and for transduction algorithms based on clustering.
C1 Technion Israel Inst Technol, IL-32000 Haifa, Israel.
RP Derbeko, P (reprint author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.
CR BLUM A, 2003, COLT, P344
   Blum PR, 2001, STUD EUROP JUDAISM, V1, P19
   BOTTOU L, 1994, EFFECTIVE VC DIMENSI
   DEMBO A, 1998, LARGE DEVIATION TECH
   ELYANIV R, 2001, ADV NEURAL INFORMATI, P1025
   Herbrich  R., 2002, LEARNING KERNEL CLAS
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Joachims T., 2003, P 20 INT C MACH LEAR
   JOACHIMS T, 1999, EUR C MACH LEARN
   Lanckriet G. R. G., 2002, LEARNING KERNEL MATR
   McAleese J, 1999, AEROSPACE AM, V37, P3
   MCALLESTER D, 2003, COLT, P203
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vapnik V. N., 1982, ESTIMATION DEPENDENC
   WU D, 1999, INT C MACH LEARN
NR 16
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1085
EP 1092
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500135
DA 2019-06-15
ER

PT S
AU Gruber, A
   Weiss, Y
AF Gruber, A
   Weiss, Y
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Factorization with uncertainty and missing data: exploiting temporal
   coherence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MOTION; PERCEPTION
AB The problem of "Structure From Motion" is a central problem in vision: given the 2D locations of certain points we wish to recover the camera motion and the 3D coordinates of the points. Under simplified camera models, the problem reduces to factorizing a measurement matrix into the product of two low rank matrices. Each element of the measurement matrix contains the position of a point in a particular image. When all elements are observed, the problem can be solved trivially using SVD, but in any realistic situation many elements of the matrix are missing and the ones that are observed have a, different directional uncertainty. Under these conditions, most existing factorization algorithms fail while human perception is relatively unchanged.
   In this paper we use the well known EM algorithm for factor analysis to perform factorization. This allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables (the camera position). We show that incorporating this prior gives a significant improvement in performance in challenging image sequences.
C1 Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.
RP Gruber, A (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.
CR Andersen RA, 1998, TRENDS COGN SCI, V2, P222, DOI 10.1016/S1364-6613(98)01181-4
   BRAND M, 2002, ECCV, P707
   DELLAERT F, 1999, ICCV, P696
   Gelb A., 1974, APPL OPTIMAL ESTIMAT
   IRANI M, 2000, ECCV, P959
   JACOBS D, 1997, CVPR, P206
   MORRIS DD, 1999, ICCV, P696
   ROWEIS S, 1997, NIPS, P431
   SHUM HY, 1995, PRINCIPAL COMPONENT, P854
   SOATTO S, 1999, IEEE T PATTERN ANAL, P943
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   TREUE S, 1991, VISION RES, V31, P59, DOI 10.1016/0042-6989(91)90074-F
   ULLMAN S, 1979, INTERPERTATION VISUA
NR 13
TC 1
Z9 1
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1507
EP 1514
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500187
DA 2019-06-15
ER

PT S
AU Harrison, RR
AF Harrison, RR
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A low-power analog VLSI visual collision detector
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID LATERAL GENICULATE-NUCLEUS; RESPONSE PROPERTIES; MOTION; COMPUTATION;
   SYSTEM; CELLS; FLY
AB We have designed and tested a single-chip analog VLSI sensor that detects imminent collisions by measuring radially expansive optic flow. The design of the chip is based on a model proposed to explain leg-extension behavior in flies during landing approaches. A new elementary motion detector (EMD) circuit was developed to measure optic flow. This EMD circuit models the bandpass nature of large monopolar cells (LMCs) immediately postsynaptic to photoreceptors in the fly visual system. A 16 X 16 array of 2-D motion detectors was fabricated on a 2.24 mm x 2.24 mm die in a standard 0.5-mum CMOS process. The chip consumes 140 muW of power from a 5 V supply. With the addition of wide-angle optics, the sensor is able to detect collisions around 500 ms before impact in complex, real-world scenes.
C1 Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.
RP Harrison, RR (reprint author), Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   BORST A, 1988, J COMP PHYSIOL A, V163, P167, DOI 10.1007/BF00612426
   DONG DW, 1995, NETWORK-COMP NEURAL, V6, P159, DOI 10.1088/0954-898X/6/2/003
   Dror RO, 2001, J OPT SOC AM A, V18, P241, DOI 10.1364/JOSAA.18.000241
   Duchon AP, 1998, ADAPT BEHAV, V6, P473, DOI 10.1177/105971239800600306
   EGELHAAF M, 1989, J OPT SOC AM A, V6, P116, DOI 10.1364/JOSAA.6.000116
   Gabbiani F, 1999, J NEUROSCI, V19, P1122
   Harrison RR, 2000, ANALOG INTEGR CIRC S, V24, P213, DOI 10.1023/A:1008361525235
   HARRISON RR, UNPUB NIPS 2003
   HASSENSTEIN B, 1956, Z NATURFORSCH PT B, V11, P513
   LAUGHLIN SB, 1994, PROG RETIN EYE RES, V13, P165, DOI 10.1016/1350-9462(94)90009-4
   Mead C., 1989, ANALOG VLSI NEURAL S
   SAUL AB, 1990, J NEUROPHYSIOL, V64, P206
   Sun HJ, 1998, NAT NEUROSCI, V1, P296
   Tammero LF, 2002, J EXP BIOL, V205, P2785
   VANHATEREN JH, 1992, J COMP PHYSIOL A, V171, P157, DOI 10.1007/BF00188924
   VanHateren JH, 1997, VISION RES, V37, P3407, DOI 10.1016/S0042-6989(97)00105-3
NR 17
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 987
EP 994
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500123
DA 2019-06-15
ER

PT S
AU Hoyle, DC
   Rattray, M
AF Hoyle, DC
   Rattray, M
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Limiting form of the sample covariance eigenspectrum in PCA and kernel
   PCA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID DIMENSIONAL RANDOM MATRICES; DISTRIBUTIONS
AB We derive the limiting form of the eigenvalue spectrum for sample co-variance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter a which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping a fixed. As a increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case.
C1 Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England.
RP Hoyle, DC (reprint author), Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England.
CR BAI ZD, 1993, ANN PROBAB, V21, P649, DOI 10.1214/aop/1176989262
   EDELMAN A, 1988, SIAM J MATRIX ANAL A, V9, P543, DOI 10.1137/0609045
   Engel A., 2001, STAT MECH LEARNING
   Halkjaer S, 1997, ADV NEUR IN, V9, P169
   HOYLE DC, IN PRESS PHYS REV E
   JOHNSTONE IM, 2001, ANN STAT, V29
   Jolliffe I., 1986, PRINCIPAL COMPONENT
   Marcenko V. A., 1967, MATH USSR SB, V72, P507
   Minka TP, 2001, ADV NEUR IN, V13, P598
   Reimann P, 1996, J PHYS A-MATH GEN, V29, P3521, DOI 10.1088/0305-4470/29/13/021
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Sengupta AM, 1999, PHYS REV E, V60, P3389, DOI 10.1103/PhysRevE.60.3389
   SILVERSTEIN JW, 1995, J MULTIVARIATE ANAL, V54, P295, DOI 10.1006/jmva.1995.1058
   SILVERSTEIN JW, 1992, IEEE T SIGNAL PROCES, V40, P2100, DOI 10.1109/78.149981
   SOLLICH P, 1994, J PHYS A-MATH GEN, V27, P7771, DOI 10.1088/0305-4470/27/23/020
   WACHTER KW, 1978, ANN PROBAB, V6, P1, DOI 10.1214/aop/1176995607
NR 16
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1181
EP 1188
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500147
DA 2019-06-15
ER

PT S
AU Kauchak, D
   Dasgupta, S
AF Kauchak, D
   Dasgupta, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI An iterative improvement procedure for hierarchical clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We describe a procedure which finds a hierarchical clustering by hill-climbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efficiently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms.
C1 Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.
RP Kauchak, D (reprint author), Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.
CR FEREA TL, 1999, P NATL ACAD SCI, V97
   HARTIGAN J. A., 1975, CLUSTERING ALGORITHM
   HARTIGAN JA, 1985, J CLASSIFICATION
   HORN WA, 1972, SIAM J APPL MATH, V23, P189, DOI 10.1137/0123021
   KAUCHAK D, 2003, UNPUB
NR 5
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 481
EP 488
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500061
DA 2019-06-15
ER

PT S
AU Kording, KP
   Wolpert, DM
AF Kording, KP
   Wolpert, DM
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Probabilistic inference in human sensorimotor processing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.
C1 UCL London, Inst Neurol, London WC1N 3BG, England.
RP Kording, KP (reprint author), UCL London, Inst Neurol, London WC1N 3BG, England.
RI Kording, Konrad/A-1233-2007
OI Kording, Konrad/0000-0001-8408-4499
CR Basso MA, 1998, J NEUROSCI, V18, P7519
   BERNRDO JM, 1994, BAYESIAN THEORY
   BERROU C, 1993, P ICC 93 GEN SWITZ
   CARPENTER RH, NATURE, V377, P59
   Fiorillo CD, 2003, SCIENCE, V299, P1898, DOI 10.1126/science.1077349
   Goodbody SJ, 1998, J NEUROPHYSIOL, V79, P1825
   Platt ML, 1999, NATURE, V400, P233, DOI 10.1038/22268
   Probability Cox R. T., 1946, AM J PHYS, V17, P1
   SIMONCELLI EP, 1996, P 3 INT C IM PROC LA
   Stickgold R, 2000, NAT NEUROSCI, V3, P1237, DOI 10.1038/81756
   Weiss Y, 2002, NAT NEUROSCI, V5, P598, DOI 10.1038/nn858
NR 11
TC 1
Z9 1
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1327
EP 1334
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500165
DA 2019-06-15
ER

PT S
AU Lee, SI
   Batzoglou, S
AF Lee, SI
   Batzoglou, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI ICA-based clustering of genes from microarray expression data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID INDEPENDENT COMPONENT ANALYSIS; CELL-CYCLE; SEPARATION; ALGORITHM
AB We propose an unsupervised methodology using independent component analysis (ICA) to cluster genes from DNA microarray data. Based on an ICA mixture model of genomic expression patterns, linear and nonlinear ICA finds components that are specific to certain biological processes. Genes that exhibit significant up-regulation or down-regulation within each component are grouped into clusters. We test the statistical significance of enrichment of gene annotations within each cluster. ICA-based clustering outperformed other leading methods in constructing functionally coherent clusters on various datasets. This result supports our model of genomic expression data as composite effect of independent biological processes. Comparison of clustering performance among various ICA algorithms including a kernel-based nonlinear ICA algorithm shows that nonlinear ICA performed the best for small datasets and natural-gradient maximization-likelihood worked well for all the datasets.
C1 Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Lee, SI (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
CR Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101
   Amari S, 1996, ADV NEUR IN, V8, P757
   Ashburner M, 2001, GENOME RES, V11, P1425
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Cardoso JF, 1999, NEURAL COMPUT, V11, P157, DOI 10.1162/089976699300016863
   Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8
   HARMELING S, ADV NEURAL INFORMATI, V8, P757
   Hori G., 2000, P INT WORKSH IND COM, P151
   Hsiao LL, 2001, PHYSIOL GENOMICS, V7, P97, DOI 10.1152/physiolgenomics.00040.2001
   Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   KANEHISA M, 2002, CURRENT TOPICS COMPU, P301
   Kim SK, 2001, SCIENCE, V293, P2087, DOI 10.1126/science.1061603
   Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719
   Liebermeister W, 2002, BIOINFORMATICS, V18, P51, DOI 10.1093/bioinformatics/18.1.51
   Misra J, 2002, GENOME RES, V12, P1112, DOI 10.1101/gr.225302
   Spellman PT, 1998, MOL BIOL CELL, V9, P3273, DOI 10.1091/mbc.9.12.3273
   Tavazoie S, 1999, NAT GENET, V22, P281
   Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520
NR 18
TC 1
Z9 1
U1 0
U2 2
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 675
EP 682
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500085
DA 2019-06-15
ER

PT S
AU Marx, Z
   Dagan, I
   Shamir, E
AF Marx, Z
   Dagan, I
   Shamir, E
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Identifying structure across pre-partitioned data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We propose an information-theoretic clustering approach that incorporates a pre-known partition of the data, aiming to identify common clusters that cut across the given partition. In the standard clustering setting the formation of clusters is guided by a single source of feature information. The newly utilized pre-partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition. The resulting algorithmic framework was applied successfully to synthetic data, as well as to identifying text-based cross-religion correspondences.
C1 Hebrew Univ Jerusalem, Neural Computat Ctr, IL-91904 Jerusalem, Israel.
RP Marx, Z (reprint author), Hebrew Univ Jerusalem, Neural Computat Ctr, IL-91904 Jerusalem, Israel.
CR CHECHIK G, 2002, ADV NEURAL PROCESSIN, V15
   COVER TM, 1991, ELEMENTS INFORMATION
   DAGAN I, 2002, P 6 C NAT LANG LEARN, P15
   FRIEDMAN N, 2002, 17 C UNC ART INT UAI, P152
   Globerson A, 2003, Journal of Machine Learning Research, V3, P1307, DOI 10.1162/153244303322753689
   HOFMANN T, 2001, J MACHINE LEARNING R, V41, P177
   MARX Z, 2002, J MACHINE LEARNING R, V3, P747
   Tishby N., 1999, 37 ANN ALL C COMM CO, P368
   Wagstaff K., 2001, 18 INT C MACH LEARN, P577
NR 9
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 489
EP 496
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500062
DA 2019-06-15
ER

PT S
AU Natschlager, T
   Maass, W
AF Natschlager, T
   Maass, W
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Information dynamics and emergent computation in recurrent circuits of
   spiking neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID MUTUAL INFORMATION; ENTROPY
AB We employ an efficient method using Bayesian and linear classifiers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information.
C1 Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Natschlager, T (reprint author), Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.
EM tnatschl@igi.tugraz.at; maass@igi.tugraz.at
CR COVER TM, 1991, ELEMENTS INFORMATION
   Duda R, 2001, PATTERN CLASSIFICATI
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   HERTZ J, READING INFORMATION
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Panzeri S, 1996, NETWORK-COMP NEURAL, V7, P87, DOI [10.1088/0954-898X/7/1/006, 10.1080/0954898X.1996.11978656]
   Pola G., 2003, NEUROSCIENCE DATABAS, P139
   Roulston MS, 1999, PHYSICA D, V125, P285, DOI 10.1016/S0167-2789(98)00269-3
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
NR 12
TC 1
Z9 1
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1255
EP 1262
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500156
DA 2019-06-15
ER

PT S
AU Neal, RM
   Beal, MJ
   Roweis, ST
AF Neal, RM
   Beal, MJ
   Roweis, ST
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Inferring state sequences for non-linear systems with embedded hidden
   Markov models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of "pools" of candidate states at each time. We then define an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efficiently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in-these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers.
C1 Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G3, Canada.
RP Neal, RM (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G3, Canada.
CR ACHAN K, 2004, UTMLTR2004001
   DOUCET A, 2000, P IEEE INT C AC SPEE, V2, P701
   Neal R. M, 2003, 0304 U TOR DEP STAT
   Neal R. M., 1993, CRGTR931 U TOR DEP C
   Scott SL, 2002, J AM STAT ASSOC, V97, P337, DOI 10.1198/016214502753479464
NR 5
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 401
EP 408
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500051
DA 2019-06-15
ER

PT S
AU Nilim, A
   El Ghaoui, L
AF Nilim, A
   El Ghaoui, L
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Robustness in Markov decision problems with uncertain transition
   matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID PROBABILITIES
AB Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to real-world problems. We propose an algorithm for solving finite-state and finite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.
C1 Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
RP Nilim, A (reprint author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
CR Bagnell J., 2001, CMURITR0125
   ELGHAOURI L, 2004, M0407 UCBERL
   Feinberg E. A., 2002, HDB MARKOV DECISION
   FERGUSON TS, 1974, ANN STAT, V2, P615, DOI 10.1214/aos/1176342752
   GIVAN R, 1997, 4 EUR C PLANN, P234
   LEHMANN EL, 1998, THEORY POINT ESTIMAT
   Putterman M. L., 1994, MARKOV DECISION PROC
   SATIA JK, 1973, OPER RES, V21, P728, DOI 10.1287/opre.21.3.728
   SHAPIRO A, 2003, IN PRESS OPTIMIZATIO
   WHITE CC, 1994, OPER RES, V42, P739, DOI 10.1287/opre.42.4.739
NR 10
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 839
EP 846
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500105
DA 2019-06-15
ER

PT S
AU Paskin, MA
AF Paskin, MA
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Sample propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MODELS
AB Rao-Blackwellization is an approximation technique for probabilistic inference that flexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efficient implementation of Rao-Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster's variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems.
C1 Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
RP Paskin, MA (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.
CR BIDYUK B, 2003, P 19 C UNC AI UAI 03
   Carter CK, 1996, BIOMETRIKA, V83, P589, DOI 10.1093/biomet/83.3.589
   Cowell R. G., 1999, PROBABILISTIC NETWOR
   DAWID A, 1995, LECT NOTES COMPUTER, V945
   DOUCET A, 2000, P 16 C UNC AI UAI 00
   JENSEN CS, 1995, INT J HUM-COMPUT ST, V42, P647, DOI 10.1006/ijhc.1995.1029
   KJAERULFF U, 1995, P 11 C UNC ART INT U
   LAURITZEN SL, 1992, J AM STAT ASSOC, V87, P1098, DOI 10.2307/2290647
   LERNER U, 2002, THESIS STANFORD U
   LERNER U, 2001, P 17 C UNC AI UAI 01
   Neal R., 1993, CRGTR931 U TOR
   Shafer G.R., 1990, ANN MATH ARTIFICIAL, V2, P327
   TEH YW, 2003, P 9 INT WORKSH AI ST
NR 13
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 425
EP 432
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500054
DA 2019-06-15
ER

PT S
AU Platt, JC
AF Platt, JC
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Fast embedding of sparse music similarity graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID ALGORITHMS
AB This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity, with 267K vertices that represent artists, albums, and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space, the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix, created by calling Dijsktra a constant number of times. Two RD algorithms are compared: Landmark MDS, which uses the Nystrom approximation to perform MDS; and a new algorithm called Fast Sparse Embedding, which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps, both in terms of speed and embedding quality.
C1 Microsoft Res, Redmond, WA 98052 USA.
RP Platt, JC (reprint author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.
OI Platt, John/0000-0002-5652-5303
CR Baker C.T.H., 1977, NUMERICAL TREATMENT
   BENGIO Y, 2004, P NIPS, V16
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   COX TF, 2001, MONOGRAPHS STAT APPL, V88
   de Silva V., 2003, P NIPS, P721
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   ELLIS DPW, 2002, P INT C MUS INF RETR
   Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163
   Floyd R., 1962, COMMUNICATONS ACM, V7, P345
   FOWLKES C, 2001, P CVPR, V1
   JOHNSON DB, 1977, J ACM, V24, P1, DOI 10.1145/321992.321993
   Platt JC, 2002, ADV NEUR IN, V14, P1425
   TAKANE Y, 1977, PSYCHOMETRIKA, V42, P7, DOI 10.1007/BF02293745
   Tenenbaum JB, 1998, ADV NEUR IN, V10, P682
NR 14
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 571
EP 578
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500072
DA 2019-06-15
ER

PT S
AU Roman, N
   Wang, DL
   Brown, GJ
AF Roman, N
   Wang, DL
   Brown, GJ
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A classification-based cocktail-party processor
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
ID SOUND
AB At a cocktail party, a listener can selectively attend to a single voice and filter out other acoustical interferences. How to simulate this perceptual ability remains a great challenge. This paper describes a novel supervised learning approach to speech segregation, in which a target speech signal is separated from interfering sounds using spatial location cues: interaural time differences (ITD) and interaural intensity differences (IID). Motivated by the auditory masking effect, we employ the notion of an ideal time-frequency binary mask, which selects the target if it is stronger than the interference in a local time-frequency unit. Within a narrow frequency band, modifications to the relative strength of the target source with respect to the interference trigger systematic changes for estimated ITD and IID. For a given spatial configuration, this interaction produces characteristic clustering in the binaural feature space. Consequently, we perform pattern classification in order to estimate ideal binary masks. A systematic evaluation in terms of signal-to-noise ratio as well as automatic speech recognition performance shows that the resulting system produces masks very close to ideal binary ones. A quantitative comparison shows that our model yields significant improvement in performance over an existing approach. Furthermore, under certain conditions the model produces large speech intelligibility improvements with normal listeners.
C1 Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA.
RP Roman, N (reprint author), Ohio State Univ, Dept Comp & Informat Sci, Columbus, OH 43210 USA.
EM niki@cis.ohio-state.edu; dwang@cis.ohio-state.edu;
   g.brown@dcs.shef.ac.uk
CR Bench J, 1979, SPEECH HEARING TESTS
   Blauert J, 1997, SPATIAL HEARING PSYC
   Bodden M., 1993, Acta Acustica, V1, P43
   Bregman A. S., 1990, AUDITORY SCENE ANAL
   Bronkhorst AW, 2000, ACUSTICA, V86, P117
   BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016
   Cooke M, 2001, SPEECH COMMUN, V34, P267, DOI 10.1016/S0167-6393(00)00034-0
   Cooke M., 1993, MODELING AUDITORY PR
   Gardner WG, 1994, 280 MIT MED LAB
   GLOTIN H, 1999, P EUROSPEECH, P2351
   HU G, 2002, P NIPS
   JOURJINE A, 2000, P ICASSP
   Liu C, 2001, J ACOUST SOC AM, V110, P3218, DOI 10.1121/1.1419090
   Roman N, 2003, J ACOUST SOC AM, V114, P2236, DOI 10.1121/1.1610463
   WHITTKOP T, 2003, SPEECH COMMUN, V39, P111
NR 15
TC 1
Z9 1
U1 0
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1425
EP 1432
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500177
DA 2019-06-15
ER

PT S
AU Rudary, MR
   Singh, S
AF Rudary, MR
   Singh, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A Nonlinear predictive state representation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems. One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models. Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs. We introduce a new notion of tests which allows us to define a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems. These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation-in particular, its potential to be exponentially larger than the equivalent POMDP.
C1 Univ Michigan, Ann Arbor, MI 48109 USA.
RP Rudary, MR (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
CR Jaeger H, 2000, NEURAL COMPUT, V12, P1371, DOI 10.1162/089976600300015411
   LITTMAN ML, 2001, ADV NEURAL INFORMATI, V14
   LITTMAN ML, 1996, THESIS BROWN U
   Lovejoy WS, 1991, ANN OPER RES, V28, P47, DOI 10.1007/BF02055574
   RIVEST RL, 1994, J ACM, V41, P555, DOI 10.1145/176584.176589
   SINGH S, 2003, IN PRESS 20 INT C MA
NR 6
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 855
EP 862
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500107
DA 2019-06-15
ER

PT S
AU Scott, C
   Nowak, R
AF Scott, C
   Nowak, R
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Near-minimax optimal classification with dyadic classification trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 08, 2003
CL CANADA
AB This paper reports on a family of computationally practical classifiers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classifiers are based on dyadic classification trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classification rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n(-1/2), the parametric rate. We are not aware of any other practical classifiers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed.
C1 Rice Univ, Houston, TX 77005 USA.
RP Scott, C (reprint author), Rice Univ, Houston, TX 77005 USA.
EM cscott@rice.edu; nowak@engr.wisc.edu
CR BARRON AR, 1991, NATO ADV SCI I C-MAT, V335, P561
   BENNETT K, 1998, P IEEE INT JOINT C N, V41, P2396
   Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808
   KEARNS M, 1998, INT C MACH LEARN, P269
   Mammen E, 1999, ANN STAT, V27, P1808
   Mansour Y., 2000, P 13 ANN C COMP LEAR, P69
   OKAMOTO M, 1958, ANN I STAT MATH, V10, P29
   SCOTT C, 2002, TREE0201 RIC U
   SCOTT C, 2002, ADV NEURAL INFORMATI, V14
   SCOTT C, 2003, 0301 TREE RIC U
NR 10
TC 1
Z9 1
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1117
EP 1124
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500139
DA 2019-06-15
ER

PT S
AU Solan, Z
   Horn, D
   Ruppin, E
   Edelman, S
AF Solan, Z
   Horn, D
   Ruppin, E
   Edelman, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Unsupervised context sensitive language acquisition from a large corpus
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Significant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of significant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proficiency. The results are encouraging: the model attains a level of performance considered to be "intermediate" for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children.
C1 Tel Aviv Univ, Sackler Fac Exact Sci, IL-69978 Tel Aviv, Israel.
RP Solan, Z (reprint author), Tel Aviv Univ, Sackler Fac Exact Sci, IL-69978 Tel Aviv, Israel.
RI Ruppin, Eytan/R-9698-2017
OI Ruppin, Eytan/0000-0002-7862-3940
CR Chomsky Noam, 1986, KNOWLEDGE LANGUAGE I
   CLARK A, 2001, THESIS U SUSSEX
   Croft William, 2001, RADICAL CONSTRUCTION
   Edelman H, 2002, TRENDS COGN SCI, V6, P125
   Goldberg A., 1995, CONSTRUCTIONS CONSTR
   Gross M, 1997, LANG SPEECH & COMMUN, P329
   HOPPER PJ, 1998, NEW PSYCHOL LANGUAGE, P155
   KLEIN D, 2002, ADV NEURAL INFORMATI, V14
   Langacker Ronald W., 1987, FDN COGNITIVE GRAMMA, V1
   Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X
   MacDonald MC, 2002, PSYCHOL REV, V109, P35, DOI 10.1037//0033-295X.109.1.35
   MACWHINNEY B, 1985, J CHILD LANG, V12, P271, DOI 10.1017/S0305000900006449
   Pereira F.C.N., 1992, ACL 30, P128
   Pinker Steven, 1994, LANGUAGE INSTINCT
   SAG IA, 1999, SYNTACTIC THEORY FOR
   SOLAN Z, 2003, ADV NEURAL INFORMATI, V15
   VANZAANEN M, 2001, 05 LEEDS U SCH COMP
   WOLFF JG, 1988, CATEGORIES PROCESSES, P179
   Wray A., 2002, FORMULAIC LANGUAGE L
NR 19
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 961
EP 968
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500120
DA 2019-06-15
ER

PT S
AU Steck, H
   Jaakkola, TS
AF Steck, H
   Jaakkola, TS
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Bias-corrected bootstrap and model uncertainty
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID BAYESIAN NETWORKS
AB The bootstrap has become a popular method for exploring model (structure) uncertainty. Our experiments with artificial and real-world data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models. Accounting for this bias is hence essential, e.g., when exploring model uncertainty. We find that this bias is intimately tied to (well-known) spurious dependences induced by the bootstrap. The leading-order bias-correction equals one half of Akaike's penalty for model complexity. We demonstrate the effect of this simple bias-correction in our experiments. We also relate this bias to the bias of the plug-in estimator for entropy, as well as to the difference between the expected test and training errors of a graphical model, which asymptotically equals Akaike's penalty (rather than one half).
C1 MIT, CSAIL, Cambridge, MA 02139 USA.
RP Steck, H (reprint author), MIT, CSAIL, 200 Technol Sq, Cambridge, MA 02139 USA.
CR Akaike H., 1973, INT S INF THEOR, P267, DOI DOI 10.2307/2334537
   CARLTON AG, 1969, PSYCHOL BULL, V71, P108, DOI 10.1037/h0026857
   COOPER GF, 1991, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P86
   Davison A, 1997, BOOTSTRAP METHODS TH
   Efron B., 1993, INTRO BOOTSTRAP
   Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961
   Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P196
   Friedman N, 1999, ARTIFICIAL INTELLIGENCE AND STATISTICS 99, PROCEEDINGS, P197
   HARTEMINK AJ, 2002, PAC S BIOC
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503
   Miller GA, 1955, INFORMATION THEORY P, P95
   Pe'er D., 2001, BIOINFORMATICS, V17, pS215
   Spiegelhalter DJ, 2002, J ROY STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353
   STECK H, 2003, 2003002 AI MIT
   STONE M, 1977, J R STAT SOC B, V39, P44
   Victor JD, 2000, NEURAL COMPUT, V12, P2797, DOI 10.1162/089976600300014728
NR 16
TC 1
Z9 1
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 521
EP 528
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500066
DA 2019-06-15
ER

PT S
AU Sugita, Y
   Tani, J
AF Sugita, Y
   Tani, J
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A holistic approach to compositional semantics: A connectionist model
   and robot experiments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the "compositionality" of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the "embodiment" of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and confirmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors.
C1 RIKEN, BSI, Wako, Saitama 3510198, Japan.
RP Sugita, Y (reprint author), RIKEN, BSI, Hirosawa 2-1, Wako, Saitama 3510198, Japan.
CR ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Evans G., 1981, WITTGENSTEIN FOLLOW
   FODOR J, 1999, 46 RUTG U
   HADLEY RF, 1994, MIND LANG, V9, P431, DOI 10.1111/j.1468-0017.1994.tb00225.x
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   ITO M, 2003, 3 RIKEN BRAIN SCI I
   IWAHASHI N, 2003, J JAPANESE SOC ARTIF, V18, P49
   JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1016/0364-0213(92)90036-T
   Miikkulainen R, 1993, SUBSYMBOLIC NATURAL
   ROY D, 2002, COMPUTER SPEECH LANG, V16
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED
   Siskind JM, 2001, J ARTIF INTELL RES, V15, P31, DOI 10.1613/jair.790
   STEELS L, 2000, P EUR C ART INT ECAI, P764
   Tani J, 1996, IEEE T SYST MAN CY B, V26, P421, DOI 10.1109/3477.499793
   Tani J, 2003, NEURAL NETWORKS, V16, P11, DOI 10.1016/S0893-6080(02)00214-9
   WINOGRAD T, 1972, COGNITIVE PSYCHOL, V3, P1, DOI 10.1016/0010-0285(72)90002-3
NR 16
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 969
EP 976
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500121
DA 2019-06-15
ER

PT S
AU Tanaka, SC
   Doya, K
   Okada, G
   Ueda, K
   Okamoto, Y
   Yamawaki, S
AF Tanaka, SC
   Doya, K
   Okada, G
   Ueda, K
   Okamoto, Y
   Yamawaki, S
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Different cortico-basal ganglia loops specialize in reward prediction at
   different time scales
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID MEDIAL PREFRONTAL CORTEX; ORBITOFRONTAL CORTEX; NEURAL RESPONSES;
   PROBABILISTIC REINFORCEMENT; DECISION-MAKING; HUMAN BRAIN; ANTICIPATION;
   SENSITIVITY; RECEPTORS
AB To understand the brain mechanisms involved in reward prediction at different time scales, we developed a Markov decision task that requires prediction of both immediate and future rewards, and analyzed subjects' brain activities using fMRI. We estimated the time course of reward prediction and reward prediction error at different time scales from subjects' performance data, and used them as the explanatory variables for multiple regression analysis with fMRI data. We found topographic maps of different time scales in medial frontal cortex and striatum. The result suggests that different cortico-basal ganglia loops are specialized for reward prediction at different time scales.
C1 Japan Sci & Technol Agcy, Nara Inst Sci & Technol, ATR Computat Neurosci Labs, CREST, Kyoto, Japan.
RP Tanaka, SC (reprint author), Japan Sci & Technol Agcy, Nara Inst Sci & Technol, ATR Computat Neurosci Labs, CREST, Kyoto, Japan.
CR Bechara A, 2000, CEREB CORTEX, V10, P295, DOI 10.1093/cercor/10.3.295
   Berns GS, 2001, J NEUROSCI, V21, P2793, DOI 10.1523/JNEUROSCI.21-08-02793.2001
   Breiter HC, 2001, NEURON, V30, P619, DOI 10.1016/S0896-6273(01)00303-8
   Celada P, 2001, J NEUROSCI, V21, P9917, DOI 10.1523/JNEUROSCI.21-24-09917.2001
   Doya K, 2000, CURR OPIN NEUROBIOL, V10, P732, DOI 10.1016/S0959-4388(00)00153-7
   Doya K, 2002, NEURAL NETWORKS, V15, P495, DOI 10.1016/S0893-6080(02)00044-8
   Elliott R, 2000, CEREB CORTEX, V10, P308, DOI 10.1093/cercor/10.3.308
   Elliott R, 2000, J NEUROSCI, V20, P6159, DOI 10.1523/JNEUROSCI.20-16-06159.2000
   Evenden JL, 1996, PSYCHOPHARMACOLOGY, V128, P161, DOI 10.1007/s002130050121
   HABER SN, 1995, J NEUROSCI, V15, P4851
   Houk JC, 1995, MODELS INFORM PROCES, P249
   Knutson B, 2001, J NEUROSCI, V21, part. no., DOI 10.1523/JNEUROSCI.21-16-j0002.2001
   Koepp MJ, 1998, NATURE, V393, P266
   Martin-Ruiz R, 2001, J NEUROSCI, V21, P9856, DOI 10.1523/JNEUROSCI.21-24-09856.2001
   Mobini S, 2000, PSYCHOPHARMACOLOGY, V152, P390, DOI 10.1007/s002130000542
   Mobini S, 2002, PSYCHOPHARMACOLOGY, V160, P290, DOI 10.1007/s00213-001-0983-0
   O'Doherty J, 2003, J NEUROSCI, V23, P7931
   O'Doherty JP, 2002, NEURON, V33, P815, DOI 10.1016/S0896-6273(02)00603-7
   O'Doherty JP, 2003, NEURON, V38, P329, DOI 10.1016/S0896-6273(03)00169-7
   Pagnoni G, 2002, NAT NEUROSCI, V5, P97, DOI 10.1038/nn802
   Rogers RD, 1999, NEUROPSYCHOPHARMACOL, V20, P322, DOI 10.1016/S0893-133X(98)00091-8
   Rogers RD, 1999, J NEUROSCI, V19, P9029, DOI 10.1523/JNEUROSCI.19-20-09029.1999
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   TANAKA S, 2002, 8 INT C FUNCT MAPP H
NR 25
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 701
EP 708
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500088
DA 2019-06-15
ER

PT S
AU Tsang, EKC
   Shi, BE
AF Tsang, EKC
   Shi, BE
BE Thrun, S
   Saul, K
   Scholkopf, B
TI A neuromorphic multi-chip model of a disparity selective complex cell
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID BINOCULAR DISPARITY; NEURAL MECHANISMS; VISUAL-CORTEX; DEPTH
   DISCRIMINATION; RESPONSES; NEURONS; POSITION; PHASE
AB The relative depth of objects causes small shifts in the left and right retinal positions of these objects, called binocular disparity. Here, we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model, which has been proposed to model the response of disparity selective cells in the visual cortex. Our system consists of two silicon chips containing spiking neurons with monocular Gabor-type spatial receptive fields (RF) and circuits that combine the spike outputs to compute a disparity selective complex cell response. The disparity selectivity of the cell can be adjusted by both position and phase shifts between the monocular RF profiles, which are both used in biology. Our neuromorphic system performs better with phase encoding, because the relative responses of neurons tuned to different disparities by phase shifts are better matched than the responses of neurons tuned by position shifts.
C1 Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong, Peoples R China.
RP Tsang, EKC (reprint author), Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong, Peoples R China.
CR ALBRECHT DG, 1991, VISUAL NEUROSCI, V7, P531, DOI 10.1017/S0952523800010336
   Anzai A, 1999, J NEUROPHYSIOL, V82, P891
   Anzai A, 1999, J NEUROPHYSIOL, V82, P874
   Anzai A, 1999, J NEUROPHYSIOL, V82, P909
   BARLOW HB, 1967, J PHYSIOL-LONDON, V193, P327, DOI 10.1113/jphysiol.1967.sp008360
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   CHOI TYW, 2003, P IEEE INT S CIRC SY, V4, P800
   Cumming BG, 1997, NATURE, V389, P280, DOI 10.1038/38487
   Fleet DJ, 1996, VISION RES, V36, P1839, DOI 10.1016/0042-6989(95)00313-4
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   OHZAWA I, 1990, SCIENCE, V249, P1037, DOI 10.1126/science.2396096
   POGGIO GF, 1985, VISION RES, V25, P397, DOI 10.1016/0042-6989(85)90065-3
   Qian N, 1997, VISION RES, V37, P1811, DOI 10.1016/S0042-6989(96)00331-8
NR 13
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 1051
EP 1058
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500131
DA 2019-06-15
ER

PT S
AU Vollgraf, R
   Scholz, M
   Meinertzhagen, IA
   Obermayerl, K
AF Vollgraf, R
   Scholz, M
   Meinertzhagen, IA
   Obermayerl, K
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Nonlinear filtering of electron micrographs by means of support vector
   regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Nonlinear filtering can solve very complex problems, but typically involve very time consuming calculations. Here we show that for filters that are constructed as a RBF network with Gaussian basis functions, a decomposition into linear filters exists, which can be computed efficiently in the frequency domain, yielding dramatic improvement in speed. We present an application of this idea to image processing. In electron micrograph images of photoreceptor terminals of the fruit fly, Drosophila, synaptic vesicles containing neurotransmitter should be detected and labeled automatically. We use hand labels, provided by human experts, to learn a RBF filter using Support Vector Regression with Gaussian kernels. We will show that the resulting nonlinear filter solves the task to a degree of accuracy, which is close to what can be achieved by human experts. This allows the very time consuming task of data evaluation to be done efficiently.
C1 Tech Univ Berlin, Dept Elect Engn & Comp Engn, Berlin, Germany.
RP Vollgraf, R (reprint author), Tech Univ Berlin, Dept Elect Engn & Comp Engn, Berlin, Germany.
CR CHANG CC, 2003, LIBSVM LIB SUPPORT V
   FABIANFINE R, 2003, IN PRESS J NEUROSCI
   HARVEY LO, 1992, ORGAN BEHAV HUM DEC, V53, P229, DOI 10.1016/0749-5978(92)90063-D
   Haykin  S.S., 1998, NEURAL NETWORKS COMP
   Mercer J, 1909, PHILOS T R SOC LOND, V209, P415, DOI 10.1098/rsta.1909.0016
   PRESS WH, 1992, RECIPES C
   Scholkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Scholkopf B., 2002, LEARNING KERNELS
   Stowers RS, 1999, GENETICS, V152, P1631
   Vapnik VN, 1995, NATURE STAT LEARNING
NR 10
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 717
EP 724
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500090
DA 2019-06-15
ER

PT S
AU Welling, M
   Agakov, F
   Williams, CKI
AF Welling, M
   Agakov, F
   Williams, CKI
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Extreme components analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
AB Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for "extreme components analysis" (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efficient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.
C1 Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G5, Canada.
RP Welling, M (reprint author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd, Toronto, ON M5S 3G5, Canada.
CR HINTON GE, 1999, P 9 INT C ART NEUR N, V1, P1
   Proakis J. G., 1992, DIGITAL SIGNAL PROCE
   Roweis S. T, 1997, ADV NEURAL INFORMATI, V10, P626
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   WELLING M, 2003, IN PRESS P C UNC ART
   Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439
   Zhu H., 1998, NEURAL NETWORKS MACH
NR 7
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 137
EP 144
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500018
DA 2019-06-15
ER

PT S
AU Welling, M
   Teh, YW
AF Welling, M
   Teh, YW
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Linear response for approximate inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID BOLTZMANN MACHINES; ALGORITHMS
AB Belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. In this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill. The first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point. The second algorithm is based on matrix inversion. Experiments compare a number of competing methods.
C1 Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
RP Welling, M (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
RI Teh, Yee Whye/C-3400-2008
CR HESKES T, 2003, ADV NEURAL INFORMATI, V15
   Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Opper  M., 2001, ADV MEAN FIELD METHO
   Tanaka K, 2003, IEICE T INF SYST, VE86D, P1228
   TEH YW, 2001, ADV NEURAL INFORMATI
   WAINWRIGHT MJ, 2003, UCBCDS31226
   Welling M, 2003, ARTIF INTELL, V143, P19, DOI 10.1016/S0004-3702(02)00361-2
   Welling M, 2004, NEURAL COMPUT, V16, P197, DOI 10.1162/08997660460734056
   YEDIDIA JS, 2000, ADV NEURAL INFORMATI, V13
   Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674
NR 11
TC 1
Z9 1
U1 0
U2 0
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 361
EP 368
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500046
DA 2019-06-15
ER

PT S
AU Wu, TF
   Lin, CJ
   Weng, RC
AF Wu, TF
   Lin, CJ
   Weng, RC
BE Thrun, S
   Saul, K
   Scholkopf, B
TI Probability estimates for multi-class classification by pairwise
   coupling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
LA English
DT Proceedings Paper
CT 17th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC   08, 2003
CL CANADA
ID RECOGNITION
AB Pairwise coupling is a popular multi-class classification method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3].
C1 Natl Taiwan Univ, Dept Comp Sci, Taipei 106, Taiwan.
RP Wu, TF (reprint author), Natl Taiwan Univ, Dept Comp Sci, Taipei 106, Taiwan.
CR Chang Chih-Chung, 2001, LIBSVM LIB SUPPORT V
   Friedman J., 1996, ANOTHER APPROACH POL
   Hastie T, 1998, ANN STAT, V26, P451
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   HUNTER DR, 2004, IN PRESS ANN STAT
   Knerr S., 1990, NEUROCOMPUTING ALGOR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin H-T, 2003, NOTE PLATTS PROBABIL
   Michie D., 1994, MACHINE LEARNING NEU
   Platt J., 2000, ADV LARGE MARGIN CLA
   Price D., 1994, NEURAL INFORM PROCES, V7, P1109
   REFREGIER P, 1991, P INT C ART NETW, P1003
NR 12
TC 1
Z9 1
U1 0
U2 1
PU M I T PRESS
PI CAMBRIDGE
PA FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA
SN 1049-5258
BN 0-262-20152-6
J9 ADV NEUR IN
PY 2004
VL 16
BP 529
EP 536
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA BBF99
UT WOS:000225309500067
DA 2019-06-15
ER

PT J
AU Gottlieb, LA
   Kontorovich, A
   Nisnevitch, P
AF Gottlieb, Lee-Ad
   Kontorovich, Aryeh
   Nisnevitch, Pinhas
TI Near-Optimal Sample Compression for Nearest Neighbors
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article; Proceedings Paper
CT 27th International Conference on Neural Information Processing Systems
   (NIPS)
CY DEC 08-13, 2014
CL Montreal, CANADA
DE Nearest neighbor methods
ID LEARNING ALGORITHMS; CLASSIFICATION; RULE; CONSISTENCY; REGRESSION;
   REDUCTION; SPACES; RISK
AB We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our performance bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.
C1 [Gottlieb, Lee-Ad] Ariel Univ, Dept Comp Sci, IL-407000 Samaria, Israel.
   [Kontorovich, Aryeh] Ben Gurion Univ Negev, Dept Comp Sci, IL-8410501 Beer Sheva, Israel.
   [Nisnevitch, Pinhas] Tel Aviv Univ, Comp Sci Dept, IL-6997801 Tel Aviv, Israel.
RP Gottlieb, LA (reprint author), Ariel Univ, Dept Comp Sci, IL-407000 Samaria, Israel.
EM leead@ariel.ac.il; karyeh@cs.bgu.ac.il; pinhasn@mail.tau.ac.il
OI Kontorovich, Aryeh/0000-0001-8038-8671
FU Israel Science Foundation [755/15, 1141/12]; Yahoo Faculty Award
FX L. Gottlieb was supported by the Israel Science Foundation under Grant
   755/15. A. Kontorovich was supported in part by the Israel Science
   Foundation under Grants 1141/12 and 755/15 and in part by a Yahoo
   Faculty Award.
CR Alon N, 1997, J ACM, V44, P615, DOI 10.1145/263867.263927
   Angiulli E, 2005, P 22 INT C MACH LEAR, P25
   Arora S., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P724, DOI 10.1109/SFCS.1993.366815
   Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43
   Bshouty NH, 2009, J COMPUT SYST SCI, V75, P323, DOI 10.1016/j.jcss.2009.01.003
   Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437
   Clarkson K. L., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P160, DOI 10.1145/177424.177609
   Clment K., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857
   Cole R., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P574
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633
   Devroye L., 1996, APPL MATH, V31
   Dinur I, 2004, INFORM PROCESS LETT, V89, P247, DOI 10.1016/j.ipl.2003.11.007
   FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797
   GATES GW, 1972, IEEE T INFORM THEORY, V18, P431, DOI 10.1109/TIT.1972.1054809
   Gottlieb L., 2014, ADV NEURAL INFORM PR, P370
   Gottlieb LA, 2017, IEEE T INFORM THEORY, V63, P4838, DOI 10.1109/TIT.2017.2713820
   Gottlieb LA, 2017, J MACH LEARN RES, V18
   Gottlieb LA, 2016, THEOR COMPUT SCI, V620, P105, DOI 10.1016/j.tcs.2015.10.040
   Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840
   Gottlieb LA, 2013, SIAM J DISCRETE MATH, V27, P1759, DOI 10.1137/120874242
   Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7
   Gupta A, 2003, ANN IEEE SYMP FOUND, P534, DOI 10.1109/SFCS.2003.1238226
   Har-Peled S, 2006, SIAM J COMPUT, V35, P1148, DOI 10.1137/S0097539704446281
   HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155
   HAUSSLER D, 1988, ARTIF INTELL, V36, P177, DOI 10.1016/0004-3702(88)90002-1
   Karacali B, 2003, IEEE T NEURAL NETWOR, V14, P127, DOI 10.1109/TNN.2002.804315
   Kontorovich A., 2014, P 31 INT C MACH LEAR, P892
   Kontorovich Aryeh, 2016, P ADV NEUR INF PROC, P856
   Kontorovich Aryeh, 2015, P 18 INT C ART INT S
   Kontorovich Aryeh, 2017, P ADV NEUR INF PROC, P1572
   Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P791
   Laviolette F, 2010, MACH LEARN, V78, P175, DOI 10.1007/s10994-009-5137-3
   Li Y., 2006, NEURAL INFORM PROCES, P889
   Littlestone N., 1986, COMPUT INF SCI
   Marchand M, 2003, J MACH LEARN RES, V3, P723, DOI 10.1162/jmlr.2003.3.4-5.723
   Mohri M., 2012, FDN MACHINE LEARNING
   RITTER GL, 1975, IEEE T INFORM THEORY, V21, P665, DOI 10.1109/TIT.1975.1055464
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Snapp RR, 1998, ANN STAT, V26, P850
   Toussaint G, 2002, LECT NOTES COMPUT SC, V2866, P273
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   von Luxburg U, 2004, J MACH LEARN RES, V5, P669
   Wilfong G., 1991, P 7 ANN ACM S COMP G, P224
   Wilson DR, 2000, MACH LEARN, V38, P257, DOI 10.1023/A:1007626913721
   Zukhba A V, 2010, Pattern Recognition and Image Analysis, V20, P484, DOI 10.1134/S1054661810040097
NR 46
TC 0
Z9 0
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9448
EI 1557-9654
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD JUN
PY 2018
VL 64
IS 6
BP 4120
EP 4128
DI 10.1109/TIT.2018.2822267
PG 9
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA GG8XR
UT WOS:000432983500009
DA 2019-06-15
ER

PT S
AU Aaronson, S
   Chen, XY
   Hazan, E
   Kale, S
   Nayak, A
AF Aaronson, Scott
   Chen, Xinyi
   Hazan, Elad
   Kale, Satyen
   Nayak, Ashwin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Online Learning of Quantum States
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Suppose we have many copies of an unknown n-qubit state rho. We measure some copies of rho using a known two-outcome measurement E-1, then other copies using a measurement E-2, and so on. At each stage t, we generate a current hypothesis omega(t) about the state rho, using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that vertical bar Tr (E-i omega(t)) - Tr (E-i rho)vertical bar, the error in our prediction for the next measurement, is at least epsilon at most O(n/epsilon(2)) times. Even in the "non-realizable" setting-where there could be arbitrary noise in the measurement outcomes-we show how to output hypothesis states that incur at most O(root Tn) excess loss over the best possible state on the first T measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states, to the online and regret-minimization settings. We give three different ways to prove our results-using convex optimization, quantum postselection, and sequential fat-shattering dimension-which have different advantages in terms of parameters and portability.
C1 [Aaronson, Scott] UT Austin, Austin, TX 78712 USA.
   [Chen, Xinyi; Hazan, Elad] Google AI Princeton, Princeton, NJ USA.
   [Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA.
   [Kale, Satyen] Google AI, New York, NY USA.
   [Nayak, Ashwin] Univ Waterloo, Waterloo, ON, Canada.
RP Aaronson, S (reprint author), UT Austin, Austin, TX 78712 USA.
EM aaronson@cs.utexas.edu; xinyic@google.com; ehazan@cs.princeton.edu;
   satyenkale@google.com; ashwin.nayak@uwaterloo.ca
FU Vannevar Bush Faculty Fellowship from the US Department of Defense; NSF
   Alan T. Waterman Award; NSERC Canada
FX Supported by a Vannevar Bush Faculty Fellowship from the US Department
   of Defense. Part of this work was done while the author was supported by
   an NSF Alan T. Waterman Award.; Research supported in part by NSERC
   Canada.
CR Aaronson S., 2016, 28 MCGILL INV WORKSH
   Aaronson S., 2006, P IEEE C COMP COMPL, P261
   Aaronson S., 2004, THEORY COMPUTING, V1, P1
   Aaronson S, 2007, P ROY SOC A-MATH PHY, V463, P3089, DOI 10.1098/rspa.2007.0113
   Aaronson S, 2018, STOC'18: PROCEEDINGS OF THE 50TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P325, DOI 10.1145/3188745.3188802
   Aaronson S, 2014, SIAM J COMPUT, V43, P1131, DOI 10.1137/110856939
   Ambainis A, 2002, J ACM, V49, P496, DOI 10.1145/581771.581773
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Arora S, 2016, J ACM, V63, DOI 10.1145/2837020
   Audenaert KMR, 2005, J MATH PHYS, V46, DOI 10.1063/1.2044667
   Badescu C., 2017, TECHNICAL REPORT
   Bhatia  R., 1997, GRADUATE TEXTS MATH, V169
   Carlen EA, 2014, J MATH PHYS, V55, DOI 10.1063/1.4871575
   Gao J., 2015, PHYS REV A, V92
   Haah J, 2017, IEEE T INFORM THEORY, V63, P5628, DOI 10.1109/TIT.2017.2719044
   Hazan E., 2015, FDN TRENDS OPTIMIZAT, V2
   Nayak A., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P369, DOI 10.1109/SFFCS.1999.814608
   O'Donnell R, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P899, DOI 10.1145/2897518.2897544
   Rakhlin A, 2015, J MACH LEARN RES, V16, P155
   Rocchetto A, 2017, ARXIV171200127
   Rocchetto A., 2017, ARXIV170500345
   Tsuda K, 2005, J MACH LEARN RES, V6, P995
   Watrous J., 2018, THEORY QUANTUM INFOR
   Wilde MM, 2013, P ROY SOC A-MATH PHY, V469, DOI 10.1098/rspa.2013.0259
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003051
DA 2019-06-15
ER

PT S
AU Abid, A
   Zou, J
AF Abid, Abubakar
   Zou, James
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Autowarp: Learning a Warping Distance from Unlabeled Time Series Using
   Sequence Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Measuring similarities between unlabeled time series trajectories is an important problem in domains as diverse as medicine, astronomy, finance, and computer vision. It is often unclear what is the appropriate metric to use because of the complex nature of noise in the trajectories (e.g. different sampling rates or outliers). Domain experts typically hand-craft or manually select a specific metric, such as dynamic time warping (DTW), to apply on their data. In this paper, we propose Autowarp, an end-to-end algorithm that optimizes and learns a good metric given unlabeled trajectories. We define a flexible and differentiable family of warping metrics, which encompasses common metrics such as DTW, Euclidean, and edit distance. Autowarp then leverages the representation power of sequence autoencoders to optimize for a member of this warping distance family. The output is a metric which is easy to interpret and can be robustly learned from relatively few trajectories. In systematic experiments across different domains, we show that Autowarp often outperforms hand-crafted trajectory similarity metrics.
C1 [Abid, Abubakar; Zou, James] Stanford Univ, Stanford, CA 94305 USA.
RP Abid, A (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM a12d@stanford.edu; jamesz@stanford.edu
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005015
DA 2019-06-15
ER

PT S
AU Abu-El-Haija, S
   Perozzi, B
   Al-Rfou, R
   Alemi, A
AF Abu-El-Haija, Sami
   Perozzi, Bryan
   Al-Rfou, Rami
   Alemi, Alex
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Watch Your Step: Learning Node Embeddings via Graph Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Graph embedding methods represent nodes in a continuous vector space, preserving different types of relational information from the graph. There are many hyperparameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph. In this paper, we replace previously fixed hyperparameters with trainable ones that we automatically learn via backpropagation. In particular, we propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective. Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk), and are not used by the model for inference. We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social, collaboration, and biological networks, where we observe that our graph attention model can reduce the error by up to 20%-40%. We show that our automatically-learned attention parameters can vary significantly per graph, and correspond to the optimal choice of hyperparameter if we manually tune existing methods.
C1 [Abu-El-Haija, Sami] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.
   [Perozzi, Bryan] Google AI, New York, NY USA.
   [Abu-El-Haija, Sami; Al-Rfou, Rami; Alemi, Alex] Google AI, Mountain View, CA USA.
RP Abu-El-Haija, S (reprint author), Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.; Abu-El-Haija, S (reprint author), Google AI, Mountain View, CA USA.
EM haija@isi.edu; bperozzi@acm.org; rmyeid@google.com; alemi@google.com
CR Abu-El-Haija  S., 2017, PROPORTIONATE GRADIE
   Abu-El-Haija  S., 2017, ACM INT C INF KNOWL
   Bahdanau  D., 2015, INT C LEARN REPR ICL
   Belkin  M., 2003, NEURAL COMPUTATION
   Bronstein M. M., 2017, IEEE SIGNAL PROCESSI
   Bruna  J., 2013, INT C LEARN REPR
   Cao  S., 2016, ASS ADV ARTIFICIAL I
   Chen H, 2018, ARXIV180802590
   Chen  H., 2018, 21 AAAI C ART INT
   Dai  H., 2016, INT C MACH LEARN
   Defferrard  M., 2016, ADV NEURAL INFORM PR
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR
   Goyal  P., 2018, KNOWLEDGE BASED SYST
   Grover  A., 2016, INT C KNOWL DISC DA
   Gupta  M., 2016, INT C COMP WORLD WID
   Halko  N., 2011, SIAM REV
   Hamilton  W., 2017, NIPS
   Hamilton W. L., 2017, IEEE DATA ENG B
   Heess Nicolas, 2014, ADV NEURAL INFORM PR
   Kipf T. N., 2017, INT C LEARN REPR ICL
   Levy  O., 2015, TACL
   Li Y., 2016, INT C LEARN REPR
   Liben-Nowell  D., 2007, J AM SOC INFORM SCI
   Luo  Y., 2015, C EMP METH NAT LANG
   Mikolov Tomas, 2013, ADV NEURAL INFORM PR
   Niepert Mathias, 2016, INT C MACH LEARN ICM
   Pennington J., 2014, C EMP METH NAT LANG
   Perozzi  B., 2017, ADV SOCIAL NETWORKS
   Perozzi  B., 2014, KNOWLEDGE DISCOVERY
   Ramanathan Vignesh, 2016, IEEE C COMP VIS PATT
   Scarselli F., 2009, IEEE T NEURAL NETWOR
   Stark  C., 2006, NUCL ACIDS RES
   Tong  H., 2006, INT C DAT MIN ICDM
   Towsley  D., 2016, ADV NEURAL INFORM PR
   Velickovic  P., 2018, INT C LEARN REPR ICL
   Wang  D., 2016, INT C KNOWL DISC DAT
   Yang  Z., 2016, INT C MACH LEARN ICM
   Yang  Z., 2016, C N AM CHAPT ASS COM
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003071
DA 2019-06-15
ER

PT S
AU Acerbi, L
AF Acerbi, Luigi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Variational Bayesian Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.
C1 [Acerbi, Luigi] Univ Geneva, Dept Basic Neurosci, Geneva, Switzerland.
RP Acerbi, L (reprint author), Univ Geneva, Dept Basic Neurosci, Geneva, Switzerland.
EM luigi.acerbi@unige.ch
CR Acerbi L., 2017, ADV NEURAL INFORM PR, V30, P1834
   Bishop C. M., 2006, PATTERN RECOGNITION
   BRIOL F. -X., 2015, ADV NEURAL INFORM PR, P1162
   Brochu E., 2010, ARXIV10122599
   Carpenter B, 2017, J STAT SOFTWARE, V76
   Gershman S., 2012, P 29 INT COF MACH LE
   Geyer C. J., 1994, TECHNICAL REPORT
   GILKS WR, 1994, STATISTICIAN, V43, P179, DOI 10.2307/2348942
   Goris RLT, 2015, NEURON, V88, P819, DOI 10.1016/j.neuron.2015.10.009
   Gramacy RB, 2012, STAT COMPUT, V22, P713, DOI 10.1007/s11222-010-9224-x
   Gunter T, 2014, ADV NEURAL INFORM PR, V27, P2789
   Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kandasamy K., 2015, 24 INT JOINT C ART I
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Miller A. C., 2017, P 34 INT C MACH LEAR, V70, P2420
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   Osborne M, 2012, ADV NEURAL INFORM PR, V25, P46
   Rasmussen C. E., 2002, ADV NEURAL INFORM PR, P505
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Wang HQ, 2018, NEURAL COMPUT, V30, P3072, DOI 10.1162/neco_a_01127
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002073
DA 2019-06-15
ER

PT S
AU Acharya, J
   Sun, ZT
   Zhang, HY
AF Acharya, Jayadev
   Sun, Ziteng
   Zhang, Huanyu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private Testing of Identity and Closeness of Discrete
   Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We study the fundamental problems of identity testing (goodness of fit), and closeness testing (two sample test) of distributions over k elements, under differential privacy. While the problems have a long history in statistics, finite sample bounds for these problems have only been established recently.
   In this work, we derive upper and lower bounds on the sample complexity of both the problems under (epsilon, delta)-differential privacy. We provide sample optimal algorithms for identity testing problem for all parameter ranges, and the first results for closeness testing. Our closeness testing bounds are optimal in the sparse regime where the number of samples is at most k.
   Our upper bounds are obtained by privatizing non-private estimators for these problems. The non-private estimators are chosen to have small sensitivity. We propose a general framework to establish lower bounds on the sample complexity of statistical tasks under differential privacy. We show a bound on differentially private algorithms in terms of a coupling between the two hypothesis classes we aim to test. By carefully constructing chosen priors over the hypothesis classes, and using Le Cam's two point theorem we provide a general mechanism for proving lower bounds. We believe that the framework can be used to obtain strong lower bounds for other statistical tasks under privacy.
C1 [Acharya, Jayadev; Sun, Ziteng; Zhang, Huanyu] Cornell Univ, Ithaca, NY 14853 USA.
RP Acharya, J (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM acharya@cornell.edu; zs335@cornell.edu; hz388@cornell.edu
FU NSF-CCF-CRII [1657471]; Cornell University
FX The authors are listed in alphabetical order. This research was
   supported by NSF-CCF-CRII 1657471, and a grant from Cornell University.
CR Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435
   Acharya Jayadev, 2018, P MACHINE LEARNING R, V80, P30
   Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3577
   Acharya Jayadev, 2018, ARXIV180802174
   Acharya Jayadev, 2018, ARXIV180204705
   Acharya Jayadev, 2013, P 16 INT C ART INT S
   Acharya S, 2014, 2014 IEEE 2ND INTERNATIONAL CONFERENCE ON EMERGING ELECTRONICS (ICEE)
   Aliakbarpour Maryam, 2018, P 35 INT C MACH LEAR, P169
   Assouad Bin Yu., 1997, FESTSCHRIFT L LECAM, P423
   Barthe G, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P55, DOI 10.1145/2976749.2978391
   Barthe G, 2016, PROCEEDINGS OF THE 31ST ANNUAL ACM-IEEE SYMPOSIUM ON LOGIC IN COMPUTER SCIENCE (LICS 2016), P749, DOI 10.1145/2933575.2934554
   Batu T, 2001, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.2001.959920
   Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113
   Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86
   Batu Tugkan, 2001, THESIS
   Bhattacharya Bhaswar B., 2015, ADV NEURAL INFORM PR, P2611
   Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148
   Cai Bryan, 2017, P 34 INT C MACH LEAR, P635
   Canonne C.L., 2015, ELECT C COMPUTATIONA, V22, P63
   Canonne Clement L., 2016, P 33 S THEOR ASP COM
   Chan Siu-On, 2014, P 25 ANN ACM SIAM S, P1193, DOI DOI 10.1137/1.9781611973402.88
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Cuff P., 2016, P 2016 ACM SIGSAC C, P43
   Dalenius T., 1977, STAT TIDSKRIFT, V15, P429
   den Hollander Frank, 2012, LECT NOTES
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78
   Diakonikolas Ilias, 2018, P 45 INT C AUT LANG
   Diakonikolas Ilias, 2015, P 26 ANN ACM SIAM S, P1841
   Diakonikolas Ilias, 2015, ADV NEURAL INFORM PR, V28, P2566
   Dinur I., 2003, P 22 ACM SIGMOD SIGA, P202, DOI DOI 10.1145/773153.773173
   Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53
   Dwork C, 2008, LECT NOTES COMPUT SC, V4890, P1
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2015, LECT NOTES COMPUT SC, V9453, P735, DOI 10.1007/978-3-662-48800-3_30
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Erlingsson Ulfar, 2014, P 2014 ACM SIGSAC C, P1054, DOI DOI 10.1145/2660267.2660348
   Gaboardi Marco, 2016, P 33 INT C MACH LEAR, P1395
   Goldreich O., 2011, STUDIES COMPLEXITY C, V6650, P68, DOI DOI 10.1007/978-3-642-22670-0
   Goldreich Oded, 2016, ELECT C COMPUTATIONA, V23
   Hardt M, 2010, ACM S THEORY COMPUT, P705
   Issa Ibrahim, 2017, P 2017 IEEE INT S IN
   Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505
   Kairouz Peter, 2016, P 33 INT C MACH LEAR, V48, P2436
   Karwa Vishesh, 2018, P 9 C INN THEOR COMP
   Knoblauch A, 2008, SIAM J APPL MATH, V69, P197, DOI 10.1137/070700024
   Lehmann Erich Leo, 2006, THEORY POINT ESTIMAT, V31
   Li C, 2015, VLDB J, V24, P757, DOI 10.1007/s00778-015-0398-x
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Mir Darakhshan J, 2012, FDN PRACTICE SECURIT, P374
   Narayanan A, 2008, P IEEE S SECUR PRIV, P111, DOI 10.1109/SP.2008.33
   Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009
   Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987
   Pastore A, 2016, IEEE INT SYMP INFO, P2694, DOI 10.1109/ISIT.2016.7541788
   Rogers Ryan, 2017, ARTIF INTELL, V54, P991
   Sankar L, 2013, IEEE T INF FOREN SEC, V8, P838, DOI 10.1109/TIFS.2013.2253320
   Sheffet Or, 2018, P 35 INT C MACH LEAR, V80, P4612
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7
   Valiant G, 2014, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2014.14
   Wainwright M. J., 2012, ADV NEURAL INFORM PR, P1430
   Wang Shaowei, 2016, ARXIV160708025
   Wang WN, 2016, IEEE T INFORM THEORY, V62, P5018, DOI 10.1109/TIT.2016.2584610
   Wang  Y., 2015, ARXIV151103376
   WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137
   Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651
   Ye M, 2018, IEEE T INFORM THEORY, V64, P5662, DOI 10.1109/TIT.2018.2809790
NR 67
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001042
DA 2019-06-15
ER

PT S
AU Acharya, J
   Bhattacharyya, A
   Daskalakis, C
   Kandasamy, S
AF Acharya, Jayadev
   Bhattacharyya, Arnab
   Daskalakis, Constantinos
   Kandasamy, Saravanan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning and Testing Causal Models with Interventions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISTRIBUTIONS; SELECTION; CIRCUITS
AB We consider testing and learning problems on causal Bayesian networks as defined by Pearl [Pea09]. Given a causal Bayesian network M on a graph with n discrete variables and bounded in-degree and bounded "confounded components", we show that O(log n) interventions on an unknown causal Bayesian network X on the same graph, and O(n/epsilon(2)) samples per intervention, suffice to efficiently distinguish whether X = M or whether there exists some intervention under which X and M are farther than epsilon in total variation distance. We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph. Although our algorithms are non-adaptive, we show that adaptivity does not help in general: Omega(log n) interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively. Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.
C1 [Acharya, Jayadev] Cornell Univ, Sch ECE, Ithaca, NY 14853 USA.
   [Bhattacharyya, Arnab] Natl Univ Singapore, Singapore, Singapore.
   [Bhattacharyya, Arnab] Indian Inst Sci, Bengaluru, Karnataka, India.
   [Daskalakis, Constantinos] MIT, EECS, Cambridge, MA 02139 USA.
   [Kandasamy, Saravanan] Tata Inst Fundamental Res, STCS, Mumbai, Maharashtra, India.
RP Acharya, J (reprint author), Cornell Univ, Sch ECE, Ithaca, NY 14853 USA.
EM acharya@cornell.edu; arnabb@iisc.ac.in; costis@csail.mit.edu;
   saravan.tuty@gmail.com
FU Google India
FX We would like to thank Vasant Honavar who told us about the problems
   considered here and for several helpful discussions that were essential
   for us to complete this work. We acknowledge the support of Google India
   and NeurIPS in the form of an International Travel Grant, which enabled
   Saravanan Kandasamy to attend the conference.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3577
   Ali R Ayesha, 2005, 21 C UNC ART INT UAI
   ALON N, 1992, RANDOM STRUCT ALGOR, V3, P289, DOI 10.1002/rsa.3240030308
   Alon N, 2004, PROBABILISTIC METHOD
   Angluin D, 2008, MACH LEARN, V72, P113, DOI 10.1007/s10994-008-5048-8
   Angluin D, 2009, J COMPUT SYST SCI, V75, P60, DOI 10.1016/j.jcss.2008.07.004
   Bareinboim E., 2013, P 16 INT C ART INT S, P135
   Bareinboim E, 2012, P 26 AAAI C ART INT, P698
   Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113
   Bhattacharyya Arnab, 2011, P ITCS, P239
   BOLLEN KA, 1992, SOCIOL METHOD RES, V21, P123, DOI 10.1177/0049124192021002001
   Bresler G., 2015, P 47 ANN ACM S THEOR, P771
   Bresler G, 2008, LECT NOTES COMPUT SC, V5171, P343
   Canonne C. L., 2017, P 30 C LEARN THEOR C, P370
   Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Daskalakis Constantinos, 2017, ADV NEURAL INFORM PR
   Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S
   Daskalakis Constantinos, 2017, P MACHINE LEARNING R, V65, P1
   Daskalakis Constantinos, 2018, WHICH DISTRIBUTION D, P2747
   Devroye Luc, 2012, COMBINATORIAL METHOD
   Diakonikolas Ilias, 2016, CORR
   Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038
   Eberhardt F, 2005, P 21 C UNC ART INT U, P178
   Eberhardt Frederick, 2007, THESIS
   Even G., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P10, DOI 10.1145/129712.129714
   Fisher R, 1925, STAT METHODS RES WOR
   Glymour Clark N, 1999, COMPUTATION CAUSATIO
   Goldreich O., 2000, ELECT C COMPUTATIONA, V7
   Goldreich O., 2017, INTRO PROPERTY TESTI
   Haavelmo T, 1943, ECONOMETRICA, V11, P1, DOI 10.2307/1905714
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   Hauser Alain, 2012, P 6 EUR WORKSH PROB, V119
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   Hyttinen A, 2013, J MACH LEARN RES, V14, P3041
   Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002
   Kang Ch., 2006, P C UNC ART INT CAMB, P233
   Klivans Adam, 2017, ARXIV170606274
   Kocaoglu Murat, 2017, INT C MACH LEARN, P1875
   Kocaoglu Murat, 2017, ADV NEURAL INFORM PR, P7021
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lee S, 2013, P 27 AAAI C ART INT
   Lehmann EL, 2006, TESTING STAT HYPOTHE
   Macosko EZ, 2015, CELL, V161, P1202, DOI 10.1016/j.cell.2015.05.002
   Meganck Stijn, 2006, P 3 EUR WORKSH PROB
   Moser RA, 2010, J ACM, V57, DOI 10.1145/1667053.1667060
   Moser RA, 2009, ACM S THEORY COMPUT, P343
   NAOR J, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P213, DOI 10.1145/100216.100244
   Neapolitan R. E., 2004, LEARNING BAYESIAN NE, V38
   Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.2307/2337329
   Pearl J., 2009, CAUSALITY
   Pearl  J., 2011, P 25 AAAI C ART INT, P247
   Peters J, 2011, IEEE T PATTERN ANAL, V33, P2436, DOI 10.1109/TPAMI.2011.71
   Reyzin Lev, 2009, ACTIVE LEARNING INTE
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Scheines Richard, 2008, CAUSAL STRUCTURE SEA
   SCHOLKOPF B., 2010, J MACHINE LEARNING R, P1
   Schulman Leonard J., 2016, P 32 C UNC ART INT U, P666
   Sen Rajat, 2017, ADV NEURAL INFORM PR, P2955
   Shanmugam K, 2015, P ADV NEUR INF PROC, P3195
   Shpitser I, 2008, J MACH LEARN RES, V9, P1941
   SPIEGELHALTER DJ, 1993, STAT SCI, V8, P219, DOI 10.1214/ss/1177010888
   Spirtes P, 1999, ALGORITHM CAUSAL INF
   Spirtes P., 2000, CAUSATION PREDICTION
   Tian Jin, 2002, STUDIES CAUSAL REASO
   Tian Jin, 2002, P 18 C UNC ART INT, P519
   Verma T., 1992, P 8 C UNC ART INT, P323
   Verma T.S., 1995, STUDIES LOGIC FDN MA, V134, P789, DOI DOI 10.1016/S0049-237X(06)80074-1
   Verma Thomas, 1990, UNCERTAINTY ARTIFICI, P69
   Vuffray M, 2016, ADV NEURAL INFORM PR, P2595
   Wang Yuhao, 2017, ADV NEURAL INFORM PR, V30, P5822
   Wright S, 1920, J AGRIC RES, V20, P0557
   Yang Karren, 2018, ARXIV180206310
   Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001
   Zhang K, 2012, ARXIV12023775
NR 78
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004005
DA 2019-06-15
ER

PT S
AU Achille, A
   Eccles, T
   Matthey, L
   Burgess, CP
   Watters, N
   Lerchner, A
   Higgins, I
AF Achille, Alessandro
   Eccles, Tom
   Matthey, Loic
   Burgess, Christopher P.
   Watters, Nick
   Lerchner, Alexander
   Higgins, Irina
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Life-Long Disentangled Representation Learning with Cross-Domain Latent
   Homologies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CONNECTIONIST MODELS; MEMORY
AB Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.
C1 [Achille, Alessandro; Eccles, Tom; Matthey, Loic; Burgess, Christopher P.; Watters, Nick; Lerchner, Alexander; Higgins, Irina] Univ Calif Los Angeles, DeepMind, Los Angeles, CA 90024 USA.
RP Achille, A (reprint author), Univ Calif Los Angeles, DeepMind, Los Angeles, CA 90024 USA.
EM achille@cs.ucla.edu; eccles@google.com; lmatthey@google.com;
   cpburgess@google.com; nwatters@google.com; lerchner@google.com;
   irinah@google.com
CR Achille A., 2017, P ICML WORKSH PRINC
   Achille A., 2018, ANN REV CONTROL ROBO, V1
   Achille A., 2018, IEEE T PATTERN ANAL, P1
   Alemi A. A., 2016, ARXIV161200410
   Ans B, 1997, CR ACAD SCI III-VIE, V320, P989, DOI 10.1016/S0764-4469(97)82472-9
   Auchter A, 2017, FRONT BEHAV NEUROSCI, V11, DOI 10.3389/fnbeh.2017.00002
   Beattie C., 2016, ARXIV161203801
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Burgess C. P., 2017, NIPS WORKSH LEARN DI
   Cichon J, 2015, NATURE, V520, P180, DOI 10.1038/nature14251
   Espeholt  L., 2018, IMPALA SCALABLE DIST
   ETCOFF NL, 1992, COGNITION, V44, P227, DOI 10.1016/0010-0277(92)90002-Y
   Freedman DJ, 2001, SCIENCE, V291, P312, DOI 10.1126/science.291.5502.312
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Furlanello T., 2016, ARXIV160602355
   Garnelo M., 2016, ARXIV160905518
   Goodfellow I. J., 2013, EMPIRICAL INVESTIGAT
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Gulyas B., 1993, WENNER GREN INT SERI
   He K., 2015, ICCV
   Higgins I., 2017, ICLR
   Higgins I., 2017, ICML
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Karaletsos T., 2016, ICLR
   Kim H., 2017, DISENTANGLING FACTOR
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kumar A., 2018, ICLR
   Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Y, 2008, J NEUROPHYSIOL, V100, P966, DOI 10.1152/jn.01354.2007
   Liu  Z., 2015, ICCV
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   Mccloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P104, DOI DOI 10.1016/S0079-7421(08)60536-8
   Milan K., 2016, NIPS
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Necker LA, 1832, LONDON EDINBURGH PHI, V1, P329, DOI DOI 10.1080/14786443208647909
   Nguyen C. V., 2018, ICLR
   Parisotto E., 2015, ICLR
   Przybyszewski AW, 1998, CURR BIOL, V8, pR135, DOI 10.1016/S0960-9822(98)70080-6
   Ramapuram J., 2017, ARXIV170509847
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318
   Rusu A. A., 2016, PROGR NEURAL NETWORK
   Ruvolo P., 2013, ICML
   Schwarz J., 2018, ICML
   Seff A., 2017, NIPS
   Shin H., 2017, NIPS
   Shwartz-Ziv R., 2017, ARXIV170300810
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Xiao  H., 2017, FASHION MNIST NOVEL
   Zenke F., 2017, ICML
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004043
DA 2019-06-15
ER

PT S
AU Adebayo, J
   Gilmer, J
   Muelly, M
   Goodfellow, I
   Hardt, M
   Kim, B
AF Adebayo, Julius
   Gilmer, Justin
   Muelly, Michael
   Goodfellow, Ian
   Hardt, Moritz
   Kim, Been
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sanity Checks for Saliency Maps
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings(2) .
C1 [Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been] Google Brain, Mountain View, CA USA.
   [Hardt, Moritz] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Adebayo, Julius] Google Residency Program, Berkeley, CA USA.
RP Adebayo, J (reprint author), Google Residency Program, Berkeley, CA USA.
EM juliusad@mit.edu; gilmer@google.com; muelly@google.com;
   goodfellow@google.com; mrtz@google.com; beenkim@google.com
CR Alain  G., 2016, ARXIV161001644
   Alber Maximilian, 2018, INT C LEARN REPR
   Ancona Marco, 2018, P 6 ICLR
   Baehrens D, 2010, J MACH LEARN RES, V11, P1803
   Cadamuro Gabriel, 2016, ICML WORKSH REL MACH
   Casillas Jorge, 2013, INTERPRETABILITY ISS, V128
   Chen Jianbo, 2018, P MACHINE LEARNING R, V80, P883
   Dabkowski Piotr, 2017, P ADV NEUR INF PROC, P6970
   Doshi-Velez Finale, 2017, ARXIV171101134
   Erhan Dumitru, 2009, BERNOULLI, V1341, P1
   Fong R., 2017, ARXIV170403296
   Ghorbani Amirata, 2017, ARXIV171010547
   Goodman Bryce, 2016, ARXIV160608813
   Kindermans Pieter-Jan, 2017, ARXIV171100867
   Lakkaraju Himabindu, 2017, ARXIV170701154
   Lundberg S. M., 2017, ADV NEURAL INFORM PR, P4768
   MAHENDRAN A, 2016, EUR C COMP VIS, V9910, P120, DOI DOI 10.1007/978-3-319-46466-4_8
   Meng Qingjie, 2018, AUTOMATIC SHADOW DET
   Montavon G., 2017, DIGITAL SIGNAL PROCE
   Nie Weili, 2018, ICML
   Oztireli A. C., 2018, INT C LEARN REPR ICL
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
   Saxe A., 2011, P 28 INT C MACH LEAR, P1089
   Selvaraju R. R., 2016, ARXIV161107450
   Shrikumar A., 2016, ARXIV160501713
   Simonyan K, 2013, ARXIV13126034
   Smilkov Daniel, 2017, ARXIV170603825
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Sundararajan M, 2017, P 34 INT C MACH LEAR, V70, P3319
   Ulyanov D., 2017, ARXIV171110925
   Vellido A, 2012, P 20 EUR S ART NEUR, V12, P163
   Wang Fulton, 2015, ARXIV151005189
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhang Chiyuan, 2017, P 5 ICLR
   Zintgraf L. M., 2017, ARXIV170204595
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004010
DA 2019-06-15
ER

PT S
AU Adler, J
   Lunz, S
AF Adler, Jonas
   Lunz, Sebastian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Banach Wasserstein GAN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered l(2) as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.
C1 [Adler, Jonas] KTH Royal Inst Technol, Dept Math, Res & Phys, Elekta, Stockholm, Sweden.
   [Lunz, Sebastian] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge, England.
RP Adler, J (reprint author), KTH Royal Inst Technol, Dept Math, Res & Phys, Elekta, Stockholm, Sweden.
EM jonasadl@kth.se; lunz@math.cam.ac.uk
FU Swedish Foundation of Strategic Research [AM13-0049, ID14-0055]; Elekta;
   EPSRC [EP/L016516/1]; Cambridge Centre for Analysis; Confab Capital
   Institute for the Mathematics of Information
FX The work by J.A. was supported by the Swedish Foundation of Strategic
   Research grants AM13-0049, ID14-0055 and Elekta. The work by S.L. was
   supported by the EPSRC grant EP/L016516/1 for the University of
   Cambridge Centre for Doctoral Training, and the Cambridge Centre for
   Analysis. We also acknowledge the support of the Confab Capital
   Institute for the Mathematics of Information.
CR Arjovsky M., 2017, INT C LEARN REPR ICL
   Arjovsky Martin, 2017, INT C MACH LEARN ICM
   Brezis  Haim, 2010, FUNCTIONAL ANAL SOBO
   Chan T. F., 2005, IMAGE PROCESSING ANA, V94
   Deza MM, 2009, ENCY DISTANCES
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Gulrajani Ishaan, 2017, ADV NEURAL INFORM PR
   Heusel  M., 2017, P ADV NEUR INF PROC, P6626
   Karras Tero, 2018, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Liu Shuang, 2017, APPROXIMATION CONVER
   Loshchilov Ilya, 2017, INT C LEARN REPR ICL
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Miyato Takeru, 2018, INT C LEARN REPR ICL
   Petzka Henning, 2018, INT C LEARN REPR ICL
   Radford A., 2016, INT C LEARN REPR ICL
   Rudin W., 1991, INT SERIES PURE APPL
   Seward Calvin, 2018, FIRST ORDER GENERATI
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wei Xiang, 2018, INT C LEARN REPR ICL
   Zhao Junbo Jake, 2017, INT C LEARN REPR ICL
NR 21
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001031
DA 2019-06-15
ER

PT S
AU Agarwal, N
   Suresh, AT
   Yu, F
   Kumar, S
   McMahan, HB
AF Agarwal, Naman
   Suresh, Ananda Theertha
   Yu, Felix
   Kumar, Sanjiv
   McMahan, H. Brendan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI cpSGD: Communication-efficient and differentially-private distributed
   SGD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID THEOREM
AB Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For d variables and n approximate to d clients, the proposed method uses O(log log(nd)) bits of communication per client per coordinate and ensures constant privacy.
   We also improve previous analysis of the Binomial mechanism showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.
C1 [Agarwal, Naman] Google Brain, Princeton, NJ 08540 USA.
   [Suresh, Ananda Theertha; Yu, Felix; Kumar, Sanjiv] Google Res, New York, NY USA.
   [McMahan, H. Brendan] Google Res, Seattle, WA USA.
RP Agarwal, N (reprint author), Google Brain, Princeton, NJ 08540 USA.
EM namanagarwal@google.com; theertha@google.com; felixyu@google.com;
   sanjivk@google.com; mcmahan@google.com
CR Abadi M., 2016, ARXIV160304467
   Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Ailon Nir, 2006, STOC
   Akiba Takuya, 2018, VARIANCE BASED GRADI
   Alistarh Dan, 2016, ARXIV161002132
   Alistarh Dan, 2017, COMMUNICATION EFFICI
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982
   Bottou L, INFINITE MNIST DATAS
   Coates A., 2013, P 30 INT C MACH LEAR, P1337
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   ELIAS P, 1975, IEEE T INFORM THEORY, V21, P194, DOI 10.1109/TIT.1975.1055349
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   Horadam K. J., 2012, HADAMARD MATRICES TH
   Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505
   Konecny J., 2016, ARXIV161005492
   Konecny Jakub, 2016, ARXIV161107555
   Li M., 2014, MATH PROBL ENG, V3, P1
   Li Mu, 2014, ADV NEURAL INFORM PR, P19
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mao Huizi, 2018, INT C LEARN REPR
   McDonald Ryan, 2010, HLT
   McMahan H. B., 2016, ARXIV160205629
   McMahan H. Brendan, 2016, P 20 INT C ART INT S
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Povey D., 2014, PARALLEL TRAINING DE
   Rakhlin Alexander, 2012, ICML
   Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911
   Seide F, 2014, INTERSPEECH, P1058
   Suresh A. T., 2017, INT C MACH LEARN, V70, P3329
   Wen W., 2017, ARXIV170507878
   Wu X, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1307, DOI 10.1145/3035918.3064047
   Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170
   Zaremba W, 2014, ARXIV14092329
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002014
DA 2019-06-15
ER

PT S
AU Aghasi, A
   Ahmed, A
   Hand, P
   Joshi, B
AF Aghasi, Alireza
   Ahmed, Ali
   Hand, Paul
   Joshi, Babhru
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A convex program for bilinear inversion of sparse vectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BLIND DECONVOLUTION; SIGNAL RECOVERY; ALGORITHMS
AB We consider the bilinear inverse problem of recovering two vectors, x is an element of R-L and w is an element of R-L, from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N, respectively. Here, K and N may be larger than, smaller than, or equal to L. We introduce l(1)-BranchHull, which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S-1- and S-2-sparse with respect to a random dictionary, with the sparse vectors satisfying an effective sparsity condition, and present a recovery guarantee that depends on the number of measurements as L >= Omega( S-1 + S-2) log(2) (K + N). Numerical experiments verify that the scaling constant in the theorem is not too large. One application of this problem is the sweep distortion removal task in dielectric imaging, where one of the signals is a nonnegative reflectivity, and the other signal lives in a known subspace, for example that given by dominant wavelet coefficients. We also introduce a variants of l(1)-BranchHull for the purposes of tolerating noise and outliers, and for the purpose of recovering piecewise constant signals. We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images.
C1 [Aghasi, Alireza] GSU, Georgia State Business Sch, Atlanta, GA 30302 USA.
   [Ahmed, Ali] ITU, Dept Elect Engn, Lahore, Pakistan.
   [Hand, Paul] Northeastern Univ, Dept Math, Boston, MA 02115 USA.
   [Hand, Paul] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
   [Joshi, Babhru] Rice Univ, Dept Computat & Appl Math, Houston, TX 77251 USA.
RP Aghasi, A (reprint author), GSU, Georgia State Business Sch, Atlanta, GA 30302 USA.
EM aaghasi@gsu.edu; ali.ahmed@itu.edu.pk; p.hand@northeastern.edu;
   babhru.joshi@rice.edu
FU HEC, Pakistan; NSF [DMS-1464525]
FX Ali Ahmed would like to acknowledge the partial support through the
   grant for the National center of cyber security (NCCS) from HEC,
   Pakistan. Paul Hand would like to acknowledge funding by the grant NSF
   DMS-1464525.
CR Aghasi A, 2016, OPTICA, V3, P754, DOI 10.1364/OPTICA.3.000754
   Aghasi  Alireza, 2016, ARXIV13120525V2
   Ahmed A, 2014, IEEE T INFORM THEORY, V60, P1711, DOI 10.1109/TIT.2013.2294644
   Akritas Michael G, 2016, TOPICS NONPARAMETRIC
   Bahmani S., 2016, ARXIV161004210
   Bahmani S., 2017, ARXIV170205327
   Candes E.J., 2012, FOUND COMPUT MATH, P1
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen T, 2006, IEEE T PATTERN ANAL, V28, P1519, DOI 10.1109/TPAMI.2006.195
   Chen Y., 2015, ADV NEURAL INFORM PR, V2, P739
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   Goldstein T., 2016, ARXIV161007531
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hold Berthold K. P., 1986, ROBOT VISION
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Koltchinskii V, 2015, INT MATH RES NOTICES, P12991, DOI 10.1093/imrn/rnv096
   Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268
   Lecue G, 2018, ANN STAT, V46, P611, DOI 10.1214/17-AOS1562
   Lecue Guillaume, 2017, J MACHINE LEARNING R, V18, P5356
   Ledoux M., 2013, PROBABILITY BANACH S
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee  Kiryung, 2017, ARXIV170204342
   Li X., 2016, ARXIV160604933
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Mendelson Shahar, 2014, C LEARN THEOR, P25
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   O'Grady PD, 2005, INT J IMAG SYST TECH, V15, P18, DOI 10.1002/ima.20035
   Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574
   STOCKHAM TG, 1975, P IEEE, V63, P678, DOI 10.1109/PROC.1975.9800
   Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725
   Tosic I, 2011, IEEE SIGNAL PROC MAG, V28, P27, DOI 10.1109/MSP.2010.939537
   Tu S., 2015, ARXIV150703566
   van de Geer S, 2013, PROBAB THEORY REL, V157, P225, DOI 10.1007/s00440-012-0455-y
   van der Vaart Aad W, 1997, J ROYAL STAT SOC A, V160, P596
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003014
DA 2019-06-15
ER

PT S
AU Ahmed, A
   Aghasi, A
   Hand, P
AF Ahmed, Ali
   Aghasi, Alireza
   Hand, Paul
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Blind Deconvolutional Phase Retrieval via Convex Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID TRANSMISSION; LIGHT; CRYSTALLOGRAPHY
AB We consider the task of recovering two real or complex m-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions k and n, then they can be recovered up to the inherent scaling ambiguity with m > > (k + n) log(2) m phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based Rademacher complexity estimates. Additionally, we provide an ADMM implementation of the method and provide numerical experiments that verify the theory.
C1 [Ahmed, Ali] Informat Technol Univ, Dept Elect Engn, Lahore, Pakistan.
   [Aghasi, Alireza] Georgia State Univ, Dept Business Analyt, Atlanta, GA 30303 USA.
   [Hand, Paul] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Ahmed, A (reprint author), Informat Technol Univ, Dept Elect Engn, Lahore, Pakistan.
EM ali.ahmed@itu.edu.pk; aaghasi@gsu.edu; p.hand@northeastern.edu
FU NSF [DMS 1464525]
FX PH acknowledges support from NSF DMS 1464525.
CR Aghasi A, 2017, CONF REC ASILOMAR C, P1622, DOI 10.1109/ACSSC.2017.8335633
   Aghasi Alireza, 2017, ARXIV170204342
   Ahmed A, 2014, IEEE T INFORM THEORY, V60, P1711, DOI 10.1109/TIT.2013.2294644
   Azhar AH, 2013, IEEE PHOTONIC TECH L, V25, P171, DOI 10.1109/LPT.2012.2231857
   Azhar AH, 2010, IEEE GLOBE WORK, P1052, DOI 10.1109/GLOCOMW.2010.5700095
   Bahmani S., 2017, P INT C ART INT STAT, V54, P252
   Bahmani S., 2017, ARXIV170205327
   Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Elser Veit, 2017, ARXIV170600399
   Fienup C., 1987, IMAGE RECOVERY THEOR, V231, P275
   Goldstein Tom, 2018, IEEE T INFORM THEORY
   Goodman J., 2008, INTRO FOURIER OPTICS
   HARRISON RW, 1993, J OPT SOC AM A, V10, P1046, DOI 10.1364/JOSAA.10.001046
   Koltchinskii V, 2015, INT MATH RES NOTICES, P12991, DOI 10.1093/imrn/rnv096
   Lecue G, 2018, ANN STAT, V46, P611, DOI 10.1214/17-AOS1562
   Lecue Guillaume, 2017, J MACHINE LEARNING R, V18, P5356
   Mendelson Shahar, 2014, C LEARN THEOR, P25
   Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642
   MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394
   Retamal JRD, 2015, OPT EXPRESS, V23, P33656, DOI 10.1364/OE.23.033656
   van de Geer S, 2013, PROBAB THEORY REL, V157, P225, DOI 10.1007/s00440-012-0455-y
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004057
DA 2019-06-15
ER

PT S
AU Ahn, K
   Lee, K
   Cha, H
   Suh, C
AF Ahn, Kwangjun
   Lee, Kangwook
   Cha, Hyunseung
   Suh, Changho
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Binary Rating Estimation with Graph Side Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs. However, the gain is not theoretically quantified. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit Our experimental results demonstrate that the algorithm performs well even with real-world graphs.
C1 [Ahn, Kwangjun] 142nd Mil Police Co, Korean Augmentat US Army, Seoul, South Korea.
   [Lee, Kangwook; Suh, Changho] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
   [Cha, Hyunseung] Kakao Brain, Jeju, South Korea.
RP Ahn, K (reprint author), 142nd Mil Police Co, Korean Augmentat US Army, Seoul, South Korea.
EM kjahnkorea@kaist.ac.kr; kw1jjang@kaist.ac.kr; tony.cha@kakaobrain.com;
   chsuh@kaist.ac.kr
FU National Research Foundation of Korea (NRF) - Korea government (MSIP)
   [2018R1A1A1A05022889]
FX The work was jointly supported by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIP) (No.
   2018R1A1A1A05022889) and Kakao Brain Corp.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Abbe Emmanuel, 2017, COMMUNICATIONS PURE
   Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Agarwal D., 2010, P 3 ACM INT C WEB SE, V10, P91, DOI DOI 10.1145/1718487.1718499
   Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216
   Balakrishnan Sivaraman, 2017, ANN STAT
   Bell RM, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P95
   Boutsidis  C., 2015, P 32 INT C MACH LEAR, P40
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   CHEN Y., 2015, P INT C MACH LEARN, P371
   Chen Yuxin, 2016, ICML
   Chiang Kai-Yang, 2015, ADV NEURAL INFORM PR, P3447
   Chin Peter, 2015, STOCHASTIC BLOCK MOD, P391
   Chouvardas S, 2017, INT CONF ACOUST SPEE, P4019, DOI 10.1109/ICASSP.2017.7952911
   Gao Chao, 2017, JMLR
   Golbeck J, 2006, CONSUM COMM NETWORK, P282
   Guo G., 2015, AAAI
   Guo Guibing, 2015, UMAP WORKSHOPS, V4
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jahrer M, 2010, P 16 ACM SIGKDD INT, P693, DOI DOI 10.1145/1835804.1835893
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Jamali M, 2009, P 3 ACM C REC SYST, P181
   Jamali M, 2010, P 4 ACM C REC SYST, V2010, P135, DOI [DOI 10.1145/1864708.1864736, 10.1145/1864708.1864736]
   Jamali M, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397
   Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113
   Jog Varun, 2015, ARXIV150906418
   Kalofolias Vassilis, 2014, ARXIV14081717
   Keshavan R. H., 2010, IEEE T INFORM THEORY
   Koren Y., 2008, SIGKDD
   Krzakala F., 2013, PNAS
   Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274
   LI WJ, 2009, IJCAI 09, P1126
   Ma H., 2011, P 4 ACM INT C WEB SE, P287, DOI DOI 10.1145/1935826.1935877
   Ma H., 2008, P 17 ACM C INF KNOWL, P931, DOI DOI 10.1145/1458082.1458205
   Ma H, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P203, DOI 10.1145/1571941.1571978
   Massa P., 2005, AAAI, V1, P121
   Mazumdar Arya, 2017, ADV NEURAL INFORM PR, P4685
   McPherson M, 2001, ANNU REV SOCIOL, V27, P415, DOI 10.1146/annurev.soc.27.1.415
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Monti  F., 2017, ADV NEURAL INFORM PR, P3700
   Mossel Elchanan, 2015, ARXIV150903281, P7
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Rao N., 2015, ADV NEURAL INFORM PR, P2107
   Rennie J. D. M., 2005, P 22 INT C MACH LEAR, P713, DOI [10.1145/1102351.1102441, DOI 10.1145/1102351.1102441]
   Rui Wu, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P449, DOI 10.1145/2745844.2745887
   Saad Hussein, 2018, IEEE J SELECTED TOPI
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.1145/1390156.1390267
   Shah Devavrat, 2017, ARXIV170308085
   Si Luo, 2003, ICML, V3, P704
   Traud AL, 2012, PHYSICA A, V391, P4165, DOI 10.1016/j.physa.2011.12.021
   van den Berg Rianne, 2017, STAT, V1050, P7
   Yang X., 2012, P 6 ACM C REC SYST, P67, DOI DOI 10.1145/2365952.2365969]
   Yang XW, 2013, IEEE T PARALL DISTR, V24, P642, DOI 10.1109/TPDS.2012.192
   Yi X., 2016, P ADV NEUR INF PROC, V29, P4152
   Yun Se-Young, 2014, ARXIV14127335
   Zhang AY, 2016, ANN STAT, V44, P2252, DOI 10.1214/15-AOS1428
   Zhang Yuan, 2015, ARXIV150901173
   Zhao H, 2017, IEEE DATA MINING, P645, DOI 10.1109/ICDM.2017.74
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304030
DA 2019-06-15
ER

PT S
AU Alemi, AA
   Fischer, I
AF Alemi, Alexander A.
   Fischer, Ian
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI GILBO: One Metric to Measure Them All
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both vAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIsT, FashionmNIST, ciEAR-10 and CelebA) and discuss the results.
C1 [Alemi, Alexander A.; Fischer, Ian] Google AI, San Jose, CA 95136 USA.
RP Alemi, AA (reprint author), Google AI, San Jose, CA 95136 USA.
EM alemi@google.com; iansf@google.com
CR Agakov Felix Vsevolodovich, 2006, THESIS
   Alemi Alex, 2017, ICML
   Arora Sanjeev, 2017, CORR
   Belghazi Mohamed Ishmael, 2018, ICML, P530
   Chen Xi, 2016, NIPS
   Danihelka I., 2017, ARXIV170505263
   Gao Weihao, 2017, NEURAL INFORM PROCES
   Goodfellow I., 2014, NEURAL INFORM PROCES
   Heusel M., 2017, ARXIV180608500
   Im Daniel Jiwoong, 2018, INT C LEARN REPR
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   LANDAUER TK, 1986, COGNITIVE SCI, V10, P477, DOI 10.1016/S0364-0213(86)80014-3
   Lipton Z. C., 2017, PRECISE RECOVERY LAT
   Lucic M, 2017, ARXIV171110337
   Marsh C., 2013, INTRO CONTINUOUS ENT
   Talts S., 2018, ARXIV180406788
   Tishby N., 2015, ARXIV150302406
   van den Oord Aaron, 2018, ARXIV180703748
   Wu Y., 2017, INT C LEARN REPR
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001057
DA 2019-06-15
ER

PT S
AU Alistarh, D
   Hoefler, T
   Johansson, M
   Khirirat, S
   Konstantinov, N
   Renggli, C
AF Alistarh, Dan
   Hoefler, Torsten
   Johansson, Mikael
   Khirirat, Sarit
   Konstantinov, Nikola
   Renggli, Cedric
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Convergence of Sparsified Gradient Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DESCENT
AB Stochastic Gradient Descent (SGD) has become the standard tool for distributed training of massive machine learning models, in particular deep neural networks. Several families of communication-reduction methods, such as quantization, large-batch methods, and gradient sparsification, have been proposed to reduce the overheads of distribution. To date, gradient sparsification methods-where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally-are known to yield some of the largest practical gains. Such methods can reduce the amount of communication per step by up to three orders of magnitude, while preserving model accuracy. Yet, this family of methods currently has no theoretical justification.
   This is the question we address in this paper. We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD. The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude. Our analysis also reveals that these methods do require analytical conditions to converge well, justifying and complementing existing heuristics.
C1 [Alistarh, Dan; Konstantinov, Nikola] IST Austria, Klosterneuburg, Austria.
   [Hoefler, Torsten; Renggli, Cedric] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Johansson, Mikael; Khirirat, Sarit] KTH, Stockholm, Sweden.
RP Alistarh, D (reprint author), IST Austria, Klosterneuburg, Austria.
EM dan.alistarh@ist.ac.at; htor@inf.ethz.ch; mikaelj@kth.se; sarit@kth.se;
   nikola.konstantinov@ist.ac.at; cedric.renggli@inf.ethz.ch
FU European Union's Horizon 2020 research and innovation programme under
   the Marie Sklodowska-Curie Grant [665385]
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under the Marie Sklodowska-Curie Grant
   Agreement No. 665385.
CR Abadi M., 2016, P 12 USENIX S OP SYS, V16, P265, DOI DOI 10.1038/NN.3331
   Aji A. F., 2017, ARXIV170405021
   Alistarh  Dan, 2017, P NIPS 2017
   Alistarh  Dan, 2018, ARXIV180910505
   Alistarh  Dan, 2018, ARXIV180308841
   Chen Tianqi, 2015, ARXIV151201274
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   De Sa  Christopher, 2015, NIPS
   Dryden N, 2016, PROCEEDINGS OF 2016 2ND WORKSHOP ON MACHINE LEARNING IN HPC ENVIRONMENTS (MLHPC), P1, DOI [10.1109/MLHPC.2016.4, 10.1109/MLHPC.2016.004]
   Duchi John C, 2015, ARXIV150800882
   Goyal Priya, 2017, ARXIV170602677
   Grubic  Demjan, 2018, EDBT, P145
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lian X., 2015, ADV NEURAL INFORM PR, P2737
   Lin Yujun, 2017, ARXIV171201887
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Rastegari Mohammad, 2016, EUR C COMP VIS
   Renggli Cedric, 2018, ARXIV180208021
   Seide F, 2014, INTERSPEECH, P1058
   Seide F, 2014, INT CONF ACOUST SPEE
   Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687
   Strom N., 2015, 16 ANN C INT SPEECH
   Sun  Xu, 2017, ARXIV170606197
   Szegedy C., 2017, AAAI, V4, P4278
   Wangni Jianqiao, 2017, ARXIV171009854
   Wen W., 2017, ADV NEURAL INFORM PR, P1508
   Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014
   Yang Y., 2017, ARXIV170803888
   Yu  Dong, 2014, MSRTR2014112
   Zhang  Jian, 2017, ARXIV170603471
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000047
DA 2019-06-15
ER

PT S
AU Alistarh, D
   Allen-Zhu, Z
   Li, J
AF Alistarh, Dan
   Allen-Zhu, Zeyuan
   Li, Jerry
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Byzantine Stochastic Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of m machines which allegedly compute stochastic gradients every iteration, an alpha-fraction are Byzantine, and may behave adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds epsilon-approximate minimizers of convex functions in T = (O) over tilde (1/epsilon(2)m + alpha(2)/epsilon(2) ) iterations. In contrast, traditional mini-batch SGD needs T = O (1/epsilon(2)m) iterations, but cannot tolerate Byzantine failures. Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sample complexity and time complexity.
C1 [Alistarh, Dan] IST Austria, Klosterneuburg, Austria.
   [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA USA.
   [Li, Jerry] Simons Inst, Berkeley, CA USA.
RP Alistarh, D (reprint author), IST Austria, Klosterneuburg, Austria.
EM dan.alistarh@ist.ac.at; zeyuan@csail.mit.edu; jerryzli@berkeley.edu
FU NSF CAREER Award [CCF-1453261, CCF-1565235]; Google Faculty Research
   Award; NSF Graduate Research Fellowship
FX We would like to thank Yuval Peres for suggesting reference [23]. Jerry
   Li is supported by NSF CAREER Award CCF-1453261, CCF-1565235, a Google
   Faculty Research Award, and an NSF Graduate Research Fellowship.
CR BALAKRISHNAN S., 2017, P 30 C LEARN THEOR C, P169
   Bekkerman R., 2011, SCALING MACHINE LEAR
   Ben-Tal A, 2001, SOC IND APPL MATH, DOI DOI 10.1137/1.9780898718829
   Bhatia K., 2015, ADV NEURAL INFORM PR, P721
   Bhatia K., 2017, ADV NEURAL INFORM PR, P2107
   Blanchard Peva, 2017, ADV NEURAL INFORM PR, P118
   Charikar M, 2017, ACM S THEORY COMPUT, P47, DOI 10.1145/3055399.3055491
   Chen Yudong, 2017, ARXIV170505491
   Diakonikolas I., 2018, ARXIV180302815
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P655, DOI 10.1109/FOCS.2016.85
   Diakonikolas Ilias, 2018, P 29 ANN ACM SIAM S, P2683, DOI DOI 10.1137/1.9781611975031.171
   Feldman P., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P148, DOI 10.1145/62212.62225
   Feng Jiashi, 2014, ARXIV14095937
   Hazan E, 2014, J MACH LEARN RES, V15, P2489
   Huber P. J., 2009, ROBUST STAT
   Klivans A., 2018, ARXIV180303241
   Konecny J., 2016, ARXIV161005492
   Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76
   LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176
   Nasrabadi N. M., 2011, ADV NEURAL INFORM PR
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2017, DOI 10.1109/TIT.2013.2240435
   Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI 10.1214/aop/1176988477
   Prasad A., 2018, ARXIV180206485
   Rakhlin Alexander, 2012, ICML
   Su LL, 2016, PROCEEDINGS OF THE 2016 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC'16), P425, DOI 10.1145/2933057.2933105
   Su Lili, 2016, ISDC
   Tukey J. W., 1975, P ICM, V6, P523
   Woodworth Blake, 2016, NIPS
   Xie Cong, 2018, ARXIV180210116
   Yin Dong, 2018, ARXIV180301498
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304061
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
AF Allen-Zhu, Zeyuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Natasha 2: Faster Non-Convex Optimization Than SGD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID CUBIC REGULARIZATION
AB We design a stochastic algorithm to find epsilon-approximate local minima of any smooth nonconvex function in rate O(epsilon(-3.25)), with only oracle access to stochastic gradients. The best result before this work was O (epsilon(-4) by stochastic gradient descent (SGD).(2)
C1 [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA.
RP Allen-Zhu, Z (reprint author), Microsoft Res AI, Redmond, WA 98052 USA.
EM zeyuan@csail.mit.edu
CR Agarwal Naman, 2017, STOC
   Allen-Zhu  Zeyuan, 2018, NEURIPS
   Allen-Zhu  Zeyuan, 2016, ICML
   Allen-Zhu  Zeyuan, 2017, STOC
   Allen-Zhu  Zeyuan, 2018, ICML
   Allen-Zhu  Zeyuan, 2016, NEURIPS
   Allen-Zhu  Zeyuan, 2017, ICML
   Allen-Zhu  Zeyuan, 2017, P 8 INN THEOR COMP S
   Allen-Zhu  Zeyuan, 2017, FOCS
   Arora S., 2015, COLT
   Carmon Yair, 2017, ICML
   Carmon Yair, 2016, ABS161100756 ARXIV
   Chen Y., 2015, ADV NEURAL INFORM PR, V2, P739
   Choromanska Anna, 2015, AISTATS
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Defazio Aaron, 2014, NEURIPS
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Frostig  Roy, 2015, ICML
   Garber Dan, 2016, ICML
   Ge Rong, 2015, P 28 ANN C LEARN THE
   Ghadimi  Saeed, 2015, MATH PROGRAM, P1
   Goodfellow I. J., 2014, QUALITATIVELY CHARAC
   Hazan Elad, 2016, P 33 INT C MACH LEAR, P1833
   He  Xi, 2016, ABS160600511 ARXIV
   Jin Chi, 2017, ICML
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kingma  Diederik, 2014, ABS14126980 ARXIV
   Lei Lihua, 2017, NEURIPS
   Li  Yuanzhi, 2017, NEURIPS
   Lin  Hongzhou, 2015, NEURIPS
   Nesterov Y.E., 2012, OPTIMA, V88, P10
   Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Reddi S. J., 2016, ICML
   Reddi Sashank J, 2017, ABS170901434 ARXIV
   Schmidt Mark, 2013, ABS13092388 ARXIV
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shalev- Shwartz Shai, 2016, ICML
   Sun R., 2015, FOCS
   Tripuraneni  Nilesh, 2017, ABS171102838 ARXIV
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xu Yi, 2017, ABS171101944 ARXIV
   Zeiler Matthew D, 2012, ABS12125701 ARXIV
   Zhang L., 2013, ADV NEURAL INFORM PR, P980
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302067
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
   Li, YZ
AF Allen-Zhu, Zeyuan
   Li, Yuanzhi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI NEON2: Finding Local Minima via First-Order Oracles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one, and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings, without hurting the algorithm's performance.
   As applications, our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD, GD, SCSG, and SVRG into algorithms finding approximate local minima, outperforming some best known results.
C1 [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA.
   [Li, Yuanzhi] Stanford Univ, Stanford, CA 94305 USA.
RP Allen-Zhu, Z (reprint author), Microsoft Res AI, Redmond, WA 98052 USA.
EM zeyuan@csail.mit.edu; yuanzhil@stanford.edu
CR Agarwal Naman, 2017, STOC
   Allen-Zhu Z., 2016, ICML
   Allen- Zhu Zeyuan, 2018, ICML
   Allen- Zhu Zeyuan, 2016, NEURIPS
   Allen- Zhu Zeyuan, 2017, ICML
   Allen- Zhu Zeyuan, 2018, NEURIPS
   Carmon Yair, 2017, ICML
   Carmon Yair, 2016, ABS161100756 ARXIV
   Choromanska Anna, 2015, AISTATS
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Defazio Aaron, 2014, NEURIPS
   Garber Dan, 2016, ICML
   Ge Rong, 2015, P 28 ANN C LEARN THE
   Goodfellow I. J., 2014, ARXIV E PRINTS
   Jin Chi, 2017, ICML
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lanczos Cornelius, 1950, J RES NBS, V45
   Lei Lihua, 2017, NEURIPS
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Reddi S. J., 2016, ICML
   Reddi Sashank J, 2017, ABS170901434 ARXIV
   Saad Y., 1992, NUMERICAL METHODS LA
   Schmidt Mark, 2013, ABS13092388 ARXIV
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Shalev- Shwartz Shai, 2016, ICML
   Trefethen LN, 2013, APPR THEOR APPR
   Xu Yi, 2017, ABS171101944 ARXIV
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303069
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
AF Allen-Zhu, Zeyuan
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI How To Make the Gradients Small Stochastically: Even Faster Convex and
   Nonconvex SGD
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MINIMIZATION; ALGORITHMS
AB Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives f(x). However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when f(x) is convex.
   If f(x) is convex, to find a point with gradient norm epsilon, we design an algorithm SGD3 with a near-optimal rate (O) over tilde(epsilon(-2)), improving the best known rate O(epsilon(-8/3)) of [17]. If f(x) is nonconvex, to find its epsilon-approximate local minimum, we de- sign an algorithm SGD5 with rate (O) over tilde(epsilon(-3.5)), where previously SGD variants only achieve (O) over tilde(epsilon(-4)) [6, 14, 30]. This is no slower than the best known stochastic version of Newton's method in all parameter regimes [27].
C1 [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA.
RP Allen-Zhu, Z (reprint author), Microsoft Res AI, Redmond, WA 98052 USA.
EM zeyuan@csail.mit.edu
CR Allen-Zhu Z., 2017, FOCS
   Allen-Zhu Z., 2017, STOC
   Allen- Zhu Zeyuan, 2016, NEURIPS
   Allen- Zhu Zeyuan, 2017, ICML
   Allen- Zhu Zeyuan, 2018, NEURIPS
   [Anonymous], 2017, OP PROBL SESS FAST I
   Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Carmon Yair, 2016, ABS161100756 ARXIV
   Cohen MB, 2017, ANN IEEE SYMP FOUND, P902, DOI 10.1109/FOCS.2017.88
   Davis Damek, 2018, ABS180202988 ARXIV
   Davis Damek, 2018, ABS180208556 ARXIV
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Ge Rong, 2015, P 28 ANN C LEARN THE
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Ghadimi S, 2012, SIAM J OPTIMIZ, V22, P1469, DOI 10.1137/110848864
   Ghadimi  Saeed, 2015, MATH PROGRAM, P1
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2014, J MACH LEARN RES, V15, P2489
   Idel Martin, 2016, ABS160906349 ARXIV
   Lei Lihua, 2017, NEURIPS
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y.E., 2012, OPTIMA, V88, P10
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Tripuraneni  Nilesh, 2017, ABS171102838 ARXIV
   Woodworth Blake, 2016, NEURIPS
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Yi Xu, 2017, ABS171101944 ARXIV
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301017
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
   Simchi-Levi, D
   Wang, XS
AF Allen-Zhu, Zeyuan
   Simchi-Levi, David
   Wang, Xinshang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The Lingering of Gradients: How to Reuse Gradients over Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NETWORK REVENUE MANAGEMENT
AB Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the "lingering" of gradients: once a gradient is computed at x(k), the additional time to compute gradients at x(k+1), x(k+2), ... may be reduced. We show how this improves the running time of gradient descent and SVRG. For instance, if the "additional time" scales linearly with respect to the traveled distance, then the "convergence rate" of gradient descent can be improved from 1/T to exp(-T-1/3). On the empirical side, we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to 10(-6) error (or 10(-12) dual error) using 6 passes of the dataset.
C1 [Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA.
   [Simchi-Levi, David; Wang, Xinshang] MIT, Cambridge, MA 02139 USA.
RP Allen-Zhu, Z (reprint author), Microsoft Res AI, Redmond, WA 98052 USA.
EM zeyuan@csail.mit.edu; dslevi@mit.edu; xinshang@mit.edu
CR Agrawal S, 2014, OPER RES, V62, P876, DOI 10.1287/opre.2014.1289
   Agrawal Shipra, 2015, P 26 ANN ACM SIAM S, P1405, DOI DOI 10.1137/1.9781611973730.93
   Alaei S., 2012, P 13 ACM C EL COMM, P18
   Allen-Zhu Z., 2016, ICML
   Allen- Zhu Zeyuan, 2016, NEURIPS
   Babanezhad  R., 2015, ADV NEURAL INFORM PR, P2251
   Chu W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1097
   Ciocan DF, 2012, MATH OPER RES, V37, P501, DOI 10.1287/moor.1120.0548
   Defazio Aaron, 2014, NEURIPS
   Devanur N. R., 2012, P ACM C EL COMM, P388
   Devanur NR, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P71
   Feldman J, 2010, LECT NOTES COMPUT SC, V6346, P182, DOI 10.1007/978-3-642-15775-2_16
   Feldman J, 2009, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2009.72
   Haeupler Bernhard, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P170, DOI 10.1007/978-3-642-25510-6_15
   Hofmann Thomas, 2015, ADV NEURAL INFORM PR, P2296
   Jaillet P, 2014, MATH OPER RES, V39, P624, DOI 10.1287/moor.2013.0621
   Jasin S, 2012, MATH OPER RES, V37, P313, DOI 10.1287/moor.1120.0537
   Johnson Ferreira Kris, 2016, ONLINE NETWORK UNPUB
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lei Lihua, 2017, NEURIPS
   Lei Lihua, 2017, ARTIF INTELL, P148
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Mahdavi  M., 2013, ADV NEURAL INFORM PR, V116, P674
   Manshadi VH, 2012, MATH OPER RES, V37, P559, DOI 10.1287/moor.1120.0551
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   Reiman MI, 2008, MATH OPER RES, V33, P257, DOI 10.1287/moor.1070.0288
   Schmidt Mark, 2013, ARXIV E PRINTS
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev- Shwartz Shai, 2016, ICML
   Shalev-Shwartz Shai, 2012, ARXIV E PRINTS
   Stein Clifford, 2016, ARXIV E PRINTS
   Talluri K, 1998, MANAGE SCI, V44, P1577, DOI 10.1287/mnsc.44.11.1577
   Wang Xinshang, 2016, WORKING PAPER
   Wang Xinshang, 2015, ARXIV E PRINTS
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang L., 2013, ADV NEURAL INFORM PR, P980
   Zhong W., 2015, P 21 ACM SIGKDD INT, P2287
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301025
DA 2019-06-15
ER

PT S
AU Almanza, M
   Chierichetti, F
   Panconesi, A
   Vattani, A
AF Almanza, Matteo
   Chierichetti, Flavio
   Panconesi, Alessandro
   Vattani, Andrea
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI A Reduction for Efficient LDA Topic Reconstruction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from the same set of topics but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions - the only ones we can hope to compute in practice - are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single-topic world - a much simpler task than direct LDA reconstruction. We show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature, p-separability and matrix-like topics.
C1 [Almanza, Matteo; Chierichetti, Flavio; Panconesi, Alessandro] Sapienza Univ, Rome, Italy.
   [Vattani, Andrea] Spiketrap, San Francisco, CA USA.
RP Almanza, M (reprint author), Sapienza Univ, Rome, Italy.
EM almanza@di.uniroma1.it; flavio@di.uniroma1.it; ale@di.uniroma1.it;
   avattani@cs.ucsd.edu
FU ERC [DMAP 680153]; "Dipartimenti di Eccellenza 2018-2022" grant; Google
   Focused Research Award; BiCi - Bertinoro international Center for
   informatics
FX Supported in part by the ERC Starting Grant DMAP 680153, and by the
   "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento
   di Informatica at Sapienza.; Supported in part by the ERC Starting Grant
   DMAP 680153, by a Google Focused Research Award, and by the
   "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Dipartimento
   di Informatica at Sapienza.; Supported in part by the ERC Starting Grant
   DMAP 680153, by a Google Focused Research Award, by the "Dipartimenti di
   Eccellenza 2018-2022" grant awarded to the Dipartimento di Informatica
   at Sapienza, and by BiCi - Bertinoro international Center for
   informatics.
CR Alvarez-Melis David, 2016, ICWSM
   Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora S, 2012, ANN IEEE SYMP FOUND, P1, DOI 10.1109/FOCS.2012.49
   Arora Sanjeev, 2013, JMLR WORKSHOP C P, V28, P280
   Bansal T., 2014, ADV NEURAL INFORM PR, P1997
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chierichetti Flavio, 2018, ZENODO, DOI [10.5281/zenodo.1470295, DOI 10.5281/ZENODO.1470295]
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hajjem M, 2017, PROCEDIA COMPUT SCI, V112, P761, DOI 10.1016/j.procs.2017.08.166
   Hong L., 2010, P 1 WORKSH SOC MED A, P80, DOI DOI 10.1145/1964858.1964870
   Li CL, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P165, DOI 10.1145/2911451.2911499
   McCallum AK, 2002, MACHINE LEARNING LAN
   Newman David, 2008, NIPS DATASET
   Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085
   Sridhar V K R, 2015, P NAACL HLT, P192
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI DOI 10.1145/1718487.1718520
   Yan Xiaohui, 2013, WWW
   Yau Chyi-Kwei, 2018, TENSOR LDA
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002042
DA 2019-06-15
ER

PT S
AU Alvarez-Melis, D
   Jaakkola, TS
AF Alvarez-Melis, David
   Jaakkola, Tommi S.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Towards Robust Interpretability with Self-Explaining Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general - explicitness, faithfulness, and stability - and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.
C1 [Alvarez-Melis, David; Jaakkola, Tommi S.] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Alvarez-Melis, D (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM dalvmel@mit.edu; tommi@csail.mit.edu
FU MIT-IBM grant on deep rationalization; Hewlett Packard; CONACYT
FX The authors would like to thank the anonymous reviewers and Been Kim for
   helpful comments. The work was partially supported by an MIT-IBM grant
   on deep rationalization and by Graduate Fellowships from Hewlett Packard
   and CONACYT.
CR Al- Shedivat M., 2017, ARXIV170510301
   Alvarez-Melis D., 2018, P 2018 ICML WORKSH H
   Alvarez-Melis D., 2017, P 2017 C EMP METH NA, P412
   Arras L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181142
   Bach S., 2015, PLOS ONE, V10.7
   Doshi-Velez F., 2017, ARXIV170208608V2, P1
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Grgic-Hlaca N., 2018, AAAI C ART INT
   Kim B., 2018, INT C MACH LEARN ICM
   Kindermans P. - J., 2017, NIPS WORKSH EXPL VIS
   Krizhevsky A., 2009, TECH REP
   Lei T., 2016, P 2016 C EMP METH NA, P107, DOI DOI 10.18653/V1/D16-1011
   Li O., 2018, AAAI C ART INT
   Lichman M., 2013, UCI MACHINE LEARNING
   Lundberg S. M., 2017, ADV NEURAL INFORM PR, P4768
   Montavon G., 2017, DIGITAL SIGNAL PROCE
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
   Selvaraju R. R., 2017, ICCV
   Shrikumar A, 2017, PMLR, V70, P3145
   Simonyan K., 2014, INT C LEARN REPR WOR
   Snoek J, 2012, ADV NEURAL INFORM PR
   Sundararajan M, 2017, ARXIV170301365
   Yosinski J., 2015, ARXIV150606579
   Zafar M. B., 2017, ADV NEURAL INFORM PR, P228
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002033
DA 2019-06-15
ER

PT S
AU Ambrogioni, L
   Guclu, U
   Gucluturk, Y
   Hinne, M
   Maris, E
   van Gerven, MAJ
AF Ambrogioni, Luca
   Guclu, Umut
   Gucluturk, Yagmur
   Hinne, Max
   Maris, Eric
   van Gerven, Marcel A. J.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Wasserstein Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.
C1 [Ambrogioni, Luca; Guclu, Umut; Gucluturk, Yagmur; Maris, Eric; van Gerven, Marcel A. J.] Radboud Univ Nijmegen, Nijmegen, Netherlands.
   [Hinne, Max] Univ Amsterdam, Amsterdam, Netherlands.
RP Ambrogioni, L (reprint author), Radboud Univ Nijmegen, Nijmegen, Netherlands.
EM l.ambrogioni@donders.ru.nl; u.guclu@donders.ru.nl;
   y.gucluturk@donders.ru.nl; m.hinne@uva.nl; e.maris@donders.ru.nl;
   m.vangerven@donders.ru.nl
CR Ambrogioni  L., 2018, ARXIV180511542
   Arjovsky  M., 2017, INT C LEARN REPR
   Arjovsky Martin, 2017, INT C MACH LEARN
   Bamler  R., 2017, ADV NEURAL INFORM PR, P5086
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Burago  D., 2001, GRADUATE STUDIEMAT, V33
   Cuturi M, 2013, ADV NEURAL INFORM PR
   Cuturi M, 2018, ARXIV180300567
   Dieng A. B., 2017, ADV NEURAL INFORM PR
   Dumoulin V., 2017, INT C LEARN REPR
   Fouskakis D, 2002, INT STAT REV, V70, P315, DOI 10.2307/1403861
   Genevay A., 2018, INT C ART INT STAT, P1608
   Gulrajani Ishaan, 2017, ADV NEURAL INFORM PR
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Huszar F., 2017, ARXIV170208235
   Kingma D.P., 2013, ARXIV13126114
   Kucukelbir A, 2017, J MACH LEARN RES, V18, P1
   Li Y., 2016, ADV NEURAL INFORM PR
   Makhzani A., 2015, ARXIV151105644
   Mescheder L., 2017, ARXIV170104722
   Montavon  G., 2016, ADV NEURAL INFORM PR
   Pu Yunchen, 2016, ADV NEURAL INFORM PR
   Ranganath  R., 2016, ADV NEURAL INFORM PR
   Ranganath  R., 2014, INT C ART INT STAT
   Rezende D. J., 2014, INT C MACH LEARN
   SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343
   Tolstikhin  I., 2018, INT C LEARN REPR
   Tran D., 2016, ARXIV161009787
   Tran  Dustin, 2017, ARXIV170208896
   Villani C., 2003, TOPICS OPTIMAL TRANS
   Weed J., 2017, ARXIV170700087
   Zhang Cheng, 2017, ARXIV171105597
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302048
DA 2019-06-15
ER

PT S
AU Amos, B
   Rodriguez, IDJ
   Sacks, J
   Boots, B
   Kolter, JZ
AF Amos, Brandon
   Rodriguez, Ivan Dario Jimenez
   Sacks, Jacob
   Boots, Byron
   Kolter, J. Zico
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentiable MPC for End-to-end Planning and Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PATH-INTEGRAL CONTROL
AB We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning in continuous state and action spaces. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.
C1 [Amos, Brandon; Kolter, J. Zico] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Rodriguez, Ivan Dario Jimenez; Sacks, Jacob; Boots, Byron] Georgia Tech, Atlanta, GA USA.
   [Kolter, J. Zico] Bosch Ctr AI, Renningen, Germany.
RP Amos, B (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
FU National Science Foundation Graduate Research Fellowship Program
   [DGE1252522]
FX BA is supported by the National Science Foundation Graduate Research
   Fellowship Program under Grant No. DGE1252522. We thank Alfredo
   Canziani, Shane Gu, and Yuval Tassa for insightful discussions.
CR Abbeel P., 2006, P 23 INT C MACH LEAR, V3, P1
   Alexis Kostas, 2011, 2011 19th Mediterranean Conference on Control & Automation (MED 2011), P1247
   Amos Brandon, 2017, P INT C MACH LEARN
   Bansal Somil, 2017, ARXIV170903153
   Boedecker Joschika, 2014, IEEE S AD DYN PROGR
   Bouffard P, 2012, IEEE INT CONF ROBOT, P279, DOI 10.1109/ICRA.2012.6225035
   Boyd Stephen, 2008, STANFORD EE 363 LINE
   Chebotar Yevgen, 2017, ARXIV170303078
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Erez T., 2012, INT C INT ROB SYST
   Farquhar Gregory, 2017, ARXIV171011417
   Gonzalez R, 2011, ROBOT AUTON SYST, V59, P711, DOI 10.1016/j.robot.2011.05.006
   Gu S., 2016, P INT C MACH LEARN
   Gu Shixiang, 2016, ARXIV161102247
   Heess N., 2015, ADV NEURAL INFORM PR, P2944
   Kamel M, 2015, IEEE INTL CONF CONTR, P1160, DOI 10.1109/CCA.2015.7320769
   Karkus Peter, 2017, ADV NEURAL INFORM PR, P4697
   Kingma D. P., 2014, ARXIV14126980
   Lenz  I., 2015, ROBOTICS SCI SYSTEMS
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Levine S, 2016, J MACH LEARN RES, V17
   Levine Sergey, 2017, 294112 BERK CS
   Li Weiwei, 2004, ITERATIVE LINEAR QUA
   Lillicrap T P, 2015, ARXIV150902971
   Liniger Alexander, 2014, OPTIMAL CONTROL APPL, P628
   Mnih V., 2013, ARXIV13125602
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nagabandi  A., 2017, ARXIV170802596
   Neunert Michael, 2016, ICRA
   Oh J., 2017, ADV NEURAL INFORM PR, P6120
   Oh Junhyuk, 2016, P 33 INT C MACH LEAR
   Okada M., 2017, ARXIV170609597
   Pascanu  R., 2017, ARXIV170706170
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pathak Deepak, 2018, ARXIV180408606
   Pereira Marcus, 2018, ARXIV180205803
   Pong  V., 2018, ARXIV180209081
   Schneider JG, 1997, ADV NEUR IN, V9, P1047
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman John, 2016, INT C LEARN REPR
   Silver D., 2016, ARXIV161208810
   Srinivas  A., 2018, ARXIV180400645
   Sun Liting, 2017, ARXIV170702515
   Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216
   Tamar Aviv, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P336, DOI 10.1109/ICRA.2017.7989043
   Tamar A., 2016, ADV NEURAL INFORM PR, V29, P2154
   Tassa Y, 2014, IEEE INT CONF ROBOT, P1168, DOI 10.1109/ICRA.2014.6907001
   Theodorou EA, 2010, J MACH LEARN RES, V11, P3137
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   Venkatraman Arun, 2016, INT S EXP ROB, P703
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Weber T., 2017, ARXIV170706203
   Williams G, 2017, J GUID CONTROL DYNAM, V40, P344, DOI 10.2514/1.G001921
   Williams G, 2016, IEEE INT CONF ROBOT, P1433, DOI 10.1109/ICRA.2016.7487277
   Xie Zhaoming, 2017, INT C ROB AUT ICRA
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002080
DA 2019-06-15
ER

PT S
AU Anand, N
   Huang, PS
AF Anand, Namrata
   Huang, Po-Ssu
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Generative Modeling for Protein Structures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID COMPUTATIONAL DESIGN; OPTIMIZATION; AFFINITY
AB Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.
C1 [Anand, Namrata; Huang, Po-Ssu] Bioengn Dept, Stanford, CA USA.
RP Anand, N (reprint author), Bioengn Dept, Stanford, CA USA.
EM namrataa@stanford.edu; possu@stanford.edu
CR Alipanahi B, 2013, J COMPUT BIOL, V20, P296, DOI 10.1089/cmb.2012.0089
   AlQuraishi Mohammed, 2018, BIORXIV
   Arjovsky M, 2017, ARXIV170107875
   Berman J.H.M., 2000, NUCLEIC ACIDS RES, V106, P16972
   Boomsma W, 2008, P NATL ACAD SCI USA, V105, P8932, DOI 10.1073/pnas.0801715105
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Boyd S., 2004, CONVEX OPTIMIZATION
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Canutescu AA, 2003, PROTEIN SCI, V12, P963, DOI 10.1110/ps.0242703
   Das R, 2008, ANNU REV BIOCHEM, V77, P363, DOI 10.1146/annurev.biochem.77.062906.171838
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   ENGH RA, 1991, ACTA CRYSTALLOGR A, V47, P392, DOI 10.1107/S0108767391001071
   Gomez-Bombarelli Rafael, 2016, ACS CENTRAL SCI
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, ADV NEURAL INFORM PR, P5769
   Hamelryck T, 2006, PLOS COMPUT BIOL, V2, P1121, DOI 10.1371/journal.pcbi.0020131
   Hopf TA, 2014, ELIFE, V3, DOI 10.7554/eLife.03430
   Huang PS, 2016, NATURE, V537, P320, DOI 10.1038/nature19946
   Huang PS, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024109
   Jiang L, 2008, SCIENCE, V319, P1387, DOI 10.1126/science.1152692
   Joosten RP, 2011, NUCLEIC ACIDS RES, V39, pD411, DOI 10.1093/nar/gkq1105
   KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211
   Kamisetty H, 2013, P NATL ACAD SCI USA, V110, P15674, DOI 10.1073/pnas.1314045110
   Killoran Nathan, 2017, ARXIV171206148
   Kingma D. P., 2014, ARXIV14126980
   Kusner M. J, 2017, ARXIV170301925
   Mandell DJ, 2009, NAT METHODS, V6, P551, DOI 10.1038/nmeth0809-551
   Metz L., 2016, ARXIV161102163
   Mirza M., 2014, ARXIV14111784
   O'Donoghue B, 2017, SCS SPLITTING CONIC
   O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pratap JV, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2012.0369
   Radford  A., 2015, ARXIV151106434
   Rothlisberger D, 2008, NATURE, V453, P190, DOI 10.1038/nature06879
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Siegel JB, 2010, SCIENCE, V329, P309, DOI 10.1126/science.1190239
   Smart AD, 2017, P NATL ACAD SCI USA, V114, pE8174, DOI 10.1073/pnas.1705064114
   Strauch EM, 2017, NAT BIOTECHNOL, V35, P667, DOI 10.1038/nbt.3907
   Tinberg CE, 2013, NATURE, V501, P212, DOI 10.1038/nature12443
   Wang Jingxue, 2018, ARXIV180107130
   Whitehead TA, 2012, NAT BIOTECHNOL, V30, P543, DOI 10.1038/nbt.2214
   Wu J., 2016, ADV NEURAL INFORM PR, P82
   Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a
   Yeh R., 2016, ARXIV160707539
   Zheng F, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178272
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002008
DA 2019-06-15
ER

PT S
AU Anari, N
   Daskalakis, C
   Maass, W
   Papadimitriou, CH
   Saberi, A
   Vempala, S
AF Anari, Nima
   Daskalakis, Constantinos
   Maass, Wolfgang
   Papadimitriou, Christos H.
   Saberi, Amin
   Vempala, Santosh
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of
   Neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon Bhaskara et al. [3] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons.
   Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their l-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.
C1 [Anari, Nima] Stanford Univ, Comp Sci, Stanford, CA 94305 USA.
   [Daskalakis, Constantinos] MIT, EECS, Cambridge, MA 02139 USA.
   [Maass, Wolfgang] Graz Univ Technol, Theoret Comp Sci, Graz, Austria.
   [Papadimitriou, Christos H.] Columbia Univ, Comp Sci, New York, NY 10027 USA.
   [Saberi, Amin] Stanford Univ, MS&E, Stanford, CA 94305 USA.
   [Vempala, Santosh] Georgia Tech, Comp Sci, Atlanta, GA USA.
RP Anari, N (reprint author), Stanford Univ, Comp Sci, Stanford, CA 94305 USA.
EM anari@cs.stanford.edu; costis@csail.mit.edu; maass@igi.tugraz.at;
   christos@cs.columbia.edu; saberi@stanford.edu; vempala@gatech.edu
CR Anandkumar A., 2012, 25 ANN C LEARN THEOR, V23, P331
   Barak B., 2015, P 47 ANN ACM S THEOR, P143, DOI DOI 10.1145/2746539.2746605
   Bhaskara A., 2014, P 46 ANN ACM S THEOR, P594
   Buzsaki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2
   De Falco E, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13408
   De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943
   Ge Rong, 2017, ADV NEURAL INFORM PR, P3656
   Ge Rong, 2015, ARXIV150405287
   Goyal N., 2014, S THEOR COMP STOC 20, P584, DOI DOI 10.1145/2591796.2591875
   HASTAD J, 1990, J ALGORITHMS, V11, P644, DOI 10.1016/0196-6774(90)90014-6
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Hopkins SB, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P178, DOI 10.1145/2897518.2897529
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Ison MJ, 2015, NEURON, V87, P220, DOI 10.1016/j.neuron.2015.06.016
   Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482
   LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071
   Ma TY, 2016, ANN IEEE SYMP FOUND, P438, DOI 10.1109/FOCS.2016.54
   MOSSEL E., 2005, P 37 ANN ACM S THEOR, P366
   PITOWSKY I, 1991, MATH PROGRAM, V50, P395, DOI 10.1007/BF01594946
   Quiroga RQ, 2012, NAT REV NEUROSCI, V13, P587, DOI 10.1038/nrn3251
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005044
DA 2019-06-15
ER

PT S
AU Anderson, P
   Gould, S
   Johnson, M
AF Anderson, Peter
   Gould, Stephen
   Johnson, Mark
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Partially-Supervised Image Captioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild - for example, as assistants for people with impaired vision - a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partially specified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.
C1 [Anderson, Peter; Johnson, Mark] Macquarie Univ, Sydney, NSW, Australia.
   [Gould, Stephen] Australian Natl Univ, Canberra, ACT, Australia.
   [Anderson, Peter] Georgia Tech, Atlanta, GA 30332 USA.
RP Anderson, P (reprint author), Macquarie Univ, Sydney, NSW, Australia.; Anderson, P (reprint author), Georgia Tech, Atlanta, GA 30332 USA.
EM p.anderson@mq.edu.au; stephen.gould@anu.edu.au; mark.johnson@mq.edu.au
FU Google award through the Natural Language Understanding Focused Program;
   Australian Research Council [DP160102156]; Data61/CSIRO [CRP 8201800363]
FX This research was supported by a Google award through the Natural
   Language Understanding Focused Program, CRP 8201800363 from
   Data61/CSIRO, and under the Australian Research Council's Discovery
   Projects funding scheme (project number DP160102156). We also thank the
   anonymous reviewers for their valuable comments that helped to improve
   the paper.
CR Anderson P., 2018, CVPR
   Anderson P., 2016, ECCV
   Anderson Peter, 2017, EMNLP
   Bandanau D., 2015, ICLR
   Chen X, 2015, CORR, V1504, P325
   Dai A. M., 2015, NIPS
   Dempster Arthur P, 1977, J ROYAL STAT SOC B
   Devlin J., 2015, ARXIV150501809
   Donahue J., 2015, CVPR
   Ghahramani Zoubin, 1994, NIPS
   Graves A, 2013, ARXIV13080850
   Hao Fang, 2015, CVPR
   He K., 2016, CVPR
   Hendricks Lisa Anne, 2016, CVPR
   Hill Felix, 2016, ARXIV160203483
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hokamp Chris, 2017, ACL
   Karpathy A., 2015, CVPR
   Koehn P., 2010, STAT MACHINE TRANSLA
   Krasin I., 2017, OPENIMAGES PUBLIC DA
   Krishna R., 2016, ARXIV160207332
   Lavie Alon, 2007, P ANN M ASS COMP LIN
   Levy Omer, 2014, ACL
   Lin T.-Y., 2014, ECCV
   Lipton Z. C., 2016, MACHINE LEARNING HEA
   Lu Jiasen, 2017, CVPR
   Lu Jiasen, 2018, CVPR
   MacLeod Haley, 2017, P 2017 CHI C HUM FAC
   Mao J, 2015, ICLR
   McLachlan GJ, 2008, WILEY SER PROBAB ST, P365
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Papadopoulos Dim P, 2017, ICCV
   Papadopoulos Dim P, 2016, CVPR
   Parveen Shahla, 2002, NIPS
   Pennington Jeffrey, 2014, EMNLP
   Post Matt, 2018, ARXIV180406609
   Ren S., 2015, NIPS
   Rennie Steven J., 2017, CVPR
   Richardson Kyle, 2018, NAACL
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Sipser M., 2012, INTRO THEORY COMPUTA
   Sutskever  I., 2014, NIPS
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tran Kenneth, 2016, IEEE C COMP VIS PATT
   Vaswani A., 2017, NIPS
   Vedantam Ramakrishna, 2015, CVPR
   Venugopalan Subhashini, 2017, CVPR
   Vinyals O, 2015, CVPR
   Wiseman S., 2016, EMNLP
   Xian Yongqin, 2017, CVPR
   Xu K, 2015, ICML
   Yang Zhilin, 2016, NIPS
   Yao Ting, 2017, CVPR
   Young P., 2014, P TACL, V2, P67
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301083
DA 2019-06-15
ER

PT S
AU Angell, R
   Sheldon, D
AF Angell, Rico
   Sheldon, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Inferring Latent Velocities from Weather Radar Data using Gaussian
   Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID WSR-88D; MIGRATION
AB Archived data from the US network of weather radars hold detailed information about bird migration over the last 25 years, including very high-resolution partial measurements of velocity. Historically, most of this spatial resolution is discarded and velocities are summarized at a very small number of locations due to modeling and algorithmic limitations. This paper presents a Gaussian process (GP) model to reconstruct high-resolution full velocity fields across the entire US. The GP faithfully models all aspects of the problem in a single joint framework, including spatially random velocities, partial velocity measurements, station-specific geometries, measurement noise, and an ambiguity known as aliasing. We develop fast inference algorithms based on the FFT; to do so, we employ a creative use of Laplace's method to sidestep the fact that the kernel of the joint process is non-stationary.
C1 [Angell, Rico; Sheldon, Daniel] Univ Massachusetts, Amherst, MA 01003 USA.
RP Angell, R (reprint author), Univ Massachusetts, Amherst, MA 01003 USA.
EM rangell@cs.umass.edu; sheldon@cs.umass.edu
FU National Science Foundation [1522054, 1661259]
FX This material is based upon work supported by the National Science
   Foundation under Grant Nos. 1522054 and 1661259.
CR Agiomyrgiannakis Y, 2009, IEEE T AUDIO SPEECH, V17, P775, DOI 10.1109/TASL.2008.2008229
   Bahlmann C, 2006, PATTERN RECOGN, V39, P115, DOI 10.1016/j.patcog.2005.05.012
   Bergen W. R., 1988, Journal of Atmospheric and Oceanic Technology, V5, P305, DOI 10.1175/1520-0426(1988)005<0305:TATDDA>2.0.CO;2
   BREITENBERGER E, 1963, BIOMETRIKA, V50, P81
   Buler JJ, 2009, IEEE T GEOSCI REMOTE, V47, P2741, DOI 10.1109/TGRS.2009.2014463
   CRUM TD, 1993, B AM METEOROL SOC, V74, P1669, DOI 10.1175/1520-0477(1993)074<1669:TWATWO>2.0.CO;2
   Dokter Adriaan M., 2010, J ROYAL SOC INTERFAC
   Doviak R., 1993, DOPPLER RADAR WEATHE
   Farnsworth A, 2016, ECOL APPL, V26, P752, DOI 10.1890/15-0023
   Fulton RA, 1998, WEATHER FORECAST, V13, P377, DOI 10.1175/1520-0434(1998)013<0377:TWRA>2.0.CO;2
   Gao JD, 2004, J APPL METEOROL, V43, P934, DOI 10.1175/1520-0450(2004)043<0934:AVTFDD>2.0.CO;2
   Horton KG, 2016, SCI REP-UK, V6, DOI 10.1038/srep21249
   Horton Kyle G, ECOLOGY LETT
   Insanic E, 2012, IEEE T GEOSCI REMOTE, V50, P553, DOI 10.1109/TGRS.2011.2161766
   Johnson JT, 1998, WEATHER FORECAST, V13, P263, DOI 10.1175/1520-0434(1998)013<0263:TSCIAT>2.0.CO;2
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kunz TH, 2008, INTEGR COMP BIOL, V48, P1, DOI 10.1093/icb/icn037
   La Sorte F. A., 2015, ROYAL SOC OPEN SCI, V2, P1
   La Sorte FA, 2015, J ANIM ECOL, V84, P1202, DOI 10.1111/1365-2656.12376
   OHSMANN M, 1995, LINEAR ALGEBRA APPL, V231, P181, DOI 10.1016/0024-3795(95)90020-9
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   RAY PS, 1983, J CLIM APPL METEOROL, V22, P1444, DOI 10.1175/1520-0450(1983)022<1444:MDRND>2.0.CO;2
   RoyChowdhury Aruni, 2016, CVPR WORKSH PERC VIS, P1
   Shamoun-Baranes J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0160106
   Sheldon Daniel R., 2013, AAAI
   Stein ML, 2013, ANN APPL STAT, V7, P1162, DOI 10.1214/13-AOAS627
   Stroud JR, 2017, J COMPUT GRAPH STAT, V26, P108, DOI 10.1080/10618600.2016.1152970
   Tabary P, 2001, J ATMOS OCEAN TECH, V18, P875, DOI 10.1175/1520-0426(2001)018<0875:RTROTW>2.0.CO;2
   Van Doren BM, 2017, P NATL ACAD SCI USA, V114, P11175, DOI 10.1073/pnas.1708574114
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson Andrew Gordon, 2014, THESIS
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003053
DA 2019-06-15
ER

PT S
AU Antognini, JM
   Sohl-Dickstein, J
AF Antognini, Joseph M.
   Sohl-Dickstein, Jascha
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI PCA of high dimensional random walks with comparison to neural network
   training
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB One technique to visualize the training of neural networks is to perform PCA on the parameters over the course of training and to project to the subspace spanned by the first few PCA components. In this paper we compare this technique to the PCA of a high dimensional random walk. We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the first few PCA components, and that the projection of the trajectory onto any subspace spanned by PCA components is a Lissajous curve. We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e., a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting, but will instead be trapped at a fixed distance from the minimum. We finally analyze PCA projected training trajectories for: a linear model trained on CIFAR-10; a fully connected model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases, both the distribution of PCA eigenvalues and the projected trajectories resemble those of a random walk with drift.
C1 [Antognini, Joseph M.] Whisper AI, San Francisco, CA 94110 USA.
   [Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA.
RP Antognini, JM (reprint author), Whisper AI, San Francisco, CA 94110 USA.
EM joe.antognini@gmail.com; jaschasd@google.com
CR Ahn S., 2012, INT C MACH LEARN
   Baity- Jesi M., 2018, ARXIV180306969
   Bottcher A, 2003, MATH COMPUT, V72, P1329, DOI 10.1090/S0025-5718-03-01505-9
   Choromanska A., 2015, ARTIF INTELL, P192
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Dinh L., 2017, ARXIV170304933
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Jozefowicz R., 2016, ARXIV160202410
   Keskar N. S., 2017, INT C LEARN REPR
   Li H., 2018, INT C LEARN REPR
   Lipton Z. C., 2016, ARXIV160207320
   Lorch E., 2016, 33 INT C MACH LEARN, V48
   Mandt S., 2016, P INT C MACH LEARN, P354
   Moore J, 2018, J THEOR BIOL, V447, P56, DOI 10.1016/j.jtbi.2018.03.022
   Novak R., 2018, INT C LEARN REPR
   Rump SM, 2006, LINEAR ALGEBRA APPL, V413, P567, DOI 10.1016/j.laa.2005.06.009
   Smith S. L., 2018, INT C LEARN REPR
   Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823
   Zhu ZH, 2017, IEEE T INFORM THEORY, V63, P2975, DOI 10.1109/TIT.2017.2676808
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004082
DA 2019-06-15
ER

PT S
AU Aoi, MC
   Pillow, JW
AF Aoi, Mikio C.
   Pillow, Jonathan W.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Model-based targeted dimensionality reduction for neuronal population
   data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use "targeted" approaches that work by identifying multiple, distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables, such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However, existing targeted methods have been developed outside of the confines of probabilistic modeling, making some aspects of the procedures ad hoc, or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data. The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates, and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm, demixed PCA.
C1 [Aoi, Mikio C.; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Aoi, MC (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
EM maoi@princeton.edu; pillow@princeton.edu
FU Simons Foundation [SCGB AWD1004351, AWD543027]; NIH [R01EY017366,
   R01NS104899]; U19 NIH-NINDS BRAIN Initiative Award [NS104648-01]
FX This work was supported by grants from the Simons Foundation (SCGB
   AWD1004351 and AWD543027), the NIH (R01EY017366, R01NS104899) and a U19
   NIH-NINDS BRAIN Initiative Award (NS104648-01).
CR Afshar A, 2011, NEURON, V71, P555, DOI 10.1016/j.neuron.2011.05.047
   Bishop C. M., 2006, PATTERN RECOGNITION
   Brody CD, 2003, CEREB CORTEX, V13, P1196, DOI 10.1093/cercor/bhg100
   Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918
   Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1
   Kobak D, 2016, ELIFE, V5, DOI 10.7554/eLife.10989
   Lawrence N., 2005, J MACHINE LEARNING R, V6, P1816
   LIU CH, 1994, BIOMETRIKA, V81, P633
   Machens CK, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00126
   Machens CK, 2010, J NEUROSCI, V30, P350, DOI 10.1523/JNEUROSCI.3276-09.2010
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Morcos Ari S, 2016, NATURE NEUROSCIENCE
   Parthasarathy A, 2017, NAT NEUROSCI, V20, P1770, DOI 10.1038/s41593-017-0003-2
   Seely JS, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005164
   Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008
   Zhao Yuan, 2016, ARXIV160403053
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001025
DA 2019-06-15
ER

PT S
AU Arbel, M
   Sutherland, DJ
   Binkowski, M
   Gretton, A
AF Arbel, Michael
   Sutherland, Dougal J.
   Binkowski, Mikolaj
   Gretton, Arthur
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI On gradient regularizers for MMD GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160 x 160 CelebA and 64 x 64 unconditional ImageNet.
C1 [Arbel, Michael; Sutherland, Dougal J.; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London, England.
   [Binkowski, Mikolaj] Imperial Coll London, Dept Math, London, England.
RP Arbel, M (reprint author), UCL, Gatsby Computat Neurosci Unit, London, England.
EM michael.n.arbel@gmail.com; dougal@gmail.com; mikbinkowski@gmail.com;
   arthur.gretton@gmail.com
CR Amos B., 2017, ICML
   Arjovsky  M., 2017, ICML
   Arjovsky Martin, 2017, ICLR
   Barratt S, 2018, ARXIV180101973
   Bellemare M. G., 2017, ARXIV170510743
   Berthelot D., 2017, ARXIV170310717
   Binkowski M., 2018, ICLR
   Bottou Leon, 2018, Braverman Readings in Machine Learning. Key Ideas from Inception to Current State. International Conference Commemorating the 40th Anniversary of Emmanuil Braverman's Decease. Invited Talks. Lecture Notes in Artificial Intelligence (LNAI 11100), P229, DOI 10.1007/978-3-319-99492-5_11
   Bounliphone W., 2016, ICLR
   Bousquet O., 2004, NIPS
   Brock A., 2017, ICLR
   Dudley R. M., 2002, REAL ANAL PROBABILIT
   Dziugaite G. K., 2015, UAI
   Genevay A., 2018, AISTATS
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Goodfellow I., 2014, NIPS
   Gretton A., 2012, JMLR, V13
   Gulrajani  Ishaan, 2017, NIPS
   Gungor A., 2007, INT J CONT MATH SCI
   Heusel M., 2017, NIPS
   Huang G, 2018, ARXIV180607755
   Huang X., 2018, ECCV
   Jin Y., 2017, ARXIV170805509
   Karras Tero, 2018, ICLR
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LI  Chongxuan, 2017, NIPS
   Li Y., 2015, ICML 2015
   Liu  Z., 2015, ICCV
   Mescheder L, 2018, ICML
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   Miyato Takeru, 2018, ICLR
   Mroueh Y., 2018, ARXIV180512062
   Mroueh Y., 2017, NIPS
   Mroueh Y., 2018, ICLR
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Nowozin Sebastian, 2016, NIPS
   Radford A., 2016, ICLR
   Retherford J. R., 1978, B AM MATH SOC, V84, P681
   Roth K., 2017, NIPS
   Russakovsky O, 2014, ARXIV14090575
   Salimans T., 2016, NIPS
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Sriperumbudur B. K., 2009, NIPS
   Sriperumbudur B, 2016, BERNOULLI, V22, P1839, DOI 10.3150/15-BEJ713
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Steinwart I, 2008, INFORM SCI STAT, P1
   Sutherland D., 2017, ICLR
   Szegedy C., 2016, CVPR
   Unterthiner T., 2018, ICLR
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Weed J., BERNOULLI
   Wendland  H., 2005, SCATTERED DATA APPRO
   Zaremba W., 2013, NIPS
NR 54
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001026
DA 2019-06-15
ER

PT S
AU Arik, SO
   Chen, JT
   Peng, KN
   Ping, W
   Zhou, YQ
AF Arik, Sercan O.
   Chen, Jitong
   Peng, Kainan
   Ping, Wei
   Zhou, Yanqi
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Voice Cloning with a Few Samples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Voice cloning is a highly desired feature for personalized speech interfaces. We introduce a neural voice cloning system that learns to synthesize a person's voice from only a few audio samples. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model. Speaker encoding is based on training a separate model to directly infer a new speaker embedding, which will be applied to a multi-speaker generative model. In terms of naturalness of the speech and similarity to the original speaker, both approaches can achieve good performance, even with a few cloning - audios.(2) While speaker adaptation can achieve slightly better naturalness and similarity, cloning time and required memory for the speaker encoding approach are significantly less, making it more favorable for low-resource deployment.
C1 [Arik, Sercan O.; Chen, Jitong; Peng, Kainan; Ping, Wei; Zhou, Yanqi] Baidu Res, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA.
RP Arik, SO (reprint author), Baidu Res, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA.
EM sercanarik@baidu.com; chenjitong01@baidu.com; pengkainan@baidu.com;
   pingwei01@baidu.com; yanqiz@baidu.com
CR Abdel-Hamid O., 2013, IEEE ICASSP
   Agiomyrgiannakis Y., 2016, IEEE ICASSP
   Amodei D., 2016, INT C MACH LEARN, P173
   Arik  S., 2017, ICML
   Azadi S., 2017, CORR
   Chen L. H., 2014, IEEE ACM T AUDIO SPE
   Cui  Xiaodong, 2017, ARXIV171006937
   Desai S., 2010, IEEE T AUDIO SPEECH
   Gibiansky A., 2017, P NIPS, P2966
   Hsu CC, 2016, ASIAPAC SIGN INFO PR
   Jia JY, 2015, IEEE PHOT SPEC CONF
   Jozefowicz R., 2016, ARXIV160202410
   Karras T., 2017, CORR
   Lake B., 2015, SCIENCE
   Lake B. M., 2013, NIPS
   Lake B. M., 2014, COGSCI
   Li X., 2015, INTERSPEECH
   Mehri S., 2016, ARXIV161207837
   Miao Y., 2015, 16 ANN C INT SPEECH
   Panayotov V., 2015, IEEE ICASSP
   Ping W., 2018, ICLR
   Prince S. J., 2007, ICCV
   Reed S. E., 2017, CORR
   Rezende D., 2016, ICML
   Shen J., 2017, ARXIV171205884
   Snyder D, 2016, IEEE W SP LANG TECH, P165, DOI 10.1109/SLT.2016.7846260
   Sotelo J., 2017, CHAR2WAV END TO END
   Taigman Y., 2018, ICLR
   v d Oord A, 2016, ADV NEURAL INFORM PR
   vanderOord A., 2016, ARXIV160903499
   Vaswani A., 2017, NIPS
   Veaux  C., 2017, CSTR VCTK CORPUS ENG
   Wang Y., 2017, CORR
   Wester M, 2016, INTERSPEECH, P1637, DOI 10.21437/Interspeech.2016-1331
   Wu YC, 2016, INTERSPEECH, P1652, DOI 10.21437/Interspeech.2016-567
   Wu Z., 2015, INTERSPEECH
   Xue S., 2014, IEEE ACM T AUDIO SPE
   Yamagishi J., 2009, IEEE T AUDIO SPEECH
   Yu D., 2013, IEEE ICASSP
NR 39
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004056
DA 2019-06-15
ER

PT S
AU Arora, R
   Dinitz, M
   Marinov, TV
   Mohri, M
AF Arora, Raman
   Dinitz, Michael
   Marinov, Teodor V.
   Mohri, Mehryar
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Policy Regret in Repeated Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The notion of policy regret in online learning is a well defined performance measure for the common scenario of adaptive adversaries, which more traditional quantities such as external regret do not take into account. We revisit the notion of policy regret and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play that achieves a favorable regret with respect to one definition must do poorly with respect to the other. We then focus on the game-theoretic setting where the adversary is a self-interested agent. In that setting, we show that external regret and policy regret are not in conflict and, in fact, that a wide class of algorithms can ensure a favorable regret with respect to both definitions, so long as the adversary is also using such an algorithm. We also show that the sequence of play of no-policy regret algorithms converges to a policy equilibrium, a new notion of equilibrium that we introduce. Relating this back to external regret, we show that coarse correlated equilibria, which no-external regret players converge to, are a strict subset of policy equilibria. Thus, in game-theoretic settings, every sequence of play with no external regret also admits no policy regret, but the converse does not hold.
C1 [Arora, Raman; Dinitz, Michael; Marinov, Teodor V.] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.
   [Mohri, Mehryar] Courant Inst & Google Res, New York, NY 10012 USA.
RP Arora, R (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.
EM arora@cs.jhu.edu; mdinitz@cs.jhu.edu; tmarino2@jhu.edu;
   mohri@cims.nyu.edu
FU NSF BIGDATA grant [IIS-1546482, IIS-1838139]; NSF [CCF-1535987,
   IIS-1618662, CCF-1464239, AITF CCF-1535887]
FX This work was supported in part by NSF BIGDATA grant IIS-1546482, NSF
   BIGDATA grant IIS-1838139, NSF CCF-1535987, NSF IIS-1618662, NSF
   CCF-1464239, and NSF AITF CCF-1535887.
CR Allen-Zhu Zeyuan, 2016, ADV NEURAL INFORM PR, P974
   Arora Raman, 2012, P UNC ART INT UAI
   Arora Raman, 2012, P INT C MACH LEARN I
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Blum A, 2007, J MACH LEARN RES, V8, P1307
   Dar EE, 2009, ACM S THEORY COMPUT, P523
   Dekel O., 2014, P STOC, P459
   Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396
   Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595
   Fudenberg D, 1999, GAME ECON BEHAV, V29, P104, DOI 10.1006/game.1999.0705
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   Hazan Elad, 2008, NIPS, P625
   Heidari Hoda, 2016, P 25 INT JOINT C ART, P1562
   Kakade S. M., 2003, THESIS
   Koren Tomer, 2017, ADV NEURAL INFORM PR, P4119
   Koren Tomer, 2017, P 2017 C LEARN THEOR, P1242
   Merhav N, 2002, IEEE T INFORM THEORY, V48, P1947, DOI 10.1109/TIT.2002.1013135
   Mohri Mehryar, 2017, ADV NEURAL INFORM PR, P5220
   Mohri Mehryar, 2014, NIPS, P1314
   Neu Gergely, 2010, ADV NEURAL INFORM PR, P1804
   de Farias DP, 2006, J ACM, V53, P762, DOI 10.1145/1183907.1183911
   Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883
   Saha Ankan, 2012, ARXIV12116158
   Stoltz G, 2007, GAME ECON BEHAV, V59, P187, DOI 10.1016/j.geb.2006.04.007
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Szepesvri C., 2010, SYNTHESIS LECT ARTIF, V4, P1, DOI DOI 10.2200/S00268ED1V01Y201005AIM009
   Yu JY, 2009, MATH OPER RES, V34, P737, DOI 10.1287/moor.1090.0397
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001029
DA 2019-06-15
ER

PT S
AU Arora, R
   Braverman, V
   Upadhyay, J
AF Arora, Raman
   Braverman, Vladimir
   Upadhyay, Jalaj
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private Robust Low-Rank Approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we study the following robust low-rank matrix approximation problem: given a matrix A is an element of R-nxd ,find a rank-k matrix M, while satisfying differential privacy, such that parallel to A - M parallel to(p )<= alpha. OPTk(A) + tau, where parallel to B parallel to(p) is the = entry-wise l(p)-norm of B and OPTk(A) := min(rank(X)<= k) parallel to A - X parallel to(p). It is well known that low-rank approximation w.r.t. entrywise l(p)-norm, for p is an element of [1, 2), yields robustness to gross outliers in the data. We propose an algorithm that guarantees alpha = (O) over tilde (k(2)), tau = (O) over tilde (k(2)(n + kd/epsilon), runs in (O) over tilde((n+d)poly k) time and uses O(k (n+d) log k) space. We study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually. We also study the related problem of differentially private robust principal component analysis (PCA), wherein we return a rank-k projection matrix Pi such that parallel to A - A Pi parallel to(p) <= alpha . OPTk(A) + tau.
C1 [Arora, Raman; Braverman, Vladimir; Upadhyay, Jalaj] Johns Hopkins Univ, Baltimore, MD 21201 USA.
RP Arora, R (reprint author), Johns Hopkins Univ, Baltimore, MD 21201 USA.
EM arora@cs.jhu.edu; vova@cs.jhu.edu; jalaj@jhu.edu
FU NSF BIGDATA [IIS-1838139, IIS-1546482]; NSF Career [CCF-1652257]; ONR
   Award [N00014-18-1-2364]
FX This research was supported in part by NSF BIGDATA grant IIS-1546482,
   NSF BIGDATA grant IIS-1838139, NSF Career CCF-1652257, and ONR Award
   N00014-18-1-2364.
CR Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31
   [Anonymous], 2016, WALL STREET J
   Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67
   Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148
   Brooks JP, 2013, COMPUT STAT DATA AN, V61, P83, DOI 10.1016/j.csda.2012.11.007
   Bun Mark, 2017, ARXIV171104740
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   CHAMBERS JM, 1976, J AM STAT ASSOC, V71, P340, DOI 10.2307/2285309
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Drineas P., 2002, P 34 ANN ACM S THEOR, P82
   Dwork C., 2014, P 46 ANN ACM S THEOR, P11
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   Erlingsson Ulfar, 2014, P 2014 ACM SIGSAC C, P1054, DOI DOI 10.1145/2660267.2660348
   Gillis  Nicolas, 2015, ARXIV150909236
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Hardt Moritz, 2013, P 45 ANN ACM S THEOR, P331, DOI [DOI 10.1145/2488608.2488650, 10.1145/2488608.2488650]
   Hardt Moritz, 2012, P 44 ANN ACM S THEOR, P1255, DOI DOI 10.1145/2213977.2214088
   Jiang Wuxuan, 2015, ARXIV151105680
   Kapralov M., 2013, SODA, V5, P1
   Ke QF, 2005, PROC CVPR IEEE, P739
   Kim E, 2015, IEEE T NEUR NET LEAR, V26, P237, DOI 10.1109/TNNLS.2014.2312535
   Markopoulos Panos P, 2017, IEEE T SIGNAL PROCES
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35
   Sohler C, 2011, ACM S THEORY COMPUT, P755
   Song Z, 2017, ACM S THEORY COMPUT, P688, DOI 10.1145/3055399.3055431
   Upadhyay J, 2013, LECT NOTES COMPUT SC, V8269, P276, DOI 10.1007/978-3-642-42033-7_15
   Upadhyay  Jalaj, 2018, ADV NEURAL INFORM PR, P4180
   Upadhyay Jalaj, 2014, ARXIV14095414
   Zolotarev VM, 1986, ONE DIMENSIONAL STAB, V65
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304017
DA 2019-06-15
ER

PT S
AU Asadi, AR
   Abbe, E
   Verdu, S
AF Asadi, Amir R.
   Abbe, Emmanuel
   Verdu, Sergio
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Chaining Mutual Information and Tightening Generalization Bounds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm's input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou '15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine the chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley's inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.
C1 [Asadi, Amir R.; Abbe, Emmanuel] Princeton Univ, Princeton, NJ 08544 USA.
   [Abbe, Emmanuel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Asadi, AR (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM aasadi@princeton.edu
FU NSF CAREER Award [CCF-1552131]
FX We gratefully acknowledge discussions with Ramon van Handel on the topic
   of chaining. This work was partly supported by the NSF CAREER Award
   CCF-1552131.
CR Audibert JY, 2007, J MACH LEARN RES, V8, P863
   Audibert JY, 2004, ADV NEUR IN, V16, P1125
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Bassily R., 2017, ARXIV171005233
   Belkin M., 2018, ARXIV180201396
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dudley R. M., 1967, J FUNCT ANAL, V1, P290
   Fernique X., 1976, PROBABILITY BANACH S, P67
   Jiao J., 2017, ARXIV170809041
   Jiao JT, 2017, IEEE INT SYMP INFO, P1475, DOI 10.1109/ISIT.2017.8006774
   Kawaguchi K., 2017, ARXIV171005468
   Littlestone  N., 1986, TECHNICAL REPORT
   McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809
   Pensia  A., 2018, ARXIV180104295
   Russo D., 2015, ARXIV151105219
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Talagrand M, 2014, UPPER LOWER BOUNDS S, V60
   van Handel R., PROBABILITY HIGH DIM
   Vershynin R., 2018, CAMBRIDGE SERIES STA
   Xu A., 2017, ADV NEURAL INFORM PR, P2524
   Zhang C, 2017, INT C LEARN REPR ICL
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001076
DA 2019-06-15
ER

PT S
AU Ashtiani, H
   Ben-David, S
   Harvey, NJA
   Liaw, C
   Mehrabian, A
   Plan, Y
AF Ashtiani, Hassan
   Ben-David, Shai
   Harvey, Nicholas J. A.
   Liaw, Christopher
   Mehrabian, Abbas
   Plan, Yaniv
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Nearly tight sample complexity bounds for learning mixtures of Gaussians
   via sample compression schemes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We prove that (Theta) over tilde (kd(2)/epsilon(2) ) samples are necessary and sufficient for learning a mixture of k Gaussians in R-d , up to error E in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that (O) over tilde (kd/epsilon(2) ) samples suffice, matching a known lower bound.
   The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R-d has a small-sized sample compression.
C1 [Ashtiani, Hassan] McMaster Univ, Dept Comp & Software, Hamilton, ON, Canada.
   [Ashtiani, Hassan] Vector Inst, Toronto, ON, Canada.
   [Ben-David, Shai] Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada.
   [Harvey, Nicholas J. A.; Liaw, Christopher; Plan, Yaniv] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
   [Mehrabian, Abbas] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
RP Ashtiani, H (reprint author), McMaster Univ, Dept Comp & Software, Hamilton, ON, Canada.; Ashtiani, H (reprint author), Vector Inst, Toronto, ON, Canada.
EM zokaeiam@mcmaster.ca; shai@uwaterloo.ca; nickhar@cs.ubc.ca;
   cvliaw@cs.ubc.ca; abbasmehrabian@gmail.com; yaniv@math.ubc.ca
FU CRM-ISM postdoctoral fellowship; IVADO-Apogee-CFREF postdoctoral
   fellowship; NSERC Discovery Grant; NSERC graduate award; NSERC
   [22R23068]
FX We thank Yaoliang Yu for pointing out a mistake in an earlier version of
   this paper, and Luc Devroye for fruitful discussions. Abbas Mehrabian
   was supported by a CRM-ISM postdoctoral fellowship and an
   IVADO-Apogee-CFREF postdoctoral fellowship. Nicholas Harvey was
   supported by an NSERC Discovery Grant. Christopher Liaw was supported by
   an NSERC graduate award. Yaniv Plan was supported by NSERC grant
   22R23068.
CR Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Ashtiani Hassan, NEAR OPTIMAL SAMPLE
   Ashtiani Hassan, 2018, P 32 AAAI C ART INT, P2679
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Chan S., 2014, STOC, P604, DOI DOI 10.1145/2591796.2591848
   Devroye Luc, 1987, PROGR PROBABILITY ST, V14
   Devroye Luc, 2001, COMBINATORIAL METHOD, DOI [10.1007/978-1-4613-0125-7, DOI 10.1007/978-1-4613-0125-7]
   Devroye Luc, 2018, ARXIV180606887
   Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16
   Diakonikolas Ilias, 2016, HDB BIG DATA CHAPMAN, P267, DOI 10.1201/b19567-21
   Diakonikolas Ilias, 2017, COLT 17, V65, P1
   Ibragimov I, 2001, INST MATH S, V36, P359, DOI 10.1214/lnms/1215090078
   Kalai Adam, 2012, COMMUNICATIONS ACM, V55
   Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155
   Littlestone  N., 1986, TECHNICAL REPORT
   Litvak AE, 2005, ADV MATH, V195, P491, DOI 10.1016/j.aim.2004.08.004
   Lucic M, 2018, J MACH LEARN RES, V18
   Moran S, 2016, J ACM, V63, DOI 10.1145/2890490
   Silverman B. W., 1986, MONOGRAPHS STAT APPL
   Suresh A. T., 2014, ADV NEURAL INFORM PR, P1395
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303041
DA 2019-06-15
ER

PT S
AU Aubin, B
   Maillard, A
   Barbier, J
   Krzakala, F
   Macris, N
   Zdeborova, L
AF Aubin, Benjamin
   Maillard, Antoine
   Barbier, Jean
   Krzakala, Florent
   Macris, Nicolas
   Zdeborova, Lenka
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI The committee machine: Computational to statistical gaps in learning a
   two-layers neural network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID MESSAGE-PASSING ALGORITHMS; SPACE
AB Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.
C1 [Aubin, Benjamin; Zdeborova, Lenka] CNRS, Inst Phys Theor, Saclay, France.
   [Aubin, Benjamin; Zdeborova, Lenka] CEA, Saclay, France.
   [Aubin, Benjamin; Zdeborova, Lenka] Univ Paris Saclay, Saclay, France.
   [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] CNRS, Lab Phys Stat, Paris, France.
   [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] Sorbonnes Univ, Paris, France.
   [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] PSL Univ, Ecole Normale Super, Paris, France.
   [Barbier, Jean; Macris, Nicolas] Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, Lausanne, Switzerland.
   [Barbier, Jean] Abdus Salaam Int Ctr Theoret Phys, Trieste, Italy.
RP Aubin, B (reprint author), CNRS, Inst Phys Theor, Saclay, France.; Aubin, B (reprint author), CEA, Saclay, France.; Aubin, B (reprint author), Univ Paris Saclay, Saclay, France.; Aubin, B (reprint author), CNRS, Lab Phys Stat, Paris, France.; Aubin, B (reprint author), Sorbonnes Univ, Paris, France.; Aubin, B (reprint author), PSL Univ, Ecole Normale Super, Paris, France.
RI Krzakala, Florent/Q-9652-2019
OI Krzakala, Florent/0000-0003-2313-2578
FU ERC under the European Union [714608-SMiLe]; French Agence Nationale de
   la Recherche [ANR-17-CE23-0023-01 PAIL]; NVIDIA Corporation; Swiss
   National Foundation [200021-156672]
FX This work is supported by the ERC under the European Union's Horizon
   2020 Research and Innovation Program 714608-SMiLe, as well as by the
   French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01
   PAIL. We gratefully acknowledge the support of NVIDIA Corporation with
   the donation of the Titan Xp GPU used for this research. Jean Barbier
   was supported by the Swiss National Foundation grant no 200021-156672.
   We also thank Leo Miolane for fruitful discussions.
CR Aubin Benjamin, 2018, ARXIV180605451
   Aubin Benjamin, 2018, AMP IMPLEMENTATION C
   Baity- Jesi M., 2018, ARXIV180306969
   Baldassi C, 2007, P NATL ACAD SCI USA, V104, P11079, DOI 10.1073/pnas.0700324104
   Bandeira Afonso S, 2018, ARXIV180311132
   Barbier Jean, 2017, ARXIV170803395
   Barbier Jean, 2017, PROBABILITY THEORY R
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Chaudhari Pratik, 2016, ICLR 2017
   Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y
   Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   El Alaoui A, 2017, IEEE INT SYMP INFO, P2780, DOI 10.1109/ISIT.2017.8007036
   El Alaoui Ahmed, 2016, ARXIV161109981
   Engel A., 2001, STAT MECH LEARNING
   Guerra F, 2003, COMMUN MATH PHYS, V233, P1, DOI 10.1007/s00220-002-0773-5
   Javanmard A, 2013, INF INFERENCE, V2, P115, DOI 10.1093/imaiai/iat004
   Kabashima Y, 2008, J PHYS C SER, V95
   Kamilov Ulugbek, 2012, ADV NEURAL INFORM PR, P2438
   Krzakala F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08009
   Martin CH, 2017, ARXIV171009553
   MATO G, 1992, J PHYS A-MATH GEN, V25, P5047, DOI 10.1088/0305-4470/25/19/017
   MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Mezard  Marc, 1987, SPIN GLASS THEORY IN, V9
   MONASSON R, 1995, PHYS REV LETT, V75, P2432, DOI 10.1103/PhysRevLett.75.2432
   Monasson R, 1995, MOD PHYS LETT B, V9, P1887, DOI 10.1142/S0217984995001868
   Opper M, 1996, PHYS REV LETT, V76, P1964, DOI 10.1103/PhysRevLett.76.1964
   Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942
   SAAD D, 1995, PHYS REV E, V52, P4225, DOI 10.1103/PhysRevE.52.4225
   Safran I., 2017, ARXIV171208968
   Schniter P, 2016, CONF REC ASILOMAR C, P1525, DOI 10.1109/ACSSC.2016.7869633
   SCHWARZE H, 1992, EUROPHYS LETT, V20, P375, DOI 10.1209/0295-5075/20/4/015
   SCHWARZE H, 1993, EUROPHYS LETT, V21, P785, DOI 10.1209/0295-5075/21/7/012
   SCHWARZE H, 1993, J PHYS A-MATH GEN, V26, P5781, DOI 10.1088/0305-4470/26/21/017
   SEUNG HS, 1992, PHYS REV A, V45, P6056, DOI 10.1103/PhysRevA.45.6056
   Talagrand  Michel, 2003, SPIN GLASSES CHALLEN, V46
   THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992
   Vapnik V. N., 1998, STAT LEARNING THEORY
   WATKIN TLH, 1993, REV MOD PHYS, V65, P499, DOI 10.1103/RevModPhys.65.499
   Zdeborova L, 2016, ADV PHYS, V65, P453, DOI 10.1080/00018732.2016.1211393
   Zhang C, 2016, ICLR 2017
   Zhu JA, 2017, IEEE T SIGNAL PROCES, V65, P2444, DOI 10.1109/TSP.2016.2646663
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303024
DA 2019-06-15
ER

PT S
AU Auricchio, G
   Gualandi, S
   Veneroni, M
   Bassetti, F
AF Auricchio, Gennaro
   Gualandi, Stefano
   Veneroni, Marco
   Bassetti, Federico
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Computing Kantorovich-Wasserstein Distances on d-dimensional histograms
   using (d+1)-partite graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID EARTH-MOVERS-DISTANCE; MINIMUM
AB This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of d-dimensional histograms having n bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a (d + 1)-partite graph with (d + 1)n nodes and dn d+1/d arcs, whenever the cost is separable along the principal d-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and d-dimensional bio medical histograms. On these types of instances, our approach is competitive with state-of-the-art optimal transport algorithms.
C1 [Auricchio, Gennaro; Gualandi, Stefano; Veneroni, Marco] Univ Pavia, Dipartimento Matemat F Casorati, Pavia, Italy.
   [Bassetti, Federico] Politecnico Milano, Dipartimento Matemat, Milan, Italy.
RP Auricchio, G (reprint author), Univ Pavia, Dipartimento Matemat F Casorati, Pavia, Italy.
EM gennaro.auricchio01@universitadipavia.it; stefano.gualandi@unipv.it;
   marco.veneroni@unipv.it; federico.bassetti@polimi.it
FU Italian Ministry of Education, University and Research (MIUR):
   Dipartimenti di Eccellenza Program (2018-2022) - Dept. of Mathematics
   "F. Casorati", University of Pavia; PRIN 2015 Modern Bayesian
   nonparametric methods [2015SNS29B-002]
FX This research was partially supported by the Italian Ministry of
   Education, University and Research (MIUR): Dipartimenti di Eccellenza
   Program (2018-2022) - Dept. of Mathematics "F. Casorati", University of
   Pavia.; The last author's research is partially supported by "PRIN 2015.
   2015SNS29B-002. Modern Bayesian nonparametric methods".
CR Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]
   Ahuja Ravindra K, 1988, NETWORK FLOWS THEORY
   Altschuler J., 2017, ADV NEURAL INFORM PR, P1961
   Ambrosio L., 2008, LECT MATH
   Arjovsky M, 2017, ARXIV170107875
   Bassetti F, 2006, THEOR PROBAB APPL+, V50, P171, DOI 10.1137/S0040585X97981664
   Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001
   Bassetti Federico, 2018, ARXIV180400445
   Bernas T, 2008, CYTOM PART A, V73A, P715, DOI 10.1002/cyto.a.20586
   Chizat Lenaic, 2016, ARXIV160705816
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292
   Cuturi M., 2014, INT C MACH LEARN, P685
   Flood Merrill M, 1953, PAC J MATH, V3, P369
   Frogner C., 2015, ADV NEURAL INFORM PR, P2053
   Goldberg Andrew V, 1989, TECHNICAL REPORT
   Kovacs P, 2015, OPTIM METHOD SOFTW, V30, P94, DOI 10.1080/10556788.2014.895828
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Liero M, 2018, INVENT MATH, V211, P969, DOI 10.1007/s00222-017-0759-8
   Ling H, 2007, IEEE T PATTERN ANAL, V29, P840, DOI 10.1109/TPAMI.2007.1058
   ORLIN JB, 1993, OPER RES, V41, P338, DOI 10.1287/opre.41.2.338
   Orlova DY, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151859
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Peyre Gabriel, 2018, 180300567 ARXIV
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Schrieber J, 2017, IEEE ACCESS, V5, P271, DOI 10.1109/ACCESS.2016.2639065
   Solomon J., 2014, P 31 INT C MACH LEAR, V32
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Solomon Justin, 2018, 180107745 ARXIV
   Vershik AM, 2013, MATH INTELL, V35, P1, DOI 10.1007/s00283-013-9380-x
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000031
DA 2019-06-15
ER

PT S
AU Avalos-Fernandez, M
   Nock, R
   Ong, CS
   Rouar, J
   Sun, K
AF Avalos-Fernandez, Marta
   Nock, Richard
   Ong, Cheng Soon
   Rouar, Julien
   Sun, Ke
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Representation Learning of Compositional Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We consider the problem of learning a low dimensional representation for compositional data. Compositional data consists of a collection of nonnegative data that sum to a constant value. Since the parts of the collection are statistically dependent, many standard tools cannot be directly applied. Instead, compositional data must be first transformed before analysis. Focusing on principal component analysis (PCA), we propose an approach that allows low dimensional representation learning directly from the original data. Our approach combines the benefits of the log-ratio transformation from compositional data analysis and exponential family PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem, that relates the perspective transform of a Bregman divergence to the Bregman divergence of a perspective transform and a remainder conformal divergence. Our proposed approach includes a convenient surrogate (upper bound) loss of the exponential family PCA which has an easy to optimize form. We also derive the corresponding form for nonlinear autoencoders. Experiments on simulated data and microbiome data show the promise of our method.
C1 [Avalos-Fernandez, Marta; Rouar, Julien] Univ Bordeaux, Bordeaux, France.
   [Nock, Richard; Ong, Cheng Soon; Sun, Ke] Data61, Sydney, NSW, Australia.
   [Nock, Richard; Ong, Cheng Soon] Australian Natl Univ, Canberra, ACT, Australia.
   [Nock, Richard] Univ Sydney, Sydney, NSW, Australia.
RP Avalos-Fernandez, M (reprint author), Univ Bordeaux, Bordeaux, France.
EM marta.avalos-fernande@u-bordeaux.fr; richard.nock@data61.csiro.au;
   cheng.ong@data61.csiro.au; julien.rouar@u-bordeaux.fr;
   ke.sun@data61.csiro.au
CR AITCHISON J, 1982, J ROY STAT SOC B MET, V44, P139
   Aitchison J, 2005, MATH GEOL, V37, P829, DOI 10.1007/s11004-005-7383-7
   AITCHISON J, 1983, BIOMETRIKA, V70, P57
   Aitchison J, 1994, INST MATH S, V24, P73, DOI 10.1214/lnms/1215463786
   Aitchison J., 1986, STAT ANAL COMPOSITIO
   Amari S, 2016, INFORM GEOMETRY ITS
   Barndorff-Nielsen O., 1978, INFORM EXPONENTIAL F
   Boissonnat JD, 2010, DISCRETE COMPUT GEOM, V44, P281, DOI 10.1007/s00454-010-9256-1
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chiquet J., 2018, ANN APPL STAT
   Clevert D.-A., 2016, 4 ICLR
   Collins M., 2002, NIPS 15
   Egozcue JJ, 2003, MATH GEOL, V35, P279, DOI 10.1023/A:1023818214614
   Gloor GB, 2016, ANN EPIDEMIOL, V26, P322, DOI 10.1016/j.annepidem.2016.03.003
   Greenacre M., 2018, COMPOSITIONAL DATA A
   Hugerth LW, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.01561
   Lahti L., 2017, MICROBIOME R PACKAGE
   Lahti L, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5344
   Landgraf A. J., 2015, THESIS
   Lovell D., 2010, EP10994 CSIRO
   Marechal P, 2005, J OPTIMIZ THEORY APP, V126, P175, DOI 10.1007/s10957-005-2667-0
   Marechal P., 2005, J OPTIMIZATION THEOR, V126, P375
   Martin-Fernandez JA, 2018, MATH GEOSCI, V50, P273, DOI 10.1007/s11004-017-9712-z
   Nock R., 2016, IEEE T IT, V62, P1
   Nock R., 2016, ADV NEURAL INFORM PR, P19
   O'Keefe SJD, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7342
   Paliy O, 2016, MOL ECOL, V25, P1032, DOI 10.1111/mec.13536
   PawlowskyGlahn V, 2011, COMPOSITIONAL DATA ANALYSIS: THEORY AND APPLICATIONS, P1, DOI 10.1002/9781119976462
   Reid G., 2016, CAN J MICROBIOL, V12, P1
   Sun K., 2014, P 31 INT C MACH LEAR, P1
   Tolosona-Delgado R., 2007, TECHNICAL REPORT
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001024
DA 2019-06-15
ER

PT S
AU Awan, J
   Slavkovic, A
AF Awan, Jordan
   Slavkovic, Aleksandra
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Differentially Private Uniformly Most Powerful Tests for Binomial Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a 'Neyman-Pearson lemma' for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin "Truncated-Uniform-Laplace" (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact p-values, which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error, and are more powerful than current techniques.
C1 [Awan, Jordan; Slavkovic, Aleksandra] Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
RP Awan, J (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
EM awan@psu.edu; sesa@psu.edu
FU NSF [SES-1534433]
FX We would like to thank Vishesh Karwa and Matthew Reimherr for helpful
   discussions and feedback on previous drafts. We also thank the reviewers
   for their helpful comments and suggestions, which have contributed to
   many improvements in the presentation of this work. This work is
   supported in part by NSF Award No. SES-1534433 to The Pennsylvania State
   University.
CR Awan Jordan, 2018, ARXIV E PRINTS
   Barrientos A., 2017, ARXIV E PRINTS
   Bishop C. M., 2006, PATTERN RECOGNITION
   Casella G., 2002, DUXBURY ADV SERIES S
   Duchi JC, 2018, J AM STAT ASSOC, V113, P182, DOI 10.1080/01621459.2017.1389735
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Gaboardi Marco, 2018, P MACHINE LEARNING R, V80, P1626
   Gaboardi Marco, 2016, P 33 INT C INT C MAC, P2111
   Geng Q, 2016, IEEE T INFORM THEORY, V62, P925, DOI 10.1109/TIT.2015.2504967
   Geng Q, 2016, IEEE T INFORM THEORY, V62, P952, DOI 10.1109/TIT.2015.2504972
   Ghosh A, 2009, ACM S THEORY COMPUT, P351
   Gibbons J. D., 2014, NONPARAMETRIC STAT I
   Inusah S, 2006, J STAT PLAN INFER, V136, P1090, DOI 10.1016/j.jspi.2004.08.014
   Karwa Vishesh, 2017, CORR
   Schervish M. J., 1996, SPRINGER SERIES STAT
   Sheffet Or, 2017, INT C MACH LEARN, V70, P3105
   Solea Eftychia, 2014, THESIS
   Steinke Thomas, 2018, PRIVATE CORRES
   Uhler Caroline, 2013, J PRIVACY CONFIDENTI, V5
   Van der Vaart A. W, 2000, CAMBRIDGE SERIES STA
   Vu D, 2009, INT CONF DAT MIN WOR, P138, DOI 10.1109/ICDMW.2009.52
   Wang Y., 2015, ARXIV E PRINTS
   Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651
   Xie MG, 2013, INT STAT REV, V81, P3, DOI 10.1111/insr.12000
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304024
DA 2019-06-15
ER

PT S
AU Aytar, Y
   Pfaff, T
   Budden, D
   Paine, T
   Wang, ZY
   de Freitas, N
AF Aytar, Yusuf
   Pfaff, Tobias
   Budden, David
   Le Paine, Tom
   Wang, Ziyu
   de Freitas, Nando
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Playing hard exploration games by watching YouTube
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE for the first time, even if the agent is not presented with any environment rewards.
C1 [Aytar, Yusuf; Pfaff, Tobias; Budden, David; Le Paine, Tom; Wang, Ziyu; de Freitas, Nando] DeepMind, London, England.
RP Aytar, Y (reprint author), DeepMind, London, England.
EM yusufaytar@google.com; tpfaff@google.com; budden@google.com;
   tpaine@google.com; ziyu@google.com; nandodefreitas@google.com
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Abbeel P., 2017, ARXIV170301703
   Anderson T. W., 1958, INTRO MULTIVARIATE S, V2
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Aytar Y, 2017, ARXIV170600932
   Aytar Yusuf, 2017, IEEE T PATTERN ANAL
   Barth-Maron Gabriel, 2018, INT C LEARN REPR ICL
   Bellemare M., 2016, ADV NEURAL INFORM PR, P1471, DOI DOI 10.3390/BS3030459
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bellemare Marc G, 2017, INT C MACH LEARN ICM
   Darrell T., 2014, ARXIV14123474
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Duan Y., 2017, ADV NEURAL INFORM PR, P1087
   Espeholt Lasse, 2018, CORR
   Finn C., 2017, ARXIV170904905
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gruslys Audrunas, 2017, INT C LEARN REPR ICL
   Hadfield-Menell Dylan, 2017, ADV NEURAL INFORM PR, P6768
   Hessel Matteo, 2017, P AAAI C ART INT
   Hester Todd, 2017, P AAAI C ART INT
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Horgan Dan, 2018, INT C LEARN REPR ICL
   Hosu Ionel-Alexandru, 2016, CORR
   Le Paine Tom, 2018, ARXIV181005017
   Liu Yuxuan, 2017, ARXIV170703374
   Osband I., 2017, ARXIV170307608
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Pathak Deepak, 2017, INT C MACH LEARN ICM, V2017
   Pathak Deepak, 2018, ARXIV180408606
   Pohlen Tobias, 2018, ARXIV180511593
   Savinov N., 2018, ARXIV180300653
   Sermanet  P., 2017, ARXIV170406888
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Singh  S., 2009, P ANN C COGN SCI SOC, V2009, P2601
   Torabi Faraz, 2018, ARXIV180501954
   Trigeorgis G, 2018, IEEE T PATTERN ANAL, V40, P1128, DOI 10.1109/TPAMI.2017.2710047
   van den Oord Aaron, 2018, ARXIV180703748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vecerik M, 2017, ARXIV170708817
   Vondrick C., 2015, ARXIV150408023
   Wang Ziyu, 2015, INT C MACH LEARN ICM
   Yu T., 2018, ARXIV180201557
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou B., 2014, CORR, V1412, P6856
   Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20
   Zhu J Y, 2017, ARXIV170310593
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302091
DA 2019-06-15
ER

PT S
AU Bach, F
AF Bach, Francis
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Efficient Algorithms for Non-convex Isotonic Regression through
   Submodular Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID STOCHASTIC-DOMINANCE
AB We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance. We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles; these algorithms also lead to improvements without isotonic constraints. Finally, our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time.
C1 [Bach, Francis] PSL Res Univ, Dept Informat, Ecole Normale Super, INRIA, Paris, France.
RP Bach, F (reprint author), PSL Res Univ, Dept Informat, Ecole Normale Super, INRIA, Paris, France.
EM francis.bach@ens.fr
FU European Research Council [SEQUOIA 724063]
FX We acknowledge support the European Research Council (grant SEQUOIA
   724063).
CR Bach F, 2013, FDN TRENDS MACHINE L, V6
   Bach F., 2018, MATH PROGRAMMING
   Bertsekas D. P., 2016, NONLINEAR PROGRAMMIN
   BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chambolle A., 2015, SIAM J COMPUT MATH, V1, P29, DOI DOI 10.5802/smai-jcm.3
   Chen Xi, 2015, 150901877 ARXIV
   Chen YN, 2016, J R STAT SOC B, V78, P729, DOI 10.1111/rssb.12137
   Dentcheva D, 2004, OPTIMIZATION, V53, P583, DOI 10.1080/02331930412331327148
   Deza M., 2009, GEOMETRY CUTS METRIC, V15
   GALLO G, 1989, SIAM J COMPUT, V18, P30, DOI 10.1137/0218003
   Hampel F. R, 2011, ROBUST STAT APPROACH, V196
   Hochbaum DS, 2008, OPER RES, V56, P992, DOI 10.1287/opre.1080.0524
   Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836
   Jegelka S., 2013, ADV NEURAL INFORM PR
   Kakade S. M., 2011, ADV NEURAL INFORM PR
   KARLIN S, 1980, J MULTIVARIATE ANAL, V10, P467, DOI 10.1016/0047-259X(80)90065-2
   Kim SY, 2003, COMPUT OPTIM APPL, V26, P143, DOI 10.1023/A:1025794313696
   LEHMANN EL, 1955, ANN MATH STAT, V26, P399, DOI 10.1214/aoms/1177728487
   LEVY H, 1992, MANAGE SCI, V38, P555, DOI 10.1287/mnsc.38.4.555
   Lorentz GG, 1953, AM MATH MONTHLY, V60, P176
   Luss R, 2014, J COMPUT GRAPH STAT, V23, P192, DOI 10.1080/10618600.2012.741550
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Rudin W., 1986, REAL COMPLEX ANAL
   SLEATOR DD, 1983, J COMPUT SYST SCI, V26, P362, DOI 10.1016/0022-0000(83)90006-5
   Spouge J, 2003, J OPTIMIZ THEORY APP, V117, P585, DOI 10.1023/A:1023901806339
   Stout QF, 2013, ALGORITHMICA, V66, P93, DOI 10.1007/s00453-012-9628-4
   Tarjan R, 2006, LECT NOTES COMPUT SC, V4168, P612
   TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305
   Yu YL, 2016, J PHYS CONF SER, V699, DOI 10.1088/1742-6596/699/1/012016
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300001
DA 2019-06-15
ER

PT S
AU Bafna, M
   Murtagh, J
   Vyas, N
AF Bafna, Mitali
   Murtagh, Jack
   Vyas, Nikhil
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Thwarting Adversarial Examples: An L-0-Robust Sparse Fourier Transform
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We give a new algorithm for approximating the Discrete Fourier transform of an approximately sparse signal that has been corrupted by worst-case L-0 noise, namely a bounded number of coordinates of the signal have been corrupted arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend against well known L-0 adversaries in the setting of image classification. We give experimental results on the Jacobian-based Saliency Map Attack (JSMA) and the Carlini Wagner (CW) L-0 attack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch on the ImageNet dataset.
C1 [Bafna, Mitali; Murtagh, Jack] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Vyas, Nikhil] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
RP Bafna, M (reprint author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM mitalibafna@g.harvard.edu; jmurtagh@g.harvard.edu; nikhilv@mit.edu
FU NSF [CCF 1715187, CNS-1565387, CCF-1552651]; Akamai Presidential
   Fellowship
FX Mitali Bafna was supported by NSF Grant CCF 1715187. Jack Murtagh was
   supported by NSF grant CNS-1565387. Nikhil Vyas was supported by an
   Akamai Presidential Fellowship and NSF Grant CCF-1552651. We would like
   to thank Yaron Singer and Adam Breuer for helpful feedback and
   encouragement in the early stages of this work. We also want to thank
   Thibaut Horel for valuable comments on the manuscript. Thanks also to
   the reviewers for helpful remarks.
CR Backurs A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2215
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Blumensath Thomas, 2008, CORR
   Brown Tom B., 2017, CORR
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Carlini Nicholas, 2016, CORR
   Evtimov I., 2017, CORR
   Goodfellow Ian J., 2015, INT C LEARN REPR
   Hassanieh  H., 2012, P 23 ANN ACM SIAM S, P1183
   Hassanieh Hitham, 2012, P 44 ANN ACM S THEOR, P563
   He K., 2015, CORR
   Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939
   Hegde C, 2014, LECT NOTES COMPUT SC, V8572, P588
   Indyk Piotr, 2014, P 25 ANN ACM SIAM S, P480
   Madry Aleksander, 2017, CORR
   Moosavi-Dezfooli Seyed-Mohsen, 2015, CORR
   Needell Deanna, 2008, ARXIV08032392
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot Nicolas, 2015, CORR
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Szegedy C., 2013, CORR
   Tramer F., 2017, ARXIV170507204
   Xiao Han, 2017, CORR
   Yann LeCun, 1998, MNIST DATABASE HANDW
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004061
DA 2019-06-15
ER

PT S
AU Bai, WR
   Noble, WS
   Bilmes, JA
AF Bai, Wenruo
   Noble, William S.
   Bilmes, Jeff A.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Submodular Maximization via Gradient Ascent: The Case of Deep Submodular
   Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID FUNCTION SUBJECT; ALGORITHMS; LOCATION
AB We study the problem of maximizing deep submodular functions (DSFs) [13, 3] subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the facility location, weighted coverage, and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach [6], but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of max(0<delta<1)(1-epsilon-delta-e(-delta 2 Omega(k))) with a running time of O(n(2) /epsilon(2) ) plus time for pipage rounding [6] to recover a discrete solution, where k is the rank of the matroid constraint. This bound is often better than the standard 1 - 1/e guarantee of the continuous greedy algorithm, but runs much faster. Our bound also holds even for fully curved (c = 1) functions where the guarantee of 1 - c/e degenerates to 1 - 1/e where c is the curvature of f [37]. We perform computational experiments that support our theoretical results.
C1 [Bai, Wenruo; Bilmes, Jeff A.] Dept Elect & Comp Engn, Seattle, WA 98195 USA.
   [Noble, William S.; Bilmes, Jeff A.] Dept Comp Sci & Engn, Seattle, WA 98195 USA.
   [Noble, William S.] Dept Genome Sci, Seattle, WA 98195 USA.
RP Bai, WR (reprint author), Dept Elect & Comp Engn, Seattle, WA 98195 USA.
EM wrbai@uw.edu; wnoble@uw.edu; bilmes@uw.edu
FU National Science Foundation [IIS-1162606]; National Institutes of Health
   [R01GM103544]; Google; Microsoft; Intel; CONIX Research Center, one of
   six centers in JUMP, a Semiconductor Research Corporation (SRC) program
   - DARPA
FX This material is based upon work supported by the National Science
   Foundation under Grant No. IIS-1162606, the National Institutes of
   Health under award R01GM103544, and by a Google, a Microsoft, and an
   Intel research award. This research is also supported by the CONIX
   Research Center, one of six centers in JUMP, a Semiconductor Research
   Corporation (SRC) program sponsored by DARPA.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2
   Bilmes Jeffrey, 2017, ABS170108939 ARXIV
   Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Chakrabarty Deeparnab, 2016, ARXIV161009800
   Chekuri C, 2004, LECT NOTES COMPUT SC, V3122, P72
   Chekuri Chandra, 2009, ARXIV09094348
   CORNUEJOLS G, 1977, MANAGE SCI, V23, P789, DOI 10.1287/mnsc.23.8.789
   CUNNINGHAM WH, 1985, COMBINATORICA, V5, P185, DOI 10.1007/BF02579361
   Djolonga J., 2014, NEURAL INFORM PROCES
   Dolhansky B. W., 2016, ADV NEURAL INFORM PR, P3396
   Dughmi Shaddin, 2009, ARXIV09120322
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Fujito T, 2000, IEICE T FUND ELECTR, VE83A, P480
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Hassani Hamed, 2017, ADV NEURAL INFORM PR, P5841
   Hui Lin, 2011, P 49 ANN M ASS COMP, V2, P170
   Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589
   Kai Wei, 2014, P ICASSP
   Kai Wei, 2016, BIORXIV
   Kai Wei, 2015, INT C MACH LEARN ICM
   Karimi Mohammad, 2017, NEURAL INFORM PROCES, P6856
   Kirchhoff Katrin, 2014, P 2014 C EMP METH NA, P131
   Kohli P, 2009, IEEE T PATTERN ANAL, V31, P1645, DOI 10.1109/TPAMI.2008.217
   Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2
   Lee YT, 2015, ANN IEEE SYMP FOUND, P1049, DOI 10.1109/FOCS.2015.68
   Libbrecht Maxwell W., 2018, PROTEINS STRUCTURE F
   Lin H., 2009, ASRU
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   Mokhtari Aryan, 2018, INT C ART INT STAT, P1886
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   RAGHAVAN P, 1988, J COMPUT SYST SCI, V37, P130, DOI 10.1016/0022-0000(88)90003-7
   Soma Tasuku, 2015, ADV NEURAL INFORM PR, P847
   Stobbe P., 2010, NIPS
   Sviridenko  M., 2015, P 26 ANN ACM SIAM S, P1134
   Vetta A, 2002, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2002.1181966
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002052
DA 2019-06-15
ER

PT S
AU Bailey, B
   Telgarsky, M
AF Bailey, Bolton
   Telgarsky, Matus
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Size-Noise Tradeoffs in Generative Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BOUNDS
AB This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a "space-filling" function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly, we provide efficient ways (using polylog(1/epsilon) nodes) for networks to pass between univariate uniform and normal distributions, using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly, we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions.
C1 [Bailey, Bolton; Telgarsky, Matus] Univ Illinois, Urbana, IL 61801 USA.
RP Bailey, B (reprint author), Univ Illinois, Urbana, IL 61801 USA.
EM boltonb2@illinois.edu; mjt@illinois.edu
FU NSF [IIS-1750051]; NVIDIA
FX The authors are grateful for support from the NSF under grant
   IIS-1750051, and for a GPU grant from NVIDIA.
CR Anthony  M., 2009, NEURAL NETWORK LEARN
   Arjovsky M., 2017, P 34 INT C MACH LEAR, P214
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645
   Chen Zhimin, 2017, ARXIV170502438
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Donahue C., 2018, ARXIV180204208
   Eldan Ronen, 2016, COLT
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Kolmogorov A. N., 1975, INTRO REAL ANAL
   Lee Holden, 2017, ABILITY NEURAL NETS, V65, P1271
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924
   Osokin A, 2017, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2017.245
   Pinelis Iosif, 2013, NONUNIFORM BERRY ESS
   Safran Itay, 2016, CORR
   Telgarsky Matus, 2016, P 29 ANN C LEARN THE, V49, P1517
   Villani C., 2003, GRADUATE STUDIES MAT
   Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002
   Zhang Liwen, 2018, ARXIV180507091
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001006
DA 2019-06-15
ER

PT S
AU Bajpai, A
   Garg, S
   Mausam
AF Bajpai, Aniket
   Garg, Sankalp
   Mausam
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Transfer of Deep Reactive Policies for MDP Planning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the specifics of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and good methods for transferring experience from policies of other instances of a domain to a new instance do not exist. Recently, researchers have begun exploring the use of deep reactive policies, trained via deep reinforcement learning (RL), for MDP planning domains. One advantage of deep reactive policies is that they are more amenable to transfer learning.
   In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves.
C1 [Bajpai, Aniket; Garg, Sankalp; Mausam] Indian Inst Technol Delhi, New Delhi, India.
RP Bajpai, A (reprint author), Indian Inst Technol Delhi, New Delhi, India.
EM quantum.computing96@gmail.com; sankalp2621998@gmail.com;
   mausam@cse.iitd.ac.in
FU Google; Bloomberg award; IBM SUR award; 1MG award; Visvesvaraya faculty
   award by Govt. of India; Microsoft Azure
FX We thank Ankit Anand and the anonymous reviewers for their insightful
   comments on an earlier draft of the paper. We also thank Alan Fern,
   Scott Sanner, Akshay Gupta and Arindam Bhattacharya for initial
   discussions on the research. This work is supported by research grants
   from Google, a Bloomberg award, an IBM SUR award, a 1MG award, and a
   Visvesvaraya faculty award by Govt. of India. We thank Microsoft Azure
   sponsorships, and the IIT Delhi HPC facility for computational
   resources.
CR Anand A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1509
   Ankit Anand, 2016, ICAPS, P29
   Bellman  Richard, 1957, INDIANA U MATH J
   Clevert  Djork-Arne, 2015, ABS151107289 CORR
   Fern  Alan, 2018, ICAPS
   Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10
   Garnelo M., 2016, ABS160905518 CORR
   Goyal  Palash, 2017, ABS170502801 CORR
   Groshev  Edward, 2018, ICAPS
   Grzes  Marek, 2014, ICAPS
   Guestrin  Carlos, 2001, INT JOINT C ART INT, P673
   Higgins  Irina, 2017, P 34 INT C MACH LEAR, P1480
   Kipf T. N., 2017, ICLR
   Kolobov A., 2012, P UAI, P438
   Littman Michael L., 1997, P 14 NAT C ART INT, P748
   Matiisen  Tambet, 2017, ABS170700183 CORR
   Mausam, 2012, PLANNING MARKOV DECI
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Parisotto  Emilio, 2015, ABS151106342 CORR
   Puterman M. L., 1994, MARKOV DECISION PROC
   Rajendran  J., 2017, ICLR
   Ravindran Balaraman, 2004, THESIS
   Sanner  Scott, 2010, RELATIONAL DYNAMIC I
   Sutton R. S., 1998, ADAPTIVE COMPUTATION
   Tamar  Aviv, 2017, P 26 INT JOINT C ART, P4949
   Toyer  Sam, 2018, P 32 AAAI C ART INT
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Younes HLS, 2005, J ARTIF INTELL RES, V24, P851, DOI 10.1613/jair.1880
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005054
DA 2019-06-15
ER

PT S
AU Baker, J
   Fearnhead, P
   Fox, EB
   Nemeth, C
AF Baker, Jack
   Fearnhead, Paul
   Fox, Emily B.
   Nemeth, Christopher
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Large-Scale Stochastic Sampling from the Probability Simplex
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DIRICHLET; LANGEVIN
AB Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space the time-discretization error can dominate when we are near the boundary of the space. We demonstrate that because of this, current SGMCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero. Unfortunately, many popular large-scale Bayesian models, such as network or topic models, require inference on sparse simplex spaces. To avoid the biases caused by this discretization error, we propose the stochastic Cox-Ingersoll-Ross process (SCIR), which removes all discretization error and we prove that samples from the SCIR process are asymptotically unbiased. We discuss how this idea can be extended to target other constrained spaces. Use of the SCIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches.
C1 [Baker, Jack] Univ Lancaster, STOR I CDT, Math & Stat, Lancaster, England.
   [Fearnhead, Paul; Nemeth, Christopher] Univ Lancaster, Math & Stat, Lancaster, England.
   [Fox, Emily B.] Univ Washington, Comp Sci & Engn & Stat, Seattle, WA 98195 USA.
RP Baker, J (reprint author), Univ Lancaster, STOR I CDT, Math & Stat, Lancaster, England.
EM j.baker1@lancaster.ac.uk; p.fearnhead@lancaster.ac.uk; ebfox@uw.edu;
   c.nemeth@lancaster.ac.uk
FU EPSRC [EP/L015692/1, EP/K014463/1, EP/R018561/1, EP/S00159X/1,
   EP/R01860X/1]; ONR [N00014-15-1-2380]; NSF CAREER Award [IIS-1350133]
FX Jack Baker gratefully acknowledges the support of the EPSRC funded
   EP/L015692/1 STOR-i Centre for Doctoral Training Paul Fearnhead was
   supported by EPSRC grants EP/K014463/1 and EP/R018561/1. Christopher
   Nemeth acknowledges the support of EPSRC grants EP/S00159X/1 and
   EP/R01860X/1. Emily Fox acknowledges the support of ONR Grant
   N00014-15-1-2380 and NSF CAREER Award IIS-1350133.
CR Baker J., 2017, CONTROL VARIATES STO
   BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Breese J. S., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P43
   Chatterji N. S., 2018, THEORY VARIANCE REDU
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   COX JC, 1985, ECONOMETRICA, V53, P385, DOI 10.2307/1911242
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154
   Dunson DB, 2009, J AM STAT ASSOC, V104, P1042, DOI 10.1198/jasa.2009.tm08439
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Kalli M, 2011, STAT COMPUT, V21, P93, DOI 10.1007/s11222-009-9150-y
   Liverani S, 2015, J STAT SOFTW, V64, P1
   Ma Y.-A., 2015, ADV NEURAL INFORM PR, V2, P2917
   Nagapetyan T., 2017, TRUE COST STOCHASTIC
   Papaspiliopoulos O., 2008, TECHNICAL REPORT
   Papaspiliopoulos O, 2008, BIOMETRIKA, V95, P169, DOI 10.1093/biomet/asm086
   ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394
   Sato I., 2014, P 31 INT C MACH LEAR, V32, P982
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   SungjinAhn M. W, 2016, P 19 INT C ART INT S, P723
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Walker SG, 2007, COMMUN STAT-SIMUL C, V36, P45, DOI 10.1080/03610910601096262
   Wallach Hanna M., 2009, P 26 ANN INT C MACH, P1105, DOI DOI 10.1145/1553374.1553515
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Zygalakis KC, 2011, SIAM J SCI COMPUT, V33, P102, DOI 10.1137/090762336
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001028
DA 2019-06-15
ER

PT S
AU Bakshi, A
   Woodruff, DP
AF Bakshi, Ainesh
   Woodruff, David P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Sublinear Time Low-Rank Approximation of Distance Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHMS
AB Let P = {p(1), p(2), . . . p(n)} and Q = {q(1),q(2 ). . . q(m)} be two point sets in an arbitrary metric space. Let A represent the m x n pairwise distance matrix with A(i,j) = d(p(i) , q(j)). Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric d, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices A, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if P = Q and d is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the singular value decomposition (SVD) and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about 8-20 times faster than input sparsity methods on real-world and and synthetic datasets of size 10(8). Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms.
C1 [Bakshi, Ainesh; Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Bakshi, A (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM abakshi@cs.cmu.edu; dwoodruf@cs.cmu.edu
FU National Science Foundation [CCF-1815840]
FX The authors thank partial support from the National Science Foundation
   under Grant No. CCF-1815840. Part of this work was done while the author
   was visiting the Simons Institute for the Theory of Computing.
CR Bourgain Jean, 2015, P 47 ANN ACM S THEOR, P499
   Cattral Robert, 2002, RECENT ADV COMPUTERS, V1, P296
   Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Cohen M., 2016, P 27 ANN ACM SIAM S, P278
   Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758
   Demaine ED, 2009, COMP GEOM-THEOR APPL, V42, P429, DOI 10.1016/j.comgeo.2008.04.005
   Dokmanic I, 2015, IEEE SIGNAL PROC MAG, V32, P12, DOI 10.1109/MSP.2015.2398954
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Guyon I, 2005, ADV NEURAL INFORM PR, V17, P545
   Jain Viren, 2004, DEP PAPERS CIS
   Meng X, 2013, P 45 ANN ACM S THEOR, P91
   Musco C, 2017, ANN IEEE SYMP FOUND, P672, DOI 10.1109/FOCS.2017.68
   Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Weinberger KQ, 2004, PROC CVPR IEEE, P988
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303075
DA 2019-06-15
ER

PT S
AU Balaji, Y
   Sankaranarayanan, S
   Chellappa, R
AF Balaji, Yogesh
   Sankaranarayanan, Swami
   Chellappa, Rama
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI MetaReg: Towards Domain Generalization using Meta-Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.
C1 [Balaji, Yogesh] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Sankaranarayanan, Swami] Butterfly Network Inc, New York, NY USA.
   [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
RP Balaji, Y (reprint author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM yogesh@cs.umd.edu; swamiviv@butterflynetinc.com; rama@umiacs.umd.edu
RI Chellappa, Rama/B-6573-2012
FU MURI from the Army Research Office [W911NF-17-1-0304]; US DOD; UK
   Engineering and Physical Research Council (EPSRC) under the
   Multidisciplinary University Research Initiative
FX This reseach was supported by MURI from the Army Research Office under
   the Grant No. W911NF-17-1-0304. This is part of the collaboration
   between US DOD, UK MOD and UK Engineering and Physical Research Council
   (EPSRC) under the Multidisciplinary University Research Initiative.
CR Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Blitzer John, 2006, P 2006 C EMP METH NA
   Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Chen Minmin, 2012, ICML
   Chen Minmin, 2011, ADV NEURAL INFORM PR, V24
   Daume Hal, 2007, P 45 ANN M ASS COMP
   Finn  C., 2017, P 34 INT C MACH LEAR, P1126
   Finn Chelsea, 2017, P MACH LEARN RES
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2014, INT CONF ACOUST SPEE
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gopalan R, 2011, PROC CVPR IEEE
   He  K., 2015, ARXIV151203385
   Hospedales Timothy M., 2017, CORR
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Khosla Aditya, 2012, P 12 EUR C COMP VIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Krogh A., 1992, NEURAL INFORM PROCES, P950
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li Ke, 2017, CORR
   Mingsheng L., 2015, INT C MACH LEARN, P97
   Muandet K., 2013, P 30 INT C MACH LEAR, P10
   Ni J, 2013, PROC CVPR IEEE, P692, DOI 10.1109/CVPR.2013.95
   Ravi S., 2017, INT C LEARN REPR ICL
   Sankaranarayanan Swami, 2018, IEEE C COMP VIS PATT
   Schmidhuber Jurgen, 1995, TECHNICAL REPORT
   Srivastava N., 2014, J MACH LEARN RES, V15
   Teney Damien, 2017, CORR
   Thrun S., 1998, LEARNING LEARN
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Zhang Chiyuan, 2017, ICLR
   Zhang Y., 2017, P IEEE INT C COMP VI, V2, P6
   Zhou Fengwei, 2018, CORR
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301003
DA 2019-06-15
ER

PT S
AU Balasubramanian, K
   Ghadimi, S
AF Balasubramanian, Krishnakumar
   Ghadimi, Saeed
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Zeroth-order (Non)-Convex Stochastic Optimization via Conditional
   Gradient and Gradient Updates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the step-size. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate depends only poly-logarithmically on the dimensionality.
C1 [Balasubramanian, Krishnakumar] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.
   [Ghadimi, Saeed] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Balasubramanian, K (reprint author), Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.
EM kbala@ucdavis.edu; sghadimi@princeton.edu
CR Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chen P. - Y., 2017, P 10 ACM WORKSH ART, P15
   Choromanski Krzysztof, 2018, P 35 INT C MACH LEAR
   Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1
   Demyanov V., 1970, APPROXIMATE METHODS
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Ghadimi Saeed, 2018, MATH PROGRAMMING
   Goodfellow I., 2016, DEEP LEARNING, V1
   Hazan E., 2016, P 33 INT C MACH LEAR, V48, P1263
   Hazan Elad, 2012, P 29 INT COF INT C M, P1843
   Hearn Donald, 1982, OPERATIONS RES LETT, V2
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058
   Jamieson K. G., 2012, ADV NEURAL INFORM PR, P2672
   Lan GG, 2016, SIAM J OPTIMIZ, V26, P1379, DOI 10.1137/140992382
   Mania Horia, 2018, ADV NEURAL INFORM PR
   Mockus J., 2012, BAYESIAN APPROACH GL, V37
   Mokhtari Aryan, 2018, INT C ART INT STAT, P1886
   Mokhtari Aryan, 2018, ARXIV180409554
   Nemirovski A. S., 1983, WILEY INTERSCIENCE S, VXV
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2
   Reddi SJ, 2016, ANN ALLERTON CONF, P1244, DOI 10.1109/ALLERTON.2016.7852377
   Rubinstein R. Y., 2016, SIMULATION MONTE CAR, V10
   Salimans T., 2017, ARXIV170303864
   Shamir O., 2013, COLT 2013, P3
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Spall J. C., 2005, INTRO STOCHASTIC SEA, V65
   Wang Yining, 2018, P 21 INT C ART INT S
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303045
DA 2019-06-15
ER

PT S
AU Balcan, MF
   Dick, T
   White, C
AF Balcan, Maria-Florina
   Dick, Travis
   White, Colin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Data-Driven Clustering via Parameterized Lloyd's Families
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Algorithms for clustering points in metric spaces is a long-studied area of research. Clustering has seen a multitude of work both theoretically, in understanding the approximation guarantees possible for many objective functions such as k-median and k-means clustering, and experimentally, in finding the fastest algorithms and seeding procedures for Lloyd's algorithm. The performance of a given clustering algorithm depends on the specific application at hand, and this may not be known up front. For example, a "typical instance" may vary depending on the application, and different clustering heuristics perform differently depending on the instance. In this paper, we define an infinite family of algorithms generalizing Lloyd's algorithm, with one parameter controlling the initialization procedure, and another parameter controlling the local search procedure. This family of algorithms includes the celebrated k-means++ algorithm, as well as the classic farthest-first traversal algorithm. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal clustering algorithm from the class. We show the best parameters vary significantly across datasets such as MNIST, CIFAR, and mixtures of Gaussians. Our learned algorithms never perform worse than k-means++, and on some datasets we see significant improvements.
C1 [Balcan, Maria-Florina; Dick, Travis; White, Colin] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Balcan, MF (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM ninamf@cs.cmu.edu; tdick@cs.cmu.edu; crwhite@cs.cmu.edu
FU NSF [CCF-1535967, IIS-1618714]; Amazon Research Award; Microsoft
   Research Faculty Fellowship; National Defense Science & Engineering
   Graduate (NDSEG) fellowship
FX This work was supported in part by NSF grants CCF-1535967, IIS-1618714,
   an Amazon Research Award, a Microsoft Research Faculty Fellowship, a
   National Defense Science & Engineering Graduate (NDSEG) fellowship, and
   by the generosity of Eric and Wendy Schmidt by recommendation of the
   Schmidt Futures program.
CR Ahmadian  Sara, 2017, P ANN S FDN COMP SCI
   Arai  Kohei, 2007, FS ENG SAG U, V36, P25
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Arya V, 2004, SIAM J COMPUT, V33, P544, DOI [10.1137/S0097539702416402, 10.1137/S00097539702416402]
   Ashtiani  Hassan, 2015, P ANN C UNC ART INT, P82
   Balcan  Maria-Florina, 2017, C LEARN THEOR, P213
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Byrka J., 2015, P 26 ANN ACM SIAM S, P737, DOI 10.1137/1.9781611973730.50
   Charikar M., 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P1, DOI 10.1145/301250.301257
   Cohen MB, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P9, DOI 10.1145/2897518.2897647
   Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcis.2004.10.006
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5
   Higgs RE, 1997, J CHEM INF COMP SCI, V37, P861, DOI 10.1021/ci9702858
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kaufman L, 2009, FINDING GROUPS DATA, V344
   Kobren  Ari, 2017, P ANN C KNOWL DISC D
   Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548
   Ostrovsky R, 2012, J ACM, V59, DOI 10.1145/2395116.2395117
   Pelleg D., 1999, P 5 ACM SIGKDD INT C, P277, DOI DOI 10.1145/312129.312248
   Pena JM, 1999, PATTERN RECOGN LETT, V20, P1027, DOI 10.1016/S0167-8655(99)00069-0
   Pruitt KD, 2012, NUCLEIC ACIDS RES, V40, pD130, DOI 10.1093/nar/gkr1079
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Wenhao Jiang, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P789, DOI 10.1007/978-3-642-33486-3_50
   Yang Q, 2009, P JOINT C 47 ANN M A, P1
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005024
DA 2019-06-15
ER

PT S
AU Baldi, P
   Vershynin, R
AF Baldi, Pierre
   Vershynin, Roman
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neuronal Capacity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models: linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive some capacity estimates and bounds for fully recurrent networks, as well as feedforward networks.
C1 [Baldi, Pierre] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
   [Vershynin, Roman] Univ Calif Irvine, Dept Math, Irvine, CA 92697 USA.
RP Baldi, P (reprint author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
EM pfbaldi@uci.edu; rvershyn@uci.edu
FU NSF [1839429]; DARPA [D17AP00002]; AFOSR [FA9550-18-1-0031]
FX Work in part supported by grants NSF 1839429 and DARPA D17AP00002 to PB,
   and AFOSR FA9550-18-1-0031 to RV.
CR Anthony M., 2001, DISCRETE MATH NEURAL, V8
   ASPNES J, 1994, COMBINATORICA, V14, P135, DOI 10.1007/BF01215346
   BALDI P, 1988, IEEE T INFORM THEORY, V34, P523, DOI 10.1109/18.6032
   Baldi P., 2018, CAPACITY NEURAL NETW
   Baldi P., 2018, ARXIV180310868
   Chow C.K., 1961, Switching Circuit Theory and Logical Design Conference, P34
   COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264136
   MUROGA S, 1965, IEEE TRANS ELECTRON, VEC14, P136, DOI 10.1109/PGEC.1965.263958
   O'Donnell R, 2008, J COMPUT SYST SCI, V74, P298, DOI 10.1016/j.jcss.2007.06.021
   O'Donnell R, 2010, COMBINATORICA, V30, P327, DOI 10.1007/s00493-010-2173-3
   Saks M., 1993, LONDON MATH SOC LECT, V1993, P211
   WANG C, 1991, DISCRETE APPL MATH, V31, P51, DOI 10.1016/0166-218X(91)90032-R
   Zuev Y. A., 1989, SOV MATH DOKL, V39, P512
   Zuev Yu. A., 1991, DISCRETE MATH, V3, P47
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852002029
DA 2019-06-15
ER

PT S
AU Balduzzi, D
   Tuyls, K
   Perolat, J
   Graepel, T
AF Balduzzi, David
   Tuyls, Karl
   Perolat, Julien
   Graepel, Thore
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Re-evaluating Evaluation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ARCADE LEARNING-ENVIRONMENT; INTELLIGENCE; PERFORMANCE; GAME; DYNAMICS;
   PAPER; GO
AB Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation - since there is no harm (computational cost aside) from including all available tasks and agents.
C1 [Balduzzi, David; Tuyls, Karl; Perolat, Julien; Graepel, Thore] DeepMind, London, England.
RP Balduzzi, D (reprint author), DeepMind, London, England.
EM dbalduzzi@google.com; karltuyls@google.com; perolat@google.com;
   thore@google.com
CR Balduzzi D, 2018, ICML
   Balsubramani A., 2016, COLT
   Beattie C., 2016, ARXIV161203801
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818
   Bringsjord S, 2011, J EXP THEOR ARTIF IN, V23, P271, DOI 10.1080/0952813X.2010.502314
   Brockman G., 2016, OPENAI GYM
   Candogan O., 2013, ACM T EC COMP, V1
   Candogan O, 2013, GAME ECON BEHAV, V82, P66, DOI 10.1016/j.geb.2013.07.001
   Candogan O, 2011, MATH OPER RES, V36, P474, DOI 10.1287/moor.1110.0500
   CHAITIN GJ, 1966, J ACM, V13, P547, DOI 10.1145/321356.321363
   Deng J., 2009, CVPR
   Diaconis P, 1988, GROUP REPRESENTATION
   Donoho D., 2015, BAS PRES TUK CENT WO
   Dudik M., 2015, COLT
   Elo AE, 1978, RATING CHESS PLAYERS
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010
   Frean M, 2001, P ROY SOC B-BIOL SCI, V268, P1323, DOI 10.1098/rspb.2001.1670
   Freund Y., 1996, J COMPUTER SYSTEM SC
   HAMBLETON R. K., 1991, FUNDAMENTALS ITEM RE
   Hartford J., 2018, ICML
   Herbrich R., 2007, NIPS
   Hernandez-Orallo J., 2017, MEASURE ALL MINDS EV
   Hernandez-Orallo J, 2017, ARTIF INTELL REV, V48, P397, DOI 10.1007/s10462-016-9505-7
   Hernandez-Orallo J, 2012, J MACH LEARN RES, V13, P2813
   Hessel M., 2018, AAAI
   Hofbauer J, 2002, ECONOMETRICA, V70, P2265, DOI 10.1111/j.1468-0262.2002.00440.x
   Horn B., 2014, FDN DIGITAL GAMES
   Hunter DR, 2004, ANN STAT, V32, P384
   Jaderberg M., 2017, ABS171109846 CORR
   Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x
   Jordan P. R., 2010, THESIS
   Jordan P. R., 2007, 6 INT JOINT C AUT AG, P193
   Kerr B, 2002, NATURE, V418, P171, DOI 10.1038/nature00823
   Kolmogorov A.N., 1965, Problems of Information Transmission, V1, P1
   Kondor R, 2008, THESIS
   Kondor R., 2003, ICML
   Kondor R., 2018, NIPS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kurakin Alexey, 2018, ARXIV180400097
   Laird R. A., 2006, AM NATURALIST, V168
   Lanctot M., 2017, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Legg S., 2005, IJCAI
   Legg S., 2013, ALGORITHMIC PROBABIL
   Leibo J. Z., 2018, ARXIV180108116
   Liapis A., 2013, ARTIFICIAL INTELLIGE
   Machado MC, 2018, J ARTIF INTELL RES, V61, P523, DOI 10.1613/jair.5699
   Martinez-Plumed F., 2017, WORKSH EV GEN PURP A
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Nielsen T. S., 2015, EVOAPPLICATIONS
   Olson RS, 2017, BIODATA MIN, V10, DOI 10.1186/s13040-017-0154-4
   Ortiz L. E., 2007, AISTATS
   Ortiz L. E., 2006, TR200621 CSAIL MIT
   Ostrovski G., 2017, ICML
   Phelps Steve, 2007, 2 INT C PERS TECHN, P188
   Phelps Steve, 2004, 2 INT C PERS TECHN, P101
   Ponsen M, 2009, ENTERTAIN COMPUT, V1, P39, DOI 10.1016/j.entcom.2009.09.002
   Sandholm W. H, 2010, POPULATION GAMES EVO
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Silva F. de Mesentier, 2017, FDN DIGITAL GAMES FD
   Silver D., 2017, ARXIV171201815
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P224, DOI 10.1016/S0019-9958(64)90131-7
   Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107
   Sukhbaatar S., 2017, ICLR
   Szegedy C, 2013, ARXIV13126199
   Szolnoki A, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0735
   Todorov Emanuel, 2012, IROS
   Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559
   Tramer  Florian, 2018, ICLR
   Tuyls Karl, 2018, AAMAS
   Uesato J., 2018, ICML
   van Hasselt H., 2016, NIPS
   Vandenberg RJ, 2000, ORGAN RES METHODS, V3, P4, DOI 10.1177/109442810031002
   Volz V., 2018, GECCO
   Walsh W. E., 2003, P 5 WORKSH AG MED EL
   Wang Z., 2016, ICML
   Wellman M. P., 2006, P AAAI, P1552
   Woolley AW, 2010, SCIENCE, V330, P686, DOI 10.1126/science.1193147
   Zaheer M., 2017, NIPS
NR 83
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303028
DA 2019-06-15
ER

PT S
AU Balkanski, E
   Breuer, A
   Singer, Y
AF Balkanski, Eric
   Breuer, Adam
   Singer, Yaron
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Non-monotone Submodular Maximization in Exponentially Fewer Iterations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB In this paper we consider parallelization for applications whose objective can be expressed as maximizing a non-monotone submodular function under a cardinality constraint. Our main result is an algorithm whose approximation is arbitrarily close to 1/2e in O( log(2) n) adaptive rounds, where n is the size of the ground set. This is an exponential speedup in parallel running time over any previously studied algorithm for constrained non-monotone submodular maximization. Beyond its provable guarantees, the algorithm performs well in practice. Specifically, experiments on traffic monitoring and personalized data summarization applications show that the algorithm finds solutions whose values are competitive with state-of-the-art algorithms while running in exponentially fewer parallel iterations.
C1 [Balkanski, Eric; Breuer, Adam; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
RP Balkanski, E (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM ericbalkanski@g.harvard.edu; breuer@g.harvard.edu;
   yaron@seas.harvard.edu
FU Google PhD Fellowship; NSF grant CAREER [CCF 1452961]; NSF [CCF
   1301976]; BSF grant [2014389]; NSF USICCS proposal [1540428]; Google
   Research award; Facebook research award
FX This research was supported by a Google PhD Fellowship, NSF grant CAREER
   CCF 1452961, NSF CCF 1301976, BSF grant 2014389, NSF USICCS proposal
   1540428, a Google Research award, and a Facebook research award.
CR AGARWAL A., 2017, COLT, P39
   Balkanski Eric, 2018, ARXIV180406355
   Balkanski Eric, 2018, STOC
   Balkanski Eric, 2018, ICML
   Barbosa RD, 2016, ANN IEEE SYMP FOUND, P645, DOI 10.1109/FOCS.2016.74
   BERGER B, 1989, ANN IEEE SYMP FOUND, P54, DOI 10.1109/SFCS.1989.63455
   Blelloch G. E., 1998, SPAA '98. Tenth Annual ACM Symposium on Parallel Algorithms and Architectures, P16
   Blelloch GE, 1996, COMMUN ACM, V39, P85, DOI 10.1145/227234.227246
   Blelloch GE, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P23
   Blelloch Guy E, 2012, SPAA, P82
   Braverman M, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P851, DOI 10.1145/2897518.2897642
   Buchbinder N, 2014, P 25 ANN ACM SIAM S, V25, P1433
   Buhrman Harry, 2012, ARXIV12093849
   CalTrans, PEMS CAL PERF MEAS S
   Canonne Clement, 2017, ARXIV170205678
   Chekuri Chandra, 2015, P 2015 C INN THEOR C, P201
   Chen Xi, 2017, ARXIV170406314
   Chierichetti F, 2010, WWW, P231
   COLE R, 1988, SIAM J COMPUT, V17, P770, DOI 10.1137/0217049
   Duris Pavol, 1984, STOC, P81
   Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34
   Ene Alina, 2018, ARXIV180405379
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46
   Feldman Moran, 2015, KNOWLEDGE INFORM SYS, V42
   Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098
   Gupta A, 2010, LECT NOTES COMPUT SC, V6484, P246, DOI 10.1007/978-3-642-17572-5_20
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Haupt Jarvis D, 2009, 2009 43rd Asilomar Conference on Signals, Systems and Computers, P1551, DOI 10.1109/ACSSC.2009.5470138
   Haupt J, 2009, 2009 IEEE 13TH DIGITAL SIGNAL PROCESSING WORKSHOP & 5TH IEEE PROCESSING EDUCATION WORKSHOP, VOLS 1 AND 2, PROCEEDINGS, P702, DOI 10.1109/DSP.2009.4786013
   Indyk P, 2011, ANN IEEE SYMP FOUND, P285, DOI 10.1109/FOCS.2011.83
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kumar  Ravi, 2015, ACM T PARALLEL COMPU, V2, P14
   Lee J, 2009, ACM S THEORY COMPUT, P323
   Mirrokni V. S., 2015, P 47 ANN ACM S THEOR, P153
   Mirzasoleiman B., 2013, ADV NEURAL INFORM PR, P2049
   Mirzasoleiman Baharan, 2016, ICML, P1358
   Namkoong Hongseok, 2017, ICML, P2574
   Nisan Noam, 1991, P 23 ANN ACM S THEOR, P419, DOI DOI 10.1145/103418.103463
   PAPADIMITRIOU CH, 1984, J COMPUT SYST SCI, V28, P260, DOI 10.1016/0022-0000(84)90069-2
   Rajagopalan S, 1998, SIAM J COMPUT, V28, P526
   Valiant L. G., 1975, SIAM Journal on Computing, V4, P348, DOI 10.1137/0204030
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302037
DA 2019-06-15
ER

PT S
AU Balle, B
   Barthe, G
   Gaboardi, M
AF Balle, Borja
   Barthe, Gilles
   Gaboardi, Marco
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Privacy Amplification by Subsampling: Tight Analyses via Couplings and
   Divergences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID SAMPLE COMPLEXITY; BOUNDS
AB Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called "privacy amplification by subsampling" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.
C1 [Balle, Borja] Amazon Res, Cambridge, England.
   [Barthe, Gilles] IMDEA Software Inst, Madrid, Spain.
   [Gaboardi, Marco] SUNY Buffalo, Buffalo, NY USA.
RP Balle, B (reprint author), Amazon Res, Cambridge, England.
EM pigem@amazon.co.uk; gilles.barthe@imdea.org; gaboardi@buffalo.edu
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Balle Borja, 2018, P 35 INT C MACH LEAR
   Barthe G, 2016, PROCEEDINGS OF THE 31ST ANNUAL ACM-IEEE SYMPOSIUM ON LOGIC IN COMPUTER SCIENCE (LICS 2016), P749, DOI 10.1145/2933575.2934554
   Barthe G, 2012, POPL 12: PROCEEDINGS OF THE 39TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P97
   Bartolomeo G, 2013, IDENTIFICATION AND MANAGEMENT OF DISTRIBUTED DATA: NGN, CONTENT-CENTRIC NETWORKS AND THE WEB, P49
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Beimel A., 2013, ITCS, P97, DOI DOI 10.1145/2422436.2422450
   Beimel A, 2014, MACH LEARN, V94, P401, DOI 10.1007/s10994-013-5404-1
   Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437
   Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24
   Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45
   Bun Mark, 2018, S THEOR COMP STOC
   Chaudhuri K, 2006, LECT NOTES COMPUT SC, V4117, P198
   Dwork C., 2016, ARXIV160301887
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Jalko Joonas, 2017, P 33 C UNC ART INT U
   Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505
   Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090
   Li  N., 2012, P 7 ACM S INF COMP C, P32, DOI DOI 10.1145/2414456.2414474
   Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11
   Murtagh J, 2016, LECT NOTES COMPUT SC, V9562, P157, DOI 10.1007/978-3-662-49096-9_7
   Osterreicher Ferdinand, 2002, RGMIA RES REP COLL
   Park Mijung, 2016, CORR
   Sason I, 2016, IEEE T INFORM THEORY, V62, P5973, DOI 10.1109/TIT.2016.2603151
   Ullman Jonathan, 2017, CS7880 RIGOROUS APPR
   Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7
   Wang Y.-X., 2015, P 32 INT C MACH LEAR, P2493
   Wang Yu- Xiang, 2019, P 22 INT C ART INT S
   Wang YJ, 2016, J MACH LEARN RES, V17, P1
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852000075
DA 2019-06-15
ER

PT S
AU Balunovic, M
   Bielik, P
   Vechev, M
AF Balunovic, Mislav
   Bielik, Pavol
   Vechev, Martin
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning to Solve SMT Formulas
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ALGORITHM; CONFIGURATION
AB We present a new approach for learning to solve SMT formulas. We phrase the challenge of solving SMT formulas as a tree search problem where at each step a transformation is applied to the input formula until the formula is solved. Our approach works in two phases: first, given a dataset of unsolved formulas we learn a policy that for each formula selects a suitable transformation to apply at each step in order to solve the formula, and second, we synthesize a strategy in the form of a loop-free program with branches. This strategy is an interpretable representation of the policy decisions and is used to guide the SMT solver to decide formulas more efficiently, without requiring any modification to the solver itself and without needing to evaluate the learned policy at inference time. We show that our approach is effective in practice - it solves 17% more formulas over a range of benchmarks and achieves up to 100 x runtime improvement over a state-of-the-art SMT solver.
C1 [Balunovic, Mislav; Bielik, Pavol; Vechev, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Balunovic, M (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM bmislav@ethz.ch; pavol.bielik@inf.ethz.ch; martin.vechev@inf.ethz.ch
CR Alemi A. A., 2016, P 30 INT C NEUR INF, P2243
   Amadini R, 2014, THEOR PRACT LOG PROG, V14, P509, DOI 10.1017/S1471068414000179
   Ansotegui C, 2009, LECT NOTES COMPUT SC, V5732, P142
   Barrett Clark, 2011, Computer Aided Verification. Proceedings 23rd International Conference, CAV 2011, P171, DOI 10.1007/978-3-642-22110-1_14
   Barrett C., 2016, SATISFIABILITY MODUL
   Barrett C., 2016, APROVE BENCHMARKS
   Barrett C., 2016, CORE BENCHMARKS
   Barrett C., 2016, HYCOMP BENCHMARKS
   Barrett C., 2016, LEIPZIG BENCHMARKS
   Barrett C., 2016, SAGE2 BENCHMARKS
   Bojanowski P, 2016, TACL, V5, P135, DOI [DOI 10.1162/TACL_, DOI 10.1162/tacl_a_00051]
   Bridge D., 2012, AUTONOMOUS SEARCH, P73
   Cimatti A., 2013, LNCS, V7795
   Clare A., 2001, LNCS LNAI, V2168, P42, DOI DOI 10.1007/3-540-44794-6_4
   de Moura Leonardo, 2013, Automated Reasoning and Mathematics. Essays in Memory of William W. McCune, P15, DOI 10.1007/978-3-642-36675-8_2
   de Moura L, 2008, LECT NOTES COMPUT SC, V4963, P337, DOI 10.1007/978-3-540-78800-3_24
   De Moura L, 2011, COMMUN ACM, V54, P69, DOI 10.1145/1995376.1995394
   Dutertre Bruno, 2014, Computer Aided Verification. 26th International Conference, CAV 2014, Held as Part of the Vienna Summer of Logic, VSL 2014. Proceedings: LNCS 8559, P737, DOI 10.1007/978-3-319-08867-9_49
   Giesl J, 2017, J AUTOM REASONING, V58, P3, DOI 10.1007/s10817-016-9388-y
   Glorot X., 2010, P INT C ART INT STAT
   Godefroid P., 2008, AUTOMATED WHITEBOX F
   Hurley Barry, 2014, Integration of AI and OR Techniques in Constraint Programming. 11th International Conference, CPAIOR 2014. Proceedings: LNCS 8451, P301, DOI 10.1007/978-3-319-07046-9_22
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Hutter F, 2010, LECT NOTES COMPUT SC, V6073, P281, DOI 10.1007/978-3-642-13800-3_30
   Hutter F, 2009, J ARTIF INTELL RES, V36, P267, DOI 10.1613/jair.2861
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427
   Kadioglu S., 2010, ECAI, V215, P751
   Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5
   Khalil E. B., 2016, P AAAI, P724
   KhudaBukhsh AR, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P517
   Kingma D. P., 2014, ABS14126980 CORR
   Lagoudakis M. G., 2001, ELECT NOTES DISCRETE, V9, P344
   Loos S. M., 2017, EPIC SERIES COMPUTIN, V46, P85
   Loth M, 2013, LECT NOTES COMPUT SC, V8124, P464, DOI 10.1007/978-3-642-40627-0_36
   Niemetz A., 2014, JSAT, V9, P53
   Nudelman E, 2004, LECT NOTES COMPUT SC, V3258, P438
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Ramirez NG, 2016, PROC INT C TOOLS ART, P247, DOI [10.1109/ICTAI.2016.0046, 10.1109/ICTAI.2016.43]
   Ross S., 2011, J MACHINE LEARNING R, P627
   Samulowitz H., 2007, AAAI, V22, P255
   Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28
   Wang M., 2017, ADV NEURAL INFORM PR, V30, P2786
   Xu L, 2008, J ARTIF INTELL RES, V32, P565, DOI 10.1613/jair.2490
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852004083
DA 2019-06-15
ER

PT S
AU Bambach, S
   Crandall, DJ
   Smith, LB
   Yu, C
AF Bambach, Sven
   Crandall, David J.
   Smith, Linda B.
   Yu, Chen
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Toddler-Inspired Visual Object Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Real-world learning systems have practical limitations on the quality and quantity of the training datasets that they can collect and consider. How should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) images that represent a highly accurate approximation of the "training data" that toddler's visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of these data, and found that child data produce significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that child data exhibit a unique combination of quality and diversity, with not only many similar large, high-quality object views but also a greater number and diversity of rare views. This novel methodology of analyzing the visual "training data" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology.
C1 [Bambach, Sven; Crandall, David J.] Indiana Univ, Sch Informat Comp & Engn, Bloomington, IN 47405 USA.
   [Smith, Linda B.; Yu, Chen] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.
RP Bambach, S (reprint author), Indiana Univ, Sch Informat Comp & Engn, Bloomington, IN 47405 USA.
EM sbambach@iu.edu; djcran@iu.edu; smith4@iu.edu; chenyu@iu.edu
FU National Science Foundation [CAREER IIS-1253549]; National Institutes of
   Health [R01 HD074601, R01 HD093792]; IU Office of the Vice Provost for
   Research, the College of Arts and Sciences, and the School of
   Informatics, Computing, and Engineering through the Emerging Areas of
   Research Project "Learning: Brains, Machines, and Children"
FX This work was supported by the National Science Foundation (CAREER
   IIS-1253549) and the National Institutes of Health (R01 HD074601, R01
   HD093792), as well as the IU Office of the Vice Provost for Research,
   the College of Arts and Sciences, and the School of Informatics,
   Computing, and Engineering through the Emerging Areas of Research
   Project "Learning: Brains, Machines, and Children." We would like to
   thank Drew Abney, Esther Chen, Steven Elmlinger, Seth Foster, Lauren
   Slone, Catalina Suarez, Charlene Tay, and Yayun Zhang for helping with
   the collection of the first-person toy play dataset.
CR Bambach  Sven, 2016, ANN C COGN SCI SOC C
   Cevallos  Alfonso, 2017, P 28 ANN ACM SIAM S
   Charikar  Moses, 1999, P 31 ANN ACM S THEOR
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Clerkin EM, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0055
   Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748
   Frank MC, 2017, J CHILD LANG, V44, P677, DOI 10.1017/S0305000916000209
   Fu YF, 2013, IEEE T CYBERNETICS, V43, P464, DOI 10.1109/TSMCB.2012.2209177
   Gathercole  Chris, 1994, P INT C EV COMP 3 C
   Greene MR, 2016, J EXP PSYCHOL GEN, V145, P82, DOI 10.1037/xge0000129
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   LANDAU B, 1988, COGNITIVE DEV, V3, P299, DOI 10.1016/0885-2014(88)90014-7
   Lin  Hui, 2009, 10 ANN C INT SPEECH
   Mahajan  Dhruv, 2018, ARXIV180500932
   Nguyen Hieu T., 2004, P 21 INT C MACH LEAR
   Nosofsky Robert M, 2018, PSYCHONOMIC B REV, P1
   Perry Jeffrey S., 2002, HUMAN VISION ELECT I
   Peterson J. C., 2016, ARXIV160802164
   Plutowski  Mark, 1994, ADV NEURAL INFORM PR, P1135
   Prasad  Adarsh, 2014, ARXIV14111752
   Redmon  J., 2016, IEEE C COMP VIS PATT
   Ritter  Samuel, 2017, ARXIV170608606
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smith LB, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02124
   Torralba  Antonio, 2003, INT J COMPUTER VISIO
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang  Panqu, 2013, ANN C COGN SCI SOC C, V35
   Wei K., 2015, ICML, P1954
   Yu C, 2017, CHILD DEV, V88, P2060, DOI 10.1111/cdev.12730
   Yu C, 2016, CURR BIOL, V26, P1235, DOI 10.1016/j.cub.2016.03.026
   Yu C, 2012, COGNITION, V125, P244, DOI 10.1016/j.cognition.2012.06.016
   Zhu X, 2015, AAAI, P4083
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301021
DA 2019-06-15
ER

PT S
AU Banner, R
   Hubara, I
   Hoffer, E
   Soudry, D
AF Banner, Ron
   Hubara, Itay
   Hoffer, Elad
   Soudry, Daniel
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Scalable Methods for 8-bit Training of Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.
C1 [Banner, Ron] Intel, AIPG, Santa Clara, CA 95054 USA.
   [Hubara, Itay; Hoffer, Elad; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel.
RP Banner, R (reprint author), Intel, AIPG, Santa Clara, CA 95054 USA.
EM ron.banner@intel.com; itayhubara@gmail.com; elad.hoffer@gmail.com;
   daniel.soudry@gmail.com
FU Israel Science Foundation [31/1031]; Taub foundation
FX This research was supported by the Israel Science Foundation (grant No.
   31/1031), and by the Taub foundation. A Titan Xp used for this research
   was donated by the NVIDIA Corporation. The authors are pleased to
   acknowledge that the work reported in this paper was substantially
   performed at Intel Artificial Intelligence Products Group (AIPG).
CR Benoit J., 2017, GEMMLOWP SMALL SELF
   Biau G., 2015, MATH STAT LIMIT THEO, P21
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chen  W., 2015, INT C MACH LEARN, P2285
   Das D., 2018, MIXED PRECISION TRAI
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   Han S., 2015, ARXIV151000149
   Hoffer E., 2018, ARXIV180301814
   Hubara I., 2016, ADV NEURAL INFORM PR
   Hubara I., 2016, ARXIV160907061
   Ioffe S., 2015, ARXIV150203167
   Jaderberg M., 2014, ARXIV14053866
   Kamath G., 2015, BOUNDS EXPECTATION M
   Lin X., 2017, ADV NEURAL INFORM PR, P344
   Mishra A., 2017, ARXIV170901134
   Miyashita  D., 2016, ARXIV160301025
   Rodriguez A., 2018, LOWER NUMERICAL PREC
   Soudry D., 2014, ADV NEURAL INFORM PR, P963
   Ullrich K., 2017, ARXIV170204008
   Wen W., 2017, ADV NEURAL INFORM PR, P1508
   Wu S., 2018, INT C LEARN REPR ICL
   Wu Shuang, 2018, ARXIV180209769
   Wu Y., 2016, ARXIV160908144
   Zhou S., 2016, ARXIV160606160
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823305018
DA 2019-06-15
ER

PT S
AU Bansal, N
   Chen, XH
   Wang, ZY
AF Bansal, Nitin
   Chen, Xiaohan
   Wang, Zhangyang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Can We Gain More from Orthogonality Regularizations in Training Deep
   CNNs?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly
C1 [Bansal, Nitin; Chen, Xiaohan; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
RP Bansal, N (reprint author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
EM bansa01@tamu.edu; chernxh@tamu.edu; atlaswang@tamu.edu
FU NSF [RI-1755701]
FX The work by N. Bansal, X. Chen and Z. Wang is supported in part by NSF
   RI-1755701. We would also like to thank all anonymous reviewers for
   their tremendously useful comments to help improve our work.
CR Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Balestriero Randall, 2018, P 35 INT C MACH LEAR
   Balestriero Randall, 2018, ARXIV180506576
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Desjardins G., 2015, ADV NEURAL INFORM PR, V28, P2071
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dorobantu Victor, 2016, ARXIV161204035
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Harandi Mehrtash, 2016, ARXIV161105927
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jia Kui, 2016, CORR
   Keskar N. S., 2016, ARXIV160904836
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lei Huang, 2017, ARXIV170906079
   Lin  Z., 2015, ARXIV150803117
   Mhammedi Zakaria, 2016, ARXIV161200188
   Mishkin Dmytro, 2015, ARXIV151106422
   Miyato T., 2018, ARXIV180205957
   Ozay Mete, 2016, ARXIV161007008
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Rodriguez Pau, 2016, ARXIV161101967
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Saxe A.M., 2013, ARXIV13126120
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039
   Sun  Yifan, 2017, SVDNET PEDESTRIAN RE
   Vorontsov Eugene, 2017, ARXIV170200071
   Wang S., 2016, INT C MACH LEARN, P718
   Wang Zhangyang, 2018, ARXIV180405515
   Wang Zhaowen, 2016, SPARS COD ITS APPL
   Wisdom S., 2016, ADV NEURAL INFORM PR, P4880
   Xie Di, 2017, ARXIV170301827
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yoshida Yuichi, 2017, ARXIV170510941
   Zagoruyko S, 2016, ARXIV160507146
   Zhang T, 2011, IEEE T INFORM THEORY, V57, P6215, DOI 10.1109/TIT.2011.2162263
   Zhou JP, 2006, IEEE T IMAGE PROCESS, V15, P511, DOI 10.1109/TIP.2005.863046
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823304029
DA 2019-06-15
ER

PT S
AU Bartunov, S
   Santoro, A
   Richards, BA
   Marris, L
   Hinton, GE
   Lillicrap, TP
AF Bartunov, Sergey
   Santoro, Adam
   Richards, Blake A.
   Marris, Luke
   Hinton, Geoffrey E.
   Lillicrap, Timothy P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Assessing the Scalability of Biologically-Motivated Deep Learning
   Algorithms and Architectures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID BACKPROPAGATION
AB The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets, explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and examine performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.
C1 [Bartunov, Sergey; Santoro, Adam; Marris, Luke] DeepMind, London, England.
   [Richards, Blake A.] Univ Toronto, Toronto, ON, Canada.
   [Hinton, Geoffrey E.] Google Brain, Mountain View, CA USA.
   [Lillicrap, Timothy P.] UCL, DeepMind, London, England.
RP Bartunov, S (reprint author), DeepMind, London, England.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Almeida Luis B, 1990, ARTIFICIAL NEURAL NE, P102
   Bengio Y, 2014, ARXIV14077906
   Bengio Y., 2015, ARXIV151002777
   Bengio Y., 2015, ARXIV150204156
   Bengio Yoshua, 2016, ARXIV160601651
   Bengio Yoshua, 2017, NEURAL COMPUTATION
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Dumoulin V., 2016, ARXIV160307285
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1016/S0364-0213(87)80025-3
   Hinton G. E, 2007, NIPS 2007 DEEP LEARN
   Hinton Geoffrey E, 1988, NEURAL INFORMATION P, P358
   Kingma D. P., 2014, ARXIV14126980
   Kording KP, 2001, J COMPUT NEUROSCI, V11, P207, DOI 10.1023/A:1013776130161
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y., 1986, DISORDERED SYSTEMS B, P233
   Lecun Y., 1987, THESIS
   Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31
   Lillicrap T. P., 2014, ARXIV14110247
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Movellan Javier R, 1991, CONNECTIONIST MODELS, P10
   Nokland Arild, 2016, ADV NEURAL INFORM PR, P1037
   O'Reilly Randall C, 1996, NEURAL COMPUT, V8, P895
   Ororbia Alexander G, 2018, ARXIV180301834
   Ororbia Alexander G, 2018, ARXIV180511703
   Parisien C, 2008, NEURAL COMPUT, V20, P1473, DOI 10.1162/neco.2008.07-06-295
   Pineda F. J., 1988, Journal of Complexity, V4, P216, DOI 10.1016/0885-064X(88)90021-0
   PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229
   Roland B, 2016, ELIFE, V5, DOI 10.7554/eLife.16335
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sacramento J, 2017, ARXIV180100062
   Samadi Arash, 2017, NEURAL COMPUTATION
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Whittington James CR, 2017, NEURAL COMPUTATION
   Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988
NR 39
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852003088
DA 2019-06-15
ER

PT S
AU Bassily, R
   Thakkar, O
   Thakurta, A
AF Bassily, Raef
   Thakkar, Om
   Thakurta, Abhradeep
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Model-Agnostic Private Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We design differentially private learning algorithms that are agnostic to the learning model assuming access to a limited amount of unlabeled public data. First, we provide a new differentially private algorithm for answering a sequence of m online classification queries (given by a sequence of m unlabeled public feature vectors) based on a private training set. Our algorithm follows the paradigm of subsample-and-aggregate, in which any generic non-private learner is trained on disjoint subsets of the private training set, and then for each classification query, the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of the distance-to-instability framework [26], and the sparse-vector technique [15, 18]. We show that our algorithm makes a conservative use of the privacy budget. In particular, if the underlying non-private learner yields a classification error of at most alpha is an element of (0, 1), then our construction answers more queries, by at least a factor of 1/alpha in some cases, than what is implied by a straightforward application of the advanced composition theorem for differential privacy. Next, we apply the knowledge transfer technique to construct a private learner that outputs a classifier, which can be used to answer an unlimited number of queries. In the PAC model, we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases Similar to non-private sample complexity, our bounds are completely characterized by the VC dimension of the concept class.
C1 [Bassily, Raef] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Thakkar, Om] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
   [Thakurta, Abhradeep] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
RP Bassily, R (reprint author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM bassily.1@osu.edu; omthkkr@bu.edu; aguhatha@ucsc.edu
FU NSF [TRIPODS-1740850, TRIPODS+X-1839317, IIS-1447700]; Sloan foundation;
   OSU; UC Santa Cruz
FX The authors would like to thank Vitaly Feldman, and Adam Smith for
   helpful discussions during the course of this project. In particular,
   the authors are grateful for Vitaly's ideas about the possible
   extensions of the results in Section 4, which we outlined in Remarks 2
   and 3. This work is supported by NSF grants TRIPODS-1740850,
   TRIPODS+X-1839317, and IIS-1447700, a grant from the Sloan foundation,
   and start-up supports from OSU and UC Santa Cruz.
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Bassily Raef, 2018, ARXIV180305101
   Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437
   Beimel Amos, 2016, THEORY COMPUTING, V12, P1
   Beimel Amos, 2013, ITCS
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45
   Chaudhuri Kamalika, 2011, JMLR Workshop Conf Proc, V2011, P155
   Chaudhuri Kamalika, 2011, JMLR
   Dwork C., 2006, EUROCRYPT
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2009, ACM S THEORY COMPUT, P381
   Dwork Cynthia, 2018, ARXIV180310266
   Hamm J, 2016, P 33 INT C MACH LEAR, P555
   Hardt Moritz, 2010, FOCS
   Kasiviswanathan SP, 2008, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2008.27
   Kearns M. J., 1994, INTRO COMPUTATIONAL
   Kifer D., 2012, J MACHINE LEARNING R, V1, P41
   Nissim K, 2007, STOC
   Papernot Nicolas, 2017, STAT, P1050
   Papernot Nicolas, 2018, ARXIV180208908
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Smith Adam, 2013, COLT
   Talwar Kunal, 2015, NIPS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001063
DA 2019-06-15
ER

PT S
AU Bastani, O
   Pu, YW
   Solar-Lezama, A
AF Bastani, Osbert
   Pu, Yewen
   Solar-Lezama, Armando
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Verifiable Reinforcement Learning via Policy Extraction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.
C1 [Bastani, Osbert; Pu, Yewen; Solar-Lezama, Armando] MIT, Cambridge, MA 02139 USA.
RP Bastani, O (reprint author), MIT, Cambridge, MA 02139 USA.
EM obastani@csail.mit.edu; yewenpu@mit.edu; asolar@csail.mit.edu
FU Toyota Research Institute; NSF InTrans award [1665282]
FX This work was funded by the Toyota Research Institute and NSF InTrans
   award 1665282.
CR Abbeel P., 2004, ICML
   Akametalu Anayo K, 2014, CDC
   Aswani A., 2013, AUTOMATICA
   Ba Jimmy, 2014, NIPS
   Barto A., 1983, IEEE T SYSTEMS MAN C
   Bastani Osbert, 2017, FAT ML
   Bastani Osbert, 2016, NIPS
   Berkenkamp Felix, 2017, NIPS
   Breiman L., 1984, CLASSIFICATION REGRE
   Bucilua Cristian, 2006, KDD
   Collins Steve, 2005, SCIENCE
   de Moura Leonardo Mendonca, 2008, TACAS
   Ernst Damien, 2005, JMLR
   Garcia Javier, 2015, JMLR
   Gehr Timon, 2018, IEEE SECURITY PRIVAC
   Goodfellow I., 2015, ICLR
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Huang Xiaowei, 2017, CAV
   Katz Guy, 2017, CAV
   Kuindersma Scott, 2016, AUTONOMOUS ROBOTS
   Levine  S., 2013, P 30 INT C MACH LEAR, P1
   Mnih V., 2015, NATURE
   Moldovan Teodor Mihai, 2012, ICML
   Parrilo P. A., 2000, THESIS
   Ross Stephane, 2011, AISTATS
   Sadigh Dorsa, 2016, IROS
   Schaal Stefan, 1999, TRENDS COGNITIVE SCI
   Schulman  J., 2017, ARXIV170706347
   Silver D., 2016, NATURE
   Szegedy C., 2014, ICLR
   Tedrake Russ, 2010, IJRR
   Tedrake Russ, 2018, UNDERACTUATED ROBOTI
   Turchetta Matteo, 2016, NIPS
   Vandewiele Gilles, 2016, NIPS WORKSH INT MACH
   Verma Abhinav, 2018, ARXIV180402477
   Wu Yifan, 2016, ICML
   Ziebart Brian D, 2008, AAAI
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302050
DA 2019-06-15
ER

PT S
AU Belbute-Peres, FD
   Smith, KA
   Allen, KR
   Tenenbaum, JB
   Kolter, JZ
AF Belbute-Peres, Filipe de A.
   Smith, Kevin A.
   Allen, Kelsey R.
   Tenenbaum, Joshua B.
   Kolter, J. Zico
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI End-to-End Differentiable Physics for Learning and Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning. As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency. Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem. Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods. Code for the engine and experiments is included with the paper.(1)
C1 [Belbute-Peres, Filipe de A.; Kolter, J. Zico] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Smith, Kevin A.; Allen, Kelsey R.; Tenenbaum, Joshua B.] MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA.
   [Kolter, J. Zico] Bosch Ctr Artificial Intelligence, Pittsburgh, PA 15213 USA.
RP Belbute-Peres, FD (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM filiped@cs.cmu.edu; k2smith@mit.edu; krallen@mit.edu; jbt@mit.edu;
   zkolter@cs.cmu.edu
CR Abbeel Pieter, 2005, ADV NEURAL INFORM PR, P1
   Al-Rfou R, 2016, ARXIV E PRINTS
   Amos Brandon, 2017, ARXIV170300443CSMATH
   Anitescu M, 1997, NONLINEAR DYNAM, V14, P231, DOI 10.1023/A:1008292328909
   Atkeson CG, 1997, IEEE INT CONF ROBOT, P3557, DOI 10.1109/ROBOT.1997.606886
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Battaglia Peter W., 2016, ARXIV161200222CS
   Brockman G., 2016, OPENAI GYM
   Catto Erin, 2010, GDC
   Chang Michael B., 2016, ARXIV161200341CS
   Chun Kai Ling, 2018, ARXIV180502777
   Cline Michael Bradley, 2002, THESIS
   Cottle R. W, 2008, ENCY OPTIMIZATION, P1873
   Coumans Erwin, 2013, OPEN SOURCE BULLETPH, V15, P5
   Degrave Jonas, 2016, ARXIV161101652CS
   Dhariwal P., 2017, OPENAI BASELINES
   Djolonga Josip, 2017, NEURAL INFORM PROCES
   Featherstone Roy, 1984, ROBOT DYNAMICS ALGOR
   Fragkiadaki Katerina, 2015, ARXIV151107404CS
   Garstenauer Helmut, 2006, THESIS
   Gregorius Dirk, 2015, GDC
   Gregorius Dirk, 2013, GDC
   Hamrick Jessica B., 2015, P 37 ANN C COGN SCI
   Heess Nicolas, 2015, ARXIV151009142CS
   Hermans M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086696
   Kurutach T., 2018, INT C LEARN REPR
   Lee J., 2018, J OPEN SOURCE SOFTWA, V3, P500, DOI DOI 10.21105/JOSS.00500
   Lerer Adam, 2016, ARXIV160301312CS
   Ljung L, 1999, SYSTEM IDENTIFICATIO
   Magnus X., 1988, MATRIX DIFFERENTIAL
   Mattingley J, 2012, OPTIM ENG, V13, P1, DOI 10.1007/s11081-011-9176-9
   Mensch Arthur, 2018, ARXIV180203676
   Mishra Nikhil, 2017, INT C MACH LEARN, P2459
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nagabandi  A., 2017, ARXIV170802596
   Paszke  A., 2017, AUTOMATIC DIFFERENTI
   Ranjan Anurag, 2016, ARXIV161100850CS
   Smith KA, 2013, TOP COGN SCI, V5, P185, DOI 10.1111/tops.12009
   SONG J, 2004, ICINCO, P2223
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   van den Bergen Gino Johannes Apolonia, 2004, M KAUFMANN SERIES IN
   van Hasselt H., 2015, CORR
   WERBOS PJ, 1989, PROCEEDINGS OF THE 28TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-3, P260, DOI 10.1109/CDC.1989.70114
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852001070
DA 2019-06-15
ER

PT S
AU Belkin, M
   Hsu, D
   Mitra, PP
AF Belkin, Mikhail
   Hsu, Daniel
   Mitra, Partha P.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Overfitting or perfect fitting? Risk bounds for classification and
   regression rules that interpolate
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID REGULARIZATION; RATES
AB Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for "overfitted" /interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise.
   Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and singularly weighted k-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions.
   Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime.
C1 [Belkin, Mikhail] Ohio State Univ, Columbus, OH 43210 USA.
   [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA.
   [Mitra, Partha P.] Cold Spring Harbor Lab, POB 100, Cold Spring Harbor, NY 11724 USA.
RP Belkin, M (reprint author), Ohio State Univ, Columbus, OH 43210 USA.
EM mbelkin@cse.ohio-state.edu; djhsu@cs.columbia.edu; mitra@cshl.edu
FU NSF [CCF-1740833, DMR-1534910]; Crick-Clay Professorship (CSHL); H N
   Mahabala Chair (IITM)
FX We would like to thank Raef Bassily, Luis Rademacher, Sasha Rakhlin, and
   Yusu Wang for conversations and valuable comments. We acknowledge
   funding from NSF. DH acknowledges support from NSF grants CCF-1740833
   and DMR-1534910. PPM acknowledges support from the Crick-Clay
   Professorship (CSHL) and H N Mahabala Chair (IITM). This work grew out
   of discussions originating at the Simons Institute for the Theory of
   Computing in 2017, and we thank the Institute for the hospitality. PPM
   and MB thank ICTS (Bangalore) for their hospitality at the 2017 workshop
   on Statistical Physics Methods in Machine Learning.
CR AFFENTRANGER F, 1991, DISCRETE COMPUT GEOM, V6, P291, DOI 10.1007/BF02574691
   Amenta N, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1106
   Anthony M., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P211
   Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217
   Bartlett Peter, 2017, NIPS
   Bartlett PL, 1996, J COMPUT SYST SCI, V52, P434, DOI 10.1006/jcss.1996.0033
   Bassily R, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1046, DOI 10.1145/2897518.2897566
   Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Belkin M, 2004, LECT NOTES COMPUT SC, V3120, P624, DOI 10.1007/978-3-540-27819-1_43
   Belkin Mikhail, 2018, ARXIV180609471
   Belkin Mikhail, 2018, P 35 INT C MACH LEAR, P541
   Belkin Mikhail, 2018, ARXIV180605161
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Cutler A., 2001, COMPUTING SCI STAT, V33, P490
   Davies S, 1997, ADV NEUR IN, V9, P1005
   Devroye L, 1998, J MULTIVARIATE ANAL, V65, P209, DOI 10.1006/jmva.1997.1725
   Fawzi Alhussein, 2016, ADV NEURAL INFORM PR, V29, P1632
   Fukuda Komei, 2004, TECHNICAL REPORT
   Golowich Noah, 2018, 31 ANN C LEARN THEOR
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   GyOrfi L., 2002, SPRINGER SERIES STAT
   Halton John H, 1991, TR91002 U N CAR CHAP
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Liang Tengytmn, 2017, ARXIV171101530
   Madry A, 2018, INT C LEARN REPR
   Mammen E, 1999, ANN STAT, V27, P1808
   Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786
   Nadaraya E., 1964, THEOR PROBAB APPL, V9, P141, DOI DOI 10.1137/1109020
   Neyshabur B., 2018, INT C LEARN REPR
   Salakhutdinov Ruslan, 2017, DEEP LEARNING TUTORI
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Schapire Robert E, 1998, ANN STAT, V26
   Schneider Rolf, 2004, HDB DISCRETE COMPUTA, P255
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shepard D, 1968, P 1968 23 ACM NAT C
   Steinwart I, 2008, INFORM SCI STAT, P1
   Su J., 2017, ARXIV171008864
   Szegedy Christian, 2014, INT C LEARN REPR
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Wang Yizhen, 2018, P 35 INT C MACH LEAR, P5133
   Wasserman L., 2004, ALL STAT
   Wasserman L, 2006, ALL NONPARAMETRIC ST
   Watson G., 1964, SANKHYA A, V26, P359, DOI DOI 10.2307/25049340
   Wyner AJ, 2017, J MACH LEARN RES, V18, P1
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Zhang Chiyuan, 2017, INT C LEARN REPR
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 54
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823302032
DA 2019-06-15
ER

PT S
AU Bellec, G
   Salaj, D
   Subramoney, A
   Legenstein, R
   Maass, W
AF Bellec, Guillaume
   Salaj, Darjan
   Subramoney, Anand
   Legenstein, Robert
   Maass, Wolfgang
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Long short-term memory and learning-to-learn in networks of spiking
   neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID PROCESSOR
AB Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT).
   A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.
C1 [Bellec, Guillaume; Salaj, Darjan; Subramoney, Anand; Legenstein, Robert; Maass, Wolfgang] Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria.
RP Bellec, G (reprint author), Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria.
EM bellec@igi.tugraz.at; salaj@igi.tugraz.at; subramoney@igi.tugraz.at;
   legenstein@igi.tugraz.at; maass@igi.tugraz.at
FU HBP Joint Platform - European Union's Horizon 2020 Framework Programme
   for Research and Innovation [720270, 785907]; NVIDIA Corporation;
   European Union [604102]
FX This research/project was supported by the HBP Joint Platform, funded
   from the European Union's Horizon 2020 Framework Programme for Research
   and Innovation under the Specific Grant Agreement No. 720270 (Human
   Brain Project SGA1) and under the Specific Grant Agreement No. 785907
   (Human Brain Project SGA2). We gratefully acknowledge the support of
   NVIDIA Corporation with the donation of the Quadro P6000 GPU used for
   this research. Research leading to these results has in parts been
   carried out on the Human Brain Project PCP Pilot Systems at the Julich
   Supercomputing Centre, which received co-funding from the European Union
   (Grant Agreement No. 604102). We gratefully acknowledge Sandra Diaz,
   Alexander Peyser and Wouter Klijn from the Simulation Laboratory
   Neuroscience of the Julich Supercomputing Centre for their support. The
   computational results presented have been achieved in part using the
   Vienna Scientific Cluster (VSC).
CR Allen Institute, 2018, ALL CELL TYP DAT CEL
   Bellec  Guillaume, 2018, INT C LEARN REPR ICL
   Bellec  Guillaume, 2018, COMPUTATIONAL UNPUB
   Costa R., 2017, ADV NEURAL INFORM PR, P272
   Courbariaux M, 2016, ARXIV160202830
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DePasquale B, 2016, ARXIV160107620
   Duan  Y., 2016, ARXIV161102779
   Eliasmith C, 2013, BUILD BRAIN NEURAL A
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gouwens NW, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02718-3
   Greff  Klaus, 2017, IEEE T NEURAL NETWOR
   Hasson U, 2015, TRENDS COGN SCI, V19, P304, DOI 10.1016/j.tics.2015.04.006
   Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87
   Huh  Dongsung, 2017, ARXIV170604698
   Kappel D, 2018, ENEURO, V5, DOI 10.1523/ENEURO.0301-17.2018
   Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485
   Le Quoc V., 2015, ABS150400941 CORR
   Mikolov T., 2014, ARXIV14127753
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   MORRIS R, 1984, J NEUROSCI METH, V11, P47, DOI 10.1016/0165-0270(84)90007-4
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Perich Matthew G, 2018, NEURON
   Pozzorini C, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004275
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schulman  J., 2017, ARXIV170706347
   Stokes MG, 2015, TRENDS COGN SCI, V19, P394, DOI 10.1016/j.tics.2015.05.004
   Subramoney  Anand, 2018, RECURRENT NETW UNPUB
   Teeter  Corinne, 2018, NATURE COMMUNICATION, V1, P1
   Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586
   Wang J. X, 2016, ARXIV161105763
   Wang Jane X, 2018, NATURE NEUROSCIENCE
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823300073
DA 2019-06-15
ER

PT S
AU Bello, K
   Honorio, J
AF Bello, Kevin
   Honorio, Jean
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Computationally and statistically efficient learning of causal Bayes
   nets using path queries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID DISCOVERY
AB Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal Bayesian network with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening on the origin node and observing samples from the target node. We theoretically show the logarithmic sample complexity for the size of interventional data per path query, for continuous and discrete networks. We then show how to learn the transitive edges using also logarithmic sample complexity (albeit in time exponential in the maximum number of parents for discrete networks), which allows us to learn the full network. We further extend our work by reducing the number of interventional path queries for learning rooted trees. We also provide an analysis of imperfect interventions.
C1 [Bello, Kevin; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
RP Bello, K (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM kbellome@purdue.edu; jhonorio@purdue.edu
CR Aho A. V., 1972, SIAM Journal on Computing, V1, P131, DOI 10.1137/0201008
   Brenner E., 2013, UAI
   Cheng J, 2002, ARTIFICIAL INTELLIGE
   Chickering D., 2002, UAI
   Chickering D.M, 1996, LEARNING DATA ARTIFI, P121, DOI DOI 10.1007/978-1-4612-2404-4_12
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Eaton D., 2007, INT C ART INT STAT, P107
   Eberhardt F, 2005, P 21 C UNC ART INT U, P178
   Harbison Christopher T, 2004, NATURE
   Hauser A., 2012, P 6 EUR WORKSH PROB
   He Y., 2008, J MACHINE LEARNING R, V9
   Hoffgen K, 1993, COLT
   Kocaoglu Murat, 2017, ADV NEURAL INFORM PR, P7021
   Koller D., 2009, PROBABILISTIC GRAPHI
   Le Gall F., 2014, P INT S SYMB ALG COM, P296, DOI DOI 10.1145/2608628.2608664
   Liu  H., 2012, ADV NEURAL INFORM PR, P2537
   Louizos Christos, 2017, NIPS
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   Murphy K, 2001, TECHNICAL REPORT
   Obozinski Guillaume R, 2009, ADV NEURAL INFORM PR
   Pearl J, 2009, CAUSALITY MODELS REA
   Peters J., 2010, INT C ART INT STAT, P597
   Peters J, 2014, J MACH LEARN RES, V15, P2009
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Shanmugam K, 2015, P ADV NEUR INF PROC, P3195
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Spirtes P., 2000, CAUSATION PREDICTION
   Tong S., 2001, INT JOINT C ART INT
   Triantafillou S, 2015, J MACH LEARN RES, V16, P2147
   Tsamardinos I, 2006, MACHINE LEARNING
   Verma T., 1991, P 6 ANN C UNC ART IN
   Wang Z., 2016, ARXIV160605183
   Xiao Yun, 2015, SCI REPORTS
   Zuk O, 2006, UAI
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3CA
UT WOS:000461852005051
DA 2019-06-15
ER

PT S
AU Bello, K
   Honorio, J
AF Bello, Kevin
   Honorio, Jean
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Learning latent variable structured prediction models with Gaussian
   perturbations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID NUMBER
AB The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs [26, 1, 5, 25]. The large-margin formulation including latent variables [30, 21] not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work [11] has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application.
C1 [Bello, Kevin; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
RP Bello, K (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM kbellome@purdue.edu; jhonorio@purdue.edu
FU National Science Foundation [1716609-IIS]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 1716609-IIS.
CR Altun Y., 2003, EUR C SPEECH COMM TE, P145
   BENNETT JF, 1956, PSYCHOMETRIKA, V21, P383, DOI 10.1007/BF02296304
   BENNETT JF, 1960, PSYCHOMETRIKA, V25, P27, DOI 10.1007/BF02288932
   Choi H., 2016, ARTIF INTELL, P667
   Collins M, 2004, TEXT SPEECH LANG TEC, P19
   Collins M., 2004, P 42 ANN M ASS COMP, P111
   Cortes C., 2016, NIPS, P2514
   COVER TM, 1967, SIAM J APPL MATH, V15, P434, DOI 10.1137/0115039
   Gane A., 2014, ARTIF INTELL, P247
   Hinton G., 2010, MOMENTUM, V9, P1, DOI DOI 10.1007/978-3-642-35289-8_32
   Honorio J., 2016, UAI
   Kulesza A., 2007, ADV NEURAL INFORM PR, P785
   Lafferty J., 2001, CONDITIONAL RANDOM F
   Li MH, 2007, BIOINFORMATICS, V23, P597, DOI 10.1093/bioinformatics/btl660
   London B., 2016, NIPS WORKSH OPT MACH
   McAllester D., 2007, PREDICTING STRUCTURE, P247
   Meshi O., 2016, INT C MACH LEARN
   Neylon T., 2006, THESIS
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Petrov S., 2008, P ADV NEUR INF PROC, P1153
   Ping W., 2014, MARGINAL STRUCTURED, P190
   Quattoni A., 2005, ADV NEURAL INFORM PR, P1097
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Sarawagi S., 2008, P 25 INT C MACH LEAR, P888
   Taskar B., 2003, P NEUR INF PROC SYST, P25
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Volkovs M., 2012, NIPS 12, P1313
   Wang HY, 2013, COMMUN ACM, V56, P92, DOI [10.1145/2436258.2436276, 10.1145/2436256.2436276]
   Wang S.B., 2006, IEEE C COMP VIS PATT, V2, P1521, DOI DOI 10.1109/CVPR.2006.132
   Yu C. -N. J., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Yuille AL, 2002, ADV NEUR IN, V14, P1033
   Zhang Y., 2014, P EMNLP 2014 DOH QUA, P1013
   Zhang Y., 2015, HLT NAACL, P42
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303017
DA 2019-06-15
ER

PT S
AU Bellot, A
   Schaar, MD
AF Bellot, Alexis
   van der Schaar, Mihaela
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Multitask Boosting for Survival Analysis with Competing Risks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
AB The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.
C1 [Bellot, Alexis; van der Schaar, Mihaela] Univ Oxford, Oxford, England.
   [van der Schaar, Mihaela] Alan Turing Inst, London, England.
RP Bellot, A (reprint author), Univ Oxford, Oxford, England.
EM alexis.bellot@eng.ox.ac.uk; mschaar@turing.ac.uk
CR Aalen O, 1980, LECT NOTES STAT, P1, DOI DOI 10.1007/978-1-4615-7397-5_1
   Alaa Ahmed, 2017, ADV NEURAL INFORM PR
   Alaa AM, 2018, INT C MACH LEARN
   Bellot Alexis, 2018, IEEE J BIOMEDICAL HL
   Bellot Alexis, 2018, P INT C ART INT STAT, P910
   Binder H, 2009, BIOINFORMATICS, V25, P890, DOI 10.1093/bioinformatics/btp088
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Caruana R.A., 1993, P 10 INT C MACH LEAR, V10, P41
   Drucker H., 1997, P 14 INT C MACH LEAR, V97, P107
   Fine JP, 1999, J AM STAT ASSOC, V94, P496, DOI 10.2307/2670170
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Galar M, 2012, IEEE T SYST MAN CY C, V42, P463, DOI 10.1109/TSMCC.2011.2161285
   Gornitz N., 2011, ADV NEURAL INFORM PR, P2690
   GRAY RJ, 1988, ANN STAT, V16, P1141, DOI 10.1214/aos/1176350951
   He K, 2016, BIOINFORMATICS, V32, P50, DOI 10.1093/bioinformatics/btv517
   Ishwaran H, 2014, BIOSTATISTICS, V15, P757, DOI 10.1093/biostatistics/kxu010
   Katzman J, 2016, ARXIV160600931
   Lee  C., 2018, AAAI
   Mercer S, 2014, ABC MULTIMORBIDITY
   Morrison D, 2016, NPJ PRIM CARE RESP M, V26, DOI 10.1038/npjpcrm.2016.43
   PRENTICE RL, 1978, BIOMETRICS, V34, P541, DOI 10.2307/2530374
   Ranganath  R., 2016, MACH LEARN HEALTHC C, P101
   Ridgeway G, 1999, COMP SCI STAT, V31, P172
   Seiffert Chris, 2008, P 21 INT FLOR ART IN, P306
   Solomatine DP, 2004, IEEE IJCNN, P1163
   Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307
   Wolbers M, 2014, BIOSTATISTICS, V15, P526, DOI 10.1093/biostatistics/kxt059
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823301038
DA 2019-06-15
ER

PT S
AU Ben-Nun, T
   Jakobovits, AS
   Hoefler, T
AF Ben-Nun, Tal
   Jakobovits, Alice Shoshana
   Hoefler, Torsten
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Neural Code Comprehension: A Learnable Representation of Code Semantics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID GRAPH; MODEL
AB With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.
C1 [Ben-Nun, Tal; Jakobovits, Alice Shoshana; Hoefler, Torsten] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.
RP Ben-Nun, T (reprint author), Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.
EM talbn@inf.ethz.ch; alicej@student.ethz.ch; htor@inf.ethz.ch
FU ETH Postdoctoral Fellowship; Marie Curie Actions for People COFUND
   program
FX We wish to thank Theodoros Theodoridis, Kfir Levy, Tobias Grosser, and
   Yunyan Guo for fruitful discussions. The authors also acknowledge
   MeteoSwiss, and thank Hussein Harake, Colin McMurtrie, and the whole
   CSCS team for granting access to the Greina machines, and for their
   excellent technical support. TBN is supported by the ETH Postdoctoral
   Fellowship and Marie Curie Actions for People COFUND program.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Allamanis  M., 2017, ABS170906182 CORR
   Allamanis M, 2015, 2015 10TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE 2015) PROCEEDINGS, P38, DOI 10.1145/2786805.2786849
   Allamanis Miltiadis, 2017, ABS170507867 CORR
   Allamanis Miltiadis, 2016, ABS160203001 CORR
   Allamanis Miltiadis, 2017, ABS171100740 CORR
   Alon Uri, 2018, ABS180309544 CORR
   Alon Uri, 2018, ABS180309473 CORR
   AMD, AMD OPENCL ACC PAR P
   Balaprakash P., 2013, CLUST COMP CLUSTER 2, P1
   Baldauf M, 2011, MON WEATHER REV, V139, P3887, DOI 10.1175/MWR-D-10-05013.1
   Bielik P., 2016, P 33 INT C MACH LEAR, P2933
   Bunel Rudy, 2017, INT C LEARN REPR
   Che SA, 2009, I S WORKL CHAR PROC, P44, DOI 10.1109/IISWC.2009.5306797
   CLICK C, 1995, SIGPLAN NOTICES, V30, P35
   Click Cliff, 1995, ACM T PROGRAMMING LA, V17
   Cummins Chris, 2017, PACT
   CYTRON R, 1991, ACM T PROGR LANG SYS, V13, P451, DOI 10.1145/115372.115320
   Dam Hoa Khanh, 2016, ABS160802715 CORR
   Danalis Anthony, 2010, SCALABLE HETEROGENEO, P63
   Dongarra Jack, 2002, BASIC LINEAR ALGEBRA, P1
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   FERRANTE J, 1987, ACM T PROGR LANG SYS, V9, P319, DOI 10.1145/24039.24041
   GitHub, 2017, GITHUB OCT
   Glorot Xavier, 2011, DEEP SPARSE RECTIFIE
   Grauer-Gray Scott, 2012, AUTOTUNING HIGH LEVE
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Grewe D, 2013, INT SYM CODE GENER, P161
   Gu XD, 2016, FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING, P631, DOI 10.1145/2950290.2950334
   Guennebaud G., 2010, EIGEN V3
   Harris Zellig S., 1981, DISTRIBUTIONAL STRUC, P3
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsiao CH, 2014, ACM SIGPLAN NOTICES, V49, P49, DOI 10.1145/2660193.2660226
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Itseez, 2015, OP SOURC COMP VIS LI
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Lattner C, 2004, INT SYM CODE GENER, P75, DOI 10.1109/CGO.2004.1281665
   Leather H, 2009, INT SYM CODE GENER, P81, DOI 10.1109/CGO.2009.21
   Levy Dor, 2017, P 34 INT C MACH LEAR, P2043
   Linux, LIN KERN SOURC COD V
   LLVM, FLANG FORTRAN COMP F
   LLVM, 2018, LLVM LANG REF MAN
   LLVM, CLANG C LANG FAM FRO
   Magni A, 2014, INT CONFER PARA, P455, DOI 10.1145/2628071.2628087
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mou L., 2016, AAAI, P1287
   Namolaru M, 2010, PROCEEDINGS OF THE 2010 INTERNATIONAL CONFERENCE ON COMPILERS, ARCHITECTURES AND SYNTHESIS FOR EMBEDDED SYSTEMS (CASES '10), P197, DOI 10.1145/1878921.1878951
   Nobre R, 2016, ACM SIGPLAN NOTICES, V51, P21, DOI [10.1145/2980930.2907959, 10.1145/2907950.2907959]
   Pantel P., 2005, P 43 ANN M ASS COMP, P125
   Park E., 2012, P 10 INT S COD GEN O, P196, DOI DOI 10.1145/2259016.2259042
   Raychev V, 2014, ACM SIGPLAN NOTICES, V49, P419, DOI 10.1145/2594291.2594321
   Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150
   Seo S, 2011, I S WORKL CHAR PROC, P137, DOI 10.1109/IISWC.2011.6114174
   Sreedhar V. C., 1995, Conference Record of POPL'95: 22nd ACM SIGPLAN-SIGACT Symposium Principles of Programming Languages, P62
   Stratton John A., 2012, REVISED BENCHMARK SU
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vechev M, 2016, FOUND TRENDS PROGRAM, V3, P231, DOI 10.1561/2500000028
   Wang Baoezeng, 2015, DEPRESS ANXIETY, P1
   White M, 2016, IEEE INT CONF AUTOM, P87, DOI 10.1145/2970276.2970326
   Xu Xiaojun, 2017, ABS170806525 CORR
   Yang YX, 2017, IEEE INT CONF AUTOM, P682, DOI 10.1109/ASE.2017.8115678
   Zadrozny B., 2014, P 31 INT C MACH LEAR, pII
NR 64
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2018
VL 31
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM3BF
UT WOS:000461823303057
DA 2019-06-15
ER

EF
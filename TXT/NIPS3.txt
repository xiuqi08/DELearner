FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Bonald, T
   Combes, R
AF Bonald, Thomas
   Combes, Richard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Minimax Optimal Algorithm for Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ERROR
AB We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data.
C1 [Bonald, Thomas] Telecom ParisTech, Paris, France.
   [Combes, Richard] Cent Supelec, L2S, Chatenay Malabry, France.
RP Bonald, T (reprint author), Telecom ParisTech, Paris, France.
EM thomas.bonald@telecom-paristech.fr; richard.combes@supelec.fr
RI Jeong, Yongwook/N-7413-2016
CR Albert PS, 2004, BIOMETRICS, V60, P427, DOI 10.1111/j.0006-341X.2004.00187.x
   Chao Gao, 2015, MINIMAX OPTIMAL CONV
   Dalvi Nilesh, 2013, P WWW
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Ghosh Arpita, 2011, P ACM EC
   HUI SL, 1980, BIOMETRICS, V36, P167, DOI 10.2307/2530508
   Karger D., 2011, P NIPS
   Karger David R., 2013, Performance Evaluation Review, V41, P81
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Liu  Qiang, 2012, P NIPS
   NITZAN S, 1982, INT ECON REV, V23, P289, DOI 10.2307/2526438
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   SHAPLEY L, 1984, PUBLIC CHOICE, V43, P329, DOI 10.1007/BF00118940
   Smyth Padhraic, 1995, P NIPS
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Welinder Peter, 2010, P IEEE CVPR WORKSH
   Whitehill J., 2009, P NIPS
   Zhang L, 2013, TENCON IEEE REGION
   Zhang  Yuchen, 2014, P NIPS
   Zhou D., 2015, REGULARIZED MINIMAX
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404041
DA 2019-06-15
ER

PT S
AU Boomsma, W
   Frellsen, J
AF Boomsma, Wouter
   Frellsen, Jes
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Spherical convolutions and their application in molecular modelling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.
C1 [Boomsma, Wouter] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
   [Frellsen, Jes] IT Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
RP Boomsma, W (reprint author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
EM wb@di.ku.dk; jefr@itu.dk
RI Jeong, Yongwook/N-7413-2016
FU Villum Foundation [VKR023445]
FX This work was supported by the Villum Foundation (W.B., grant number
   VKR023445).
CR Aurisano A, 2016, J INSTRUM, V11, DOI 10.1088/1748-0221/11/09/P09001
   Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401
   Boomsma W, 2008, P NATL ACAD SCI USA, V105, P8932, DOI 10.1073/pnas.0801715105
   Boomsma W, 2014, P NATL ACAD SCI USA, V111, P13852, DOI 10.1073/pnas.1404948111
   Cohen Taco, 2016, INT C MACH LEARN, P2990
   Cohen TS, 2018, INT C LEARN REPR
   Conchuir SO, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130433
   Eastman P, 2013, J CHEM THEORY COMPUT, V9, P461, DOI 10.1021/ct300857j
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Hornak V, 2006, PROTEINS, V65, P712, DOI 10.1002/prot.21123
   Irback A, 2006, J COMPUT CHEM, V27, P1548, DOI 10.1002/jcc.20452
   Jasrasaria D., 2016, 160805747 ARXIV
   KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211
   Kingma D. P., 2015, 3 INT C LEARN REPR S
   Mardia K.V., 2009, DIRECTIONAL STAT
   Mills K, 2017, PHYS REV A, V96, DOI 10.1103/PhysRevA.96.042113
   Min S, 2017, BRIEF BIOINFORM, V18, P851, DOI 10.1093/bib/bbw068
   Ronchi C, 1996, J COMPUT PHYS, V124, P93, DOI 10.1006/jcph.1996.0047
   Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Simonyan Karen, 2015, 3 INT C LEARN REPR S
   Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a
   Torng W, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1702-0
   Wallach I., 2015, 151002855 ARXIV
   Wang GL, 2003, BIOINFORMATICS, V19, P1589, DOI 10.1093/bioinformatics/btg224
   Wang S., 2016, SCI REPORTS, V6
   Word JM, 1999, J MOL BIOL, V285, P1735, DOI 10.1006/jmbi.1998.2401
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403049
DA 2019-06-15
ER

PT S
AU Borgs, C
   Chayes, J
   Lee, CE
   Shah, D
AF Borgs, Christian
   Chayes, Jennifer
   Lee, Christina E.
   Shah, Devavrat
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse
   Matrix Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPLETION
AB The sparse matrix estimation problem consists of estimating the distribution of an n x n matrix Y, from a sparsely observed single instance of this matrix where the entries of Y are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to 0 at the rate of O(d(2) (pn)(-2/5)) as long as omega(d(5) n) random entries from a total of n(2) entries of Y are observed (uniformly sampled), E [Y] has rank d, and the entries of Y have bounded support. The maximum squared error across all entries converges to 0 with high probability as long as we observe a little more, Omega (d(5) n ln(5) (n)) entries. Our results are the best known sample complexity results in this generality.
C1 [Borgs, Christian; Chayes, Jennifer; Lee, Christina E.] Microsoft Res New England, One Mem Dr, Cambridge, MA 02142 USA.
   [Shah, Devavrat] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Borgs, C (reprint author), Microsoft Res New England, One Mem Dr, Cambridge, MA 02142 USA.
EM borgs@microsoft.com; jchayes@microsoft.com; celee@mit.edu;
   devavrat@mit.edu
FU NSF [CMMI-1462158, CMMI-1634259]; DARPA [W911NF-16-1-0551]; NSF Graduate
   Fellowship; Claude E. Shannon Research Assistantship
FX This work is supported in parts by NSF under grants CMMI-1462158 and
   CMMI-1634259, by DARPA under grant W911NF-16-1-0551, and additionally by
   a NSF Graduate Fellowship and Claude E. Shannon Research Assistantship.
CR Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Abbe Emmanuel, 2015, ADV NEURAL INFORM PR
   Abbe Emmanuel, 2016, ADV NEURAL INFORM PR
   Airoldi EM, 2013, ADV NEURAL INFORM PR, V26, P692
   ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3
   Anandkumar A., 2013, C LEARN THEOR, P867
   Austin Tim, 2012, NOT IAS WORKSH
   Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86
   Borgs C, 2008, ADV MATH, V219, P1801, DOI 10.1016/j.aim.2008.07.008
   Borgs C., 2014, ARXIV14080744
   Borgs C., 2014, ARXIV14012906
   BORGS C., 2015, ADV NEURAL INFORM PR, P1369
   Borgs Christian, 2016, ARXIV160107134
   Borgs Christian, 2015, ARXIV150806675
   Candes Emmanuel, 2009, COMMUN ACM, V55, P111
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Chen Y., 2015, ARXIV150903025
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Diaconis P, 2008, REND MAT APPL, V28, P33
   Gao C, 2015, ANN STAT, V43, P2624, DOI 10.1214/15-AOS1354
   Goldberg D., 1992, COMMUN ACM
   Hoover D. N., 1981, EXCHANGEABILITY PROB, P281
   Karrer B, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.208702
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Klopp Olga, 2015, ANN STAT
   Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Lovasz L., 2012, LARGE NETWORKS GRAPH, V60
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   Mossel Elchanan, 2017, COMBINATORICA
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Ning X., 2015, RECOMMENDER SYSTEMS, P37, DOI DOI 10.1007/978-1-4899-7637-6_2
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Song  D., 2016, ADV NEURAL INFORM PR, P2155
   Steurer David, 2017, BAYESIAN ESTIMATION
   Veitch V., 2015, ARXIV151203099
   WOLFE P. J., 2013, ARXIV13095936
   Xu Jiaming, 2014, P 27 C LEARN THEOR J, P903
   Zhang Yuan, 2015, ARXIV150908588
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404076
DA 2019-06-15
ER

PT S
AU Bornschein, J
   Mnih, A
   Zoran, D
   Rezende, DJ
AF Bornschein, Jorg
   Mnih, Andriy
   Zoran, Daniel
   Rezende, Danilo J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variational Memory Addressing in Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory.
C1 [Bornschein, Jorg; Mnih, Andriy; Zoran, Daniel; Rezende, Danilo J.] DeepMind, London, England.
RP Bornschein, J (reprint author), DeepMind, London, England.
EM bornschein@google.com; amnih@google.com; danielzoran@google.com;
   danilor@google.com
RI Jeong, Yongwook/N-7413-2016
CR [Anonymous], 2015, DRAW RECURRENT NEURA
   Bartunov Sergey, 2016, ARXIV161202192
   Bornschein Jorg, 2014, ARXIV14062751
   Burda Y., 2015, ARXIV150900519
   DAS S, 1992, PROCEEDINGS OF THE FOURTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P791
   Dilokthanakul N., 2016, ARXIV161102648
   Edwards Harrison, 2017, NEURAL STAT, V2
   Gemici Mevlana, 2017, ARXIV170204649
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Jang E., 2017, STAT, V1050, P1
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma D. P., 2016, ARXIV160604934
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lei BA Jimmy, 2015, NIPS, P2593
   Li C., 2016, INT C MACH LEARN, P1177
   Maddison Chris J, 2016, ARXIV161100712
   Mnih Andriy, 2014, ARXIV14020030
   Mnih Andriy, 2016, ARXIV160206725
   Nalisnick Eric, 2016, NIPS WORKSH BAYES DE
   Rezende D.J., 2016, ARXIV160305106
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Zaremba Wojciech, 2015, ARXIV150500521, V362
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403095
DA 2019-06-15
ER

PT S
AU Bouchard, KE
   Bujan, AF
   Roosta-Khorasani, F
   Ubaru, S
   Prabhat
   Snijders, AM
   Mao, JH
   Chang, EF
   Mahoney, MW
   Bhattacharyya, S
AF Bouchard, Kristofer E.
   Bujan, Alejandro F.
   Roosta-Khorasani, Farbod
   Ubaru, Shashanka
   Prabhat
   Snijders, Antoine M.
   Mao, Jian-Hua
   Chang, Edward F.
   Mahoney, Michael W.
   Bhattacharyya, Sharmodeep
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Union of Intersections (UoI) for Interpretable Data Driven Discovery and
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SELECTION
AB The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce Union of Intersections (UoI), a flexible, modular, and scalable framework for enhanced model selection and estimation. Methods based on UoI perform model selection and model estimation through intersection and union operations, respectively. We show that UoI-based methods achieve low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm (UoI(Lasso)) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the UoI(L1Logistic) and UoI(CUR) variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on the UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.
C1 [Bouchard, Kristofer E.] LBNL, Biol Syst & Engn Div, Berkeley, CA 94720 USA.
   [Bujan, Alejandro F.] Univ Calif Berkeley, Redwood Ctr, Berkeley, CA USA.
   [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, ICSI, Berkeley, CA USA.
   [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA USA.
   [Ubaru, Shashanka] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
   [Prabhat] LBNL, NERSC, Berkeley, CA USA.
   [Snijders, Antoine M.; Mao, Jian-Hua] LBNL, Biol Syst & Engn Div, Berkeley, CA USA.
   [Chang, Edward F.] UC San Francisco, Dept Neurol Surg, San Francisco, CA USA.
   [Bhattacharyya, Sharmodeep] Oregon State Univ, Dept Stat, Corvallis, OR 97331 USA.
RP Bouchard, KE (reprint author), LBNL, Biol Syst & Engn Div, Berkeley, CA 94720 USA.
EM kebouchard@lbl.gov; afbujan@gmail.com; farbod@icsi.berkeley.edu;
   ubaru001@umn.edu; prabhat@lbl.gov; AMSnijders@lbl.gov; jhmao@lbl.gov;
   Chang@ucsf.edu; mmahoney@icsi.berkeley.edu;
   bhattash@science.oregonstate.edu
RI Jeong, Yongwook/N-7413-2016
CR Bach F. R, 2008, P 25 INT C MACH LEAR, P33, DOI [DOI 10.1145/1390156.1390161, 10.1145/1390156.1390161]
   Bickel PJ, 2006, TEST-SPAIN, V15, P271, DOI 10.1007/BF02607055
   Bouchard K. E., 2017, TECHNICAL REPORT
   Bouchard K. E., 2015, TECHNICAL REPORT
   Bouchard KE, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00092
   Bouchard KE, 2014, J NEUROSCI, V34, P12662, DOI 10.1523/JNEUROSCI.1219-14.2014
   Bouchard KE, 2013, NATURE, V495, P327, DOI 10.1038/nature11911
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Ganguli S, 2012, ANNU REV NEUROSCI, V35, P485, DOI 10.1146/annurev-neuro-062111-150410
   Hastie T., 2003, ELEMENTS STAT LEARNI
   Javanmard A, 2014, J MACH LEARN RES, V15, P2869
   Mao JH, 2015, SCI REP-UK, V5, DOI 10.1038/srep16247
   Marx V, 2013, NATURE, V498, P255, DOI 10.1038/498255a
   National Research Council, 2013, FRONT MASS DAT AN
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Sejnowski TJ, 2014, NAT NEUROSCI, V17, P1440, DOI 10.1038/nn.3839
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401012
DA 2019-06-15
ER

PT S
AU Bringmann, K
   Kolev, P
   Woodruff, DP
AF Bringmann, Karl
   Kolev, Pavel
   Woodruff, David P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Approximation Algorithms for l(0)-Low Rank Approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MATRIX FACTORIZATION; COMPONENT ANALYSIS; COMPRESSION; FIELDS
AB We study the l(0)-Low Rank Approximation Problem, where the goal is, given an m x n matrix A, to output a rank-k matrix A' for which parallel to A' - A parallel to(0) is minimized. Here, for a matrix B, parallel to B parallel to(0) denotes the number of its non-zero entries. This NP-hard variant of low rank approximation is natural for problems with no underlying metric, and its goal is to minimize the number of disagreeing data positions.
   We provide approximation algorithms which significantly improve the running time and approximation factor of previous work. For k > 1, we show how to find, in poly(mn) time for every k, a rank O(k log(n/k)) matrix A' for which parallel to A' - A parallel to(0) <= O(k(2) log(n/k)) OPT. To the best of our knowledge, this is the first algorithm with provable guarantees for the l(0)-Low Rank Approximation Problem for k > 1, even for bicriteria algorithms.
   For the well-studied case when k = 1, we give a (2+epsilon)-approximation in sublinear time, which is impossible for other variants of low rank approximation such as for the Frobenius norm. We strengthen this for the well-studied case of binary matrices to obtain a (1 + O(psi))-approximation in sublinear time, where psi = OPT /parallel to A parallel to(0). For small psi, our approximation factor is 1 + o(1).
C1 [Bringmann, Karl; Kolev, Pavel] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.
   [Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Bringmann, K (reprint author), Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.
EM kbringma@mpi-inf.mpg.de; pkolev@mpi-inf.mpg.de; dwoodruf@cs.cmu.edu
FU Cluster of Excellence "Multimodal Computing and Interaction" within the
   Excellence Initiative of the German Federal Government
FX This work has been funded by the Cluster of Excellence "Multimodal
   Computing and Interaction" within the Excellence Initiative of the
   German Federal Government.
CR Alekhnovich M, 2011, COMPUT COMPLEX, V20, P755, DOI 10.1007/s00037-011-0029-x
   Alon N, 2009, LECT NOTES COMPUT SC, V5687, P339, DOI 10.1007/978-3-642-03685-9_26
   Arora S, 2016, SIAM J COMPUT, V45, P1582, DOI 10.1137/130913869
   Belohlavek R, 2010, J COMPUT SYST SCI, V76, P3, DOI 10.1016/j.jcss.2009.05.002
   Berman P, 2002, SIAM PROC S, P514
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chierichetti Flavio, 2017, P 34 INT C MACH LEAR, P806
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27
   Dagum P, 2000, SIAM J COMPUT, V29, P1484, DOI 10.1137/S0097539797315306
   Dan C., 2015, ARXIV E PRINTS
   Fomin Fedor V., 2017, STACS
   Gillis Nicolas, 2015, CORR
   Grigoriev D., 1980, J SOVIET MATH, V14, P1450
   Grigoriev D., 1976, NOTES LENINGRAD BRAN
   Gutch HW, 2012, SIGNAL PROCESS, V92, P1796, DOI 10.1016/j.sigpro.2011.10.003
   Jiang P, 2014, STUD BIG DATA, V1, P281, DOI 10.1007/978-3-642-40837-3_9
   Kannan R, 2008, FOUND TRENDS THEOR C, V4, P157, DOI 10.1561/0400000025
   Koyuturk M, 2005, IEEE T KNOWL DATA EN, V17, P447, DOI 10.1109/TKDE.2005.55
   Koyuturk M, 2006, ACM T MATH SOFTWARE, V32, P33, DOI 10.1145/1132973.1132976
   Li T, 2005, P 11 ACM SIGKDD INT, P188, DOI 10.1145/1081870.1081894
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Meeds Edward, 2006, ADV NEURAL INFORM PR, P977
   Mehmet K., 2003, P 9 ACM SIGKDD INT C, V147156, P147, DOI DOI 10.1145/956750.956770
   Meng X, 2013, P 45 ANN ACM S THEOR, P91
   Miettinen P, 2008, IEEE T KNOWL DATA EN, V20, P1348, DOI 10.1109/TKDE.2008.53
   Miettinen Pauli, 2014, TKDD, V8
   Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21
   Painsky A., 2015, ARXIV E PRINTS
   Ravanbakhsh S., 2015, ARXIV E PRINTS
   Razenshteyn I, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P250, DOI 10.1145/2897518.2897639
   Seppanen JK, 2003, LECT NOTES ARTIF INT, V2838, P423
   Shen BH, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P757
   Singliar T, 2006, J MACH LEARN RES, V7, P2189
   Song Zhao, 2016, CORR
   Song Zhao, 2018, ENTRYWISE LOW RANK A
   Vaidya J, 2007, SACMAT'07: PROCEEDINGS OF THE 12TH ACM SYMPOSIUM ON ACCESS CONTROL MODELS AND TECHNOLOGIES, P175
   Valiant L., 1977, LECT NOTES COMPUTER, P162, DOI [10.1007/3-540-08353-7_135, DOI 10.1007/3-540-08353-7_135]
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yeredor A, 2011, IEEE T INFORM THEORY, V57, P5342, DOI 10.1109/TIT.2011.2145090
   Zhang ZY, 2010, DATA MIN KNOWL DISC, V20, P28, DOI 10.1007/s10618-009-0145-2
   Zhang ZG, 2007, IEEE DATA MINING, P391, DOI 10.1109/ICDM.2007.99
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406069
DA 2019-06-15
ER

PT S
AU Brown, N
   Sandholm, T
AF Brown, Noam
   Sandholm, Tuomas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Safe and Nested Subgame Solving for Imperfect-Information Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold' em poker.
C1 [Brown, Noam; Sandholm, Tuomas] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.
RP Brown, N (reprint author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.
EM noamb@cs.cmu.edu; sandholm@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO
   [W911NF-17-1-0082]; Carnegie Mellon University; TNG Technology
   Consulting; Intel; Optimized Markets, Inc.; GreatPoint Ventures;
   Avenue4Analytics; Rivers Casino
FX This material is based on work supported by the National Science
   Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and
   the ARO under award W911NF-17-1-0082, as well as XSEDE computing
   resources provided by the Pittsburgh Supercomputing Center. The Brains
   vs. AI competition was sponsored by Carnegie Mellon University, Rivers
   Casino, GreatPoint Ventures, Avenue4Analytics, TNG Technology
   Consulting, Artificial Intelligence, Intel, and Optimized Markets, Inc.
   We thank Kristen Gardner, Marcelo Gutierrez, Theo Gutman-Solo, Eric
   Jackson, Christian Kroer, Tim Reiff, and the anonymous reviewers for
   helpful feedback.
CR Billings  Darse, 2003, P 18 INT JOINT C ART
   Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown N., 2017, SCIENCE
   Brown Noam, 2015, P INT JOINT C ARTIFI
   Brown Noam, 2017, AAAI C ART INT AAAI, P421
   Burch Neil, 2014, AAAI C ART INT AAAI, P602
   Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1
   Ganzfried S., 2014, AAAI C ART INT AAAI
   Ganzfried S, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P37
   Ganzfried Sam, 2013, P 23 INT JOINT C ART, P120
   Gilpin A., 2007, INT C AUT AG MULT SY, P1168
   Gilpin A., 2008, P 7 INT C AUT AG MUL, P911
   Gilpin A., 2006, P NAT C ART INT AAAI, V21, P1007
   Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2
   Jackson Eric, 2014, AAAI WORKSH COMP POK
   Johanson.  M., 2013, TECHNICAL REPORT
   Johanson M., 2007, ADV NEURAL INFORM PR, P1729
   Johanson Michael, 2012, AAAI, P1371
   Kroer C., 2017, P ACM C EC COMP EC
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Moravcik Matej, 2017, SCIENCE
   Moravcik Matej, 2016, AAAI C ART INT AAAI
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Nesterov Y, 2005, SIAM J OPTIMIZ, V16, P235, DOI 10.1137/S1052623403422285
   Sandholm T, 2015, SCIENCE, V347, P122, DOI 10.1126/science.aaa4614
   Sandholm T, 2010, AI MAG, V31, P13, DOI 10.1609/aimag.v31i4.2311
   Sandholm Tuomas, 2015, AAAI C ART INT AAAI, P4127
   Schaeffer J, 2007, SCIENCE, V317, P1518, DOI 10.1126/science.1144079
   Schnizlein D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P278
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645
   Waugh Kevin, 2009, P ANN C NEUR INF PRO
NR 32
TC 0
Z9 0
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400066
DA 2019-06-15
ER

PT S
AU Caiafa, CF
   Sporns, O
   Saykin, AJ
   Pestilli, F
AF Caiafa, Cesar F.
   Sporns, Olaf
   Saykin, Andrew J.
   Pestilli, Franco
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unified representation of tractography and diffusion-weighted MRI data
   using sparse multidimensional arrays
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TENSOR DECOMPOSITIONS; COMPARTMENT MODELS; WHITE-MATTER; SIGNAL
AB Recently, linear formulations and convex optimization methods have been proposed to predict diffusion-weighted Magnetic Resonance Imaging (dMRI) data given estimates of brain connections generated using tractography algorithms. The size of the linear models comprising such methods grows with both dMRI data and connectome resolution, and can become very large when applied to modern data. In this paper, we introduce a method to encode dMRI signals and large connectomes, i.e., those that range from hundreds of thousands to millions of fascicles (bundles of neuronal axons), by using a sparse tensor decomposition. We show that this tensor decomposition accurately approximates the Linear Fascicle Evaluation (LiFE) model, one of the recently developed linear models. We provide a theoretical analysis of the accuracy of the sparse decomposed model, LiFESD, and demonstrate that it can reduce the size of the model significantly. Also, we develop algorithms to implement the optimization solver using the tensor representation in an efficient way.
C1 [Caiafa, Cesar F.; Sporns, Olaf; Pestilli, Franco] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.
   [Caiafa, Cesar F.] Consejo Nacl Invest Cient & Tecn, CIC PBA, IAR CCT La Plata, RA-1894 V Elisa, Argentina.
   [Saykin, Andrew J.] Indiana Univ Sch Med, Dept Radiol, Indianapolis, IN 46202 USA.
RP Caiafa, CF (reprint author), Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.; Caiafa, CF (reprint author), Consejo Nacl Invest Cient & Tecn, CIC PBA, IAR CCT La Plata, RA-1894 V Elisa, Argentina.
EM ccaiafa@gmail.com; osporns@indiana.edu; asaykin@iupui.edu;
   franpest@indiana.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1636893, BCS-1734853, BCS 1228397]; NIH [ULTTR001108]; Indiana
   University Areas of Emergent Research initiative Learning: Brains,
   Machines, Children
FX This research was supported by (NSF IIS-1636893; BCS-1734853; NIH
   ULTTR001108) to F.P. Data provided by Stanford University (NSF BCS
   1228397). F.P. were partially supported by the Indiana University Areas
   of Emergent Research initiative Learning: Brains, Machines, Children.
CR Alexander D. C., 2017, HUMAN BRAIN MAPPING, P1
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2239
   Barnathan M, 2011, NEUROIMAGE, V58, P537, DOI 10.1016/j.neuroimage.2011.06.043
   Basser PJ, 2000, MAGNET RESON MED, V44, P625, DOI 10.1002/1522-2594(200010)44:4<625::AID-MRM17>3.0.CO;2-O
   BASSER PJ, 1994, J MAGN RESON SER B, V103, P247, DOI 10.1006/jmrb.1994.1037
   Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502
   Bastiani M, 2012, NEUROIMAGE, V62, P1732, DOI 10.1016/j.neuroimage.2012.06.002
   Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043
   Caiafa CF, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-09250-w
   Caiafa Cesar F, 2012, NEURAL COMPUT, P186
   Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439
   Comon P, 2014, IEEE SIGNAL PROC MAG, V31, P44, DOI 10.1109/MSP.2014.2298533
   Cong FY, 2015, J NEUROSCI METH, V248, P59, DOI 10.1016/j.jneumeth.2015.03.018
   Daducci A., 2016, FRONTIERS NEUROSCIEN, V10, P1374
   Daducci A, 2015, IEEE T MED IMAGING, V34, P246, DOI 10.1109/TMI.2014.2352414
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Descoteaux M, 2009, IEEE T MED IMAGING, V28, P269, DOI 10.1109/TMI.2008.2004424
   Drysdale A. T., 2016, NAT MED, P1
   GILBERT JR, 1992, SIAM J MATRIX ANAL A, V13, P333, DOI 10.1137/0613024
   Glasser MF, 2016, NATURE, V536, P171, DOI 10.1038/nature18933
   Hazlett HC, 2017, NATURE, V542, P348, DOI 10.1038/nature21369
   Kim D, 2013, OPTIM METHOD SOFTW, V28, P1012, DOI 10.1080/10556788.2012.656368
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kroonenberg P.M., 2008, APPL MULTIWAY DATA A
   Li JN, 2016, IEEE SIGNAL PROC MAG, V33, P36, DOI 10.1109/MSP.2015.2510024
   Miwakeichi F, 2004, NEUROIMAGE, V22, P1035, DOI 10.1016/j.neuroimage.2004.03.039
   Morup M, 2006, NEUROIMAGE, V29, P938, DOI 10.1016/j.neuroimage.2005.08.005
   Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1
   Nedjati-Gilani GL, 2017, NEUROIMAGE, V150, P119, DOI 10.1016/j.neuroimage.2017.02.013
   Neher P. F., 2017, BIORXIV, P1
   Panagiotaki E, 2012, NEUROIMAGE, V59, P2241, DOI 10.1016/j.neuroimage.2011.09.081
   Pestilli F, 2014, NAT METHODS, V11, P1058, DOI 10.1038/nmeth.3098
   Rokem A, 2017, J VISION, V17, DOI 10.1167/17.2.4
   Rokem A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0123272
   Shah Parikshit, 2015, NIPS
   Smith RE, 2015, NEUROIMAGE, V119, P338, DOI 10.1016/j.neuroimage.2015.06.092
   Smith SM, 2015, NAT NEUROSCI, V18, P1565, DOI 10.1038/nn.4125
   Sporns O, 2013, NAT METHODS, V10, P491, DOI 10.1038/nmeth.2485
   Takemura H, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004692
   Tax C. M. W., 2016, HUMAN BRAIN MAPPING, P1
   Tournier JD, 2012, INT J IMAG SYST TECH, V22, P53, DOI 10.1002/ima.22005
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   van den Heuvel MP, 2016, TRENDS COGN SCI, V20, P345, DOI 10.1016/j.tics.2016.03.001
   van den Heuvel MP, 2011, J NEUROSCI, V31, P15775, DOI 10.1523/JNEUROSCI.3539-11.2011
   Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041
   Wandell BA, 2016, ANNU REV NEUROSCI, V39, P103, DOI 10.1146/annurev-neuro-070815-013815
   Wimalawarne K., 2014, NIPS
   Yu YY, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098441
   Zhao QB, 2011, ADV NEURAL INFORM PR, P1269
   Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254
   Zhu DJ, 2016, I S BIOMED IMAGING, P554, DOI 10.1109/ISBI.2016.7493329
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404040
DA 2019-06-15
ER

PT S
AU Calandriello, D
   Lazaric, A
   Valko, M
AF Calandriello, Daniele
   Lazaric, Alessandro
   Valko, Michal
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Second-Order Online Kernel Learning with Adaptive Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Online kernel learning (OKL) is a flexible framework for prediction problems, since the large approximation space provided by reproducing kernel Hilbert spaces often contains an accurate function for the problem. Nonetheless, optimizing over this space is computationally expensive. Not only first order methods accumulate O (root T) more loss than the optimal function, but the curse of kernelization results in a O(t) per-step complexity. Second-order methods get closer to the optimum much faster, suffering only O (log T) regret, but second-order updates are even more expensive with their O(t(2)) per-step cost. Existing approximate OKL methods reduce this complexity either by limiting the support vectors (SV) used by the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversarial environment can always exploit the approximation process. In this paper, we propose PROS-N-KONS, a method that combines Nystrom sketching to project the input point to a small and accurate embedded space; and to perform efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate. We show that the per-step cost only grows with the effective dimension of the problem and not with T. Moreover, the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably.
C1 [Calandriello, Daniele; Lazaric, Alessandro; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France.
RP Calandriello, D (reprint author), INRIA Lille Nord Europe, SequeL Team, Lille, France.
EM daniele.calandriello@inria.fr; alessandro.lazaric@inria.fr;
   michal.valko@inria.fr
RI Jeong, Yongwook/N-7413-2016
FU French Ministry of Higher Education and Research; Nord-Pas-de-Calais
   Regional Council; Inria; Univertat Potsdam associated-team
   north-european project Allocate; French National Research Agency
   [ANR-14-CE24-0010-01, ANR-16-CE23-0003]
FX The research presented was supported by French Ministry of Higher
   Education and Research, Nord-Pas-de-Calais Regional Council, Inria and
   Univertat Potsdam associated-team north-european project Allocate, and
   French National Research Agency projects ExTra-Learn
   (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003).
CR Calandriello Daniele, 2017, INT C MACH LEARN
   Calandriello Daniele, 2017, AISTATS
   Cavallanti G, 2007, MACH LEARN, V69, P143, DOI 10.1007/s10994-007-5003-0
   Cohen Michael B, 2016, INT WORKSH APPR RAND
   Dekel O, 2008, SIAM J COMPUT, V37, P1342, DOI 10.1137/060666998
   Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718
   Hazan Elad, 2006, C LEARN THEOR SPRING
   He WW, 2014, NEURAL NETWORKS, V60, P17, DOI 10.1016/j.neunet.2014.07.006
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Le Quoc, 2013, INT C MACH LEARN
   Le Trung, 2016, NEURAL INFORM PROCES
   Lu J, 2016, J MACH LEARN RES, V17
   Luo Haipeng, 2016, NEURAL INFORM PROCES
   Mahoney Michael W., 2015, NEURAL INFORM PROCES
   Orabona Francesco, 2008, INT C MACH LEARN
   Wang Z, 2012, J MACH LEARN RES, V13, P3103
   Wathen AJ, 2015, NUMER ALGORITHMS, V70, P709, DOI 10.1007/s11075-015-9970-0
   Williams Christopher, 2001, NEURAL INFORM PROCES
   Yang Tianbao, 2012, NEURAL INFORM PROCES
   Yang Y., 2017, ANN STAT
   Yi Sun, 2012, INT C MACH LEARN
   Yi Xu, 2017, AAAI C ART INT
   Zhao Peilin, 2012, INT C MACH LEARN
   Zhdanov Fedor, 2010, ALGORITHMIC LEARNING
   Zhu Changbo, 2015, ARXIV151202394
   Zinkevich Martin, 2003, INT C MACH LEARN
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406021
DA 2019-06-15
ER

PT S
AU Calmon, FP
   Wei, D
   Vinzamuri, B
   Ramamurthy, KN
   Varshney, KR
AF Calmon, Flavio P.
   Wei, Dennis
   Vinzamuri, Bhanukiran
   Ramamurthy, Karthikeyan Natesan
   Varshney, Kush R.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Optimized Pre-Processing for Discrimination Prevention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.
C1 [Calmon, Flavio P.] Harvard Univ, Cambridge, MA 02138 USA.
   [Wei, Dennis; Vinzamuri, Bhanukiran; Ramamurthy, Karthikeyan Natesan; Varshney, Kush R.] IBM Res AI, Yorktown Hts, NY USA.
RP Calmon, FP (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM flavio@seas.harvard.edu; dwei@us.ibm.com; bhanu.vinzamuri@ibm.com;
   knatesa@us.ibm.com; krvarshn@us.ibm.com
CR Calders T., 2013, DISCRIMINATION PRIVA, P43, DOI DOI 10.1007/978-3-642-30487-3_3
   Chouldechova Alexandra, 2016, ARXIV161007524
   Corbett-Davies Sam, 2017, ARXIV170108230
   Diamond S, 2016, J MACH LEARN RES, V17
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Fish B, 2016, P 2016 SIAM INT C DA, P144
   Friedler Sorelle A, 2016, ARXIV160907236
   Hajian S., 2013, THESIS
   Hajian S, 2013, IEEE T KNOWL DATA EN, V25, P1445, DOI 10.1109/TKDE.2012.72
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Johnson Kory D, 2016, ARXIV160800528
   Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Kleinberg J., 2017, P INN THEOR COMP SCI
   Kusner Matt J, 2017, ARXIV170306856
   Li N, 2007, INT CONF NANO MICRO, P692
   Lichman M., 2013, UCI MACHINE LEARNING
   Pearl J, 2014, AM STAT, V68, P8, DOI 10.1080/00031305.2014.876829
   Pedreschi D., 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959
   Pedreschi D., 2012, P ACM INT S APPL COM, P126
   ProPublica, COMPAS REC RISK SCOR
   Ruggieri S, 2014, TRANS DATA PRIV, V7, P99
   T. U. EEOC, 1979, UN GUID EMPL SEL PRO
   Zafar Muhammad Bilal, 2016, ARXIV161008452
   Zemel R., 2013, JMLR P, P325
   Zhang Z., 2016, P NIPS WORKSH INT MA
   Zliobaite I., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P992, DOI 10.1109/ICDM.2011.72
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404007
DA 2019-06-15
ER

PT S
AU Cecchi, F
   Hegde, N
AF Cecchi, Fabio
   Hegde, Nidhi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Active Hypothesis Testing under Limited Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SEQUENTIAL DESIGN
AB We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses. The decision maker may choose for a set of actions, where the outcome of an action is corrupted by independent noise. In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action, in that only a binary value is observed. Our objective is to infer the true hypothesis with low error, while minimizing the number of action sampled. Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.
C1 [Cecchi, Fabio] Eindhoven Univ Technol, Eindhoven, Netherlands.
   [Hegde, Nidhi] Nokia Bell Labs, Paris, France.
RP Cecchi, F (reprint author), Eindhoven Univ Technol, Eindhoven, Netherlands.
EM f.cecchi@tue.nl; nidhi.hegde@nokia-bell-labs.com
RI Jeong, Yongwook/N-7413-2016
CR ALBERT AE, 1961, ANN MATH STAT, V32, P774, DOI 10.1214/aoms/1177704973
   Berry SM, 2010, CH CRC BIOSTAT SER, P1, DOI 10.1201/EBK1439825488
   CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205
   Ghosh B., 1991, HDB SEQUENTIAL ANAL, V1
   Hui SC, 2000, INFORM MANAGE-AMSTER, V38, P1, DOI 10.1016/S0378-7206(00)00051-3
   KIEFER J, 1963, ANN MATH STAT, V34, P705, DOI 10.1214/aoms/1177704000
   Lalitha A, 2014, IEEE INT SYMP INFO, P551, DOI 10.1109/ISIT.2014.6874893
   Naghshvar M, 2013, ANN STAT, V41, P2703, DOI 10.1214/13-AOS1144
   Naghshvar M, 2012, ANN ALLERTON CONF, P1626, DOI 10.1109/Allerton.2012.6483415
   Naghshvar M, 2012, IEEE INT SYMP INFO
   Nowak RD, 2009, ADV NEURAL INFORM PR, V57, P1366
   Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293
   Vaidhiyan N. K., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2201, DOI 10.1109/ISIT.2012.6283844
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404011
DA 2019-06-15
ER

PT S
AU Cesa-Bianchi, N
   Gentile, C
   Lugosi, G
   Neu, G
AF Cesa-Bianchi, Nicole
   Gentile, Claudio
   Lugosi, Gabor
   Neu, Gergely
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Boltzmann Exploration Done Right
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID BOUNDS
AB Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions for the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon T and the suboptimality gap Delta). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order K log(2) T/Delta and a distribution- independent bound of order root KT log K without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.
C1 [Cesa-Bianchi, Nicole] Univ Milan, Milan, Italy.
   [Gentile, Claudio] INRIA Lille Nord Europe, Villeneuve Dascq, France.
   [Lugosi, Gabor] ICREA, Barcelona, Spain.
   [Lugosi, Gabor; Neu, Gergely] Univ Pompeu Fabra, Barcelona, Spain.
RP Cesa-Bianchi, N (reprint author), Univ Milan, Milan, Italy.
EM nicolo.cesa-bianchi@unimi.it; cla.gentile@gmail.com;
   gabor.lugosi@gmail.com; gergely.neu@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU Spanish Ministry of Economy and Competitiveness [MTM2015-67304-P];
   FEDER, EU; UPFellows Fellowship (Marie Curie COFUND program) [600387]
FX Gabor Lugosi was supported by the Spanish Ministry of Economy and
   Competitiveness, Grant MTM2015-67304-P and FEDER, EU. Gergely Neu was
   supported by the UPFellows Fellowship (Marie Curie COFUND program no
   600387).
CR Abernethy J., 2014, P 27 C LEARN THEOR C, V35, P807
   Agrawal S, 2013, P 16 INT C ART INT S, P99
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Audibert Jean-Yves, 2009, P 22 ANN C LEARN THE
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6
   Bubeck S, 2012, REGRET ANAL STOCHAST
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454
   Cesa-Bianchi N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P100
   Garivier A., 2016, NIPS
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18
   Kuleshov V, 2014, ARXIV14026028
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Osband I., 2016, GEN EXPLORATION VIA
   Perkins T. J., 2003, ADV NEURAL INFORM PR, P1595
   Seldin  Y., 2014, INT C MACH LEARN ICM, P1287
   Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559
   Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Szepesvari C., 2010, SYNTHESIS LECT ARTIF
   Vermorel J, 2005, LECT NOTES ARTIF INT, V3720, P437
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406035
DA 2019-06-15
ER

PT S
AU Chamon, LFO
   Ribeiro, A
AF Chamon, Luiz F. O.
   Ribeiro, Alejandro
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Approximate Supermodularity Bounds for Experimental Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SENSOR SELECTION; ALGORITHMS
AB This work provides performance guarantees for the greedy solution of experimental design problems. In particular, it focuses on A- and E-optimal designs, for which typical guarantees do not apply since the mean-square error and the maximum eigenvalue of the estimation error covariance matrix are not supermodular. To do so, it leverages the concept of approximate supermodularity to derive non-asymptotic worst-case suboptimality bounds for these greedy solutions. These bounds reveal that as the SNR of the experiments decreases, these cost functions behave increasingly as supermodular functions. As such, greedy A- and E-optimal designs approach (1 - e(-1))-optimality. These results reconcile the empirical success of greedy experimental design with the non-supermodularity of the A- and E-optimality criteria.
C1 [Chamon, Luiz F. O.; Ribeiro, Alejandro] Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA.
RP Chamon, LFO (reprint author), Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA.
EM luizf@seas.upenn.edu; aribeiro@seas.upenn.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [CCF 1717120]; ARO [W911NF1710438]
FX This work was supported by the National Science Foundation CCF 1717120
   and in part by the ARO W911NF1710438.
CR Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Benzi M, 2002, J COMPUT PHYS, V182, P418, DOI 10.1006/jcph.2002.7176
   Bian A., 2017, ICML
   Borodin A., 2014, ARXIV14016697V5
   Boyd S., 2004, CONVEX OPTIMIZATION
   BRAATZ RD, 1994, SIAM J CONTROL OPTIM, V32, P1763, DOI 10.1137/S0363012992238680
   Chamon L. F. O., 2016, GLOB C SIGN INF PROC
   Chamon L. F. O., 2017, ARXIV171101501
   Das A., 2011, INT C MACH LEARN
   Das A, 2008, ACM S THEORY COMPUT, P45
   Digital Equipment Corporation, EACHMOVIE DAT
   Elenberg E. R., 2016, ARXIV161200804
   Flaherty P., 2006, ADV NEURAL INFORM PR, P363
   Horel T., 2016, NIPS, V29, P3045
   Horel T., 2014, LAT AM THEOR INF S
   Horn R. A., 2013, MATRIX ANAL
   Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095
   Krause A., 2010, INT C MACH LEARN
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2014, TRACTABILITY, P71
   Liu SJ, 2016, IEEE T SIGNAL PROCES, V64, P3509, DOI 10.1109/TSP.2016.2550005
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109
   Sagnol G, 2013, DISCRETE APPL MATH, V161, P258, DOI 10.1016/j.dam.2012.07.016
   Sagnol G, 2011, J STAT PLAN INFER, V141, P1684, DOI 10.1016/j.jspi.2010.11.031
   Summers TH, 2016, IEEE TRANS CONTROL N, V3, P91, DOI 10.1109/TCNS.2015.2453711
   Wang Y., 2017, ARXIV160102068V5
   Washizawa Y., 2009, INT WORKSH MACH LEAR
   Yu Kai, 2006, P 23 INT C MACH LEAR, P1081
   Zhu X., 2008, SEMISUPERVISED LEARN
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405047
DA 2019-06-15
ER

PT S
AU Chang, HS
   Learned-Miller, E
   McCallum, A
AF Chang, Haw-Shiuan
   Learned-Miller, Erik
   McCallum, Andrew
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Active Bias: Training More Accurate Neural Networks by Emphasizing High
   Variance Samples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRADIENT; ONLINE
AB Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.
C1 [Chang, Haw-Shiuan; Learned-Miller, Erik; McCallum, Andrew] Univ Massachusetts, 140 Governors Dr, Amherst, MA 01003 USA.
RP Chang, HS (reprint author), Univ Massachusetts, 140 Governors Dr, Amherst, MA 01003 USA.
EM hschang@cs.umass.edu; elm@cs.umass.edu; mccallum@cs.umass.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1514053]; DARPA [FA8750-1 3-2-0020,
   HRO011-15-2-0036]
FX This material is based on research sponsored by National Science
   Foundation under Grant No. 1514053 and by DARPA under agreement number
   FA8750-1 3-2-0020 and HRO011-15-2-0036. The U.S. Government is
   authorized to reproduce and distribute reprints for Governmental
   purposes notwithstanding any copyright notation thereon. The views and
   conclusions contained herein are those of the authors and should not be
   interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of DARPA or the U.S.
   Government.
CR Alain G., 2015, ARXIV151106481
   Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420
   Avramova V., 2015, CURRICULUM LEARNING
   Bengio Y., 2009, ICML
   Bordes A, 2005, J MACH LEARN RES, V6, P1579
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chaudhari Pratik, 2017, ICLR
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Druck G., 2011, P 20 ACM INT C INF K, P947
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gao J., 2015, ARXIV151203880
   Gomez S., 2016, NIPS
   Gopal S., 2016, ICML
   Guillory A., 2009, AISTATS
   Gulcehre C., 2017, ICLR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6
   Houlsby N., 2011, ARXIV11125745
   Hovy E., 2006, HLT NAACL
   Johnson R., 2013, NIPS
   Kim Yoon, 2014, EMNLP
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kumar M. P., 2010, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee G. - H., 2016, ARXIV161009274
   Li X., 2002, COLING
   Loshchilov I., 2015, ARXIV151106343
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590
   Mandt S., 2016, AISTATS
   Mandt S., 2016, ICML
   Meng D., 2015, ARXIV151106049
   Mu Y., 2016, IEEE T KNOWLEDGE DAT
   Northcutt Curtis G, 2017, ARXIV170501936
   Pi T., 2016, IJCAI
   PREGIBON D, 1982, BIOMETRICS, V38, P485, DOI 10.2307/2530463
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Rennie JD, 2005, REGULARIZED LO UNPUB
   Sang E. F. Tjong Kim, 2003, HLT NAACL
   Schaul T., 2013, ICML
   Schein AI, 2007, MACH LEARN, V68, P235, DOI 10.1007/s10994-007-5019-5
   Schohn G., 2000, ICML
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Shrivastava A., 2016, CVPR
   Strubell E., 2017, ARXIV170202098
   Wang C., 2013, NIPS
   Wang Y., 2016, ARXIV160603860
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang Chiyuan, 2017, ICLR
   Zhao P., 2014, ARXIV14122753
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401005
DA 2019-06-15
ER

PT S
AU Chang, SY
   Zhang, Y
   Han, W
   Yu, M
   Guo, XX
   Tan, W
   Cui, XD
   Witbrock, M
   Hasegawa-Johnson, M
   Huang, TS
AF Chang, Shiyu
   Zhang, Yang
   Han, Wei
   Yu, Mo
   Guo, Xiaoxiao
   Tan, Wei
   Cui, Xiaodong
   Witbrock, Michael
   Hasegawa-Johnson, Mark
   Huang, Thomas S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dilated Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DILATEDRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections, and can be combined flexibly with diverse RNN cells. Moreover, the DILATEDRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DILATEDRNN over other recurrent neural architectures. The code for our method is publicly available(1).
C1 [Chang, Shiyu; Zhang, Yang; Yu, Mo; Guo, Xiaoxiao; Tan, Wei; Cui, Xiaodong; Witbrock, Michael] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Han, Wei; Hasegawa-Johnson, Mark; Huang, Thomas S.] Univ Illinois, Urbana, IL 61801 USA.
RP Chang, SY (reprint author), IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM shiyu.chang@ibm.com; yang.zhang2@ibm.com; weihan3@illinois.edu;
   yum@us.ibm.com; xiaoxiao.guo@ibm.com; wtan@us.ibm.com; cuix@us.ibm.com;
   witbrock@us.ibm.com; jhasegaw@illinois.edu; t-huang1@illinois.edu
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, ARXIV160304467
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Ba J. L., 2016, ARXIV160706450
   CAIANIELLO ER, 1982, INT J GEN SYST, V8, P81, DOI 10.1080/03081078208934843
   Chung J., 2014, CORR
   Chung J., 2016, ARXIV160901704
   Cooijmans T., 2016, ARXIV160309025
   Dieleman S., 2016, CORR
   El Hihi Salah, 1995, NIPS, V409
   Ha D., 2016, ARXIV160909106
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jaeger Herbert, 2001, SHORT TERM MEMORY EC, V5
   Koutnik J., 2014, ARXIV14023511
   Krueger D., 2016, ARXIV160601305
   Le Q. V., 2015, ARXIV150400941
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Neil Daniel, 2016, ARXIV161009513
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Sainath TN, 2015, 16 ANN C INT SPEECH
   Semeniuta S., 2016, ARXIV160305118
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Vezhnevets Alexander Sasha, 2017, ARXIV170301161
   Wisdom S., 2016, ADV NEURAL INFORM PR, P4880
   Xing Z., 2010, ACM SIGKDD EXPLOR NE, V12, P40, DOI DOI 10.1145/1882471.1882478
   Yamagishi Junichi, 2012, ENGLISH MULTI SPEAKE
   Yu A W, 2017, ARXIV170406877
   Yu F., 2015, ARXIV151107122
   Yu F, 2017, ARXIV170509914
   Zhang S., 2016, ADV NEURAL INFORM PR, P1822
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400008
DA 2019-06-15
ER

PT S
AU Chao, P
   Zhu, M
AF Chao, Pan
   Zhu, Michael
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Group Additive Structure Identification for Kernel Nonparametric
   Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However, its main drawback is that it neglects possible interactions between predictor variables. In this paper, we reexamine the group additive model proposed in the literature, and rigorously define the intrinsic group additive structure for the relationship between the response variable Y and the predictor vector X, and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure. Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression.
C1 [Chao, Pan; Zhu, Michael] Purdue Univ, Dept Stat, W Lafayette, IN 47906 USA.
   [Zhu, Michael] Tsinghua Univ, Dept Ind Engn, Ctr Stat Sci, Beijing, Peoples R China.
RP Chao, P (reprint author), Purdue Univ, Dept Stat, W Lafayette, IN 47906 USA.
EM panchao25@gmail.com; yuzhu@purdue.edu
RI Jeong, Yongwook/N-7413-2016
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bishop C. M., 2006, PATTERN RECOGNITION
   Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503
   Cucker F, 2002, B AM MATH SOC, V39, P1
   Gu C, 2013, SPRINGER SER STAT, V297, P1, DOI 10.1007/978-1-4614-5369-7
   Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604
   Kandasamy K., 2016, P 33 INT C INT C MAC, V48, P69
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kuhn T, 2011, J COMPLEXITY, V27, P489, DOI 10.1016/j.jco.2011.01.005
   Marlin B., 2009, P 26 INT C MACH LEAR, P705
   Murphy KP, 2012, MACHINE LEARNING PRO
   Pan C., 2015, P 21 ACM SIGKDD INT, P905
   Ramsay JO, 2002, APPL FUNCTIONAL DATA, V77
   Smola A. J., 1998, LEARNING KERNELS
   Steinwart I, 2008, INFORM SCI STAT, P1
   STONE CJ, 1982, ANN STAT, V10, P1040, DOI 10.1214/aos/1176345969
   Vapnik V., 2013, NATURE STAT LEARNING
   Zhou DX, 2002, J COMPLEXITY, V18, P739, DOI 10.1006/jcom.2002.0635
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404095
DA 2019-06-15
ER

PT S
AU Chatterji, NS
   Bartlett, PL
AF Chatterji, Niladri S.
   Bartlett, Peter L.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Alternating minimization for dictionary learning with random
   initialization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SPARSE; REPRESENTATIONS; ALGORITHM; SELECTION
AB We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize vector samples y(1), y(2),..., y(n) into an appropriate basis (dictionary) A* and sparse vectors x(1)*,..., x(n)*. Our algorithm is a simple alternating minimization procedure that switches between l(1 )minimization and gradient descent in alternate steps. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However, in contrast to previous theoretical analyses for this problem, we replace a condition on the operator norm (that is, the largest magnitude singular value) of the true underlying dictionary A* with a condition on the matrix infinity norm (that is, the largest magnitude term). This not only allows us to get convergence rates for the error of the estimated dictionary measured in the matrix infinity norm, but also ensures that a random initialization will provably converge to the global optimum. Our guarantees are under a reasonable generative model that allows for dictionaries with growing operator norms, and can handle an arbitrary level of over completeness, while having sparsity that is information theoretically optimal We also establish upper bounds on the sample complexity of our algorithm.
C1 [Chatterji, Niladri S.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Chatterji, NS (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM niladri.chatterji@berkeley.edu; peter@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1619362]; Australian Research Council [FL110100281]; Australian
   Research Council through ARC Centre of Excellence for Mathematical and
   Statistical Frontiers
FX We gratefully acknowledge the support of the NSF through grant
   IIS-1619362, and of the Australian Research Council through an
   Australian Laureate Fellowship (FL110100281) and through the ARC Centre
   of Excellence for Mathematical and Statistical Frontiers. Thanks also to
   the Simons Institute for the Theory of Computing Spring 2017 Program on
   Foundations of Machine Learning. The authors would like to thank Sahand
   Negahban for pointing out an error in the mu-incoherence assumption in
   an earlier version.
CR Agarwal A., 2013, ARXIV13091952
   Agarwal Alekh, 2014, C LEARN THEOR, P123
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Arora S., 2013, NEW ALGORITHMS LEARN
   Arora  S., 2015, COLT, P113
   Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435
   Barak B., 2015, P 47 ANN ACM S THEOR, P143, DOI DOI 10.1145/2746539.2746605
   Belloni A., 2014, ARXIV14127216
   Belloni A., 2016, J ROYAL STAT SOC B
   Boucheron S., 2013, CONCENTRATION INEQUA
   Chen Y., 2013, NOISY MISSING DATA R
   Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Fuchs J.-J., 2004, AC SPEECH SIGN PROC, V2, pii
   Gautier E., 2011, ARXIV11052454
   Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031
   Gribonval R, 2015, IEEE T INFORM THEORY, V61, P6298, DOI 10.1109/TIT.2015.2472522
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Hazan E., 2016, ADV NEURAL INFORM PR, P3306
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826
   Loh PL, 2011, ADV NEURAL INFORM PR, P2726
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Netrapalli P., 2014, P ADV NEUR INF PROC, P1107
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854
   Rosenbaum M., 2013, PROBABILITY STAT BAC, P276
   Rosenbaum M, 2010, ANN STAT, V38, P2620, DOI 10.1214/10-AOS793
   Spielman D. A., 2012, COLT, P37
   Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162
   Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Wu S., 2015, ARXIV150504363
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402005
DA 2019-06-15
ER

PT S
AU Chen, GB
   Choi, WG
   Yu, X
   Han, T
   Chandraker, M
AF Chen, Guobin
   Choi, Wongun
   Yu, Xiang
   Han, Tony
   Chandraker, Manmohan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Efficient Object Detection Models with Knowledge Distillation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Despite significant accuracy improvement in convolutional neural networks (CNN) based object detectors, they often require prohibitive runtimes to process an image for real-time applications. State-of-the-art models often use very deep networks with a large number of floating point operations. Efforts such as model compression learn compact models with fewer number of parameters, but with much reduced accuracy. In this work, we propose a new framework to learn compact and fast object detection networks with improved accuracy using knowledge distillation [20] and hint learning [34]. Although knowledge distillation has demonstrated excellent improvements for simpler classification setups, the complexity of detection poses new challenges in the form of regression, region proposals and less voluminous labels. We address this through several innovations such as a weighted cross-entropy loss to address class imbalance, a teacher bounded loss to handle the regression component and adaptation layers to better learn from intermediate teacher distributions. We conduct comprehensive empirical evaluation with different distillation configurations over multiple datasets including PASCAL, KITTI, ILSVRC and MS-COCO. Our results show consistent improvement in accuracy-speed trade-offs for modern multi-class detection models.
C1 [Chen, Guobin; Choi, Wongun; Yu, Xiang; Chandraker, Manmohan] NEC Labs Amer, Princeton, NJ 08540 USA.
   [Chen, Guobin; Han, Tony] Univ Missouri, Columbia, MO 65211 USA.
   [Chandraker, Manmohan] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Chen, GB (reprint author), NEC Labs Amer, Princeton, NJ 08540 USA.
RI Jeong, Yongwook/N-7413-2016
CR Ashraf K., 2016, CORR
   Ba J, 2014, ADV NEURAL INFORM PR, V1, P2654
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Chatfield Ken, 2014, ARXIV14053531
   Chen W., 2015, CORR
   Chen X, 2015, CORR, V1504, P325
   Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327
   Dai J., 2016, NIPS, P379
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, V27, P1269
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R. B., 2012, DISCRIMINATIVELY TRA, V5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong Y., 2014, ARXIV14126115
   Gupta Saurabh, 2015, ARXIV150700448
   Han S., 2016, ARXIV160201528
   Han S., 2015, CORR
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton G., 2015, ARXIV150302531
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Iandola F., 2016, ARXIV160207360
   Jaderberg M., 2014, ARXIV14053866
   JitendraMalik R. J. T., RICH FEATURE HIERARC
   Kim K., 2016, CORR
   Kim Y.-D., 2015, ARXIV151106530
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lebedev V., 2014, ARXIV14126553
   Novikov A., 2015, ADV NEURAL INFORM PR, P442
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren S., 2016, IEEE T PATTERN ANAL
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero Adriana, 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen J., 2016, ARXIV161200478
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Su J.-C., 2016, ARXIV160400433
   Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800
   Zhang C, 2016, ARXIV161103530
   Zhang X., 2015, ACCELERATING VERY DE
   Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400071
DA 2019-06-15
ER

PT S
AU Chen, H
   Wang, XQ
   Deng, C
   Huang, H
AF Chen, Hong
   Wang, Xiaoqian
   Deng, Cheng
   Huang, Heng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Group Sparse Additive Machine
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SOFT MARGIN CLASSIFIERS; MINIMAX-OPTIMAL RATES; SELECTION; RISK;
   CLASSIFICATION; CONSISTENCY; REGRESSION
AB A family of learning algorithms generated from additive models have attracted much attention recently for their flexibility and interpretability in high dimensional data analysis. Among them, learning models with grouped variables have shown competitive performance for prediction and variable selection. However, the previous works mainly focus on the least squares regression problem, not the classification task. Thus, it is desired to design the new additive classification model with variable selection capability for many real-world applications which focus on high-dimensional data classification. To address this challenging problem, in this paper, we investigate the classification with group sparse additive models in reproducing kernel Hilbert spaces. A novel classification method, called as group sparse additive machine (GroupSAM), is proposed to explore and utilize the structure information among the input variables. Generalization error bound is derived and proved by integrating the sample error analysis with empirical covering numbers and the hypothesis error estimate with the stepping stone technique. Our new bound shows that GroupSAM can achieve a satisfactory learning rate with polynomial decay. Experimental results on synthetic data and seven benchmark datasets consistently show the effectiveness of our new approach.
C1 [Chen, Hong; Wang, Xiaoqian; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
   [Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.
RP Huang, H (reprint author), Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
EM chenh@mail.hzau.edu.cn; xgwang1991@gmail.com; chdeng@mail.xidian.edu.cn;
   heng.huang@pitt.edu
RI Jeong, Yongwook/N-7413-2016
FU U.S. NSF [IIS 1302675]; NSF [IIS 1344152, IIS 1619308, IIS 1633753];
   NSF-DBI [1356628]; NIH [AG049371]; National Natural Science Foundation
   of China (NSFC) [11671161]
FX This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS
   1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH
   AG049371. Hong Chen was partially supported by National Natural Science
   Foundation of China (NSFC) 11671161. We are grateful to the anonymous
   NIPS reviewers for the insightful comments.
CR Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen DR, 2004, J MACH LEARN RES, V5, P1143
   Chen H, 2013, NEURAL COMPUT, V25, P1107, DOI 10.1162/NECO_a_00421
   Christmann A, 2016, ANAL APPL, V14, P449, DOI 10.1142/S0219530515500050
   Christmann A, 2012, COMPUT STAT DATA AN, V56, P854, DOI 10.1016/j.csda.2011.04.006
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   Edmunds D. E., 1996, FUNCTION SPACES ENTR
   Guo ZC, 2013, ADV COMPUT MATH, V38, P207, DOI 10.1007/s10444-011-9238-8
   Huang JA, 2010, ANN STAT, V38, P2282, DOI 10.1214/09-AOS781
   Kandasamy Kirthevasan, 2016, ICML
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin Y, 2006, ANN STAT, V34, P2272, DOI 10.1214/009053606000000722
   Lv S., 2017, ANN STAT
   Meier L, 2009, ANN STAT, V37, P3779, DOI 10.1214/09-AOS692
   Raskutti G, 2012, J MACH LEARN RES, V13, P389
   Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x
   Shi L, 2013, APPL COMPUT HARMON A, V34, P252, DOI 10.1016/j.acha.2012.05.001
   Shi L, 2011, APPL COMPUT HARMON A, V31, P286, DOI 10.1016/j.acha.2011.01.001
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wu Q, 2005, NEURAL COMPUT, V17, P1160, DOI 10.1162/0899766053491896
   Wu Q, 2007, J COMPLEXITY, V23, P108, DOI 10.1016/j.jco.2006.06.007
   Yang L, 2016, J MACH LEARN RES, V17
   Yin J., 2012, ICML
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Yuan M, 2016, ANN STAT, V44, P2564, DOI 10.1214/15-AOS1422
   Zhang T, 2004, ANN STAT, V32, P56
   Zhao T., 2012, AISTATS
   Zhong L. W., 2011, ICML
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400019
DA 2019-06-15
ER

PT S
AU Chen, JB
   Stern, M
   Wainwright, MJ
   Jordan, MI
AF Chen, Jianbo
   Stern, Mitchell
   Wainwright, Martin J.
   Jordan, Michael, I
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Kernel Feature Selection via Conditional Covariance Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DEPENDENCE; REDUCTION
AB We propose a method for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we show how to perform feature selection via a constrained optimization problem involving the trace of the conditional covariance operator. We prove various consistency results for this procedure, and also demonstrate that our method compares favorably with other state-of-the-art algorithms on a variety of synthetic and real data sets.
C1 [Chen, Jianbo; Stern, Mitchell; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Chen, JB (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM jianbochen@berkeley.edu; mitchell@berkeley.edu; wainwrig@berkeley.edu;
   jordan@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
CR Allen GI, 2013, J COMPUT GRAPH STAT, V22, P284, DOI 10.1080/10618600.2012.681213
   Bachrachy R., 2004, P 21 INT C MACH LEAR, P43
   BAKER CR, 1973, T AM MATH SOC, V186, P273, DOI 10.2307/1996566
   Bolon-Canedo V, 2015, KNOWL-BASED SYST, V86, P33, DOI 10.1016/j.knosys.2015.05.014
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82
   Cao B, 2007, P 24 INT C MACH LEAR, P121, DOI [10.1145/1273496.1273512, DOI 10.1145/1273496.1273512]
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Friedman J., 2001, SPRINGER SERIES STAT, V1
   Fukumizu K, 2004, J MACH LEARN RES, V5, P73
   Fukumizu K., 2012, ADV NEURAL INFORM PR, P2114
   Fukumizu K, 2009, ANN STAT, V37, P1871, DOI 10.1214/08-AOS637
   Grandvalet Yves, 2002, P 15 INT C NEUR INF, P569
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Jaganathan P, 2011, COMM COM INF SC, V190, P683
   Li J., 2016, ARXIV160107996
   Masaeli M., 2010, P 27 INT C MACH LEAR, P751
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Rahimi Ali, 2007, NEURAL INFOM PROCESS
   Ren Shaogang, 2015, P 18 INT C ART INT S, V38, P781
   Song L., 2007, P 24 INT C MACH LEAR, P823, DOI DOI 10.1145/1273496.1273600
   Song L, 2012, J MACH LEARN RES, V13, P1393
   Sun SQ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102541
   Weston J., 2003, Journal of Machine Learning Research, V3, P1439, DOI 10.1162/153244303322753751
   Weston J, 2000, P 13 INT C NEUR INF, V13, P647
   Yamada M, 2014, NEURAL COMPUT, V26, P185, DOI 10.1162/NECO_a_00537
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407004
DA 2019-06-15
ER

PT S
AU Chen, JF
   Li, CX
   Ru, YZ
   Zhu, J
AF Chen, Jianfei
   Li, Chongxuan
   Ru, Yizhong
   Zhu, Jun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Population Matching Discrepancy and Applications in Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A differentiable estimation of the distance between two distributions based on samples is important for many deep learning tasks. One such estimation is maximum mean discrepancy (MMD). However, MMD suffers from its sensitive kernel bandwidth hyper-parameter, weak gradients, and large mini-batch size when used as a training objective. In this paper, we propose population matching discrepancy (PMD) for estimating the distribution distance based on samples, as well as an algorithm to learn the parameters of the distributions using PMD as an objective. PMD is defined as the minimum weight matching of sample populations from each distribution, and we prove that PMD is a strongly consistent estimator of the first Wasserstein metric. We apply PMD to two deep learning tasks, domain adaptation and generative modeling Empirical results demonstrate that PMD overcomes the aforementioned drawbacks of MMD, and outperforms MMD on both tasks in terms of the performance as well as the convergence speed.
C1 [Chen, Jianfei; Li, Chongxuan; Ru, Yizhong; Zhu, Jun] Tsinghua Univ, State Key Lab Intell Tech & Sys, TNList Lab, Dept Comp Sci & Tech, Beijing 100084, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, State Key Lab Intell Tech & Sys, TNList Lab, Dept Comp Sci & Tech, Beijing 100084, Peoples R China.
EM chenjian14@mails.tsinghua.edu.cn; licx14@mails.tsinghua.edu.cn;
   ruyz13@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant
   of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support
   Program, Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA
   NVAIL Program
FX This work is supported by the National NSF of China (Nos. 61620106010,
   61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No.
   2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua
   Tiangong Institute for Intelligent Computing and the NVIDIA NVAIL
   Program.
CR Abadi M., 2016, ARXIV160304467
   Agrawal Siddharth, GENERATIVE MOMENT MA
   Arjovsky M., 2017, ARXIV170107875
   Bellemare M. G., 2017, ARXIV170510743
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bertsekas D. P., 1998, NETWORK OPTIMIZATION
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bousmalis K., 2016, ADV NEURAL INFORM PR, P343
   Cho  K., 2014, EMNLP
   Darrell T., 2014, ARXIV14123474
   Date K, 2016, PARALLEL COMPUT, V57, P52, DOI 10.1016/j.parco.2016.05.012
   Drake Doratha, 2003, APPROXIMATION ROM, P21
   Dziugaite G. K., 2015, UAI
   Ganin Y, 2016, J MACH LEARN RES, V17
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Glorot X., 2011, AISTATS, V15, P275
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gretton  A., 2012, ADV NEURAL INFORM PR, P1205
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang G.B., 2007, 0749 U MASS AMH
   Ioffe S., 2015, ICML
   Kingma D., 2014, ADAM METHOD STOCHAST
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kuhn H. W., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Chun- Liang, 2017, ARXIV170508584
   Li N, 2007, INT CONF NANO MICRO, P692
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Lloyd JR, 2015, ADV NEURAL INFORM PR, P829
   Long M., 2017, ICML
   Manne F, 2008, LECT NOTES COMPUT SC, V4967, P708
   Mohamed S., 2016, ARXIV161003483
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Radford A., 2015, ARXIV151106434
   Rezende D. J., 2014, ICML
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Simonyan Karen, 2015, ICLR
   Song Le, 2011, P INT C ART INT STAT, P707
   Sriperumbudur BK, 2010, IEEE INT SYMP INFO, P1428, DOI 10.1109/ISIT.2010.5513626
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26
   van den Oord A., 2016, ICML
   Varadarajan VS, 1958, SANKHYA, V19, P15
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wan XJ, 2007, INFORM SCIENCES, V177, P3718, DOI 10.1016/j.ins.2007.02.045
   Zeiler M.D., 2012, ARXIV12125701
   Zellinger Werner, 2017, ICLR
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406033
DA 2019-06-15
ER

PT S
AU Chen, JS
   Wang, C
   Xiao, L
   He, J
   Li, LH
   Deng, L
AF Chen, Jianshu
   Wang, Chong
   Xiao, Lin
   He, Ji
   Li, Lihong
   Deng, Li
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision
   Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.
C1 [Chen, Jianshu; Wang, Chong; Xiao, Lin; He, Ji; Li, Lihong; Deng, Li] Microsoft Res, Redmond, WA 98052 USA.
   [Wang, Chong; Li, Lihong] Google Inc, Kirkland, WA USA.
   [He, Ji; Deng, Li] Citadel LLC, Seattle, WA USA.
   [He, Ji; Deng, Li] Citadel LLC, Chicago, IL USA.
RP Chen, JS (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM jianshuc@microsoft.com; chongw@google.com; lin.xiao@microsoft.com;
   Ji.He@citadel.com; lihong@google.com; Li.Deng@citadel.com
CR Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116
   Blei D., 2007, ADV NEURAL INFORM PR, P121
   Blei D. M., 2006, ICML, P113, DOI DOI 10.1145/1143844.1143859
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bouchard G., 2004, P COMP STAT 16 S IAS, P721
   Chen Jianshu, 2015, ADV NEURAL INFORM PR, V28, P1765
   Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377
   Hausknecht Matthew, 2015, P AAAI SDMIA NOV
   He Ji, 2016, P ACL
   Holub A, 2005, PROC CVPR IEEE, P664
   Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105
   Kapadia S., 1998, THESIS
   Lasserre J, 2007, BAYESIAN STAT, V8, P3
   Levine S, 2016, J MACH LEARN RES, V17
   Lin LJ, 1993, TECHNICAL REPORT
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Narasimhan Karthik, 2015, P EMNLP
   Saul L, 1998, NATO ADV SCI I D-BEH, V89, P541
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Yakhnenko Oksana, 2005, P IEEE ICDM
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405006
DA 2019-06-15
ER

PT S
AU Chen, L
   Krause, A
   Karbasi, A
AF Chen, Lin
   Krause, Andreas
   Karbasi, Amin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Interactive Submodular Bandit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible. In many real life situations, however, the utility function is not fully known in advance and can only be estimated via interactions. For instance, whether a user likes a movie or not can be reliably evaluated only after it was shown to her. Or, the range of influence of a user in a social network can be estimated only after she is selected to advertise the product. We model such problems as an interactive submodular bandit optimization, where in each round we receive a context (e.g., previously selected movies) and have to choose an action (e.g., propose a new movie). We then receive a noisy feedback about the utility of the action (e.g., ratings) which we model as a submodular function over the context-action space. We develop SM-UCB that efficiently trades off exploration (collecting more data) and exploration (proposing a good action given gathered data) and achieves a O(root T) regret bound after T rounds of interaction. More specifically, given a bounded-RKHS norm kernel over the context-action-payoff space that governs the smoothness of the utility function, SM-UCB keeps an upper-confidence bound on the payoff function that allows it to asymptotically achieve no-regret. Finally, we evaluate our results on four concrete applications, including movie recommendation (on the MovieLense data set), news recommendation (on Yahoo! Webscope dataset), interactive influence maximization (on a subset of the Facebook network), and personalized data summarization (on Reuters Corpus). In all these applications, we observe that SM-UCB consistently outperforms the prior art.
C1 [Chen, Lin; Karbasi, Amin] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.
   [Chen, Lin; Karbasi, Amin] Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.
   [Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Chen, L (reprint author), Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.; Chen, L (reprint author), Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.
EM lin.chen@yale.edu; krausea@ethz.ch; amin.karbasi@yale.edu
RI Jeong, Yongwook/N-7413-2016
OI Crawford, Forrest/0000-0002-0046-0547
FU DARPA Young Faculty Award [D16AP00046]; ERC StG; SCADAPT
FX This research was supported by DARPA Young Faculty Award (D16AP00046),
   grant SCADAPT and ERC StG.
CR Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Awasthi P., 2014, ICML, P550
   Badanidiyuru A., 2016, P 27 ANN ACM SIAM S, P414
   Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Barrenetxea G, 2008, SENSYS'08: PROCEEDINGS OF THE 6TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P43
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chen Lin, 2017, AAAI, P1798
   Chen Wei, 2016, JMLR, V17, P1746
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Cohn D. A., 1995, Advances in Neural Information Processing Systems 7, P705
   Dasgupta S., 2008, ADV NEURAL INFORM PR, P353
   Dasgupta Sanjoy, 2005, ADV NEURAL INFORM PR, V17, P337
   El-Arini K, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P289
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Gartner T, 2008, SER MACH PERCEPT ART, V72, P1
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Gomez-Rodriguez Manuel, 2016, ACM TOIS
   Grover Aditya, 2016, KDD, V2016, P855
   Guillory Andrew, 2010, ICML
   Hassidim Avinatan, 2017, COLT, P1069
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Huang RZ, 2009, DATA KNOWL ENG, V68, P49, DOI 10.1016/j.datak.2008.08.008
   Javdani  S., 2014, AISTATS, V14, P430
   Kahraman HT, 2013, KNOWL-BASED SYST, V37, P283, DOI 10.1016/j.knosys.2012.08.009
   Karbasi Amin, 2012, P 29 INT C MACH LEAR, P855
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Krause A., 2012, TRACTABILITY PRACT A, V3, P8
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2
   Krause Andreas, 2011, ADV NEURAL INFORM PR, P2447
   Lei S., 2015, P 21 ACM SIGKDD INT, P645
   Leskovec J., 2012, ADV NEURAL INFORM PR, P539
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Lin T., 2015, P INT C NEUR INF PRO, P352
   Lu Tyler, 2010, INT C ART INT STAT, V9, P485
   Mirzasoleiman B., 2016, NIPS, P3594
   Mirzasoleiman Baharan, 2016, ICML, P1358
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033
   Streeter M., 2009, P ADV NEUR INF PROC, P1577
   TONG S, 2001, J MACHINE LEARNING R, V2, P45, DOI DOI 10.1162/153244302760185243
   Vanchinathan Hastagiri, 2015, ACM SIGKDD
   Wang Sida I., 2016, ACL
   Wang Y, 2009, NIPS, P1729
   Yue Y., 2011, NIPS, V23, P2483
   Zhang Yuanxing, 2016, P 2016 IEEE GLOB COM, P1
   Zhou Jiaji, 2013, INF WORKSH ICML CIT
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400014
DA 2019-06-15
ER

PT S
AU Chen, S
   Banerjee, A
AF Chen, Sheng
   Banerjee, Arindam
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Alternating Estimation for Structured High-Dimensional Multi-Response
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REGRESSION; LASSO
AB We consider the problem of learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among different responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector (GDS). Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation with general structures.
C1 [Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Chen, S (reprint author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM shengc@cs.umn.edu; banerjee@cs.umn.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986,
   CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX The research was supported by NSF grants IIS-1563950, IIS-1447566,
   IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and
   Yahoo.
CR Agarwal A., 2013, CORR
   Anderson T. W, 2003, INTRO MULTIVARIATE S
   Argyriou A., 2012, NIPS
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bach Francis, 2011, OPTIMIZATION MACHINE, V5
   Banerjee A., 2014, ADV NEURAL INFORM PR
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee S., 2014, ADV NEURAL INFORM PR
   Chen S., 2016, ADV NEURAL INFORM PR
   Chen S., 2015, NIPS, P2908
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Goncalves A R, 2014, CIKM, P451, DOI DOI 10.1145/2661829.2662091
   GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761
   Greene William H., 2011, ECONOMETRIC ANAL
   Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1
   Jacob L., 2009, ICML
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jain P., 2015, ADV NEURAL INFORM PR, P1126
   Jalali A, 2010, ADV NEURAL INFORM PR, P964
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013
   Liu H., 2009, P 26 ANN INT C MACH, P649, DOI DOI 10.1145/1553374.1553458
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Netrapalli P., 2013, NIPS
   Rai P., 2012, NIPS, P3185
   Rao N., 2012, INT C ART INT STAT A
   Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188
   Sohn K. -A., 2012, P INT C ART INT STAT, P1081
   Sun R., 2015, FOCS
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp J. A., 2015, CONVEX RECOVERY STRU, P67
   Wytock M., 2013, P 30 INT C MACH LEAR, P1265
   Yi X., 2014, P 31 INT C MACH LEAR, P613
   Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402086
DA 2019-06-15
ER

PT S
AU Chen, SX
   Ma, SQ
   Liu, W
AF Chen, Shixiang
   Ma, Shiqian
   Liu, Wei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Geometric Descent Method for Convex Composite Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMIZATION
AB In this paper, we extend the geometric descent method recently proposed by Bubeck, Lee and Singh [1] to tackle nonsmooth and strongly convex composite problems. We prove that our proposed algorithm, dubbed geometric proximal gradient method (GeoPG), converges with a linear rate (1 - 1 root kappa) and thus achieves the optimal rate among first-order methods, where i is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method, especially when the problem is ill-conditioned.
C1 [Chen, Shixiang] Chinese Univ Hong Kong, Dept SEEM, Hong Kong, Peoples R China.
   [Ma, Shiqian] Univ Calif Davis, Dept Math, Davis, CA USA.
   [Liu, Wei] Tencent AI Lab, Beijing, Peoples R China.
RP Chen, SX (reprint author), Chinese Univ Hong Kong, Dept SEEM, Hong Kong, Peoples R China.
RI Jeong, Yongwook/N-7413-2016
FU CUHK Research Postgraduate Student Grant for Overseas Academic
   Activities
FX Shixiang Chen is supported by CUHK Research Postgraduate Student Grant
   for Overseas Academic Activities. Shiqian Ma is supported by a startup
   funding in UC Davis.
CR Attouch H., 2016, MATH PROGRAMMING
   Beck A, 2007, J GLOBAL OPTIM, V39, P113, DOI 10.1007/s10898-006-9127-8
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BLAND RG, 1981, OPER RES, V29, P1039, DOI 10.1287/opre.29.6.1039
   Brent Richard P, 1973, ALGORITHMS MINIMIZAT
   Bubeck S., 2015, ARXIV150608187
   Bubeck S., 2016, ICML
   Dekker T. J., 1969, CONSTRUCTIVE ASPECTS
   Drusvyatskiy D., 2016, SIAM J OPTIMIZATION
   Eldar YC, 2008, IEEE T SIGNAL PROCES, V56, P1388, DOI 10.1109/TSP.2007.908945
   Gerdts M, 2017, J IND MANAG OPTIM, V13, P47, DOI 10.3934/jimo.2016003
   Hans E, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/2/025005
   Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Scheinberg K, 2014, FOUND COMPUT MATH, V14, P389, DOI 10.1007/s10208-014-9189-9
   Su Weijie, 2014, NIPS
   Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400061
DA 2019-06-15
ER

PT S
AU Cheng, CA
   Boots, B
AF Cheng, Ching-An
   Boots, Byron
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variational Inference for Gaussian Process Models with Linear Complexity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Large-scale Gaussian process inference has long faced practical challenges due to time and space complexity that is superlinear in dataset size. While sparse variational Gaussian process models are capable of learning from large-scale data, standard strategies for sparsifying the model can prevent the approximation of complex functions. In this work, we propose a novel variational Gaussian process model that decouples the representation of mean and covariance functions in reproducing kernel Hilbert space. We show that this new parametrization generalizes previous models. Furthermore, it yields a variational inference problem that can be solved by stochastic gradient ascent with time and space complexity that is only linear in the number of mean function parameters, regardless of the choice of kernels, likelihoods, and inducing points. This strategy makes the adoption of large-scale expressive Gaussian process models possible. We run several experiments on regression tasks and show that this decoupled approach greatly outperforms previous sparse variational Gaussian process inference procedures.
C1 [Cheng, Ching-An; Boots, Byron] Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
RP Cheng, CA (reprint author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
EM cacheng@gatech.edu; bboots@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF NRI [1637758]
FX This work was supported in part by NSF NRI award 1637758. The authors
   additionally thank the reviewers and Hugh Salimbeni for productive
   discussion which improved the quality of the paper.
CR Bauer Matthias, 2016, ADV NEURAL INFORM PR, V29, P1525
   Cheng CA, 2016, IEEE T CYBERNETICS, V46, P3247, DOI 10.1109/TCYB.2015.2501842
   Cheng Ching- An, 2016, ADV NEURAL INFORM PR, P4403
   Csato L, 2001, ADV NEUR IN, V13, P444
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   de Alexander G., 2016, P 19 INT C ART INT S
   Eldredge Nathaniel, 2016, ARXIV160703591
   Figueiras-vidal Anibal, 2009, ADV NEURAL INFORM PR, P1087
   Germain P., 2016, ADV NEURAL INFORM PR, P1884
   Gross L., 1967, P 5 BERK S MATH STAT, P31
   Hensman J, 2013, ARXIV13096835
   Hensman J, 2016, ARXIV161106740
   Hensman James, 2015, INT C ART INT STAT
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Meier F., 2014, ADV NEURAL INFORM PR, P972
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Seeger M., 2003, ARTIFICIAL INTELLIGE, V9
   Sheth Rishit, 2015, P 32 INT C MACH LEAR, P1302
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Titsias M, 2009, ARTIF INTELL, P567
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Walder C., 2008, P 25 INT C MACH LEAR, P1112
   Wilson A., 2015, INT C MACH LEARN, P1775
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405026
DA 2019-06-15
ER

PT S
AU Chevallier, J
   Oudard, S
   Allassonniere, S
AF Chevallier, Juliette
   Oudard, Stephan
   Allassonniere, Stephanie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning spatiotemporal piecewise-geodesic trajectories from
   longitudinal manifold-valued data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MAXIMUM-LIKELIHOOD; GUIDELINES; MODELS
AB We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements and individual variability. This model falls into the well defined mixed-effect models. The subject-specific trajectories are defined through spatial and temporal transformations of the group-average piecewise-geodesic path, component by component. Thus we can apply our model to a wide variety of situations. Due to the non-linearity of the model, we use the Stochastic Approximation Expectation-Maximization algorithm to estimate the model parameters. Experiments on synthetic data validate this choice. The model is then applied to the metastatic renal cancer chemotherapy monitoring: we run estimations on RECIST scores of treated patients and estimate the time they escape from the treatment. Experiments highlight the role of the different parameters on the response to treatment.
C1 [Chevallier, Juliette] Ecole Polytech, CMAP, Palaiseau, France.
   [Oudard, Stephan] USPC, AP HP, HEGP, Oncol Dept, Paris, France.
   [Allassonniere, Stephanie] Univ Paris 05, CRC, Paris, France.
RP Chevallier, J (reprint author), Ecole Polytech, CMAP, Palaiseau, France.
EM juliette.chevallier@polytechnique.edu;
   atephanie.allassonniere@parisdescartes.fr
FU Investissement d'avenir [ANR-11-LABX-0056-LMH]; Fondation of Medical
   Research [DBI20131228564]
FX Ce travail beneficie d'un financement public Investissement d'avenir,
   reference ANR-11-LABX-0056-LMH. This work was supported by a public
   grant as part of the Investissement d'avenir, project reference
   ANR-11-LABX-0056-LMH.; Travail realise dans le cadre d'un projet finance
   par la Fondation de la Recherche Medicale, "DBI20131228564". Work
   performed as a part of a project funded by the Fondation of Medical
   Research, grant number "DBI20131228564".
CR Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229
   Burotto M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096316
   Delyon B, 1999, ANN STAT, V27, P94
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Escudier B, 2016, ANN ONCOL, V27, pv58, DOI 10.1093/annonc/mdw328
   Gallot S, 2004, RIEMANNIAN GEOMETRY
   Kuhn E, 2005, COMPUT STAT DATA AN, V49, P1020, DOI 10.1016/j.csda.2004.07.002
   LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876
   Robert Christian P., 1999, SPRINGER TEXTS STAT
   Rothermundt C, 2017, WORLD J UROL, V35, P641, DOI 10.1007/s00345-016-1903-6
   Rothermundt C, 2015, ONCOLOGIST, V20, P1028, DOI 10.1634/theoncologist.2015-0145
   Schiratti JB, 2015, ADV NEUR IN, V28
   Stein WD, 2008, ONCOLOGIST, V13, P1046, DOI 10.1634/theoncologist.2008-0075
   Therasse P, 2000, J NATL CANCER I, V92, P205, DOI 10.1093/jnci/92.3.205
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401019
DA 2019-06-15
ER

PT S
AU Chierichetti, F
   Kumar, R
   Lattanzi, S
   Vassilvitskii, S
AF Chierichetti, Flavio
   Kumar, Ravi
   Lattanzi, Silvio
   Vassilvitskii, Sergei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fair Clustering Through Fairlets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the question of fair clustering under the disparate impact doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions-for instance a point may no longer be assigned to its nearest cluster center!
   En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms. While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow.
   We empirically demonstrate the price of fairness by quantifying the value of fair clustering on real-world datasets with sensitive attributes.
C1 [Chierichetti, Flavio] Sapienza Univ, Dipartimento Informat, Rome, Italy.
   [Kumar, Ravi] Google Res, 1600 Amphitheater Pkwy, Mountain View, CA 94043 USA.
   [Lattanzi, Silvio; Vassilvitskii, Sergei] Google Res, 76 9th Ave, New York, NY 10011 USA.
RP Chierichetti, F (reprint author), Sapienza Univ, Dipartimento Informat, Rome, Italy.
RI Jeong, Yongwook/N-7413-2016
FU ERC [DMAP 680153]; Google Focused Research Award; SIR Grant [RBSI14Q743]
FX Flavio Chierichetti was supported in part by the ERC Starting Grant DMAP
   680153, by a Google Focused Research Award, and by the SIR Grant
   RBSI14Q743.
CR Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1
   Arya V, 2004, SIAM J COMPUT, V33, P544, DOI [10.1137/S0097539702416402, 10.1137/S00097539702416402]
   Biddle D. A., 2006, ADVERSE IMPACT TEST
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Joseph Matthew, 2016, ADV NEURAL INFORM PR, V29, P325
   Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kleinberg J., 2017, 23180 NBER
   Kleinberg Jon, 2017, ITCS
   Kohavi R., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P202
   Li S., 2013, P 45 ANN ACM S THEOR, P901
   Lichman M., 2013, UCI MACHINE LEARNING
   Luong B. T., 2011, P 17 ACM SIGKDD INT, P502, DOI DOI 10.1145/2020408.2020488
   Moro S, 2014, DECIS SUPPORT SYST, V62, P22, DOI 10.1016/j.dss.2014.03.001
   Xu R, 2009, CLUSTERING
   Zafar Muhammad Bilal, 2017, AISTATS, P259
   Zemel R., 2013, JMLR P, P325
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405011
DA 2019-06-15
ER

PT S
AU Cho, M
   Lee, J
AF Cho, Minhyung
   Lee, Jaehyung
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Riemannian approach to batch normalization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GEOMETRY
AB Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.
C1 [Cho, Minhyung] Appl Res Korea, Seongnam, South Korea.
   [Lee, Jaehyung] Gracenote Inc, Emeryville, CA USA.
RP Cho, M (reprint author), Appl Res Korea, Seongnam, South Korea.
EM mhyung.cho@gmail.com; jaehyung.lee@kaist.ac.kr
RI Jeong, Yongwook/N-7413-2016
CR Absil PA, 2004, ACTA APPL MATH, V80, P199, DOI 10.1023/B:ACAP.0000013855.14971.91
   Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Arpit Devansh, 2016, INT C MACH LEARN, P1168
   Ba J. L., 2016, ARXIV160706450
   Badrinarayanan Vijay, 2015, ARXIV151101029
   Clevert D.A., 2015, ARXIV151107289
   do Carmo M.P., 1976, DIFFERENTIAL GEOMETR
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Ghahramani Z, 1996, CRGTR961 U TOR
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Hamm J., 2009, ADV NEURAL INFORM PR, P601
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe  S., 2017, ARXIV170203275
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2009, THESIS
   Lee C.-Y., 2016, INT C ART INT STAT
   Martens J., 2015, INT C MACH LEARN, P2408
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Neyshabur B., 2015, ADV NEURAL INFORM PR, P2422
   Pascanu Razvan, 2014, INT C LEARN REPR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Snoek J., 2015, P 32 INT C MACH LEAR, P2171
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Zagoruyko S., 2016, BMVC
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405030
DA 2019-06-15
ER

PT S
AU Choi, A
   Shen, YJ
   Darwiche, A
AF Choi, Arthur
   Shen, Yujia
   Darwiche, Adnan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Tractability in Structured Probability Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recently, the Probabilistic Sentential Decision Diagram (PSDD) has been proposed as a framework for systematically inducing and learning distributions over structured objects, including combinatorial objects such as permutations and rankings, paths and matchings on a graph, etc. In this paper, we study the scalability of such models in the context of representing and learning distributions over routes on a map. In particular, we introduce the notion of a hierarchical route distribution and show how they can be leveraged to construct tractable PSDDs over route distributions, allowing them to scale to larger maps. We illustrate the utility of our model empirically, in a route prediction task, showing how accuracy can be increased significantly compared to Markov models.
C1 [Choi, Arthur; Shen, Yujia; Darwiche, Adnan] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
RP Choi, A (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
EM aychoi@cs.ucla.edu; yujias@cs.ucla.edu; darwiche@cs.ucla.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1514253]; ONR [N00014-15-1-2339]; DARPA XAI [N66001-17-2-4032]
FX We greatly thank Noah Hadfield-Menell and Andy Shih for their
   contributions, and Eunice Chen for helpful discussions. This work has
   been partially supported by NSF grant #IIS-1514253, ONR grant
   #N00014-15-1-2339 and DARPA XAI grant #N66001-17-2-4032.
CR Choi A., 2015, P IJCAI
   Choi A., 2016, P 30 AAAI C ART INT
   Darwiche A., 2011, P INT JOINT C ART IN, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143
   Froehlich J., 2008, ROUTE PREDICTION TRI
   Inoue T., 2014, INT J SOFTW TOOLS TE, P1
   Kisa D., 2014, KR
   Kisa D., 2014, ICML WORKSH LEARN TR
   Knuth Donald E., 2009, BITWISE TRICKS TECHN, V4, P1
   Krumm J., 2008, MARKOV MODEL DRIVER
   LETCHNER J, 2006, P 18 INN APPL ART IN, P1795
   Liang Y., 2017, P 33 C UNC ART INT
   Lu T., 2011, P 28 INT C MACH LEAR, P145
   Mallows C. L., 1957, BIOMETRIKA
   Minato S, 2013, IEICE T INF SYST, VE96D, P1419, DOI 10.1587/transinf.E96.D.1419
   Nishino M., 2016, AAAI, P1058
   Nishino M., 2017, P 31 C ART INT AAAI
   Shen Y., 2016, ADV NEURAL INFORM PR
   Simmons R., 2006, IEEE INT TRANSP SYST, P127, DOI DOI 10.1109/ITSC.2006.1706730
   VALIANT LG, 1979, SIAM J COMPUT, V8, P410, DOI 10.1137/0208032
   Xue Y., 2012, P 26 AAAI C ART INT, P842
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403053
DA 2019-06-15
ER

PT S
AU Choromanski, K
   Sindhwani, V
AF Choromanski, Krzysztof
   Sindhwani, Vikas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Blackbox Backpropagation and Jacobian Sensing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB From a small number of calls to a given "blackbox" on random input perturbations, we show how to efficiently recover its unknown Jacobian, or estimate the left action of its Jacobian on a given vector. Our methods are based on a novel combination of compressed sensing and graph coloring techniques, and provably exploit structural prior knowledge about the Jacobian such as sparsity and symmetry while being noise robust. We demonstrate efficient backpropagation through noisy blackbox layers in a deep neural net, improved data-efficiency in the task of linearizing the dynamics of a rigid body system, and the generic ability to handle a rich class of input-output dependency structures in Jacobian estimation problems.
C1 [Choromanski, Krzysztof; Sindhwani, Vikas] Google Brain, New York, NY 10011 USA.
RP Choromanski, K (reprint author), Google Brain, New York, NY 10011 USA.
EM kchoro@google.com; sindhwani@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Abdel-Khali H. S., 2008, ADV AUTOMATIC DIFFER
   Bajwa W. U., 2007, IEEE SP WORKSH STAT
   Bandeira A. S., 2012, MATH PROGRAMMING, V134
   Boyd S., 2011, FDN TRENDS MACHINE L, V3
   Candes E., 2008, IEEE SIGNAL PROCESSI, V25
   Candes E. J., 2006, COMMUNICATIONS PURE, V59
   Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1
   Donoho D., 2006, IEEE T INFORM THEORY, V52
   Gebremedhin AH, 2005, SIAM REV, V47, P629, DOI 10.1137/S0036144504444711
   Golub G. H., 2012, MATRIX COMPUTATIONS
   Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006
   Griewank A, 2008, EVALUATING DERIVATIV
   Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC
   Jensen T, 1995, GRAPH COLORING PROBL
   Levine  S., 2016, JMLR, V17
   Li W., 2004, INT C INF CONTR AUT
   Lin W., 2010, P SPIE INT SOC OPTIC
   Newsam G. N., 1983, SIAM J ALG DISCR MET
   Rauhutk H., 2010, SPARS 09 SIGNAL PROC
   Tedrake R., 2016, DRAKE PLANNING CONTR
   Toft B., 1996, HDB COMBINATORICS
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406057
DA 2019-06-15
ER

PT S
AU Choromanski, K
   Rowland, M
   Weller, A
AF Choromanski, Krzysztof
   Rowland, Mark
   Weller, Adrian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Unreasonable Effectiveness of Structured Random Orthogonal
   Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications.
C1 [Choromanski, Krzysztof] Google Brain Robot, Mountain View, CA 94043 USA.
   [Rowland, Mark; Weller, Adrian] Univ Cambridge, Cambridge, England.
   [Weller, Adrian] Alan Turing Inst, London, England.
RP Choromanski, K (reprint author), Google Brain Robot, Mountain View, CA 94043 USA.
EM kchoro@google.com; mr504@cam.ac.uk; aw665@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU UK Engineering and Physical Sciences Research Council (EPSRC)
   [EP/L016516/1]; Alan Turing Institute under the EPSRC grant
   [EP/N510129/1]; Leverhulme Trust via the CFI
FX We thank Vikas Sindhwani at Google Brain Robotics and Tamas Sarlos at
   Google Research for inspiring conversations that led to this work. We
   thank Matej Balog, Maria Lomeli, Jiri Hron and Dave Janz for helpful
   comments. MR acknowledges support by the UK Engineering and Physical
   Sciences Research Council (EPSRC) grant EP/L016516/1 for the University
   of Cambridge Centre for Doctoral Training, the Cambridge Centre for
   Analysis. AW acknowledges support by the Alan Turing Institute under the
   EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.
CR Ailon Nir, 2006, STOC
   Andoni A., 2015, NIPS
   Bojarski M., 2017, AISTATS
   Cho Youngmin, 2009, NIPS
   Choromanska A., 2016, ICML
   Choromanski K., 2016, ICML
   Hinrichs A, 2011, RANDOM STRUCT ALGOR, V39, P391, DOI 10.1002/rsa.20360
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI DOI 10.1145/1150402.1150429
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Le  Q., 2013, ICML
   Rahimi A., 2007, NIPS
   Samo Y. -L. K., 2015, CORR
   Schmidt Ludwig, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1650, DOI 10.1109/ICASSP.2014.6853878
   Sidorov G., 2014, COMPUTACION SISTEMAS, V18
   Sundaram N, 2013, PROC VLDB ENDOW, V6, P1930, DOI 10.14778/2556549.2556574
   Vybiral J, 2011, J FUNCT ANAL, V260, P1096, DOI 10.1016/j.jfa.2010.11.014
   Williams CKI, 1998, NEURAL COMPUT, V10, P1203, DOI 10.1162/089976698300017412
   Yu Felix X, 2016, ADV NEURAL INFORM PR, P1975
   Zhang H., 2013, CORR
   Zhang X, 2015, IEEE I CONF COMP VIS, P2929, DOI 10.1109/ICCV.2015.335
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400021
DA 2019-06-15
ER

PT S
AU Choudhury, S
   Javdani, S
   Srinivasa, S
   Scherer, S
AF Choudhury, Sanjiban
   Javdani, Shervin
   Srinivasa, Siddhartha
   Scherer, Sebastian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between these configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of existing methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter. Open-source code and details can be found here: https://github.com/sanjibac/matlab_learning_collision_checking
C1 [Choudhury, Sanjiban; Javdani, Shervin; Srinivasa, Siddhartha; Scherer, Sebastian] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
RP Choudhury, S (reprint author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
EM sanjiban@cmu.edu; sjavdani@cmu.edu; siddh@cs.cmu.edu; basti@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR [N000141310821]
FX We would like to acknowledge the support from ONR grant N000141310821.
   We would like to thank Shushman Choudhury for insightful discussions and
   the 7D arm planning datasets. We would like to thank Oren Salzaman,
   Mohak Bhardwaj, Vishal Dugar and Paloma Sodhi for feedback on the paper.
CR Bohlin Robert, 2000, ICRA
   Burns Brendan, 2005, ICRA
   Canny J., 1988, COMPLEXITY ROBOT MOT
   Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939
   Chen Yuxin, 2015, AAAI
   Choudhury Sanjiban, 2016, ICRA
   Choudhury Shushman, 2016, IROS
   Cohen Benjamin, 2015, 8 ANN S COMB SEARCH
   Cover Hugh, 2013, ICRA
   Dasgupta S., 2004, NIPS
   Dellin C. M., 2016, EXPT ROBOTICS
   Dellin Christopher M, 2016, ICAPS
   Frieze A., 2015, INTRO RANDOM GRAPHS
   Gammell J. D., 2015, ICRA
   Golovin Daniel, 2011, J ARTIFICIAL INTELLI
   Golovin Daniel, 2010, NIPS
   Hart P. E., 1968, IEEE T SYSTEMS SCI C
   Howard Ronald A, 1966, IEEE T SYSTEMS SCI C
   Javdani Shervin, 2014, AISTATS
   Karaman S, 2011, INT J ROBOT RES, V30, P846, DOI 10.1177/0278364911406761
   Kononenko Igor, 2001, ARTIFICIAL INTELLIGE
   Krause A, 2009, J ARTIF INTELL RES, V35, P557, DOI 10.1613/jair.2737
   LaValle S.M., 2006, PLANNING ALGORITHMS
   LaValle Steven M, 2001, IJRR
   Narayanan Venkatraman, 2017, ICAPS
   Nielsen Christian L, 2000, IROS
   Pivtoraiko Mihail, 2009, J FIELD ROBOTICS
   Yoshizumi T, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P923
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404068
DA 2019-06-15
ER

PT S
AU Christiano, PF
   Leike, J
   Brown, TB
   Martic, M
   Legg, S
   Amodei, D
AF Christiano, Paul F.
   Leike, Jan
   Brown, Tom B.
   Martic, Miljan
   Legg, Shane
   Amodei, Dario
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Reinforcement Learning from Human Preferences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.
C1 [Christiano, Paul F.; Brown, Tom B.; Amodei, Dario] OpenAI, San Francisco, CA 94110 USA.
   [Leike, Jan; Martic, Miljan; Legg, Shane] DeepMind, London, England.
   [Brown, Tom B.] Google Brain, Mountain View, CA USA.
RP Christiano, PF (reprint author), OpenAI, San Francisco, CA 94110 USA.
EM paul@openai.com; leike@google.com; tombbrown@google.com;
   miljanm@google.com; legg@google.com; damodei@openai.com
RI Jeong, Yongwook/N-7413-2016
NR 0
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404036
DA 2019-06-15
ER

PT S
AU Ciliberto, C
   Rudi, A
   Rosasco, L
   Pontil, M
AF Ciliberto, Carlo
   Rudi, Alessandro
   Rosasco, Lorenzo
   Pontil, Massimiliano
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Consistent Multitask Learning with Nonlinear Output Relations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Key to multitask learning is exploiting the relationships between different tasks in order to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.
C1 [Ciliberto, Carlo; Pontil, Massimiliano] UCL, Dept Comp Sci, London, England.
   [Rudi, Alessandro] INRIA Sierra Project Team, Paris, France.
   [Rudi, Alessandro] Ecole Normale Super, Paris, France.
   [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA.
   [Rosasco, Lorenzo] Univ Genoa, Genoa, Italy.
   [Rosasco, Lorenzo; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy.
RP Ciliberto, C (reprint author), UCL, Dept Comp Sci, London, England.
EM c.ciliberto@ucl.ac.uk; alessandro.rudi@inria.fr; lrosasco@mit.edu;
   m.pontil@ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU EPSRC [EP/P009069/1]
FX This work was supported in part by EPSRC grant EP/P009069/1.
CR Agarwal A., 2010, ADV NEURAL INFORM PR, P46
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bishop C. M., 2006, INFORM SCI STAT
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Ciliberto C, 2015, PROC CVPR IEEE, P131, DOI 10.1109/CVPR.2015.7298608
   Ciliberto Carlo, 2015, INT C MACH LEARN ICM
   Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412
   Cormen T H., 2009, INTRO ALGORITHMS
   Dekel Ofer, 2004, ADV NEURAL INFORM PR
   Dinuzzo Francesco, 2011, INT C MACH LEARN
   Duchi John C, 2010, P 27 INT C MACH LEAR, P327
   Dwivedi Y, 2016, NEURAL PLAST, DOI 10.1155/2016/7383724
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Evgeniou T., 2004, P 10 ACM SIGKDD INT
   Fergus Rob, 2010, EUR C COMP VIS
   Herbrich R, 2000, ADV NEUR IN, P115
   Hofmann Thomas, 2007, PREDICTING STRUCTURE
   Jacob L, 2008, ADV NEURAL INFORM PR
   Jawanpuria P., 2015, P 29 ADV NEUR INF PR, P1189
   Jawanpuria Pratik, 2012, INT C MACH LEARN
   Maurer A., 2013, C LEARNING THEORY CO, V30, P55
   Maurer A, 2016, J MACH LEARN RES, V17
   Maxwell Harper F., 2015, ACM T INTERACT INTEL, V5, P19
   Micchelli C. A., 2004, ADV NEURAL INFORM PR, P921
   Nowozin S., 2011, FDN TRENDS COMPUTER
   Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Sciavicco Lorenzo, 1996, MODELING CONTROL ROB, V8
   Sindhwani Vikas, 2012, ABS12104792 CORR
   Sra S, 2016, ADV COMPUT VIS PATT, P73, DOI 10.1007/978-3-319-45026-1_3
   Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189
   Steinke  Florian, 2009, ADV NEURAL INFORM PR, P1561
   Steinwart I., 2008, INFORM SCI STAT
   Thrun S., 2012, LEARNING LEARN
   Tsochantaridis Ioannis, 2005, J MACHINE LEARNING R
   Yu Zhang, 2010, C UNC ART INT UAI
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402004
DA 2019-06-15
ER

PT S
AU Cisse, M
   Adi, Y
   Neverova, N
   Keshet, J
AF Cisse, Moustapha
   Adi, Yossi
   Neverova, Natalia
   Keshet, Joseph
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Houdini: Fooling Deep Structured Visual and Speech Recognition Models
   with Adversarial Examples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION
AB Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.
C1 [Cisse, Moustapha; Neverova, Natalia] Facebook AI Res, Menlo Pk, CA 94025 USA.
   [Adi, Yossi; Keshet, Joseph] Bar Ilan Univ, Ramat Gan, Israel.
RP Cisse, M (reprint author), Facebook AI Res, Menlo Pk, CA 94025 USA.
EM moustaphacisse@fb.com; yossiadidrum@gmail.com; nneverova@fb.com;
   jkeshet@cs.biu.ac.il
CR Amodei D., 2015, ARXIV151202595
   Amodei D., 2016, INT C MACH LEARN, P173
   Andriluka M., 2014, CVPR
   Bahdanau D., 2014, ARXIV14090473
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bulat A., 2016, ECCV
   Cisse M., 2017, ARXIV170408847
   Cords M., 2016, CVPR
   Do C., 2008, ADV NEURAL INFORM PR, V22
   Fawzi A., 2015, ARXIV150202590
   Fawzi A., 2016, ADV NEURAL INFORM PR, P1624
   Goodfellow I. J., 2015, P ICLR
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Hazan Tamir, 2010, ADV NEURAL INFORM PR, P1594
   He K., 2016, ARXIV170306870
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Keshet J, 2011, INT CONF ACOUST SPEE, P2224
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kurakin A., 2016, ARXIV160702533
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707, DOI DOI 10.1109/TVCG.2012.323
   McAllester D., 2010, ADV NEURAL INFORM PR, V24
   McAllester  David, 2011, P ADV NEUR INF PROC, P2205
   Moosavi-Dezfooli S., 2015, ARXIV151104599
   Newell Alejandro, 2016, ECCV
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot Nicolas, 2016, ARXIV160202697
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Shaham  U., 2015, ARXIV151105432
   Szegedy C., 2014, P ICLR
   Tabacof P., 2015, ARXIV151005328
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie C., 2017, CORR
   Yu F., 2016, ICLR
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407007
DA 2019-06-15
ER

PT S
AU Clemencon, S
   Achab, M
AF Clemencon, Stephan
   Achab, Mastane
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Ranking Data with Continuous Labels through Oriented Recursive
   Partitions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space chi and the goal is to order all possible observations x in chi by means of a scoring function s : chi -> R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional criterion, called the IROC curve here, or as maximization of the Kendall tau related to the pair (s(X), Y). From the theoretical side, we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall tau maximization under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.
C1 [Clemencon, Stephan; Achab, Mastane] Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.
RP Clemencon, S (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.
EM stephan.clemencon@telecom-paristech.fr;
   mastane.achab@telecom-paristech.fr
RI Jeong, Yongwook/N-7413-2016
FU industrial chair Machine Learning for Big Data from Telecom ParisTech;
   Investissement d'avenir project [ANR-11-LABX-0056-LMH]
FX This work was supported by the industrial chair Machine Learning for Big
   Data from Telecom ParisTech and by a public grant (Investissement
   d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH).
CR Agarwal S, 2005, J MACH LEARN RES, V6, P393
   Breiman L., 1984, CLASSIFICATION REGRE
   Clemencon S, 2005, LECT NOTES COMPUT SC, V3559, P1, DOI 10.1007/11503415_1
   Clemencon S., 2014, J NONPARAMETR STAT, V25, P107
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Clemencon S, 2013, MACH LEARN, V91, P67, DOI 10.1007/s10994-012-5325-4
   Clemencon S, 2013, J MACH LEARN RES, V14, P39
   Clemencon S, 2010, CONSTR APPROX, V32, P619, DOI 10.1007/s00365-010-9084-9
   Clemencon S, 2009, IEEE T INFORM THEORY, V55, P4316, DOI 10.1109/TIT.2009.2025558
   Cortes C, 2004, ADV NEUR IN, V16, P313
   Devroye L., 1996, PROBABILISTIC THEORY
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933
   Menon Aditya Krishna, 2016, J MACHINE LEARNING R, V17, P1
   QUINLAN J. R., 1986, MACH LEARN, V1, P1
   Rajaram S., 2005, NIPS 2005 WORKSH LEA
   Rakotomamonjy A., 2004, P 1 WORKSH ROC AN AI
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404065
DA 2019-06-15
ER

PT S
AU Cohen, J
   Heliou, A
   Mertikopoulos, P
AF Cohen, Johanne
   Heliou, Amelie
   Mertikopoulos, Panayotis
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning with Bandit Feedback in Potential Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DYNAMICS
AB This paper examines the equilibrium convergence properties of no-regret learning with exponential weights in potential games. To establish convergence with minimal information requirements on the player's side, we focus on two frameworks: the semi-bandit case (where players have access to a noisy estimate of their payoff vectors, including strategies they did not play), and the bandit case (where players are only able to observe their in-game, realized payoffs). In the semi-bandit case, we show that the induced sequence of play converges almost surely to a Nash equilibrium at a quasi-exponential rate. In the bandit case, the same result holds for epsilon-approximations of Nash equilibria if we introduce an exploration factor epsilon > 0 that guarantees that action choice probabilities never fall below epsilon. In particular, if the algorithm is run with a suitably decreasing exploration factor, the sequence of play converges to a bona fide Nash equilibrium with probability 1.
C1 [Cohen, Johanne] Univ Paris Saclay, Univ Paris Sud, LRI CNRS, Paris, France.
   [Heliou, Amelie] Univ Paris Saclay, Inria, CNRS, AMIBio,LIX,Ecole Polytech, Paris, France.
   [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, Inria, LIG, F-38000 Grenoble, France.
RP Cohen, J (reprint author), Univ Paris Saclay, Univ Paris Sud, LRI CNRS, Paris, France.
EM johanne.cohen@lri.fr; amelie.heliou@polytechnique.edu;
   panayotis.mertikopoulos@imag.fr
RI Jeong, Yongwook/N-7413-2016
FU CNRS PEPS MASTODONS project ADOC; Huawei Innovation Research Program
   ULTRON; ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]
FX Johanne Cohen was partially supported by the grant CNRS PEPS MASTODONS
   project ADOC 2017. Amelie Heliou and Panayotis Mertikopoulos gratefully
   acknowledge financial support from the Huawei Innovation Research
   Program ULTRON and the ANR JCJC project ORACLESS (grant no.
   ANR-16-CE33-0004-01).
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Auer Peter, 1995, P 36 ANN S FDN COMP
   Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617
   Benaim Michel, 1999, SEMINAIRE PROBABILIT, V33
   Blum A, 2007, ALGORITHMIC GAME THEORY, P79
   Blum A, 2008, ACM S THEORY COMPUT, P373
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Coucheney P, 2015, MATH OPER RES, V40, P611, DOI 10.1287/moor.2014.0687
   Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595
   Foster Dylan J., 2016, ADV NEURAL INFORM PR, P4727
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Hannan J., 1957, ANN MATH STUD, V3, P97
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   Hofbauer J, 2003, B AM MATH SOC, V40, P479, DOI 10.1090/S0273-0979-03-00988-1
   Kleinberg R, 2011, DISTRIB COMPUT, V24, P21, DOI 10.1007/s00446-011-0129-5
   Kleinberg R, 2009, ACM S THEORY COMPUT, P533
   Krichene W, 2015, SIAM J CONTROL OPTIM, V53, P1056, DOI 10.1137/140980685
   Lasaulce Samson, 2010, GAME THEORY LEARNING
   Mehta Ruta, 2015, ITCS 15 P 6 C INN TH
   Mertikopoulos Panayotis, SODA 18 P 29 ANN ACM
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   Palaiopanos Gerasimos, 2017, NIPS 17 P 31 INT C N
   Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883
   Sandholm W. H, 2010, POPULATION GAMES EVO
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989
   TAYLOR PD, 1978, MATH BIOSCI, V40, P145, DOI 10.1016/0025-5564(78)90077-9
   Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406043
DA 2019-06-15
ER

PT S
AU Cohen-Addad, V
   Kanade, V
   Mallmann-Trenn, F
AF Cohen-Addad, Vincent
   Kanade, Varun
   Mallmann-Trenn, Frederik
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hierarchical Clustering Beyond the Worst-Case
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, Dasgupta [9] recently proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [7, 22, 8]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic block model (HSBM), and show that in certain regimes the SVD approach of McSherry [18] combined with specific linkage methods results in a clustering that give an O(1) approximation to Dasgupta's cost function. Finally, we report empirical evaluation on synthetic and real-world data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.
C1 [Cohen-Addad, Vincent] Univ Copenhagen, Copenhagen, Denmark.
   [Kanade, Varun] Univ Oxford, Alan Turing Inst, Oxford, England.
   [Mallmann-Trenn, Frederik] MIT, Cambridge, MA 02139 USA.
RP Cohen-Addad, V (reprint author), Univ Copenhagen, Copenhagen, Denmark.
EM vcohenad@gmail.com; varunk@cs.ox.ac.uk; mallmann@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU European Union [748094]; EPSRC [EP/N510129/1]; NSF [BIO-1455983,
   CCF-1461559, CCF-0939370]
FX The project leading to this application has received funding from the
   European Union's Horizon 2020 research and innovation programme under
   the Marie Sklodowska-Curie grant agreement No. 748094. This work was
   supported in part by EPSRC grant EP/N510129/1. This work was supported
   in part by NSF Award Numbers BIO-1455983, CCF-1461559, and CCF-0939370.
CR Asuncion D. N. A., 2007, UCI MACHINE LEARNING
   Balcan M.-F., STOC 08, P671
   Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575
   Buitinck L., 2013, ECML PKDD WORKSH LAN, P108
   Carlsson G, 2010, J MACH LEARN RES, V11, P1425
   Castro RM, 2004, IEEE T SIGNAL PROCES, V52, P2308, DOI 10.1109/TSP.2004.831124
   Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841
   Cohen-Addad V., 2017, SODA 17
   Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcis.2004.10.006
   Dasgupta S., 2016, P 48 ANN ACM S THEOR
   Feige U, 2001, J COMPUT SYST SCI, V63, P639, DOI 10.1006/jcss.2001.1773
   Felsenstein J, 2004, INFERRING PHYLOGENIE, V2
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   GUENOCHE A, 1991, J CLASSIF, V8, P5, DOI 10.1007/BF02616245
   Jardine N., 1972, WILEY SERIES PROBABI
   Lin GL, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1147, DOI 10.1145/1109557.1109684
   Lyzinski V, 2017, IEEE TRANS NETW SCI, V4, P13, DOI 10.1109/TNSE.2016.2634322
   McSherry F., FOCS 01, P529
   MURTAGH F, 1983, COMPUT J, V26, P354, DOI 10.1093/comjnl/26.4.354
   Plaxton C.G., STOC 03, P40
   Reddy C. K., 2013, DATA CLUSTERING ALGO, V87
   Roy A., NIPS 16, P2316
   SNEATH PHA, 1962, NATURE, V193, P855, DOI 10.1038/193855a0
   Steinbach Michael, 2000, KDD WORKSH TEXT MIN
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406027
DA 2019-06-15
ER

PT S
AU Colombo, N
   Silva, R
   Kang, S
AF Colombo, Nicolo
   Silva, Ricardo
   Kang, Soong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Tomography of the London Underground: a Scalable Model for
   Origin-Destination Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MULTICAST-BASED INFERENCE; NETWORK TOMOGRAPHY; TRANSPORTATION
AB The paper addresses the classical network tomography problem of inferring local traffic given origin-destination observations. Focusing on large complex public transportation systems, we build a scalable model that exploits input-output information to estimate the unobserved link/station loads and the users' path preferences. Based on the reconstruction of the users' travel time distribution, the model is flexible enough to capture possible different path-choice strategies and correlations between users travelling on similar paths at similar times. The corresponding likelihood function is intractable for medium or large-scale networks and we propose two distinct strategies, namely the exact maximum-likelihood inference of an approximate but tractable model and the variational inference of the original intractable model. As an application of our approach, we consider the emblematic case of the London underground network, where a tap-in/tap-out system tracks the starting/exit time and location of all journeys in a day. A set of synthetic simulations and real data provided by Transport For London are used to validate and test the model on the predictions of observable and unobservable quantities.
C1 [Colombo, Nicolo; Silva, Ricardo] UCL, Dept Stat Sci, London, England.
   [Silva, Ricardo] UCL, Alan Turing Inst, London, England.
   [Kang, Soong] UCL, Sch Management, London, England.
RP Colombo, N (reprint author), UCL, Dept Stat Sci, London, England.
EM nicolo.colombo@ucl.ac.uk; ricardo.silva@ucl.ac.uk; smkang@ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU EPSRC [EP/N020723/1]; Alan Turing Institute under the EPSRC
   [EP/N510129/1]; Alan Turing Institute-Lloyd's Register Foundation
   programme on Data-Centric Engineering
FX We thank Transport for London for kindly providing access to data. This
   work has been funded by a EPSRC grant EP/N020723/1. RS also acknowledges
   support by The Alan Turing Institute under the EPSRC grant EP/N510129/1
   and the Alan Turing Institute-Lloyd's Register Foundation programme on
   Data-Centric Engineering.
CR Airoldi EM, 2013, J AM STAT ASSOC, V108, P149, DOI 10.1080/01621459.2012.756328
   Banavar JR, 1999, NATURE, V399, P130, DOI 10.1038/20144
   Bell M. G. H., 1997, TRANSPORTATION NETWO
   BOELTER LMK, 1960, P NATL ACAD SCI USA, V46, P824, DOI 10.1073/pnas.46.6.824
   Caceres R, 1999, IEEE T INFORM THEORY, V45, P2462, DOI 10.1109/18.796384
   Canevet O, 2016, INT C MACH LEARN, P1454
   Cao J, 2000, J AM STAT ASSOC, V95, P1063, DOI 10.2307/2669743
   Castro R, 2004, STAT SCI, V19, P499, DOI 10.1214/088342304000000422
   Christakis NA, 2013, STAT MED, V32, P556, DOI 10.1002/sim.5408
   Coates Mark, 2001, IEEE SIGNAL PROCESSI
   Coates Mark J, 2000, ITC C IP TRAFF MOD M
   Du JL, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1501
   Eagle N, 2009, P NATL ACAD SCI USA, V106, P15274, DOI 10.1073/pnas.0900282106
   Gershman Samuel J, 2014, COGSCI
   Gopalan P., 2012, ADV NEURAL INFORM PR, P2249
   Kingma D.P., 2013, ARXIV13126114
   Kumar Akshat, 2013, ARXIV13096841
   Kurant M, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.138701
   Lo Presti F, 2002, IEEE ACM T NETWORK, V10, P761, DOI 10.1109/TNET.2002.805026
   Newman Mark, 2011, STRUCTURE DYNAMICS N
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Nielsen BF, 2014, TRANSPORTMETRICA A, V10, P502, DOI 10.1080/23249935.2013.795199
   Nuzzolo A, 2013, IEEE INT C INTELL TR, P1894, DOI 10.1109/ITSC.2013.6728505
   Rogers E. M., 1981, COMMUNICATION NETWOR
   Roth C, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0015923
   Silva R, 2015, P NATL ACAD SCI USA, V112, P5643, DOI 10.1073/pnas.1412908112
   Tebaldi C, 1998, J AM STAT ASSOC, V93, P557, DOI 10.2307/2670105
   TSANG Y, 2002, ACOUST SPEECH SIG PR, P2045
   Vanderbei Robert J, 1994, 9404 SOR PRINC U
   Vandewiele Gilles, 2017, P 26 INT C WORLD WID, P1469
   Vardi Y, 1996, J AM STAT ASSOC, V91, P365, DOI 10.2307/2291416
   Wasserman Stanley, 1994, SOCIAL NETWORK ANAL, V8
   Willumsen Luis G, 1978, ESTIMATION OD MATRIX
   Yin HD, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167126
   Yu Zheng, 2011, COMPUTING SPATIAL TR
   Zhang J, 2016, ARXIV161000081
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
   Zheng Y, 2014, ACM T INTEL SYST TEC, V5, P38, DOI 10.1145/2629592
   Zhong C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0149222
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403013
DA 2019-06-15
ER

PT S
AU Combes, R
   Magureanu, S
   Proutiere, A
AF Combes, Richard
   Magureanu, Stefan
   Proutiere, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Minimal Exploration in Structured Stochastic Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMIZATION
AB This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties. Most existing structures (e.g. linear, Lipschitz, unimodal, combinatorial, dueling, . . . ) are covered by our framework. We derive an asymptotic instance-specific regret lower bound for these problems, and develop OSSB, an algorithm whose regret matches this fundamental limit. OSSB is not based on the classical principle of "optimism in the face of uncertainty" or on Thompson sampling, and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound. We illustrate the efficiency of OSSB using numerical experiments in the case of the linear bandit problem and show that OSSB outperforms existing algorithms, including Thompson sampling.
C1 [Combes, Richard] Cent Supelec, L2S, Gif Sur Yvette, France.
   [Magureanu, Stefan; Proutiere, Alexandre] KTH, EE Sch, ACL, Stockholm, Sweden.
RP Combes, R (reprint author), Cent Supelec, L2S, Gif Sur Yvette, France.
EM richard.combes@supelec.fr; magur@kth.se; alepro@kth.se
RI Jeong, Yongwook/N-7413-2016
FU ERC FSA [308267]; French Agence Nationale de la Recherche (ANR)
   [ANR-16-CE40-0002]
FX A. Proutiere's research is supported by the ERC FSA (308267) grant. This
   work is supported by the French Agence Nationale de la Recherche (ANR),
   under grant ANR-16-CE40-0002 (project BADASS).
CR Abbasi-Yadkori Y., 2011, NIPS
   Agarwal A, 2011, ADV NEURAL INFORM PR, V24, P1035
   AGRAWAL R, 1995, SIAM J CONTROL OPTIM, V33, P1926, DOI 10.1137/S0363012992237273
   Agrawal S., 2013, ICML
   Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007
   Carpentier A., 2016, AISTATS
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen W., 2013, ICML
   Combes R., 2015, SIGMETRICS
   Combes R., 2014, ICML
   Combes  Richard, 2015, NIPS
   Dani V., 2008, COLT
   Durand A., 2014, 28 AAAI C ART INT
   Filippi S., 2010, ADV NEURAL INFORM PR, V22, P586
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   GARIVIER A, 2011, COLT
   Glashoff K., 1983, LINEAR OPTIMIZATION
   Gopalan A., 2014, ICML
   Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440
   Gyorgy A., 2007, J MACHINE LEARNING R, V8
   Herkenrath U., 1983, Metrika, V30, P195, DOI 10.1007/BF02056924
   Honda J., 2010, COLT
   Kaufmann E., 2012, ALT
   Kaufmann E, 2016, J MACH LEARN RES, V17
   Komiyama J., 2015, COLT
   Kveton B., 2015, AISTATS
   Kveton  Branislav, 2015, NIPS
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lattimore T., 2016, AISTATS
   Magureanu S., 2014, COLT
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446
   Wen Z., 2015, ICML
   Yu J. Y., 2011, ICML
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401077
DA 2019-06-15
ER

PT S
AU Costa, RP
   Assael, YM
   Shillingford, B
   de Freitas, N
   Vogels, TP
AF Costa, Rui Ponte
   Assael, Yannis M.
   Shillingford, Brendan
   de Freitas, Nando
   Vogels, Tim P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Cortical microcircuits as gated-recurrent neural networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SHUNTING INHIBITION; SYNAPTIC PLASTICITY; WORKING-MEMORY; EXCITATION;
   CORTEX; MODEL; HETEROGENEITY; CONNECTIVITY; DYNAMICS; BALANCE
AB Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.
C1 [Costa, Rui Ponte; Vogels, Tim P.] Univ Oxford, Dept Physiol Anat & Genet, Ctr Neural Circuits & Behav, Oxford, England.
   [Assael, Yannis M.; Shillingford, Brendan] Univ Oxford, Dept Comp Sci, Oxford, England.
   [Assael, Yannis M.; Shillingford, Brendan; de Freitas, Nando] DeepMind, London, England.
RP Costa, RP (reprint author), Univ Oxford, Dept Physiol Anat & Genet, Ctr Neural Circuits & Behav, Oxford, England.
EM rui.costa@cncb.ox.ac.uk; yannis.assael@cs.ox.ac.uk;
   brendan.shillingford@cs.ox.ac.uk; nandodefreitas@google.com;
   tim.vogels@cncb.ox.ac.uk
RI Assael, Yannis M/E-8160-2013; Jeong, Yongwook/N-7413-2016
OI Assael, Yannis M/0000-0001-7408-3847; 
FU Wellcome Trust; Royal Society [WT 100000]; EPSRC; Research Council UK
   (RCUK); Clarendon Fund
FX We would like to thank Everton Agnes, Caglar Gulcehre, Gabor Melis and
   Jake Stroud for helpful comments and discussion. R.P.C. and T.P.V. were
   supported by a Sir Henry Dale Fellowship by the Wellcome Trust and the
   Royal Society (WT 100000). Y.M.A. was supported by the EPSRC and the
   Research Council UK (RCUK). B.S. was supported by the Clarendon Fund.
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Assael Y. M., 2016, ARXIV161101599
   Bartho P, 2009, EUR J NEUROSCI, V30, P1767, DOI 10.1111/j.1460-9568.2009.06954.x
   Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Bhalla U. S., 2017, HIPPOCAMPUS
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Chung J, 2014, EMPIRICAL EVALUATION
   Costa RP, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00075
   Costa RP, 2017, NEURON, V96, P177, DOI 10.1016/j.neuron.2017.09.021
   Costa RP, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0153
   Costa RP, 2015, ELIFE, V4, DOI 10.7554/eLife.09457
   Cox DD, 2014, CURR BIOL, V24, pR921, DOI 10.1016/j.cub.2014.08.026
   Deneve S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   Doiron B, 2001, NEURAL COMPUT, V13, P227, DOI 10.1162/089976601300014691
   DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624
   Douglas RJ, 1989, NEURAL COMPUT, V1, P480, DOI 10.1162/neco.1989.1.4.480
   Egorov AV, 2002, NATURE, V420, P173, DOI 10.1038/nature01171
   El-Boustani S, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6689
   Froemke R. C., 2007, NATURE
   Froemke RC, 2015, ANNU REV NEUROSCI, V38, P195, DOI 10.1146/annurev-neuro-071714-034002
   Gerstner W., 2002, SINGLE NEURONS POPUL
   Gerstner W., 2014, SINGLE NEURONS NETWO
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043
   Graves A., 2013, RECURRENT NEURAL NET
   Graves A., 2013, ARXIV13035778
   Harris KD, 2013, NATURE, V503, P51, DOI 10.1038/nature12654
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hennequin G, 2017, ANNU REV NEUROSCI, V40, P557, DOI 10.1146/annurev-neuro-072116-031005
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Holt GR, 1997, NEURAL COMPUT, V9, P1001, DOI 10.1162/neco.1997.9.5.1001
   Huang Y, 2016, ELIFE, V5, DOI 10.7554/eLife.15441
   Jiang XL, 2015, SCIENCE, V350, DOI 10.1126/science.aac9462
   Kandel E. R., 2000, PRINCIPLES NEURAL SC
   Kornblith S., 2017, CURRENT BIOL
   Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010
   Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014
   Kuchibhotla KV, 2017, NAT NEUROSCI, V20, P62, DOI 10.1038/nn.4436
   Le Q. V, 2015, SIMPLE WAY INITIALIZ
   Letzkus J. J., 2015, NEURON
   Luczak A., 2015, NATURE REV NEUROSCIE
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Markram H, 2004, NAT REV NEUROSCI, V5, P793, DOI 10.1038/nrn1519
   Mejias JF, 2013, AIP CONF PROC, V1510, P185, DOI 10.1063/1.4776513
   Merity S., 2016, POINTER SENTINEL MIX
   O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909
   Paken JMP, 2016, ELIFE, V5, DOI 10.7554/eLife.14985
   Pascanu R., 2012, DIFFICULTY TRAINING
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Poort J, 2015, NEURON, V86, P1478, DOI 10.1016/j.neuron.2015.05.037
   Prescott SA, 2003, P NATL ACAD SCI USA, V100, P2076, DOI 10.1073/pnas.0337591100
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   Senn W, 2001, NEURAL COMPUT, V13, P35, DOI 10.1162/089976601300014628
   Seybold BA, 2015, NEURON, V87, P1181, DOI 10.1016/j.neuron.2015.09.013
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Srivastava  R., 2015, LSTM SEARCH SPACE OD
   Strata P, 1999, BRAIN RES BULL, V50, P349, DOI 10.1016/S0361-9230(99)00100-8
   Sutskever I., 2014, SEQUENCE SEQUENCE LE
   Thomson AM, 2002, CEREB CORTEX, V12, P936, DOI 10.1093/cercor/12.9.936
   Tieleman T., 2012, MACH LEARN, V4, P26
   van den Oord A., 2016, CONDITIONAL IMAGE GE
   van Kerkoerle T, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13804
   vanVreeswijk C, 1996, SCIENCE, V274, P1724
   Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276
   Wang Y, 2006, NAT NEUROSCI, V9, P534, DOI 10.1038/nn1670
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
   York LC, 2009, J COMPUT NEUROSCI, V27, P607, DOI 10.1007/s10827-009-0172-4
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
NR 73
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400026
DA 2019-06-15
ER

PT S
AU Courty, N
   Flamary, R
   Habrard, A
   Rakotomamonjy, A
AF Courty, Nicolas
   Flamary, Remi
   Habrard, Amaury
   Rakotomamonjy, Alain
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Joint distribution optimal transportation for domain adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function f in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a nonlinear transformation between the joint feature/label space distributions of the two domain P-s and P-t that can be estimated with optimal transport. We propose a solution of this problem that allows to recover an estimated target P-t(f) = (X, f (X)) by optimizing simultaneously the optimal coupling and f. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.
C1 [Courty, Nicolas] Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France.
   [Flamary, Remi] Univ Cote dAzur, Lagrange, UMR 7293, CNRS,OCA, Nice, France.
   [Habrard, Amaury] Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 Lyon, France.
   [Rakotomamonjy, Alain] Normandie Univ, Univ Rouen, LITIS EA 4108, Rouen, France.
RP Courty, N (reprint author), Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France.
EM courty@univ-ubs.fr; remi.flamary@unice.fr;
   amaury.habrard@univ-st-etienne.fr; alain.rakoto@insa-rouen.fr
FU French National Research Agency (ANR) [OATMIL ANR-17-CE23-0012];
   Normandie Projet GRR-DAISI, European funding FEDER DAISI; CNRS - Defi
   Imag'In
FX This work benefited from the support of the project OATMIL
   ANR-17-CE23-0012 of the French National Research Agency (ANR), the
   Normandie Projet GRR-DAISI, European funding FEDER DAISI and CNRS
   funding from the Defi Imag'In. The authors also wish to thank Kai Zhang
   and Qiaojun Wang for providing the Wifi localization dataset.
CR Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Ben-David S., 2012, P ISAIM
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Chen Minmin, 2012, ICML
   Courty  N., 2016, IEEE T PATTERN ANAL
   Courty N., 2014, ECML PKDD
   Cuturi M., 2013, NIPS
   Donahue J., 2014, ICML
   Fernando B., 2013, ICCV
   Ganin Y., 2015, INT C MACH LEARN, P1180
   Ganin Y, 2016, J MACH LEARN RES, V17
   Genevay Aude, 2016, P NIPS, P3432
   Gong B., 2012, CVPR
   Gong Mingming, 2016, JMLR Workshop Conf Proc, V48, P2839
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Mansour  Yishay, 2009, P COLT
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patel Vishal M, 2015, IEEE SIGNAL PROCESSI, V32
   Perrot Michael, 2016, NIPS, P4197
   Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Santambrogio F., 2015, OPTIMAL TRANSPORT AP
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Sugiyama M., 2008, NIPS
   Thorpe M., 2016, CORR
   Urner R., 2011, P 28 INT C MACH LEAR, P641
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Zhang  K., 2013, ICML
   Zhang K., 2015, P 29 AAAI C ART INT, P3150
   Zhong E., 2010, ECML PKDD
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403077
DA 2019-06-15
ER

PT S
AU Cowley, BR
   Williamson, RC
   Acar, K
   Smith, MA
   Yu, BM
AF Cowley, Benjamin R.
   Williamson, Ryan C.
   Acar, Katerina
   Smith, Matthew A.
   Yu, Byron M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive stimulus selection for optimizing neural population responses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMIZATION; SHAPE
AB Adaptive stimulus selection methods in neuroscience have primarily focused on maximizing the firing rate of a single recorded neuron. When recording from a population of neurons, it is usually not possible to find a single stimulus that maximizes the firing rates of all neurons. This motivates optimizing an objective function that takes into account the responses of all recorded neurons together. We propose "Adept," an adaptive stimulus selection method that can optimize population objective functions. In simulations, we first confirmed that population objective functions elicited more diverse stimulus responses than single-neuron objective functions. Then, we tested Adept in a closed-loop electrophysiological experiment in which population activity was recorded from macaque V4, a cortical area known for mid-level visual processing. To predict neural responses, we used the outputs of a deep convolutional neural network model as feature embeddings. Natural images chosen by Adept elicited mean neural responses that were 20% larger than those for randomly-chosen natural images, and also evoked a larger diversity of neural responses. Such adaptive stimulus selection methods can facilitate experiments that involve neurons far from the sensory periphery, for which it is often unclear which stimuli to present.
C1 [Cowley, Benjamin R.; Williamson, Ryan C.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Cowley, Benjamin R.; Williamson, Ryan C.; Acar, Katerina; Smith, Matthew A.; Yu, Byron M.] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
   [Yu, Byron M.] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
   [Yu, Byron M.] Carnegie Mellon Univ, Dept Biomed Engn, Pittsburgh, PA 15213 USA.
   [Williamson, Ryan C.] Univ Pittsburgh, Sch Med, Pittsburgh, PA 15260 USA.
   [Acar, Katerina] Univ Pittsburgh, Dept Neurosci, Pittsburgh, PA 15260 USA.
   [Smith, Matthew A.] Univ Pittsburgh, Dept Ophthalmol, Pittsburgh, PA 15260 USA.
RP Cowley, BR (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.; Cowley, BR (reprint author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.
EM bcowley@cs.cmu.edu; rcw30@pitt.edu; kac216@pitt.edu; smithma@pitt.edu;
   byronyu@cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU BrainHub Richard K. Mellon Fellowship; NIH [R01 EY022928, P30 EY008098,
   T32 GM008208, T90 DA022762, R01 HD071686, R01 NS105318]; Richard K.
   Mellon Foundation; NSF [GRFP 1747452]; NSF-NCS [BCS-1533672,
   BCS-1734901/1734916]; Simons Foundation [364994]
FX B.R.C. was supported by a BrainHub Richard K. Mellon Fellowship. R.C.W.
   was supported by NIH T32 GM008208, T90 DA022762, and the Richard K.
   Mellon Foundation. K. A. was supported by NSF GRFP 1747452. M.A.S. and
   B.M.Y. were supported by NSF-NCS BCS-1734901/1734916. M.A.S. was
   supported by NIH R01 EY022928 and NIH P30 EY008098. B.M.Y. was supported
   by NSF-NCS BCS-1533672, NIH R01 HD071686, NIH R01 NS105318, and Simons
   Foundation 364994.
CR Benda J, 2007, CURR OPIN NEUROBIOL, V17, P430, DOI 10.1016/j.conb.2007.07.009
   Carlson ET, 2011, CURR BIOL, V21, P288, DOI 10.1016/j.cub.2011.01.013
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   Cohen MR, 2009, NAT NEUROSCI, V12, P1594, DOI 10.1038/nn.2439
   DiMattina C., 2014, CLOSING LOOP NEURAL, P258
   Felsen G, 2005, PLOS BIOL, V3, P1819, DOI 10.1371/journal.pbio.0030342
   Foldiak P, 2001, NEUROCOMPUTING, V38, P1217, DOI 10.1016/S0925-2312(01)00570-7
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hung CC, 2012, NEURON, V74, P1099, DOI 10.1016/j.neuron.2012.04.029
   Kohn A, 2016, ANNU REV NEUROSCI, V39, P237, DOI 10.1146/annurev-neuro-070815-013851
   Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594
   Lin IC, 2015, NEURON, V87, P644, DOI 10.1016/j.neuron.2015.06.035
   Machens CK, 2005, NEURON, V47, P447, DOI 10.1016/j.neuron.2005.06.015
   Machens CK, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.228104
   O'Connor KN, 2005, J NEUROPHYSIOL, V94, P4051, DOI 10.1152/jn.00046.2005
   Okun M, 2015, NATURE, V521, P511, DOI 10.1038/nature14273
   Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321
   Paninski L, 2005, NEURAL COMPUT, V17, P1480, DOI 10.1162/0899766053723032
   Park M, 2014, NEURAL COMPUT, V26, P1519, DOI 10.1162/NECO_a_00615
   Pillow JW, 2016, CLOSED LOOP NEUROSCIENCE, P3, DOI 10.1016/B978-0-12-802452-2.00001-9
   Radford A., 2015, ARXIV151106434
   Ringach D, 2004, COGNITIVE SCI, V28, P147, DOI 10.1016/j.cogsci.2003.11.003
   Roe AW, 2012, NEURON, V74, P12, DOI 10.1016/j.neuron.2012.03.011
   Rust NC, 2005, NAT NEUROSCI, V8, P1647, DOI 10.1038/nn1606
   Schwartz O, 2006, J VISION, V6, P484, DOI 10.1167/6.4.13
   Simoncelli E. P., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P444, DOI 10.1109/ICIP.1995.537667
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vedaldi A., 2015, P ACM INT C MULT
   Watson G., 1964, SANKHYA A, V26, P359, DOI DOI 10.2307/25049340
   Xiao J., 2013, PRINCETON VISION ROB
   Yamane Y, 2008, NAT NEUROSCI, V11, P1352, DOI 10.1038/nn.2202
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401042
DA 2019-06-15
ER

PT S
AU Cusumano-Towner, MF
   Mansinghka, VK
AF Cusumano-Towner, Marco F.
   Mansinghka, Vikash K.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI AIDE: An algorithm for measuring the accuracy of probabilistic inference
   algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.
C1 [Cusumano-Towner, Marco F.; Mansinghka, Vikash K.] MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.
RP Cusumano-Towner, MF (reprint author), MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.
EM marcoct@mit.edu; vkm@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA (PPAML program) [FA8750-14-2-0004]; Office of Naval Research
   [N000141310333]; Army Research Office [W911NF-13-1-0212]; DoD, Air Force
   Office of Scientific Research, National Defense Science and Engineering
   Graduate (NDSEG) Fellowship [32 CFR 168a]; IARPA [2015-15061000003]
FX This research was supported by DARPA (PPAML program, contract number
   FA8750-14-2-0004), IARPA (under research contract 2015-15061000003), the
   Office of Naval Research (under research contract N000141310333), the
   Army Research Office (under agreement number W911NF-13-1-0212), and
   gifts from Analog Devices and Google. This research was conducted with
   Government support under and awarded by DoD, Air Force Office of
   Scientific Research, National Defense Science and Engineering Graduate
   (NDSEG) Fellowship, 32 CFR 168a.
CR Ackerman Nathanael L, 2010, ARXIV10053014
   Agapiou S, 2017, STAT SCI, V32, P405, DOI 10.1214/17-STS611
   Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Angelino E, 2016, FOUND TRENDS MACH LE, V9, pI, DOI 10.1561/2200000052
   Chatterjee Sourav, 2015, ARXIV151101437
   Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Drinkwater MJ, 2004, PUBL ASTRON SOC AUST, V21, P89, DOI 10.1071/AS03057
   Freer Cameron E, 2010, NIPS WORKSH ADV MONT
   Gelman A, 1992, STAT SCI, V7, P457, DOI DOI 10.1214/SS/1177011136
   Geweke J, 2004, J AM STAT ASSOC, V99, P799, DOI 10.1198/0162145040000001132
   GOODMAN ND, 2008, UNCERTAINTY ARTIFICI, P77067
   Gorham J., 2015, ADV NEURAL INFORM PR, P226
   Grosse RB, 2015, ARXIV151102543
   Grosse Roger B, 2016, ADV NEURAL INFORM PR, P2451
   Gunsel B., 2010, P 13 INT C ART INT S, V9, P876
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Huggins Jonathan H, 2015, ARXIV150300966
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   KONG A., 1992, 348 U CHIC DEP STAT
   Le T. A., 2017, ARXIV170510306
   Maddison Chris J, 2017, ARXIV170509279
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Naesseth Christian A, 2017, ARXIV170511140
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Rubin D. B, 1988, BAYESIAN STATISTICS, V3, P395
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   SMITH AFM, 1992, AM STAT, V46, P84, DOI 10.2307/2684170
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403007
DA 2019-06-15
ER

PT S
AU Cutkosky, A
   Boahen, K
AF Cutkosky, Ashok
   Boahen, Kwabena
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Stochastic and Adversarial Online Learning without Hyperparameters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving O (root T) regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving O (log(T)) regret. Algorithms that focus on the former problem hitherto achieved O (root T) in the stochastic setting rather than O (log(T)). Here we introduce an online optimization algorithm that achieves O (log(4)(T)) regret in a wide class of stochastic settings while gracefully degrading to the optimal O (root T) regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance.
C1 [Cutkosky, Ashok] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Boahen, Kwabena] Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.
RP Cutkosky, A (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM ashokc@cs.stanford.edu; boahen@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Abernethy Jacob, 2008, P 19 ANN C COMP LEAR
   Cutkosky A., 2016, ADV NEURAL INFORM PR, V29, P748
   Cutkosky Ashok, 2017, ARXIV170302629
   Duchi J., 2010, C LEARN THEOR COLT
   Hazan E., 2007, P 21 ANN C ADV NEUR, V20, P65
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Koolen Wouter M., 2016, ADV NEURAL INFORM PR, V29, P4457
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan H. B., 2010, P 23 ANN C LEARN THE
   Orabona F., 2016, ADV NEURAL INFORM PR, V29, P577
   Orabona Francesco, 2013, ADV NEURAL INFORM PR, P1806
   Sani  Amir, 2014, ADV NEURAL INFORM PR, V27, P810
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   van Erven Tim, 2016, ADV NEURAL INFORM PR, V29, P3666
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405014
DA 2019-06-15
ER

PT S
AU Czarnecki, WM
   Osindero, S
   Jaderberg, M
   Swirszcz, G
   Pascanu, R
AF Czarnecki, Wojciech Marian
   Osindero, Simon
   Jaderberg, Max
   Swirszcz, Grzegorz
   Pascanu, Razvan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sobolev Training for Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB At the heart of deep learning we aim to use neural networks as function approxi-mators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.
C1 [Czarnecki, Wojciech Marian; Osindero, Simon; Jaderberg, Max; Swirszcz, Grzegorz; Pascanu, Razvan] DeepMind, London, England.
RP Czarnecki, WM (reprint author), DeepMind, London, England.
EM lejlot@google.com; osindero@google.com; jaderberg@google.com;
   swirszcz@google.com; razp@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, ARXIV160304467
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dieleman S., 2016, CORR
   Fairbank M., 2012, NEUR NETW IJCNN 2012, P1
   Fairbank M, 2012, IEEE T NEUR NET LEAR, V23, P1671, DOI 10.1109/TNNLS.2012.2205268
   GALLANT AR, 1992, NEURAL NETWORKS, V5, P129, DOI 10.1016/S0893-6080(05)80011-5
   Han S., 2015, ARXIV151000149
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, ARXIV150302531
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Jaderberg M., 2016, ARXIV160805343
   Konda V., 1999, NIPS, V11, P1008
   Krantz Steven G, 2012, HDB COMPLEX VARIABLE
   Maeda Shin- ichi, 2017, ICLR WORKSH P
   Miller W., 1995, NEURAL NETWORKS CONT
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Pascanu R., 2013, ARXIV13013584
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Rusu AA, 2015, ARXIV151106295
   Sau B.B., 2016, ARXIV161009650
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simard P., 1991, P NIPS 91, P895
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Tassa Y, 2007, IEEE T NEURAL NETWOR, V18, P1031, DOI 10.1109/TNN.2007.899249
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Werbos P. J., 1992, HDB INTELLIGENT CONT
   Wu Anqi, 2017, ARXIV170400060
   Zagoruyko S., 2016, ARXIV161203928
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404034
DA 2019-06-15
ER

PT S
AU Dabkowski, P
   Gal, Y
AF Dabkowski, Piotr
   Gal, Yarin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Real Time Image Saliency for Black Box Classifiers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.
C1 [Dabkowski, Piotr; Gal, Yarin] Univ Cambridge, Cambridge, England.
   [Gal, Yarin] Alan Turing Inst, London, England.
RP Dabkowski, P (reprint author), Univ Cambridge, Cambridge, England.
EM pd437@cam.ac.uk; yarin.gal@eng.cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Fong R., 2017, ARXIV170403296
   He K., 2015, CORR
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Romero A., 2014, CORR
   Ronneberger O., 2015, CORR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2013, 13126034 ARXIV
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Springenberg J. T., 2014, CORR
   Szegedy C., 2013, CORR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Zeiler Matthew D., 2013, CORR
   Zhang Jianming, 2016, TOP DOWN NEURAL ATTE
   Zhou B, 2015, CORR
   Zhou B., 2014, CORR, V1412, P6856
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407006
DA 2019-06-15
ER

PT S
AU Dai, B
   Lin, DH
AF Dai, Bo
   Lin, Dahua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Contrastive Learning for Image Captioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.
C1 [Dai, Bo; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
RP Dai, B (reprint author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
EM db014@ie.cuhk.edu.hk; dhlin@ie.cuhk.edu.hk
RI Jeong, Yongwook/N-7413-2016
FU Big Data Collaboration Research grant from SenseTime Group (CUHK)
   [TS1610626]; General Research Fund (GRF) of Hong Kong [14236516]; Early
   Career Scheme (ECS) of Hong Kong [24204215]
FX This work is partially supported by the Big Data Collaboration Research
   grant from SenseTime Group (CUHK Agreement No.TS1610626), the General
   Research Fund (GRF) of Hong Kong (No.14236516) and the Early Career
   Scheme (ECS) of Hong Kong (No.24204215).
CR Dai Bo, 2017, P IEEE INT C COMP VI
   Denkowski M., 2014, P 9 WORKSH STAT MACH, P376
   Devlin J., 2015, ARXIV150504467
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gutmann MU, 2012, J MACH LEARN RES, V13, P307
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jas M, 2015, PROC CVPR IEEE, P2727, DOI 10.1109/CVPR.2015.7298889
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova P., 2014, J T ASS COMPUT LINGU, V2, P351
   Lin C. Y., 2004, TEXT SUMMARIZATION B, V8
   Liu S., 2016, ARXIV161200370
   Lu J., 2016, ARXIV161201887
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Park Cesc Chunseong, 2017, CVPR
   Rennie S. J., 2016, ARXIV161200563
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Vedantam R., 2017, ARXIV170102870
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xu K., 2015, ICML, V14, P77
   Yao Ting, 2016, ARXIV161101646
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhou L., 2016, ARXIV160604621
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400086
DA 2019-06-15
ER

PT S
AU Dai, HJ
   Khalil, EB
   Zhang, YY
   Dilkina, B
   Song, L
AF Dai, Hanjun
   Khalil, Elias B.
   Zhang, Yuyu
   Dilkina, Bistra
   Song, Le
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Combinatorial Optimization Algorithms over Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.
C1 [Dai, Hanjun; Khalil, Elias B.; Zhang, Yuyu; Dilkina, Bistra; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China.
RP Dai, HJ (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM hanjun.dai@cc.gatech.edu; elias.khalil@cc.gatech.edu;
   yuyu.zhang@cc.gatech.edu; bdilkina@cc.gatech.edu; lsong@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701, CCF-1522054]; NIH
   BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR
   [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; ExxonMobil
FX This project was supported in part by NSF IIS-1218749, NIH BIGDATA
   1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF
   CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS.
   Dilkina is supported by NSF grant CCF-1522054 and ExxonMobil.
CR Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Applegate D. L., 2006, CONCORDE TSP SOLVER
   Applegate D. L., 2011, TRAVELING SALESMAN P
   BALAS E, 1980, COMBINATORIAL OPTIMI, V12, P37
   Bello I., 2016, ARXIV161109940
   Boyan J. A., 2000, J MACHINE LEARNING R, V1, P77
   Chen Yutian, 2016, ARXIV161103824
   Dai H., 2016, ICML
   Du  N., 2013, NIPS
   Erdos P., 1960, PUBL MATH I HUNG, V5, P17, DOI DOI 10.2307/1999405
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Gu Shixiang, 2016, ARXIV161102247
   He  H., 2014, ADV NEURAL INFORM PR, P3293
   IBM, 2014, CPLEX US MAN VERS 12
   Johnson DS, 2007, TRAVELING SALESMAN P, P369
   Karp R. M., 1972, COMPLEXITY COMPUTER, P85, DOI [DOI 10.1007/978-1-4684-2001-2_9, 10.1007/978-1-4684-2001-29]
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Khalil E. B., 2016, P AAAI, P724
   Khalil Elias B., 2017, 26 INT JOINT C ART I
   Khalil Elias B., 2014, KNOWLEDGE DISCOVERY
   Kingma D. P., 2014, ARXIV14126980
   Kleinberg J, 2006, ALGORITHM DESIGN
   Lagoudakis M. G., 2001, ELECT NOTES DISCRETE, V9, P344
   Li Ke, 2016, ARXIV160601885
   Mnih V., 2013, ABS13125602 CORR
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI
   Peleg D., 1993, Proceedings of the 2nd Israel Symposium on Theory and Computing Systems (Cat. No.93TH0520-7), P69, DOI 10.1109/ISTCS.1993.253482
   Reinelt G., 1991, ORSA Journal on Computing, V3, P376
   Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317
   Rodriguez M Gomez, 2010, P 16 ACM SIGKDD INT, P1019, DOI [DOI 10.1145/1835804.1835933, 10.1145/1835804.1835933]
   Sabharwal Ashish, 2012, Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems. Proceedings 9th International Conference, CPAIOR 2012, P356, DOI 10.1007/978-3-642-29828-8_23
   Samulowitz Horst, 2007, AAAI
   Sutskever I., 2015, ADV NEURAL INFORM PR, P2692
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Zhang Wei, 2000, J ARTIF INTELL RES, V1, P1
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406041
DA 2019-06-15
ER

PT S
AU Dai, ZW
   Alvarez, MA
   Lawrence, ND
AF Dai, Zhenwen
   Alvarez, Mauricio A.
   Lawrence, Neil D.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Modeling of Latent Information in Supervised Learning using
   Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.
C1 [Dai, Zhenwen] Inferentia Ltd, Chesterfield, England.
   [Alvarez, Mauricio A.; Lawrence, Neil D.] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England.
   [Dai, Zhenwen; Lawrence, Neil D.] Amazon Com, Seattle, WA 98109 USA.
RP Dai, ZW (reprint author), Inferentia Ltd, Chesterfield, England.; Dai, ZW (reprint author), Amazon Com, Seattle, WA 98109 USA.
EM zhenwend@amazon.com; mauricio.alvarez@sheffield.ac.uk;
   lawrennd@amazon.com
RI Jeong, Yongwook/N-7413-2016
FU Engineering and Physical Research Council (EPSRC) [EP/N014162/1]
FX MAA has been financed by the Engineering and Physical Research Council
   (EPSRC) Research Project EP/N014162/1.
CR Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Alvarez MA, 2011, J MACH LEARN RES, V12, P1459
   Bonilla Edwin V., 2008, NIPS, V20
   Bussas Matthias, 2017, MACH LEARN, P1
   Goovaerts P, 1997, GEOSTATISTICS NATURA
   Hensman J., 2013, UAI
   Journel A, 1978, MINING GEOSTATISTICS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Matthews A. G. de G, 2016, AISTATS
   Qian PZG, 2008, TECHNOMETRICS, V50, P383, DOI 10.1198/004017008000000262
   Quinlan J. R., 1992, Proceedings of the 5th Australian Joint Conference on Artificial Intelligence. AI '92, P343
   Stegle O., 2011, NIPS, V24, P630
   Sutskever  I., 2014, ADV NEURAL INFORM PR
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Titsias M., 2009, AISTATS
   Titsias M. K., 2010, AISTATS
   Zamora-Martinez F, 2014, ENERG BUILDINGS, V83, P162, DOI 10.1016/j.enbuild.2014.04.034
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405021
DA 2019-06-15
ER

PT S
AU Dai, ZH
   Yang, ZL
   Yang, F
   Cohen, WW
   Salakhutdinov, R
AF Dai, Zihang
   Yang, Zhilin
   Yang, Fan
   Cohen, William W.
   Salakhutdinov, Ruslan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Good Semi-supervised Learning That Requires a Bad GAN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically we show that given the discriminator objective, good semi-supervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets(2).
C1 [Dai, Zihang; Yang, Zhilin; Yang, Fan; Cohen, William W.; Salakhutdinov, Ruslan] Carnegie Melon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Dai, ZH (reprint author), Carnegie Melon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM dzihang@cs.cmu.edu; zhiliny@cs.cmu.edu; fanyang1@cs.cmu.edu;
   wcohen@cs.cmu.edu; rsalakhu@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA [D17AP00001]; Google focused award; Nvidia NVAIL award
FX This work was supported by the DARPA award D17AP00001, the Google
   focused award, and the Nvidia NVAIL award. The authors would also like
   to thank Han Zhao for his insightful feedback.
CR Arjovsky  M., 2017, NIPS 2016 WORKSH ADV, V2016
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Dai Zihang, 2017, ARXIV170201691
   Dumoulin V., 2016, ARXIV160600704
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Jeff Donahue, 2016, ARXIV160509782
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kipf T. N., 2016, ARXIV160902907
   Laine S, 2016, ARXIV161002242
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li C, 2017, ARXIV170302291
   Maaloe Lars, 2016, ARXIV160205473
   Miyato  T., 2015, ARXIV150700677
   Miyato  Takeru, 2017, ARXIV170403976
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Salimans T., 2016, NIPS
   Salimans Tim, 2017, ARXIV170105517
   Sonderby C. K., 2016, ARXIV161004490
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Ulyanov Dmitry, 2017, ARXIV170402304
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Yang Z., 2017, ARXIV170202206
   Yang Z., 2016, ARXIV160308861
   Zhao J., 2016, ARXIV160903126
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406056
DA 2019-06-15
ER

PT S
AU Daniely, A
AF Daniely, Amit
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI SGD Learns the Conjugate Kernel Class of the Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely et al. [2016]. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two.
   As corollaries, it follows that for neural networks of any depth between 2 and log (n), SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.
C1 [Daniely, Amit] Hebrew Univ Jerusalem, Jerusalem, Israel.
   [Daniely, Amit] Google Res, Mountain View, CA 94041 USA.
RP Daniely, A (reprint author), Hebrew Univ Jerusalem, Jerusalem, Israel.; Daniely, A (reprint author), Google Res, Mountain View, CA 94041 USA.
EM amit.daniely@mail.huji.ac.il
RI Jeong, Yongwook/N-7413-2016
CR Andoni A., 2014, P 31 INT C MACH LEAR, P1908
   Anselmi F., 2015, ARXIV50801084
   Arora S., 2014, P 31 INT C MACH LEAR, P584
   Arora Sanjeev, 2016, ARXIV61208795
   Bach F., 2015, EQUIVALENCE KERNEL Q
   Blum A., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P253, DOI 10.1145/195058.195147
   Blum A., 1988, ADV NEURAL INFORMATI, P494
   Bshouty NH, 1998, INFORM PROCESS LETT, V65, P217, DOI 10.1016/S0020-0190(97)00204-4
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Daniely A., 2014, STOC
   Daniely A., 2016, COLT
   Daniely Amit, 2016, NIPS
   Daniely Amit, 2017, ARXIV170307872
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Hazan T., 2015, ARXIV150805133
   Kamath G., 2015, BOUNDS EXPECTATION M
   Kar P., 2012, ARXIV12016530
   KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647
   Kharitonov M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P372, DOI 10.1145/167088.167197
   Klivans A. R., 2006, FOCS
   Klivans AR, 2007, MACH LEARN, V69, P97, DOI 10.1007/s10994-007-5010-1
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402046
DA 2019-06-15
ER

PT S
AU Dann, C
   Lattimore, T
   Brunskill, E
AF Dann, Christoph
   Lattimore, Tor
   Brunskill, Emma
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.
C1 [Dann, Christoph] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Brunskill, Emma] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Lattimore, Tor] DeepMind, London, England.
RP Dann, C (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM cdann@cdann.net; tor.lattimore@gmail.com; ebrun@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER award
FX We appreciate the support of a NSF CAREER award and a gift from Yahoo.
CR Agarwal Alekh, 2014, J MACHINE LEARNING R, V32
   Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016
   Auer P, 2000, ANN IEEE SYMP FOUND, P270, DOI 10.1109/SFCS.2000.892116
   Auer Peter, 2005, P 1 AUSTR COGN VIS W
   Azar Mohammad Gheshlaghi, 2017, INT C MACH LEARN
   Balsubramani Akshay, 2016, UNCERTAINTY ARTIFICI
   Bartlett Peter L, 2009, P 25 C UNC ART INT U, P35
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bubeck Sebastien, 2012, REGRET ANAL STOCHAST, P138
   Dann Christoph, 2015, NEURAL INFORM PROCES
   Durrett R., 2010, PROBABILITY THEORY E
   Francois-Lavet Vincent, 2015, NIPS 2015 WORKSH DEE
   Garivier Aurelien, 2016, ADV NEURAL INFORM PR
   Garivier Aurelien, 2011, C LEARN THEOR
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Jamieson Kevin, 2013, LIL UCB OPTIMAL EXPL
   Jiang Nan, 2017, INT C MACH LEARN
   Lattimore Tor, 2014, THEORETICAL COMPUTER, V558
   Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4
   Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2
   Pazis Jason, 2016, AAAI C ART INT
   Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Strehl AL, 2009, J MACH LEARN RES, V10, P2413
   Szita I., 2010, INT C MACH LEARN
   Weissman Tsachy, 2003, TECHNICAL REPORT
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405077
DA 2019-06-15
ER

PT S
AU Dao, T
   De Sa, C
   Re, C
AF Dao, Tri
   De Sa, Christopher
   Re, Christopher
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gaussian Quadrature for Kernel Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that O(epsilon(-2)) samples are required to achieve an approximation error of at most E. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any gamma > 0, to achieve error epsilon with O(e(e gamma) + epsilon-(1/gamma)) samples as epsilon goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.
C1 [Dao, Tri; Re, Christopher] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [De Sa, Christopher] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Dao, T (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM trid@stanford.edu; cdesa@cs.cornell.edu; chrismre@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Defense Advanced Research Projects Agency (DARPA) [FA8750-17-2-0095];
   DARPA SIMPLEX program [N66001-15-C-4043]; DARPA [FA8750-12-2-0335,
   FA8750-13-2-0039]; National Institute of Health (NIH) [U54EB020405];
   National Science Foundation (NSF) [CCF-1563078]; Office of Naval
   Research (ONR) [N000141210041, N000141310129]; Moore Foundation; Okawa
   Research Grant; American Family Insurance; Intel; Stanford DAWN project:
   Intel; Microsoft; VMware; DOE [108845]; Accenture; Toshiba; Teradata
FX This material is based on research sponsored by Defense Advanced
   Research Projects Agency (DARPA) under agreement number
   FA8750-17-2-0095. We gratefully acknowledge the support of the DARPA
   SIMPLEX program under No. N66001-15-C-4043, DARPA FA8750-12-2-0335 and
   FA8750-13-2-0039, DOE 108845, National Institute of Health (NIH)
   U54EB020405, the National Science Foundation (NSF) under award No.
   CCF-1563078, the Office of Naval Research (ONR) under awards No.
   N000141210041 and No. N000141310129, the Moore Foundation, the Okawa
   Research Grant, American Family Insurance, Accenture, Toshiba, and
   Intel. This research was supported in part by affiliate members and
   other supporters of the Stanford DAWN project: Intel, Microsoft,
   Teradata, and VMware. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright notation thereon. The views and conclusions contained herein
   are those of the authors and should not be interpreted as necessarily
   representing the official policies or endorsements, either expressed or
   implied, of DARPA or the U.S. Government. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of DARPA, AFRL,
   NSF, NIH, ONR, or the U.S. government.
CR Bach F. R., 2015, ARXIV150206800
   Bungartz HJ, 2004, ACT NUMERIC, V13, P147, DOI 10.1017/S0962492904000182
   Clenshaw C. W., 1960, NUMER MATH, V2, P197, DOI DOI 10.1007/BF01386223
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Garofolo J. S., 1993, DARPA TIMIT ACOUSTIC
   Gauss Carl Friedrich, 1815, METHODUS NOVA INTEGR
   Gehring J., 2017, ARXIV170503122
   Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720
   Hale N, 2013, SIAM J SCI COMPUT, V35, pA652, DOI 10.1137/120889873
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Holtz Markus, 2010, SPARSE GRID QUADRATU, V77
   Isaacson E, 1994, ANAL NUMERICAL METHO
   Kim Y., P 2014 C EMP METH NA, P1746
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin M, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2611378
   Lu ZY, 2016, INT CONF ACOUST SPEE, P5070, DOI 10.1109/ICASSP.2016.7472643
   Lu Zhiyun, 2014, ARXIV14114000CSSTAT
   Maji Subhransu, 2009, UCBEECS2009159
   May A, 2016, INT CONF ACOUST SPEE, P2424, DOI 10.1109/ICASSP.2016.7472112
   May Avner, 2017, ARXIV170103577
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rudin W, 1990, FOURIER ANAL GROUPS
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Simard PY, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P958
   SMOLIAK SA, 1963, DOKL AKAD NAUK SSSR+, V148, P1042
   Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144
   Stitson MO, 1999, ADVANCES IN KERNEL METHODS, P285
   Sutherland Dougal J., 2015, P 31 ANN C UNC ART I
   Townsend Alex, 2015, IMA J NUMER ANAL
   Trefethen LN, 2008, SIAM REV, V50, P67, DOI 10.1137/060659831
   Yang J., 2014, P 31 INT C MACH LEAR, P485
   Yang T., 2012, ADV NEURAL INFORM PR, P476
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406018
DA 2019-06-15
ER

PT S
AU de Vries, H
   Strub, F
   Mary, J
   Larochelle, H
   Pietquin, O
   Courville, A
AF de Vries, Harm
   Strub, Florian
   Mary, Jeremie
   Larochelle, Hugo
   Pietquin, Olivier
   Courville, Aaron
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Modulating early visual processing by language
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VISION
AB It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the entire visual processing by a linguistic input. Specifically, we introduce Conditional Batch Normalization (CBN) as an efficient mechanism to modulate convolutional feature maps by a linguistic embedding. We apply CBN to a pre-trained Residual Network (ResNet), leading to the MODulatEd ResNet (MODERN) architecture, and show that this significantly improves strong baselines on two visual question answering tasks. Our ablation study confirms that modulating from the early stages of the visual processing is beneficial.
C1 [de Vries, Harm; Courville, Aaron] Univ Montreal, Montreal, PQ, Canada.
   [Strub, Florian; Mary, Jeremie] Univ Lille, CNRS, Cent Lille, Inria,UMR 9189 CRIStAL, Lille, France.
   [Larochelle, Hugo] Google Brain, Mountain View, CA USA.
   [Pietquin, Olivier] DeepMind, London, England.
RP de Vries, H (reprint author), Univ Montreal, Montreal, PQ, Canada.
EM mail@harmdevries.com; florian.strub@inria.fr;
   jeremie.mary@univ-lille3.fr; hugolarochelle@google.com;
   pietquin@google.com; aaron.courville@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU CHISTERA IGLU; CPER Nord-Pas de Calais/FEDER DATA Advanced data science
   and technologies; NSERC; Calcul Quebec; Compute Canada; Canada Research
   Chairs; CIFAR
FX The authors would like to acknowledge the stimulating research
   environment of the SequeL lab. We thank Vincent Dumoulin for helpful
   discussions about conditional batch normalization. We acknowledge the
   following agencies for research funding and computing support: CHISTERA
   IGLU and CPER Nord-Pas de Calais/FEDER DATA Advanced data science and
   technologies 2015-2020, NSERC, Calcul Quebec, Compute Canada, the Canada
   Research Chairs and CIFAR. We thank NVIDIA for providing access to a
   DGX-1 machine used in this work.
CR Antol S., 2015, P ICCV
   Ben-younes H., 2017, ARXIV170506676
   Boutonnet B, 2015, J NEUROSCI, V35, P9329, DOI 10.1523/JNEUROSCI.5111-14.2015
   Cho  K., 2014, P EMNLP
   Das A., 2017, P CVPR
   De Vries H., 2017, P CVPR
   Dumoulin V., 2017, P ICLR
   Ferreira F, 2007, J MEM LANG, V57, P455, DOI 10.1016/j.jml.2007.08.002
   Fukui A., 2016, P EMNLP
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jiasen J., 2016, P NIPS
   Kaiming K., 2016, P CVPR
   Kim J., 2016, P NIPS
   Kim J. -H., 2017, P ICLR
   Kok P, 2014, J COGNITIVE NEUROSCI, V26, P1546, DOI 10.1162/jocn_a_00562
   Lin  T.-Y., 2014, P ECCV
   Malinowski M., 2016, ARXIV160502697
   Malinowski M., 2015, P ICCV
   Pennington J., 2014, P EMNLP
   Ren M., 2015, P NIPS
   Sergey I., 2015, P ICML
   Simonyan K, 2015, VERY DEEP CONVOLUTIO
   Thierry G, 2009, P NATL ACAD SCI USA, V106, P4567, DOI 10.1073/pnas.0811155106
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu H., 2015, P ECCV
   Xu Kelvin, 2015, P ICML
   Yang Zichao, 2016, P CVPR
   Yashand G., 2017, P CVPR
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406064
DA 2019-06-15
ER

PT S
AU Dekel, O
   Flajolet, A
   Haghtalab, N
   Jaillet, P
AF Dekel, Ofer
   Flajolet, Arthur
   Haghtalab, Nika
   Jaillet, Patrick
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Learning with a Hint
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS
AB We study a variant of online linear optimization where the player receives a hint about the loss function at the beginning of each round. The hint is given in the form of a vector that is weakly correlated with the loss vector on that round. We show that the player can benefit from such a hint if the set of feasible actions is sufficiently round. Specifically, if the set is strongly convex, the hint can be used to guarantee a regret of O (log(T)), and if the set is q-uniformly convex for q epsilon (2; 3), the hint can be used to guarantee a regret of o (root T). In contrast, we establish Omega(root T) lower bounds on regret when the set of feasible actions is a polyhedron.
C1 [Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA.
   [Flajolet, Arthur] MIT, Operat Res Ctr, Cambridge, MA 02139 USA.
   [Haghtalab, Nika] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
   [Jaillet, Patrick] MIT, ORC, LIDS, EECS, Cambridge, MA 02139 USA.
RP Dekel, O (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM oferd@microsoft.com; flajolet@mit.edu; nika@cmu.edu; jaillet@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU IBM Ph.D. fellowship; Microsoft Ph.D. fellowship; Office of Naval
   Research (ONR) [N00014-15-1-2083]
FX Haghtalab was partially funded by an IBM Ph.D. fellowship and a
   Microsoft Ph.D. fellowship. Jaillet acknowledges the research support of
   the Office of Naval Research (ONR) grant N00014-15-1-2083. This work was
   partially done when Haghtalab was an intern at Microsoft Research,
   Redmond WA.
CR Abernethy J., 2008, P 21 ANN C LEARN THE, P415
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   BALL K, 1994, INVENT MATH, V115, P463, DOI 10.1007/BF01231769
   Blum A, 2007, ALGORITHMIC GAME THEORY, P79
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chiang CK, 2010, PROC APPL MATH, V135, P616
   Chiang Chao- Kai, 2012, COLT, P6
   Flajolet Arthur, 2014, ARXIV14115649
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36
   Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287
   Hazan Elad, 2008, P 23 C LEARN THEOR C
   Huang Ruitong, 2016, ADV NEURAL INFORM PR, P4970
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   McMahan H Brendan, 2017, J MACHINE LEARNING R, V18, P1
   Pisier Gilles, 2011, COURSE IHP UNPUB FEB, P2
   Rakhlin Alexander, 2013, COLT, P993
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Sridharan Karthik, 2010, P 23 ANN C LEARN THE, P1
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Vovk V, 2007, MACH LEARN, V69, P193, DOI 10.1007/s10994-007-5021-y
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405037
DA 2019-06-15
ER

PT S
AU Deng, ZJ
   Zhang, H
   Liang, XD
   Yang, LN
   Xu, SZ
   Zhu, J
   Xing, EP
AF Deng, Zhijie
   Zhang, Hao
   Liang, Xiaodan
   Yang, Luona
   Xu, Shizhen
   Zhu, Jun
   Xing, Eric P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Structured Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.
C1 [Deng, Zhijie; Xu, Shizhen; Zhu, Jun] Tsinghua Univ, Beijing, Peoples R China.
   [Zhang, Hao; Liang, Xiaodan; Yang, Luona; Xu, Shizhen] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Zhang, Hao; Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA.
RP Zhu, J (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM dzj17@mails.tsinghua.edu.cn; hao@cs.cmu.edu; xiaodan1@cs.cmu.edu;
   luonay1@cs.cmu.edu; xsz12@mails.tsinghua.edu.cn;
   dcszj@mail.tsinghua.edu.cn; epxing@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF China [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man.
   Comp. Stan [2016ZXFB00001]; Tsinghua Tiangong Institute for Intelligent
   Computing; NVIDIA NVAIL Program; AFRL/DARPA project [FA872105C0003]; 
   [FA870215D0002]
FX Zhijie Deng and Jun Zhu are supported by NSF China (Nos. 61620106010,
   61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No.
   2016ZXFB00001), Tsinghua Tiangong Institute for Intelligent Computing
   and the NVIDIA NVAIL Program. Hao Zhang is supported by the AFRL/DARPA
   project FA872105C0003. Xiaodan Liang is supported by award
   FA870215D0002.
CR Abadi M., 2016, USENIX S OP SYST DES
   Bergstra James, 2010, THEANO CPU GPU MATH, P3
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Fu T.-C., 2017, ARXIV170501314
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hu Zhiting, 2017, ARXIV170300955
   Isola  P., 2016, ARXIV161107004
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Laine S, 2016, ARXIV161002242
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li C., 2015, P INT C ADV NEUR INF, P1837
   Li  C., 2017, ADV NEURAL INFORM PR
   Liang Xiaodan, 2017, ARXIV170307022
   Maaloe Lars, 2016, ARXIV160205473
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Netzer Yuval, READING DIGITS NATUR
   Radford A., 2015, ARXIV151106434
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Reed S., 2016, P 33 INT C MACH LEAR, P1060
   Reed S. E., 2016, ADV NEURAL INFORM PR, P217
   Reed Scott, 2017, INT C LEARN REPR
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Tran Luan, 2017, C COMP VIS PATT REC
   Wang H., 2017, ARXIV170307255
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Zhang  H., 2015, ARXIV151206216
NR 33
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403093
DA 2019-06-15
ER

PT S
AU Denton, E
   Birodkar, V
AF Denton, Emily
   Birodkar, Vighnesh
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised Learning of Disentangled Representations from Video
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a new model DRNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.
C1 [Denton, Emily; Birodkar, Vighnesh] NYU, Dept Comp Sci, New York, NY 10003 USA.
RP Denton, E (reprint author), NYU, Dept Comp Sci, New York, NY 10003 USA.
EM denton@cs.nyu.edu; vighneshbirodkar@nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU Google Fellowship
FX We thank Rob Fergus, Will Whitney and Jordan Ash for helpful comments
   and advice. Emily Denton is grateful for the support of a Google
   Fellowship
CR Agrawal Pulkit, 2016, ARXIV160607419
   Chiappa S., 2017, ICLR
   Cricri F, 2016, ARXIV161201756
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Finn C., 2016, ARXIV160507157
   Goodfellow I., 2014, NIPS
   Grathwohl Will, 2016, ARXIV161204440
   Hartley R., 2000, MULTIPLE VIEW GEOMET
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2011, ICANN
   Kalchbrenner N., 2016, ARXIV161000527
   Kingma D. P., 2015, INT C LEARN REPR
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Le QV, 2011, PROC CVPR IEEE
   LeCun Y, 2004, CVPR
   Liu  C., 2009, THESIS
   Mathieu M, 2015, ARXIV151105440
   Mathieu M., 2016, ADV NEURAL INFORM PR
   Oh  J., 2015, NIPS
   Radford A., 2016, INT C LEARN REPR
   Ranzato M., 2014, 14126604 ARXIV
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Reed S. E., 2015, NIPS
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Salimans T., 2016, ARXIV160603498
   Salimans Tim, 2017, ARXIV170105517
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Simonyan  K., 2015, INT C LEARN REPR
   Song Shuran, 2017, IEEE C COMP VIS PATT
   Srivastava N, 2015, ICML
   van den Oord A., 2016, ICML
   Villegas R., 2017, ICLR
   Vondrick C, 2016, ARXIV160902612
   Walker  Jacob, 2015, ICCV, P2
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Whitney W. F., 2016, ARXIV150204623
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Xue  T., 2016, NIPS
   Zhao J, 2016, INT CONF LIGHTN PROT
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404047
DA 2019-06-15
ER

PT S
AU Derezinski, M
   Warmuth, MK
AF Derezinski, Michal
   Warmuth, Manfred K.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unbiased estimates for linear regression via volume sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID APPROXIMATION; MATRICES
AB Given a full rank matrix X with more columns than rows, consider the task of estimating the pseudo inverse X+ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times X+inverted perpendicular X+.
   Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from X. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels.
   We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.
C1 [Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
RP Derezinski, M (reprint author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.
EM mderezin@ucsc.edu; manfred@ucsc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1619271]
FX Thanks to Daniel Hsu and Wojciech Kotlowski for many valuable
   discussions. This research was supported by NSF grant IIS-1619271.
CR Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287
   BENTAL A, 1990, LINEAR ALGEBRA APPL, V139, P165, DOI 10.1016/0024-3795(90)90395-S
   Boutsidis Christos, 2012, ABS12023505 CORR
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681
   Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Fedorov Valerii V., 1972, PROBABILITY MATH STA
   Gartrell Mike, 2016, P 10 ACM C REC SYST, P349
   Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207
   Hsu Daniel, 2017, ABS170507048 CORR
   Kang B., 2013, P C NEUR INF PROC SY, V26, P2319
   Kulesza A., 2011, P INT C MACH LEARN, P1193
   Kulesza Alex, 2012, DETERMINANTAL POINT
   Li C., 2017, ARXIV E PRINTS
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Sarlos T, 2006, ANN IEEE SYMP FOUND, P143
   Sugiyama M, 2009, MACH LEARN, V75, P249, DOI 10.1007/s10994-009-5100-3
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403015
DA 2019-06-15
ER

PT S
AU Deshmukh, AA
   Dogan, U
   Scott, C
AF Deshmukh, Aniket Anand
   Dogan, Urun
   Scott, Clayton
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Task Learning for Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.
C1 [Deshmukh, Aniket Anand; Scott, Clayton] Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA.
   [Dogan, Urun] Microsoft Res, Cambridge CB1 2FB, England.
RP Deshmukh, AA (reprint author), Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA.
EM aniketde@umich.edu; urun.dogan@skype.net; clayscot@umich.edu
RI Jeong, Yongwook/N-7413-2016
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P., 2002, J MACHINE LEARNING R, V3, p397 , DOI DOI 10.1162/153244303321897663
   Blanchard G., 2011, ADV NEURAL INFORM PR, V1, P2178
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N, 2013, ADV NEURAL INFORM PR, P737
   Christmann A., 2010, ADV NEURAL INFORM PR, P406
   Chu W., CONTEXTUAL BANDITS L
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054
   Krause Andreas, 2011, ADV NEURAL INFORM PR, P2447
   Kuleshov V, 2014, ARXIV14026028
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Li S, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P539, DOI 10.1145/2911451.2911548
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Steinwart I, 2008, INFORM SCI STAT, P1
   Valko  M., 2013, UNCERTAINTY ARTIFICI, P654
   Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504
   White J., 2012, BANDIT ALGORITHMS WE
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404089
DA 2019-06-15
ER

PT S
AU Devlin, J
   Bunel, R
   Singh, R
   Hausknecht, M
   Kohli, P
AF Devlin, Jacob
   Bunel, Rudy
   Singh, Rishabh
   Hausknecht, Matthew
   Kohli, Pushmeet
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural Program Meta-Induction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a k-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language [17]. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.
C1 [Devlin, Jacob] Google, Mountain View, CA 94043 USA.
   [Bunel, Rudy] Univ Oxford, Oxford, England.
   [Singh, Rishabh; Hausknecht, Matthew] Microsoft Res, Redmond, WA USA.
   [Kohli, Pushmeet] DeepMind, London, England.
RP Devlin, J (reprint author), Google, Mountain View, CA 94043 USA.
EM jacobdevlin@google.com; rudy@robots.ox.ac.uk; risin@microsoft.com;
   mahauskn@microsoft.com; pushmeet@google.com
RI Jeong, Yongwook/N-7413-2016
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Andrychowicz Marcin, 2016, CORR
   Cai Jonathon, 2017, ICLR
   Devlin Jacob, 2017, CORR
   Duan Y., 2017, CORR
   Graves A., 2014, ARXIV14105401
   Grefenstette  Edward, 2015, NIPS
   Gulwani Sumit, 2012, COMMUNICATIONS ACM
   Huh Mi-Young, 2016, CORR
   Joulin A., 2015, ADV NEURAL INFORM PR, V28, P190
   Kurach  Karol, 2016, ICLR
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Li Chengtao, 2017, ICLR
   Luong Minh-Thang, 2015, STANFORD NEURAL MACH
   Neelakantan Arvind, 2016, ICLR
   Pattis R. E., 1981, KAREL ROBOT GENTLE I
   Reed Scott, 2016, ICLR
   Santoro A., 2016, INT C MACH LEARN, V48, P1842
   Sukhbaatar S., 2015, NIPS
   Wayne G., 2016, ICML
   Zaremba Wojciech, 2015, CORR
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402013
DA 2019-06-15
ER

PT S
AU Devraj, AM
   Meyn, SP
AF Devraj, Adithya M.
   Meyn, Sean P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Zap Q-Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STOCHASTIC-APPROXIMATION; CONVERGENCE-RATES
AB The Zap Q-learning algorithm introduced in this paper is an improvement of Watkins' original algorithm and recent competitors in several respects. It is a matrix-gain algorithm designed so that its asymptotic variance is optimal Moreover, an ODE analysis suggests that the transient behavior is a close match to a deterministic Newton-Raphson implementation. This is made possible by a two time-scale update equation for the matrix gain sequence. The analysis suggests that the approach will lead to stable and efficient computation even for non-ideal parameterized settings. Numerical experiments confirm the quick convergence, even in such non-ideal cases.
C1 [Devraj, Adithya M.; Meyn, Sean P.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32608 USA.
RP Devraj, AM (reprint author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32608 USA.
EM adithyamdevraj@ufl.edu; meyn@ece.ufl.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [EPCN-1609131, CPS-1259040]
FX This research was supported by the National Science Foundation under
   grants EPCN-1609131 and CPS-1259040.
CR Azar M. G., 2011, ADV NEURAL INFORM PR
   Barman K, 2008, SYST CONTROL LETT, V57, P784, DOI 10.1016/j.sysconle.2008.03.003
   Benveniste A, 1990, APPL MATH
   Borkar V. S., 2008, STOCHASTIC APPROXIMA
   Borkar VS, 2000, SIAM J CONTROL OPTIM, V38, P447, DOI 10.1137/S0363012997331639
   Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646
   Choi D, 2006, DISCRETE EVENT DYN S, V16, P207, DOI 10.1007/s10626-006-8134-8
   Devraj A. M., 2017, ARXIV E PRINTS
   Even-Dar E, 2003, J MACH LEARN RES, V5, P1
   Givchi A., 2015, AS C MACH LEARN, P159
   Glynn PW, 2002, STAT PROBABIL LETT, V56, P143, DOI 10.1016/S0167-7152(01)00158-4
   Konda VR, 2004, ANN APPL PROBAB, V14, P796
   Kushner H. J., 1997, APPL MATH, V35
   Lund RB, 1996, ANN APPL PROBAB, V6, P218
   MA DJ, 1990, STOCH PROC APPL, V35, P27, DOI 10.1016/0304-4149(90)90120-H
   Mehta P, 2009, IEEE DECIS CONTR P, P3598, DOI 10.1109/CDC.2009.5399753
   Meyn SP, 1994, ANN APPL PROBAB, V4, P981, DOI 10.1214/aoap/1177004900
   Moulines E., 1981, ADV NEURAL INFORM PR, V24, p[451, 1133]
   Pan Y., 2017, AAAI, P2464
   Paulin D, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-4039
   Polyak Boris Teodorovich, 1990, AVTOMAT TELEMEKH, V51, P98
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   RUPPERT D, 1985, ANN STAT, V13, P236, DOI 10.1214/aos/1176346589
   Ruppert D., 1988, 781 CORN U SCH OP RE
   Szepesvari C., 1997, NEURAL INFORM PROCES, V10, P1064
   Tsitsiklis JN, 1999, IEEE T AUTOMAT CONTR, V44, P1840, DOI 10.1109/9.793723
   TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1007/BF00993306
   Watkins C. J. C. H., 1989, THESIS
   YAO H. S., 2008, P 25 INT C MACH LEAR, P1208
   Yao HS, 2009, IEEE DECIS CONTR P, P1181, DOI 10.1109/CDC.2009.5400370
   Yu HZ, 2013, ANN OPER RES, V208, P95, DOI 10.1007/s10479-012-1128-z
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402028
DA 2019-06-15
ER

PT S
AU Diakonikolas, I
   Grigorescu, E
   Li, J
   Natarajan, A
   Onak, K
   Schmidt, L
AF Diakonikolas, Ilias
   Grigorescu, Elena
   Li, Jerry
   Natarajan, Abhiram
   Onak, Krzysztof
   Schmidt, Ludwig
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Communication-Efficient Distributed Learning of Discrete Probability
   Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DENSITY-ESTIMATION; MULTIVARIATE HISTOGRAMS; ALGORITHMS
AB We initiate a systematic investigation of distribution learning (density estimation) when the data is distributed across multiple servers. The servers must communicate with a referee and the goal is to estimate the underlying distribution with as few bits of communication as possible. We focus on non-parametric density estimation of discrete distributions with respect to the l(1) and ,l(2) norms. We provide the first non-trivial upper and lower bounds on the communication complexity of this basic estimation task in various settings of interest. Specifically, our results include the following:
   1. When the unknown discrete distribution is unstructured and each server has only one sample, we show that any blackboard protocol (i.e., any protocol in which servers interact arbitrarily using public messages) that learns the distribution must essentially communicate the entire sample.
   2. For the case of structured distributions, such as k-histograms and monotone distributions, we design distributed learning algorithms that achieve significantly better communication guarantees than the naive ones, and obtain tight upper and lower bounds in several regimes. Our distributed learning algorithms run in near-linear time and are robust to model misspecification.
   Our results provide insights on the interplay between structure and communication efficiency for a range of fundamental distribution estimation tasks.
C1 [Diakonikolas, Ilias] USC, CS, Los Angeles, CA 90007 USA.
   [Grigorescu, Elena; Natarajan, Abhiram] Purdue Univ, CS, W Lafayette, IN 47907 USA.
   [Li, Jerry; Schmidt, Ludwig] MIT, EECS & CSAIL, Cambridge, MA 02139 USA.
   [Onak, Krzysztof] IBM Res Corp, Albany, NY USA.
RP Diakonikolas, I (reprint author), USC, CS, Los Angeles, CA 90007 USA.
EM diakonik@usc.edu; elena-g@purdue.edu; jerryzli@mit.edu;
   nataraj2@purdue.edu; konak@us.ibm.com; ludwigs@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1652862, CCF-1618981, CCF-1649515]; Sloan Research Fellowship;
   NSF CAREER [CCF-1453261, CCF-1565235]; Google Faculty Research Award;
   NSF Graduate Research Fellowship; Purdue Research Foundation; Google PhD
   Fellowship
FX The authors would like to thank the reviewers for their insightful and
   constructive comments. ID was supported by NSF Award CCF-1652862
   (CAREER) and a Sloan Research Fellowship. EG was supported by NSF Award
   CCF-1649515. JL was supported by NSF CAREER Award CCF-1453261,
   CCF-1565235, a Google Faculty Research Award, and an NSF Graduate
   Research Fellowship. AN was supported in part by a grant from the Purdue
   Research Foundation and NSF Awards CCF-1618981 and CCF-1649515. LS was
   funded by a Google PhD Fellowship.
CR Acharya J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1278
   Acharya J, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P249, DOI 10.1145/2745754.2745772
   Balcan M. -F., 2014, ADV NEURAL INFORM PR, P3113
   Balcan Maria-Florina, 2012, C LEARN THEOR, V23, P26
   BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488
   Braverman M, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1011, DOI 10.1145/2897518.2897582
   CHAN S., 2014, NIPS, P1844
   Chan S., 2014, STOC, P604, DOI DOI 10.1145/2591796.2591848
   Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380
   Chaudhuri S., 1998, SIGMOD Record, V27, P436
   Daskalakis C., 2012, SODA, P1371
   Daskalakis C, 2015, ALGORITHMICA, V72, P316, DOI 10.1007/s00453-015-9971-3
   Daskalakis C, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1833
   Daskalakis C, 2013, ANN IEEE SYMP FOUND, P217, DOI 10.1109/FOCS.2013.31
   Daume Hal  III, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P154, DOI 10.1007/978-3-642-34106-9_15
   Devroye L, 2004, TEST, V13, P129, DOI 10.1007/BF02603004
   Devroye Luc, 2012, COMBINATORIAL METHOD
   Diakonikolas Ilias, 2016, HDB BIG DATA CHAPMAN, P267, DOI 10.1201/b19567-21
   Diakonikolas Ilias, 2016, CORR
   Duchi J. C., 2014, ARXIV E PRINTS
   FREEDMAN D, 1981, Z WAHRSCHEINLICHKEIT, V57, P453, DOI 10.1007/BF01025868
   Fuller SH, 2011, FUTURE OF COMPUTING PERFORMANCE: GAME OVER OR NEXT LEVEL?, P1
   Garg  Ankit, 2014, ADV NEURAL INFORM PR, P2726
   Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389
   Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873
   Hu Y, 2007, CIS: 2007 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY, PROCEEDINGS, P28, DOI 10.1109/CIS.2007.138
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Jordan M. I., 2016, CORR
   Kannan Ravi, 2014, P 27 C LEARN THEOR C, P1040
   Klemela J, 2009, STAT SINICA, V19, P159
   Kowalczyk W., 2004, ADV NEURAL INFORM PR, V17, P713
   Lugosi G, 1996, ANN STAT, V24, P687
   N. R. Council, 2013, FRONT MASS DAT AN
   Nowak R, 2003, INT CONF ACOUST SPEE, P836
   Pearson K., 1895, PHILOS T R SOC A, V186, P343, DOI [10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010]
   Phillips J. M., 2012, AISTATS, P282
   Slavov V, 2014, VLDB J, V23, P51, DOI 10.1007/s00778-013-0314-1
   Thaper N., 2002, P 2002 ACM SIGMOD IN, P428
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Zhang Y., 2013, ADV NEURAL INFORM PR, P2328
   Zhou MQ, 2012, PROC INT CONF DATA, P594, DOI 10.1109/ICDE.2012.19
NR 41
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406045
DA 2019-06-15
ER

PT S
AU Dieng, AB
   Tran, D
   Ranganath, R
   Paisley, J
   Blei, DM
AF Dieng, Adji B.
   Tran, Dustin
   Ranganath, Rajesh
   Paisley, John
   Blei, David M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variational Inference via chi Upper Bound Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions q and finds the closest member to the exact posterior p. Closeness is usually measured via a divergence D (q parallel to p) from q to p. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes D chi(p parallel to q), the chi-divergence from p to q. CHIVI minimizes an upper bound of the model evidence, which we term the chi upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.
C1 [Dieng, Adji B.; Tran, Dustin; Paisley, John; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA.
RP Dieng, AB (reprint author), Columbia Univ, New York, NY 10027 USA.
FU NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA PPAML
   [FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan
   Foundation; John Simon Guggenheim Foundation
FX We thank Alp Kucukelbir, Francisco J. R. Ruiz, Christian A. Naesseth,
   Scott W. Linderman, Maja Rudolph, and Jaan Altosaar for their insightful
   comments. This work is supported by NSF IIS-1247664, ONR
   N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX
   N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John Simon
   Guggenheim Foundation.
CR Beal M. J., 2003, VARIATIONAL ALGORITH
   Bishop C. M, 2006, MACHINE LEARNING, V128
   Burda Y., 2016, INT C LEARN REPR
   Dehaene G., 2015, NIPS
   Gelman Andrew, 2017, ARXIV14124869
   Grosse RB, 2015, ARXIV151102543
   Hensman J., 2014, JMLR
   Hernandez-Lobato J., 2016, ICML
   Hoffman M. D., 2013, JMLR
   Hoffman M. D., 2017, INT C MACH LEARN
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kim H., 2003, P WORKSH PROB GRAPH, P37
   Kingma Diederik P, 2014, ICLR
   Kuleshov Volodymyr, 2017, NIPS
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   Li Y., 2015, NIPS
   Li Y., 2016, NIPS
   MacKay D. J, 2003, INFORM THEORY INFERE
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415
   Miller A., 2014, ICML
   Minka T. P, 2005, TECHNICAL REPORT
   Minka T. P., 2001, THESIS
   Minka Thomas, 2004, TECHNICAL REPORT
   Murphy KP, 2012, MACHINE LEARNING PRO
   Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881
   Paisley J. W., 2012, ICML
   Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063
   Ranganath Rajesh, 2014, AISTATS
   Ranganath Rajesh, 2016, NIPS
   Rezende D. J., 2014, ICML
   Robert C. P., 2004, MONTE CARLO STAT MET
   Sunehag P., 2009, P 12 INT C ART INT S, V5, P560
   Teh Y. W., 2015, ARXIV151209327
   Tran D., 2016, ARXIV161009787
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402076
DA 2019-06-15
ER

PT S
AU Dimitrakakis, C
   Parkes, DC
   Radanovic, G
   Tylkin, P
AF Dimitrakakis, Christos
   Parkes, David C.
   Radanovic, Goran
   Tylkin, Paul
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-View Decision Processes: The Helper-AI Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider a two-player sequential game in which agents have the same reward function but may disagree on the transition probabilities of an underlying Markovian model of the world. By committing to play a specific policy, the agent with the correct model can steer the behavior of the other agent, and seek to improve utility. We model this setting as a multi-view decision process, which we use to formally analyze the positive effect of steering policies. Furthermore, we develop an algorithm for computing the agents' achievable joint policy, and we experimentally show that it can lead to a large utility increase when the agents' models diverge.
C1 [Dimitrakakis, Christos] Chalmers Univ Technol, Gothenburg, Sweden.
   [Dimitrakakis, Christos] Univ Lille, Lille, France.
   [Parkes, David C.; Radanovic, Goran; Tylkin, Paul] Harvard Univ, Cambridge, MA 02138 USA.
RP Dimitrakakis, C (reprint author), Chalmers Univ Technol, Gothenburg, Sweden.; Dimitrakakis, C (reprint author), Univ Lille, Lille, France.
EM christos.dimitrakakis@gmail.com; parkes@eecs.harvard.edu;
   gradanovic@g.harvard.edu; ptylkin@g.harvard.edu
FU People Programme (Marie Curie Actions) of the European Union's Seventh
   Framework Programme (FP7/2007-2013) under REA [608743]; Swedish national
   science foundation (VR); Future of Life Institute; SEAS TomKat fund;
   SNSF Early Postdoc Mobility fellowship
FX The research has received funding from: the People Programme (Marie
   Curie Actions) of the European Union's Seventh Framework Programme
   (FP7/2007-2013) under REA grant agreement 608743, the Swedish national
   science foundation (VR), the Future of Life Institute, the SEAS TomKat
   fund, and a SNSF Early Postdoc Mobility fellowship.
CR Amir Ofra, 2016, IJCAI 2016
   Bosansky B, 2016, ARTIF INTELL, V237, P1, DOI 10.1016/j.artint.2016.03.005
   Bosansky Branislav, 2015, COMPUTATION STACKELB
   Elmalech Avshalom, 2015, P 29 AAAI C ART INT, P1313
   Even-Dar E, 2003, LECT NOTES ARTIF INT, V2777, P581, DOI 10.1007/978-3-540-45167-9_42
   Gal Y, 2008, J ARTIF INTELL RES, V33, P109, DOI 10.1613/jair.2503
   Guo X, 2013, ADV NEURAL INFORM PR, P2130
   Hadfield-Menell D., 2016, COOPERATIVE INVERSE
   Letchford Joshua, 2012, P 26 AAAI C ART INT
   Licklider J.C.R., 1960, Institute of Radio Engineers Transactions on Human Factors in Electronics, VHFE-1, P4, DOI 10.1109/THFE2.1960.4503259
   Littman M. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P394
   Mansour Y, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P401
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Sorg J, 2010, P 27 INT C MACH LEAR, P1007
   Zhang H, 2008, AAAI, V8, P208
   Zhang HQ, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P295
   Zinkevich Martin, 2005, ADV NEURAL INFORM PR
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405051
DA 2019-06-15
ER

PT S
AU Ding, N
   Soricut, R
AF Ding, Nan
   Soricut, Radu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Cold-Start Reinforcement Learning with Softmax Policy Gradient
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.
C1 [Ding, Nan; Soricut, Radu] Google Inc, Venice, CA 90291 USA.
RP Ding, N (reprint author), Google Inc, Venice, CA 90291 USA.
EM dingnan@google.com; rsoricut@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Anderson P., 2016, ECCV
   Bahdanau D., 2015, P ICLR
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Evans L. C., PREPRINT
   Graff David, 2003, ENGLISH GIGAWORD FIF
   Huszar Ferenc, 2015, CORR
   Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721
   Koehn Philipp, 2004, P EMNLP, P388
   Lin Chin-Yew, 2004, P ACL
   Lin T., 2014, CORR
   Liu Z, 2017, INT CONF IMAG VIS
   Neu Gergely, 2017, CORR
   Norouzi M., 2016, ADV NEURAL INFORM PR, V29, P1723
   Papineni  K., 2002, P ACL
   Ranzato M., 2015, CORR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R. S., 1999, NIPS
   Szegedy C., 2015, ARXIV151200567
   Vedantam Ramakrishna, 2015, IEEE C COMP VIS PATT
   Venkatrarnan A., 2015, AAAI, P3024
   Vinyals O., 2015, P IEEE C COMP VIS PA
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu Y., 2016, CORR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402084
DA 2019-06-15
ER

PT S
AU Ding, Y
   Kondor, R
   Eskreis-Winkler, J
AF Ding, Yi
   Kondor, Risi
   Eskreis-Winkler, Jonathan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multiresolution Kernel Approximation for Gaussian Process Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MATRIX
AB Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, K, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm. Important points about MKA are that it is memory efficient, and it is a direct method, which means that it also makes it easy to approximate K-1 and det(K).
C1 [Ding, Yi; Kondor, Risi] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
   [Kondor, Risi; Eskreis-Winkler, Jonathan] Univ Chicago, Dept Stat, Chicago, IL 60637 USA.
RP Ding, Y (reprint author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
EM dingy@uchicago.edu; risi@uchicago.edu; eskreiswinkler@uchicago.edu
RI Jeong, Yongwook/N-7413-2016
CR Abou-Rjeili Amine, 2006, P 20 INT C PAR DISTR
   Allard William K, 2012, APPL COMPUTATIONAL H
   Ambikasaran Sivaram, 2015, ARXIV14036015V2
   Balcan M. -F., 2014, ADV NEURAL INFORM PR, P3113
   Berthet Q., 2013, C LEARN THEOR, V30, P1046
   Borm Steffen, 2007, ECML
   Chandrasekaran S, 2005, CALCOLO, V42, P171, DOI 10.1007/s10092-005-0103-3
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   Greengard L., 1987, J COMPUT PHYS
   Hackbusch W, 1999, COMPUTING, V62, P89, DOI 10.1007/s006070050015
   Hackbusch W., 1999, LECT APPL MATH, P9, DOI DOI 10.1007/978-3-642-59709-1_2
   Jin Rong, 2013, IEEE T INF THEORY
   Kondor Risi, 2014, ICML
   Kuleshov Volodymyr, 2013, J MACHINE LEARNING R, P1418
   Kumar Sanjiv, 2009, NIPS
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rahimi Ali, 2008, NIPS
   Rajani Nazneen, 2015, 1 HIGH PERF GRAPH MI
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Savas Berkant, 2011, P SIAM INT C DAT MIN
   Si S, 2014, ICML
   Smola A.J., 2000, P 17 INT C MACH LEAR, P911
   Snelson E., 2005, NIPS
   Stein M, 1999, STAT INTERPOLATION S
   Sun SL, 2015, INFORM FUSION, V26, P36, DOI 10.1016/j.inffus.2015.03.001
   Teneva Nedelina, 2016, P 19 INT C AR INT ST
   Wang Ruoxi, 2015, ARXIV150500398
   Wang Shusen, 2014, AISTATS
   Williams C. K. I., 2001, ADV NEURAL INFORM PR, V13
   Wilson A., 2015, INT C MACH LEARN, P1775
   Zhang YX, 2013, CHINESE PHYS LETT, V30, DOI 10.1088/0256-307X/30/4/043101
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403078
DA 2019-06-15
ER

PT S
AU Djolonga, J
   Krause, A
AF Djolonga, Josip
   Krause, Andreas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Differentiable Learning of Submodular Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SENSITIVITY-ANALYSIS; ALGORITHMS
AB Can we incorporate discrete optimization algorithms within modern machine learning models? For example, is it possible to incorporate in deep architectures a layer whose output is the minimal cut of a parametrized graph? Given that these models are trained end-to-end by leveraging gradient information, the introduction of such layers seems very challenging due to their non-continuous output. In this paper we focus on the problem of submodular minimization, for which we show that such layers are indeed possible. The key idea is that we can continuously relax the output without sacrificing guarantees. We provide an easily computable approximation to the Jacobian complemented with a complete theoretical analysis. Finally, these contributions let us experimentally learn probabilistic log-supermodular models via a bi-level variational inference formulation.
C1 [Djolonga, Josip; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Djolonga, J (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM josipd@inf.ethz.ch; krausea@ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU ERC [StG 307036]; Google European PhD Fellowship
FX The research was partially supported by ERC StG 307036 and a Google
   European PhD Fellowship.
CR Amos B., 2017, INT C MACH LEARN ICM
   Bach F., 2011, ADV NEURAL INFORM PR
   Bach F., 2013, FDN TRENDS MACHINE L, V6, P2
   Bach F., 2010, ADV NEURAL INFORM PR
   Barbero  A., 2014, ARXIV14110589
   BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873
   BOOT JCG, 1963, OPER RES, V11, P771, DOI 10.1287/opre.11.5.771
   Borenstein E, 2008, IEEE T PATTERN ANAL, V30, P2109, DOI 10.1109/TPAMI.2007.70840
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   CHAKRAVARTI N, 1993, DISCRETE APPL MATH, V45, P183, DOI 10.1016/0166-218X(93)90008-C
   DJOLONGA J., 2015, INT C MACH LEARN ICM
   Djolonga J., 2014, ADV NEURAL INFORM PR
   Dolhansky B., 2016, NEURAL INFORM PROCES
   Domke J, 2013, IEEE T PATTERN ANAL, V35, P2454, DOI 10.1109/TPAMI.2013.31
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Edmonds J., 1970, COMBINATORIAL STRUCT, P69
   Freitas D, 2016, INT CONF EUR ENERG
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Fujishige Satoru, 2006, MINIMUM NORM POINT A
   Groenevelt H., 1991, EUROPEAN J OPERATION, V54
   Kingma D. P., 2014, ARXIV14126980
   Kohli P., 2008, COMPUTER VISION PATT
   Kumar K., 2015, ARXIV150602852
   Kunisch K, 2013, SIAM J IMAGING SCI, V6, P938, DOI 10.1137/120882706
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Narasimhan M., 2006, ADV NEURAL INFORM PR, P979
   Niculae V., 2017, ARXIV170507704
   Ochs Peter, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P654, DOI 10.1007/978-3-319-18461-6_52
   Queyranne M, 1998, MATH PROGRAM, V82, P3, DOI 10.1007/BF01585863
   Robertson T., 1988, TECH REP
   Rockafellar R. T., 2009, VARIATIONAL ANAL, V317
   Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989
   SHAPIRO A, 1988, SIAM J CONTROL OPTIM, V26, P628, DOI 10.1137/0326037
   Suehiro D., 2012, INT C ALG LEARN THEO
   Tappen M. F., 2007, COMPUTER VISION PATT
   Taskar B, 2004, ADV NEUR IN, V16, P25
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Wainwright M. J., 2006, J MACHINE LEARNING R, V7
   Yu Y L, 2013, ADV NEURAL INFORM PR, V26, P91
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401006
DA 2019-06-15
ER

PT S
AU Dobbe, R
   Fridovich-Keil, D
   Tomlin, C
AF Dobbe, Roel
   Fridovich-Keil, David
   Tomlin, Claire
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fully Decentralized Policies for Multi-Agent Systems: An Information
   Theoretic Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings, the structure of a problem allows a distributed solution with limited communication. Here, we consider a scenario where no communication is available, and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover, this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the performance of its individual policy.
C1 [Dobbe, Roel; Fridovich-Keil, David; Tomlin, Claire] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Dobbe, R (reprint author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM dobbe@eecs.berkeley.edu; dfk@eecs.berkeley.edu; tomlin@eecs.berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF under the CPS Frontiers VehiCal project [1545126];
   UC-Philippine-California Advanced Research Institute [IIID-2016-005,
   IIID-2015-10]; ONR MURI Embedded Humans [N00014-16-1-2206]; NSF GRFP
FX The authors would like to acknowledge Roberto Calandra for his
   insightful suggestions and feedback on the manuscript. This research is
   supported by NSF under the CPS Frontiers VehiCal project (1545126), by
   the UC-Philippine-California Advanced Research Institute under projects
   IIID-2016-005 and IIID-2015-10, and by the ONR MURI Embedded Humans
   (N00014-16-1-2206). David Fridovich-Keil was also supported by the NSF
   GRFP.
CR Abbeel P, 2004, INT C MACH LEARN
   Aumann R. J., 1974, International Journal of Game Theory, V3, P217, DOI 10.1007/BF01766876
   BARAN ME, 1989, IEEE T POWER DELIVER, V4, P725, DOI 10.1109/61.19265
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Brambilla M, 2013, SWARM INTELL-US, V7, P1, DOI 10.1007/s11721-012-0075-2
   Christofides PD, 2013, COMPUT CHEM ENG, V51, P21, DOI 10.1016/j.compchemeng.2012.05.011
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dall'Anese E, 2014, IEEE T SUSTAIN ENERG, V5, P487, DOI 10.1109/TSTE.2013.2292828
   DAVISON EJ, 1990, IEEE T AUTOMAT CONTR, V35, P652, DOI 10.1109/9.53544
   Farivar M, 2013, IEEE DECIS CONTR P, P4329, DOI 10.1109/CDC.2013.6760555
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Goldman CV, 2004, J ARTIF INTELL RES, V22, P143, DOI 10.1613/jair.1427
   IEEE PES, 2017, IEEE DISTR TEST FEED
   Jiao J., 2014, ARXIV14066956
   Low SH, 2014, IEEE TRANS CONTROL N, V1, P15, DOI 10.1109/TCNS.2014.2309732
   Lunze J., 1992, FEEDBACK CONTROL LAR
   Modi PJ, 2005, ARTIF INTELL, V161, P149, DOI 10.1016/j.artint.2004.09.003
   Nair R., 2005, AAAI, P133
   Oliehoek F.A., 2016, CONCISE INTRO DECENT
   Ortega P. A., 2015, ARXIV151206789
   Pecan Street Inc, 2017, DAT 2017
   Peshkin L., 2000, P 16 C UNC ART INT U, P489
   Pu Y., 2014, C DEC CONTR LOS ANG
   Raffard R. L., 2004, C DEC CONTR NASS BAH
   Sammut C, 1996, KNOWL ENG REV, V11, P27, DOI 10.1017/S0269888900007669
   Siljak D. D., 2011, DECENTRALIZED CONTRO
   Sondermeijer O., 2016, POW EN SOC GEN M BOS
   Sun AX, 2013, IEEE POW ENER SOC GE
   Xu Y, 2017, IEEE T POWER SYST, V32, P4398, DOI 10.1109/TPWRS.2017.2669343
   Zeilinger M. N., 2013, C DEC CONTR FLOR IT
   Zhang BS, 2015, IEEE T POWER SYST, V30, P1714, DOI 10.1109/TPWRS.2014.2347281
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403001
DA 2019-06-15
ER

PT S
AU Dong, K
   Eriksson, D
   Nickisch, H
   Bindel, D
   Wilson, AG
AF Dong, Kun
   Eriksson, David
   Nickisch, Hannes
   Bindel, David
   Wilson, Andrew Gordon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Log Determinants for Gaussian Process Kernel Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an n x n positive definite matrix, and its derivatives - leading to prohibitive O(n(3)) computations. We propose novel O(n) approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.
C1 [Dong, Kun; Eriksson, David; Bindel, David; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14850 USA.
   [Nickisch, Hannes] Phillips Res Hamburg, Hamburg, Germany.
RP Dong, K (reprint author), Cornell Univ, Ithaca, NY 14850 USA.
RI Jeong, Yongwook/N-7413-2016
CR Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349
   Bai Zhaojun, 1998, TECHNICAL REPORT
   Boutsidis C., 2015, ARXIV150300374
   Buhmann MD, 2001, ACT NUMERIC, V9, P1
   Cullum J. K., 2002, LANCZOS ALGORITHMS L, VI
   Fasshauer GE, 2007, MESHFREE APPROXIMATI, V6
   FIEDLER M, 1984, LINEAR ALGEBRA APPL, V58, P75, DOI 10.1016/0024-3795(84)90205-2
   Flaxman Seth R, 2015, P 32 INT C MACH LEAR, V37, P607
   Gil A, 2007, NUMERICAL METHODS SP
   Golub GH, 2010, PRINC SER APPL MATH, P1
   Han I, 2015, ICML, P908
   Hensman James, 2013, UNCERTAINTY ARTIFICI
   Herlands William, 2016, ARTIFICIAL INTELLIGE
   Higham N. J., 2008, FUNCTIONS MATRICES T
   HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866
   Kulesza A, 2012, FOUND TRENDS MACH LE, V5, P123, DOI 10.1561/2200000044
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   MacKay D, 1997, NEURAL COMPUTATION
   MacKay D. J, 2003, INFORM THEORY INFERE
   Mackay D. J. C., 1992, THESIS
   Quinonero-Candela J., 2007, LARGE SCALE KERNEL M, P203
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen C. E., 2001, NEURAL INFORM PROCES
   Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rue H., 2005, GAUSSIAN MARKOV RAND
   Saad Y., 1992, NUMERICAL METHODS LA
   Schaback R, 2006, ACT NUMERIC, V15, P543, DOI 10.1017/S0962492906270016
   SILVERMAN BW, 1985, J R STAT SOC B, V47, P1
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Stein ML, 2013, ANN APPL STAT, V7, P1162, DOI 10.1214/13-AOAS627
   Ubaru Shashanka, FAST ESTIMATION TR F
   Wendland H., 2004, SCATTERED DATA APPRO, V17
   Weyl H, 1912, MATH ANN, V71, P441, DOI 10.1007/BF01456804
   Wilson A. G., 2014, ADV NEURAL INF PROCE, P3626
   Wilson A. G., 2015, INT C MACH LEARN ICM
   Wilson A. G., 2016, ADV NEURAL INFORM PR, P2586
   Wilson A. G, 2016, P 19 INT C ART INT S, P370
   Wilson Andrew Gordon, 2014, THESIS
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406039
DA 2019-06-15
ER

PT S
AU Dong, X
   Chen, SY
   Pan, SJ
AF Dong, Xin
   Chen, Shangyu
   Pan, Sinno Jialin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain
   Surgeon
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB How to develop slim and accurate deep neural networks has become crucial for real-world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. By controlling layer-wise errors properly, one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. Codes of our work are released at: https://github.com/csyhhu/L-OBS.
C1 [Dong, Xin; Chen, Shangyu; Pan, Sinno Jialin] Nanyang Technol Univ, Singapore, Singapore.
RP Dong, X (reprint author), Nanyang Technol Univ, Singapore, Singapore.
EM n1503521a@e.ntu.edu.sg; schen025@e.ntu.edu.sg; sinnopan@ntu.edu.sg
RI PAN, Sinno Jialin/P-6696-2014; Jeong, Yongwook/N-7413-2016
OI PAN, Sinno Jialin/0000-0001-6565-3836; 
FU NTU Singapore Nanyang Assistant Professorship (NAP) grant
   [M4081532.020]; Singapore MOE AcRF Tier-2 grant [MOE2016-T2-2-060];
   Singapore MOE AcRF Tier-1 grant [2016-T1-001-159]
FX This work is supported by NTU Singapore Nanyang Assistant Professorship
   (NAP) grant M4081532.020, Singapore MOE AcRF Tier-2 grant
   MOE2016-T2-2-060, and Singapore MOE AcRF Tier-1 grant 2016-T1-001-159.
CR Abadi M., 2016, ARXIV160304467
   de Vivo L, 2017, SCIENCE, V355, P507, DOI 10.1126/science.aah5982
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Glorot X., 2011, AISTATS, V15, P275
   Gong Y., 2014, ARXIV14126115
   Goodfellow Ian, 2013, JMLR W CP, P1319
   Guo Y., 2016, ADV NEURAL INFORM PR, P1379
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Hassibi B., 1993, ADV NEURAL INFORMATI, V5, P164
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hu H., 2016, ARXIV160703250
   Jin X., 2016, ARXIV160705423
   Kailath  T., 1980, LINEAR SYSTEMS, V156
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lecun  Y., 1989, NEURAL INFORM PROCES, P598
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Hao, 2016, ARXIV160808710
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Nguyen N., 2016, J MACHINE LEARNING R
   REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452
   Rockafellar R. T., 1997, CONVEX ANAL PRINCETO
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sun Y, 2016, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR.2016.525
   Tai Cheng, 2015, ARXIV151106067
   Wolfe Nikolas, 2017, ARXIV170104465
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404090
DA 2019-06-15
ER

PT S
AU Donti, PL
   Amos, B
   Kolter, JZ
AF Donti, Priya L.
   Amos, Brandon
   Kolter, J. Zico
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Task-based End-to-end Model Learning in Stochastic Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.
C1 [Donti, Priya L.] Carnegie Mellon Univ, Dept Comp Sci, Dept Engr & Publ Policy, Pittsburgh, PA 15213 USA.
   [Amos, Brandon; Kolter, J. Zico] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
RP Donti, PL (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Dept Engr & Publ Policy, Pittsburgh, PA 15213 USA.
EM pdonti@cs.cmu.edu; bamos@cs.cmu.edu; zkolter@cs.cmu.edu
FU National Science Foundation Graduate Research Fellowship Program
   [DGE1252522]; Department of Energy Computational Science Graduate
   Fellowship
FX This material is based upon work supported by the National Science
   Foundation Graduate Research Fellowship Program under Grant No.
   DGE1252522, and by the Department of Energy Computational Science
   Graduate Fellowship.
CR Amodei D., 2015, ARXIV151202595
   Amos Brandon, 2016, ARXIV160907152
   Amos Brandon, 2017, ARXIV170300443
   Bansal S., 2017, ARXIV170309260
   Bengio Y, 1997, INT J NEURAL SYST, V8, P433, DOI 10.1142/S0129065797000422
   Boggs P.T., 1995, ACTA NUMER, V4, P1, DOI DOI 10.1017/S0962492900002518
   Buzacott J.A., 1993, STOCHASTIC MODELS MA, V4
   Elmachtoub Adam N, 2017, ARXIV171008005
   Finn C, 2017, ARXIV170303400
   Gould S., 2016, ARXIV160705447
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Harada K, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P659
   Hazan Tamir, 2010, ADV NEURAL INFORM PR, P1594
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S., 2015, ARXIV150203167
   Jaderberg M., 2016, ARXIV161105397
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Levine S, 2016, J MACH LEARN RES, V17
   Linderoth J, 2006, ANN OPER RES, V142, P215, DOI 10.1007/s10479-006-6169-8
   Mossalam Hossam, 2016, ARXIV161002707
   Muller U., 2005, NIPS, P739
   ROCKAFELLAR RT, 1991, MATH OPER RES, V16, P119, DOI 10.1287/moor.16.1.119
   Shapiro A., 2007, TUTORIAL STOCH UNPUB
   Song Y., 2016, P 33 INT C MACH LEAR, P2169
   Stoyanov Veselin, 2011, JMLR P, P725
   Tamar A, 2016, ADV NEURAL INFORM PR, P2146
   Thomas RW, 2006, IEEE COMMUN MAG, V44, P51, DOI 10.1109/MCOM.2006.273099
   Van Moffaert K, 2014, J MACH LEARN RES, V15, P3483
   Wallace SW, 2003, HDBK OPER R, V10, P637
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Wiering Marco A, 2014, AD DYN PROGR REINF L, P1
   Ziemba William T, 2006, STOCHASTIC OPTIMIZAT, V1
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405055
DA 2019-06-15
ER

PT S
AU Downey, C
   Hefny, A
   Li, BY
   Boots, B
   Gordon, G
AF Downey, Carlton
   Hefny, Ahmed
   Li, Boyue
   Boots, Byron
   Gordon, Geoff
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Predictive State Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a new model, Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources. We show that such bilinear functions arise naturally from state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We also show that PSRNNs can be learned effectively by combining Backpropogation Through Time (BPTT) with an initialization derived from a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). Finally, we show that PSRNNs can be factorized using tensor decomposition, reducing model size and suggesting interesting connections to existing multiplicative architectures such as LSTMs and GRUs. We apply PSRNNs to 4 datasets, and show that we outperform several popular alternative approaches to modeling dynamical systems in all cases.
C1 [Downey, Carlton; Hefny, Ahmed; Li, Boyue; Gordon, Geoff] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Boots, Byron] Georgia Tech, Atlanta, GA 30332 USA.
RP Downey, C (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM cmdowney@cs.cmu.edu; ahefny@cs.cmu.edu; boyue@cs.cmu.edu;
   bboots@cc.gatech.edu; ggordon@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR [N000141512365]; DARPA [FA87501720152]
FX The authors gratefully acknowledge support from ONR (grant number
   N000141512365) and DARPA (grant number FA87501720152).
CR Alpaydin E, 1998, PEN BASED RECOGNITIO
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Belanger David, 2015, P ICML, P833
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boots  B., 2011, P 25 NAT C ART INT A
   Boots B., 2013, ABS13096819 CORR
   Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092
   Cho KyungHyun, 2014, ABS14091259 CORR
   Downey Carlton, 2017, TECHNICAL REPORT
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Glorot X., 2010, P INT C ART INT STAT
   Haarnoja T., 2016, ADV NEURAL INFORM PR, P4376
   Hefny A., 2015, ADV NEURAL INFORM PR, P1963
   Hitchcock F. L., 1927, J MATH PHYS, V6, P164, DOI DOI 10.1002/SAPM192761164
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsu Daniel J., 2008, ABS08114413 CORR
   Kalman R.E., 1960, ASME J BASIC ENG
   Ko J, 2011, AUTON ROBOT, V30, P3, DOI 10.1007/s10514-010-9213-0
   Kossaifi J., 2017, ARXIV170708308
   Littman M. L., 2001, NIPS, P1555
   Ljung L, 1999, SYSTEM IDENTIFICATIO
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Martens J., 2010, P 27 INT C MACH LEAR, P743
   Pasa Luca, 2014, 22 EUR S ART NEUR NE
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674
   Shaban Amirreza, 2015, P INT C UNC ART INT
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Song L., 2010, P 27 INT C MACH LEAR, P991
   Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961
   Sutskever I., 2014, ABS14093215 CORR
   Thrun S., 2005, PROBABILISTIC ROBOTI
   VANOVERSCHEE P, 1994, AUTOMATICA, V30, P75, DOI 10.1016/0005-1098(94)90230-5
   VanOverschee P., 1993, P IFAC WORLD C SYDN, P361
   Wu Yuhuai, 2016, ABS160606630 CORR
   Zhang Y., 2014, ADV NEURAL INFORM PR, P1260
NR 37
TC 0
Z9 0
U1 3
U2 3
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406013
DA 2019-06-15
ER

PT S
AU Drouin, A
   Hocking, TD
   Laviolette, F
AF Drouin, Alexandre
   Hocking, Toby Dylan
   Laviolette, Francois
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Maximum Margin Interval Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REGRESSION
AB Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.
C1 [Drouin, Alexandre; Laviolette, Francois] Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada.
   [Hocking, Toby Dylan] McGill Univ, McGill Genome Ctr, Montreal, PQ, Canada.
RP Drouin, A (reprint author), Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada.
EM alexandre.drouin.8@ulaval.ca; toby.hocking@r-project.org;
   francois.laviolette@ift.ulaval.ca
RI Jeong, Yongwook/N-7413-2016
FU National Sciences and Engineering Research Council of Canada through an
   Alexander Graham Bell Canada Graduate Scholarship Doctoral Award
   [262067]
FX We are grateful to Ulysse Cote-Allard, Mathieu Blanchette, Pascal
   Germain, Sebastien Giguere, Gael Letarte, Mario Marchand, and Pier-Luc
   Plante for their insightful comments and suggestions. This work was
   supported by the National Sciences and Engineering Research Council of
   Canada, through an Alexander Graham Bell Canada Graduate Scholarship
   Doctoral Award awarded to AD and a Discovery Grant awarded to FL
   (#262067).
CR Basak Debasish, 2007, NEURAL INFORM PROCES, V11, P203, DOI DOI 10.4258/HIR.2010.16.4.224
   Breiman L., 1984, CLASSIFICATION REGRE
   Cai T, 2009, BIOMETRICS, V65, P394, DOI 10.1111/j.1541-0420.2008.01074.x
   Hocking TD, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-164
   Hothorn T., 2017, ARXIV170102110
   Hothorn T, 2006, BIOSTATISTICS, V7, P355, DOI 10.1093/biostatistics/kxj011
   Huang J., 2005, 349 U IOW DEP STAT A
   Klein J. P., 2005, SURVIVAL ANAL TECHNI
   Lichman M., 2013, UCI MACHINE LEARNING
   Molinaro AM, 2004, J MULTIVARIATE ANAL, V90, P154, DOI 10.1016/j.jmva.2004.02.003
   Polsterl S., 2016, ARXIV161107054
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Rigaill G, 2013, P 30 INT C MACH LEAR, V28, P172
   SEGAL MR, 1988, BIOMETRICS, V44, P35, DOI 10.2307/2531894
   WEI LJ, 1992, STAT MED, V11, P1871, DOI 10.1002/sim.4780111409
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405003
DA 2019-06-15
ER

PT S
AU Du, SS
   Koushik, J
   Singh, A
   Poczos, B
AF Du, Simon S.
   Koushik, Jayanth
   Singh, Aarti
   Poczos, Barnabas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hypothesis Transfer Learning via Transformation Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation function, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. Experiments on real world data demonstrate the effectiveness of our framework.
C1 [Du, Simon S.; Koushik, Jayanth; Singh, Aarti; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Du, SS (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ssdu@cs.cmu.edu; jayanthkoushik@cmu.edu; aartisingh@cmu.edu;
   bapoczos@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS1563887]; ARPA-E Terra program; AFRL grant [FA8750-17-2-0212]
FX S.S.D. and B.P. were supported by NSF grant IIS1563887 and ARPA-E Terra
   program. A.S. was supported by AFRL grant FA8750-17-2-0212.
CR Ben-David  S., 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Ben-David Shai, 2013, NEW DIR TRANSF MULT
   Blitzer J., 2008, ADV NEURAL INFORM PR, V20, P129
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Carroll RJ, 2006, MEASUREMENT ERROR NO
   Cortes C., 2015, INT C KNOWL DISC DAT, P169
   Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027
   Cortes C, 2011, LECT NOTES ARTIF INT, V6925, P308, DOI 10.1007/978-3-642-24412-4_25
   Craig CC, 1933, ANN MATH STAT, V4, P94, DOI 10.1214/aoms/1177732803
   Huang J., 2006, ADV NEURAL INFORM PR, P601
   Kpotufe S., 2013, ADV NEURAL INFORM PR, V26, P3075
   Kuzborskij I., 2013, ICML, P942
   Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431
   Kuzborskij Ilja, 2016, COMPUTER VISION IMAG
   Kuzborskij Ilja, 2016, MACH LEARN, P1
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu T, 2016, IEEE INT POWER ELEC
   Mansour Yishay, 2009, ARXIV09023430
   Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13
   Nuske S, 2014, FIELD SERVICE ROBOTI, P343
   Orabona Francesco, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P2897, DOI 10.1109/ROBOT.2009.5152247
   Rasmussen Carl Edward, 1996, DELVE DATA EVALUATIN
   Steinwart Ingo, 2009, COLT
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037//0096-3445.121.1.15
   Sugiyama M., 2008, ADV NEURAL INFORM PR, V20, P1433
   Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064
   Verstynen TD, 2014, J NEUROPHYSIOL, V112, P2457, DOI 10.1152/jn.00221.2014
   Vovk V., 2013, EMPIRICAL INFERENCE, P105, DOI DOI 10.1007/978-3-642-41136-6_11
   Wang X., 2014, P ADV NEUR INF PROC, V27, P1898
   Wang Xuezhi, 2016, 25 INT JOINT C ART I, V1, P2
   Wang Xuezhi, 2015, GEN BOUNDS TRANSFER
   Wasserman L, 2006, ALL NONPARAMETRIC ST
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yu Y., 2012, ARXIV12064650
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
   Zhang Yu, 2015, AAAI, V2, P6
   Zhou DX, 2008, J COMPUT APPL MATH, V220, P456, DOI 10.1016/j.cam.2007.08.023
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400055
DA 2019-06-15
ER

PT S
AU Du, SS
   Jin, C
   Lee, JD
   Jordan, MI
   Poczos, B
   Singh, A
AF Du, Simon S.
   Jin, Chi
   Lee, Jason D.
   Jordan, Michael, I
   Poczos, Barnabas
   Singh, Aarti
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gradient Descent Can Take Exponential Time to Escape Saddle Points
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points-it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.
C1 [Du, Simon S.; Poczos, Barnabas; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Jin, Chi; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Lee, Jason D.] Univ Southern Calif, Los Angeles, CA USA.
RP Du, SS (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ssdu@cs.cmu.edu; chijin@berkeley.edu; jasonlee@marshall.usc.edu;
   jordan@cs.berkeley.edu; bapoczos@cs.cmu.edu; aartisingh@cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS1563887]; ARPA-E Terra program; Mathematical Data Science
   program of the Office of Naval Research [N00014-15-1-2670]; ARO
   [W911NF-17-1-0304]; DARPA [D17AP00001]; AFRL [FA8750-17-2-0212]; CMU
   ProSEED/BrainHub Seed Grant
FX S.S.D. and B.P. were supported by NSF grant IIS1563887 and ARPA-E Terra
   program. C.J. and M.I.J. were supported by the Mathematical Data Science
   program of the Office of Naval Research under grant number
   N00014-15-1-2670. J.D.L. was supported by ARO W911NF-17-1-0304. A.S. was
   supported by DARPA grant D17AP00001, AFRL grant FA8750-17-2-0212 and a
   CMU ProSEED/BrainHub Seed Grant. The authors thank Rong Ge, Qing Qu,
   John Wright, Elad Hazan, Sham Kakade, Benjamin Recht, Nathan Srebro, and
   Lin Xiao for useful discussions. The authors thank Stephen Wright and
   Michael O'Neill for pointing out calculation errors in the older
   version.
CR Agarwal Naman, 2017, STOC
   BHOJANAPALLI S, 2016, ADV NEURAL INFORM PR, P3873
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Carmon Yair, 2016, ARXIV161100756
   Carmon Yair, 2016, ARXIV161200547
   Chang Alan, 2015, ARXIV150801779
   Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2
   DOUGHERTY RL, 1989, MATH COMPUT, V52, P471, DOI 10.2307/2008477
   Du S. S., 2017, ARXIV170906129
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Ge R., 2017, P 34 INT C MACH LEAR, V70, P1233
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Jain P., 2017, P 20 INT C ART INT S, P479
   Jin C., 2017, INT C MACH LEARN, V70, P1724
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Levy Kfir Y, 2016, ARXIV161104831
   LI X, 2016, ARXIV161209296
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Palis Jr J, 2012, GEOMETRIC THEORY DYN
   Park Dohyung, 2017, P 20 INT C ART INT S, P65
   PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853
   Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162
   Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725
   Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574
   Whitney H, 1934, T AM MATH SOC, V36, P63, DOI 10.2307/1989708
   Woodworth B., 2016, ADV NEURAL INFORM PR, P3639
   Yi X., 2016, P ADV NEUR INF PROC, V29, P4152
   Yin G George, 2003, STOCHASTIC APPROXIMA, V35
   Zhang Xiao, 2017, ARXIV170100481
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401011
DA 2019-06-15
ER

PT S
AU Du, SS
   Wang, YN
   Singh, A
AF Du, Simon S.
   Wang, Yining
   Singh, Aarti
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On the Power of Truncated SVD for General High-rank Matrix Estimation
   Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMAL RATES; COVARIANCE; COMPLETION; CONVERGENCE
AB We show that given an estimate (A) over cap that is close to a general high-rank positive semi- definite (PSD) matrix A in spectral norm (i.e., parallel to(A) over cap -A parallel to(2) <= delta), the simple truncated Singular Value Decomposition of (A) over cap produces a multiplicative approximation of A in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems:
   1. High-rank matrix completion: we show that it is possible to recover a general high-rank matrix A up to (1 + epsilon) relative error in Frobenius norm from partial observations, with sample complexity independent of the spectral gap of A.
   2. High-rank matrix denoising: we design an algorithm that recovers a matrix A with error in Frobenius norm from its noise-perturbed observations, without assuming A is exactly low-rank.
   3. Low-dimensional approximation of high-dimensional covariance: given N i.i.d. samples of dimension n from N-n(0, A), we show that it is possible to approximate the covariance matrix A with relative error in Frobenius norm with N approximate to n, improving over classical covariance estimation results which requires N approximate to n(2).
C1 [Du, Simon S.; Wang, Yining; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Du, SS (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ssdu@cs.cmu.edu; yiningwa@cs.cmu.edu; aartisingh@cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ARPA-E Terra program; NSF CAREER [IIS-1252412]
FX S.S.D. was supported by ARPA-E Terra program. Y.W. and A.S. were
   supported by the NSF CAREER grant IIS-1252412.
CR Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097
   Allen- Zhu Z., 2016, ADV NEURAL INFORM PR, P974
   Anderson D., 2015, ARTIF INTELL, P19
   Balcan  Maria-Florina, 2016, P C LEARN THEOR, P284
   Bunea F, 2015, BERNOULLI, V21, P1200, DOI 10.3150/14-BEJ602
   Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081
   Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998
   Cai TT, 2013, PROBAB THEORY REL, V156, P101, DOI 10.1007/s00440-012-0422-7
   Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Chen YD, 2016, IEEE T INFORM THEORY, V62, P503, DOI 10.1109/TIT.2015.2499247
   Donoho D, 2014, ANN STAT, V42, P2413, DOI 10.1214/14-AOS1257
   Donoho DL, 2013, P NATL ACAD SCI USA, V110, P8405, DOI 10.1073/pnas.1306110110
   Eriksson B., 2012, P INT C ART INT STAT, P373
   Gavish M, 2014, IEEE T INFORM THEORY, V60, P5040, DOI 10.1109/TIT.2014.2323359
   Hardt M., 2014, P 27 C LEARN THEOR, P638
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Kneip A, 2011, ANN STAT, V39, P2410, DOI 10.1214/11-AOS905
   Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894
   Liu Z., 2015, P 9 ACM C REC SYST, P171, DOI DOI 10.1145/2792838.2800191
   Mackey L, 2014, ANN PROBAB, V42, P906, DOI 10.1214/13-AOP892
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574
   Tu S., 2015, ARXIV150703566
   Van der Vaart A.W., 2000, ASYMPTOTIC STAT, V3
   Wang Lingxiao, 2016, ARXIV161005275
   Yi X., 2016, P ADV NEUR INF PROC, V29, P4152
   Zhang Lijun, 2015, ARXIV150406817
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400043
DA 2019-06-15
ER

PT S
AU Duan, Y
   Andrychowicz, M
   Stadie, B
   Ho, J
   Schneider, J
   Sutskeyer, I
   Abbeel, P
   Zaremba, W
AF Duan, Yan
   Andrychowicz, Marcin
   Stadie, Bradly
   Ho, Jonathan
   Schneider, Jonas
   Sutskeyer, Ilya
   Abbeel, Pieter
   Zaremba, Wojciech
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI One-Shot Imitation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.
   Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained such that when it takes as input the first demonstration demonstration and a state sampled from the second demonstration, it should predict the action corresponding to the sampled state. At test time, a full demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.
C1 [Duan, Yan; Stadie, Bradly; Ho, Jonathan; Abbeel, Pieter] Berkeley AI Res Lab, Berkeley, CA 94720 USA.
   [Duan, Yan; Andrychowicz, Marcin; Stadie, Bradly; Ho, Jonathan; Schneider, Jonas; Sutskeyer, Ilya; Abbeel, Pieter; Zaremba, Wojciech] OpenAI, San Francisco, CA USA.
RP Duan, Y (reprint author), Berkeley AI Res Lab, Berkeley, CA 94720 USA.
EM rockyduan@eecs.berkeley.edu; marcin@openai.com; bstadie@openai.com;
   jonathanho@eecs.berkeley.edu; jonas@openai.com; ilyasu@openai.com;
   pabbeel@eecs.berkeley.edu; woj@openai.com
RI Jeong, Yongwook/N-7413-2016
FU ONR through a PECASE award; Huawei Fellowship; NSF
FX We would like to thank our colleagues at UC Berkeley and OpenAI for
   insightful discussions. This research was funded in part by ONR through
   a PECASE award. Yan Duan was also supported by a Huawei Fellowship.
   Jonathan Ho was also supported by an NSF Fellowship.
CR Abbeel  P., 2004, INT C MACH LEARN ICM
   Andrychowicz M., 2016, NEURAL INFORM PROCES
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Ba Jimmy, 2016, NEURAL INFORM PROCES
   Bahdanau D., 2014, ARXIV14090473
   Battaglia P., 2016, ADV NEURAL INFORM PR, P4502
   Bengio Samy, 1992, OPTIMALITY ARTIFICIA, P6
   Bengio Y., 1990, LEARNING SYNAPTIC LE
   Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953
   Calinon S., 2009, ROBOT PROGRAMMING DE
   Chang Michael B, 2017, INT C LEARN REPR ICL
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Darrell T., 2014, ARXIV14123474
   Donahue J., 2014, P INT C MACH LEARN, P647
   Duan L., 2012, ARXIV12064660
   Duan  Y., 2016, ARXIV161102779
   Edwards H., 2017, INT C LEARN REPR ICL
   Finn  C., 2016, P 33 INT C MACH LEAR, V48
   Finn C, 2017, ARXIV170303400
   Gupta Abhishek, 2017, INT C LEARN REPR ICL
   Heess N., 2015, ADV NEURAL INFORM PR, P2944
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Hochreiter S., 2001, INT C ART NEUR NETW
   Hoffman J., 2013, ARXIV13013224
   Jordan Michael I, 2003, NIPS, V16
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Koch  G., 2015, ICML DEEP LEARN WORK
   Krueger D., 2016, ARXIV160601305
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Levine S, 2016, J MACH LEARN RES, V17
   Levine Sergey, 2011, ADV NEURAL INFORM PR
   Li Ke, 2016, ARXIV160601885
   Lillicrap T P, 2015, ARXIV150902971
   Long M., 2015, ABS150202791 CORR
   Mansour Yishay, 2009, ARXIV09023430
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Naik Devang K, 1992, INT JOINT C NEUR NET
   Ng Andrew, 2000, INT C MACH LEARN ICM
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Pomerleau D. A., 1989, ADV NEURAL INFORMATI, P305
   Ravi S., 2017, ICLR
   Rezende D. J., 2016, INT C MACH LEARN ICM
   Ross S., 2011, AISTATS, P6
   Rusu A. A., 2016, ARXIV160604671
   Sadeghi Fereshteh, 2016, CAD RL REAL SINGLE I
   Santoro A., 2016, INT C MACH LEARN ICM
   Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3
   SCHMIDHUBER J, 1987, THESIS
   Schmidhuber J., 1992, NEURAL COMPUTATION
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stadie Bradlie, 2017, INT C LEARN REPR ICL
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Thrun S., 1998, LEARNING LEARN
   Tzeng  E., 2015, ARXIV151107111
   Vinyals O., 2016, NEURAL INFORM PROCES
   Wang J. X, 2016, ARXIV161105763
   Xu K., 2015, ICML, V14, P77
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yu  Fisher, 2016, INT C LEARN REPR ICL
   Ziebart B., 2008, AAAI C ART INT
NR 66
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401013
DA 2019-06-15
ER

PT S
AU Nguyen, DT
   Kumar, A
   Lau, HC
AF Duc Thien Nguyen
   Kumar, Akshat
   Lau, Hoong Chuin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Policy Gradient With Value Function Approximation For Collective
   Multiagent Planning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPLEXITY
AB Decentralized (PO) MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing C Dec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.
C1 [Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin] Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.
RP Nguyen, DT (reprint author), Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.
EM dtnguyen.2014@smu.edu.sg; akshatkumar@smu.edu.sg; hclau@smu.edu.sg
RI Jeong, Yongwook/N-7413-2016
FU National Research Foundation Singapore under its Corp Lab @ University
   scheme; Fujitsu Limited; A*STAR graduate scholarship
FX This research project is supported by National Research Foundation
   Singapore under its Corp Lab @ University scheme and Fujitsu Limited.
   First author is also supported by A*STAR graduate scholarship.
CR Aberdeen D., 2006, ADV NEURAL INFORM PR, P9
   Amato C, 2015, IEEE INT CONF ROBOT, P1241, DOI 10.1109/ICRA.2015.7139350
   Bagnell J. A., 2005, INT C NEUR INF PROC, P91
   Becker R, 2004, J ARTIF INTELL RES, V22, P423, DOI 10.1613/jair.1497
   Becker R., 2004, P INT JOINT C AUT AG, P302
   Bernstein DS, 2002, MATH OPER RES, V27, P819, DOI 10.1287/moor.27.4.819.297
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Foerster  J., 2016, ADV NEURAL INFORM PR, V2016, P2137
   Guestrin C., 2002, P 19 INT C MACH LEAR, V2, P227
   Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691
   Kumar A., 2011, P 22 INT JOINT C ART, P2140
   Kumar A, 2015, J ARTIF INTELL RES, V53, P223, DOI 10.1613/jair.4649
   Leibo J. Z., 2017, INT C AUT AG MULT SY
   Meyers CA, 2012, NETWORKS, V59, P252, DOI 10.1002/net.20439
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair R., 2005, AAAI, P133
   Nguyen Duc Thien, 2017, AAAI C ART INT, P3036
   Pajarinen J, 2014, IEEE T MOBILE COMPUT, V13, P866, DOI 10.1109/TMC.2013.39
   Peshkin L., 2000, P 16 C UNC ART INT U, P489
   Robbel Philipp, 2016, AAAI C ART INT, P2537
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sonu Ekhlas, 2015, INT C AUT PLANN SCHE, P202
   SUTTON R.S., 1999, NIPS, V99, P1057
   Varakantham P., 2014, AAAI C ART INT, P2505
   Varakantham Pradeep Reddy, 2012, UNCERTAINTY ARTIFICI, P1471
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Winstein K, 2013, ACM SIGCOMM COMP COM, V43, P123, DOI 10.1145/2534169.2486020
   Witwicki S. J., 2010, INT C AUT PLANN SCHE, P185
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404038
DA 2019-06-15
ER

PT S
AU Dudik, M
   Lahaie, S
   Rogers, R
   Vaughan, JW
AF Dudik, Miroslav
   Lahaie, Sebastien
   Rogers, Ryan
   Vaughan, Jennifer Wortman
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Decomposition of Forecast Error in Prediction Markets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We analyze sources of error in prediction market forecasts in order to bound the difference between a security's price and the ground truth it estimates. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into three components: sampling error, arising because traders only possess noisy estimates of ground truth; market-maker bias, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and convergence error, arising because, at any point in time, market prices may still be in flux. Our goal is to make explicit the tradeoffs between these error components, influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We consider a specific model in which traders have exponential utility and exponential-family beliefs representing noisy estimates of ground truth. In this setting, sampling error vanishes as the number of traders grows, but there is a tradeoff between the other two components. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter and into the forecasting benefits of enforcing coherent prices across securities.
C1 [Dudik, Miroslav; Vaughan, Jennifer Wortman] Microsoft Res, New York, NY 10011 USA.
   [Lahaie, Sebastien] Google, New York, NY USA.
   [Rogers, Ryan] Univ Penn, Philadelphia, PA 19104 USA.
RP Dudik, M (reprint author), Microsoft Res, New York, NY 10011 USA.
EM mdudik@microsoft.com; slahaie@google.com; rrogers386@gmail.com;
   jenn@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12, DOI DOI 10.1145/2465769.2465777
   Abernethy Jacob, 2014, P 15 ACM C EC COMP E
   Barndorff-Nielsen Ole, 1982, EXPONENTIAL FAMILIES
   Berg Joyce, 2008, HDB EXPT EC RESULTS, V1, P742, DOI DOI 10.1016/S1574-0722(07)00080-7
   Bousquet O., 2008, ADV NEURAL INFORM PR
   Chen Yiling, 2010, P 11 ACM C EL COMM E
   Chen Yiling, 2007, P 23 C UNC ART INT U
   Dudik Miroslav, 2013, P 14 ACM C EL COMM E
   Frongillo Rafael, 2015, ADV NEURAL INFORM PR
   Hanson R, 2003, INFORM SYST FRONT, V5, P107, DOI 10.1023/A:1022058209073
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Olson Kenneth C., 2015, COLL INT
   Othman A., 2013, ACM T EC COMPUTATION, V1, P14
   Petersen K., 2012, TECHNICAL REPORT
   Rockafellar R T, 1970, CONVEX ANAL
   Rockafellar RT, 2009, VARIATIONAL ANAL
   Rothschild D, 2009, PUBLIC OPIN QUART, V73, P895, DOI 10.1093/poq/nfp082
   Slamka C, 2013, IEEE T ENG MANAGE, V60, P169, DOI 10.1109/TEM.2012.2191618
   Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404043
DA 2019-06-15
ER

PT S
AU Dunner, C
   Parnell, T
   Jaggi, M
AF Dunner, Celestine
   Parnell, Thomas
   Jaggi, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Use of Limited-Memory Accelerators for Linear Learning on
   Heterogeneous Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a generic algorithmic building block to accelerate training of machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.
C1 [Dunner, Celestine; Parnell, Thomas] IBM Res Zurich, Zurich, Switzerland.
   [Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Dunner, C (reprint author), IBM Res Zurich, Zurich, Switzerland.
EM cdu@zurich.ibm.com; tpa@zurich.ibm.com; martin.jaggi@epfl.ch
RI Jeong, Yongwook/N-7413-2016
CR Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Chang K. -W., 2011, P 17 ACM SIGKDD INT, P699
   Csiba Dominik, 2015, ICML 2015 P 32 INT C
   Dunner Celestine, 2016, P 33 INT C MACH LEAR, V48, P783
   Fercoq O, 2016, SIAM REV, V58, P739, DOI 10.1137/16M1085905
   Heinze Christina, 2016, AISTATS P 20 INT C A, P875
   Matsushima S., 2012, P 18 ACM SIGKDD INT, P177
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Osokin A., 2016, P INT C MACH LEARN, P593
   Parnell Thomas, 2017, P 6 INT WORKSH PAR D
   Perekrestenko Dmytro, 2017, AISTATS ARTIFICIAL I, V54, P869
   Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Smith Virginia, 2016, COCOA GEN FRAMEWORK
   Yen Ian En-Hsu, 2015, ADV NEURAL INFORM PR, V28, P3582
   Yu Hsiang-Fu, 2012, ACM T KNOWL DISCOV D, V5, P1
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404032
DA 2019-06-15
ER

PT S
AU Dutil, F
   Gulcehre, C
   Trischler, A
   Bengio, Y
AF Dutil, Francis
   Gulcehre, Caglar
   Trischler, Adam
   Bengio, Yoshua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Plan, Attend, Generate: Planning for Sequence-to-Sequence Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We investigate the integration of a planning mechanism into sequence-to-sequence models using attention. We develop a model which can plan ahead in the future when it computes its alignments between input and output sequences, constructing a matrix of proposed future alignments and a commitment vector that governs whether to follow or recompute the plan. This mechanism is inspired by the recently proposed strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our proposed model is end-to-end trainable using primarily differentiable operations. We show that it outperforms a strong baseline on character-level translation tasks from WMT' 15, the algorithmic task of finding Eulerian circuits of graphs, and question generation from the text. Our analysis demonstrates that the model computes qualitatively intuitive alignments, converges faster than the baselines, and achieves superior performance with fewer parameters.
C1 [Dutil, Francis; Gulcehre, Caglar; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Trischler, Adam] Microsoft Res Maluuba, Montreal, PQ, Canada.
RP Dutil, F (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.
EM frdutil@gmail.com; ca9lar@gmail.com; adam.trischler@microsoft.com;
   yoshua.umontreal@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Ba J. L., 2016, ARXIV160706450
   Bahdanau  D., 2015, INT C LEARN REPR ICL
   Bahdanau Dzmitry, 2016, ARXIV160707086
   Bengio Y., 2013, ARXIV13083432
   Bengio Y., 2016, ARXIV160306147
   Chan W., 2015, ARXIV150801211
   Cho K., 2014, ARXIV14091259
   Cho K, 2014, ARXIV14061078
   Dietterich Thomas G, 2000, HIERARCHICAL REINFOR
   Gulcehre  C., 2017, ARXIV170108718
   Gulcehre Caglar, 2016, ARXIV160308148
   Jang Eric, 2016, ARXIV161101144
   Kalchbrenner N., 2016, ARXIV161010099
   Lee J., 2016, ARXIV161003017
   Li Y., 2015, ARXIV151105493
   Luo  Yuping, 2016, ARXIV160801281
   Luong MT, 2016, ARXIV160400788
   Maddison Chris J, 2016, ARXIV161100712
   Pascanu R., 2013, ARXIV13126026
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Rajpurkar P., 2016, ARXIV160605250
   Sennrich  R., 2015, ARXIV150807909
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Vezhnevets A., 2016, ADV NEURAL INFORM PR, P3486
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xu K, 2015, INT C MACH LEARN, V32, P2048
   Yang ZJ, 2016, ADV SOC SCI EDUC HUM, V64, P1480
   Yuan X., 2017, ARXIV170502012
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405054
DA 2019-06-15
ER

PT S
AU Dutta, A
   Vijayaraghavan, A
   Wang, A
AF Dutta, Abhratanu
   Vijayaraghavan, Aravindan
   Wang, Alex
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Clustering Stable Instances of Euclidean k-means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The Euclidean k-means problem is arguably the most widely-studied clustering problem in machine learning. While the k-means objective is NP-hard in the worst-case, practitioners have enjoyed remarkable success in applying heuristics like Lloyd's algorithm for this problem. To address this disconnect, we study the following question: what properties of real-world instances will enable us to design efficient algorithms and prove guarantees for finding the optimal clustering? We consider a natural notion called additive perturbation stability that we believe captures many practical instances of Euclidean k-means clustering. Stable instances have unique optimal k-means solutions that does not change even when each point is perturbed a little (in Euclidean distance). This captures the property that k-means optimal solution should be tolerant to measurement errors and uncertainty in the points. We design efficient algorithms that provably recover the optimal clustering for instances that are additive perturbation stable. When the instance has some additional separation, we can design a simple, efficient algorithm with provable guarantees that is also robust to outliers. We also complement these results by studying the amount of stability in real datasets, and demonstrating that our algorithm performs well on these benchmark datasets.
C1 [Dutta, Abhratanu; Vijayaraghavan, Aravindan] Northwestern Univ, Evanston, IL 60208 USA.
   [Wang, Alex] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Dutta, A (reprint author), Northwestern Univ, Evanston, IL 60208 USA.
EM adutta@u.northwestern.edu; aravindv@northwestern.edu;
   alexwang@u.northwestern.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation (NSF) [CCF-1637585]
FX Supported by the National Science Foundation (NSF) under Grant No.
   CCF-1637585.
CR Ackerman M., 2009, P 12 INT C ART INT S, V5, P1
   Angelidakis Haris, 2017, S THEOR COMP STOC
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4
   Awasthi P., 2015, LEIBNIZ INT P INFORM, P754
   Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006
   Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575
   Balcan MF, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1068
   Ben-David S., 2015, ABS150100437 CORR
   Ben-David S, 2014, THEOR COMPUT SCI, V558, P51, DOI 10.1016/j.tcs.2014.09.025
   Bilu Yonatan, 2010, P 1 S INN COMP SCI I, P332
   BLOCK HD, 1962, REV MOD PHYS, V34, P123, DOI 10.1103/RevModPhys.34.123
   Blum Avrim, 2002, P S DISCR ALG SODA
   Dasgupta S., 2008, HARDNESS K MEANS CLU
   Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35
   Makarychev Konstantin, 2014, P 22 S DISCR ALG SOD
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   Williams DE, 2011, PHARM BIOL, V49, P296, DOI 10.3109/13880209.2010.517540
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406055
DA 2019-06-15
ER

PT S
AU Eickenberg, M
   Exarchakis, G
   Hirn, M
   Mallat, S
AF Eickenberg, Michael
   Exarchakis, Georgios
   Hirn, Matthew
   Mallat, Stephane
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy
   from Invariant Descriptors of 3D Electronic Densities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a solid harmonic wavelet scattering representation, invariant to rigid motion and stable to deformations, for regression and classification of 2D and 3D signals. Solid harmonic wavelets are computed by multiplying solid harmonic functions with Gaussian windows dilated at different scales. Invariant scattering coefficients are obtained by cascading such wavelet transforms with the complex modulus nonlinearity. We study an application of solid harmonic scattering invariants to the estimation of quantum molecular energies, which are also invariant to rigid motion and stable with respect to deformations. A multilinear regression over scattering invariants provides close to state of the art results over small and large databases of organic molecules.
C1 [Eickenberg, Michael; Exarchakis, Georgios] PSL Res Univ, Ecole Normale Super, Dept Comp Sci, F-75005 Paris, France.
   [Hirn, Matthew] Michigan State Univ, Dept Computat Math Sci & Engn, Dept Math, E Lansing, MI 48824 USA.
   [Mallat, Stephane] PSL Res Univ, Ecole Normale Super, Coll France, F-75005 Paris, France.
RP Eickenberg, M (reprint author), PSL Res Univ, Ecole Normale Super, Dept Comp Sci, F-75005 Paris, France.
EM michael.eickenberg@nsup.org; georgios.exarchakis@ens.fr; mhirn@msu.edu
RI Jeong, Yongwook/N-7413-2016
FU ERC [320959]; Alfred P. Sloan Fellowship; DARPA YFA; NSF [1620216]
FX M.E., G.E. and S.M. are supported by ERC grant InvariantClass 320959;
   M.H. is supported by the Alfred P. Sloan Fellowship, the DARPA YFA, and
   NSF grant 1620216.
CR Bartok A. P., 2013, PHYS REV B, V87
   Collins Christopher R., 2017, CONSTANT SIZE MOL DE
   De S, 2016, PHYS CHEM CHEM PHYS, V18, P13754, DOI 10.1039/c6cp00415f
   Deglmann P, 2015, INT J QUANTUM CHEM, V115, P107, DOI 10.1002/qua.24811
   Faber Felix A., J CHEM THEORY COMPUT
   Gilmer Justin, 2017, CORR
   Hansen K, 2015, J PHYS CHEM LETT, V6, P2326, DOI 10.1021/acs.jpclett.5b00831
   Hansen K, 2013, J CHEM THEORY COMPUT, V9, P3404, DOI 10.1021/ct400195d
   Hirn M, 2017, MULTISCALE MODEL SIM, V15, P827, DOI 10.1137/16M1075454
   Huang B, 2016, J CHEM PHYS, V145, DOI 10.1063/1.4964627
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203
   Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413
   Memisevic R, 2011, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2011.6126419
   Montavon G, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/9/095003
   Muller K., 2012, ADV NEURAL INFORM PR, V25, P449
   Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22
   Rupp M, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.058301
   Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406059
DA 2019-06-15
ER

PT S
AU El Housni, O
   Goyal, V
AF El Housni, Omar
   Goyal, Vineet
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Beyond Worst-case: A Probabilistic Analysis of Affine Policies in
   Dynamic Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ROBUST SOLUTIONS; INEQUALITIES
AB Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is O(root m) that is also tight (see Bertsimas and Goyal [8]), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we show that affine policies give a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. On the other hand, we also present a distribution such that the performance bound for affine policies on instances generated according to that distribution is Omega(root m) with high probability; however, the constraint coefficients are not i.i.d.. This demonstrates that the empirical performance of affine policies can depend on the generative model for instances.
C1 [El Housni, Omar; Goyal, Vineet] Columbia Univ, IEOR Dept, New York, NY 10027 USA.
RP El Housni, O (reprint author), Columbia Univ, IEOR Dept, New York, NY 10027 USA.
EM oe2148@columbia.edu; vg2277@columbia.edu
RI Jeong, Yongwook/N-7413-2016
CR Ben-Tal A, 1998, MATH OPER RES, V23, P769, DOI 10.1287/moor.23.4.769
   Ben-Tal A, 2004, MATH PROGRAM, V99, P351, DOI 10.1007/s10107-003-0454-y
   Ben-Tal A, 2002, MATH PROGRAM, V92, P453, DOI 10.1007/s101070100286
   Ben-Tal A, 1999, OPER RES LETT, V25, P1, DOI 10.1016/S0167-6377(99)00016-4
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Bertsimas D, 2004, OPER RES, V52, P35, DOI 10.1287/opre.1030.0065
   Bertsimas D, 2003, MATH PROGRAM, V98, P49, DOI 10.1007/s10107-003-0396-4
   Bertsimas D, 2016, INFORMS J COMPUT, V28, P500, DOI 10.1287/ijoc.2016.0689
   Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510
   Bertsimas D, 2011, MATH OPER RES, V36, P24, DOI 10.1287/moor.1110.0482
   Bertsimas D, 2012, MATH PROGRAM, V134, P491, DOI 10.1007/s10107-011-0444-4
   Dantzig GB, 1955, MANAGE SCI, V1, P197, DOI 10.1287/mnsc.1.3-4.197
   El Housni O., 2017, MATH PROGRAM, P1
   ElGhaoui L, 1997, SIAM J MATRIX ANAL A, V18, P1035, DOI 10.1137/S0895479896298130
   Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115
   Feige U, 2007, LECT NOTES COMPUT SC, V4513, P439
   Goldfarb D, 2003, MATH OPER RES, V28, P1, DOI 10.1287/moor.28.1.1.14260
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kall  P., 1994, STOCHASTIC PROGRAMMI
   Prekopa A., 1995, STOCHASTIC PROGRAMMI
   Shapiro A, 2009, LECT STOCHASTIC PROG
   Shapiro A, 2008, MATH PROGRAM, V112, P183, DOI 10.1007/s10107-006-0090-4
   SOYSTER AL, 1973, OPER RES, V21, P1154, DOI 10.1287/opre.21.5.1154
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404080
DA 2019-06-15
ER

PT S
AU El Mhamdi, EM
   Guerraoui, R
   Hendrikx, H
   Maurer, A
AF El Mhamdi, El Mandi
   Guerraoui, Rachid
   Hendrikx, Hadrien
   Maurer, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dynamic Safe Interruptibility for Decentralized Multi-Agent
   Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to interrupt an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong [16] defined safe interruptibility for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces dynamic safe interruptibility, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: joint action learners and independent learners. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.
C1 [El Mhamdi, El Mandi; Guerraoui, Rachid; Maurer, Alexandre] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Hendrikx, Hadrien] Ecole Polytech, Palaiseau, France.
RP Hendrikx, H (reprint author), Ecole Polytech, Palaiseau, France.
EM elmandi.elmhamdi@epfl.ch; rachid.guerraoui@epfl.ch;
   hadrien.hendrikx@gmail.com; alexandre.maurer@epfl.ch
RI Jeong, Yongwook/N-7413-2016
FU European ERC [339539 - AOC]; Swiss National Science Foundation
   [200021_169588]
FX This work has been supported in part by the European ERC (Grant 339539 -
   AOC) and by the Swiss National Science Foundation (Grant 200021_169588
   TARBDA).
CR Boutilier C, 1996, THEORETICAL ASPECTS OF RATIONALITY AND KNOWLEDGE, P195
   Claus Caroline, 1998, AAAI IAAI, V746, P752
   Crites RH, 1998, MACH LEARN, V33, P235, DOI 10.1023/A:1007518724497
   Foerster  J., 2016, ADV NEURAL INFORM PR, V2016, P2137
   Goertzel B., 2007, ARTIFICIAL GEN INTEL, V2
   Gomes E. R., 2009, P 26 ANN INT C MACH, P369
   LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176
   Lattimore T, 2011, LECT NOTES ARTIF INT, V6925, P368, DOI 10.1007/978-3-642-24412-4_29
   Littman M., 1994, P 11 INT C MACH LEAR, V157, P157
   Littman M. L., 2001, Cognitive Systems Research, V2, P55, DOI 10.1016/S1389-0417(01)00015-8
   Littman M. L., 2001, P 18 INT C MACH LEAR, V1, P322
   Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Orseau Laurent, 2016, UNC ART INT 32 C UAI, P557
   Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2
   Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tampuu Ardi, 2015, ARXIV151108779
   Tesauro G, 2004, ADV NEUR IN, V16, P871
   Tesauro G, 2002, AUTON AGENT MULTI-AG, V5, P289, DOI 10.1023/A:1015504423309
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Wang X., 2002, ADV NEURAL INFORM PR, P1571
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wunder Michael, 2010, P 27 INT C MACH LEAR, P1167
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400013
DA 2019-06-15
ER

PT S
AU Eleftheriadis, S
   Nicholson, TFW
   Deisenroth, MP
   Hensman, J
AF Eleftheriadis, Stefanos
   Nicholson, Thomas F. W.
   Deisenroth, Marc P.
   Hensman, James
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Identification of Gaussian Process State Space Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The Gaussian process state space model (GPSSM) is a non-linear dynamical system, where unknown transition and/or measurement mappings are described by GPs. Most research in GPSSMs has focussed on the state estimation problem, i.e., computing a posterior of the latent state given the model. However, the key challenge in GPSSMs has not been satisfactorily addressed yet: system identification, i.e., learning the model. To address this challenge, we impose a structured Gaussian variational posterior distribution over the latent states, which is parameterised by a recognition model in the form of a bi-directional recurrent neural network. Inference with this structure allows us to recover a posterior smoothed over sequences of data. We provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick. This further allows for the use of arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can efficiently generate plausible future trajectories of the identified system after only observing a small number of episodes from the true system.
C1 [Eleftheriadis, Stefanos; Nicholson, Thomas F. W.; Deisenroth, Marc P.; Hensman, James] PROWLER Io, Cambridge CB2 1LA, England.
   [Deisenroth, Marc P.] Imperial Coll London, London, England.
RP Eleftheriadis, S (reprint author), PROWLER Io, Cambridge CB2 1LA, England.
EM stefanos@prowler.io; tom@prowler.io; marc@prowler.io; james@prowler.io
RI Jeong, Yongwook/N-7413-2016
FU Google faculty research award
FX Marc P. Deisenroth has been supported by a Google faculty research
   award.
CR Al-Shedivat Maruan, 2016, ARXIV161008936
   Beal M.J., 2003, THESIS
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Brown EN, 1998, J NEUROSCI, V18, P7411
   Calandra Roberto, 2016, IEEE INT JOINT C NEU
   Cesar Lincoln C., 2015, INT C LEARN REPR
   Cho K., 2014, ARXIV14091259
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Dai Zhenwen, 2015, INT C LEARN REPR
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618
   Deisenroth Marc P., 2009, P 26 ANN INT C MACH, P225
   Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218
   Deisenroth MP, 2012, IEEE T AUTOMAT CONTR, V57, P1865, DOI 10.1109/TAC.2011.2179426
   Frigola R., 2013, ADV NEURAL INFORM PR, V26, P3156
   Frigola Roger, 2015, THESIS
   Ghahramani Z, 2014, ADV NEURAL INFORM PR, P3680
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x
   Ko Jonathan, 2009, ROBOTICS SCI SYSTEMS
   Kocijan J, 2016, ADV IND CONTROL, P1, DOI 10.1007/978-3-319-21021-6
   Likar B, 2007, COMPUT CHEM ENG, V31, P142, DOI 10.1016/j.compchemeng.2006.05.011
   Ljung L, 1999, SYSTEM IDENTIFICATIO
   Matthews A. G. de G., 2016, J MACH LEARN RES, P231
   Matthews AGD, 2017, J MACH LEARN RES, V18, P1
   Matthews Alexander G. de G., 2017, THESIS
   Murray-Smith R., 2001, IR SIGN SYST C MAYN, P147
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550
   Schneider Jeff G., 1997, ADV NEURAL INFORM PR
   Sjoberg J, 1995, AUTOMATICA, V31, P1691, DOI 10.1016/0005-1098(95)00120-8
   Titsias M, 2009, ARTIF INTELL, P567
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Titsias M. K., 2010, P 13 INT C ART INT S, P844
   Turner R., 2010, W CP, P868
   Turner R. D, 2011, THESIS
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405038
DA 2019-06-15
ER

PT S
AU Elenberg, ER
   Dimakis, AG
   Feldman, M
   Karbasi, A
AF Elenberg, Ethan R.
   Dimakis, Alexandros G.
   Feldman, Moran
   Karbasi, Amin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI StreamingWeak Submodularity: Interpreting Neural Networks on the Fly
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID APPROXIMATIONS
AB In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].
C1 [Elenberg, Ethan R.; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
   [Feldman, Moran] Open Univ Israel, Dept Math & Comp Sci, Raanana, Israel.
   [Karbasi, Amin] Yale Univ, Dept Elect Engn, Dept Comp Sci, New Haven, CT 06520 USA.
RP Elenberg, ER (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM elenberg@utexas.edu; dimakis@austin.utexas.edu; moranfe@openu.ac.il;
   amin.karbasi@yale.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [CCF 1344364, 1407278, 1422549, 1618689]; ARO [YIP
   W911NF-14-1-0258]; ISF [1357/16]; Google Faculty Research Award; DARPA
   Young Faculty Award [D16AP00046]
FX This research has been supported by NSF Grants CCF 1344364, 1407278,
   1422549, 1618689, ARO YIP W911NF-14-1-0258, ISF Grant 1357/16, Google
   Faculty Research Award, and DARPA Young Faculty Award (D16AP00046).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Altschuler  J., 2016, INT C MACH LEARN, P2539
   Bach F, 2013, FDN TRENDS MACHINE L, V6
   Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Bahmani S, 2013, J MACH LEARN RES, V14, P807
   Barbosa RD, 2016, ANN IEEE SYMP FOUND, P645, DOI 10.1109/FOCS.2016.74
   Bian Andrew An, 2017, AISTATS, P111
   Buchbinder N., 2015, SODA, P1202
   Buchbinder N., 2016, SODA, P392
   Buchbinder  Niv, 2016, ABS161103253 CORR
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   CHAN THH, 2017, SODA, P1204
   CHEKURI C, 2015, ICALP, V9134, P318, DOI DOI 10.1007/978-3-662-47672-7_26
   CONFORTI M, 1984, DISCRETE APPL MATH, V7, P251, DOI 10.1016/0166-218X(84)90003-9
   da Ponte Barbosa R., 2015, ICML, P1236
   Das A., 2011, P 28 INT C MACH LEAR, P1057
   Elenberg Ethan R., 2016, NIPS WORKSH LEARN HI
   Elenberg Ethan R., 2016, ABS161200804 CORR
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73
   Hassidim Avinatan, 2017, COLT, P1069
   Hoi S. C., 2006, P 23 INT C MACH LEAR, P417, DOI DOI 10.1145/1143844.1143897
   Horel Thibaut, 2016, NIPS
   Khanna R., 2017, ICML, P1837
   Khanna  R., 2017, AISTATS, P1560
   Krause A., 2010, P 27 INT C MACH LEAR, P567
   Krause A, 2014, TRACTABILITY, P71
   Lichman M., 2013, UCI MACHINE LEARNING
   Mirzasoleiman B., 2015, P 29 AAAI C ART INT, P1812
   Mirzasoleiman B., 2013, ADV NEURAL INFORM PR, P2049
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Pan X, 2014, NIPS, P118
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   Sundararajan M, 2017, P 34 INT C MACH LEAR, V70, P3319
   Sviridenko  M., 2015, P 26 ANN ACM SIAM S, P1134
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vondrak Jan, 2010, RIMS KOKYUROKU BES B, VB23, P253
   Wei K., 2015, ICML, P1954
   Yang Z., 2016, ICML, P2472
NR 40
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404012
DA 2019-06-15
ER

PT S
AU Elhamifar, E
   Kaluza, MCD
AF Elhamifar, Ehsan
   Kaluza, M. Clara De Paolis
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Subset Selection and Summarization in Sequential Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Subset selection, which is the task of finding a small subset of representative items from a large ground set, finds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relationships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that finds a set of representatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efficiently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives.
C1 [Elhamifar, Ehsan; Kaluza, M. Clara De Paolis] Northeastern Univ, Comp & Informat Sci Coll, Boston, MA 02115 USA.
RP Elhamifar, E (reprint author), Northeastern Univ, Comp & Informat Sci Coll, Boston, MA 02115 USA.
EM eelhami@ccs.neu.edu; clara@ccs.neu.edu
FU NSF [IIS-1657197]; Northeastern University, College of Computer and
   Information Science
FX This work is supported by NSF IIS-1657197 award and startup funds from
   the Northeastern University, College of Computer and Information
   Science.
CR Affandi R. H., 2012, C UNC ART INT
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Alayrac J.-B., 2016, COMPUTER VISION PATT
   Awasthi P., 2015, C INN THEOR COMP SCI
   Bishop C., 2007, PATTERN RECOGNITION
   Borodin A., 2000, COMMUNICATIONS MATH, V211
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Carbonell J., 1998, SIGIR
   Civril A., 2009, THEORETICAL COMPUTER, V410
   Duda R O, 2004, PATTERN CLASSIFICATI
   Elhamifar E., 2016, IEEE T PATTERN ANAL
   Elhamifar E., 2015, AAAI C ART INT
   Elhamifar E., 2014, WORLD C INT FED AUT
   Elhamifar E., 2012, NEURAL INFORM PROCES
   Elhamifar E., 2017, IEEE C COMP VIS PATT
   Elhamifar E, 2011, PROC CVPR IEEE
   Esser E, 2012, IEEE T IMAGE PROCESS, V21, P3239, DOI 10.1109/TIP.2012.2190081
   Feige U., 1998, J ACM
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Garcia S, 2012, IEEE T PATTERN ANAL, V34, P417, DOI 10.1109/TPAMI.2011.142
   Ghahramani Z., 1997, MACHINE LEARNING, V29
   Ghahramani Z., 2008, NIPS
   Gong B., 2014, NEURAL INFORM PROCES
   Gonzalez T., 1985, THEORETICAL COMPUTER, V38
   Guyon I., 2003, J MACHINE LEARNING R
   Gygli M., 2014, EUR C COMP VIS
   Hadlock F., 1975, SIAM J COMPUTING, V4
   Hartline J., 2008, WORLD WID WEB C
   Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095
   Kim G., 2011, INT C COMP VIS
   Krause A., 2008, J MACHINE LEARNING R, V9
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Kulesza A., 2011, INT C MACH LEARN
   Kulesza A, 2012, FDN TRENDS MACHINE L, V5
   Lin H., 2012, C UNC ART INT
   McSherry D., 2002, ADV CASE BASED REASO
   Mirchandani P. B., 1990, DISCRETE LOCATION TH
   Misra I., 2014, WINT C APPL COMP VIS
   Motwani R., 1995, RANDOMIZED ALGORITHM
   Nellore A., 2015, INFORM COMPUTATION
   Nemhauser G., 1978, MATH PROGRAMMING, V14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Reichart R., 2013, C ASS COMP LING
   Shah A., 2013, C UNC ART INT
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   Tschiatschek S., 2017, AAAI
NR 46
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401008
DA 2019-06-15
ER

PT S
AU Emamjomeh-Zadeh, E
   Kempe, D
AF Emamjomeh-Zadeh, Ehsan
   Kempe, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A General Framework for Robust Interactive Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ORDER
AB We propose a general framework for interactively learning models, such as (binary or non-binary) classifiers, orderings/rankings of items, or clusterings of data points. Our framework is based on a generalization of Angluin's equivalence query model and Littlestone's online learning model: in each iteration, the algorithm proposes a model, and the user either accepts it or reveals a specific mistake in the proposal. The feedback is correct only with probability p > 1/2 (and adversarially incorrect with probability 1 - p), i.e., the algorithm must be able to learn in the presence of arbitrary noise. The algorithm's goal is to learn the ground truth model using few iterations.
   Our general framework is based on a graph representation of the models and user feedback. To be able to learn efficiently, it is sufficient that there be a graph G whose nodes are the models, and (weighted) edges capture the user feedback, with the property that if s, s* are the proposed and target models, respectively, then any (correct) user feedback s' must lie on a shortest s-s* path in G. Under this one assumption, there is a natural algorithm, reminiscent of the Multiplicative Weights Update algorithm, which will efficiently learn s* even in the presence of noise in the user's feedback.
   From this general result, we rederive with barely any extra effort classic results on learning of classifiers and a recent result on interactive clustering; in addition, we easily obtain new interactive learning algorithms for ordering/ranking.
C1 [Emamjomeh-Zadeh, Ehsan; Kempe, David] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
RP Emamjomeh-Zadeh, E (reprint author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM emamjome@usc.edu; dkempe@usc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [1619458]
FX Research supported in part by NSF grant 1619458. We would like to thank
   Sanjoy Dasgupta, Ilias Diakonikolas, Shaddin Dughmi, Haipeng Luo,
   Shanghua Teng, and anonymous reviewers for useful feedback and
   suggestions.
CR Agichtein E., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P3, DOI 10.1145/1148170.1148175
   Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746
   Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753
   Awasthi P., 2010, ADV NEURAL INFORM PR, P91
   Awasthi P, 2017, J MACH LEARN RES, V18
   Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27
   Bubley R, 1999, DISCRETE MATH, V201, P81, DOI 10.1016/S0012-365X(98)00333-1
   Bubley R., 2001, DISTINGUISHED DISSER
   Crammer K, 2002, ADV NEUR IN, V14, P641
   Emamjomeh-Zadeh E, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P519, DOI 10.1145/2897518.2897656
   Huber M, 2006, DISCRETE MATH, V306, P420, DOI 10.1016/j.disc.2006.01.003
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   KARZANOV A, 1991, ORDER, V8, P7, DOI 10.1007/BF00385809
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   MAASS W, 1994, MACH LEARN, V14, P251, DOI 10.1023/A:1022653511837
   Maass W., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P203, DOI 10.1109/FSCS.1990.89539
   MAASS W, 1992, MACH LEARN, V9, P107, DOI 10.1007/BF00992674
   Radlinski F., 2005, P 11 ACM SIGKDD INT, P239, DOI DOI 10.1145/1081870.1081899
   Sauer N., 1972, Journal of Combinatorial Theory, Series A, V13, P145, DOI 10.1016/0097-3165(72)90019-2
   SHELAH S, 1972, PAC J MATH, V41, P247, DOI 10.2140/pjm.1972.41.247
   Wagstaff K. L., 2002, THESIS
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407017
DA 2019-06-15
ER

PT S
AU Ene, A
   Nguyen, HL
   Vegh, LA
AF Ene, Alina
   Nguyen, Huy L.
   Vegh, Laszlo A.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Decomposable Submodular Function Minimization Discrete and Continuous
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHM
AB This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.
C1 [Ene, Alina] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
   [Nguyen, Huy L.] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
   [Vegh, Laszlo A.] London Sch Econ, Dept Math, London, England.
RP Ene, A (reprint author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
EM aene@bu.edu; hu.nguyen@northeastern.edu; L.Vegh@lse.ac.uk
CR Arora C, 2012, LECT NOTES COMPUT SC, V7576, P17, DOI 10.1007/978-3-642-33715-4_2
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Chakrabarty  D., 2014, ADV NEURAL INFORM PR, P802
   Edmonds J., 1970, COMBINATORIAL STRUCT, P69
   ENE A., 2015, P 32 INT C MACH LEAR
   Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993
   Fix A, 2014, PROC CVPR IEEE, P1138, DOI 10.1109/CVPR.2014.149
   Fix A, 2013, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2013.385
   Fleischer L, 2003, DISCRETE APPL MATH, V131, P311, DOI 10.1016/S0166-218X(02)00458-4
   FUJISHIGE S, 1980, MATH OPER RES, V5, P186, DOI 10.1287/moor.5.2.186
   Fujishige S., 1992, JAPAN J IND APPL MAT, V9, P369
   Fujishige S, 2011, PAC J OPTIM, V7, P3
   GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051
   GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273
   Iwata S, 2003, SIAM J COMPUT, V32, P833, DOI 10.1137/S0097539701397813
   Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096
   Iwata S., 2009, ACM SIAM S DISCR ALG
   Jegelka S., 2011, ADV NEURAL INFORM PR, P460
   Jegelka S., 2013, ADV NEURAL INFORM PR
   Jegelka Stefanie, 2011, ICML, P345
   Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025
   Lee Y. T., 2015, IEEE FDN COMPUTER SC
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nishihara R., 2014, ADV NEURAL INFORM PR, P640
   Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989
   Schrijver A., 2003, COMBINATORIAL OPTIMI
   Shanu I, 2016, PROC CVPR IEEE, P5365, DOI 10.1109/CVPR.2016.579
   Stobbe P., 2010, ADV NEURAL INFORM PR
   WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402090
DA 2019-06-15
ER

PT S
AU Erdogdu, MA
   Deshpande, Y
   Montanari, A
AF Erdogdu, Murat A.
   Deshpande, Yash
   Montanari, Andrea
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inference in Graphical Models via Semidefinite Programming Hierarchies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PROPAGATION; RELAXATIONS; ALGORITHM; RANK; CUT
AB Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with n vertices require solving an SDP with n(Theta(d)) variables where d is the degree in the hierarchy. In practice, for d >= 4, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation.
C1 [Erdogdu, Murat A.; Deshpande, Yash] Microsoft Res, Cambridge, MA 02142 USA.
   [Deshpande, Yash] MIT, Cambridge, MA 02139 USA.
   [Montanari, Andrea] Stanford Univ, Stanford, CA 94305 USA.
RP Erdogdu, MA (reprint author), Microsoft Res, Cambridge, MA 02142 USA.
EM erdogdu@cs.toronto.edu; yash@mit.edu; montanari@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023
   BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028
   Barak Boaz, 2016, COURSE NOTES
   BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Cowell Robert G., 2006, PROBABILISTIC NETWOR
   EDWARDS SF, 1975, J PHYS F MET PHYS, V5, P965, DOI 10.1088/0305-4608/5/5/017
   Erdogdu Murat A, 2015, ADV NEURAL INFORM PR, V2, P3052
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lasserre J. B., 2001, Integer Programming and Combinatorial Optimization. 8th International IPCO Conference. Proceedings (Lecture Notes in Computer Science Vol.2081), P293
   Mei Song, 2017, ARXIV170308729
   Mezard M., 2009, INFORM PHYS COMPUTAT
   MORE JJ, 1983, SIAM J SCI STAT COMP, V4, P553, DOI 10.1137/0904038
   Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5
   Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339
   PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X
   Richardson T., 2008, MODERN CODING THEORY
   SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036
   SHOR NZ, 1987, CYBERNETICS+, V23, P731
   Sun J, 2002, LECT NOTES COMPUT SC, V2351, P510
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2004, ADV NEUR IN, V16, P369
   Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585
   Weller A., 2016, ARTIF INTELL, V51, P47
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
   Yedidia JS, 2000, NIPS, V13, P689
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400040
DA 2019-06-15
ER

PT S
AU Fai, K
   Wei, Q
   Carin, L
   Heller, K
AF Fai, Kai
   Wei, Qi
   Carin, Lawrence
   Heller, Katherine
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI An Inner-loop Free Solution to Inverse Problems using Deep Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FUSION
AB We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.
C1 [Fai, Kai; Wei, Qi; Carin, Lawrence; Heller, Katherine] Duke Univ, Durham, NC 27706 USA.
RP Fai, K (reprint author), Duke Univ, Durham, NC 27706 USA.
EM kai.fan@stat.duke.edu; qi.wei@duke.edu; lcarin@duke.edu;
   kheller@stat.duke.edu
RI Jeong, Yongwook/N-7413-2016
FU Siemens Corporate Research
FX The authors would like to thank Siemens Corporate Research for
   supporting this work and thank NVIDIA for the GPU donations.
CR Adler Jonas, 2017, ARXIV170404058
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Bruna J., 2015, ARXIV151105666
   Chang J.H., 2017, ARXIV170309912
   Csaji B. C., 2001, TECH REP, V24, P48
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Dosovitskiy Alexey, 2016, ADV NEURAL INFORM PR, V29, P658
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu ST, 2017, IEEE T SIGNAL PROCES, V65, P3120, DOI 10.1109/TSP.2017.2679687
   MAURER H, 1979, MATH PROGRAM, V16, P98, DOI 10.1007/BF01582096
   Nguyen A, 2016, ARXIV161200005
   Schlemper Jo, 2017, ARXIV170300555
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simoes M, 2015, IEEE T GEOSCI REMOTE, V53, P3373, DOI 10.1109/TGRS.2014.2375320
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sonderby C. K., 2016, ARXIV161004490
   Tarantola A, 2005, INVERSE PROBLEM THEO
   Tikhonov A. N., 1977, SCRIPTA SERIES MATH
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wei Q, 2016, IEEE SIGNAL PROC LET, V23, P1632, DOI 10.1109/LSP.2016.2608858
   Wei Q, 2015, IEEE T IMAGE PROCESS, V24, P4109, DOI 10.1109/TIP.2015.2458572
   Wei Q, 2015, IEEE J-STSP, V9, P1117, DOI 10.1109/JSTSP.2015.2407855
   Zhao NN, 2016, IEEE T IMAGE PROCESS, V25, P3683, DOI 10.1109/TIP.2016.2567075
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402041
DA 2019-06-15
ER

PT S
AU Falahatgar, M
   Ohannessian, M
   Orlitsky, A
   Pichapati, V
AF Falahatgar, Moein
   Ohannessian, Mesrob
   Orlitsky, Alon
   Pichapati, Venkatadheeraj
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The power of absolute discounting: all-dimensional distribution
   estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Categorical models are a natural fit for many problems. When learning the distribution of categories from samples, high-dimensionality may dilute the data. Minimax optimality is too pessimistic to remedy this issue. A serendipitously discovered estimator, absolute discounting, corrects empirical frequencies by subtracting a constant from observed categories, which it then redistributes among the unobserved. It outperforms classical estimators empirically, and has been used extensively in natural language modeling. In this paper, we rigorously explain the prowess of this estimator using less pessimistic notions. We show that (1) absolute discounting recovers classical minimax KL-risk rates, (2) it is adaptive to an effective dimension rather than the true dimension, (3) it is strongly related to the Good-Turing estimator and inherits its competitive properties. We use power-law distributions as the cornerstone of these results. We validate the theory via synthetic data and an application to the Global Terrorism Database.
C1 [Falahatgar, Moein; Orlitsky, Alon; Pichapati, Venkatadheeraj] UCSD, La Jolla, CA 92093 USA.
   [Ohannessian, Mesrob] TTIC, Chicago, IL USA.
RP Falahatgar, M (reprint author), UCSD, La Jolla, CA 92093 USA.
EM moein@ucsd.edu; mesrob@gmail.com; alon@ucsd.edu; dheerajpv7@ucsd.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CIF-1564355, CIF-1619448]
FX We thank Vaishakh Ravindrakumar for very helpful suggestions, and NSF
   for supporting this work through grants CIF-1564355 and CIF-1619448.
CR Acharya Jayadev, 2013, C LEARN THEOR, P764
   Allahverdyan AE, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.062804
   Boucheron S, 2015, IEEE T INFORM THEORY, V61, P4948, DOI 10.1109/TIT.2015.2455058
   Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010
   Chen S. F., 1996, P ACL 1996 SANT CRUZ, V34, P310, DOI DOI 10.3115/981863.981904
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Falahatgar M, 2015, IEEE INT SYMP INFO, P2001, DOI 10.1109/ISIT.2015.7282806
   Falahatgar Moein, 2016, NIPS, P4860
   Favaro S, 2016, BIOMETRICS, V72, P136, DOI 10.1111/biom.12366
   Gale William A., 1995, J QUANT LINGUIST, V2, P217, DOI DOI 10.1080/09296179508590051
   GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344
   Hamou Anna Ben, 2017, CONCENTRATION INEQUA
   Jozefowicz R., 2016, ARXIV160202410
   Katz Slava M., 1987, ESTIMATION PROBABILI
   Kneser R., 1995, P IEEE INT C AC SPEE, V1, P181, DOI DOI 10.1109/ICASSP.1995.479394
   LaFree Gary, 2016, GLOB TERR DAT
   Mercer R., 1985, IBM TECHNICAL DISCLO, V28, P2591
   Natalini P, 2000, MATH INEQUAL APPL, V3, P69
   Neuman Edward, 2013, RESULTS MATH, P1
   NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001
   Ohannessian Mesrob I, 2012, COLT, P21
   Orlitsky A, 2003, SCIENCE, V302, P427, DOI 10.1126/science.1088284
   Orlitsky Alon, 2015, ADV NIPS, P2143
   Paninski L., 2004, P ADV NEUR INF PROC, P1033
   Pitman J, 1997, ANN PROBAB, V25, P855
   Smith FA, 2003, ECOLOGY, V84, P3403, DOI 10.1890/02-9003
   Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Valiant Gregory, 2015, ARXIV150405321
   Zipf G. K, 1935, PSYCHOBIOLOGY LANGUA
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406070
DA 2019-06-15
ER

PT S
AU Falahatgar, M
   Hao, Y
   Orlitsky, A
   Pichapati, V
   Ravindrakumar, V
AF Falahatgar, Moein
   Hao, Yi
   Orlitsky, Alon
   Pichapati, Venkatadheeraj
   Ravindrakumar, Vaishakh
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Maxing and Ranking with Few Assumptions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB PAC maximum selection (maxing) and ranking of n elements via random pairwise comparisons have diverse applications and have been studied under many models and assumptions. With just one simple natural assumption: strong stochastic transitivity, we show that maxing can be performed with linearly many comparisons yet ranking requires quadratically many. With no assumptions at all, we show that for the Borda-score metric, maximum selection can be performed with linearly many comparisons and ranking can be performed with O(n log n) comparisons.
C1 [Falahatgar, Moein; Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj; Ravindrakumar, Vaishakh] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Falahatgar, M (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM moein@ucsd.edu; yih179@ucsd.edu; alon@ucsd.edu; dheerajpv7@ucsd.edu;
   vaishakhr@ucsd.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CIF-1564355, CIF-1619448]
FX We thank NSF for supporting this work through grants CIF-1564355 and
   CIF-1619448.
CR Acharya J, 2014, IEEE INT SYMP INFO, P1682, DOI 10.1109/ISIT.2014.6875120
   Acharya Jayadev, 2014, NIPS
   Acharya Jayadev, 2016, ARXIV160602786
   Ajtai M, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2701427
   [Anonymous], 2017, NONTR DIC
   Busa-Fekete R., 2014, P 31 INT C MACH LEAR, V32, P1071
   Busa-Fekete Robert, 2014, AAAI
   Falahatgar Moein, 2017, INT C MACH LEARN, P1088
   FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877
   Heckel Reinhard, 2016, ARXIV160608842
   Herbrich R., 2006, ADV NEURAL INFORM PR, P569
   Jamieson Kevin, 2015, ARTIF INTELL, P416
   Jang Minje, 2016, ARXIV160304153
   Lee D. T., 2014, 2 AAAI C HUM COMP CR
   Luce R. D, 2005, INDIVIDUAL CHOICE BE
   Negahban Sahand, 2016, OPERATIONS RES
   Negahban Sahand, 2012, ADV NEURAL INFORM PR, P2474
   Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567
   Radlinski F., 2008, P 17 ACM C INF KNOWL, P43
   Radlinski F, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P570
   Rajkumar Arun, 2014, P 31 INT C MACH LEAR, P118
   Szorenyi Balazs, 2015, P NEUR INF PROC SYST, P604
   Urvoy T., 2013, P 30 INT C MACH LEAR, V28, P91
   Wang Jialei, 2014, P ACM SIGKDD NEW YOR, P502
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
   Zhou Y., 2014, P 31 INT C MACH LEAR, P217
   Zhou Yuan, 2014, OPTIMAL PAC MULTIPLE
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407015
DA 2019-06-15
ER

PT S
AU Fan, LX
AF Fan, Lixin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU
   with Generalized Hamming Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the "normalized" bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.
C1 [Fan, Lixin] Nokia Technol, Tampere, Finland.
RP Fan, LX (reprint author), Nokia Technol, Tampere, Finland.
EM lixin.fan@nokia.com
CR Atanassov Krassimir, 2011, NOTES INTUITIONISTIC, V17, P1
   Bedregal BC, 2009, ELECTRON NOTES THEOR, V247, P5, DOI 10.1016/j.entcs.2009.07.045
   Belohlavek R., 2017, FUZZY LOGIC MATH HIS
   Benitez JM, 1997, IEEE T NEURAL NETWOR, V8, P1156, DOI 10.1109/72.623216
   Bulsari A., 1992, Complex Systems, V6, P443
   Calonder M., 2010, COMPUT VIS ECCV, V2010, P778, DOI DOI 10.1007/978-3-642-15561-1_56
   Courbariaux M., 2016, CORR
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   GUPTA MM, 1994, FUZZY SET SYST, V61, P1, DOI 10.1016/0165-0114(94)90279-8
   Hahnloser R., 2000, DIGITAL SELECTION AN, V405
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu Zhiting, 2016, P 54 ANN M ASS COMP, V1
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   JANG JSR, 1993, IEEE T NEURAL NETWOR, V4, P156, DOI 10.1109/72.182710
   Kim Y, 2014, CORR
   Kingma D.P., 2013, ARXIV13126114
   Kulis B., 2009, ADV NEURAL INFORM PR, P1042
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin K, 2015, IEEE COMPUT SOC CONF
   Liu P., 2004, SERIES MACHINE PERCE
   Mcculloch W, 1943, B MATH BIOPHYS, V5, P127
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Norouzi M., 2011, INT C MACH LEARN, P353
   Norouzi M., 2012, ADV NEURAL INFORM PR, V25, P1061
   Pedrycz Witold, 2002, SOFT COMPUTING, V7
   Rastegari M., 2016, CORR
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Salimans Tim, 2016, WEIGHT NORMALIZATION, P901
   Seung H. S., 2001, NIPS
   Tick J, 2005, COMPUT INFORM, V24, P591
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zimmermann H.J., 2001, FUZZY SET THEORY ITS
   Zimmermann H. - J, 2010, ADV REV
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401092
DA 2019-06-15
ER

PT S
AU Fan, YB
   Lyu, SW
   Ying, YM
   Hu, BG
AF Fan, Yanbo
   Lyu, Siwei
   Ying, Yiming
   Hu, Bao-Gang
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning with Average Top-k Loss
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SUPPORT; ALGORITHM
AB In this work, we introduce the average top-k (AT(k)) loss as a new aggregate loss for supervised learning, which is the average over the k largest individual losses over a training dataset. We show that the AT(k) loss is a natural generalization of the two widely used aggregate losses, namely the average loss and the maximum loss, but can combine their advantages and mitigate their drawbacks to better adapt to different data distributions. Furthermore, it remains a convex function over all individual losses, which can lead to convex optimization problems that can be solved effectively with conventional gradient-based methods. We provide an intuitive interpretation of the AT(k) loss based on its equivalent effect on the continuous individual loss functions, suggesting that it can reduce the penalty on correctly classified data. We further give a learning theory analysis of MAT(k) learning on the classification calibration of the AT(k) loss and the error bounds of AT(k)-SVM. We demonstrate the applicability of minimum average top-k learning for binary classification and regression using synthetic and real datasets.
C1 [Fan, Yanbo; Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
   [Ying, Yiming] SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA.
   [Fan, Yanbo; Hu, Bao-Gang] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Fan, Yanbo; Hu, Bao-Gang] Univ Chinese Acad Sci, Beijing, Peoples R China.
RP Lyu, SW (reprint author), SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
EM yanbolan@nlpr.ia.ac.cn; slyu@albany.edu; yying@albany.edu;
   hubg@nlpr.ia.ac.cn
RI Jeong, Yongwook/N-7413-2016
FU University of Chinese Academy of Sciences (UCAS); National Science
   Foundation (NSF) [IIS-1537257]; Simons Foundation [422504]; Presidential
   Innovation Fund for Research and Scholarship (PIFRS) program from SUNY
   Albany; National Science Foundation of China (NSFC) [61620106003]
FX We thank the anonymous reviewers for their constructive comments. This
   work was completed when the first author was a visiting student at SUNY
   Albany, supported by a scholarship from University of Chinese Academy of
   Sciences (UCAS). Siwei Lyu is supported by the National Science
   Foundation (NSF, Grant IIS-1537257) and Yiming Ying is supported by the
   Simons Foundation (#422504) and the 2016-2017 Presidential Innovation
   Fund for Research and Scholarship (PIFRS) program from SUNY Albany. This
   work is also partially supported by the National Science Foundation of
   China (NSFC, Grant 61620106003) for Bao-Gang Hu and Yanbo Fan.
CR Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crammer K., 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628
   De Vito E, 2005, FOUND COMPUT MATH, V5, P59, DOI 10.1007/s10208-004-0134-1
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Fan Y., 2017, P 31 AAAI C ART INT, P1877
   He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220
   Kumar M. P., 2010, ADV NEURAL INFORM PR, P1189
   Lapin M., 2015, P ADV NEUR INF PROC, P325
   Lapin M, 2016, PROC CVPR IEEE, P1468, DOI 10.1109/CVPR.2016.163
   Lin Y, 2004, STAT PROBABIL LETT, V68, P73, DOI 10.1016/j.spl.2004.03.002
   Masnadi-Shirazi H., 2009, ADV NEURAL INFORM PR, P1049
   Ogryczak W, 2003, INFORM PROCESS LETT, V85, P117, DOI 10.1016/S0020-0190(02)00370-8
   Rudin C, 2009, J MACH LEARN RES, V10, P2233
   Scholkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Shalev- Shwartz Shai, 2016, ICML
   Srebro N., 2010, ICML TUTORIAL
   Steinwart I, 2003, IEEE T PATTERN ANAL, V25, P1274, DOI 10.1109/TPAMI.2003.1233901
   Steinwart I, 2008, INFORM SCI STAT, P1
   Usunier N., 2009, P 26 ANN INT C MACH, P1057
   Vapnik V. N., 1998, STAT LEARNING THEORY, V1
   Wu Q, 2006, FOUND COMPUT MATH, V6, P171, DOI 10.1007/s10208-004-0155-9
   Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617
   Yu Y., 2010, P ADV NEUR INF PROC, V23, P2532
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400048
DA 2019-06-15
ER

PT S
AU Fang, C
   Cheng, F
   Lin, ZC
AF Fang, Cong
   Cheng, Feng
   Lin, Zhouchen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of
   Multipliers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE RATE
AB We study stochastic convex optimization subjected to linear equality constraints. Traditional Stochastic Alternating Direction Method of Multipliers [1] and its Nesterov's acceleration scheme [2] can only achieve ergodic O(1/ root K) convergence rates, where K is the number of iteration. By introducing Variance Reduction (VR) techniques, the convergence rates improve to ergodic O(1/ root K) [3, 4]. In this paper, we propose a new stochastic ADMM which elaborately integrates Nesterov's extrapolation and VR techniques. With Nesterov's extrapolation, our algorithm can achieve a non-ergodic O(1/K) convergence rate which is optimal for separable linearly constrained non-smooth convex problems, while the convergence rates of VR based ADMM methods are actually tight O(1/ root K) in non-ergodic sense. To the best of our knowledge, this is the first work that achieves a truly accelerated, stochastic convergence rate for constrained convex problems. The experimental results demonstrate that our algorithm is faster than the existing state-of-the-art stochastic ADMM methods.
C1 [Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.
   Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China.
RP Lin, ZC (reprint author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.
EM fangcong@pku.edu.cn; fengcheng@pku.edu.cn; zlin@pku.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National Basic Research Program of China (973 Program) [2015CB352502];
   National Natural Science Foundation (NSF) of China [61625301, 61731018,
   61231002]
FX Zhouchen Lin is supported by National Basic Research Program of China
   (973 Program) (grant no. 2015CB352502) and National Natural Science
   Foundation (NSF) of China (grant no.s 61625301, 61731018, and 61231002).
CR Argyriou Andreas, 2007, P C ADV NEUR INF PRO
   AzadiSra Samaneh, 2014, P INT C MACH LEARN
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Chen CH, 2015, SIAM J IMAGING SCI, V8, P2239, DOI 10.1137/15100463X
   Davis D, 2016, SCI COMPUT, P115, DOI 10.1007/978-3-319-41589-5_4
   Defazio A, 2014, ADV NEUR IN, V27
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936
   Hua Ouyang, 2013, P INT C MACH LEARN
   Johnson Rie, 2013, P C ADV NEUR INF PRO
   Kim S, 2009, BIOINFORMATICS, V25, pI204, DOI 10.1093/bioinformatics/btp218
   Li H., 2016, ARXIV160806366
   Li Shen, 2015, P INT JOINT C ART IN
   Lin Z.C., 2010, 100920105055 ARXIV, V1009, P5055, DOI DOI 10.1016/J.JSB.2012.10.010
   Lin ZC, 2015, MACH LEARN, V99, P287, DOI 10.1007/s10994-014-5469-5
   Lin Zhouchen, 2011, P C ADV NEUR INF PRO
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Nesterov Yu, 1988, EKONOM I MAT METODY, V24, P509
   Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6
   Suzuki Taiji, 2013, P INT C MACH LEARN
   Suzuki Taiji, 2014, P INT C MACH LEARN
   Tseng P., 2008, TECHNICAL REPORT
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Zeyuan Allen-Zhu, 2017, ANN S THEOR COMP
   Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8
   Zheng Shuai, 2016, P INT JOINT C ART IN
   Zhong Wenliang, 2014, P INT C MACH LEARN
   Zuo WM, 2011, IEEE T IMAGE PROCESS, V20, P2748, DOI 10.1109/TIP.2011.2131665
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404053
DA 2019-06-15
ER

PT S
AU Fang, L
   Yang, F
   Dong, W
   Guan, T
   Qiao, CM
AF Fang, Le
   Yang, Fan
   Dong, Wen
   Guan, Tong
   Qiao, Chunming
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Expectation Propagation with Stochastic Kinetic Model in Complex
   Interaction Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INFERENCE
AB Technological breakthroughs allow us to collect data with increasing spatio-temporal resolution from complex interaction systems. The combination of high-resolution observations, expressive dynamic models, and efficient machine learning algorithms can lead to crucial insights into complex interaction dynamics and the functions of these systems. In this paper, we formulate the dynamics of a complex interacting network as a stochastic process driven by a sequence of events, and develop expectation propagation algorithms to make inferences from noisy observations. To avoid getting stuck at a local optimum, we formulate the problem of minimizing Be the free energy as a constrained primal problem and take advantage of the concavity of dual problem in the feasible domain of dual variables guaranteed by duality theorem. Our expectation propagation algorithms demonstrate better performance in inferring the interaction dynamics in complex transportation networks than competing models such as particle filter, extended Kalman filter, and deep neural networks.
C1 [Fang, Le; Yang, Fan; Dong, Wen; Guan, Tong; Qiao, Chunming] Univ Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
RP Fang, L (reprint author), Univ Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM lefang@buffalo.edu; fyang24@buffalo.edu; wendong@buffalo.edu;
   tongguan@buffalo.edu; qiao@buffalo.edu
RI Jeong, Yongwook/N-7413-2016
CR Arkin A, 1998, GENETICS, V149, P1633
   Balmer M., 2009, MULTIAGENT SYSTEMS T, P57, DOI DOI 10.4018/978-1-60566-226-8.CH003
   Boyd S., 2004, CONVEX OPTIMIZATION
   Del Moral P., 1996, MARKOV PROCESS RELAT, V2, P555
   Doucet Arnaud, 2000, P 16 C UNC ART INT, P176, DOI DOI 10.1049/IET-SPR:20070075.
   Friston KJ, 2003, NEURAL NETWORKS, V16, P1325, DOI 10.1016/j.neunet.2003.06.005
   Gillespie DT, 2007, ANNU REV PHYS CHEM, V58, P35, DOI 10.1146/annurev.physchem.58.032806.104637
   Golightly Andrew, 2013, Methods Mol Biol, V1021, P169, DOI 10.1007/978-1-62703-450-0_9
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   GRASSMANN WK, 1977, COMPUT OPER RES, V4, P47, DOI 10.1016/0305-0548(77)90007-7
   Heskes T, 2002, P 18 C UNC ART INT, P216
   Julier SJ, 2004, P IEEE, V92, P401, DOI 10.1109/JPROC.2003.823141
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Minka Thomas P, 2001, EP ENERGY FUNCTION M
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rao Vinayak, 2012, ARXIV12023760
   Tebaldi C, 1998, J AM STAT ASSOC, V93, P557, DOI 10.2307/2670105
   Tong Guan, 2017, WIR COMM NETW C WCNC, P1
   Vrettas MD, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012148
   Welling M., 2001, P 17 C UNC ART INT, P554
   Wen Dong, 2012, ARXIV12104864
   Wilkinson D. J., 2011, STOCHASTIC MODELLING
   Xu Zhen, 2016, ADV NEURAL INFORM PR, P2775
   Yang F, 2017, LECT NOTES COMPUT SC, V10354, P193, DOI 10.1007/978-3-319-60240-0_23
   Yedidia J. S., 2003, EXPLORING ARTIF INTE, V8, P236
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402008
DA 2019-06-15
ER

PT S
AU Fanti, G
   Viswanath, P
AF Fanti, Giulia
   Viswanath, Pramod
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deanonymization in the Bitcoin P2P Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recent attacks on Bitcoin's peer-to-peer (P2P) network demonstrated that its transaction-flooding protocols, which are used to ensure network consistency, may enable user deanonymization-the linkage of a user's IP address with her pseudonym in the Bitcoin network. In 2015, the Bitcoin community responded to these attacks by changing the network's flooding mechanism to a different protocol, known as diffusion. However, it is unclear if diffusion actually improves the system's anonymity. In this paper, we model the Bitcoin networking stack and analyze its anonymity properties, both pre- and post-2015. The core problem is one of epidemic source inference over graphs, where the observational model and spreading mechanisms are informed by Bitcoin's implementation; notably, these models have not been studied in the epidemic source detection literature before. We identify and analyze near-optimal source estimators. This analysis suggests that Bitcoin's networking protocols (both pre- and post-2015) offer poor anonymity properties on networks with a regular-tree topology. We confirm this claim in simulation on a 2015 snapshot of the real Bitcoin P2P network topology.
C1 [Fanti, Giulia] Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.
   [Viswanath, Pramod] Univ Illinois, ECE Dept, Champaign, IL USA.
RP Fanti, G (reprint author), Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1705007]
FX This work was funded by NSF grant CCF-1705007.
CR Androulaki Elli, 2013, INT C FIN CRYPT DAT, P34, DOI DOI 10.1007/978-3-642-39884-1_
   Athreya Krishna B, 2012, BRANCHING PROCESSES, V196
   Bender C. M., 1999, ADV MATH METHODS SCI
   Biryukov A., 2014, P 2014 ACM SIGSAC C, P15, DOI [10.1145/2660267.2660379, DOI 10.1145/2660267.2660379]
   Biryukov A, 2015, P IEEE S SECUR PRIV, P122, DOI 10.1109/SP.2015.15
   Boneh A., 1997, COMM STAT STOCHASTIC, P39, DOI [DOI 10.1080/15326349708807412), 10.1080/15326349708807412]
   Chen Z, 2016, IEEE TRANS NETW SCI, V3, P17, DOI 10.1109/TNSE.2016.2523804
   Eggenberger F, 1923, Z ANGEW MATH MECH, V3, P279, DOI 10.1002/zamm.19230030407
   Fanti G., 2015, ICML
   Fioriti V., 2012, ARXIV12112333
   Janson S, 2004, STOCH PROC APPL, V110, P177, DOI 10.1016/j.spa.2003.12.002
   Khim J., 2015, ARXIV151005461
   Koshy P, 2014, LECT NOTES COMPUT SC, V8437, P469, DOI 10.1007/978-3-662-45472-5_30
   Lokhov A. Y., 2013, ARXIV13035315
   Mah Paul, 2016, TOP 5 VPN SERVICES P
   Mahmoud H., 2008, POLYA URN MODELS
   Meiklejohn S., 2013, P 2013 C INT MEAS C, P127, DOI DOI 10.1145/2504730.2504747
   Miller A., 2015, DISCOVERING BITCOINS
   Milling Chris, 2012, Performance Evaluation Review, V40, P223, DOI 10.1145/2318857.2254784
   Morris David Z., 2017, LEGAL SPARRING CONTI
   Nakamoto S, 2008, BITCOIN PEER TO PEER
   Ober M, 2013, FUTURE INTERNET, V5, P237, DOI 10.3390/fi5020237
   Pinto PC, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.068702
   Prakash BA, 2012, IEEE DATA MINING, P11, DOI 10.1109/ICDM.2012.136
   Reid F., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P1318, DOI 10.1109/PASSAT/SocialCom.2011.79
   Ron D., 2013, LNCS, P6, DOI DOI 10.1007/978-3-642-39884-1
   Shah Devavrat, 2012, Performance Evaluation Review, V40, P199, DOI 10.1145/2318857.2254782
   Shah D, 2011, IEEE T INFORM THEORY, V57, P5163, DOI 10.1109/TIT.2011.2158885
   Shah D, 2010, PERF E R SI, V38, P203, DOI 10.1145/1811099.1811063
   Wang Z., 2014, ACM SIGMETRICS
   Weisstein Eric W, 2002, EULER MASCHERONI CON
   Zhu K., 2014, COMPUT SOC NETW, V1
   Zhu K., 2013, ARXIV13094846
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401039
DA 2019-06-15
ER

PT S
AU Farahmand, AM
   Pourazarm, S
   Nikovski, D
AF Farahmand, Amir-massoud
   Pourazarm, Sepideh
   Nikovski, Daniel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Random Projection Filter Bank for Time Series Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction, classification with time series data, etc. Different filters in RPFB extract different aspects of the time series, and together they provide a reasonably good summary of the time series. RPFB is easy to implement, fast to compute, and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series.
C1 [Farahmand, Amir-massoud; Pourazarm, Sepideh; Nikovski, Daniel] MERL, Cambridge, MA 02139 USA.
RP Farahmand, AM (reprint author), MERL, Cambridge, MA 02139 USA.
EM farahmand@merl.com; sepid@bu.edu; nikovski@merl.com
RI Jeong, Yongwook/N-7413-2016
CR Baraniuk RG, 2009, FOUND COMPUT MATH, V9, P51, DOI 10.1007/s10208-007-9011-z
   Bishop C. M., 2006, PATTERN RECOGNITION
   Cho K., 2014, ARXIV14091259
   Doukhan P., 1994, LECT NOTES STAT, V85
   Farahmand AM, 2012, J STAT PLAN INFER, V142, P493, DOI 10.1016/j.jspi.2011.08.007
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hastie T, 2001, ELEMENTS STAT LEARNI
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kakade Sham, 2017, ARXIV161202526V2
   Kuznetsov V., 2015, ADV NEURAL INFORM PR, V28, P541
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810
   Mohri M., 2009, ADV NEURAL INFORM PR, P1097
   Mohri M, 2010, J MACH LEARN RES, V11, P789
   Oliva Junier B., 2017, P 34 INT C MACH LEAR, P2671
   Oppenheim AV, 1999, DISCRETE TIME SIGNAL
   Pourazarm Sepideh, 2017, ANN C PROGN HLTH MAN, P242
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rakhlin Alexander, 2014, PROBABILITY THEORY R
   Rakhlin Alexander, 2010, ADV NEURAL INFORM PR
   Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001
   Steinwart Ingo, 2009, ADV NEURAL INF PROCE, p[1768, 6]
   Vempala Santosh S., 2004, DIMACS SERIES DISCRE
   Wasserman Larry, 2007, SPRINGER TEXTS STAT
   White Martha, 2015, P 29 AAAI C ART INT
   YU B, 1994, ANN PROBAB, V22, P94, DOI 10.1214/aop/1176988849
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406061
DA 2019-06-15
ER

PT S
AU Farhan, M
   Tariq, J
   Zaman, A
   Shabbir, M
   Khan, IU
AF Farhan, Muhammad
   Tariq, Juvaria
   Zaman, Arif
   Shabbir, Mudassir
   Khan, Imdad Ullah
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Approximation Algorithms for String Kernel Based Sequence
   Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Sequence classification algorithms, such as SVM, require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition, by considering two k-mers to match if their distance is at most m, yields better classification performance. This, however, makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work, we develop novel techniques to efficiently and accurately estimate the pairwise similarity score, which enables us to use much larger values of k and m, and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio, images, and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem, which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.
C1 [Farhan, Muhammad; Zaman, Arif; Khan, Imdad Ullah] Lahore Univ Management Sci, Sch Sci & Engn, Dept Comp Sci, Lahore, Pakistan.
   [Tariq, Juvaria] Lahore Univ Management Sci Lahore, Sch Sci & Engn, Dept Math, Lahore, Pakistan.
   [Shabbir, Mudassir] Informat Technol Univ, Dept Comp Sci, Lahore, Pakistan.
RP Farhan, M (reprint author), Lahore Univ Management Sci, Sch Sci & Engn, Dept Comp Sci, Lahore, Pakistan.
EM 14030031@lums.edu.pk; jtariq@emory.edu; arifz@lums.edu.pk;
   mudassir.shabbir@itu.edu.pk; imdad.khan@lums.edu.pk
RI Jeong, Yongwook/N-7413-2016; Khan, Imdadullah/G-1037-2016
OI Khan, Imdadullah/0000-0002-6955-6168
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng JL, 2006, BIOINFORMATICS, V22, P1456, DOI 10.1093/bioinformatics/btl102
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Ding CHQ, 2001, BIOINFORMATICS, V17, P349, DOI 10.1093/bioinformatics/17.4.349
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Ellis D. P., 2007, P 8 INT C MUS INF RE, V7, P339
   Francis FB, 2005, P 22 INT C MACH LEAR, P33
   Haussler D., 1999, UCSCRL9910
   Kuang Rui, 2005, Journal of Bioinformatics and Computational Biology, V3, P527, DOI 10.1142/S021972000500120X
   Kuksa P., 2012, P SIAM INT C DAT MIN, P873
   Kuksa P., 2011, THESIS
   Kuksa P., 2008, 19 INT C PATT REC IC, P1
   Kuksa P., 2009, ADV NEURAL INF PROCE, P881
   Kuksa P P, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3320, DOI 10.1109/ICPR.2010.1159
   Kuksa P. P., 2013, CORR
   Kuksa P, 2010, LECT NOTES ARTIF INT, V6322, P128, DOI 10.1007/978-3-642-15883-4_9
   Leslie C, 2004, J MACH LEARN RES, V5, P1435
   Leslie C, 2003, ADV NEURAL INFORM PR, V15, P1441, DOI 10.1.1.58.4737
   Leslie CS, 2002, P PSB, V7, P566
   Li T., 2003, P 26 ANN INT ACM SIG, P282
   Lo Conte L, 2000, NUCLEIC ACIDS RES, V28, P257, DOI 10.1093/nar/28.1.257
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rahimi A., 2008, ADV NEURAL INFORM PR, V21, P1313
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Sonnenburg S., 2005, P 22 INT C MACH LEAR, P848, DOI DOI 10.1145/1102351.1102458
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Vapnik V. N., 1998, STAT LEARNING THEORY, V1
   Vishwanathan S., 2002, NIPS 02, P585
   Waterman M., PHYLOGENETIC ANAL DN, P59
   Watkins C., 1999, ADV LARGE MARGIN CLA, P39
   Weston J, 2004, ADV NEUR IN, V16, P595
   Weston J, 2005, BIOINFORMATICS, V21, P3241, DOI 10.1093/bioinformatics/bti497
   Williams C. K., 2000, P 13 INT C NEUR INF, P661
   Yang T., 2012, ADV NEURAL INFORM PR, P476
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407003
DA 2019-06-15
ER

PT S
AU Fathony, R
   Bashiri, M
   Ziebart, BD
AF Fathony, Rizal
   Bashiri, Mohammad
   Ziebart, Brian D.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adversarial Surrogate Losses for Ordinal Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONSISTENCY
AB Ordinal regression seeks class label predictions when the penalty incurred for mistakes increases according to an ordering over the labels. The absolute error is a canonical example. Many existing methods for this task reduce to binary classification problems and employ surrogate losses, such as the hinge loss. We instead derive uniquely defined surrogate ordinal regression loss functions by seeking the predictor that is robust to the worst-case approximations of training data labels, subject to matching certain provided training data statistics. We demonstrate the advantages of our approach over other surrogate losses based on hinge loss approximations using UCI ordinal prediction tasks.
C1 [Fathony, Rizal; Bashiri, Mohammad; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
RP Fathony, R (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM rfatho2@uic.edu; mbashi4@uic.edu; bziebart@uic.edu
RI Jeong, Yongwook/N-7413-2016
FU Future of Life Institute FLI-RFP-AI1 program [2016-158710]; NSF
   [1526379]
FX This research was supported as part of the Future of Life Institute
   (futureoflife.org) FLI-RFP-AI1 program, grant#2016-158710 and by NSF
   grant RI-#1526379.
CR Asif Kaiser, 2015, P C UNC ART INT
   Cardoso JS, 2007, J MACH LEARN RES, V8, P1393
   Cheng JL, 2008, IEEE IJCNN, P1279, DOI 10.1109/IJCNN.2008.4633963
   Chu W, 2005, J MACH LEARN RES, V6, P1019
   Chu  W., 2005, P 22 INT C MACH LEAR, P145
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Crammer Koby, 2001, ADV NEURAL INFORM PR, V14
   Dembczynski K., 2007, P ECML PKDD 07 WORKS, P169
   Deng WY, 2010, NEUROCOMPUTING, V74, P447, DOI 10.1016/j.neucom.2010.08.022
   Dudik M, 2006, LECT NOTES ARTIF INT, V4005, P123, DOI 10.1007/11776420_12
   Farnia Farzan, 2016, ADV NEURAL INFORM PR, P4233
   Fathony Rizal, 2016, ADV NEURAL INFORM PR, V29, P559
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin  H., 2008, THESIS
   Lin HT, 2006, LECT NOTES ARTIF INT, V4264, P319
   Lin Hsuan-Tien, 2014, JMLR WORKSHOP C P, P371
   Lin L, 2007, ADV NEURAL INFORM PR, P865
   Liu A., 2014, ADV NEURAL INFORM PR, P37
   Liu Yufeng, 2007, AISTATS, P291
   Mathieson M., 1996, Neural Networks in Financial Engineering. Proceedings of the Third International Conference on Neural Networks in the Capital Markets, P523
   MCCULLAGH P, 1980, J ROY STAT SOC B MET, V42, P109
   NARULA SC, 1982, INT STAT REV, V50, P317, DOI 10.2307/1402501
   Pedregosa F, 2017, J MACH LEARN RES, V18, P1
   Ramaswamy Harish G, 2012, ADV NEURAL INFORM PR, P2078
   Rennie J. D., 2005, P IJCAI MULT WORKSH, P180
   Schmidt Mark, 2015, NONUNIFORM STOCHASTI
   Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6
   Shashua  A., 2003, P ADV NEUR INF PROC, P961
   Sun BY, 2010, IEEE T KNOWL DATA EN, V22, P906, DOI 10.1109/TKDE.2009.170
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   TOPSOE F, 1979, KYBERNETIKA, V15, P8
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Tu H.-H., 2010, P 27 INT C MACH LEAR, P1095
   Wang Hong, 2015, ADV NEURAL INFORM PR, P2710
   Xiong H, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P1039
   Yu S., 2006, ICML 06 P 23 INT C M, P1089
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400054
DA 2019-06-15
ER

PT S
AU Feizi, S
   Javadi, H
   Tse, D
AF Feizi, Soheil
   Javadi, Hamid
   Tse, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Tensor Biclustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace, modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover, we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally, we show the efficiency of our proposed method in several synthetic and real datasets.
C1 [Feizi, Soheil; Javadi, Hamid; Tse, David] Stanford Univ, Stanford, CA 94305 USA.
RP Feizi, S (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM sfeizi@stanford.edu; hrhakim@stanford.edu; dntse@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR ANANDKUMAR A, 2014, ARXIV14025180
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2239
   Ardlie KG, 2015, SCIENCE, V348, P648, DOI 10.1126/science.1262110
   Chen R, 2012, CELL, V148, P1293, DOI 10.1016/j.cell.2012.02.009
   Chen Y., 2014, ARXIV14021267
   Hopkins Samuel B, 2015, COLT JMLR WORKSHOP C, P956
   Hopkins Samuel B, 2015, ARXIV151202337
   Hore V, 2016, NAT GENET, V48, P1094, DOI 10.1038/ng.3624
   Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248
   Lesieur T., 2017, ARXIV170108010
   Montanari A., 2014, ADV NEURAL INFORM PR, P2897
   Montanari  Andrea, 2015, ADV NEURAL INFORM PR, P217
   Perry A., 2016, ARXIV161207728
   Tanay A., 2005, HDB COMPUTATIONAL MO, V9, P122, DOI 10.1.1.133.9434
   Tony Cai  T, 2015, ARXIV150201988
   Zhang Anru, 2017, ARXIV170302724
   Zhong Yiqiao, 2017, ARXIV170306605
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401034
DA 2019-06-15
ER

PT S
AU Flajolet, A
   Jaillet, P
AF Flajolet, Arthur
   Jaillet, Patrick
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Real-Time Bidding with Side Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the problem of repeated bidding in online advertising auctions when some side information (e.g. browser cookies) is available ahead of submitting a bid in the form of a d-dimensional vector. The goal for the advertiser is to maximize the total utility (e.g. the total number of clicks) derived from displaying ads given that a limited budget B is allocated for a given time horizon T. Optimizing the bids is modeled as a contextual Multi-Armed Bandit (MAB) problem with a knapsack constraint and a continuum of arms. We develop UCB-type algorithms that combine two streams of literature: the confidence-set approach to linear contextual MABs and the probabilistic bisection search method for stochastic root-finding. Under mild assumptions on the underlying unknown distribution, we establish distribution-independent regret bounds of order (O) over tilde (d . root T) when either B = infinity or when B scales linearly with T.
C1 [Flajolet, Arthur] MIT, ORC, Cambridge, MA 02139 USA.
   [Jaillet, Patrick] MIT, EECS, LIDS, ORC, Cambridge, MA 02139 USA.
RP Flajolet, A (reprint author), MIT, ORC, Cambridge, MA 02139 USA.
EM flajolet@mit.edu; jaillet@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU Office of Naval Research (ONR) [N00014-15-1-2083]
FX Research funded in part by the Office of Naval Research (ONR) grant
   N00014-15-1-2083.
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Agrawal S., 2016, ADV NEURAL INFORM PR, P3450
   Agrawal S, 2016, P 29 ANN C LEARN THE, V49, P4
   Amin Kareem, 2012, P 28 C UNC ART INT, P54
   Amin Kareem, 2014, ADV NEURAL INFORM PR, P622
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P., 2002, J MACHINE LEARNING R, V3, p397 , DOI DOI 10.1162/153244303321897663
   Babaioff M., 2012, P 13 ACM C EL COMM, P74
   Badanidiyuru A., 2014, P 27 ANN C LEARN THE, P1109
   Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30
   Balseiro S., 2017, P 18 ACM C EC COMP, P609
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bastani H, 2015, WORKING PAPER
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Cohen MC, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P817, DOI 10.1145/2940716.2940728
   Dani V., 2008, COLT, P355
   Ghosh Arpita, 2009, P 18 INT C WORLD WID, P251
   Lei Y., 2015, WORKING PAPER
   Lueker GS, 1998, J ALGORITHM, V29, P277, DOI 10.1006/jagm.1998.0954
   Pasupathy R, 2011, ACM T MODEL COMPUT S, V21, DOI 10.1145/1921598.1921603
   Tran-Thanh L., 2014, P 30 C UNC ART INT, P809
   Wang ZZ, 2014, OPER RES, V62, P318, DOI 10.1287/opre.2013.1245
   Weed J., 2016, 29 ANN C LEARN THEOR, P1562
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405024
DA 2019-06-15
ER

PT S
AU Fletcher, AK
   Sahraee-Ardakan, M
   Rangan, S
   Schniter, P
AF Fletcher, Alyson K.
   Sahraee-Ardakan, Mojtaba
   Rangan, Sundeep
   Schniter, Philip
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned
   Linear Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MESSAGE-PASSING ALGORITHMS; APPROXIMATE; INFERENCE
AB We consider the problem of estimating a random vector x from noisy linear measurements y = Ax + w in the setting where parameters theta on the distribution of x and w must be learned in addition to the vector x. This problem arises in a wide range of statistical learning and linear inverse problems. Our main contribution shows that a computationally simple iterative message passing algorithm can provably obtain asymptotically consistent estimates in a certain high-dimensional large system limit (LSL) under very general parametrizations. Importantly, this LSL applies to all right-rotationally random A - a much larger class of matrices than Lid. sub-Gaussian matrices to which many past message passing approaches are restricted. In addition, a simple testable condition is provided in which the mean square error (MSE) on the vector x matches the Bayes optimal MSE predicted by the replica method. The proposed algorithm uses a combination of Expectation-Maximization (EM) with a recently-developed Vector Approximate Message Passing (VAMP) technique. We develop an analysis framework that shows that the parameter estimates in each iteration of the algorithm converge to deterministic limits that can be precisely predicted by a simple set of state evolution (SE) equations. The SE equations, which extends those of VAMP without parameter adaptation, depend only on the initial parameter estimates and the statistical properties of the problem and can be used to predict consistency and precisely characterize other performance measures of the method.
C1 [Fletcher, Alyson K.] UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.
   [Sahraee-Ardakan, Mojtaba] UC Los Angeles, Dept EE, Los Angeles, CA USA.
   [Rangan, Sundeep] NYU, Dept ECE, New York, NY 10003 USA.
   [Schniter, Philip] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
RP Fletcher, AK (reprint author), UC Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.
EM akfletcher@ucla.edu; msahraee@ucla.edu; srangan@nyu.edu;
   schniter@ece.osu.edu
RI Jeong, Yongwook/N-7413-2016; Fletcher, Alyson K/C-3226-2015
OI Fletcher, Alyson K/0000-0002-3756-6580
FU National Science Foundation [1254204, 1738286, 1116589, 1302336,
   1547332, CCF-1527162]; Office of Naval Research [N00014-15-1-2677]; NYU
   WIRELESS
FX A. K. Fletcher and M. Saharee-Ardakan were supported in part by the
   National Science Foundation under Grants 1254204 and 1738286 and the
   Office of Naval Research under Grant N00014-15-1-2677. S. Rangan was
   supported in part by the National Science Foundation under Grants
   1116589, 1302336, and 1547332, and the industrial affiliates of NYU
   WIRELESS. The work of P. Schniter was supported in part by the National
   Science Foundation under Grant CCF-1527162.
CR Barbier J., 2016, ARXIV160702335
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Fletcher A., 2017, RIGOROUS DYNAMICS CO
   Fletcher A. K., 2017, P IEEE ICASSP
   Fletcher A. K., 2011, P NEUR INF PROC SYST, P2555
   Fletcher A, 2016, IEEE INT SYMP INFO, P190, DOI 10.1109/ISIT.2016.7541287
   Ghahramani Z., 2014, ADV NEURAL INFORM PR, V27, P2843
   Heskes T, 2004, ADV NEUR IN, V16, P353
   Javanmard A, 2013, INF INFERENCE, V2, P115, DOI 10.1093/imaiai/iat004
   Kamilov US, 2014, IEEE T INFORM THEORY, V60, P2969, DOI 10.1109/TIT.2014.2309005
   Krzakala F, 2012, PHYS REV X, V2, DOI 10.1103/PhysRevX.2.021005
   Manoel A., 2015, P ICML, P1123
   Opper M, 2005, J MACH LEARN RES, V6, P2177
   Opper M., 2004, P NIPS, P1001
   Rangan S., 2011, P IEEE INT S INF THE, P2174
   Rangan S., 2017, P IEEE ISIT JUN
   Rangan S, 2017, IEEE T INFORM THEORY, V63, P676, DOI 10.1109/TIT.2016.2619373
   Rangan S, 2014, IEEE INT SYMP INFO, P236, DOI 10.1109/ISIT.2014.6874830
   Rangan S, 2012, IEEE T INFORM THEORY, V58, P1902, DOI 10.1109/TIT.2011.2177575
   Reeves G., 2016, P IEEE ISIT
   Seeger M. W., 2011, INT C ART INT STAT, P652
   Seeger MW, 2008, J MACH LEARN RES, V9, P759
   Takeuchi K., 2017, INT EL DEVICES MEET
   Tulino AM, 2013, IEEE T INFORM THEORY, V59, P4243, DOI 10.1109/TIT.2013.2250578
   van den Berg E, 2008, SIAM J SCI COMPUT, V31, P890, DOI 10.1137/080714488
   Vila J, 2015, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2015.7178325
   Vila JP, 2014, IEEE T SIGNAL PROCES, V62, P4689, DOI 10.1109/TSP.2014.2337841
   Vila JP, 2013, IEEE T SIGNAL PROCES, V61, P4658, DOI 10.1109/TSP.2013.2272287
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402058
DA 2019-06-15
ER

PT S
AU Foster, DJ
   Kale, S
   Mohri, M
   Sridharan, K
AF Foster, Dylan J.
   Kale, Satyen
   Mohri, Mehryar
   Sridharan, Karthik
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Parameter-Free Online Learning via Model Selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INEQUALITIES; CONVERGENCE; ALGORITHMS
AB We introduce an efficient algorithmic framework for model selection in online learning, also known as parameter-free online learning. Departing from previous work, which has focused on highly structured function classes such as nested balls in Hilbert space, we propose a generic meta-algorithm framework that achieves online model selection oracle inequalities under minimal structural assumptions. We give the first computationally efficient parameter-free algorithms that work in arbitrary Banach spaces under mild smoothness assumptions; previous results applied only to Hilbert spaces. We further derive new oracle inequalities for matrix classes, non-nested convex sets, and R-d with generic regularizers. Finally, we generalize these results by providing oracle inequalities for arbitrary non-linear classes in the online supervised learning model. These results are all derived through a unified meta-algorithm scheme using a novel "multi-scale" algorithm for prediction with expert advice based on random playout, which may be of independent interest.
C1 [Foster, Dylan J.; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA.
   [Kale, Satyen; Mohri, Mehryar] Google Res, Mountain View, CA USA.
   [Mohri, Mehryar] NYU, New York, NY 10003 USA.
RP Foster, DJ (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
RI Jeong, Yongwook/N-7413-2016
FU NDSEG fellowship
FX We thank Francesco Orabona and David Pal for inspiring initial
   discussions. Part of this work was done while DF was an intern at Google
   Research and while DF and KS were visiting the Simons Institute for the
   Theory of Computing. DF is supported by the NDSEG fellowship.
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   BALL K, 1994, INVENT MATH, V115, P463, DOI 10.1007/BF01231769
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812
   Ben-David Shai, 2009, P 22 ANN C LEARN THE
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bubeck Sebastien, 2017, 18 ACM C EC COMP EC
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P297
   Cutkosky A., 2016, ADV NEURAL INFORM PR, V29, P748
   Cutkosky Ashok, 2017, 30 ANN C LEARN THEOR
   de Rooij S, 2014, J MACH LEARN RES, V15, P1281
   Devroye L., 1996, PROBABILISTIC THEORY
   Foster D. J., 2015, ADV NEURAL INFORM PR, V28, P3375
   Hazan E., 2014, P 27 C LEARN THEOR, P197
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan E, 2017, SIAM J COMPUT, V46, P744, DOI 10.1137/120895731
   Kakade S. M., 2009, ADV NEURAL INFORM PR, P793
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926
   Koolen W. M., 2015, COLT, V40, P1155
   Luo Haipeng, 2015, P 28 C LEARN THEOR, P1286
   Massart Pascal, 1896, LECT NOTES MATH
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724
   McMahan H. B., 2014, P 27 C LEARN THEOR C, P1020
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nie JZ, 2013, LECT NOTES ARTIF INT, V8139, P98
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Orabona Francesco, 2016, ARXIV160204128
   Pisier G, 2011, MARTINGALES BANACH S
   Rakhlin Alexander, 2010, ADV NEURAL INF PROCE, P1984
   Rakhlin S., 2012, ADV NEURAL INFORM PR, V25, P2150
   RENEGAR J, 1988, MATH PROGRAM, V40, P59, DOI 10.1007/BF01580724
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Srebro  N., 2011, ADV NEURAL INFORM PR, P2645
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
   Vapnik V. N., 1982, ESTIMATION DEPENDENC, V40
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406010
DA 2019-06-15
ER

PT S
AU Fout, A
   Byrd, J
   Shariat, B
   Ben-Hur, A
AF Fout, Alex
   Byrd, Jonathon
   Shariat, Basir
   Ben-Hur, Asa
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Protein Interface Prediction using Graph Convolutional Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task.
C1 [Fout, Alex; Byrd, Jonathon; Shariat, Basir; Ben-Hur, Asa] Colorado State Univ, Dept Comp Sci, Ft Collins, CO 80525 USA.
RP Fout, A (reprint author), Colorado State Univ, Dept Comp Sci, Ft Collins, CO 80525 USA.
EM fout@colostate.edu; jonbyrd@colostate.edu; basir@cs.colostate.edu;
   asa@cs.colostate.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [DBI-1564840]
FX This work was supported by the National Science Foundation under grant
   no DBI-1564840.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Ahmad S, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029104
   Angermueller C, 2016, MOL SYST BIOL, V12, DOI 10.15252/msb.20156651
   Atwood J., 2016, ADV NEURAL INFORM PR, V29, P1993
   Aumentado-Armstrong TT, 2015, ALGORITHM MOL BIOL, V10, DOI 10.1186/s13015-015-0033-9
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Bronstein Michael M, 2017, IEEE SIG P MAGAZINE
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Esmaielbeiki R., 2015, BRIEF BIOINFORM, P1
   He K., 2015, CORR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Kipf T. N., 2017, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Minhas FUA, 2014, PROTEINS, V82, P1142, DOI 10.1002/prot.24479
   Niepert Mathias, 2016, P 33 ANN INT C MACH
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sael L, 2008, PROTEINS, V72, P1259, DOI 10.1002/prot.22030
   Sael L, 2010, CH CRC DATA MIN KNOW, P89
   Schlichtkrull  M., 2017, ARXIV170306103
   Schrodinger LLC, 2015, PYMOL MOL GRAPH SYST
   Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Vreven T, 2015, J MOL BIOL, V427, P3031, DOI 10.1016/j.jmb.2015.07.016
NR 25
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406058
DA 2019-06-15
ER

PT S
AU Fraccaro, M
   Kamronn, S
   Paquet, U
   Winther, O
AF Fraccaro, Marco
   Kamronn, Simon
   Paquet, Ulrich
   Winther, Ole
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.
C1 [Fraccaro, Marco; Kamronn, Simon; Winther, Ole] Tech Univ Denmark, Lyngby, Denmark.
   [Paquet, Ulrich] DeepMind, London, England.
RP Fraccaro, M (reprint author), Tech Univ Denmark, Lyngby, Denmark.
RI Jeong, Yongwook/N-7413-2016
FU Microsoft Research
FX We would like to thank Lars Kai Hansen for helpful discussions on the
   model design. Marco Fraccaro is supported by Microsoft Research through
   its PhD Scholarship Programme. We thank NVIDIA Corporation for the
   donation of TITAN X GPUs.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Archer E., 2015, ARXIV151107367
   Battaglia Peter W., 2016, NIPS
   Chang Michael B, 2017, ICLR
   Chiappa S., 2017, ICLR
   Chung J, 2015, NIPS
   Finn Chelsea, 2016, NIPS
   Fraccaro Marco, 2016, NIPS
   Fragkiadaki Katerina, 2016, ICLR
   Gao Y., 2016, NIPS
   Haarnoja T., 2016, NIPS
   Higgins I., 2017, BETA VAE LEARNING BA
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jang Eric, 2016, ARXIV161101144
   Johnson M. J., 2016, NIPS
   Karl Maximilian, 2017, ICLR
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma Diederik P, 2014, ICLR
   Krishnan Rahul G, 2017, AAAI
   Linderman S., 2017, AISTATS
   Maddison C. J., 2017, ICLR
   Murphy K.P., 1998, TECHNICAL REPORT
   Oh  J., 2015, NIPS
   Patraucean V, 2015, ARXIV151106309
   Rezende D. J., 2014, ICML
   Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674
   Seeger M. W., 2016, NIPS
   Shi Wenzhe, 2016, CVPR
   Srivastava N, 2015, ICML
   Sun Wen, 2016, ICML
   Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3
   Wahlstrom N., 2015, ARXIV150202251
   Watter M, 2015, NIPS
   Wu  Jiajun, 2015, NIPS, P2
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403065
DA 2019-06-15
ER

PT S
AU Fruit, R
   Pirotta, M
   Lazaric, A
   Brunskill, E
AF Fruit, Ronan
   Pirotta, Matteo
   Lazaric, Alessandro
   Brunskill, Emma
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Regret Minimization in MDPs with Options without Prior Knowledge
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MARKOV; BOUNDS
AB The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAx - SMDP and UCRL - SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAx-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while the regret analysis of UCRL SMDP requires prior knowledge of the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical results supporting the theoretical findings.
C1 [Fruit, Ronan; Pirotta, Matteo; Lazaric, Alessandro] Inria Lille, Sequel Team, Lille, France.
   [Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA.
RP Fruit, R (reprint author), Inria Lille, Sequel Team, Lille, France.
EM ronan.fruit@inria.fr; matteo.pirotta@inria.fr;
   alessandro.lazaric@inria.fr; ebrun@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU French Ministry of Higher Education and Research; French National
   Research Agency (ANR) [ANR-14-CE24-0010-01]; Nord-Pas-de-Calais Regional
   Council
FX This research was supported in part by French Ministry of Higher
   Education and Research, Nord-Pas-de-Calais Regional Council and French
   National Research Agency (ANR) under project ExTra-Learn (n.
   ANR-14-CE24-0010-01).
CR Bartlett Peter L, 2009, P 25 C UNC ART INT U, P35
   Bremaud Pierre, 1999, APPL PROBABILITY MOD
   Bremaud Pierre, 1999, APPL PROBABILITY MOD
   Brunskill E., 2014, P 31 INT C MACH LEAR, P316
   Castro Pablo Samuel, 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P140, DOI 10.1007/978-3-642-29946-9_16
   Cho GE, 2001, LINEAR ALGEBRA APPL, V335, P137, DOI 10.1016/S0024-3795(01)00320-2
   Dann C, 2015, P 28 INT C NEUR INF, V28, P2818
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   FEDERGRUEN A, 1983, MATH OPER RES, V8, P298, DOI 10.1287/moor.8.2.298
   Fruit Ronan, 2017, P MACH LEARN RES ART, V54, P576
   Hsu D. J., 2015, ADV NEURAL INFORM PR, V28, P1459
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Jong Nicholas K., 2008, 7 INT JOINT C AUT AG
   Kirkland SJ, 2008, NUMER MATH, V110, P521, DOI 10.1007/s00211-008-0172-8
   Levy Kfir Y, 2011, LECT NOTES COMPUTER, P153
   Mann  T., 2014, P 31 INT C MACH LEAR, P127
   Mann Timothy Arthur, 2014, P 31 INT C MACH LEAR, V32, P1350
   McGovern A., 2001, P 18 INT C MACH LEAR, P361
   Menache I, 2002, LECT NOTES ARTIF INT, V2430, P295
   Ortner R, 2008, MIND MACH, V18, P521, DOI 10.1007/s11023-008-9115-5
   Paulin D, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-4039
   Puterman M. L., 1994, MARKOV DECISION PROC
   Sairamesh Munu, 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P165, DOI 10.1007/978-3-642-29946-9_18
   SENETA E, 1993, STAT PROBABIL LETT, V17, P163, DOI 10.1016/0167-7152(93)90011-7
   Simsek Ozgur, 2004, P 21 INT C MACH LEAR
   Stolle M., 2002, Abstraction, Reformulation, and Approximation. 5th International Symposium, SARA 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2371), P212
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Tessler C, 2017, AAAI, P1553
   Wainwright Martin, 2015, COURSE MATH STAT
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403023
DA 2019-06-15
ER

PT S
AU Fu, J
   Co-Reyes, JD
   Levine, S
AF Fu, Justin
   Co-Reyes, John D.
   Levine, Sergey
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI EX2: Exploration with Exemplar Models for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.
C1 [Fu, Justin; Co-Reyes, John D.; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Fu, J (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM justinfu@eecs.berkeley.edu; jcoreyes@eecs.berkeley.edu;
   sylevine@eecs.berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1700696, IIS-1614653]; ONR Young Investigator Program award;
   Berkeley DeepDrive
FX We would like to thank Adam Stooke, Sandy Huang, and Haoran Tang for
   providing efficient and parallelizable policy search code. We thank
   Joshua Achiam for help with setting up benchmark tasks. This research
   was supported by NSF IIS-1614653, NSF IIS-1700696, an ONR Young
   Investigator Program award, and Berkeley DeepDrive.
CR Abel David, 2016, ADV NEURAL INFORM PR
   Achiam Joshua, 2017, CORR
   Barto Andrew G., 2003, DISCRETE EVENT DYNAM, V13
   Bellemare M., 2016, ADV NEURAL INFORM PR
   Brafman R. I., 2002, J MACHINE LEARNING R
   Bubeck Sebastien, 2012, FDN TRENDS MACHINE L, V5
   Chapelle O., 2011, ADV NEURAL INFORM PR
   Chentanez Nuttapong, 2005, ADV NEURAL INFORM PR
   Duan Yan, 2016, INT C MACH LEARN ICM
   Florensa Carlos Campo, 2017, INT C LEARN REPR ICL
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Heess N., 2016, CORR
   Houthooft Rein, 2016, ADV NEURAL INFORM PR
   Kakade Sham, 2003, INT C MACH LEARN ICM
   Kearns M. J., 2002, MACHINE LEARNING
   Kolter J. Zico, 2009, INT C MACH LEARN ICM
   Kulkarni Tejas D, 2016, ADV NEURAL INFORM PR
   Lillicrap Timothy P., 2015, INT C LEARN REPR ICL
   Malisiewicz Tomasz, 2011, INT C COMP VIS ICCV
   Mathieu M., 2015, CORR
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Oh J., 2015, ADV NEURAL INFORM PR
   Osband I., 2016, ADV NEURAL INFORM PR
   Pathak Deepak, 2017, INT C MACH LEARN ICM
   Pazis Jason, 2013, AAAI C ART INT AAAI
   Salimans  T., 2016, ADV NEURAL INFORM PR
   Schmidhuber Jurgen, 1990, P 1 INT C SIM AD BEH
   Schulman J., 2015, INT C MACH LEARN ICM
   Stadie B. C., 2015, CORR
   Stolle Martin, 2002, LEARNING OPTIONS REI, DOI [10.1007/3-540-45622-8_16, DOI 10.1007/3-540-45622-8_16]
   Strehl Alexander L., 2009, J COMPUTER SYSTEM SC
   Tang Haoran, 2017, ADV NEURAL INFORM PR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402061
DA 2019-06-15
ER

PT S
AU Futami, F
   Sato, I
   Sugiyama, M
AF Futami, Futoshi
   Sato, Issei
   Sugiyama, Masashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Expectation Propagation for t-Exponential Family Using q-Algebra
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Exponential family distributions are highly useful in machine learning since their calculation can be performed efficiently through natural parameters. The exponential family has recently been extended to the t-exponential family, which contains Student-t distributions as family members and thus allows us to handle noisy data well. However, since the t-exponential family is defined by the deformed exponential, an efficient learning algorithm for the t-exponential family such as expectation propagation (EP) cannot be derived in the same way as the ordinary exponential family. In this paper, we borrow the mathematical tools of q-algebra from statistical physics and show that the pseudo additivity of distributions allows us to perform calculation of t-exponential family distributions through natural parameters. We then develop an expectation propagation (EP) algorithm for the t-exponential family, which provides a deterministic approximation to the posterior or predictive distribution with simple moment matching. We finally apply the proposed EP algorithm to the Bayes point machine and Student-t process classification, and demonstrate their performance numerically.
C1 [Futami, Futoshi; Sato, Issei; Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan.
RP Futami, F (reprint author), Univ Tokyo, RIKEN, Tokyo, Japan.
EM futami@ms.k.u-tokyo.ac.jp; sato@k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU JST CREST [JPMJCR1403]; KAKENHI [17H00757]
FX FF acknowledges support by JST CREST JPMJCR1403 and MS acknowledges
   support by KAKENHI 17H00757.
CR Bishop C. M., 2006, PATTERN RECOGNITION
   Ding N., 2011, ADV NEURAL INF PROCE, P1494
   Ding N., 2010, ADV NEURAL INFORM PR, V23, P514
   Jylanki P, 2011, J MACH LEARN RES, V12, P3227
   Kim HC, 2008, LECT NOTES COMPUT SC, V5342, P896
   Minka T. P., 2001, THESIS
   Nivanen L, 2003, REP MATH PHYS, V52, P437, DOI 10.1016/S0034-4877(03)80040-X
   Seeger M., 2005, TECHNICAL REPORT
   Shah A., 2014, JMLR W CP, P877
   Suyari H, 2005, IEEE T INFORM THEORY, V51, P753, DOI 10.1109/TIT.2004.840862
   Williams C. K., 2006, GAUSSIAN PROCESSES M, V1
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402029
DA 2019-06-15
ER

PT S
AU Gal, Y
   Hron, J
   Kendall, A
AF Gal, Yarin
   Hron, Jiri
   Kendall, Alex
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Concrete Dropout
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary- a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.
C1 [Gal, Yarin; Hron, Jiri; Kendall, Alex] Univ Cambridge, Cambridge, England.
   [Gal, Yarin] Alan Turing Inst, London, England.
RP Gal, Y (reprint author), Univ Cambridge, Cambridge, England.; Gal, Y (reprint author), Alan Turing Inst, London, England.
EM yarin.gal@eng.cam.ac.uk; jh2084@cam.acuk; agk34@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Amodei Dario, 2016, ARXIV160606565
   [Anonymous], 2015, FRANCOIS CHOLLET
   Ba  J., 2013, ADV NEURAL INFORM PR, P3084
   Beal M., 2003, BAYESIAN STAT
   Bui T., 2016, P 33 INT C INT C MAC, P1472
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   Gal Y., 2016, THESIS
   Gal Y., 2016, NIPS
   Gal  Yarin, 2016, ICML
   Gal  Yarin, 2016, DAT EFF MACH LEARN W
   Glasserman P., 2013, MONTE CARLO METHODS, V53
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Hernandez-Lobato J. M., 2015, ICML
   Hinton G. E, 2012, ARXIV12070580
   Huang  Gao, 2016, ARXIV160806993
   Jang Eric, 2016, BAYES DEEP LEARN WOR
   Jegou S., 2016, ARXIV161109326
   Kahn Gregory, 2017, 170201182 ARXIV
   Kampffmeyer M., 2016, IEEE C COMP VIS PATT
   Kendall A., 2015, ARXIV151102680
   Kendall Alex, 2017, 170304977 ARXIV
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, NIPS
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li Yingzhen, 2017, 170302914 ARXIV
   Lichman M., 2013, UCI MACHINE LEARNING
   Maddison Chris J., 2016, BAYES DEEP LEARN WOR
   Molchanov Dmitry, 2016, BAYES DEEP LEARN WOR
   Paisley J. W., 2012, ICML
   Rezende D. J., 2014, ICML
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   US Department of Transportation (NHTSA), 2017, TECHNICAL REPORT
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403063
DA 2019-06-15
ER

PT S
AU Gan, Z
   Chen, LQ
   Wang, WY
   Pu, YC
   Zhang, YZ
   Liu, H
   Li, CY
   Carin, L
AF Gan, Zhe
   Chen, Liqun
   Wang, Weiyao
   Pu, Yunchen
   Zhang, Yizhe
   Liu, Hao
   Li, Chunyuan
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Triangle Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A Triangle Generative Adversarial Network (Delta-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. Delta-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.
C1 [Gan, Zhe; Chen, Liqun; Wang, Weiyao; Pu, Yunchen; Zhang, Yizhe; Liu, Hao; Li, Chunyuan; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA.
RP Gan, Z (reprint author), Duke Univ, Durham, NC 27706 USA.
EM zhe.gan@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO; DARPA; DOE; NGA; ONR
FX This research was supported in part by ARO, DARPA, DOE, NGA and ONR.
CR Arjovsky M, 2017, ARXIV170107875
   Arjovsky Martin, 2017, ICLR
   Denton E. L., 2015, NIPS
   Donahue J., 2017, ICLR
   Dumoulin V., 2017, ICLR
   Gan Zhe, 2017, CVPR
   Goodfellow I., 2014, NIPS
   Isola  Phillip, 2017, CVPR
   Kim Taeksoo, 2017, ICML
   Krizhevsky Alex, 2009, CITESEER
   Ledig C., 2017, CVPR
   LI  Chongxuan, 2017, NIPS
   Lin T.-Y., 2014, ECCV
   Liu M. -Y., 2017, NIPS
   Liu Ming-Yu, 2016, NIPS
   Liu  Z., 2015, ICCV
   Metz L, 2017, ICLR
   Mirza M., 2014, ARXIV14111784
   Perarnau G, 2016, ARXIV161106355
   Pu Y., 2016, NIPS
   Pu Y., 2017, NIPS
   Radford A., 2016, ICLR
   Reed S., 2016, ICML
   Salimans T., 2016, NIPS
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Taigman  Y., 2017, ICLR
   Xia Yingce, 2017, ICML
   Yann LeCun, 1998, P IEEE
   Yi Z, 2017, ICCV
   Yu Lantao, 2017, AAAI
   Zhang Han, 2017, ICCV
   Zhang Y., 2016, NIPS WORKSH ADV TRAI
   Zhang Y., 2017, ICML
   Zhao J., 2017, ICLR
   Zhu J.-Y., 2017, ICCV
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405032
DA 2019-06-15
ER

PT S
AU Gao, WH
   Kannan, S
   Oh, S
   Viswanath, P
AF Gao, Weihao
   Kannan, Sreeram
   Oh, Sewoong
   Viswanath, Pramod
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Estimating Mutual Information for Discrete-Continuous Mixtures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DIVERGENCE ESTIMATION; FEATURE-SELECTION; ENTROPY; DISTRIBUTIONS;
   FUNCTIONALS
AB Estimation of mutual information from observed samples is a basic primitive in machine learning, useful in several learning tasks including correlation mining, information bottleneck, Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a quantity well-defined for general probability spaces, estimators have been developed only in the special case of discrete or continuous pairs of random variables. Most of these estimators operate using the 3H-principle, i.e., by calculating the three (differential) entropies of X, Y and the pair (X, Y). However, in general mixture spaces, such individual entropies are not well defined, even though mutual information is. In this paper, we develop a novel estimator for estimating mutual information in discrete-continuous mixtures. We prove the consistency of this estimator theoretically as well as demonstrate its excellent empirical performance. This problem is relevant in a wide-array of applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.
C1 [Gao, Weihao; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Dept ECE, Urbana, IL 61801 USA.
   [Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Oh, Sewoong] Univ Illinois, Coordinated Sci Lab, Dept IESE, Urbana, IL 61801 USA.
RP Gao, WH (reprint author), Univ Illinois, Coordinated Sci Lab, Dept ECE, Urbana, IL 61801 USA.
EM wgao9@illinois.edu; ksreeram@uw.edu; swoh@illinois.edu;
   pramodv@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CNS-1527754, CCF-1553452, CCF-1705007, CCF-1651236, CCF-1617745,
   CNS-1718270]; GOOGLE Faculty Research Award
FX This work was partially supported by NSF grants CNS-1527754,
   CCF-1553452, CCF-1705007, CCF-1651236, CCF-1617745, CNS-1718270 and
   GOOGLE Faculty Research Award.
CR Acharya Jayadev, MAXIMUM LIKELIHOOD A
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Beirlant J., 1997, INT J MATH STAT SCI, V6, P17
   Berrett Thomas B, 2016, ARXIV160600304
   Biau G., 2015, LECT NEAREST NEIGHBO
   Bishop C. M., 2006, MACH LEARN, V128, P1
   Bu YH, 2016, IEEE INT SYMP INFO, P1118, DOI 10.1109/ISIT.2016.7541473
   Chan C, 2015, P IEEE, V103, P1883, DOI 10.1109/JPROC.2015.2458316
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Cover T. M., 1991, ELEMENTS INFORM THEO, P279
   Darbellay GA, 1999, IEEE T INFORM THEORY, V45, P1315, DOI 10.1109/18.761290
   Finak G, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0844-5
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Gao S, 2015, ARTIFICIAL INTEL STA, P277
   Gao S., 2015, ARXIV150800536
   Gao W., 2016, ADV NEURAL INFORM PR, P2460
   Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732
   Gelfand I., 1959, CALCULATION AMOUNT I
   Han YJ, 2015, IEEE T INFORM THEORY, V61, P6343, DOI 10.1109/TIT.2015.2478816
   Han YJ, 2015, IEEE INT SYMP INFO, P1372, DOI 10.1109/ISIT.2015.7282680
   Jiao JT, 2017, IEEE T INFORM THEORY, V63, P6774, DOI 10.1109/TIT.2017.2733537
   Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]
   Jiao Jiantao, 2014, NONASYMPTOTIC THEORY
   Kharchenko PV, 2014, NAT METHODS, V11, P740, DOI [10.1038/NMETH.2967, 10.1038/nmeth.2967]
   Kozachenko L. F., 1987, PROBL PEREDACHI INF, V23, P9
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689
   Li Pan, 2017, ARXIV170901249
   Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/nmeth.2016, 10.1038/NMETH.2016]
   Moon Kevin R, 2017, ARXIV170108083
   Muller A. C., 2012, INFORM THEORETIC CLU
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Paninski L, 2008, IEEE T INFORM THEORY, V54, P4384, DOI 10.1109/TIT.2008.928251
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Perez A, 1959, THEORY PROBABILITY I, V4
   Pierson E, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0805-z
   PINSKER M. S., 1960, INFORM INFORM STABIL
   Polyanskiy Y, 2017, IMA VOL MATH APPL, V161, P211, DOI 10.1007/978-1-4939-7005-6_7
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Rieke F, 1999, SPIKES EXPLORING NEU
   Ross BC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087357
   Singh S., 2014, ADV NEURAL INFORM PR, P3032
   Singh S., 2016, ADV NEURAL INFORM PR, P1217
   Singh Shashank, 2017, ARXIV170207803
   Sricharan K, 2013, IEEE T INFORM THEORY, V59, P4374, DOI 10.1109/TIT.2013.2251456
   Steeg G. Ver, 2014, STAT, V1050, P27
   Szabo Z, 2014, J MACH LEARN RES, V15, P283
   Valiant G, 2011, ACM S THEORY COMPUT, P685
   Wang Q, 2005, IEEE T INFORM THEORY, V51, P3064, DOI 10.1109/TIT.2005.853314
   Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060
   Wu AR, 2014, NAT METHODS, V11, P41, DOI 10.1038/nmeth.2694
   Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406007
DA 2019-06-15
ER

PT S
AU Garber, D
AF Garber, Dan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Online Linear Optimization with Approximation Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We revisit the problem of online linear optimization in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor alpha multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied offline linear optimization problems which are NP-hard, yet admit efficient approximation algorithms. The goal here is to minimize the alpha-regret which is the natural extension of the standard regret in online learning to this setting. We present new algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly, for both variants, we present alpha-regret bounds of O(T-1/3), were T is the number of prediction rounds, using only O(log(T)) calls to the approximation oracle per iteration, on average. These are the first results to obtain both average oracle complexity of O(log(T)) (or even poly-logarithmic in T) and alpha-regret bound O(T-c) for a constant c > 0, for both variants.
C1 [Garber, Dan] Technion Israel Inst Technol, Haifa, Israel.
RP Garber, D (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM dangar@technion.ac.il
RI Jeong, Yongwook/N-7413-2016
CR Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Awerbuch  B., 2004, P 36 ANN ACM S THEOR, P45
   Balcan M.-F., 2006, P 7 ACM C EL COMM, P29
   BARYEHUDA R, 1981, J ALGORITHM, V2, P198, DOI 10.1016/0196-6774(81)90020-1
   Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Christofides  N., 1976, WORST CASE ANAL NEW
   Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233
   Fujita T, 2013, LECT NOTES ARTIF INT, V8139, P68
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273
   Hazan E., 2016, P 33 INT C MACH LEAR, V48, P1263
   Hazan Elad, 2014, J MACHINE LEARNING R, V35, P408
   Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Papadimitriou CH, 2008, J ACM, V55, DOI 10.1145/1379759.1379762
   Weinberg S. M., 2014, THESIS
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400060
DA 2019-06-15
ER

PT S
AU Garcia-Duran, A
   Niepert, M
AF Garcia-Duran, Alberto
   Niepert, Mathias
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Graph Representations with Embedding Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LINK-PREDICTION
AB We propose Embedding Propagation (EP), an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.
C1 [Garcia-Duran, Alberto; Niepert, Mathias] NEC Labs Europe, Heidelberg, Germany.
RP Garcia-Duran, A (reprint author), NEC Labs Europe, Heidelberg, Germany.
EM alberto.duran@neclab.eu; mathias.niepert@neclab.eu
RI Jeong, Yongwook/N-7413-2016
CR Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y
   Almeida Luis B, 1990, ARTIFICIAL NEURAL NE, P102
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Breitkreutz BJ, 2008, NUCLEIC ACIDS RES, V36, pD637, DOI 10.1093/nar/gkm1001
   Breuleux Olivier, 2010, P 9 PYTH SCI C, P1
   Bromley J., 1994, NEURAL INFORM PROCES
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   De Raedt Luc, 2016, SYNTHESIS LECT ARTIF, V10, P1, DOI DOI 10.2200/S00692ED1V01Y201601AIM032
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Getoor L, 2007, INTRO STAT RELATIONA
   Gilmer J., 2017, ARXIV170401212
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Grover Aditya, 2016, KDD, V2016, P855
   Kersting K., 2014, 28 AAAI C ART INT
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kipf T. N., 2016, ARXIV160902907
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kruskal J. B, 1978, MULTIDIMENSIONAL SCA
   Li Y., 2015, ARXIV151105493
   Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591
   Low Y., 2010, C UNC ART INT UAI JU
   Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   Macskassy S. A., 2003, TECHNICAL REPORT DTI
   Mahoney M, 2009, LARGE TEXT COMPRESSI
   Malewicz G., 2010, P 2010 ACM SIGMOD IN, P135, DOI DOI 10.1145/1807167.1807184
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Ngiam J, 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.1145/2647868
   Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592
   Page L, 1999, PAGERANK CITATION RA
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   Tang J, 2015, P 24 INT C WORLD WID, P1067, DOI DOI 10.1145/2736277.2741093
   Tang L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P817
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Xin R. S., 2013, 1 INT WORKSH GRAPH D, DOI DOI 10.1145/2484425.2484427
   Yang  Z., 2016, P 33 INT C MACH LEAR, V48, P40
   Zafarani R., 2009, SOCIAL COMPUTING DAT
   Zhu  X., 2002, CMUCALD02107
NR 46
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405020
DA 2019-06-15
ER

PT S
AU Garg, VK
   Jaakkola, T
AF Garg, Vikas K.
   Jaakkola, Tommi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Local Aggregative Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Aggregative games provide a rich abstraction to model strategic multi-agent interactions. We introduce local aggregative games, where the payoff of each player is a function of its own action and the aggregate behavior of its neighbors in a connected digraph. We show the existence of a pure strategy epsilon-Nash equilibrium in such games when the payoff functions are convex or sub-modular. We prove an information theoretic lower bound, in a value oracle model, on approximating the structure of the digraph with non-negative monotone sub-modular cost functions on the edge set cardinality. We also define a new notion of structural stability, and introduce gamma-aggregative games that generalize local aggregative games and admit epsilon-Nash equilibrium that is stable with respect to small changes in some specified graph property. Moreover, we provide algorithms for our models that can meaningfully estimate the game structure and the parameters of the aggregator function from real voting data.
C1 [Garg, Vikas K.; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Garg, VK (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM vgarg@csail.mit.edu; tommi@csail.mit.edu
CR Azrieli Y, 2013, MATH OPER RES, V38, P350, DOI 10.1287/moor.1120.0557
   Bach F., 2010, NIPS
   Balcan M., 2011, STOC
   Bonami P., 2012, IMA VOLUMES MATH ITS, V154
   Bonami P, 2008, DISCRETE OPTIM, V5, P186, DOI 10.1016/j.disopt.2006.10.011
   Bowers J., 2014, NY TIMES
   Boyd S., 2011, FDN TRENDS MACHINE L, V3
   Cornes R, 2012, ECON LETT, V116, P631, DOI 10.1016/j.econlet.2012.06.024
   Cummings R., 2015, WINE
   Daskalakis C, 2015, J ECON THEORY, V156, P207, DOI 10.1016/j.jet.2014.02.002
   Defazio  A., 2012, NIPS
   Feige U., 2007, FOCS
   Feller W., 1957, INTRO PROBABILITY TH, V1
   Garg V. K., 2016, NIPS
   GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098
   Goel G., 2009, FOCS
   Goemans M. X., 2009, SODA
   Honorio J, 2015, J MACH LEARN RES, V16, P1157
   Jensen MK, 2010, ECON THEOR, V43, P45, DOI 10.1007/s00199-008-0419-8
   Kalai E, 2004, ECONOMETRICA, V72, P1631, DOI 10.1111/j.1468-0262.2004.00549.x
   Kearns M., 2002, UAI
   Lafferty J. O., 2001, ICML
   Lasry JM, 2007, JAP J MATH, V2, P229, DOI 10.1007/s11537-007-0657-8
   NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529
   NOVSHEK W, 1985, REV ECON STUD, V52, P85, DOI 10.2307/2297471
   Selten R., 1970, PREISPOLITIK MEHRPRO
   Svitkina Z., 2008, FOCS
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Yao A., 1977, FOCS
   Zaheer M., 2017, ARXIV170306114
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405041
DA 2019-06-15
ER

PT S
AU Garimella, K
   Gionis, A
   Parotsidis, N
   Tatti, N
AF Garimella, Kiran
   Gionis, Aristides
   Parotsidis, Nikos
   Tatti, Nikolaj
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Balancing information exposure in social networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPETITION; DIFFUSION
AB Social media has brought a revolution on how people are consuming news. Beyond the undoubtedly large number of advantages brought by social-media platforms, a point of criticism has been the creation of echo chambers and filter bubbles, caused by social homophily and algorithmic personalization.
   In this paper we address the problem of balancing the information exposure in a social network. We assume that two opposing campaigns (or viewpoints) are present in the network, and that network nodes have different preferences towards these campaigns. Our goal is to find two sets of nodes to employ in the respective campaigns, so that the overall information exposure for the two campaigns is balanced. We formally define the problem, characterize its hardness, develop approximation algorithms, and present experimental evaluation results.
   Our model is inspired by the literature on influence maximization, but there are significant differences from the standard model. First, balance of information exposure is modeled by a symmetric difference function, which is neither monotone nor submodular, and thus, not amenable to existing approaches. Second, while previous papers consider a setting with selfish agents and provide bounds on best-response strategies (i.e., move of the last player), we consider a setting with a centralized agent and provide bounds for a global objective function.
C1 [Garimella, Kiran; Gionis, Aristides; Tatti, Nikolaj] Aalto Univ, Helsinki, Finland.
   [Garimella, Kiran; Gionis, Aristides; Tatti, Nikolaj] Aalto Univ, Helsinki, Finland.
   [Parotsidis, Nikos] Univ Roma Tor Vergata, Rome, Italy.
RP Garimella, K (reprint author), Aalto Univ, Helsinki, Finland.; Garimella, K (reprint author), Aalto Univ, Helsinki, Finland.
EM kiran.garimella@aalto.fi; aristides.gionis@aalto.fi;
   nikos.parotsidis@uniroma2.it; nikolaj.tatti@aalto.fi
RI Gionis, Aristides/G-2225-2013; Jeong, Yongwook/N-7413-2016
OI Gionis, Aristides/0000-0002-5211-112X; Tatti,
   Nikolaj/0000-0002-2087-5360
FU Academy of Finland [286211, 313927]; EC H2020 RIA project "SoBigData"
   [654024]
FX This work has been supported by the Academy of Finland projects "Nestor"
   (286211) and "Agra" (313927), and the EC H2020 RIA project "SoBigData"
   (654024).
CR Adamic L. A., 2005, P 3 INT WORKSH LINK, P36, DOI DOI 10.1145/1134271.1134277
   Alon N, 2010, INFORM PROCESS LETT, V110, P221, DOI 10.1016/j.ipl.2009.12.009
   Bharathi S., 2007, WINE
   Borodin A., 2013, WWW, P141
   Borodin A., 2010, WINE
   Budak C, 2011, P 20 INT C WORLD WID, P665, DOI DOI 10.1145/1963405.1963499
   Carnes T., 2007, EC
   Chen W., 2010, P 16 ACM SIGKDD INT, P1029, DOI DOI 10.1145/1835804.1835934
   Conover Michael, 2011, ICWSM
   Dubey P., 2006, WINE
   Farajtabar M., 2016, ADV NEURAL INFORM PR, P4718
   Fu Wai-Tat, 2014, P 32 ANN ACM C HUM F, P2745
   Garimella K., 2017, WSDM
   Garimella K, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P33, DOI 10.1145/1235
   Garrett RK, 2009, J COMPUT-MEDIAT COMM, V14, P265, DOI 10.1111/j.1083-6101.2009.01440.x
   Gottfried Jeffrey, 2016, NEWS USE SOCIAL MEDI
   Goyal Sanjeev, 2014, GAMES EC BEHAV
   Jie RL, 2016, PHYSICA A, V454, P129, DOI 10.1016/j.physa.2016.02.048
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Lu  H., 2015, P 24 ACM INT C INF K, P213
   Lu W, 2015, PROC VLDB ENDOW, V9, P60
   Morales AJ, 2015, CHAOS, V25, DOI 10.1063/1.4913758
   Myers SA, 2012, IEEE DATA MINING, P539, DOI 10.1109/ICDM.2012.159
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Pariser E., 2011, FILTER BUBBLE WHAT I
   Tzoumas V., 2012, WINE
   Valera I., 2015, ICDM
   Zarezade Ali, 2017, P AAAI, P238
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404071
DA 2019-06-15
ER

PT S
AU Ge, R
   Ma, TY
AF Ge, Rong
   Ma, Tengyu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On the Optimization Landscape of Tensor Decompositions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPLEXITY
AB Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult.
   In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant epsilon > 0, among the set of points with function values (1 + epsilon)-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem.
   Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local optima of a highly-structured random polynomial with dependent coefficients.
C1 [Ge, Rong] Duke Univ, Durham, NC 27706 USA.
   [Ma, Tengyu] Facebook AI Res, Menlo Pk, CA USA.
RP Ge, R (reprint author), Duke Univ, Durham, NC 27706 USA.
EM rongge@cs.duke.edu; tengyuma@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Abo H., 2015, ARXIV E PRINTS
   Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Adler R. J., 2009, RANDOM FIELDS GEOMET
   Anandkumar A., 2012, COLT
   Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25
   Anandkumar Anima, 2016, JMLR
   Anandkumar Animashree, 2015, P C LEARN THEOR COLT
   Arora Sanjeev, 2015, P 28 C LEARN THEOR
   Auffinger A, 2013, ANN PROBAB, V41, P4214, DOI 10.1214/13-AOP862
   Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422
   Bandeira AS, 2016, ARXIV160204426
   Barak B., 2015, P 47 ANN ACM S THEOR, P143, DOI DOI 10.1145/2746539.2746605
   Bhaskara A., 2014, P 46 ANN ACM S THEOR, P594
   Bhojanapalli Srinadh, 2016, ARXIV160507221
   Boumal N., 2016, ARXIV E PRINTS
   Cartwright D, 2013, LINEAR ALGEBRA APPL, V438, P942, DOI 10.1016/j.laa.2011.05.040
   Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2
   Choromanska Anna, 2015, AISTATS
   Cohen Nadav, 2016, CORR
   Comon P, 2009, J CHEMOMETR, V23, P393, DOI 10.1002/cem.1236
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943
   Ge R., 2016, ARXIV160507272
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Ge R., 2017, ARXIV E PRINTS
   Ge Rong, 2015, ARXIV150405287
   Goyal N., 2013, ARXIV13065825
   Hardt Moritz, 2016, CORR
   HASTAD J, 1990, J ALGORITHMS, V11, P644, DOI 10.1016/0196-6774(90)90014-6
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Hopkins SB, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P178, DOI 10.1145/2897518.2897529
   Hsu D., 2013, 4 INNOVATIONS THEORE
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Janzamin M., 2015, ARXIV E PRINTS
   Kawaguchi K., 2016, ARXIV E PRINTS
   Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Ma Tengyu, 2016, FOCS 2016
   Mossel E, 2006, ANN APPL PROBAB, V16, P583, DOI 10.1214/105051606000000024
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Novikov A., 2015, ARXIV E PRINTS
   Sun J., 2015, ARXIV151006096
NR 42
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403070
DA 2019-06-15
ER

PT S
AU Geifman, Y
   El-Yaniv, R
AF Geifman, Yonatan
   El-Yaniv, Ran
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Selective Classification for Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REJECT
AB Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.
C1 [Geifman, Yonatan; El-Yaniv, Ran] Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.
RP Geifman, Y (reprint author), Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.
EM yonatan.g@cs.technion.ac.il; rani@cs.technion.ac.il
RI Jeong, Yongwook/N-7413-2016
FU Israel Science Foundation [1890/14]
FX This research was supported by The Israel Science Foundation (grant No.
   1890/14)
CR Chow C.K., 1957, Institute of Radio Engineers Transactions on Electronic Computers, VEC-6, P247
   CORDELLA LP, 1995, IEEE T NEURAL NETWOR, V6, P1140, DOI 10.1109/72.410358
   Cortes Corinna, 2016, ADV NEURAL INFORM PR, P1660
   De Stefano C, 2000, IEEE T SYST MAN CY C, V30, P84, DOI 10.1109/5326.827457
   El-Yaniv R, 2012, J MACH LEARN RES, V13, P255
   El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605
   Freund Y, 2004, ANN STAT, V32, P1698, DOI 10.1214/009053604000000058
   Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68
   Gal Y., 2016, INT C MACH LEARN, P1050
   GASCUEL O, 1992, PATTERN RECOGN LETT, V13, P757, DOI 10.1016/0167-8655(92)90125-J
   Gelbhart R., 2017, ARXIV E PRINTS
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   HELLMAN ME, 1970, IEEE T SYST SCI CYB, VSSC6, P179, DOI 10.1109/TSSC.1970.300339
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Varshney KR, 2011, 2011 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P769, DOI 10.1109/SSP.2011.5967817
   Zagoruyko S, 2016, ARXIV160507146
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404092
DA 2019-06-15
ER

PT S
AU Geist, M
   Piot, B
   Pietquin, O
AF Geist, Matthieu
   Piot, Bilal
   Pietquin, Olivier
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Is the Bellman residual a bad proxy?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual parallel to T(*)v(pi) - v(pi)parallel to(1,v) over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.
C1 [Geist, Matthieu] Univ Lorraine, F-57070 Metz, France.
   [Geist, Matthieu] CNRS, LIEC, UMR 7360, F-57070 Metz, France.
   [Piot, Bilal; Pietquin, Olivier] Univ Lille, Inria, Centrale Lille, CNRS,UMR 9189 CRIStAL, F-59000 Lille, France.
   [Piot, Bilal; Pietquin, Olivier] Google DeepMind, London, England.
RP Geist, M (reprint author), Univ Lorraine, F-57070 Metz, France.; Geist, M (reprint author), CNRS, LIEC, UMR 7360, F-57070 Metz, France.
EM matthieu.geist@univ-lorraine.fr; bilal.piot@univ-lille1.fr;
   olivier.pietquin@univ-lille1.fr
RI Jeong, Yongwook/N-7413-2016
CR Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2
   ARCHIBALD TW, 1995, J OPER RES SOC, V46, P354, DOI 10.2307/2584329
   BAIRD LC, 1995, INT C MACH LEARN, P30
   Bhatnagar S, 2009, AUTOMATICA, V45, P2471, DOI 10.1016/j.automatica.2009.07.008
   Bradtke SJ, 1996, MACH LEARN, V22, P33, DOI 10.1023/A:1018056104778
   De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Desai VV, 2012, OPER RES, V60, P655, DOI 10.1287/opre.1120.1044
   Ernst D, 2005, J MACH LEARN RES, V6, P503
   Filar Jerzy A, 1991, STOCHASTIC GAMES REL, P59
   Gordon Geoffrey, 1995, INT C MACH LEARN ICM
   Kakade Sham, 2002, INT C MACH LEARN ICM
   Lagoudakis M., 2003, J MACHINE LEARNING R, V4, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107
   Lazaric Alessandro, 2010, INT C MACH LEARN ICM
   Lillicrap T. P., 2016, INT C LEARN REPR ICL
   Maei Hamid R, 2010, INT C MACH LEARN ICM
   Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384
   Perolat Julien, 2016, INT C MACH LEARN ICM
   Piot B., 2014, ADV NEURAL INFORM PR
   Scherrer B, 2014, P 31 INT C MACH LEAR, V32, P1314
   Scherrer Bruno, 2010, INT C MACH LEARN ICM
   Scherrer Bruno, 2014, EUR C MACH LEARN PRI
   Schulman J., 2015, INT C MACH LEARN ICM
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403027
DA 2019-06-15
ER

PT S
AU Geumlek, J
   Song, S
   Chaudhuri, K
AF Geumlek, Joseph
   Song, Shuang
   Chaudhuri, Kamalika
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Renyi Differential Privacy Mechanisms for Posterior Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB With the newly proposed privacy definition of Renyi Differential Privacy (RDP) in [14], we re-examine the inherent privacy of releasing a single sample from a posterior distribution. We exploit the impact of the prior distribution in mitigating the influence of individual data points. In particular, we focus on sampling from an exponential family and specific generalized linear models, such as logistic regression. We propose novel RDP mechanisms as well as offering a new RDP analysis for an existing method in order to add value to the RDP framework. Each method is capable of achieving arbitrary RDP privacy guarantees, and we offer experimental results of their efficacy.
C1 [Geumlek, Joseph; Song, Shuang; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA.
RP Geumlek, J (reprint author), Univ Calif San Diego, San Diego, CA 92103 USA.
EM jgeumlek@cs.ucsd.edu; shs037@eng.ucsd.edu; kamalika@cs.ucsd.edu
FU NSF [IIS 1253942]; ONR [N00014-16-1-2616]; Google Faculty Research Award
FX This work was partially supported by NSF under IIS 1253942, ONR under
   N00014-16-1-2616, and a Google Faculty Research Award.
CR Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24
   Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45
   Chaudhuri K., 2014, NEURAL INF PROCESSIN
   Chaudhuri K, 2006, LECT NOTES COMPUT SC, V4117, P198
   Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21
   Dwork C., 2016, ARXIV160301887
   Dwork C., 2014, ALGORITHMIC FDN DIFF, V9
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2009, ACM S THEORY COMPUT, P371
   Foulds J., 2016, P 32 C UNC ART INT U
   Machanavajjhala A, 2008, PROC INT CONF DATA, P277, DOI 10.1109/ICDE.2008.4497436
   McSherry F., 2017, MANY SECRETS YOU HAV
   Minami K., 2016, ADV NEURAL INFORM PR, P956
   Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11
   Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911
   Thakurta Abhradeep, 2013, C LEARN THEOR, V30, P819
   Vempala S., 2005, COMBINATORIAL COMPUT, V52, P573
   Wang Y.-X., 2015, P 32 INT C MACH LEAR, P2493
   Wang YX, 2016, LECT NOTES COMPUT SC, V9867, P121, DOI 10.1007/978-3-319-45381-1_10
   Zhang Z, 2016, PR INT CONGR SOUND V
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405036
DA 2019-06-15
ER

PT S
AU Ghassami, A
   Salehkaleybar, S
   Kiyavash, N
   Zhang, K
AF Ghassami, AmirEmad
   Salehkaleybar, Saber
   Kiyavash, Negar
   Zhang, Kun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Causal Structures Using Regression Invariance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NETWORK INFERENCE; MODELS
AB We study causal discovery in a multi-environment setting, in which the functional relations for producing the variables from their direct causes remain the same across environments, while the distribution of exogenous noises may vary. We introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments for structure learning. We define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm. Additionally, we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm. Experiment results show that the proposed algorithm outperforms the other existing algorithms.
C1 [Ghassami, AmirEmad; Kiyavash, Negar] Univ Illinois, Dept ECE, Urbana, IL 61801 USA.
   [Ghassami, AmirEmad; Salehkaleybar, Saber; Kiyavash, Negar] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
   [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA.
RP Ghassami, A (reprint author), Univ Illinois, Dept ECE, Urbana, IL 61801 USA.; Ghassami, A (reprint author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
EM ghassam2@illinois.edu; sabersk@illinois.edu; kiyavashl@illinois.edu;
   kunzl@cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO [W911NF-15-1-0281]; ONR [W911NF-15-1-0479];  [NIH-1R01EB022858-01
   FAIN-R01EB022858];  [NIH-1R01LM012087];  [NIH-5U54HG008540-02
   FAINU54HG008540]
FX This work was supported in part by ARO grant W911NF-15-1-0281 and ONR
   grant W911NF-15-1-0479. Also, KZ acknowledges the support from
   NIH-1R01EB022858-01 FAIN-R01EB022858, NIH-1R01LM012087, and
   NIH-5U54HG008540-02 FAINU54HG008540. The content is solely the
   responsibility of the authors and does not necessarily represent the
   official views of the NIH.
CR Bollen K. A., 1989, WILEY SERIES PROBABI
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Daniusis  Povilas, 2010, P 26 C UNC ART INT U
   Eberhardt F, 2005, P 21 C UNC ART INT U, P178
   Eberhardt Frederick, 2007, THESIS
   Etesami J., 2016, NEURAL COMPUTATION
   Etesami J, 2014, P AMER CONTR CONF, P2563, DOI 10.1109/ACC.2014.6859362
   Ghassami A., 2017, ARXIV170208567
   Ghassami A., 2017, ARXIV170903625
   Ghassami A, 2017, IEEE INT SYMP INFO, P1326, DOI 10.1109/ISIT.2017.8006744
   Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007
   HOYER P., 2009, ADV NEURAL INFORM PR, P689
   Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002
   Janzing D, 2010, IEEE T INFORM THEORY, V56, P5168, DOI 10.1109/TIT.2010.2060095
   Kalisch M, 2012, J STAT SOFTW, V47, P1
   Kim S, 2014, P IEEE, V102, P683, DOI 10.1109/JPROC.2014.2307888
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lutkepohl H, 2005, NEW INTRO MULTIPLE T
   Marbach D, 2009, J COMPUT BIOL, V16, P229, DOI 10.1089/cmb.2008.09TT
   Meek C., 1997, GRAPHICAL MODELS SEL
   Pearl J., 2009, CAUSALITY
   Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043
   Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Peters J, 2014, J MACH LEARN RES, V15, P2009
   Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440
   Quinn CJ, 2013, IEEE T SIGNAL PROCES, V61, P3173, DOI 10.1109/TSP.2013.2259161
   ROUSE CE, 1995, J BUS ECON STAT, V13, P217, DOI 10.2307/1392376
   Schaffter T, 2011, BIOINFORMATICS, V27, P2263, DOI 10.1093/bioinformatics/btr373
   Scholkopf B., 2012, P 29 INT C MACH LEAR, P1255
   Sgouritsa E., 2015, AISTATS
   Shanmugam K, 2015, P ADV NEUR INF PROC, P3195
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Spirtes P., 2000, CAUSATION PREDICTION
   Sun J, 2015, SIAM J APPL DYN SYST, V14, P73, DOI 10.1137/140956166
   Tian J, 2001, P 17 C UNC ART INT, P512
   Verma T., 1992, P 8 C UNC ART INT, P323
   Zhang  Kun, 2009, P 25 C UNC ART INT U
   Zhang  Kun, 2017, P INT JOINT C ART IN
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403008
DA 2019-06-15
ER

PT S
AU Ghodsi, Z
   Gu, TY
   Garg, S
AF Ghodsi, Zahra
   Gu, Tianyu
   Garg, Siddharth
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted
   Cloud
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task. However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three-and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99:4%) and TIMIT speech recognition tasks (75:22%).
C1 [Ghodsi, Zahra; Gu, Tianyu; Garg, Siddharth] NYU, New York, NY 10003 USA.
RP Ghodsi, Z (reprint author), NYU, New York, NY 10003 USA.
EM zg451@nyu.edu; tg1553@nyu.edu; sg175@nyu.edu
RI Jeong, Yongwook/N-7413-2016
CR Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090
   Ba J, 2014, ADV NEURAL INFORM PR, V1, P2654
   Coates A, 2011, P 14 INT C ART INT S, P215
   Cormode G, 2011, PROC VLDB ENDOW, V5, P25, DOI 10.14778/2047485.2047488
   Dowlin N., 2016, P 33 INT C MACH LEAR, V48, P201, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?
   Gautier A., 2016, ADV NEURAL INFORM PR, P1687
   Gennaro R, 2010, LECT NOTES COMPUT SC, V6223, P465, DOI 10.1007/978-3-642-14623-7_25
   Goldwasser S, 2008, ACM S THEORY COMPUT, P113
   Goodfellow I.J., 2013, ARXIV13024389
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   LUND C, 1992, J ACM, V39, P859, DOI 10.1145/146585.146605
   Mohassel Payman, 2017, IACR CRYPTOLOGY EPRI
   Monrose F., 1999, NDSS, P3
   Papernot N., 2016, ARXIV161103814
   Parno B, 2013, P IEEE S SECUR PRIV, P238, DOI 10.1109/SP.2013.47
   Thaler J, 2013, LECT NOTES COMPUT SC, V8043, P71, DOI 10.1007/978-3-642-40084-1_5
   Vu V, 2013, P IEEE S SECUR PRIV, P223, DOI 10.1109/SP.2013.48
   Wahby RS, 2016, P IEEE S SECUR PRIV, P759, DOI 10.1109/SP.2016.51
   Walfish M, 2015, COMMUN ACM, V58, P74, DOI 10.1145/2641562
   Zeiler MD, 2013, ARXIV201313013557
   Zhang Y., 2016, ARXIV160901000
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404072
DA 2019-06-15
ER

PT S
AU Ghoshal, A
   Honorio, J
AF Ghoshal, Asish
   Honorio, Jean
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and
   Sample Complexity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COVARIANCE ESTIMATION; MAXIMUM-LIKELIHOOD; MODEL; SELECTION
AB Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many non-identifiability and hardness results are known. In this paper we propose a provably polynomial time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance - a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data - under high-dimensional settings. We show that O(k(4) log p) number of samples suffices for our method to recover the true DAG structure with high probability, where p is the number of variables and k is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called restricted strong adjacency faithfulness (RSAF), which is strictly weaker than strong faithfulness - a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on p. We validate our theoretical findings through synthetic experiments.
C1 [Ghoshal, Asish; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
RP Ghoshal, A (reprint author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.
EM aghoshal@purdue.edu; jhonorio@purdue.edu
RI Jeong, Yongwook/N-7413-2016
CR Aragam B, 2015, J MACH LEARN RES, V16, P2273
   Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Bishop C. M., 2006, PATTERN RECOGNITION
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Chickering D.M, 1996, LEARNING DATA ARTIFI, P121, DOI DOI 10.1007/978-1-4612-2404-4_12
   Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Ghoshal Asish, 2017, P MACHINE LEARNING R, V54, P767
   Hsieh C.-J., 2013, ADV NEURAL INFORM PR, V26, P3165
   Hsieh C.-J., 2012, P ADV NEUR INF PROC, V25, P2330
   Hsu Daniel, 2011, P COLT CIT
   Jaakkola T., 2010, P 13 INT C ART INT S, P358
   Johnson C. C., 2012, P 15 INT C ART INT S, P574
   Kalisch M, 2007, J MACH LEARN RES, V8, P613
   Lu Y, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001149
   Mazumder R, 2012, J MACH LEARN RES, V13, P781
   Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P411
   Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403
   Park Gunwoong, 2015, ADV NEURAL INFORM PR, P631
   Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043
   Peters J, 2014, J MACH LEARN RES, V15, P2009
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Robinson R.W., 1977, COMBINATORIAL MATH, P28, DOI DOI 10.1007/BFB0069178
   Rolfs B., 2012, ADV NEURAL INFORM PR, P1574
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Shubbar E., 2013, BIOMEDCENTRAL CANC, V13
   Spirtes P., 2000, CAUSATION PREDICTION
   Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7
   Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080
   Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085
   Vershynin Roman, 2010, ARXIV10113027CSMATH
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
   Zhang J, 2008, MIND MACH, V18, P239, DOI 10.1007/s11023-008-9096-4
   Zhang Jiji, 2002, P 19 C UNC ART INT, P632
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406051
DA 2019-06-15
ER

PT S
AU Gioyannucci, A
   Friedrich, J
   Kaufman, M
   Churchland, AK
   Chkloyskii, D
   Paninski, L
   Pneymatikakis, EA
AF Gioyannucci, Andrea
   Friedrich, Johannes
   Kaufman, Matthew
   Churchland, Anne K.
   Chkloyskii, Dmitri
   Paninski, Liam
   Pneymatikakis, Eftychios A.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI OnACID: Online Analysis of Calcium Imaging Data in Real Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CELLULAR RESOLUTION; SCALE; DECONVOLUTION; SIGNALS; AWAKE
AB Optical imaging methods using calcium indicators are critical for monitoring the activity of large neuronal populations in vivo. Imaging experiments typically generate a large amount of data that needs to be processed to extract the activity of the imaged neuronal sources. While deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of data at a time, rendering them vulnerable to the volume of the recorded data, and preventing real-time experimental interrogation. Here we introduce OnACID, an Online framework for the Analysis of streaming Calcium Imaging Data, including i) motion artifact correction, ii) neuronal source extraction, and iii) activity denoising and deconvolution. Our approach combines and extends previous work on online dictionary learning and calcium imaging data analysis, to deliver an automated pipeline that can discover and track the activity of hundreds of cells in real time, thereby enabling new types of closed-loop experiments. We apply our algorithm on two large scale experimental datasets, benchmark its performance on manually annotated data, and show that it outperforms a popular offline approach.
C1 [Gioyannucci, Andrea; Friedrich, Johannes; Chkloyskii, Dmitri; Pneymatikakis, Eftychios A.] Flatiron Inst, New York, NY 10010 USA.
   [Kaufman, Matthew; Churchland, Anne K.] Cold Spring Harbor Lab, POB 100, Cold Spring Harbor, NY 11724 USA.
   [Friedrich, Johannes; Paninski, Liam] Columbia Univ, New York, NY 10027 USA.
RP Pneymatikakis, EA (reprint author), Flatiron Inst, New York, NY 10010 USA.
EM agiovannucci@flatironinstitute.org; jfriedrich@flatironinstitute.org;
   mkaufman@cshl.edu; churchland@cshl.edu;
   dchklovskii@flatironinstitute.org; liam@stat.columbia.edu;
   epnevmatikakis@flatironinstitute.org
RI Jeong, Yongwook/N-7413-2016
FU Simons Foundation; SNSF [P300P2_ 158428]; NIH BRAIN Initiative
   [R01EB22913]; DARPA [N66001-15-C-4032]; IARPA MICRONS [D16PC00003]
FX We thank Sue Ann Koay, Jeff Gauthier and David Tank (Princeton
   University) for sharing their cortex and hippocampal data with us. We
   thank Lindsey Myers, Sonia Villani and Natalia Roumelioti for providing
   manual annotations. We thank Daniel Barabasi (Cold Spring Harbor
   Laboratory) for useful discussions. AG, DC, and EAP were internally
   funded by the Simons Foundation. Additional support was provided by SNSF
   P300P2_ 158428 (JF), and NIH BRAIN Initiative R01EB22913, DARPA
   N66001-15-C-4032, IARPA MICRONS D16PC00003 (LP).
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   Apthorpe N. J., 2016, ADV NEURAL INFORM PR, V29, P3270
   Barlow R, 1972, STAT INFERENCE ORDER
   Bouchard MB, 2015, NAT PHOTONICS, V9, P113, DOI [10.1038/nphoton.2014.323, 10.1038/NPHOTON.2014.323]
   Boyden ES, 2005, NAT NEUROSCI, V8, P1263, DOI 10.1038/nn1525
   Clancy KB, 2014, NAT NEUROSCI, V17, P807, DOI 10.1038/nn.3712
   Dombeck DA, 2007, NEURON, V56, P43, DOI 10.1016/j.neuron.2007.08.003
   Emiliani V, 2015, J NEUROSCI, V35, P13917, DOI 10.1523/JNEUROSCI.2916-15.2015
   Freeman J, 2014, NAT METHODS, V11, P941, DOI [10.1038/nmeth.3041, 10.1038/NMETH.3041]
   Friedrich J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005423
   Friedrich Johannes, 2016, BIORXIV
   Garg Sahil, 2017, ARXIV170106106
   Giovannucci A, 2017, COSYNE ABSTRACTS
   Greenberg DS, 2009, J NEUROSCI METH, V176, P1, DOI 10.1016/j.jneumeth.2008.08.020
   Grosenick L, 2015, NEURON, V86, P106, DOI 10.1016/j.neuron.2015.03.034
   Guizar-Sicairos M, 2008, OPT LETT, V33, P156, DOI 10.1364/OL.33.000156
   Hu T, 2014, CONF REC ASILOMAR C, P613, DOI 10.1109/ACSSC.2014.7094519
   Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009
   O'Shea DJ, 2017, EXP NEUROL, V287, P437, DOI 10.1016/j.expneurol.2016.08.003
   Pachitariu M., 2017, BIORXIV, DOI [10.1101/061507, DOI 10.1101/061507]
   Pachitariu M, 2013, ADV NEURAL INFORM PR, P1745
   Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217
   Pnevmatikakis EA, 2017, J NEUROSCI METH, V291, P83, DOI 10.1016/j.jneumeth.2017.07.031
   Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037
   Reynolds S, 2017, ENEURO, V4, DOI 10.1523/ENEURO.0012-17.2017
   Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620
   Spaen Q, 2017, ARXIV170301999
   Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009
   Walker Theo, 2014, CELL MAGIC WAND TOOL
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402042
DA 2019-06-15
ER

PT S
AU Girdhar, R
   Ramanan, D
AF Girdhar, Rohit
   Ramanan, Deva
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Attentional Pooling for Action Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.
C1 [Girdhar, Rohit; Ramanan, Deva] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Girdhar, R (reprint author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation (NSF) [CNS-1518865, IIS-1618903]; Defense
   Advanced Research Projects Agency (DARPA) [HR001117C0051]; Intel Science
   and Technology Center for Visual Cloud Systems (ISTC-VCS)
FX Authors would like to thank Olga Russakovsky for initial review. This
   research was supported in part by the National Science Foundation (NSF)
   under grant numbers CNS-1518865 and IIS-1618903, and the Defense
   Advanced Research Projects Agency (DARPA) under Contract No.
   HR001117C0051. Additional support was provided by the Intel Science and
   Technology Center for Visual Cloud Systems (ISTC-VCS). Any opinions,
   findings, conclusions or recommendations expressed in this material are
   those of the authors and do not necessarily reflect the view(s) of their
   employers or the above-mentioned funding sources.
CR Baluch F., 2011, TRENDS NEUROSCIENCES
   Cao  Z., 2017, CVPR
   Carreira J., 2012, ECCV
   Carreira J., 2017, CVPR
   Chao Y. -W., 2015, ICCV
   Cheron G., 2015, ICCV
   Delaitre V., 2011, NIPS
   Delaitre V., 2010, BMVC
   Desai C., 2010, CVPR WORKSH
   Donahue J., 2015, CVPR
   Everingham M., 2010, IJCV
   Feichtenhofer C., 2016, NIPS
   Feichtenhofer C, 2016, CVPR
   Gao Y, 2016, CVPR
   Girdhar  Rohit, 2017, CVPR
   Gkioxari G, 2015, ICCV
   Gupta A., 2009, PAMI
   He K., 2016, CVPR
   Huang X., 2015, ICCV
   Ioffe S., 2015, ICML
   Jiang Y.-G., 2013, THUMOS CHALLENGE ACT
   Kay W., 2017, ARXIV170506950
   Kim J. H, 2017, ICLR
   Kong S., 2017, CVPR
   Kuehne  H., 2011, ICCV
   Lin  T.-Y., 2015, ICCV
   Maji S., 2011, CVPR
   Mallya A., 2016, ECCV
   Navalpakkam V., 2006, CVPR, P2
   Perronnin F., 2007, CVPR
   Piergiovanni A., 2017, AAAI
   Pishchulin L., 2014, GCPR
   Ramanan D., 2003, NIPS
   Ronchi M. R., 2015, BMVC
   Rutishauser U., 2004, CVPR
   Santoro A, 2017, ARXIV170601427
   Sharma S, 2016, IEEE INT WORK SIGN P
   Shi Y., 2016, ARXIV161105215
   Sigurdsson G. A., 2016, ECCV
   Simonyan K., 2014, NIPS
   Simonyan Karen, 2015, ICLR
   Song S., 2017, AAAI
   Soomro K., 2012, CRCVTR1201
   Szegedy C., 2016, INCEPTION V4 INCEPTI
   Torralba A., 2016, CVPR
   Tran D., 2015, ICCV
   Ullman S., 1984, COGNITION
   Varol G., 2016, ABS160404494 CORR
   Vaswani A., 2017, NIPS
   Wang Fei, 2017, CVPR
   Wang H, 2013, ICCV
   Wang  Heng, 2011, CVPR
   Wang L., 2016, ECCV
   Yang W., 2010, CVPR
   Yao B, 2010, CVPR
   Yao B., 2011, ICCV
   Zolfaghari Mohammadreza, 2017, ICCV
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400004
DA 2019-06-15
ER

PT S
AU Goel, S
   Klivans, A
AF Goel, Surbhi
   Klivans, Adam
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Eigenvalue Decay Implies Polynomial-Time Learnability for Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRAM MATRIX; ERROR
AB We consider the problem of learning function classes computed by neural networks with various activations (e.g. ReLU or Sigmoid), a task believed to be computationally intractable in the worst-case. A major open problem is to understand the minimal assumptions under which these classes admit provably efficient algorithms. In this work we show that a natural distributional assumption corresponding to eigenvalue decay of the Gram matrix yields polynomial-time algorithms in the non-realizable setting for expressive classes of networks (e.g. feed-forward networks of ReLUs). We make no assumptions on the structure of the network or the labels. Given sufficiently-strong eigenvalue decay, we obtain fully-polynomial time algorithms in all the relevant parameters with respect to square-loss. This is the first purely distributional assumption that leads to polynomial-time algorithms for networks of ReLUs. Further, unlike prior distributional assumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has been observed in practice on common data sets.
C1 [Goel, Surbhi; Klivans, Adam] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Goel, S (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM surbhi@cs.utexas.edu; klivans@cs.utexas.edu
RI Jeong, Yongwook/N-7413-2016
FU Microsoft Data Science Initiative Award
FX Work supported by a Microsoft Data Science Initiative Award.
CR Auer P, 1996, ADV NEUR IN, V8, P316
   Avron Haim, 2014, ADV NEURAL INFORM PR, V27, P2258
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett Peter, COMMUNICATION
   Bartlett Peter L., 2005, LOCAL RADEMACHER COM, V33
   Brach Pawel, 2015, CORR
   Brutzkus Alon, 2017, CORR
   Choromanska Anna, 2015, JMLR WORKSHOP C P, V38
   Cotter A., 2013, P 30 INT C MACH LEAR, P266
   Daniely A, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P105, DOI 10.1145/2897518.2897520
   Daniely Amit, 2017, CORR
   Daniely Amit, 2016, NIPS, P2253
   David Ofir, 2016, ARXIV161003592
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Goel S., 2016, ARXIV161110258
   Janzamin Majid, 2015, ARXIV150608473
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Klivans A., 2014, APPROX RANDOM 2014, V28, P793
   Klivans AR, 2009, J COMPUT SYST SCI, V75, P2, DOI 10.1016/j.jcss.2008.07.008
   Klivans Adam R., 2013, ELECT C COMPUTATIONA, V20, P8
   Krohmer Anton, 2012, THESIS
   Kuzmin D, 2007, J MACH LEARN RES, V8, P2047
   Littlestone  N., 1986, TECHNICAL REPORT
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Ma Siyuan, 2017, CORR
   Musco  Cameron, 2016, ARXIV160507583
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Scholkopf B., 1999, 99035 NEUROCOLT
   Sedghi H., 2014, ARXIV14122693
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shamir O, 2015, J MACH LEARN RES, V16, P3475
   Shamir Ohad, 2016, ARXIV160901037
   Shawe-Taylor J, 2005, IEEE T INFORM THEORY, V51, P2510, DOI 10.1109/TIT.2005.850052
   Song Le, 2017, ARXIV170704615
   Soudry Daniel, 2016, CORR
   Talwalkar Ameet, 2014, CORR
   Williams C. K., 2000, P 13 INT C NEUR INF, P661
   Xie Bo, 2016, CORR
   Zhang Q, 2017, CORR
   Zhang Tong, 2003, ADV NEURAL INFORM PR, P471
   Zhang Y., 2016, INT C MACH LEARN, P993
   Zhang Yi, 2015, CORR
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402024
DA 2019-06-15
ER

PT S
AU Gomez, AN
   Ren, MY
   Urtasun, R
   Grosse, RB
AF Gomez, Aidan N.
   Ren, Mengye
   Urtasun, Raquel
   Grosse, Roger B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Reversible Residual Network: Backpropagation Without Storing
   Activations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.
C1 [Gomez, Aidan N.; Ren, Mengye; Urtasun, Raquel; Grosse, Roger B.] Univ Toronto, Toronto, ON, Canada.
   [Ren, Mengye; Urtasun, Raquel; Grosse, Roger B.] Vector Inst Artificial Intelligence, Toronto, ON, Canada.
   [Ren, Mengye; Urtasun, Raquel] Uber Adv Technol Grp, Toronto, ON, Canada.
RP Gomez, AN (reprint author), Univ Toronto, Toronto, ON, Canada.
EM aidan@cs.toronto.edu; mren@cs.toronto.edu; urtasun@cs.toronto.edu;
   rgrosse@cs.toronto.edu
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, ARXIV160304467
   Al-Rfou R., 2016, ARXIV160502688
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chen J., 2016, ARXIV160400981
   Chen Tianqi, 2016, ARXIV160406174
   Chollet F, 2016, ARXIV161002357
   Dean J., 2012, NIPS
   Deco G., 1995, Advances in Neural Information Processing Systems 7, P247
   Dinh L., 2015, NICE NONLINEAR INDEP
   Dinh Laurent, 2017, ICLR
   Goyal Priya, 2017, ARXIV170602677
   Gruslys A., 2016, NIPS
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S., 2015, ICML
   Jaderberg M., 2016, ARXIV160805343
   Kalchbrenner N., 2016, NIPS
   Kalchbrenner N., 2016, ARXIV161010099
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   LeCun Y, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Lin T.-Y., 2014, ECCV
   Maclaurin D., 2015, ICML
   Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27
   Nair V., 2010, ICML
   Rall Louis B., 1981, AUTOMATIC DIFFERENTI
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sifre L., 2014, THESIS
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava R. K., 2015, ARXIV150500387
   Szegedy C., 2015, CVPR
   van den Oord A., 2016, ABS160903499 CORR
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wu Z., 2016, CORR
   Wu Z., 2016, ARXIV160404339
   Zhao H., 2017, CVPR
   Zhu J Y, 2017, ARXIV170310593
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402026
DA 2019-06-15
ER

PT S
AU Gong, CY
   Huang, WB
AF Gong, Chengyue
   Huang, Win-bin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Dynamic Poisson Factorization Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.
C1 [Gong, Chengyue; Huang, Win-bin] Peking Univ, Dept Informat Management, Beijing, Peoples R China.
RP Gong, CY (reprint author), Peking Univ, Dept Informat Management, Beijing, Peoples R China.
EM cygong@pku.edu.cn; huangwb@pku.edu.cn
RI Jeong, Yongwook/N-7413-2016
CR Ahmed A., 2008, SDM
   Ayan A., 2015, AISTATS
   Bengio Y., 2006, NIPS
   Blei D. M., 2004, NIPS
   Bui T. D., ARXIV160507066
   Bui T. D., 2016, ICML
   Cong  Y., 2017, ICML
   Gan Z., 2015, AISTATS
   Gan Zhe, 2015, ICML
   Gopalan P., ARXIV13111704
   Gopalan P., 2015, UAI
   Hoffman M., 2017, ICML
   Hosseini S. A., 2017, KDD
   Kingma Diederik P, 2014, ICLR
   Larochelle H., 2012, NIPS
   Paisley J., 2015, PAMI
   Ranganath Rajesh, 2014, AISTATS
   Ricardo H., 2015, NIPS
   Schein  A., 2016, NIPS
   Zhao S., 2017, ICML
   Zhou M., ARXIV160407464
   Zhou M., 2012, AISTATS
   Zhou  M., 2012, ADV NEURAL INFORM PR, P2546
   Zhou MY, 2016, J MACH LEARN RES, V17
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401068
DA 2019-06-15
ER

PT S
AU Gorbach, NS
   Bauer, S
   Buhmann, JM
AF Gorbach, Nico S.
   Bauer, Stefan
   Buhmann, Joachim M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Variational Inference for Dynamical Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PARAMETER-ESTIMATION
AB Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach, which, for fully observable systems is at times competitive with numerical integration. However, for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why, despite the high computational cost, numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.
C1 [Gorbach, Nico S.; Bauer, Stefan; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Gorbach, NS (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM ngorbach@inf.ethz.ch; bauers@inf.ethz.ch; jbuhmann@inf.ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU Max Planck ETH Center for Learning Systems; SystemsX.ch project SignalX
FX This research was partially supported by the Max Planck ETH Center for
   Learning Systems and the SystemsX.ch project SignalX.
CR Archambeau Cedric, 2008, NEURAL INFORM PROCES
   Babtie AC, 2014, P NATL ACAD SCI USA, V111, P18507, DOI 10.1073/pnas.1414026112
   Barenco M, 2006, GENOME BIOL, V7, DOI 10.1186/gb-2006-7-3-r25
   Calderhead Ben, 2008, NEURAL INFORM PROCES
   Dondelinger Frank, 2013, INT C ART INT STAT A
   Lorenz EN, 1998, J ATMOS SCI, V55, P399, DOI 10.1175/1520-0469(1998)055<0399:OSFSWO>2.0.CO;2
   Lotka Alfred J, 1978, GOLDEN AGE THEORETIC, P274, DOI [10.1007/978-3-642-50151-7_12, DOI 10.1007/978-3-642-50151-7_12]
   Lyons Simon, 2012, NEURAL INFORM PROCES
   Macdonald Benn, 2015, INT C MACH LEARN ICM
   Macdonald Benn, 2015, FRONTIERS BIOENGINEE, V3
   Niu Mu, 2016, INT C MACH LEARN ICM
   Ramsay JO, 2007, J ROY STAT SOC B, V69, P741, DOI 10.1111/j.1467-9868.2007.00610.x
   Ruttor Andreas, 2013, NEURAL INFORM PROCES
   Ruttor Andreas, 2010, AISTATS
   Schillings C, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004457
   Stephan KE, 2008, NEUROIMAGE, V42, P649, DOI 10.1016/j.neuroimage.2008.04.262
   VARAH JM, 1982, SIAM J SCI STAT COMP, V3, P28, DOI 10.1137/0903003
   Vrettas MD, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012148
   Vyshemirsky V, 2008, BIOINFORMATICS, V24, P833, DOI 10.1093/bioinformatics/btm607
   Wang Yali, 2014, INT C MACH LEARN ICM
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404085
DA 2019-06-15
ER

PT S
AU Goyal, A
   Ke, NR
   Ganguli, S
   Bengio, Y
AF Goyal, Anirudh
   Ke, Nan Rosemary
   Ganguli, Surya
   Bengio, Yoshua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variational Walkback: Learning a Transition Operator as a Stochastic
   Recurrent Net
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems. The proposed training objective, which we derive via principled variational methods, encourages the transition operator to "walk back" (prefer to revert its steps) in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution.
C1 [Goyal, Anirudh; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Ke, Nan Rosemary] Ecole Polytech Montreal, MILA, Montreal, PQ, Canada.
   [Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA.
RP Goyal, A (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.
EM anirudhgoyal9119@gmail.com; rosemary.nan.ke@gmail.com;
   sganguli@stanford.edu; yoshua.umontreal@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU NSERC; CIFAR; Google; Samsung; Nuance; IBM; Canada Research Chairs;
   Simons Foundation; McKnight Foundation; James S. McDonnell Foundation;
   Burroughs Wellcome Foundation; Office of Naval Research
FX The authors would like to thank Benjamin Scellier, Ben Poole, Tim
   Cooijmans, Philemon Brakel, Gaetan Marceau Caron, and Alex Lamb for
   their helpful feedback and discussions, as well as NSERC, CIFAR, Google,
   Samsung, Nuance, IBM and Canada Research Chairs for funding, and Compute
   Canada for computing resources. S.G. would like to thank the Simons,
   McKnight, James S. McDonnell, and Burroughs Wellcome Foundations and the
   Office of Naval Research for support. Y.B would also like to thank Geoff
   Hinton for an analogy which is used in this work, while discussing
   contrastive divergence (personnal communication). The authors would also
   like to express debt of gratitude towards those who contributed to
   theano over the years (as it is no longer maintained), making it such a
   great tool.
CR Alain G, 2014, J MACH LEARN RES, V15, P3563
   AlRfou R, 2016, ABS160502688 CORR
   Arora S., 2015, ARXIV151105653
   Bengio Y., 2013, BETTER MIXING VIA DE
   Bengio Y., 2014, P 31 INT C INT C MAC, V32
   Bengio Y., 2013, NIPS 2013
   Bengio Y., 2015, ABS150905936 CORR
   Bordes F., 2017, ABS170306975 CORR
   Burda Y., 2014, ABS14128566 CORR
   Crooks GE, 2000, PHYS REV E, V61, P2361, DOI 10.1103/PhysRevE.61.2361
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Gingrich T. R., 2016, P NATL ACAD SCI USA
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor K., 2015, ARXIV150204623
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2013, ARXIV13126114
   Kingma Diederik P., 2016, ABS160604934 CORR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lillicrap T. P., 2014, ARXIV14110247
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Markram H., 1995, SOC NEUR ABS, V21
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Rezende D. J, 2014, ARXIV14014082
   Salakhutdinov Ruslan, 2009, ARTIFICIAL INTELLIGE
   Schmiedl T, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.108301
   Sivak DA, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.190602
   Sohl-Dickstein J., 2015, ABS150303585 CORR
   Sonderby C. K., 2016, ADV NEURAL INFORM PR, P3738
   Theis L., 2016, INT C LEARN REPR
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404045
DA 2019-06-15
ER

PT S
AU Goyal, A
   Sordoni, A
   Cote, MA
   Ke, NR
   Bengio, Y
AF Goyal, Anirudh
   Sordoni, Alessandro
   Cote, Marc-Alexandre
   Ke, Nan Rosemary
   Bengio, Yoshua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Z-Forcing: Training Stochastic Recurrent Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.
C1 [Goyal, Anirudh; Ke, Nan Rosemary; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Sordoni, Alessandro; Cote, Marc-Alexandre] Microsoft Maluuba, Montreal, PQ, Canada.
RP Goyal, A (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.
RI Jeong, Yongwook/N-7413-2016
FU NSERC; CIFAR; Google; Samsung; IBM; Canada Research Chairs
FX The authors would like to thank Phil Bachman, Alex Lamb and Adam
   Trischler for the useful discussions. AG and YB would also like to thank
   NSERC, CIFAR, Google, Samsung, IBM and Canada Research Chairs for
   funding, and Compute Canada and NVIDIA for computing resources. The
   authors would also like to express debt of gratitude towards those who
   contributed to Theano over the years (as it is no longer maintained),
   making it such a great tool.
CR Bachman P., 2015, TRAINING DEEP GENERA
   Bachman Philip, 2016, ADV NEURAL INFORM PR, P4826
   Bayer Justin, 2014, ARXIV14117610
   Boulanger-Lewandowski Nicolas, 2012, ARXIV12066392
   Bowman S. R., 2015, ARXIV151106349
   Burda Y., 2015, ARXIV150900519
   Chen X., 2017, P ICLR
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Diao Q., 2014, P 20 ACM SIGKDD INT, P193, DOI DOI 10.1145/2623330.2623758
   Fraccaro  M., 2016, ADV NEURAL INFORM PR, V2016, P2199
   Germain M., 2015, ICML, V37, P881
   Gregor K., 2015, ARXIV150204623
   Gulrajani I., 2017, P ICLR
   Gulrajani Ishaan, 2016, ARXIV161105013
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762
   Hu Zhiting, 2017, ARXIV170300955
   Karl M., 2016, ARXIV160506432
   King S., 2013, 9 ANN BLIZZARD CHALL
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2016, ADV NEURAL INFORM PR, P4743
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lamb A. M., 2016, ADV NEURAL INFORM PR, P4601
   Larochelle H., 2011, P 14 INT C ART INT S, P29
   Louizos C, 2017, ARXIV170301961
   Oord A. v. d., 2016, ARXIV160106759
   Raiko T., 2014, P ADV NEUR INF PROC, P325
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   RUMELHART DE, 1986, COGNITIVE MODELING, V323, P533
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Salimans T, 2014, ARXIV14106460
   Semeniuta S., 2017, ARXIV170202390
   Serban I. V., 2017, P 2017 C EMP METH NA, P422
   Serban Iulian Vlad, 2017, P AAAI
   Uria B, 2016, J MACH LEARN RES, V17
   Zhao T., 2017, ARXIV170310960
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406075
DA 2019-06-15
ER

PT S
AU Grave, E
   Cisse, M
   Joulin, A
AF Grave, Edouard
   Cisse, Moustapha
   Joulin, Armand
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unbounded cache model for online language modeling with open vocabulary
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.
C1 [Grave, Edouard; Cisse, Moustapha; Joulin, Armand] Facebook AI Res, Menlo Pk, CA 94025 USA.
RP Grave, E (reprint author), Facebook AI Res, Menlo Pk, CA 94025 USA.
EM egrave@fb.com; moustaphacisse@fb.com; ajoulin@fb.com
RI Jeong, Yongwook/N-7413-2016
CR Alabdulmohsin I., ECML PKDD
   Amodei  D., 2016, ICML
   Bahl L. R., 1983, PAMI
   Bandanau D., 2015, ICLR
   Bellegarda J. R., 2000, P IEEE
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y., 2003, JMLR
   Bengio Y., 2009, ICML
   Buck C., 2014, LREC
   Caruana R, 1998, LEARNING TO LEARN, P95
   Charikar Moses S, 2002, STOC
   Chelba C., 2013, ARXIV13123005
   Chung J., 2014, CORR
   Coccaro N., 1998, ICSLP
   Della Pietra S., 1992, P WORKSH SPEECH NAT
   Elman J. L., 1990, COGNITIVE SCI
   Federico  M., 2008, INTERSPEECH
   Ge T., 2013, CVPR
   Gong Y., 2011, CVPR
   Goodman J. T., 2001, COMPUTER SPEECH LANG
   Grave E, 2017, ICLR
   Grave E., 2017, ICML
   Graves A., 2013, ICASSP
   Heafield  K., 2011, P 6 WORKSH STAT MACH
   Hochreiter S., 1997, NEURAL COMPUTATION
   Iyer R. M., 1999, IEEE T SPEECH AUDIO
   Jegou H., 2011, PAMI
   Jegou Herve, 2008, ECCV
   Jelinek F., 1991, HLT
   Joulin Armand, 2016, ARXIV161203651
   Jozefowicz R., 2016, ARXIV160202410
   Katz S. M., 1987, ICASSP
   Khudanpur S., 2000, COMPUTER SPEECH LANG
   Kneser R., 1993, ICASSP
   Kneser R., 1995, ICASSP
   Kuhn R., 1988, P 12 C COMP LING, V1
   Kuhn R., 1990, PAMI
   Kupiec J., 1989, P WORKSH SPEECH NAT
   Kuzborskij I., 2013, CVPR
   Lahiri S., 2014, P STUD RES WORKSH 14
   Lampert C. H., 2014, PAMI
   Lau R., 1993, ICASSP
   Merity S., 2017, ICLR
   Mikolov T., 2012, SLT
   Mikolov Tomas, 2011, INTERSPEECH
   Mikolov Tomas, 2010, INTERSPEECH
   Muhlbaier MD, 2009, IEEE T NEURAL NETWOR, V20, P152, DOI 10.1109/TNN.2008.2008326
   Rosenfeld R., 1996, COMPUTER SPEECH LANG
   Scheirer W. J., 2013, PAMI
   Serban  I., 2016, AAAI
   Terrell G. R., 1992, ANN STAT
   Vinyals O., 2015, NIPS
   Wang T., 2016, ACL
   Weiss Y., 2009, NIPS
   Zilly J. G., 2017, ICML
   Zobel J., 2006, ACM COMPUTING SURVEY
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406012
DA 2019-06-15
ER

PT S
AU Greenewald, K
   Tewari, A
   Klasnja, P
   Murphy, S
AF Greenewald, Kristjan
   Tewari, Ambuj
   Klasnja, Predrag
   Murphy, Susan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Action Centered Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Contextual bandits have become popular as they offer a middle ground between very simple approaches based on multi-armed bandits and very complex approaches using the full power of reinforcement learning. They have demonstrated success in web applications and have a rich body of associated theoretical guarantees. Linear models are well understood theoretically and preferred by practitioners because they are not only easily interpretable but also simple to implement and debug. Furthermore, if the linear model is true, we get very strong performance guarantees. Unfortunately, in emerging applications in mobile health, the time-invariant linear model assumption is untenable. We provide an extension of the linear model for contextual bandits that has two parts: baseline reward and treatment effect. We allow the former to be complex but keep the latter simple. We argue that this model is plausible for mobile health applications. At the same time, it leads to algorithms with strong performance guarantees as in the linear model setting, while still allowing for complex nonlinear baseline modeling. Our theory is supported by experiments on data gathered in a recently concluded mobile health study.
C1 [Greenewald, Kristjan; Murphy, Susan] Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.
   [Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
   [Klasnja, Predrag] Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA.
   [Murphy, Susan] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA.
RP Greenewald, K (reprint author), Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.
EM kgreenewald@fas.harvard.edu; tewaria@umich.edu; klasnja@umich.edu;
   samurphy@fas.harvard.edu
RI Jeong, Yongwook/N-7413-2016
FU NHLBI/NIA [R01 AA023187, P50 DA039838, U54EB020404, R01 HL125440]; NSF
   CAREER [IIS-1452099]; Sloan Research Fellowship
FX This work was supported in part by grants R01 AA023187, P50 DA039838,
   U54EB020404, R01 HL125440 NHLBI/NIA, NSF CAREER IIS-1452099, and a Sloan
   Research Fellowship.
CR Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P12
   Agrawal S., 2013, P 30 INT C MACH LEAR, P127
   Bastani Hamsa, 2015, ONLINE DECISION MAKI
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Dudik M., 2011, P 27 C UNC ART INT U, P169
   Klasnja P, 2015, HEALTH PSYCHOL, V34, P1220, DOI 10.1037/hea0000305
   Li L., 2011, P 4 ACM INT C WEB SE, P297, DOI DOI 10.1145/1935826.1935878
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Liao Peng, 2015, STAT MED
   May BC, 2012, J MACH LEARN RES, V13, P2069
   Puterman M. L., 2005, MARKOV DECISION PROC
   Seldin Yevgeny, 2011, NIPS, P1683
   Slivkins A, 2014, J MACH LEARN RES, V15, P2533
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tewari Ambuj, 2017, MOBILE HLTH SENSORS
   Valko  M., 2013, UNCERTAINTY ARTIFICI, P654
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406006
DA 2019-06-15
ER

PT S
AU Greenewald, K
   Park, S
   Zhou, SH
   Giessing, A
AF Greenewald, Kristjan
   Park, Seyoung
   Zhou, Shuheng
   Giessing, Alexander
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Time-dependent spatially varying graphical models, with application to
   brain fMRI data analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONNECTIVITY NETWORKS; COVARIANCE ESTIMATION
AB In this work, we present an additive model for space-time data that splits the data into a temporally correlated component and a spatially correlated component. We model the spatially correlated portion using a time-varying Gaussian graphical model. Under assumptions on the smoothness of changes in covariance matrices, we derive strong single sample convergence results, confirming our ability to estimate meaningful graphical structures as they evolve over time. We apply our methodology to the discovery of time-varying spatial structures in human brain fMRI signals.
C1 [Greenewald, Kristjan] Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.
   [Park, Seyoung] Yale Univ, Dept Biostat, New Haven, CT 06520 USA.
   [Zhou, Shuheng; Giessing, Alexander] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
RP Greenewald, K (reprint author), Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [DMS-1316731]; Elizabeth Caroline Crosby Research Award from the
   Advance Program at the University of Michigan; AFOSR [FA9550-13-1-0043]
FX This work was supported in part by NSF under Grant DMS-1316731,
   Elizabeth Caroline Crosby Research Award from the Advance Program at the
   University of Michigan, and by AFOSR grant FA9550-13-1-0043.
CR Arbabshirani MR, 2014, NEUROIMAGE, V102, P294, DOI 10.1016/j.neuroimage.2014.07.045
   Biswal BB, 2010, P NATL ACAD SCI USA, V107, P4734, DOI 10.1073/pnas.0911855107
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Calhoun VD, 2014, NEURON, V84, P262, DOI 10.1016/j.neuron.2014.10.015
   Carvalho CM, 2007, BAYESIAN ANAL, V2, P69, DOI 10.1214/07-BA204
   Chang C, 2010, NEUROIMAGE, V50, P81, DOI 10.1016/j.neuroimage.2009.12.011
   Chen S., 2015, ARXIV150903927
   Cressie N., 2015, STAT SPATIAL DATA
   Greenewald K, 2015, IEEE T SIGNAL PROCES, V63, P6368, DOI 10.1109/TSP.2015.2472364
   Huang SA, 2010, NEUROIMAGE, V50, P935, DOI 10.1016/j.neuroimage.2009.12.120
   Kim J, 2015, NEUROIMAGE-CLIN, V9, P625, DOI 10.1016/j.nicl.2015.10.004
   Liu X, 2013, P NATL ACAD SCI USA, V110, P4392, DOI 10.1073/pnas.1216856110
   Monti RP, 2014, NEUROIMAGE, V103, P427, DOI 10.1016/j.neuroimage.2014.07.033
   Narayan M., 2015, ARXIV150203853
   Qiu HT, 2016, J R STAT SOC B, V78, P487, DOI 10.1111/rssb.12123
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Rudelson M, 2017, ELECTRON J STAT, V11, P1699, DOI 10.1214/17-EJS1234
   Tsiligkaridis T, 2013, IEEE T SIGNAL PROCES, V61, P5347, DOI 10.1109/TSP.2013.2279355
   Varoquaux G., 2010, ADV NEURAL INFORM PR, V23, P2334
   Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575
   Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187
   Zhou SH, 2011, J MACH LEARN RES, V12, P2975
   Zhou SH, 2010, MACH LEARN, V80, P295, DOI 10.1007/s10994-010-5180-0
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405088
DA 2019-06-15
ER

PT S
AU Greff, K
   van Steenkiste, S
   Schmidhuber, J
AF Greff, Klaus
   van Steenkiste, Sjoerd
   Schmidhuber, Juergen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural Expectation Maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID THRESHOLDING ALGORITHM; BINDING; MODEL; SPARSE
AB Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects. We demonstrate that the learned representations are useful for next-step prediction.
C1 [Greff, Klaus; van Steenkiste, Sjoerd; Schmidhuber, Juergen] IDSIA, Manno, Switzerland.
RP Greff, K (reprint author), IDSIA, Manno, Switzerland.
EM klaus@idsia.ch; sjoerd@idsia.ch; juergen@idsia.ch
FU Swiss National Science Foundation [200021_165675/1]; EU [687795]; NVIDIA
   Corporation
FX The authors wish to thank Paulo Rauber and the anonymous reviewers for
   their constructive feedback. This research was supported by the Swiss
   National Science Foundation grant 200021_165675/1 and the EU project
   "INPUT" (H2020-ICT-2015 grant no. 687795). We are grateful to NVIDIA
   Corporation for donating us a DGX-1 as part of the Pioneers of AI
   Research award, and to IBM for donating a "Minsky" machine.
CR Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412
   Beck A, 2009, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2009.4959678
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Chen X., 2016, ARXIV160603657
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Greff K., 2015, ARXIV151106418CS
   Greff Klaus, 2016, ARXIV160606724CS
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Guerrero-Colon JA, 2008, IEEE IMAGE PROC, P565, DOI 10.1109/ICIP.2008.4711817
   Hershey J. R., 2014, ARXIV14092574
   Higgins  I., 2017, P INT C LEARN REPR I
   Hinton GE, 1984, DISTRIBUTED REPRESEN
   Hyvarinen A, 2006, IEEE IJCNN, P4167
   Ilin Alexander, 2017, ARXIV170709219CS STA
   Isola Phillip, 2015, ARXIV151106811CS
   Jojic Nebojsa, 2001, COMP VIS PATT REC 20, V1
   Kannan Anitha, 2007, ADN NEURAL INFORM PR, P657
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Le Roux N, 2011, NEURAL COMPUT, V23, P593, DOI 10.1162/NECO_a_00086
   MILNER PM, 1974, PSYCHOL REV, V81, P521, DOI 10.1037/h0037149
   Odena A., 2016, DECONVOLUTION CHECKE
   Pathak Deepak, 2016, ARXIV161206370CSSTAT
   Rao AR, 2008, IEEE T NEURAL NETWOR, V19, P168, DOI 10.1109/TNN.2007.905852
   Rao AR, 2010, INT J INTELL COMPUT, V3, P173, DOI 10.1108/17563781011049160
   Reichert D. P., 2013, ARXIV13126115CSQBIOS
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   SAUND E, 1995, NEURAL COMPUT, V7, P51, DOI 10.1162/neco.1995.7.1.51
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779
   Treisman A, 1996, CURR OPIN NEUROBIOL, V6, P171, DOI 10.1016/S0959-4388(96)80070-5
   Vijayanarasimhan Sudheendra, 2017, ARXIV170407804CS
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   von der Malsburg C., 1981, CORRELATION THEORY B
   VONDERMALSBURG C, 1995, CURR OPIN NEUROBIOL, V5, P520, DOI 10.1016/0959-4388(95)80014-X
   WANG DL, 1995, IEEE T NEURAL NETWOR, V6, P283, DOI 10.1109/72.363423
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Wersing H, 2001, NEURAL COMPUT, V13, P357, DOI 10.1162/089976601300014574
   Williams Ronald J., 1989, NUCCS8927
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406073
DA 2019-06-15
ER

PT S
AU Gross, R
   Gu, Y
   Li, W
   Gauci, M
AF Gross, Roderich
   Gu, Yue
   Li, Wei
   Gauci, Melvin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Generalizing GANs: A Turing Perspective
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FINITE-STATE MACHINES; COEVOLUTION; BEHAVIOR
AB Recently, a new class of machine learning algorithms has emerged, where models and discriminators are generated in a competitive setting. The most prominent example is Generative Adversarial Networks (GANs). In this paper we examine how these algorithms relate to the Turing test, and derive what-from a Turing perspective-can be considered their defining features. Based on these features, we outline directions for generalizing GANs-resulting in the family of algorithms referred to as Turing Learning. One such direction is to allow the discriminators to interact with the processes from which the data samples are obtained, making them "interrogators", as in the Turing test. We validate this idea using two case studies. In the first case study, a computer infers the behavior of an agent while controlling its environment. In the second case study, a robot infers its own sensor configuration while controlling its movements. The results confirm that by allowing discriminators to interrogate, the accuracy of models is improved.
C1 [Gross, Roderich; Gu, Yue] Univ Sheffield, Dept Automat Control & Syst Engn, Sheffield, S Yorkshire, England.
   [Li, Wei] Univ York, Dept Elect, York, N Yorkshire, England.
   [Gauci, Melvin] Harvard Univ, Wyss Inst Biol Inspired Engn, Cambridge, MA 02138 USA.
RP Gross, R (reprint author), Univ Sheffield, Dept Automat Control & Syst Engn, Sheffield, S Yorkshire, England.
EM r.gross@sheffield.ac.uk; ygu16@sheffield.ac.uk; wei.li@york.ac.uk;
   mgauci@g.harvard.edu
RI GROSS, Roderich/A-6657-2008
OI GROSS, Roderich/0000-0003-1826-1375
CR Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI 10.1109/4235.985690
   Bongard J, 2005, J MACH LEARN RES, V6, P1651
   Bongard JC, 2005, IEEE T EVOLUT COMPUT, V9, P361, DOI 10.1109/TEVC.2005.850293
   Bongard J, 2006, SCIENCE, V314, P1118, DOI 10.1126/science.1133687
   Cartlidge J, 2004, EVOL COMPUT, V12, P193, DOI 10.1162/106365604773955148
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   French RM, 2000, TRENDS COGN SCI, V4, P115, DOI 10.1016/S1364-6613(00)01453-4
   Gauci M, 2014, INT J ROBOT RES, V33, P1145, DOI 10.1177/0278364914525244
   Glover F., 2015, SCHOLARPEDIA, V10, P6532, DOI DOI 10.4249/SCH0LARPEDIA.6532
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2017, CORR
   Harnad S., 2000, Journal of Logic, Language and Information, V9, P425, DOI 10.1023/A:1008315308862
   HILLIS WD, 1990, PHYSICA D, V42, P228, DOI 10.1016/0167-2789(90)90076-2
   Im D. J., 2016, CORR
   Jakobi N, 1995, LECT NOTES ARTIF INT, V929, P704
   Juille H., 1998, Genetic Programming 1998. Proceedings of the Third Annual Conference, P519
   Li W, 2016, SWARM INTELL-US, V10, P211, DOI 10.1007/s11721-016-0126-1
   Li W, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P223
   Magnenat  S., 2011, ENKI FAST 2D ROBOT S
   MILLER GF, 1994, COM ADAP SY, P411
   Mondada F., 2009, P 9 C AUT ROB SYST C, V1, P59
   Nolfi S, 1998, ARTIF LIFE, V4, P311, DOI 10.1162/106454698568620
   Radford A., 2015, ARXIV151106434
   Rosin CD, 1997, EVOL COMPUT, V5, P1, DOI 10.1162/evco.1997.5.1.1
   Saygin AP, 2000, MIND MACH, V10, P463
   Schawinski K, 2017, MON NOT R ASTRON SOC, V467, pL110, DOI 10.1093/mnrasl/slx008
   Turing A. M., 1950, MIND, V59, P433, DOI DOI 10.1093/MIND/LIX.236.433
   Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1013, DOI 10.1109/TPAMI.2005.147
   Vidal E, 2005, IEEE T PATTERN ANAL, V27, P1026, DOI 10.1109/TPAMI.2005.148
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406038
DA 2019-06-15
ER

PT S
AU Gu, SX
   Lillicrap, T
   Ghahramani, Z
   Turner, RE
   Scholkopf, B
   Levine, S
AF Gu, Shixiang
   Lillicrap, Timothy
   Ghahramani, Zoubin
   Turner, Richard E.
   Scholkopf, Bernhard
   Levine, Sergey
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient
   Estimation for Deep Reinforcement Learning Shixiang
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.
C1 [Gu, Shixiang] Univ Cambridge, Max Planck Inst, Cambridge, England.
   [Lillicrap, Timothy] DeepMind, London, England.
   [Ghahramani, Zoubin] Univ Cambridge, Uber AI Labs, Cambridge, England.
   [Turner, Richard E.] Univ Cambridge, Cambridge, England.
   [Scholkopf, Bernhard] Max Planck Inst, Munich, Bavaria, Germany.
   [Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA.
RP Gu, SX (reprint author), Univ Cambridge, Max Planck Inst, Cambridge, England.
EM sg717@cam.ac.uk; countzero@google.com; zoubin@eng.cam.ac.uk;
   ret26@cam.ac.uk; bs@tuebingen.mpg.de; sylevine@eecs.berkeley.edu
FU Cambridge-Tubingen PhD Fellowship; NSERC; Google Focused Research Award
FX This work is supported by generous sponsorship from Cambridge-Tubingen
   PhD Fellowship, NSERC, and Google Focused Research Award.
CR Bagnell J Andrew, 2003, IJCAI
   Brockman G, 2016, ARXIV160601540
   Degris  Thomas, 2012, ARXIV12054839
   Duan Yan, 2016, INT C MACH LEARN ICM
   Gu S., 2017, ICLR
   Heess N., 2015, ADV NEURAL INFORM PR, P2944
   Jiang N., 2016, P 33 INT C MACH LEAR, P652
   Jie Tang, 2010, ADV NEURAL INFORM PR, P1000
   Kakade  S., 2002, ICML, P267
   Levine  S., 2013, P 30 INT C MACH LEAR, P1
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2016, ICLR
   Mahmood A Rupam, 2014, P INT C NEUR INF PRO, P3014
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Munos Remi, 2016, ARXIV160602647
   O'Donoghue Brendan, 2017, ICLR
   Peshkin Leonid, 2002, P 19 INT C MACH LEAR
   Peters J., 2010, AAAI
   Precup D., 2000, COMPUTER SCI DEP FAC, P80
   Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317
   Ross S. M., 2006, SIMULATION
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman John, 2016, INT C LEARN REPR ICL
   Silver D., 2014, INT C MACH LEARN ICM
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   SUTTON R.S., 1999, NIPS, V99, P1057
   Thomas Philip, 2014, P 31 INT C MACH LEAR, P441
   Thomas Philip, 2016, INT C MACH LEARN, P2139
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Wang Ziyu, 2017, ICLR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403088
DA 2019-06-15
ER

PT S
AU Gucluturk, Y
   Guclu, U
   Seeliger, K
   Bosch, S
   van Lier, R
   van Gerven, M
AF Gucluturk, Yagmur
   Guclu, Umut
   Seeliger, Katja
   Bosch, Sander
   van Lier, Rob
   van Gerven, Marcel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Reconstructing perceived faces from brain activations with deep
   adversarial neural decoding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NATURAL IMAGES; REPRESENTATIONS; MODELS; SHAPE; ATTRACTIVENESS;
   INFORMATION; PERCEPTION; RESPONSES; NETWORKS; STREAM
AB Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.
C1 [Gucluturk, Yagmur; Guclu, Umut; Seeliger, Katja; Bosch, Sander; van Lier, Rob; van Gerven, Marcel] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.
RP Gucluturk, Y (reprint author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.
EM y.gucluturk@donders.ru.nl; u.guclu@donders.ru.nl
RI Jeong, Yongwook/N-7413-2016; Bosch, Sander Erik/O-7378-2019
OI Bosch, Sander Erik/0000-0001-6845-0911
FU VIDI grant from the Netherlands Organization for Scientific Research
   [639.072.513]; GPU grant (GeForce Titan X) from the Nvidia Corporation
FX This work has been partially supported by a VIDI grant (639.072.513)
   from the Netherlands Organization for Scientific Research and a GPU
   grant (GeForce Titan X) from the Nvidia Corporation.
CR Birkas B, 2014, PERS INDIV DIFFER, V69, P56, DOI 10.1016/j.paid.2014.05.012
   Carrito MD, 2016, EVOL HUM BEHAV, V37, P125, DOI 10.1016/j.evolhumbehav.2015.09.006
   Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755
   Cowen AS, 2014, NEUROIMAGE, V94, P12, DOI 10.1016/j.neuroimage.2014.03.018
   Donderi DC, 2005, DISPLAYS, V26, P71, DOI 10.1016/j.displa.2005.02.002
   Dosovitskiy A., 2016, CORR
   Du C., 2017, CORR
   Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001
   Friston K, 2007, STATISTICAL PARAMETRIC MAPPING: THE ANALYSIS OF FUNCTIONAL BRAIN IMAGES, P1
   Goesaert E, 2013, J NEUROSCI, V33, P8549, DOI 10.1523/JNEUROSCI.1829-12.2013
   Goodfellow I., 2014, ARXIV14062661
   Guclu U., 2013, BELG DUTCH C MACH LE
   Guclu U., 2016, ADV NEURAL INFORM PR
   Guclu U, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00007
   Guclu U, 2017, NEUROIMAGE, V145, P329, DOI 10.1016/j.neuroimage.2015.12.036
   Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015
   Gucluturk Yagmur, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P810, DOI 10.1007/978-3-319-46604-0_56
   Gucluturk Y, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00112
   Hahn AC, 2014, NEUROSCI BIOBEHAV R, V46, P591, DOI 10.1016/j.neubiorev.2014.08.015
   Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736
   Horikawa T, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15037
   Ioffe S., 2015, CORR
   Jenkinson M, 2012, NEUROIMAGE, V62, P782, DOI 10.1016/j.neuroimage.2011.09.015
   Jia Y., 2014, ARXIV14085093
   Johnson J., 2016, CORR
   Kamitani Y, 2005, NAT NEUROSCI, V8, P679, DOI 10.1038/nn1444
   Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713
   Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kriegeskorte N, 2015, ANNU REV VIS SC, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Ledig C., 2016, CORR
   Lee H, 2016, J NEUROSCI, V36, P6069, DOI 10.1523/JNEUROSCI.4286-15.2016
   Little AC, 2014, BRIT J PSYCHOL, V105, P364, DOI 10.1111/bjop.12043
   Liu  Z., 2015, P INT C COMP VIS ICC
   Ma DS, 2015, BEHAV RES METHODS, V47, P1122, DOI 10.3758/s13428-014-0532-5
   Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876
   Miyawaki Y, 2008, NEURON, V60, P915, DOI 10.1016/j.neuron.2008.11.004
   Mumford JA, 2012, NEUROIMAGE, V59, P2636, DOI 10.1016/j.neuroimage.2011.08.076
   Naselaris T, 2011, NEUROIMAGE, V56, P400, DOI 10.1016/j.neuroimage.2010.07.073
   Naselaris T, 2009, NEURON, V63, P902, DOI 10.1016/j.neuron.2009.09.006
   Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031
   Parkhi O. M., 2016, BRIT MACH VIS C
   Pathak D., 2016, CORR
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   PERRETT DI, 1994, NATURE, V368, P239, DOI 10.1038/368239a0
   Petersen K. B., 2012, MATRIX COOKBOOK
   Radford A., 2015, ARXIV151106434
   Schoenmakers S, 2015, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00173
   Schoenmakers S, 2013, NEUROIMAGE, V83, P951, DOI 10.1016/j.neuroimage.2013.07.043
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Strohminger N, 2016, BEHAV RES METHODS, V48, P1197, DOI 10.3758/s13428-015-0641-9
   Strom MA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041193
   Thaler L, 2013, VISION RES, V76, P31, DOI 10.1016/j.visres.2012.10.012
   Thirion B, 2006, NEUROIMAGE, V33, P1104, DOI 10.1016/j.neuroimage.2006.06.062
   Tokui S., 2015, ADV NEUR INF PROC SY
   van Gerven M., 2012, 2012 2 INT WORKSH PA
   van Gerven MAJ, 2017, J MATH PSYCHOL, V76, P172, DOI 10.1016/j.jmp.2016.06.009
   van Gerven MAJ, 2010, NEURAL COMPUT, V22, P3127, DOI 10.1162/NECO_a_00047
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
NR 63
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404031
DA 2019-06-15
ER

PT S
AU Guo, ZD
   Thomas, PS
   Brunskill, E
AF Guo, Zhaohan Daniel
   Thomas, Philip S.
   Brunskill, Emma
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Using Options and Covariance Testing for Long Horizon Off-Policy Policy
   Evaluation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Evaluating a policy by deploying it in the real world can be risky and costly. Off-policy policy evaluation (OPE) algorithms use historical data collected from running a previous policy to evaluate a new policy, which provides a means for evaluating a policy without requiring it to ever be deployed. Importance sampling is a popular OPE method because it is robust to partial observability and works with continuous states and actions. However, the amount of historical data required by importance sampling can scale exponentially with the horizon of the problem: the number of sequential decisions that are made. We propose using policies over temporally extended actions, called options, and show that combining these policies with importance sampling can significantly improve performance for long-horizon problems. In addition, we can take advantage of special cases that arise due to options-based policies to further improve the performance of importance sampling. We further generalize these special cases to a general covariance testing rule that can be used to decide which weights to drop in an IS estimate, and derive a new IS algorithm called Incremental Importance Sampling that can provide significantly more accurate estimates for a broad class of domains.
C1 [Guo, Zhaohan Daniel] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Thomas, Philip S.] Univ Massachusetts, Amherst, MA 01003 USA.
   [Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA.
RP Guo, ZD (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM zguo@cs.cmu.edu; pthomas@cs.umass.edu; ebrun@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR Young Investigator award; NSF CAREER award; Institute of Education
   Sciences, U.S. Department of Education
FX The research reported here was supported in part by an ONR Young
   Investigator award, an NSF CAREER award, and by the Institute of
   Education Sciences, U.S. Department of Education. The opinions expressed
   are those of the authors and do not represent views of NSF, IES or the
   U.S. Dept. of Education.
CR Bastani M., 2014, THESIS
   Brunskill E., 2014, P 31 INT C MACH LEAR, P316
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   Dudik M., 2011, P 28 INT C MACH LEAR, P1097
   Hallak A., 2015, ARXIV150905172
   Hallak Assaf, 2015, ICML, P711
   Jiang N., 2016, P 33 INT C MACH LEAR, P652
   Li  Lihong, 2015, P INT C ART INT STAT, P608
   Mankowitz Daniel J, 2014, INT C MACH LEAR
   Mann Timothy A, 2013, RLDM, P9
   Munos Remi, 2016, ADV NEURAL INFORM PR, P1046
   Precup D., 2000, COMPUTER SCI DEP FAC, P80
   Precup Doina, 2001, P 18 INT C MACH LEAR, P417
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Theocharous G., 2015, INT JOINT C ART INT
   Thomas P., 2015, P 29 C ART INT
   Thomas P. S., 2016, INT C MACH LEARN
   Thomas P. S., 2015, ADV NEURAL INFORM PR, P334
   Thomas P. S., 2017, AAAI
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402053
DA 2019-06-15
ER

PT S
AU Gurbuzbalaban, M
   Ozdaglar, A
   Parrilo, PA
   Vanli, ND
AF Gurbuzbalaban, Mert
   Ozdaglar, Asuman
   Parrilo, Pablo A.
   Vanli, N. Denizcan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE; REGULARIZATION; CONVEXITY
AB The coordinate descent (CD) method is a classical optimization algorithm that has seen a revival of interest because of its competitive performance in machine learning applications. A number of recent papers provided convergence rate estimates for their deterministic (cyclic) and randomized variants that differ in the selection of update coordinates. These estimates suggest randomized coordinate descent (RCD) performs better than cyclic coordinate descent (CCD), although numerical experiments do not provide clear justification for this comparison. In this paper, we provide examples and more generally problem classes for which CCD (or CD with any deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore, we provide lower and upper bounds on the amount of improvement on the rate of CCD relative to RCD, which depends on the deterministic order used. We also provide a characterization of the best deterministic order (that leads to the maximum improvement in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective function.
C1 [Gurbuzbalaban, Mert] Rutgers State Univ, Piscataway, NJ 08854 USA.
   [Ozdaglar, Asuman; Parrilo, Pablo A.; Vanli, N. Denizcan] MIT, Cambridge, MA 02139 USA.
RP Gurbuzbalaban, M (reprint author), Rutgers State Univ, Piscataway, NJ 08854 USA.
EM mg1366@rutgers.edu; asuman@mit.edu; parrilo@mit.edu; denizcan@mit.edu
FU NSF [DMS-1723085]; DARPA Foundations of Scalable Statistical Learning
   grants
FX This work is supported by NSF DMS-1723085 and DARPA Foundations of
   Scalable Statistical Learning grants.
CR Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Bertsekas D. P., 2015, CONVEX OPTIMIZATION
   Chung Fan R. K., 1997, SPECTRAL GRAPH THEOR
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Friedman J, 2010, J STAT SOFTW, V33, P1
   KINGMAN JF, 1961, Q J MATH, V12, P283, DOI 10.1093/qmath/12.1.283
   Kirkland S. J., 2012, GROUP INVERSES M MAT
   Lu ZS, 2015, MATH PROGRAM, V152, P615, DOI 10.1007/s10107-014-0800-2
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   Malioutov DM, 2006, J MACH LEARN RES, V7, P2031
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   NUSSBAUM RD, 1986, LINEAR ALGEBRA APPL, V73, P59
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Ortega J., 2000, ITERATIVE SOLUTION N
   PLEMMONS RJ, 1977, LINEAR ALGEBRA APPL, V18, P175, DOI 10.1016/0024-3795(77)90073-8
   Rantzer A., 2014, ARXIV12030047
   Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6
   Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054
   Sun R., 2015, ADV NEURAL INFORM PR, P1306
   Sun Ruoyu, 2016, ARXIV160407130
   Varga RS, 2009, MATRIX ITERATIVE ANA
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Young D. M., 1971, ITERATIVE SOLUTION L
   Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407009
DA 2019-06-15
ER

PT S
AU Habeck, M
AF Habeck, Michael
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Model evidence from nonequilibrium simulations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FREE-ENERGY DIFFERENCES
AB The marginal likelihood, or model evidence, is a key quantity in Bayesian parameter estimation and model comparison. For many probabilistic models, computation of the marginal likelihood is challenging, because it involves a sum or integral over an enormous parameter space. Markov chain Monte Carlo (MCMC) is a powerful approach to compute marginal likelihoods. Various MCMC algorithms and evidence estimators have been proposed in the literature. Here we discuss the use of nonequilibrium techniques for estimating the marginal likelihood. Nonequilibrium estimators build on recent developments in statistical physics and are known as annealed importance sampling (AIS) and reverse AIS in probabilistic machine learning. We introduce estimators for the model evidence that combine forward and backward simulations and show for various challenging models that the evidence estimators outperform forward and reverse AIS.
C1 [Habeck, Michael] Univ Gottingen, Stat Inverse Problems Biophys, Max Planck Inst Biophys Chem, D-37077 Gottingen, Germany.
   [Habeck, Michael] Univ Gottingen, Inst Math Stochast, D-37077 Gottingen, Germany.
RP Habeck, M (reprint author), Univ Gottingen, Stat Inverse Problems Biophys, Max Planck Inst Biophys Chem, D-37077 Gottingen, Germany.; Habeck, M (reprint author), Univ Gottingen, Inst Math Stochast, D-37077 Gottingen, Germany.
EM mhabeck@gwdg.de
RI Jeong, Yongwook/N-7413-2016
FU Deutsche Forschungsgemeinschaft (DFG) [SFB 860]
FX This work has been funded by the Deutsche Forschungsgemeinschaft (DFG)
   SFB 860, subproject B09.
CR Beale PD, 1996, PHYS REV LETT, V76, P78, DOI 10.1103/PhysRevLett.76.78
   BENNETT CH, 1976, J COMPUT PHYS, V22, P245, DOI 10.1016/0021-9991(76)90078-4
   Burda Y., 2015, ACCURATE CONSERVATIV, P102
   Crooks G. E., 1999, THESIS
   Crooks GE, 1999, PHYS REV E, V60, P2721, DOI 10.1103/PhysRevE.60.2721
   Crooks GE, 1998, J STAT PHYS, V90, P1481, DOI 10.1023/A:1023208217925
   Gelman A, 1998, STAT SCI, V13, P163
   Geyer C. J., 1991, COMP SCI STAT, P156, DOI DOI 10.1080/01621459.1995.10476590
   Grosse RB, 2015, ARXIV151102543
   Grosse Roger B, 2016, ADV NEURAL INFORM PR, P2451
   Habeck M., 2012, P 15 INT C ART INT S, V22, P486
   Habeck M, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.100601
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hummer G, 2001, J CHEM PHYS, V114, P7330, DOI 10.1063/1.1363668
   Jarzynski C, 1997, PHYS REV LETT, V78, P2690, DOI 10.1103/PhysRevLett.78.2690
   Jaynes E. T., 2003, PROBABILITY THEORY L
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Knuth KH, 2015, DIGIT SIGNAL PROCESS, V47, P50, DOI 10.1016/j.dsp.2015.06.012
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu J, 2001, MONTE CARLO STRATEGI
   MacKay D. J, 2003, INFORM THEORY INFERE
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Shirts MR, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.140601
   Skilling J, 2006, BAYESIAN ANAL, V1, P833, DOI 10.1214/06-BA127
   SWENDSEN RH, 1986, PHYS REV LETT, V57, P2607, DOI 10.1103/PhysRevLett.57.2607
NR 25
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401076
DA 2019-06-15
ER

PT S
AU Hadfield-Menell, D
   Milli, S
   Abbeel, P
   Russell, S
   Dragan, AD
AF Hadfield-Menell, Dylan
   Milli, Smitha
   Abbeel, Pieter
   Russell, Stuart
   Dragan, Anca D.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inverse Reward Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.
C1 [Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca D.] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA.
   [Abbeel, Pieter] Int Comp Sci Inst, OpenAI, Berkeley, CA 94704 USA.
RP Hadfield-Menell, D (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA.
EM dhm@cs.berkeley.edu; smilli@cs.berkeley.edu; pabbeel@cs.berkeley.edu;
   russell@cs.berkeley.edu; anca@cs.berkeley.edu
FU Center for Human Compatible AI; Open Philanthropy Project; Future of
   Life Institute; AFOSR; NSF [DGE 1106400]
FX This work was supported by the Center for Human Compatible AI and the
   Open Philanthropy Project, the Future of Life Institute, AFOSR, and NSF
   Graduate Research Fellowship Grant No. DGE 1106400.
CR Amodei D., 2016, FAULTY REWARD FUNCTI
   Amodei  D., 2016, ABS160606565 CORR
   Duan Yan, 2016, ABS161102779 CORR
   Evans  Owain, 2016, P 30 AAAI C ART INT, P323
   Frank M. C., 2009, P 31 ANN C COGN SCI, P1228
   Goodman Noah D, 2014, HDB CONT SEMANTIC TH, V2
   Grice H. Paul, 1975, LOGIC CONVERSATION, P43
   Hadfield-Menell Dylan, 2016, P 30 ANN C NEUR INF
   Hadfield-Menell Dylan, 2017, P INT JOINT C ART IN
   Jain A, 2015, INT J ROBOT RES, V34, P1296, DOI 10.1177/0278364915581193
   Javdani Shervin, 2015, P ROBOTICS SCI SYSTE
   Murray Iain, 2006, P 22 C UNC ART INT
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Puterman M. L., 2009, MARKOV DECISION PROC
   Russell S., 2010, ARTIFICIAL INTELLIGE
   Singh Satinder, 2010, P INT S AI INSP BIOL, P111
   Sorg Jonathan, 2010, ADV NEURAL INFORM PR, P2190
   Sunnaker M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002803
   Syed U., 2007, ADV NEURAL INFORM PR, P1449
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406080
DA 2019-06-15
ER

PT S
AU Hafner, D
   Irpan, A
   Davidson, J
   Heess, N
AF Hafner, Danijar
   Irpan, Alex
   Davidson, James
   Heess, Nicolas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Hierarchical Information Flow with Recurrent Neural Modules
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.
C1 [Hafner, Danijar; Irpan, Alex; Davidson, James] Google Brain, Mountain View, CA 94043 USA.
   [Heess, Nicolas] Google DeepMind, London, England.
RP Hafner, D (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM mail@danijar.com; alexirpan@google.com; jcdavidson@google.com;
   heess@google.com
RI Jeong, Yongwook/N-7413-2016
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Ba Jimmy, 2016, ADV NEURAL INFORM PR, P4331
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103, DOI DOI 10.3115/V1/W14-4012
   Cooijmans T., 2016, ARXIV160309025
   Devin C, 2016, ARXIV160907088
   Fernando C, 2017, ARXIV170108734
   Ganguli S., 2017, ARXIV170304200
   Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019
   Graves A., 2014, ARXIV14105401
   Graves A., 2016, ARXIV160308983
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hawkins  J., 2006, TECHNICAL REPORT
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Huang G, 2016, ARXIV160806993
   Kingma D. P., 2015, INT C LEARN REPR
   Kirkpatrick James, 2017, P NATL ACAD SCI USA
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krueger D., 2016, ARXIV160601305
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Mahoney M., 2011, ABOUT THE TEST DATA
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Pham T., 2017, ARXIV170207021
   Reed S., 2015, INT C LEARN REPR
   Rusu A. A., 2016, ARXIV160604671
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Sherman SM, 2016, NAT NEUROSCI, V19, P533, DOI 10.1038/nn.4269
   Srivastava R. K., 2015, ARXIV150500387
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406076
DA 2019-06-15
ER

PT S
AU Halloran, JT
   Rocke, DM
AF Halloran, John T.
   Rocke, David M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gradients of Generative Models for Improved Discriminative Analysis of
   Tandem Mass Spectra
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FALSE DISCOVERY RATE; PEPTIDE IDENTIFICATION; SHOTGUN
AB Tandem mass spectrometry (MS/MS) is a high-throughput technology used to identify the proteins in a complex biological sample, such as a drop of blood. A collection of spectra is generated at the output of the process, each spectrum of which is representative of a peptide (protein subsequence) present in the original complex sample. In this work, we leverage the log-likelihood gradients of generative models to improve the identification of such spectra. In particular, we show that the gradient of a recently proposed dynamic Bayesian network (DBN) [7] may be naturally employed by a kernel-based discriminative classifier. The resulting Fisher kernel substantially improves upon recent attempts to combine generative and discriminative models for post-processing analysis, outperforming all other methods on the evaluated datasets. We extend the improved accuracy offered by the Fisher kernel framework to other search algorithms by introducing Theseus, a DBN representing a large number of widely used MS/MS scoring functions. Furthermore, with gradient ascent and max-product inference at hand, we use Theseus to learn model parameters without any supervision.
C1 [Halloran, John T.; Rocke, David M.] Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.
RP Halloran, JT (reprint author), Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.
EM jthalloran@ucdavis.edu; dmrocke@ucdavis.edu
RI Jeong, Yongwook/N-7413-2016
FU National Center for Advancing Translational Sciences (NCATS), National
   Institutes of Health [UL1 TR001860]
FX This work was supported by the National Center for Advancing
   Translational Sciences (NCATS), National Institutes of Health, through
   grant UL1 TR001860.
CR BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Craig R, 2004, BIOINFORMATICS, V20, P1466, DOI 10.1093/bioinformatics/bth092
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Elkan C, 2005, LECT NOTES COMPUT SC, V3772, P295
   Eng JK, 2013, PROTEOMICS, V13, P22, DOI 10.1002/pmic.201200439
   ENG JK, 1994, J AM SOC MASS SPECTR, V5, P976, DOI 10.1016/1044-0305(94)80016-2
   Halloran JT, 2016, J PROTEOME RES, V15, P2749, DOI 10.1021/acs.jproteome.6b00290
   Halloran John T., 2017, GRADIENTS GENERATIVE
   Halloran John T., 2014, UNCERTAINTY ARTIFICI
   Jaakkola T, 1999, Proc Int Conf Intell Syst Mol Biol, P149
   Jaakkola T., 1998, ADV NEURAL INFORM PR
   Jeffry Howbert  J, 2014, MOL CELLULAR PROTEOM, pmcp
   Kall L, 2007, NAT METHODS, V4, P923, DOI 10.1038/NMETH1113
   Keich U, 2015, J PROTEOME RES, V14, P3148, DOI 10.1021/acs.jproteome.5b00081
   Keich U, 2015, J PROTEOME RES, V14, P1147, DOI 10.1021/pr5010983
   Kim  Sangtae, 2014, NATURE COMMUNICATION, V5
   McIlwain  Sean, 2014, J PROTEOME RES
   Pearl J, 1988, PROBABILISTIC REASON
   Spivak M, 2012, MOL CELLULAR PROTEOM, V11
   Spivak M, 2009, J PROTEOME RES, V8, P3737, DOI 10.1021/pr801109k
   Wang SJ, 2016, BIOINFORMATICS, V32, P322, DOI 10.1093/bioinformatics/btw269
   Wenger C. D., 2013, J PROTEOME RES
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405078
DA 2019-06-15
ER

PT S
AU Hamilton, L
   Koehler, F
   Moitra, A
AF Hamilton, Linus
   Koehler, Frederic
   Moitra, Ankur
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Information Theoretic Properties of Markov Random Fields, and their
   Algorithmic Applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Markov random fields are a popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler [4] gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models.
   Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on n nodes with r-order interactions in n(r) time and log n sample complexity. Our algorithms also extend to various partial observation models.
C1 [Hamilton, Linus; Koehler, Frederic] MIT, Dept Math, Cambridge, MA 02139 USA.
   [Moitra, Ankur] MIT, Dept Math & Comp Sci, Cambridge, MA 02139 USA.
   [Moitra, Ankur] MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA.
RP Hamilton, L (reprint author), MIT, Dept Math, Cambridge, MA 02139 USA.
EM luh@mit.edu; fkoehler@mit.edu; moitra@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU Hertz Fellowship; NSF CAREER Award [CCF-1453261]; NSF [CCF-1565235];
   David and Lucile Packard Fellowship; Alfred P. Sloan Fellowship
FX This work was supported in part by Hertz Fellowship.; This work was
   supported in part by NSF CAREER Award CCF-1453261, NSF Large
   CCF-1565235, a David and Lucile Packard Fellowship and an Alfred P.
   Sloan Fellowship.
CR Abbeel P, 2006, J MACH LEARN RES, V7, P1743
   Anandkumar A., 2012, ADV NEURAL INFORM PR, P1052
   Anandkumar A, 2012, ANN STAT, V40, P1346, DOI 10.1214/12-AOS1009
   Bresler G., 2015, P 47 ANN ACM S THEOR, P771
   Bresler G, 2008, LECT NOTES COMPUT SC, V5171, P343
   BRUSH SG, 1967, REV MOD PHYS, V39, P883, DOI 10.1103/RevModPhys.39.883
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Csiszar I, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P170
   Dasarathy Gautam, 2016, J MACH LEARN RES
   Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134
   Jalali A., 2011, AISTATS, P378
   Kleinberg J, 2002, J ACM, V49, P616, DOI 10.1145/585265.585268
   Lee S, 2006, ADV NEURAL INFORM PR, V20, P817
   MARTINELLI F, 1994, COMMUN MATH PHYS, V161, P447, DOI 10.1007/BF02101929
   Mossel E, 2009, PROBAB THEORY REL, V143, P401, DOI 10.1007/s00440-007-0131-9
   ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Santhanam NP, 2012, IEEE T INFORM THEORY, V58, P4117, DOI 10.1109/TIT.2012.2191659
   Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56
   Sly A, 2010, ANN IEEE SYMP FOUND, P287, DOI 10.1109/FOCS.2010.34
   Srebro N., 2001, P 17 C UNC ART INT U, P504
   Valiant G, 2012, ANN IEEE SYMP FOUND, P11, DOI 10.1109/FOCS.2012.27
   Vuffray M, 2016, ADV NEURAL INFORM PR, P2595
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402050
DA 2019-06-15
ER

PT S
AU Hamilton, WL
   Ying, R
   Leskovec, J
AF Hamilton, William L.
   Ying, Rex
   Leskovec, Jure
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inductive Representation Learning on Large Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.
C1 [Hamilton, William L.; Ying, Rex; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Hamilton, WL (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM wleif@stanford.edu; rexying@stanford.edu; jure@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1149837]; DARPA SIMPLEX; SAP Stanford Graduate Fellowship;
   NSERC PGS-D grant; Stanford Data Science Initiative; Huawei; Chan
   Zuckerberg Biohub
FX The authors thank Austin Benson, Aditya Grover, Bryan He, Dan Jurafsky,
   Alex Ratner, Marinka Zitnik, and Daniel Selsam for their helpful
   discussions and comments on early drafts. The authors would also like to
   thank Ben Johnson for his many useful questions and comments on our
   code. This research has been supported in part by NSF IIS-1149837, DARPA
   SIMPLEX, Stanford Data Science Initiative, Huawei, and Chan Zuckerberg
   Biohub. W.L.H. was also supported by the SAP Stanford Graduate
   Fellowship and an NSERC PGS-D grant. The views and conclusions expressed
   in this material are those of the authors and should not be interpreted
   as necessarily representing the official policies or endorsements,
   either expressed or implied, of the above funding agencies,
   corporations, or the U.S. and Canadian governments.
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Arora  Sanjeev, 2017, ICLR
   Benson AR, 2016, SCIENCE, V353, P163, DOI 10.1126/science.aad9029
   Bruna J., 2014, ICLR
   Cao S., 2015, KDD
   Chen J., 2017, ARXIV171010568
   Dai H., 2016, ICML
   Defferrard M., 2016, NIPS
   Duvenaud David K, 2015, NIPS
   Gori M, 2005, IEEE IJCNN, P729
   Grover A., 2016, KDD
   Hamilton W. L., 2016, ACL
   He K., 2016, EACV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Kingma D. P., 2015, ICLR
   Kipf T. N., 2016, ICLR
   Kipf Thomas N., 2016, NIPS WORKSH BAYES DE
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Levy O., 2014, NIPS
   Li Y., 2015, ICLR
   Mikolov T., 2013, NIPS
   NG A., 2001, NIPS
   Niepert M., 2016, ICML
   Page  L, 1999, TECHNICAL REPORT
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pennington Jeffrey, 2014, EMNLP
   Perozzi B., 2014, KDD
   Qi C. R., 2017, CVPR
   Rehurek  R., 2010, LREC
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Siegal S., 1956, NONPARAMETRIC STAT B
   Subramanian A, 2005, P NATL ACAD SCI USA, V102, P15545, DOI 10.1073/pnas.0506580102
   Tang J., 2015, WWW
   Wang Daixin, 2016, KDD
   Wang X., 2017, AAAI
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Xu L., 2017, WWW
   Yang  Z., 2016, ICML
   Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252
NR 41
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401007
DA 2019-06-15
ER

PT S
AU Hashimoto, TB
   Duchi, JC
   Liang, P
AF Hashimoto, Tatsunori B.
   Duchi, John C.
   Liang, Percy
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised Transformation Learning via Convex Relaxations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors. On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images. Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.
C1 [Hashimoto, Tatsunori B.; Duchi, John C.; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA.
RP Hashimoto, TB (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM thashim@cs.stanford.edu; jduchi@cs.stanford.edu; pliang@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF-CAREER award [1553086]; DARPA [N66001-14-2-4055]; DAIS ITA program
   [W911NF-16-3-0001]
FX We thank Arun Chaganty for helpful discussions and comments. This work
   was supported by NSF-CAREER award 1553086, DARPA (Grant
   N66001-14-2-4055), and the DAIS ITA program (W911NF-16-3-0001).
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406090
DA 2019-06-15
ER

PT S
AU Hassani, H
   Soltanolkotabi, M
   Karbasi, A
AF Hassani, Hamed
   Soltanolkotabi, Mahdi
   Karbasi, Amin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gradient Methods for Submodular Maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHM
AB In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor 1/2 approximation to the global maxima. We also study stochastic gradient methods and show that after O(1/epsilon(2)) iterations these methods reach solutions which achieve in expectation objective values exceeding (OPT/2 - epsilon). An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor 1/2 approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient ascent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.
C1 [Hassani, Hamed] Univ Penn, ESE Dept, Philadelphia, PA 19104 USA.
   [Soltanolkotabi, Mahdi] Univ Southern Calif, EE Dept, Los Angeles, CA USA.
   [Karbasi, Amin] Yale Univ, ECE Dept, New Haven, CT USA.
RP Hassani, H (reprint author), Univ Penn, ESE Dept, Philadelphia, PA 19104 USA.
EM hassani@seas.upenn.edu; soltanol@usc.edu; amin.karbasi@yale.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA [YFA D16AP00046]
FX This work was done while the authors were visiting the Simon's Institute
   for the Theory of Computing. A. K. is supported by DARPA YFA D16AP00046.
   The authors would like to thank Jeff Bilmes, Volkan Cevher, Chandra
   Chekuri, Maryam Fazel, Stefanie Jegelka, Mohammad-Reza Karimi, Andreas
   Krause, Mario Lucic, and Andrea Montanari for helpful discussions.
CR Bach F., 2015, ARXIV151100394
   Bian A., 2016, ARXIV160605615
   BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5
   Calinescu G., 2011, SIAM J COMPUTING
   Chakrabarty D., 2017, STOC
   Chekuri C., 2011, P 43 ACM S THEOR COM
   Chekuri C, 2011, ACM S THEORY COMPUT, P783
   Chekuri Chandra, 2015, P 2015 C INN THEOR C, P201
   Das  A., 2011, ICML
   Djolonga J., 2014, NIPS
   Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [DOI 10.1007/BF01584082, 10.1007/BF01584082]
   Eghbali R, 2016, ADV NEURAL INFORM PR, V29, P3287
   El-Arini K., 2009, KDD
   Ene A., 2016, ARXIV160608362
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Gomez R. M., 2010, P KDD
   Gottschalk C., 2015, INT WORKSH APPR ONL
   Guestrin Carlos, 2005, ICML
   Hassani H., 2017, ARXIV170803949
   Iyer R., 2015, ARTIFICIAL INTELLIGE
   Karimi M., 2017, STOCHASTIC SUBMODULA
   Kempe D., 2003, KDD
   Kim Been, 2016, NIPS
   Leskovec  J., 2007, KDD
   Lin H., 2011, P ANN M ASS COMP LIN
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Mirzasoleiman Baharan, 2016, ICML
   Mirzasoleiman Baharan, 2013, NIPS
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Oymak S., 2015, ARXIV150704793
   PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748
   Singla A., 2014, ICML
   Soltanolkotabi M, 2017, ARXIV170206175
   Soltanolkotabi Mahdi, 2017, ARXIV170504591
   Soma T., 2014, ICML
   Soma T., 2015, NIPS
   Stan S. A., 2017, ICML
   WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435
NR 38
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405089
DA 2019-06-15
ER

PT S
AU Haupt, J
   Li, XG
   Woodruff, DP
AF Haupt, Jarvis
   Li, Xingguo
   Woodruff, David P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Near Optimal Sketching of Low-Rank Tensor Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DESCENT METHOD; DECOMPOSITIONS
AB We study the least squares regression problem
   min(Theta is an element of Rp1)(X...XpD)parallel to A(Theta) - b parallel to(2)(2),
   where Theta is a low-rank tensor, defined as Theta = Sigma(R)(r=1) theta((r))(1)circle...circle theta((r))(D), for vectors theta((r))(d)is an element of R-pd for all r is an element of [R] and d is an element of [D]. Here, circle denotes the outer product of vectors, and A(Theta) is a linear function on Theta. This problem is motivated by the fact that the number of parameters in Theta is only R. Sigma(D)(d=1)pd, which is significantly smaller than the Pi(D)(d=1)pd number of parameters in ordinary least squares regression.We consider the above CP decomposition model of tensors Theta, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on sparse random projections Phi is an element of R-mxn, with m << n, to reduce the problem to a much smaller problem min(Theta)parallel to Phi A(Theta) - Phi b parallel to(2)(2), for which parallel to Phi A(Theta) - Phi b parallel to(2 )(2)= (1 +/-epsilon)parallel to A(Theta) - b parallel to(2)(2) holds simultaneously for all Theta. We obtain a significantly smaller dimension and sparsity in the randomized linear mapping Phi than is possible for ordinary least squares regression. Finally, we give a number of numerical simulations supporting our theory.
C1 [Haupt, Jarvis; Li, Xingguo] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Li, Xingguo] Georgia Tech, Atlanta, GA 30332 USA.
   [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Li, XG (reprint author), Univ Minnesota, Minneapolis, MN 55455 USA.; Li, XG (reprint author), Georgia Tech, Atlanta, GA 30332 USA.
EM jdhaupt@umn.edu; lixx1661@umn.edu; dwoodruf@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU University of Minnesota
FX The authors acknowledge support from University of Minnesota Startup
   Funding and Doctoral Dissertation Fellowship from University of
   Minnesota.
CR Bahadori MT, 2014, ADV NEURAL INFORM PR, P3491
   Bourgain J, 2015, GEOM FUNCT ANAL, V25, P1009, DOI 10.1007/s00039-015-0332-9
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Dasgupta A, 2010, ACM S THEORY COMPUT, P341
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Dirksen Sjoerd, 2015, FDN COMPUTATIONAL MA, P1
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Guo WW, 2012, IEEE T IMAGE PROCESS, V21, P816, DOI 10.1109/TIP.2011.2165291
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Haupt Jarvis, 2017, ARXIX170907093
   Hoff PD, 2015, ANN APPL STAT, V9, P1169, DOI 10.1214/15-AOAS839
   Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Li BX, 2013, CALCOLO, V50, P69, DOI 10.1007/s10092-012-0058-0
   Li X, 2013, ARXIV13045637
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]
   Nelson J, 2014, LECT NOTES COMPUT SC, V8572, P883
   Park SW, 2007, IEEE T SYST MAN CY B, V37, P1156, DOI 10.1109/TSMCB.2007.904575
   Raskutti G, 2015, ARXIV151201215
   Romera-Paredes B., 2013, P 30 INT C MACH LEAR, V28, P1444
   Rosset A, 2004, J DIGIT IMAGING, V17, P205, DOI 10.1007/s10278-004-1014-6
   Shen Zhongmin, 2001, LECT FINSLER GEOMETR, V2001
   Sidiropoulos N., 2017, IEEE T SIGNAL PROCES
   Talagrand M., 2006, GENERIC CHAINING UPP
   Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0
   Vershynin  Roman, 2010, ARXIV10113027
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yang Y., 2016, ARXIV160506391
   Yu Rose, 2015, INT C MACH LEARN
   Yu  Rose, 2016, INT C MACH LEARN, P373
   Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254
   Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499
   Zhou Hua, 2013, MATLAB TENSORREG TOO
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403052
DA 2019-06-15
ER

PT S
AU Hauser, M
   Ray, A
AF Hauser, Michael
   Ray, Asok
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Principles of Riemannian Geometry in Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective, the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are finite difference approximations to dynamical systems of first order differential equations, as opposed to ordinary networks that are static. This implies that the network is learning systems of differential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric fibre space in the principal and associated bundles on the data manifold. Toy experiments were run to confirm parts of the proposed theory, as well as to provide intuitions as to how neural networks operate on data.
C1 [Hauser, Michael; Ray, Asok] Penn State Univ, Dept Mech Engn, State Coll, PA 16801 USA.
RP Hauser, M (reprint author), Penn State Univ, Dept Mech Engn, State Coll, PA 16801 USA.
EM mzh190@psu.edu; axr2@psu.edu
RI Jeong, Yongwook/N-7413-2016
FU U.S. Air Force Of<SUP>f</SUP>ice of Scientific Research (AFOSR)
   [FA9550-15-1-0400]; PSU/ARL Walker Fellowship
FX This work has been supported in part by the U.S. Air Force
   Of<SUP>f</SUP>ice of Scientific Research (AFOSR) under Grant No.
   FA9550-15-1-0400. The first author has been supported by PSU/ARL Walker
   Fellowship. Any opinions,findings and conclusions or recommendations
   expressed in this publication are those of the authors and do not
   necessarily reflect the views of the sponsoring agencies.
CR Amari S.-i., 2007, METHODS INFORM GEOME, V191
   Bastien F, 2012, ARXIV12115590
   Bengio Y., 2013, ICML, P552
   Boulch Alexandre, 2017, ARXIV170208782
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Graves A., 2014, ARXIV14105401
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J., 2003, INTRO SMOOTH MANIFOL, P1
   Mikolov T., 2013, P NAACL 2013, P746
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rumelhart D. E., 1985, LEARNING INTERNAL RE
   Theano Development Team, 2016, ABS160502688 ARXIV T
   Veit A., 2016, ADV NEURAL INFORM PR, P550
   Walschap Gerard, 2012, METRIC STRUCTURES DI, V224
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 19
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402083
DA 2019-06-15
ER

PT S
AU Hausman, K
   Chebotar, Y
   Schaal, S
   Sukhatme, G
   Lim, JJ
AF Hausman, Karol
   Chebotar, Yevgen
   Schaal, Stefan
   Sukhatme, Gaurav
   Lim, Joseph J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Modal Imitation Learning from Unstructured Demonstrations using
   Generative Adversarial Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at http://sites.google.com/view/nips17intentiongan.
C1 [Hausman, Karol; Chebotar, Yevgen; Schaal, Stefan; Sukhatme, Gaurav; Lim, Joseph J.] Univ Southern Calif, Los Angeles, CA 90007 USA.
   [Chebotar, Yevgen; Schaal, Stefan] Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Hausman, K (reprint author), Univ Southern Calif, Los Angeles, CA 90007 USA.
EM hausman@usc.edu; ychebota@usc.edu; sschaal@usc.edu; gaurav@usc.edu;
   limjj@usc.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [IIS-1205249, IIS-1017134, EECS-0926052];
   Office of Naval Research; Okawa Foundation; Max-Planck-Society
FX This research was supported in part by National Science Foundation
   grants IIS-1205249, IIS-1017134, EECS-0926052, the Office of Naval
   Research, the Okawa Foundation, and the Max-Planck-Society. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the author(s) and do not necessarily reflect the
   views of the funding organizations.
CR Abbeel P., 2004, P ICML
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Arjovsky M., 2017, ARXIV170107875
   Babes M., 2011, P 28 INT C MACH LEAR, P897
   Billard A., 2008, SPRINGER HDB ROBOTIC, P1371, DOI DOI 10.1007/978-3-540-30301-5_60
   Chebotar Y., 2016, ARXIV161000529
   Chen X, 2016, INFOGAN INTERPRETABL
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Dimitrakakis C., 2011, EUR WORKSH REINF LEA, P273
   Duan Yan, 2017, ARXIV170307326
   Finn  C., 2016, P 33 INT C MACH LEAR, V48
   Finn C., 2016, ARXIV161103852
   Florensa Carlos, 2017, ARXIV170403012
   Fox  Roy, 2017, ARXIV170308294
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Ho Jonathan, 2016, ABS160603476 CORR
   Kalakrishnan M, 2011, IEEE INT C INT ROBOT, P4639, DOI 10.1109/IROS.2011.6048825
   Kim  T., 2017, P 34 INT C MACH LEAR, P1857
   Kroemer O, 2015, IEEE INT CONF ROBOT, P1503, DOI 10.1109/ICRA.2015.7139389
   Levine S., 2011, ADV NEURAL INFORM PR, P19
   Li Yunzhu, 2017, ABS170308840 CORR
   Mathieu M, 2015, ARXIV151105440
   Mulling K, 2013, INT J ROBOT RES, V32, P263, DOI 10.1177/0278364912472380
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Niekum  S., 2013, ROBOTICS SCI SYSTEMS, V9
   Pfau David, 2016, ARXIV161001945
   Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88
   Ross S., 2010, AISTATS, P3
   Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sonderby Casper Kaae, 2016, ABS161004490 CORR
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Vezhnevets Alexander Sasha, 2017, ARXIV170301161
   Zhu J Y, 2017, ARXIV170310593
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401027
DA 2019-06-15
ER

PT S
AU Havrylov, S
   Titov, I
AF Havrylov, Serhii
   Titov, Ivan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Emergence of Language with Multi-agent Games: Learning to Communicate
   with Sequences of Symbols
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SIMULATION
AB Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.
C1 [Havrylov, Serhii; Titov, Ivan] Univ Edinburgh, Sch Informat, ILCC, Edinburgh, Midlothian, Scotland.
   [Titov, Ivan] Univ Amsterdam, ILLC, Amsterdam, Netherlands.
RP Havrylov, S (reprint author), Univ Edinburgh, Sch Informat, ILCC, Edinburgh, Midlothian, Scotland.
EM s.havrylov@inf.ed.ac.uk; ititov@inf.ed.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU SAP ICN; ERC Starting Grant BroadSem [678254]; NWO Vidi Grant
   [639.022.518]
FX This project is supported by SAP ICN, ERC Starting Grant BroadSem
   (678254) and NWO Vidi Grant (639.022.518). We would like to thank Jelle
   Zuidema and anonymous reviewers for their helpful suggestions and
   comments.
CR [Anonymous], 2003, ADV NEURAL INFORM PR
   Baronchelli A, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/06/P06014
   Batali John, 1998, APPROACHES EVOLUTION, V405, P426
   Bengio Y., 2013, ARXIV13083432
   Bhatnagar S., 2012, STOCHASTIC RECURSIVE, V434
   Brighton H, 2002, ARTIF LIFE, V8, P25, DOI 10.1162/106454602753694756
   Byron D., 2009, P 12 EUR WORKSH NAT
   Chen X, 2015, CORR, V1504, P325
   Chrupala Grzegorz, 2015, P 53 ANN M ASS COMP
   Das A, 2017, INT CONF IMAG PROC
   Foerster  J., 2016, ADV NEURAL INFORM PR, V2016, P2137
   Golland D., 2010, P 2010 C EMP METH NA, P410
   Gulcehre  C., 2017, ARXIV170108718
   Henderson J, 2008, COMPUT LINGUIST, V34, P487, DOI 10.1162/coli.2008.07-028-R2-05-82
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jang E., 2017, P INT C LEARN REPR
   Jorge Emilio, 2016, NEURAL INFORM PROCES
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Kirby S, 2002, ARTIF LIFE, V8, P185, DOI 10.1162/106454602320184248
   Kocisky Tomas, 2016, ARXIV160909315
   Lazaridou Angeliki, 2017, P INT C LEARN REPR
   Lemon O, 2002, TRAITEMENT AUTOMATIQ, V43, P131
   Lewis D. K., 1969, CONVENTION PHILOS ST
   Maddison Chris J, 2017, P INT C LEARN REPR
   Miao Yishu, 2016, P C EMP METH NAT LAN
   Mikolov Tomas, 2015, NEURAL INFORM PROCES
   Mnih Andriy, 2014, P 31 INT C MACH LEAR
   Mordatch Igor, 2017, ARXIV170304908
   Nolfi S, 2010, EVOLUTION OF COMMUNICATION AND LANGUAGE IN EMBODIED AGENTS, P1, DOI 10.1007/978-3-642-01250-1
   Nowak MA, 1999, P NATL ACAD SCI USA, V96, P8028, DOI 10.1073/pnas.96.14.8028
   Polyak BT, 1973, PSEUDOGRADIENT ADAPT
   Schatzmann J, 2006, KNOWL ENG REV, V21, P97, DOI 10.1017/S0269888906000944
   Simonyan K., 2015, P INT C LEARN REPR
   Steels Luc, 2005, WHAT TRIGGERS EMERGE
   Sukhbaatar S, 2016, ADV NEURAL INFORM PR, V29, P2244
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wagner K, 2003, ADAPT BEHAV, V11, P37, DOI 10.1177/10597123030111003
   Werning M., 2011, OXFORD HDB COMPOSITI
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402020
DA 2019-06-15
ER

PT S
AU Hayashi, K
   Yoshida, Y
AF Hayashi, Kohei
   Yoshida, Yuichi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fitting Low-Rank Tensors in Constant Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee. Given an order-K tensor X is an element of R-N1x...xNK, our algorithm randomly samples a constant number s of indices for each mode and creates a "mini" tensor (X) over tilde is an element of R-sx...xs, whose elements are given by the intersection of the sampled indices on X. Then, we show that the residual error of the Tucker decomposition of (X) over tilde is sufficiently close to that of X with high probability. This result implies that we can figure out how much we can fit a low-rank tensor to X in constant time, regardless of the size of X. This is useful for guessing the favorable rank of Tucker decomposition. Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets.
C1 [Hayashi, Kohei] RIKEN AIP, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.
   [Yoshida, Yuichi] Natl Inst Informat, Tokyo, Japan.
RP Hayashi, K (reprint author), RIKEN AIP, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.
EM hayashi.kohei@gmail.com; yyoshida@nii.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU ONR [N62909-17-1-2138]; JSPS KAKENHI [JP17H04676]; JST ERATO, Japan
   [JPMJER1305]
FX Supported by ONR N62909-17-1-2138.; Supported by JSPS KAKENHI Grant
   Number JP17H04676 and JST ERATO Grant Number JPMJER1305, Japan.
CR Acar E, 2008, J CHEMOMETR, V22, P91, DOI 10.1002/cem.1106
   Acar E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-239
   AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Blankertz B, 2007, NEUROIMAGE, V37, P539, DOI 10.1016/j.neuroimage.2007.01.051
   Borgs C, 2008, ADV MATH, V219, P1801, DOI 10.1016/j.aim.2008.07.008
   Caiafa CF, 2010, LINEAR ALGEBRA APPL, V433, P557, DOI 10.1016/j.laa.2010.03.020
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023
   Frieze A, 1996, AN S FDN CO, P12, DOI 10.1109/SFCS.1996.548459
   Hayashi K., 2016, NIPS, P2217
   Lawaetz AJ, 2012, METABOLOMICS, V8, pS111, DOI 10.1007/s11306-011-0310-7
   Lovasz L., 2012, LARGE NETWORKS GRAPH
   Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Steele R. J., 2010, FRONTIERS STAT DECIS, P113
   Tsourakakis C. E., 2010, SDM, P689
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vezzani R, 2010, MULTIMED TOOLS APPL, V50, P359, DOI 10.1007/s11042-009-0402-9
   Watanabe S., 2009, ALGEBRAIC GEOMETRY S, V25
   Zhou G., 1885, ARXIV14121885
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402051
DA 2019-06-15
ER

PT S
AU Hayes, J
   Danezis, G
AF Hayes, Jamie
   Danezis, George
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Generating steganographic images via adversarial training
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.
C1 [Hayes, Jamie] UCL, London, England.
   [Danezis, George] UCL, Alan Turing Inst, London, England.
RP Hayes, J (reprint author), UCL, London, England.
EM j.hayes@cs.ucl.ac.uk; g.danezis@ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU UK Government Communications Headquarters (GCHQ), University College
   London; Google PhD Fellowship in Machine Learning
FX The authors would like to acknowledge financial support from the UK
   Government Communications Headquarters (GCHQ), as part of University
   College London's status as a recognised Academic Centre of Excellence in
   Cyber Security Research. Jamie Hayes is supported by a Google PhD
   Fellowship in Machine Learning. We thank the anonymous reviewers for
   their comments.
CR Abadi M, 2016, TENSORFLOW SYSTEM LA
   Abadi M., 2016, ARXIV160304467
   Abadi M., 2016, ARXIV161006918
   Borisenko Boris, 2016, ICLR
   Filler Tomas, 2009, IS T SPIE ELECT IMAG
   Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hayden M., 2014, PRICE PRIVACY REEVAL
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   LeCun Yann A., 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P9, DOI 10.1007/978-3-642-35289-8_3
   Lerch-Hostalot D, 2016, ENG APPL ARTIF INTEL, V50, P45, DOI 10.1016/j.engappai.2015.12.013
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Maas A. L., 2013, P ICML
   Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
NR 18
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402001
DA 2019-06-15
ER

PT S
AU Hazan, E
   Singh, K
   Zhang, C
AF Hazan, Elad
   Singh, Karan
   Zhang, Cyril
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Linear Dynamical Systems via Spectral Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.
C1 [Hazan, Elad; Singh, Karan; Zhang, Cyril] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
RP Hazan, E (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
EM fehazan@cs.princeton.edu; karans@cs.princeton.edu;
   cyril.zhang@cs.princeton.edu
RI Jeong, Yongwook/N-7413-2016
CR Audenaert Koenraad MR, 2014, ARXIV14104941
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Beckermann Bernhard, 2016, ARXIV160909494
   CHOI MD, 1983, AM MATH MON, V90, P301, DOI 10.2307/2975779
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Ghahramani Z., 1996, TECHNICAL REPORT
   GRUNBAUM FA, 1982, LINEAR ALGEBRA APPL, V43, P119, DOI 10.1016/0024-3795(82)90247-6
   Hardt Moritz, 2016, ARXIV160905191
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hilbert D, 1894, ACTA MATH, V18, P155
   Huang WB, 2016, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2016.427
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Ljung L, 2002, CIRC SYST SIGNAL PR, V21, P11, DOI 10.1007/BF01211648
   Ljung L., 1998, SYSTEM IDENTIFICATIO
   Martens J., 2010, P 27 INT C MACH LEAR, P743
   Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674
   Schur J, 1911, J REINE ANGEW MATH, V140, P1
   Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x
   SLEPIAN D, 1978, BELL SYST TECH J, V57, P1371, DOI 10.1002/j.1538-7305.1978.tb02104.x
   Van Overschee P, 2012, SUBSPACE IDENTIFICAT
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406074
DA 2019-06-15
ER

PT S
AU He, D
   Lu, HQ
   Xia, YC
   Qin, T
   Wang, LW
   Liu, TY
AF He, Di
   Lu, Hanqing
   Xia, Yingce
   Qin, Tao
   Wang, Liwei
   Liu, Tie-Yan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Decoding with Value Networks for Neural Machine Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence x, the currently available decoding output y(1), ...,y(t-1) and a candidate word w at step t as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network value network. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when choosing a word w for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks.
C1 [He, Di; Wang, Liwei] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China.
   [Lu, Hanqing] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Xia, Yingce] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Qin, Tao; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA.
   [Wang, Liwei] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China.
RP He, D (reprint author), Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China.
EM di_he@pku.edu.cn; hanqinglu@cmu.edu; xiayingc@mail.ustc.edu.cn;
   taoqin@microsoft.com; wanglw@cis.pku.edu.cn; tie-yan.liu@microsoft.com
RI Jeong, Yongwook/N-7413-2016
FU National Basic Research Program of China (973 Program) [2015CB352502];
   NSFC [61573026]
FX This work was partially supported by National Basic Research Program of
   China (973 Program) (grant no. 2015CB352502), NSFC (61573026). We would
   like to thank the anonymous reviewers for their valuable comments on our
   paper.
CR Bahdanau Dzmitry, 2017, ICLR
   Bandanau D., 2015, ICLR
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Jean S, 2015, P 53 ANN M ASS COMP, P1
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Ranzato Marc Aurelio, 2016, ICLR
   Shen S., 2016, ACL
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215
   Tu Z., 2017, AAAI, P3097
   Tu Zhaopeng, 2016, CORR
   Wiseman S., 2016, EMNLP
   Wu  Lijun, 2017, ARXIV170406933
   Wu Y., 2016, ARXIV160908144
   Xia  Y., 2016, ADV NEURAL INFORM PR, P820
   Xia Y., 2017, 31 ANN C NEUR INF PR
   Xia Yingce, 2017, ICML
   Zeiler M.D., 2012, ARXIV12125701
   Zhou J., 2016, ARXIV160604199
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400017
DA 2019-06-15
ER

PT S
AU He, H
   Xin, B
   Ikehata, S
   Wipf, D
AF He, Hao
   Xin, Bo
   Ikehata, Satoshi
   Wipf, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI From Bayesian Sparsity to Gated Recurrent Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights. This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.
C1 [He, Hao] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Xin, Bo; Wipf, David] Microsoft Res, Beijing, Peoples R China.
   [Ikehata, Satoshi] Natl Inst Informat, Tokyo, Japan.
RP He, H (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM haohe@mit.edu; jimxinbo@gmail.com; satoshi.ikehata@gmail.com;
   davidwipf@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Andrychowicz M., 2016, ARXIV160604474
   [Anonymous], 2010, J SELECTED TOPICS SI, V4
   Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Blumensath T., 2010, IEEE J SELECTED TOPI, V4
   Blumensath T., 2009, APPL COMPUTATIONAL H, V27
   BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Cho  K., 2014, C EMP METH NAT LANG
   Chung J Y, 2015, INT C MACH LEARN
   Cotter S. F., 2002, IEEE T COMMUNICATION, V50
   Dai J., 2017, IEEE SIGNAL PROCESSI, V24
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fan J., 2001, J AM STAT ASS, V96
   Figueiredo M. A. T., 2002, NIPS
   Gers F. A., 2000, INT JOINT C NEUR NET
   Gerstoft P., 2016, IEEE SIGNAL PROCESSI, V23
   Gregor K., 2010, ICML
   Hochreiter  S, 1997, NEURAL COMPUTATION, V9
   Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836
   Ikehata S., 2012, COMPUTER VISION PATT
   Ikehata S, 2014, IEEE T PATTERN ANAL, V36, P1816, DOI 10.1109/TPAMI.2014.2299798
   Koutnik J., 2014, INT C MACH LEARN
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415
   Malioutov D. M., 2005, IEEE T SIGNAL PROCES, V53
   Manolakis D.G., 2000, STAT ADAPTIVE SIGNAL
   Nair V, 2010, INT C MACH LEARN
   Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779
   Sriperumbudu B. K., 2012, NEURAL COMPUTATION, V24
   Tibshirani R., 1996, J ROYAL STAT SOC
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Wang Z., 2016, AAAI C ART INT
   Wipf D. P., 2012, ADV NERUAL INFORM PR, V24
   Woodham R. J., 1980, OPTICAL ENG, V19
   Xin B, 2016, ADV NEUR IN, V29
   Yang Z, 2013, IEEE T SIGNAL PROCES, V61, P38, DOI 10.1109/TSP.2012.2222378
   Zamir A. R., 2016, ARXIV161209508
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405062
DA 2019-06-15
ER

PT S
AU He, Z
   Gao, SB
   Xiao, L
   Liu, DX
   He, HG
   Barber, D
AF He, Zhen
   Gao, Shaobing
   Xiao, Liang
   Liu, Daxue
   He, Hangen
   Barber, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.
C1 [He, Zhen; Barber, David] UCL, London, England.
   [He, Zhen; Xiao, Liang; Liu, Daxue; He, Hangen] Natl Univ Def Technol, Changsha, Hunan, Peoples R China.
   [Gao, Shaobing] Sichuan Univ, Chengdu, Sichuan, Peoples R China.
   [Barber, David] Alan Turing Inst, London, England.
RP He, Z (reprint author), UCL, London, England.; He, Z (reprint author), Natl Univ Def Technol, Changsha, Hunan, Peoples R China.; Gao, SB (reprint author), Sichuan Univ, Chengdu, Sichuan, Peoples R China.
EM hezhen.cs@gmail.com; gaoshaobing@scu.edu.cn
RI Xiao, Liang/A-5160-2016; Jeong, Yongwook/N-7413-2016
OI Xiao, Liang/0000-0001-6959-4343; 
FU NSFC [91220301]; Alan Turing Institute under the EPSRC [EP/N510129/1];
   China Scholarship Council
FX This work is supported by the NSFC grant 91220301, the Alan Turing
   Institute under the EPSRC grant EP/N510129/1, and the China Scholarship
   Council.
CR Appleyard Jeremy, 2016, ARXIV160401946
   Arjovsky M., 2016, ICML
   Ba J. L., 2016, ARXIV160706450
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2009, LEARNING DEEP ARCHIT
   Bertinetto L., 2016, NIPS
   Bradbury J., 2017, ICLR
   Chang Shiyu, 2017, NIPS
   Chen J., 2016, NIPS
   Chung J., 2015, ICML
   Chung Junyoung, 2017, ICLR
   Collobert R, 2011, NIPS WORKSH
   Cooijmans T., 2017, ICLR
   De Brabandere  B., 2016, NIPS
   Denil  Misha, 2013, NIPS
   Diamos Greg, 2016, ICML
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Garipov Timur, 2016, NIPS WORKSHOP
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Graves A, 2013, ARXIV13080850
   Graves A., 2013, ICASSP
   Graves A., 2016, ARXIV160308983
   Ha D., 2017, ICLR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hutter  M., 2012, HUMAN KNOWLEDGE COMP
   Irsoy Ozan, 2015, ICLR
   Jozefowicz Rafal, 2015, ICML
   Kaiser Lukasz, 2016, NIPS
   Kaiser Lukasz, 2016, ICLR
   Kalchbrenner N., 2016, ICLR
   Kingma D. P., 2015, ICLR
   Krause Ben, 2017, ICLR WORKSH
   Le Quoc V, 2015, ARXIV150400941
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lei T., 2017, ARXIV170902755
   Leifert G, 2016, J MACH LEARN RES, V17
   Mujika Asier, 2017, NIPS
   Novikov A., 2015, NIPS
   Patraucean  V., 2016, ICLR WORKSH
   Romera-Paredes B., 2016, ECCV
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Shi X., 2015, NIPS
   Stollenga Marijn F, 2015, NIPS
   Sutskever I., 2011, ICML
   Taylor G., 2009, ICML
   Van Den Oord A., 2016, ARXIV160903499
   van den Oord A., 2016, ICML
   Wisdom S., 2016, NIPS
   Wu L., 2016, ARXIV160601609
   Wu Yuhuai, 2016, NIPS
   Zhang Saizheng, 2016, NIPS
   Zilly J. G., 2017, ICML
NR 54
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400001
DA 2019-06-15
ER

PT S
AU Heikkila, M
   Lagerspetz, E
   Kaski, S
   Shimizu, K
   Tarkoma, S
   Honkela, A
AF Heikkila, Mikko
   Lagerspetz, Eemil
   Kaski, Samuel
   Shimizu, Kana
   Tarkoma, Sasu
   Honkela, Antti
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Differentially private Bayesian learning on distributed data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NOISE
AB Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.
C1 [Heikkila, Mikko; Honkela, Antti] Univ Helsinki, Helsinki Inst Informat Technol, Dept Math & Stat, Helsinki, Finland.
   [Lagerspetz, Eemil; Tarkoma, Sasu] Univ Helsinki, HIIT, Dept Comp Sci, Helsinki, Finland.
   [Kaski, Samuel] Aalto Univ, Helsinki Inst Informat Technol, Dept Comp Sci, Helsinki, Finland.
   [Shimizu, Kana] Waseda Univ, Dept Comp Sci & Engn, Tokyo, Japan.
   [Honkela, Antti] Univ Helsinki, Dept Publ Hlth, Helsinki, Finland.
RP Heikkila, M (reprint author), Univ Helsinki, Helsinki Inst Informat Technol, Dept Math & Stat, Helsinki, Finland.
EM mikko.a.heikkila@helsinki.fi; eemil.lagerspetz@helsinki.fi;
   samuel.kaski@aalto.fi; shimizu.kana.g@gmail.com;
   sasu.tarkoma@helsinki.fi; antti.honkela@helsinki.fi
RI Jeong, Yongwook/N-7413-2016; Kaski, Samuel/B-6684-2008
OI Kaski, Samuel/0000-0003-1925-9154; Lagerspetz, Eemil/0000-0003-3875-8135
FU Academy of Finland [Centre of Excellence COIN] [259440, 278300, 292334,
   294238, 297741, 303815, 303816]; Japan Agency for Medical Research and
   Development (AMED); JST CREST [JPMJCR1688]
FX This work was funded by the Academy of Finland [Centre of Excellence
   COIN and projects 259440, 278300, 292334, 294238, 297741, 303815,
   303816], the Japan Agency for Medical Research and Development (AMED),
   and JST CREST [JPMJCR1688].
CR Abadi M., 2016, P CCS 2016
   Acs Gergely, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P118, DOI 10.1007/978-3-642-24178-9_9
   Chan T.-H. H., 2012, FINANCIAL CRYPTOGRAP, P200, DOI DOI 10.1007/978-3-642-32946-3_15
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016
   Dimitrakakis C, 2017, J MACH LEARN RES, V18
   Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Eigner F., 2014, P 30 ANN COMP SEC AP, P316
   Foulds J, 2016, P UAI 2016, P192
   Goryczka S., 2015, IEEE T DEPENDABLE SE
   Hamm J., 2016, ICML
   Honkela A, 2016, ARXIV160602109STATML
   Jalko Joonas, 2017, P 33 C UNC ART INT U
   Lichman M., 2013, UCI MACHINE LEARNING
   Park M., 2016, ARXIV161100340
   Pathak  M., 2010, ADV NEURAL INFORM PR, P1876
   Rajkumar A., 2012, P 15 INT C ART INT S, V22, P933
   Rastogi V., 2010, P ACM SIGMOD INT C M, P735, DOI DOI 10.1145/1807167.1807247
   Shi Elaine, 2011, P NDSS
   Smith A., 2008, ARXIV08094794CSCR
   Wang Y.-X., 2015, P 32 INT C MACH LEAR, P2493
   Williams O., 2010, ADV NEURAL INF PROCE, V23
   Wu GQ, 2016, IEEE TRUST BIG, P921, DOI [10.1109/TrustCom.2016.0157, 10.1109/TrustCom.2016.351]
   Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253
   Zhang Z., 2016, P AAAI 2016
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403029
DA 2019-06-15
ER

PT S
AU Hein, M
   Andriushchenko, M
AF Hein, Matthias
   Andriushchenko, Maksym
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Formal Guarantees on the Robustness of a Classifier against Adversarial
   Manipulation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier with no or small loss in prediction performance.
C1 [Hein, Matthias; Andriushchenko, Maksym] Saarland Univ, Dept Math & Comp Sci, Saarbrucken Informat Campus, Saarbrucken, Germany.
RP Hein, M (reprint author), Saarland Univ, Dept Math & Comp Sci, Saarbrucken Informat Campus, Saarbrucken, Germany.
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Bastani Osbert, 2016, NIPS
   Carlini Nicholas, 2017, ACM WORKSH ART INT S
   Cisse  Moustapha, 2017, ICML
   Dalvi N., 2004, KDD
   Drucker H., 1992, IJCNN
   Goodfellow I., 2015, ICLR
   Gu Shixiang, 2015, ICLR WORKSH
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Helmbold DP, 2015, J MACH LEARN RES, V16, P3403
   Hochreiter S., 1995, NIPS
   Huang  R., 2016, ICLR
   Kos Jernej, 2017, ICLR WORKSH
   Kurakin  Alexey, 2017, ICLR WORKSH
   Liu Y., 2017, ICLR
   Lowd D., 2005, KDD
   Moosavi-Dezfooli S. M., 2017, CVPR
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Scholkopf B., 2002, LEARNING KERNELS
   Shaham U., 2016, NIPS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Szegedy C., 2014, ICLR, P2503
   Zagoruyko S., BMVC
   Zheng  S., 2016, CVPR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402031
DA 2019-06-15
ER

PT S
AU Hensel, M
   Ramsauer, H
   Unterthiner, T
   Nessler, B
   Hochreiter, S
AF Hensel, Martin
   Ramsauer, Hubert
   Unterthiner, Thomas
   Nessler, Bernhard
   Hochreiter, Sepp
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
   Equilibrium
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STOCHASTIC-APPROXIMATION; HEAVY BALL; DISTANCE; GRADIENT
AB Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Frechet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.
C1 [Hensel, Martin] Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.
   Johannes Kepler Univ Linz, Inst Bioinformat, A-4040 Linz, Austria.
RP Hensel, M (reprint author), Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.
EM mhe@bioinf.jku.at; ramsauer@bioinf.jku.at; unterthiner@bioinf.jku.at;
   nessler@bioinf.jku.at; hochreit@bioinf.jku.at
RI Jeong, Yongwook/N-7413-2016; Unterthiner, Thomas/K-7231-2018
OI Unterthiner, Thomas/0000-0001-5361-3087
FU NVIDIA Corporation; Bayer AG [09/2017]; Zalando SE [01/2016]; Audi.JKU
   Deep Learning Center; Audi Electronic Venture GmbH; IWT research grant
   [IWT150865]; H2020 project grant [671555]; FWF [P 28660-N31]
FX This work was supported by NVIDIA Corporation, Bayer AG with Research
   Agreement 09/2017, Zalando SE with Research Agreement 01/2016, Audi.JKU
   Deep Learning Center, Audi Electronic Venture GmbH, IWT research grant
   IWT150865 (Exaptation), H2020 project grant 671555 (ExCAPE) and FWF
   grant P 28660-N31.
CR Arjovsky M., 2017, ARXIV170107875
   Arora S., 2017, P 34 INT C MACH LEAR, V70, P224
   Attouch H, 2000, COMMUN CONTEMP MATH, V2, P1, DOI 10.1142/S0219199700000025
   Berthelot D., 2017, ARXIV170310717
   Bertsekas DP, 2000, SIAM J OPTIMIZ, V10, P627, DOI 10.1137/S1052623497331063
   Bhatnagar S., 2013, LECT NOTES CONTROL I
   Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3
   Borkar VS, 2000, SIAM J CONTROL OPTIM, V38, P447, DOI 10.1137/S0363012997331639
   Che T., 2017, P INT C LEARN REPR I
   Chelba C., 2013, ARXIV13123005
   Clevert D. - A., 2016, P INT C LEARN REPR I
   Di Castro D, 2010, J MACH LEARN RES, V11, P367
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   FRECHET M, 1957, CR HEBD ACAD SCI, V244, P689
   Gadat S., 2016, ARXIV160904228
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2015, ARXIV14126515
   Goodfellow I. J., 2017, ARXIV170100160
   Goudou X, 2009, MATH PROGRAM, V116, P173, DOI 10.1007/s10107-007-0109-5
   Grnarova P., 2017, ARXIV170603269
   HIRSCH MW, 1989, NEURAL NETWORKS, V2, P331, DOI 10.1016/0893-6080(89)90018-X
   Hjelm RD, 2017, ARXIV170208431
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P., 2017, P IEEE C COMP VIS PA
   Karmakar P., 2017, MATH OPERATIONS RES
   Kingma D., 2015, P INT C LEARN REPR I
   Konda V., 2002, THESIS
   Konda VR, 2003, SYST CONTROL LETT, V50, P95, DOI 10.1016/S0167-6911(03)00132-4
   Kushner H.J., 2003, STOCHASTIC APPROXIMA
   Ledig C, 2016, ARXIV160904802
   Li CL, 2017, ADV NEUR IN, V30
   Li J., 2017, ARXIV170609884
   Lim J. H., 2017, ARXIV170502894
   Liu S., 2017, ADV NEURAL INFORM PR, V31
   Mescheder L, 2017, ADV NEUR IN, V30
   Metz L., 2017, P INT C LEARN REPR I
   Mroueh Y, 2017, ADV NEUR IN, V30
   Nagarajan V, 2017, ADV NEUR IN, V30
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Prasad HL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1371
   Radford  A., 2016, P INT C LEARN REPR I
   Ramaswamy A, 2016, STOCHASTICS, V88, P1173, DOI 10.1080/17442508.2016.1215450
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Theis L., 2016, P INT C LEARN REPR I
   Tolstikhin I, 2017, ADV NEUR IN, V30
   Vasershtein L. N., 1969, PROBL INFORM TRANSM, V5, P47
   Wang Ruohan, 2017, ARXIV170403817
   Wu Y, 2017, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON RELIABILITY SYSTEMS ENGINEERING (ICRSE 2017)
   Zhang JS, 2007, IEEE INFOCOM SER, P222, DOI 10.1109/INFCOM.2007.34
NR 50
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406067
DA 2019-06-15
ER

PT S
AU Hofer, C
   Kwitt, R
   Niethammer, M
   Uhl, A
AF Hofer, Christoph
   Kwitt, Roland
   Niethammer, Marc
   Uhl, Andreas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Learning with Topological Signatures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PERSISTENCE; SHAPES
AB Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.
C1 [Hofer, Christoph; Kwitt, Roland; Uhl, Andreas] Univ Salzburg, Dept Comp Sci, Salzburg, Austria.
   [Niethammer, Marc] Univ N Carolina, Chapel Hill, NC USA.
RP Hofer, C (reprint author), Univ Salzburg, Dept Comp Sci, Salzburg, Austria.
EM chofer@cosy.sbg.ac.at; roland.kwitt@sbg.ac.at; mn@cs.unc.edu;
   Uhl@cosy.sbg.ac.at
RI Jeong, Yongwook/N-7413-2016
FU Austrian Science Fund FWF [00012]; Spinal Cord Injury and Tissue
   Regeneration Center Salzburg (SCI-TReCS), Paracelsus Medical University,
   Salzburg
FX This work was partially funded by the Austrian Science Fund FWF (KLI
   project 00012) and the Spinal Cord Injury and Tissue Regeneration Center
   Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg.
CR ADAMS H, 2017, JMLR, V18
   Adcock A., 2013, CORR
   Bai X., 2009, ICCV WORKSH
   Barnett I., 2016, CORR
   Bendich P, 2016, ANN APPL STAT, V10, P198, DOI 10.1214/15-AOAS886
   Bubenik P, 2015, J MACH LEARN RES, V16, P77
   Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x
   Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X
   Chazal F., 2014, JOCG, V6, P140
   Chazal F, 2013, J ACM, V60, DOI 10.1145/2535927
   Chazal F, 2009, COMPUT GRAPH FORUM, V28, P1393, DOI 10.1111/j.1467-8659.2009.01516.x
   Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5
   Cohen-Steiner D, 2010, FOUND COMPUT MATH, V10, P127, DOI 10.1007/s10208-010-9060-6
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO
   Hatcher A, 2002, ALGEBRAIC TOPOLOGY
   He K., 2016, CVPR
   Krizhevsky A., 2012, NIPS
   Kusano G., 2016, ICML
   Kwitt R., 2015, NIPS
   Latecki L., 2000, CVPR
   Li C, 2014, CVPR
   Mischaikow K, 2013, DISCRETE COMPUT GEOM, V50, P330, DOI 10.1007/s00454-013-9529-6
   Niepert M., 2016, ICML
   Qi C. R., 2017, CVPR
   Ravanbakhsh S., 2017, ICLR
   Reininghaus R, 2015, CVPR
   Singh G, 2008, J VISION, V8, DOI 10.1167/8.8.11
   Turner K, 2014, INF INFERENCE, V3, P310, DOI 10.1093/imaiai/iau011
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008
   Yanardag P., 2015, KDD
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401065
DA 2019-06-15
ER

PT S
AU Hoffer, E
   Hubara, I
   Soudry, D
AF Hoffer, Elad
   Hubara, Itay
   Soudry, Daniel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Train longer, generalize better: closing the generalization gap in large
   batch training of neural networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ANOMALOUS DIFFUSION; MEDIA
AB Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem.
   Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.
C1 [Hoffer, Elad; Hubara, Itay; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel.
RP Hoffer, E (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM elad.hoffer@gmail.com; itayhubara@gmail.com; daniel.soudry@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU Taub Foundation; Intelligence Advanced Research Projects Activity
   (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC)
   [D16PC00003]
FX We wish to thank Nir Ailon, Dar Gilboa, Kfir Levy and Igor Berman for
   their feedback on the initial manuscript. The research was partially
   supported by the Taub Foundation, and the Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior/ Interior Business
   Center (DoI/IBC) contract number D16PC00003. The U.S. Government is
   authorized to reproduce and distribute reprints for Governmental
   purposes notwithstanding any copyright annotation thereon. Disclaimer:
   The views and conclusions contained herein are those of the authors and
   should not be interpreted as necessarily representing the official
   policies or endorsements, either expressed or implied, of IARPA,
   DoI/IBC, or the U.S. Government.
CR Amodei D., 2015, ARXIV151202595
   Bottou L, 1998, LECT NOTES COMPUTER, V1524
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   BOUCHAUD JP, 1987, J PHYS-PARIS, V48, P1445, DOI 10.1051/jphys:019870048090144500
   BOUCHAUD JP, 1990, PHYS REP, V195, P127, DOI 10.1016/0370-1573(90)90099-N
   Bray A. J., 2007, PHYS REV LETT, V98, P1
   Choromanska A., 2015, AISTATS15, V38
   Das  D., 2016, ARXIV160206709
   Dauphin Y., 2014, ARXIV14062572, P1
   Dauphin Y. N., 2015, ABS150204390 CORR
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Deng J., 2009, CVPR09
   Dinh L., 2017, ARXIV170304933
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   DURRETT R, 1986, COMMUN MATH PHYS, V104, P87, DOI 10.1007/BF01210794
   Ge R., 2015, P 28 C LEARN THEOR, P797
   GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219
   Goyal Priya, 2017, ARXIV170602677
   Hardt M., 2016, ICML, P1
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S., 2015, ARXIV150203167
   Keskar Nitish Shirish, 2017, ICLR
   Kingma D. P., 2014, ARXIV14126980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2014, ARXIV14045997
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Maofei, 2017, THESIS
   Li Mu, 2014, P 20 ACM SIGKDD INT, P661, DOI DOI 10.1145/2623330.2623612
   Luong M.T., 2015, ARXIV150804025
   MARINARI E, 1983, PHYS REV LETT, V50, P1223, DOI 10.1103/PhysRevLett.50.1223
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Montavon G., 2012, NEURAL NETWORKS TRIC
   Ruder S., 2016, CORR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K, 2014, ARXIV14091556
   Soudry D., 2017, ARXIV E PRINTS
   Soudry  D., 2017, ARXIV170205777
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wan L., 2013, ICML 13
   Wu Y., 2016, CORR
   You Yang, 2017, ARXIV170803888
   Zagoruyko K., 2016, BMVC
   Zhang Chiyuan, 2017, ICLR
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401074
DA 2019-06-15
ER

PT S
AU Hoshen, Y
AF Hoshen, Yedid
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI VAIN: Attentional Multi-agent Predictive Modeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.
C1 [Hoshen, Yedid] Facebook AI Res, NYC, New York, NY 10003 USA.
RP Hoshen, Y (reprint author), Facebook AI Res, NYC, New York, NY 10003 USA.
EM yedidh@fb.com
RI Jeong, Yongwook/N-7413-2016
CR Alahi  A., 2016, CVPR
   Amodei  D., 2016, ICML
   Battaglia Peter W., 2016, NIPS
   Bruna J., 2014, ICLR
   Campbell Murray, 2002, ARTIFICIAL INTELLIGE
   David Omid E, 2016, ICANN
   Duvenaud David K, 2015, NIPS
   Gori M., 2005, IJCNN
   Graves A., 2014, ARXIV14105401
   Hinton  G., 2012, IEEE SIGNAL PROCESSI
   Hochreiter S., 1997, NEURAL COMPUTATION
   Johnson J., 2016, ARXIV161206890
   Kaiser Lukasz, 2016, ICLR
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2012, NIPS
   Lai M., 2015, ARXIV150901549
   LeCun Y., 1989, NEURAL COMPUTATION
   Li Y., 2016, ICLR
   Peng P., 2017, ARXIV170310069
   Pettersen Svein Arne, 2014, P 5 ACM MULT SYST C, P18
   Qi C. R., 2017, CVPR
   Santoro A, 2017, ARXIV170601427
   Scarselli F., 2009, IEEE T NEURAL NETWOR
   Schroff F., 2015, CVPR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sukhbaatar S., 2015, NIPS
   Sukhbaatar Sainbayar, 2016, NIPS
   Taigman Y., 2014, CVPR
   Tesauro Gerald, 1990, IJCNN
   Tian Yuandong, 2016, ICLR
   Usunier Nicolas, 2017, ICLR
   Vaswani A., 2017, ARXIV170603762
   Weston J., 2014, ARXIV14103916
   Wu Y., 2016, ARXIV160908144
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402073
DA 2019-06-15
ER

PT S
AU Hou, BJ
   Zhang, LJ
   Zhou, ZH
AF Hou, Bo-Jian
   Zhang, Lijun
   Zhou, Zhi-Hua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning with Feature Evolvable Streams
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this paper, we propose a novel learning paradigm: Feature Evolvable Streaming Learning where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.
C1 [Hou, Bo-Jian; Zhang, Lijun; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
RP Hou, BJ (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM houbj@lamda.nju.edu.cn; zhanglj@lamda.nju.edu.cn;
   zhouzh@lamda.nju.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU NSFC [61333014, 61603177]; JiangsuSF [BK20160658]; Huawei Fund
   [YBN2017030027]; Collaborative Innovation Center of Novel Software
   Technology and Industrialization
FX This research was supported by NSFC (61333014, 61603177), JiangsuSF
   (BK20160658), Huawei Fund (YBN2017030027) and Collaborative Innovation
   Center of Novel Software Technology and Industrialization.
CR Aggarwal CC, 2006, IEEE T KNOWL DATA EN, V18, P577, DOI 10.1109/TKDE.2006.69
   Aggarwal CC, 2010, SCIENTIFIC DATA MINING AND KNOWLEDGE DISCOVERY: PRINCIPLES AND FOUNDATIONS, P377, DOI 10.1007/978-3-642-02788-8_14
   Amini M., 2009, ADV NEURAL INFORM PR, V22, P28
   Bifet A, 2010, J MACH LEARN RES, V11, P1601
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   de Andrade Silva J, ACM COMPUTING SURVEY
   Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gaber MM, 2005, SIGMOD REC, V34, P18, DOI 10.1145/1083784.1083789
   Gama J, 2009, STUD COMPUT INTELL, V206, P29
   Guan SU, 2001, NEURAL PROCESS LETT, V14, P241, DOI 10.1023/A:1012799113953
   Nguyen HL, 2015, KNOWL INF SYST, V45, P535, DOI 10.1007/s10115-014-0808-1
   Hai-Long Nguyen, 2012, Advances in Knowledge Discovery and Data Mining. Proceedings 16th Pacific-Asia Conference (PAKDD 2012), P1, DOI 10.1007/978-3-642-30220-6_1
   Hashemi S, 2009, IEEE T KNOWL DATA EN, V21, P624, DOI 10.1109/TKDE.2008.181
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hoi SCH, 2014, J MACH LEARN RES, V15, P495
   Hou C., 2016, ARXIV160509082
   Khalid S, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P372, DOI 10.1109/SAI.2014.6918213
   Kibria B. M. Golam, 2007, TECHNOMETRICS, V49, P230
   Leite Daniel F, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1736, DOI 10.1109/IJCNN.2009.5178895
   Li S.-Y., 2014, AAAI, P1968
   Muslea Ion, 2002, P 19 INT C MACH LEAR, V2, P435
   Oza NC, 2005, IEEE SYS MAN CYBERN, P2340
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Reed J., 2011, WAPA, V17, P19
   Seidl T., 2009, P 12 INT C EXT DAT T, P311
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Tsang I.W., 2007, P 24 INT C MACH LEAR, P911
   [王衬 Wang Chen], 2016, [中国海洋药物, Chinese Journal of Marine Drugs], V35, P1
   Wang Haixun, 2003, P 9 ACM SIGKDD INT C, P226, DOI DOI 10.1145/956750.956778
   Xu C., 2013, ARXIV13045634
   Zhang P., 2011, P 17 ACM SIGKDD INT, P177
   Zhao PL, 2014, ARTIF INTELL, V216, P76, DOI 10.1016/j.artint.2014.06.003
   Zhou G, 2012, INT C ART INT STAT, V22, P1453
   Zhou Z. - H., 2012, ENSEMBLE METHODS FDN
   Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401044
DA 2019-06-15
ER

PT S
AU Hoy, D
   Nekipelov, D
   Syrgkanis, V
AF Hoy, Darrell
   Nekipelov, Denis
   Syrgkanis, Vasilis
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Welfare Guarantees from Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Analysis of efficiency of outcomes in game theoretic settings has been a main item of study at the intersection of economics and computer science. The notion of the price of anarchy takes a worst-case stance to efficiency analysis, considering instance independent guarantees of efficiency. We propose a data-dependent analog of the price of anarchy that refines this worst-case assuming access to samples of strategic behavior. We focus on auction settings, where the latter is non-trivial due to the private information held by participants. Our approach to bounding the efficiency from data is robust to statistical errors and mis-specification. Unlike traditional econometrics, which seek to learn the private information of players from observed behavior and then analyze properties of the outcome, we directly quantify the inefficiency without going through the private information. We apply our approach to datasets from a sponsored search auction system and find empirical results that are a significant improvement over bounds from worst-case analysis.
C1 [Hoy, Darrell] Univ Maryland, College Pk, MD 20742 USA.
   [Nekipelov, Denis] Univ Virginia, Charlottesville, VA 22903 USA.
   [Syrgkanis, Vasilis] Microsoft Res, Redmond, WA USA.
RP Hoy, D (reprint author), Univ Maryland, College Pk, MD 20742 USA.
EM darrell.hoy@gmail.com; denis@virginia.edu; vasy@microsoft.com
CR Caragiannis Ioannis, 2014, BOUNDING INEFFICIENC, P1
   Edelman B, 2007, AM ECON REV, V97, P242, DOI 10.1257/aer.97.1.242
   Guerre E, 2000, ECONOMETRICA, V68, P525, DOI 10.1111/1468-0262.00123
   Hartline J., 2014, P 15 ACM C EC COMP, P693
   Kosorok M. R., 2007, INTRO EMPIRICAL PROC
   Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404
   Krishna V., 2002, AUCTION THEORY
   Paarsch HJ, 2006, INTRO STRUCTURAL ECO
   Pollard D., 1984, CONVERGENCE STOCHAST
   Roughgarden T, 2002, J ACM, V49, P236, DOI 10.1145/506147.506153
   Roughgarden Tim, 2016, ABS160707684 CORR
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Syrgkanis V., 2013, P 45 ANN ACM S THEOR, P211, DOI DOI 10.1145/2488608.2488635
   Varian HR, 2009, AM ECON REV, V99, P430, DOI 10.1257/aer.99.2.430
   VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403081
DA 2019-06-15
ER

PT S
AU Hsu, D
   Shi, K
   Sun, XR
AF Hsu, Daniel
   Shi, Kevin
   Sun, Xiaorui
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Linear regression without correspondence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. Finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.
C1 [Hsu, Daniel; Shi, Kevin] Columbia Univ, New York, NY 10027 USA.
   [Sun, Xiaorui] Microsoft Res, Redmond, WA USA.
RP Hsu, D (reprint author), Columbia Univ, New York, NY 10027 USA.
EM djhsu@cs.columbia.edu; kshi@cs.columbia.edu; xiaoruisun@cs.columbia.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [DMR-1534910, IIS-1563785]; Sloan Research Fellowship; Simons
   Foundation [320173]; Bloomberg Data Science Research Grant
FX We are grateful to Ashwin Pananjady, Michal Derezinski, and Manfred
   Warmuth for helpful discussions. DH was supported in part by NSF awards
   DMR-1534910 and IIS-1563785, a Bloomberg Data Science Research Grant,
   and a Sloan Research Fellowship. XS was supported in part by a grant
   from the Simons Foundation (#320173 to Xiaorui Sun). This work was done
   in part while DH and KS were research visitors and XS was a research
   fellow at the Simons Institute for the Theory of Computing.
CR Abid A., 2017, ARXIV170501342
   Andoni Alexandr, 2017, C LEARN THEOR
   Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287
   BOBKOV S., 2014, PREPRINT
   Boutsidis C, 2013, IEEE T INFORM THEORY, V59, P6880, DOI 10.1109/TIT.2013.2272457
   DAVIDSON K. R, 2001, HDB GEOMETRY BANACH, V1, P131
   Derezinski Michal, 2017, ARXIV170506908
   Elhami Golnooshsadat, 2017, P 42 IEEE INT C AC S
   FRIEZE AM, 1986, SIAM J COMPUT, V15, P536, DOI 10.1137/0215038
   Garey M. R, 1979, COMPUTERS INTRACTABI
   HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943
   LAGARIAS JC, 1985, J ACM, V32, P229, DOI 10.1145/2455.2461
   Laurent B, 2000, ANN STAT, V28, P1302
   LECAM L, 1973, ANN STAT, V1, P38, DOI 10.1214/aos/1193342380
   Ledoux Michel, 2000, CONCENTRATION MEASUR
   LENSTRA AK, 1982, MATH ANN, V261, P515, DOI 10.1007/BF01457454
   Massart Pascal, 2007, CONCENTRATION INEQUA, V6
   Pananjady A., 2017, ARXIV170407461
   Pananjady A, 2016, ANN ALLERTON CONF, P417, DOI 10.1109/ALLERTON.2016.7852261
   Reiss  R.-D., 2012, APPROXIMATE DISTRIBU
   Rudelson  M., 2010, ARXIV10032990
   Unnikrishnan Jayakrishnan, 2015, ARXIV151200115
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401055
DA 2019-06-15
ER

PT S
AU Hsu, WN
   Zhang, Y
   Glass, J
AF Hsu, Wei-Ning
   Zhang, Yu
   Glass, James
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised Learning of Disentangled and Interpretable Representations
   from Sequential Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.
C1 [Hsu, Wei-Ning; Zhang, Yu; Glass, James] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Hsu, WN (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM wnhsu@csail.mit.edu; yzhang87@csail.mit.edu; glass@csail.mit.edu
RI Jeong, Yongwook/N-7413-2016
CR Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Chung J., 2016, ARXIV160901704
   Dehak N., 2009, P INT, V9, P1559
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Dieleman S., 2016, CORR
   Dumoulin V., 2016, ARXIV160600704
   Edwards  H., 2016, ARXIV160602185
   Fabius Otto, 2014, ARXIV14126581
   Fraccaro  M., 2016, ADV NEURAL INFORM PR, V2016, P2199
   Garofolo J. S., 1993, 93 NASA STIRECON
   Glass J., 2017, AUT SPEECH REC UND A
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Higgins  I., 2016, BETA VAE LEARNING BA
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsu WN, 2017, INTERSPEECH, P1273, DOI 10.21437/Interspeech.2017-349
   Hu Zhiting, 2017, ARXIV170300955
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma Diederik P, 2016, IMPROVED VARIATIONAL
   Kinnunen T., 2017, ICASSP
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Larsen A. B. L., 2015, ARXIV151209300
   Li H., 2013, CHINASIP
   Makhzani A., 2015, ARXIV151105644
   Mogran N, 2004, SPEECH PROCESSING AU, P309
   Nakashika T, 2016, IEEE-ACM T AUDIO SPE, V24, P2032, DOI 10.1109/TASLP.2016.2593263
   Oord  A.v.d., 2016, ARXIV160106759
   PAUL DB, 1992, SPEECH AND NATURAL LANGUAGE, P357
   Pearce D., 2002, THESIS
   Radford A., 2015, ARXIV151106434
   Rezende D. J, 2014, ARXIV14014082
   Sak H, 2014, INTERSPEECH, P338
   Serban Iulian Vlad, 2017, 31 AAAI C ART INT
   Serdyuk D., 2016, CORR
   Shinohara Y, 2016, INTERSPEECH, P2369, DOI 10.21437/Interspeech.2016-879
   Yu  D., 2013, ARXIV13013605
   Zhang Y, 2016, INT CONF ACOUST SPEE, P5755, DOI 10.1109/ICASSP.2016.7472780
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401088
DA 2019-06-15
ER

PT S
AU Hu, AJ
   Negahban, SN
AF Hu, Addison J.
   Negahban, Sahand N.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Minimax Estimation of Bandable Precision Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMAL RATES; COVARIANCE; CONVERGENCE; SELECTION
AB The inverse covariance matrix provides considerable insight for understanding statistical models in the multivariate setting. In particular, when the distribution over variables is assumed to be multivariate normal, the sparsity pattern in the inverse covariance matrix, commonly referred to as the precision matrix, corresponds to the adjacency matrix representation of the Gauss-Markov graph, which encodes conditional independence statements between variables. Minimax results under the spectral norm have previously been established for covariance matrices, both sparse and banded, and for sparse precision matrices. We establish minimax estimation bounds for estimating banded precision matrices under the spectral norm. Our results greatly improve upon the existing bounds; in particular, we find that the minimax rate for estimating banded precision matrices matches that of estimating banded covariance matrices. The key insight in our analysis is that we are able to obtain barely-noisy estimates of k X k subblocks of the precision matrix by inverting slightly wider blocks of the empirical covariance matrix along the diagonal. Our theoretical results are complemented by experiments demonstrating the sharpness of our bounds.
C1 [Hu, Addison J.; Negahban, Sahand N.] Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA.
RP Hu, AJ (reprint author), Yale Univ, Dept Stat & Data Sci, New Haven, CT 06520 USA.
EM addison.hu@yale.edu; sahand.negahban@yale.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [DMS 1723128]
FX We would like to thank Harry Zhou for stimulating discussions regarding
   matrix estimation problems. SN acknowledges funding from NSF Grant DMS
   1723128.
CR Bickel PJ, 2011, J R STAT SOC B, V73, P711, DOI 10.1111/j.1467-9868.2011.00779.x
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai T. T., 2011, ARXIV11022233STAT
   Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081
   Cai TT, 2016, ANN STAT, V44, P455, DOI 10.1214/13-AOS1171
   Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998
   Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752
   Friedman J., 2007, BIOSTATISTICS
   Frith CD, 1995, HUM BRAIN MAPP, V3, P153, DOI 10.1002/hbm.460030209
   Hosseini M. J., 2016, ADV NEURAL INFORM PR, P3808
   Hu A. J., 2017, ARXIV171007006V1
   Lauritzen S.L., 1996, OXFORD STAT SCI SERI
   Lee K., 2017, ARXIV170701143STAT
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Padmanabhan N, 2016, MON NOT R ASTRON SOC, V460, P1567, DOI 10.1093/mnras/stw1042
   Ren Z, 2015, ANN STAT, V43, P991, DOI 10.1214/14-AOS1286
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Saon G, 2011, INT CONF ACOUST SPEE, P5056
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   VISSER H, 1995, J CLIMATE, V8, P969, DOI 10.1175/1520-0442(1995)008<0969:TEARAI>2.0.CO;2
   Woodbury MA., 1950, 42 PRINC U STAT RES
   Yu B., 1997, FESTSCHRIFT L LECAM, P423
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404093
DA 2019-06-15
ER

PT S
AU Hu, YT
   Huang, JB
   Schwing, AG
AF Hu, Yuan-Ting
   Huang, Jia-Bin
   Schwing, Alexander G.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI MaskRNN: Instance Level Video Object Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Instance level video object segmentation is an important technique for video editing and compression. To capture the temporal coherence, in this paper, we develop MaskRNN, a recurrent neural net approach which fuses in each frame the output of two deep nets for each object instance - a binary segmentation net providing a mask and a localization net providing a bounding box. Due to the recurrent component and the localization component, our method is able to take advantage of long-term temporal structures of the video data as well as rejecting outliers. We validate the proposed algorithm on three challenging benchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the Segtrack v2 dataset, achieving state-of-the-art performance on all of them.
C1 [Hu, Yuan-Ting; Schwing, Alexander G.] UIUC, Champaign, IL 61820 USA.
   [Huang, Jia-Bin] Virginia Tech, Blacksburg, VA USA.
RP Hu, YT (reprint author), UIUC, Champaign, IL 61820 USA.
EM ythu2@illinois.edu; jbhuang@vt.edu; aschwing@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1718221]; NVIDIA
FX This material is based upon work supported in part by the National
   Science Foundation under Grant No. 1718221. We thank NVIDIA for
   providing the GPUs used in this research.
CR Avinash Ramakanth S, 2014, P CVPR
   Badrinarayanan V., 2010, P CVPR
   Belongie S., 2002, TPAMI
   Brendel W., 2009, P ICCV
   Brutzer S., 2011, P CVPR
   Caelles S., 2017, ARXIV170401926
   Caelles S., 2017, P CVPR
   Chang J., 2013, P CVPR
   Cheng H.-T., 2012, P CVPR
   Criminisi A., 2006, P CVPR
   Elgammal A., 2002, P IEEE
   Faktor  A., 2014, BMVC
   Fan Q., 2015, ACM TOG P SIGGRAPH
   Galasso F., 2013, P ICCV
   Girshick R., 2015, P CVPR
   Grundmann M., 2010, P CVPR
   Haymanand E., 2003, P ICCV
   He K., 2017, P ICCV
   Ilg E., 2017, P CVPR
   Irani M., 1994, IJCV
   Irani M., 1998, PAMI
   Jain S., 2017, P CVPR
   Jain S. D., 2014, P ECCV
   Jampani V., 2017, P CVPR
   Khoreva A., 2017, ARXIV170309554
   Khoreva A., 2017, P CVPR
   Kingma  D., 2014, P ICLR
   Lee Y. J., 2011, P ICCV
   Lezama J., 2011, P CVPR
   Li F., 2013, P ICCV
   Li W., 2016, ACM TOG P SIGGRAPH
   Long J., 2015, P CVPR
   Maerki N., 2016, P CVPR
   Nagaraja N., 2015, P ICCV
   Ochs P., 2014, TPAMI
   Papazoglou A., 2013, P ICCV
   Perazzi F., 2015, P ICCV
   Perazzi F., 2016, P CVPR
   Pont-Tuset J., 2017, ARXIV170400675
   Price B. L., 2009, P ICCV
   Ren Y., 2003, PRL, P3
   Schwing A.G., 2015, FULLY CONNECTED DEEP
   Simonyan K., 2015, P ICLR
   Torr P. H. S., 1998, P ECCV
   Tsai D., 2010, P BMVC
   Tsai Y.-H., 2016, P CVPR
   Vijayanarasimhan S., 2012, P ECCV
   Xiao F., 2016, P CVPR
   Yuan C., 2007, PAMI
   Zhang D., 2013, P CVPR
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400031
DA 2019-06-15
ER

PT S
AU Huang, WB
   Harandi, M
   Zhang, T
   Fan, LJ
   Sun, FC
   Huang, JZ
AF Huang, Wenbing
   Harandi, Mehrtash
   Zhang, Tong
   Fan, Lijie
   Sun, Fuchun
   Huang, Junzhou
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Optimization for Linear Dynamical Systems with Applications to
   Clustering and Sparse Coding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID KERNELS
AB Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling, analyzing LDSs is not free of difficulty, mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper, we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end, we first derive a novel canonical form for representing the parameters of an LDS, and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies, our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.
C1 [Huang, Wenbing; Huang, Junzhou] Tencent AI Lab, Bellevue, WA 98004 USA.
   [Harandi, Mehrtash; Zhang, Tong] CSIRO, Data61, Canberra, ACT, Australia.
   [Harandi, Mehrtash; Zhang, Tong] Australian Natl Univ, Canberra, ACT, Australia.
   [Huang, Wenbing; Fan, Lijie; Sun, Fuchun] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing, Peoples R China.
RP Huang, WB (reprint author), Tencent AI Lab, Bellevue, WA 98004 USA.
EM helendhuang@tencent.com; mehrtash.harandi@data61.csiro.au;
   tong.zhang@anu.edu.cn; flj14@mails.tsinghua.edu.cn;
   fcsun@mail.tsinghua.edu.cn; joehhuang@tencent.com
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation of China (NSFC) [91420302, 91520201,
   61210013, 61327809]; NSFC in project Crossmodal Learning [NSFC
   61621136008/DFG TRR-169]; German Research of Foundation (DFG) in project
   Crossmodal Learning [NSFC 61621136008/DFG TRR-169]; National High-Tech
   Research and Development Plan [2015AA042306]; Australian Research
   Council's Discovery Projects funding scheme [DP150104645]
FX This research was supported in part by the National Science Foundation
   of China (NSFC) (Grant No: 91420302, 91520201,61210013 and 61327809),
   the NSFC and the German Research of Foundation (DFG) in project
   Crossmodal Learning (Grant No: NSFC 61621136008/DFG TRR-169), and the
   National High-Tech Research and Development Plan under Grant
   2015AA042306. Besides, Tong Zhang was supported by Australian Research
   Council's Discovery Projects funding scheme (project DP150104645).
CR Afsari B., 2014, GEOMETRIC THEORY INF, P219
   Afsari B, 2012, PROC CVPR IEEE, P2208, DOI 10.1109/CVPR.2012.6247929
   BARRAUD AY, 1977, IEEE T AUTOMAT CONTR, V22, P883, DOI 10.1109/TAC.1977.1101604
   Chan A. B., 2007, P IEEE C COMP VIS PA, P1
   Chan AB, 2005, PROC CVPR IEEE, P846
   Chan AB, 2010, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2010.5539878
   Chan Antoni B., 2013, CLUSTERING DYNAMIC T, V35, P1606
   De Cock K, 2002, SYST CONTROL LETT, V46, P265, DOI 10.1016/S0167-6911(02)00135-4
   Derpanis KG, 2012, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2012.6247815
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Duda R.O., 2012, PATTERN CLASSIFICATI
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan Chuang, CVPR, P2568
   Ghanem B, 2010, LECT NOTES COMPUT SC, V6312, P223
   Harandi M, 2015, INT J COMPUT VISION, V114, P113, DOI 10.1007/s11263-015-0833-x
   Huang A., 2008, P 6 NZ COMP SCI RES, P49
   Huang Wenbing, 2016, P INT JOINT C ARTIFI
   Johansen S., 1995, LIKELIHOOD BASED INF
   Kalman D., 1996, COLL MATH J, V27, P2, DOI DOI 10.2307/2687269
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Martin RJ, 2000, IEEE T SIGNAL PROCES, V48, P1164, DOI 10.1109/78.827549
   Mumtaz A, 2015, IEEE T PATTERN ANAL, V37, P697, DOI 10.1109/TPAMI.2014.2359432
   Pan T, 2016, IEEE CONF COMPUT
   Peteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Ravichandran A, 2013, IEEE T PATTERN ANAL, V35, P342, DOI 10.1109/TPAMI.2012.83
   Saisan P, 2001, PROC CVPR IEEE, P58
   Siddiqi SM, 2007, ADV NEURAL INFORM PR
   Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52
   Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0
   Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1
   WOOLFE F, 2006, EUR C COMP VIS ECCV, V3952, P549
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403050
DA 2019-06-15
ER

PT S
AU Huang, XR
   Liang, ZX
   Bajaj, C
   Huang, QX
AF Huang, Xiangru
   Liang, Zhenxiao
   Bajaj, Chandrajit
   Huang, Qixing
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Translation Synchronization via Truncated Least Squares
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we introduce a robust algorithm, TranSync, for the 1D translation synchronization problem, in which the aim is to recover the global coordinates of a set of nodes from noisy measurements of relative coordinates along an observation graph. The basic idea of TranSync is to apply truncated least squares, where the solution at each step is used to gradually prune out noisy measurements. We analyze TranSync under both deterministic and randomized noisy models, demonstrating its robustness and stability. Experimental results on synthetic and real datasets show that TranSync is superior to state-of-the-art convex formulations in terms of both efficiency and accuracy.
C1 [Huang, Xiangru; Bajaj, Chandrajit; Huang, Qixing] Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.
   [Liang, Zhenxiao] Tsinghua Univ, Beijing 100084, Peoples R China.
RP Huang, XR (reprint author), Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.
EM xrhuang@cs.utexas.edu; liangzx14@mails.tsinghua.edu.cn;
   bajaj@cs.utexas.edu; huangqx@cs.utexas.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [DMS-1700234]; National Institute of Health [R41 GM116300, R01
   GM117594]
FX Qixing Huang would like to acknowledge support this research from NSF
   DMS-1700234. Chandrajit Bajaj would like to acknowledge support for this
   research from the National Institute of Health grants #R41 GM116300 and
   #R01 GM117594.
CR Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x
   Arrigoni F., 2015, CORR
   Chatterjee A., 2013, 2013 IEEE INT C COMP
   Chen Y., 2016, CORR
   Cho T. S., 2010, IEEE C COMP VIS PATT
   Chung F, 2007, INTERNET MATH, V4, P225, DOI 10.1080/15427951.2007.10129296
   Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626
   Cruz R, 2016, IEEE IJCNN, P2182, DOI 10.1109/IJCNN.2016.7727469
   Daubechies I., COMM PURE APPL MATH
   Gelfand N., 2005, P 3 EUR S GEOM PROC
   Goldberg D., 2002, P 18 ANN S COMP GEOM, P82
   Heiberger R. M., 1992, DESIGN S FUNCTION RO, P112
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   HUBER D., 2002, THESIS
   Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x
   Kim V. G., 2012, T GRAPHICS, V31
   Liu HL, 2014, INT J OPT, DOI 10.1155/2014/693807
   Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Pachauri D., 2014, ADV NEURAL INFORM PR, P541
   Pachauri D., 2013, ADV NEURAL INFORM PR, P1860
   Roberts R., 2011, STRUCTURE MOTION SCE, P3137
   Shen  Y., 2016, ADV NEURAL INFORM PR, P4925
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Wang L., 2012, CORR
   Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801
   Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401048
DA 2019-06-15
ER

PT S
AU Huang, ZB
   Geng, SN
   Page, D
AF Huang, Zhaobin
   Geng, Sinong
   Page, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Screening Rule for l(1)-Regularized Ising Model Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INVERSE COVARIANCE ESTIMATION; MARKOV NETWORKS; SELECTION; LASSO
AB We discover a screening rule for l(1)-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exactly recovering the blockwise structure of a solution under any given regularization parameters. With enough sparsity, the screening rule can be combined with various optimization procedures to deliver solutions efficiently in practice. The screening rule is especially suitable for large-scale exploratory data analysis, where the number of variables in the dataset can be thousands while we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability. Experimental results on various datasets demonstrate the efficiency and insights gained from the introduction of the screening rule.
C1 [Huang, Zhaobin; Geng, Sinong; Page, David] Univ Wisconsin, Madison, WI 53706 USA.
RP Huang, ZB (reprint author), Univ Wisconsin, Madison, WI 53706 USA.
EM zkuang@wisc.edu; sgeng2@wisc.edu; page@biostat.wisc.edu
FU NIH BD2K Initiative grant [U54 AI117924]; NIGMS [2RO1 GM097618]
FX The authors would like to gratefully acknowledge the NIH BD2K Initiative
   grant U54 AI117924 and the NIGMS grant 2RO1 GM097618.
CR Banerjee O, 2008, J MACH LEARN RES, V9, P485
   Barber RF, 2015, ELECTRON J STAT, V9, P567, DOI 10.1214/15-EJS1012
   Chen H, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-147
   Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033
   Fercoq  O., 2015, ICML, V37, P333
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Geng S., 2017, EFFICIENT PSEUDO LIK
   Ghaoui L. E., 2010, SAFE FEATURE ELIMINA
   Hastie  T., 2015, STAT LEARNING SPARSI
   Hofling H, 2009, J MACH LEARN RES, V10, P883
   Honorio J., 2010, P 27 INT C MACH LEAR, P447
   Hsieh C.-J., 2013, ADV NEURAL INFORM PR, V26, P3165
   Karger D, 2001, SIAM PROC S, P392
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lee S., 2017, IEEE T PATTERN ANAL
   Liu Han, 2010, Adv Neural Inf Process Syst, V24, P1432
   Liu J, 2014, THESIS
   Liu J., 2013, ICML 2013 WORKSH STR
   Liu J., 2013, ARXIV13077577
   Liu J, 2016, ANN APPL STAT, V10, P1699, DOI 10.1214/16-AOAS956
   Liu Jie, 2014, JMLR Workshop Conf Proc, V33, P576
   Liu Jie, 2014, Proc Int Conf Mach Learn, V2014, P955
   Liu Jie, 2012, Uncertain Artif Intell, V2012, P511
   Loh P.-L., 2012, NIPS, P2096
   Loh PL, 2013, ANN STAT, V41, P3022, DOI 10.1214/13-AOS1162
   Luo S., 2014, ARXIV14077819
   Mazumder R, 2012, J MACH LEARN RES, V13, P781
   Ndiaye E, 2015, NIPS, P811
   Pena Javier, 2016, LECT NOTES MACHINE L
   Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Uhlen M, 2015, SCIENCE, V347, DOI 10.1126/science.1260419
   Viallon V, 2014, BIOMETRICAL J, V56, P307, DOI 10.1002/bimj.201200253
   Vuffray M, 2016, ADV NEURAL INFORM PR, P2595
   Wainwright MJ, 2006, ADV NEURAL INFORM PR, P1465
   Wan YW, 2016, BMC SYST BIOL, V10, DOI 10.1186/s12918-016-0313-0
   Wan YW, 2015, BIOINFORMATICS
   Wang J., 2014, ADV NEURAL INFORM PR, V27, P1053
   Wang J., 2013, ADV NEURAL INFORM PR, V26, P1070
   Weinstein JN, 2013, NAT GENET, V45, P1113, DOI 10.1038/ng.2764
   Witten DM, 2011, J COMPUT GRAPH STAT, V20, P892, DOI 10.1198/jcgs.2011.11051a
   Xiang ZJ, 2017, IEEE T PATTERN ANAL, V39, P1008, DOI 10.1109/TPAMI.2016.2568185
   Yang S, 2015, SIAM J OPTIMIZ, V25, P916, DOI 10.1137/130936397
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400069
DA 2019-06-15
ER

PT S
AU Huggins, JH
   Adams, RP
   Broderick, T
AF Huggins, Jonathan H.
   Adams, Ryan P.
   Broderick, Tamara
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI PASS-GLM: polynomial approximate sufficient statistics for scalable
   Bayesian GLM inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generalized linear models (GLMs)-such as logistic regression, Poisson regression, and robust regression-provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy-including on an advertising data set with 40 million data points and 20,000 covariates.
C1 [Huggins, Jonathan H.; Broderick, Tamara] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Adams, Ryan P.] Google Brain & Princeton, Princeton, NJ USA.
RP Huggins, JH (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM jhuggins@mit.edu; rpa@princeton.edu; tbroderick@csail.mit.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR [N00014-17-1-2072]; ONR MURI [N00014-11-1-0688]; Google Faculty
   Research Award; NSF [IIS-1421780]; Alfred P. Sloan Foundation
FX JHH and TB are supported in part by ONR grant N00014-17-1-2072, ONR MURI
   grant N00014-11-1-0688, and a Google Faculty Research Award. RPA is
   supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation.
CR Ahn S., 2012, INT C MACH LEARN
   Alquier P, 2016, STAT COMPUT, V26, P29, DOI 10.1007/s11222-014-9521-x
   Angelino E, 2016, FOUND TRENDS MACH LE, V9, pI, DOI 10.1561/2200000052
   Bachem O., 2017, PRACTICAL CORESET CO
   Bardenet R., 2014, P 31 INT C MACH LEAR
   Bardenet R, 2017, J MACH LEARN RES, V18, P1
   Betancourt M. J., 2015, INT C MACH LEARN
   Bierkens J., 2016, ZIG ZAG PROCESS SUPE
   Bouchard-Cote A., 2016, BOUNCY PARTICLE SAMP, P1
   Broderick T., 2013, ADV NEURAL INFORM PR
   Campbell T., 2015, ADV NEURAL INFORM PR
   Entezari R., 2016, LIKELIHOOD INFLATING
   Feldman D., 2011, ADV NEURAL INFORM PR, P2142
   Fithian W, 2014, ANN STAT, V42, P1693, DOI 10.1214/14-AOS1220
   Gelman A., 2014, EXPECTATION PROPAGAT
   Han L., 2016, LOCAL UNCERTAINTY SA
   Hasenclever L, 2017, J MACH LEARN RES, V18
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Huggins  Jonathan, 2016, ADV NEURAL INFORM PR
   Jaakkola T, 1997, 6 INT WORKSH ART INT, V82
   Korattikara  Anoop, 2014, INT C MACH LEARN
   Kucukelbir A., 2015, ADV NEURAL INFORM PR
   Li P., 2006, SIGKDD C KNOWL DISC
   Lucic M., 2017, TRAINING MIXTURE MOD
   Maclaurin D., 2014, UNCERTAINTY ARTIFICI
   Mason JC, 2003, CHEBYSHEV POLYNOMIAL
   Minka T. P., 2001, UNCERTAINTY ARTIFICI
   Nishihara R., 2017, WORKSH HOT TOP OP SY
   Pakman A., 2017, INT C MACH LEARN
   Pillai N. S., 2014, ERGODICITY APPROXIMA
   Pollock M., 2016, SCALABLE LANGEVIN EX
   Rabinovich M., 2015, VARIATIONAL CONSENSU
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Scott S. L., 2013, BAYES 250
   Srivastava S., 2015, INT C ART INT STAT
   Stephanou M, 2017, ELECTRON J STAT, V11, P570, DOI 10.1214/17-EJS1245
   Szego? G, 1975, ORTHOGONAL POLYNOMIA
   TIERNEY L, 1986, J AM STAT ASSOC, V81, P82, DOI 10.2307/2287970
   Van der Vaart AW, 1998, ASYMPTOTIC STAT
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Welling  M., 2011, INT C MACH LEARN
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403066
DA 2019-06-15
ER

PT S
AU Ide, JS
   Cappabianco, FA
   Faria, FA
   Li, CSR
AF Ide, Jaime S.
   Cappabianco, Fabio A.
   Faria, Fabio A.
   Li, Chiang-shan R.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Detrended Partial Cross Correlation for Brain Connectivity Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FMRI; DIMENSIONALITY; DYNAMICS
AB Brain connectivity analysis is a critical component of ongoing human connectome projects to decipher the healthy and diseased brain. Recent work has highlighted the power-law (multi-time scale) properties of brain signals; however, there remains a lack of methods to specifically quantify short- vs. long- time range brain connections. In this paper, using detrended partial cross-correlation analysis (DPCCA), we propose a novel functional connectivity measure to delineate brain interactions at multiple time scales, while controlling for covariates. We use a rich simulated fMRI dataset to validate the proposed method, and apply it to a real fMRI dataset in a cocaine dependence prediction task. We show that, compared to extant methods, the DPCCA-based approach not only distinguishes short and long memory functional connectivity but also improves feature extraction and enhances classification accuracy. Together, this paper contributes broadly to new computational methodologies in understanding neural information processing.
C1 [Ide, Jaime S.; Li, Chiang-shan R.] Yale Univ, New Haven, CT 06519 USA.
   [Cappabianco, Fabio A.; Faria, Fabio A.] Univ Fed Sao Paulo, BR-12231 Sao Jose Dos Campos, Brazil.
RP Ide, JS (reprint author), Dept Psychiat, 34 Pk St S110, New Haven, CT 06519 USA.
EM jaime.ide@yale.edu; cappabianco@unifesp.br; ffaria@unifesp.br
RI Jeong, Yongwook/N-7413-2016
FU FAPESP [2016/21591-5]; CNPq [408919/2016-7]; NSF [BCS1309260]; NIH
   [AA021449, DA023248]
FX Supported by FAPESP (2016/21591-5), CNPq (408919/2016-7), NSF
   (BCS1309260) and NIH (AA021449, DA023248).
CR Bassett DS, 2009, CURR OPIN NEUROL, V22, P340, DOI 10.1097/WCO.0b013e32832d93dd
   Chang C, 2010, NEUROIMAGE, V50, P81, DOI 10.1016/j.neuroimage.2009.12.011
   Ciuciu P, 2014, NEUROIMAGE, V95, P248, DOI 10.1016/j.neuroimage.2014.03.047
   Correa NM, 2009, INT CONF ACOUST SPEE, P385, DOI 10.1109/ICASSP.2009.4959601
   Friman O, 2001, MAGNET RESON MED, V45, P323, DOI 10.1002/1522-2594(200102)45:2<323::AID-MRM1041>3.0.CO;2-#
   Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He BYJ, 2014, TRENDS COGN SCI, V18, P480, DOI 10.1016/j.tics.2014.04.003
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu S, 2015, NEUROIMAGE, V119, P286, DOI 10.1016/j.neuroimage.2015.06.032
   Ide JS, 2016, NEUROIMAGE-CLIN, V11, P349, DOI 10.1016/j.nicl.2016.03.004
   Kantelhardt JW, 2002, PHYSICA A, V316, P87, DOI 10.1016/S0378-4371(02)01383-3
   Kristoufek L, 2014, PHYSICA A, V402, P291, DOI 10.1016/j.physa.2014.01.058
   LOGAN GD, 1984, J EXP PSYCHOL HUMAN, V10, P276, DOI 10.1037/0096-1523.10.2.276
   Matthews PM, 2016, NEURON, V91, P511, DOI 10.1016/j.neuron.2016.07.031
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Podobnik B, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.084102
   Qian XY, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.062816
   Seth AK, 2015, J NEUROSCI, V35, P3293, DOI 10.1523/JNEUROSCI.4399-14.2015
   Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063
   Sporns O, 2013, NEUROIMAGE, V80, P53, DOI 10.1016/j.neuroimage.2013.03.023
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041
   Xia Mingrui, 2017, NEUROIMAGE
   Yuan NM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27707
   Yuan NM, 2015, SCI REP-UK, V5, DOI 10.1038/srep08143
   Zhou DL, 2009, NEUROIMAGE, V47, P1590, DOI 10.1016/j.neuroimage.2009.05.089
NR 27
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400085
DA 2019-06-15
ER

PT S
AU Ilievski, I
   Feng, JS
AF Ilievski, Ilija
   Feng, Jiashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multimodal Learning and Reasoning for Visual Question Answering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB Reasoning about entities and their relationships from multimodal data is a key goal of Artificial General Intelligence. The visual question answering (VQA) problem is an excellent way to test such reasoning capabilities of an AI model and its multimodal representation learning. However, the current VQA models are oversimplified deep neural networks, comprised of a long short-term memory (LSTM) unit for question comprehension and a convolutional neural network (CNN) for learning single image representation. We argue that the single visual representation contains a limited and general information about the image contents and thus limits the model reasoning capabilities. In this work we introduce a modular neural network model that learns a multimodal and multifaceted representation of the image and the question. The proposed model learns to use the multimodal representation to reason about the image entities and achieves a new state-of-the-art performance on both VQA benchmark datasets, VQA v1.0 and v2.0, by a wide margin.
C1 [Ilievski, Ilija] Natl Univ Singapore, Integrat Sci & Engn, Singapore, Singapore.
   [Feng, Jiashi] Natl Univ Singapore, Elect & Comp Engn, Singapore, Singapore.
RP Ilievski, I (reprint author), Natl Univ Singapore, Integrat Sci & Engn, Singapore, Singapore.
EM ilija.ilievski@u.nus.edu; elefjia@nus.edu.sg
RI Jeong, Yongwook/N-7413-2016
FU National University of Singapore [R-263-000-C08-133]; Ministry of
   Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS
   grant [R-263-000-C67-646]
FX The work of Jiashi Feng was partially supported by National University
   of Singapore startup grant R-263-000-C08-133, Ministry of Education of
   Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant
   R-263-000-C67-646.
CR Andreas J., 2016, P IEEE C COMP VIS PA
   Andreas Jacob, 2016, HLT NAACL
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Charikar M., 2002, AUTOMATA LANGUAGES P, P784
   Chen K, 2015, ARXIV151105960
   Fukui  Akira, 2016, C EMP METH NAT LANG
   Goyal Y., 2017, C COMP VIS PATT REC
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hu R., 2017, P IEEE C COMP VIS PA
   Ilievski I, 2016, ARXIV160401485
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Johnson Justin, 2017, IEEE INT C COMP VIS
   Johnson Justin, 2017, P IEEE C COMP VIS PA
   Kim J.H., 2016, ADV NEURAL INFORM PR, P361
   Kim Jin-Hwa, 2017, 5 INT C LEARN REPR
   Kingma D. P., 2015, INT C LEARN REPR
   Kiros R., 2015, ADV NEURAL INFORM PR, P3294
   Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423
   Krishna R., 2017, INT J COMPUTER VISIO
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levi Gil, 2015, P ACM INT C MULT INT
   Levi Gil, 2015, EMOTION RECOGNITION
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Dhruv Batra Jiasen, 2015, DEEPER LSTM NORMALIZ
   Lu J., 2016, ADV NEURAL INFORM PR, P289
   Malinowski M, 2014, ADV NEURAL INFORM PR, P1682
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Pinheiro P. O., 2016, LEARNING REFINE OBJE
   Pinheiro P. O., 2016, ECCV
   Rothe R., 2016, INT J COMPUTER VISIO
   Rothe Rasmus, 2016, DEEP EXPECTATION REA
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shih K. J., 2016, P IEEE C COMP VIS PA
   Simonyan  K., 2015, INT C LEARN REPR
   SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tenenbaum JB, 1997, ADV NEUR IN, V9, P662
   Xie B, 2017, IEEE ICC
   Xiong, 2016, P INT C MACH LEARN, P2397
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhang K., 2016, JOINT FACE DETECTION
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhou B., 2017, IEEE T PATTERN ANAL
   Zhou Bolei, 2017, PLACES 10 MILLION IM
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400053
DA 2019-06-15
ER

PT S
AU Imaizumi, M
   Maehara, T
   Hayashi, K
AF Imaizumi, Masaaki
   Maehara, Takanori
   Hayashi, Kohei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Tensor Train Rank Minimization: Statistical Efficiency and Scalable
   Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MATRIX COMPLETION
AB Tensor train (TT) decomposition provides a space-efficient representation for higher-order tensors. Despite its advantage, we face two crucial limitations when we apply the TT decomposition to machine learning problems: the lack of statistical theory and of scalable algorithms. In this paper, we address the limitations. First, we introduce a convex relaxation of the TT decomposition problem and derive its error bound for the tensor completion task. Next, we develop a randomized optimization method, in which the time complexity is as efficient as the space complexity is. In experiments, we numerically confirm the derived bounds and empirically demonstrate the performance of our method with a real higher-order tensor.
C1 [Imaizumi, Masaaki] RIKEN Ctr Adv Intelligence Project, Inst Stat Math, Tokyo, Japan.
   [Maehara, Takanori] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
   [Hayashi, Kohei] RIKEN Ctr Adv Intelligence Project, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.
RP Imaizumi, M (reprint author), RIKEN Ctr Adv Intelligence Project, Inst Stat Math, Tokyo, Japan.
EM imaizumi@ism.ac.jp; takanori.maehara@riken.jp; hayashi.kohei@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU JSPS [15J10206]; ONR [N62909-17-1-2138]
FX We thank Prof. Taiji Suzuki for comments that greatly improved the
   manuscript. M. Imaizumi is supported by Grant-in-Aid for JSPS Research
   Fellow (15J10206) from the JSPS. K. Hayashi is supported by ONR
   N62909-17-1-2138.
CR Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Chen J., 2017, ARXIV170104831
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Grasedyck Lars, 2015, ARXIV150900311
   Hamilton J. D, 1994, TIME SERIES ANAL, V2
   Harshman R. A., 1970, UCLA WORKING PAPERS, V16, P1
   Jurafsky D., 2014, SPEECH LANGUAGE PROC, V3
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR, P836
   Li P., 2006, P 12 ACM SIGKDD INT, P287, DOI DOI 10.1145/1150402.1150436
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850
   Novikov A., 2014, INT C MACH LEARN, P811
   Novikov A., 2015, ADV NEURAL INFORM PR, P442
   Novikov A., 2016, ARXIV160503795
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Oseledets I, 2010, LINEAR ALGEBRA APPL, V432, P70, DOI 10.1016/j.laa.2009.07.024
   Phien H. N., 2016, ARXIV160101083
   Signoretto M, 2011, IEEE SIGNAL PROC LET, V18, P403, DOI 10.1109/LSP.2011.2151856
   Stoudenmire E., 2016, ADV NEURAL INFORM PR, P4799
   Suzuki T., 2016, ADV NEURAL INFORM PR, P3783
   Tomioka R., 2013, P ADV NEUR INF PROC, P1331
   Tomioka R., 2011, ADV NEURAL INFORM PR
   Tomioka  R., 2010, ARXIV10100789
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Wang  W., 2016, ARXIV160905587
   Yadong Mu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2609, DOI 10.1109/CVPR.2011.5995369
   Zhang Z., 2016, T SIGNAL PROCESSING
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404001
DA 2019-06-15
ER

PT S
AU Inan, H
   Erdogdu, MA
   Schnitzer, MJ
AF Inan, Hakan
   Erdogdu, Murat A.
   Schnitzer, Mark J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust Estimation of Neural Signals in Calcium Imaging
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity from imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets contain gross contaminating sources which could originate from the technology used, or the underlying biological tissue. Although past work has considered the effects of contamination under limited circumstances, there has not been a general framework treating contamination and its effects on the statistical estimation of calcium signals. In this work, we proceed in a new direction and propose to extract cells and their activity using robust statistical estimation. Using the theory of M-estimation, we derive a minimax optimal robust loss, and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.
C1 [Inan, Hakan; Schnitzer, Mark J.] Stanford Univ, Stanford, CA 94305 USA.
   [Erdogdu, Murat A.] Microsoft Res, Redmond, WA USA.
   [Erdogdu, Murat A.] Vector Inst, Toronto, ON, Canada.
   [Schnitzer, Mark J.] Howard Hughes Med Inst, Chevy Chase, MD USA.
RP Inan, H (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM inanh@stanford.edu; erdogdu@cs.toronto.edu; mschnitz@stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA
FX We gratefully acknowledge support from DARPA and technical assistance
   from Biafra Ahanonu, Lacey Kitch, Yaniv Ziv, Elizabeth Otto and Margaret
   Carr.
CR Apthorpe N. J., 2016, ARXIV160607372
   COLLINS JR, 1976, ANN STAT, V4, P68, DOI 10.1214/aos/1176343348
   DENK W, 1990, SCIENCE, V248, P73, DOI 10.1126/science.2321027
   Flusberg BA, 2008, NAT METHODS, V5, P935, DOI 10.1038/nmeth.1256
   Ghosh KK, 2011, NAT METHODS, V8, P871, DOI [10.1038/NMETH.1694, 10.1038/nmeth.1694]
   Helmchen F, 2005, NAT METHODS, V2, P932, DOI 10.1038/NMETH818
   HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   JAECKEL LA, 1971, ANN MATH STAT, V42, P1020, DOI 10.1214/aoms/1177693330
   Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080
   Kokic P. N., 1994, J OFF STAT, V10, P419
   MARTIN RD, 1993, ANN STAT, V21, P338, DOI 10.1214/aos/1176349029
   Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009
   Pachitariu M, 2013, ADV NEURAL INFORM PR, P1745
   Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037
   Zhou P., 2016, ARXIV160507266
   Ziv Y, 2013, NAT NEUROSCI, V16, P264, DOI 10.1038/nn.3329
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402093
DA 2019-06-15
ER

PT S
AU Indyk, P
   Razenshteyn, I
   Wagner, T
AF Indyk, Piotr
   Razenshteyn, Ilya
   Wagner, Tal
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Practical Data-Dependent Metric Compression with Provable Guarantees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEAREST-NEIGHBOR; SEARCH
AB We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given n points in a d-dimensional space where each coordinate is represented using B bits (i.e., dB bits per point), it produces a representation of size O(dlog(dB/is an element of) + log n) bits per point from which one can approximate the distances up to a factor of 1 +/- is an element of. Our algorithm almost matches the recent bound of [6] while being much simpler. We compare our algorithm to Product Quantization (PQ) [7], a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT (used in [7]), MNIST [11], New York City taxi time series [4] and a synthetic one-dimensional data set embedded in a high-dimensional space. With appropriately tuned parameters, our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.
C1 [Indyk, Piotr; Razenshteyn, Ilya; Wagner, Tal] MIT, Cambridge, MA 02139 USA.
RP Indyk, P (reprint author), MIT, Cambridge, MA 02139 USA.
RI Jeong, Yongwook/N-7413-2016
CR Arandjelovic R, 2014, IEEE T PATTERN ANAL, V36, P2396, DOI 10.1109/TPAMI.2014.2339821
   Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Guha S., 2016, INT C MACH LEARN, P2712
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Indyk P, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P710
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson J., 2017, CORR
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kushilevitz E, 2000, SIAM J COMPUT, V30, P457, DOI 10.1137/S0097539798347177
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Li Ping, 2011, ADV NEURAL INFORM PR, P2672
   Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002
   Norouzi M., 2012, ADV NEURAL INFORM PR, V25, P1061
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Raginsky M., 2009, P ADV NEUR INF PROC, P1509
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shrivastava  A., 2014, P 31 INT C MACH LEAR, P557
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Weiss Y., 2009, ADV NEURAL INFORM PR, P1753
   YAU MM, 1983, COMMUN ACM, V26, P504, DOI 10.1145/358150.358158
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402065
DA 2019-06-15
ER

PT S
AU Ioffe, S
AF Ioffe, Sergey
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Batch Renormalization: Towards Reducing Minibatch Dependence in
   Batch-Normalized Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.
C1 [Ioffe, Sergey] Google, Mountain View, CA 94043 USA.
RP Ioffe, S (reprint author), Google, Mountain View, CA 94043 USA.
EM sioffe@google.com
RI Jeong, Yongwook/N-7413-2016
CR Arpit D, 2016, ARXIV160301431
   Ba J. L., 2016, ARXIV160706450
   Chen J., 2016, ARXIV160400981
   Goldberger Jacob, 2004, ADV NEURAL INFORM PR, V17
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Schroff F., 2015, CORR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   TIELEMAN T, 2012, COURSERA NEURAL NETW
NR 14
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401094
DA 2019-06-15
ER

PT S
AU Ishida, T
   Niu, G
   Hu, WH
   Sugiyama, M
AF Ishida, Takashi
   Niu, Gang
   Hu, Weihua
   Sugiyama, Masashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning from Complementary Labels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.
C1 [Ishida, Takashi] Sumitomo Mitsui Asset Management, Tokyo, Japan.
   [Ishida, Takashi; Niu, Gang; Hu, Weihua; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan.
   [Ishida, Takashi; Niu, Gang; Hu, Weihua; Sugiyama, Masashi] RIKEN, Tokyo, Japan.
RP Ishida, T (reprint author), Sumitomo Mitsui Asset Management, Tokyo, Japan.; Ishida, T (reprint author), Univ Tokyo, Tokyo, Japan.; Ishida, T (reprint author), RIKEN, Tokyo, Japan.
EM ishida@ms.k.u-tokyo.ac.jp; gang@ms.k.u-tokyo.ac.jp;
   hu@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU JST CREST [JPMJCR1403]
FX GN and MS were supported by JST CREST JPMJCR1403. We thank Ikko Yamane
   for the helpful discussions.
CR Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Cour T, 2011, J MACH LEARN RES, V12, P1501
   Davis J. V., 2007, ICML
   Denis F., 1998, ALT
   du Plessis  M.C., 2014, NIPS
   du Plessis M. C., 2015, ICML
   Dwork C., 2008, TAMC
   Elkan  C., 2008, KDD
   Goldberger J., 2004, NIPS
   Grandvalet Y., 2004, NIPS
   Howe J., 2009, CROWDSOURCING WHY PO
   Kingma D. P., 2015, ICLR
   Kipf T. N., 2017, ICLR
   Kiryo R., 2017, NIPS
   Laine S., 2017, ICLR
   Ledoux M., 1991, PROBABILITY BANACH S
   Li YF, 2015, IEEE T PATTERN ANAL, V37, P175, DOI 10.1109/TPAMI.2014.2299812
   Mann G. S., 2007, ICML
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Mohri M., 2012, FDN MACHINE LEARNING
   Nair V., 2010, ICML
   Niu  G., 2016, NIPS
   Niu G., 2013, ICML
   Niu G, 2014, NEURAL COMPUT, V26, P1717, DOI 10.1162/NECO_a_00614
   Sakai  T., 2017, ICML
   Scholkopf B., 2001, LEARNING KERNELS
   Tokui  S., 2015, P WORKSH MACH LEARN
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xing E. P., 2002, NIPS
   Yang  Z., 2016, ICML
   Zhang T, 2004, J MACH LEARN RES, V5, P1225
   Zhou D., 2003, NIPS
   Zhu  X., 2003, ICML
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405070
DA 2019-06-15
ER

PT S
AU Ito, S
   Hatano, D
   Sumita, H
   Yabe, A
   Fukunaga, T
   Kakimura, N
   Kawarabayashi, K
AF Ito, Shinji
   Hatano, Daisuke
   Sumita, Hanna
   Yabe, Akihiro
   Fukunaga, Takuro
   Kakimura, Naonori
   Kawarabayashi, Ken-ichi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Efficient Sublinear-Regret Algorithms for Online Sparse Linear
   Regression with Limited Observation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP subset of BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.
C1 [Ito, Shinji; Yabe, Akihiro] NEC Corp Ltd, Tokyo, Japan.
   [Hatano, Daisuke; Sumita, Hanna; Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan.
   [Fukunaga, Takuro] JST, PRESTO, Tokyo, Japan.
   [Kakimura, Naonori] Keio Univ, Tokyo, Japan.
RP Ito, S (reprint author), NEC Corp Ltd, Tokyo, Japan.
EM s-ito@me.jp.nec.com; hatano@nii.ac.jp; sumita@nii.ac.jp;
   a-yabe@cq.jp.nec.com; takuro@nii.ac.jp; kakimura@math.keio.ac.jp;
   k-keniti@nii.ac.jp
FU JST ERATO Grant, Japan [JPMJER1201]
FX This work was supported by JST ERATO Grant Number JPMJER1201, Japan.
CR Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cesa-Bianchi N., 2010, JOINT ICML COLT WORK
   Cesa-Bianchi N, 2011, J MACH LEARN RES, V12, P2857
   Chen X., 2012, ADV NEURAL INFORM PR, P395
   Foster D. J., 2016, 29 ANN C LEARN THEOR, P960
   Hazan E., 2012, P 29 INT C MACH LEAR, P807
   Kale  S., 2017, P 34 INT C MACH LEAR, V70, P1780
   Kale S., 2014, P 27 C LEARN THEOR, P1299
   Koiran P, 2014, IEEE T INFORM THEORY, V60, P4999, DOI 10.1109/TIT.2014.2331341
   Lichman M., 2013, UCI MACHINE LEARNING
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
   Zolghadr N., 2013, P NIPS, P1241
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404017
DA 2019-06-15
ER

PT S
AU Jagatap, G
   Hegde, C
AF Jagatap, Gauri
   Hegde, Chinmay
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fast, Sample-Efficient Algorithms for Structured Phase Retrieval
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABLE SIGNAL RECOVERY; SPARSE SIGNALS; CRYSTALLOGRAPHY
AB We consider the problem of recovering a signal x* is an element of R-n, from magnitude-only measurements, y(i) = vertical bar < a(i), x*> vertical bar for i = {1, 2,..., m}. Also known as the phase retrieval problem, it is a fundamental challenge in nano-, bio-and astronomical imaging systems, and speech processing. The problem is ill-posed, and therefore additional assumptions on the signal and/or the measurements are necessary.
   In this paper, we first study the case where the underlying signal x* is s-sparse. We develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating Minimization, or CoPRAM. Our algorithm is simple and can be obtained via a natural combination of the classical alternating minimization approach for phase retrieval, with the CoSaMP algorithm for sparse recovery. Despite its simplicity, we prove that our algorithm achieves a sample complexity of O (s(2) log n) with Gaussian samples, which matches the best known existing results. It also demonstrates linear convergence in theory and practice and requires no extra tuning parameters other than the signal sparsity level s. We then consider the case where the underlying signal x * arises from structured sparsity models.
   We specifically examine the case of block-sparse signals with uniform block size of b and block sparsity k - s/b. For this problem, we design a recovery algorithm that we call Block CoPRAM that further reduces the sample complexity to O(ks log n). For sufficiently large block lengths of b = circle minus( s), this bound equates to O(s log n). To our knowledge, this constitutes the first end-toend linearly convergent family of algorithms for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity level of the signal.
C1 [Jagatap, Gauri; Hegde, Chinmay] Iowa State Univ, Elect & Comp Engn, Ames, IA 50011 USA.
RP Jagatap, G (reprint author), Iowa State Univ, Elect & Comp Engn, Ames, IA 50011 USA.
EM gauri@iastate.edu; chinmay@iastate.edu
RI Jeong, Yongwook/N-7413-2016
CR Bahmani  S., 2015, ADV NEURAL INFORM PR, P523
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Bentkus V, 2003, J THEOR PROBAB, V16, P161, DOI 10.1023/A:1022234622381
   Cai S, 2014, IEEE INT SYMP INFO, P2007, DOI 10.1109/ISIT.2014.6875185
   Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Cevher V., 2009, P SAMPL THEOR APPL S
   Cevher V., 2008, ADV NEURAL INF PROC
   Chen Y., 2015, ADV NEURAL INFORM PR, V2, P739
   Chen YX, 2015, IEEE T INFORM THEORY, V61, P4034, DOI 10.1109/TIT.2015.2429594
   DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001
   Dirksen S, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-3760
   Do Ba K, 2010, PROC APPL MATH, V135, P1190
   Duarte  M., 2009, P IEEE C INF SCI SYS
   Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837
   Fickus M, 2014, LINEAR ALGEBRA APPL, V449, P475, DOI 10.1016/j.laa.2014.02.011
   FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758
   Goldstein T., 2016, ARXIV161007531
   Gross D, 2017, APPL COMPUT HARMON A, V42, P37, DOI 10.1016/j.acha.2015.05.004
   HARRISON RW, 1993, J OPT SOC AM A, V10, P1046, DOI 10.1364/JOSAA.10.001046
   Hegde C., 2015, P INT C MACH LEARN I
   Hegde C., 2014, P INT C AUT LANG PRO
   Hegde C., 2015, B EATCS, V1, P197
   Hegde C., 2014, P ACM S DISCR ALG SO
   Hegde C., 2014, P IEEE INT S INF THE
   Huang JZ, 2011, J MACH LEARN RES, V12, P3371
   Iwen M, 2017, APPL COMPUT HARMON A, V42, P135, DOI 10.1016/j.acha.2015.06.007
   Jaganathan Kishore, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1473, DOI 10.1109/ISIT.2012.6283508
   Jaganathan K, 2013, IEEE INT SYMP INFO, P1022, DOI 10.1109/ISIT.2013.6620381
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Laurent B, 2000, ANN STAT, V28, P1302
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Maiden AM, 2009, ULTRAMICROSCOPY, V109, P1256, DOI 10.1016/j.ultramic.2009.05.012
   Marchesini S, 2007, J OPT SOC AM A, V24, P3289, DOI 10.1364/JOSAA.24.003289
   Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642
   MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Needell D, 2008, CONF REC ASILOMAR C, P1048, DOI 10.1109/ACSSC.2008.5074572
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Nugent KA, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.203902
   Ohlsson H., 2012, ADV NEURAL INFORM PR, P1367
   Pedarsani  R., 2017, IEEE T INFORM THEORY
   Qiao H, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P522, DOI 10.1109/GlobalSIP.2015.7418250
   Saxton W., 1972, OPTIK, V35
   Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673
   Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687
   Soltanolkotabi M, 2017, ARXIV170206175
   Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725
   Talagrand M., 2006, GENERIC CHAINING UPP
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wang G., 2016, ARXIV161107641
   Wang G., 2016, P ADV NEUR INF PROC, P568
   Wei K, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/12/125008
   Yin D, 2016, ANN ALLERTON CONF, P758, DOI 10.1109/ALLERTON.2016.7852309
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhang H., 2016, ADV NEURAL INFORM PR, P2622
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404096
DA 2019-06-15
ER

PT S
AU Jain, L
   Mason, B
   Nowak, R
AF Jain, Lalit
   Mason, Blake
   Nowak, Robert
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Low-Dimensional Metrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper investigates the theoretical foundations of metric learning, focused on three key questions that are not fully addressed in prior work: 1) we consider learning general low-dimensional (low-rank) metrics as well as sparse metrics; 2) we develop upper and lower (minimax) bounds on the generalization error; 3) we quantify the sample complexity of metric learning in terms of the dimension of the feature space and the dimension/rank of the underlying metric; 4) we also bound the accuracy of the learned metric relative to the underlying true generative metric. All the results involve novel mathematical approaches to the metric learning problem, and also shed new light on the special case of ordinal embedding (aka non-metric multidimensional scaling).
C1 [Jain, Lalit] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Mason, Blake; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA.
RP Jain, L (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM lalitj@umich.edu; bmason3@wisc.edu; rdnowak@wisc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1218189, IIS-1623605]
FX This work was partially supported by the NSF grants CCF-1218189 and
   IIS-1623605
CR Abramovich F, 2016, IEEE T INFORM THEORY, V62, P3721, DOI 10.1109/TIT.2016.2555812
   Bellet A., 2015, SYNTH LECT ARTIF INT, V9, P1
   Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044
   Bian W, 2012, IEEE T NEUR NET LEAR, V23, P1194, DOI 10.1109/TNNLS.2012.2198075
   Bunea F, 2007, ANN STAT, V35, P1674, DOI 10.1214/009053606000001587
   Dattorro J., 2011, CONVEX OPTIMIZATION
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   DAVIDSON K. R, 2001, HDB GEOMETRY BANACH, V1, P131
   Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556
   Heim Eric, 2015, ARXIV151102254
   Jain Lalit, 2016, ADV NEURAL INFORM PR, P2703
   Joel A, 2015, INTRO MATRIX CONCENT
   Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574
   Rau Martina A, 2016, P 9 INT C ED DAT MIN, P199
   Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854
   Shi Yuan, 2014, ARXIV14044105
   Ying Y., 2009, ADV NEURAL INFORM PR, P2214
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404021
DA 2019-06-15
ER

PT S
AU Jalali, A
   Willett, R
AF Jalali, Amin
   Willett, Rebecca
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Subspace Clustering via Tangent Cones
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Given samples lying on any of a number of subspaces, subspace clustering is the task of grouping the samples based on the their corresponding subspaces. Many subspace clustering methods operate by assigning a measure of affinity to each pair of points and feeding these affinities into a graph clustering algorithm. This paper proposes a new paradigm for subspace clustering that computes affinities based on the corresponding conic geometry. The proposed conic subspace clustering (CSC) approach considers the convex hull of a collection of normalized data points and the corresponding tangent cones. The union of subspaces underlying the data imposes a strong association between the tangent cone at a sample x and the original subspace containing x. In addition to describing this novel geometric perspective, this paper provides a practical algorithm for subspace clustering that leverages this perspective, where a tangent cone membership test is used to estimate the affinities. This algorithm is accompanied with deterministic and stochastic guarantees on the properties of the learned affinity matrix, on the true and false positive rates and spread, which directly translate into the overall clustering accuracy.
C1 [Jalali, Amin] Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53715 USA.
   [Willett, Rebecca] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA.
RP Jalali, A (reprint author), Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53715 USA.
EM amin.jalali@wisc.edu; willett@discovery.wisc.edu
RI Jeong, Yongwook/N-7413-2016
CR Blair David E., 2000, STUDENT MATH LIB, V9
   Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Elhamifar E, 2010, INT CONF ACOUST SPEE, P1926, DOI 10.1109/ICASSP.2010.5495317
   GORDON Y, 1988, LECT NOTES MATH, V1317, P84
   Heckel R, 2015, IEEE T INFORM THEORY, V61, P6320, DOI 10.1109/TIT.2015.2472520
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   MOREAU JJ, 1962, CR HEBD ACAD SCI, V255, P238
   Nasihatkon B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2137, DOI 10.1109/CVPR.2011.5995679
   Park  D., 2014, ADV NEURAL INFORM PR, P2753
   Renegar J, 2016, SIAM J OPTIMIZ, V26, P2649, DOI 10.1137/15M1027371
   Scholz  F., 2008, CONFIDENCE BOUNDS IN
   Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199
   Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wang  Y., 2013, ADV NEURAL INFORM PR, P64
   Wang Yining, 2016, ARTIF INTELL, P538
   Wang Yu-Xiang, 2016, J MACH LEARN RES, V17, P41
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406078
DA 2019-06-15
ER

PT S
AU Jang, M
   Kim, S
   Suh, C
   Oh, S
AF Jang, Minje
   Kim, Sunghyun
   Suh, Changho
   Oh, Sewoong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Optimal Sample Complexity of M-wise Data for Top-K Ranking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CHOICE
AB We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight l(infinity) estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in l(infinity) error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.
C1 [Jang, Minje; Suh, Changho] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
   [Kim, Sunghyun] Elect & Telecommun Res Inst, Daejeon, South Korea.
   [Oh, Sewoong] UIUC, Ind & Enterprise Syst Engn Dept, Champaign, IL USA.
RP Jang, M (reprint author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
EM jmj427@kaist.ac.kr; koishkim@etri.re.kr; chsuh@kaist.ac.kr;
   swoh@illinois.edu
FU Institute for Information & communications Technology Promotion(IITP) -
   Korea government(MSIT) [2017-0-00694]
FX This work was supported by Institute for Information & communications
   Technology Promotion(IITP) grant funded by the Korea government(MSIT)
   (2017-0-00694, Coding for High-Speed Distributed Networks).
CR Ailon N., 2007, ARXIV07102889
   Ailon N, 2012, J MACH LEARN RES, V13, P137
   Ammar Ammar, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P776
   Azari Soufiani H., 2014, P NEUR INF PROC SYST, P3185
   Baltrunas L, 2010, P 4 ACM C REC SYST, P119, DOI DOI 10.1145/1864708.1864733
   BELL DE, 1982, OPER RES, V30, P961, DOI 10.1287/opre.30.5.961
   Bergstrom CT, 2008, J NEUROSCI, V28, P11433, DOI 10.1523/JNEUROSCI.0003-08.2008
   Bonacich P, 2001, SOC NETWORKS, V23, P191, DOI 10.1016/S0378-8733(01)00038-7
   Borda J, 1781, MEMOIRE ELECTIONS SC
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Braverman M, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P851, DOI 10.1145/2897518.2897642
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   CAPLIN A, 1991, ECONOMETRICA, V59, P1, DOI 10.2307/2938238
   Chen X., 2013, WSDM, P193
   CHEN Y., 2015, P INT C MACH LEARN, P371
   Cheng W., 2010, P 27 INT C MACH LEAR, P215
   Cooley O., 2016, ELECTRON J COMB, P2
   Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI DOI 10.1145/371920.372165
   FISHBURN PC, 1982, J MATH PSYCHOL, V26, P31, DOI 10.1016/0022-2496(82)90034-7
   FORD JR L. R., 1957, AM MATH MONTHLY, V64, P28, DOI DOI 10.2307/2308513
   Graham L., 1982, ECON J, V92, P805
   Guiver J., 2009, P 26 ANN INT C MACH, P377
   HAJEK B., 2014, ADV NEURAL INFORM PR, P1475
   HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943
   Heckel Reinhard, 2016, ARXIV160608842
   Hunter DR, 2004, ANN STAT, V32, P384
   Jamieson K. G., 2011, P NEUR INF PROC SYST, P2240
   Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008
   Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x
   Khetan A, 2016, J MACH LEARN RES, V17
   Luce R. D., 1959, INDIVIDUAL CHOICE BE
   Maystre L, 2015, ADV NEURAL INFORM PR, P172
   MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093
   McFadden D, 1974, FRONTIERS ECONOMETRI, P105, DOI DOI 10.1108/EB028592
   Mohajer S., 2017, P INT C MACH LEARN, P2488
   Negahban S., 2016, OPER RES, V65, P266
   Oh S., 2015, P NEUR INF PROC SYST, P1909
   Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567
   Rajkumar Arun, 2014, P 31 INT C MACH LEAR, P118
   Seeley JR, 1949, CAN J PSYCHOLOGY, V3, P234, DOI 10.1037/h0084096
   Shah  N., 2015, ARXIV151208949
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
   Szorenyi Balazs, 2015, P NEUR INF PROC SYST, P604
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vigna S, 2016, NETW SCI, V4, P433, DOI 10.1017/nws.2016.21
   Walker J, 2002, MATH SOC SCI, V43, P303, DOI 10.1016/S0165-4896(02)00023-9
   Wei T. H, 1952, THESIS
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401070
DA 2019-06-15
ER

PT S
AU Jang, PA
   Loeb, AE
   Davidow, MB
   Wilson, AG
AF Jang, Phillip A.
   Loeb, Andrew E.
   Davidow, Matthew B.
   Wilson, Andrew Gordon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Levy Process Priors for Spectral Kernel Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a Levy process. The resulting distribution has support for all stationary covariances-including the popular RBF, periodic, and Matern kernels-combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the Levy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for O (n) training and O (1) predictions. We perform extrapolations having reasonable uncertainty estimates on several benchmarks, show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization.
C1 [Jang, Phillip A.; Loeb, Andrew E.; Davidow, Matthew B.; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA.
RP Jang, PA (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
FU Natural Sciences and Engineering Research Council of Canada [PGS-D
   502888]; National Science Foundation [DGE 1144153, IIS-1563887]
FX This work is supported in part by the Natural Sciences and Engineering
   Research Council of Canada (PGS-D 502888) and the National Science
   Foundation DGE 1144153 and IIS-1563887 awards.
CR Bochner S., 1959, LECT FOURIER INTEGRA, V42
   Clyde Merlise A, 2007, BAYESIAN STAT, V8, P91
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Green P. J., 2003, TRANSDIMENSIONAL MAR
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   Hyndman R. J., 2005, TIME SERIES DATA LIB
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   Micchelli CA, 2006, J MACH LEARN RES, V7, P2651
   Neal RM, 1996, BAYESIAN LEARNING NE
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Turner R., 2010, THESIS
   Wilson A., 2013, GAUSSIAN PROCESS KER
   Wilson A. G., 2013, INT C MACH LEARN ICM
   Wilson A. G., 2015, INT C MACH LEARN ICM
   Wilson Andrew Gordon, 2014, THESIS
   Wolpert RL, 2011, ANN STAT, V39, P1916, DOI 10.1214/11-AOS889
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404002
DA 2019-06-15
ER

PT S
AU Janner, M
   Wu, JJ
   Kulkarni, TD
   Yildirim, I
   Tenenbaum, JB
AF Janner, Michael
   Wu, Jiajun
   Kulkarni, Tejas D.
   Yildirim, Ilker
   Tenenbaum, Joshua B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Self-Supervised Intrinsic Image Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REFLECTANCE
AB Intrinsic decomposition from a single image is a highly challenging task, due to its inherent ambiguity and the scarcity of training data. In contrast to traditional fully supervised learning approaches, in this paper we propose learning intrinsic image decomposition by explaining the input image. Our model, the Rendered Intrinsics Network (RIN), joins together an image decomposition pipeline, which predicts reflectance, shape, and lighting conditions given a single image, with a recombination function, a learned shading model used to recompose the original input based off of intrinsic image predictions. Our network can then use unsupervised reconstruction error as an additional signal to improve its intermediate representations. This allows large-scale unlabeled data to be useful during training, and also enables transferring learned knowledge to images of unseen object categories, lighting conditions, and shapes. Extensive experiments demonstrate that our method performs well on both intrinsic image decomposition and knowledge transfer.
C1 [Janner, Michael; Wu, Jiajun; Yildirim, Ilker; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA.
   [Kulkarni, Tejas D.] DeepMind, London, England.
RP Janner, M (reprint author), MIT, Cambridge, MA 02139 USA.
EM janner@mit.edu; jiajunwu@mit.edu; tejasdkulkarni@gmail.com;
   ilkery@mit.edu; jbt@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR MURI [N00014-16-1-2007]; Center for Brain, Minds and Machines (NSF)
   [1231216]; Toyota Research Institute; Samsung
FX This work is supported by ONR MURI N00014-16-1-2007, the Center for
   Brain, Minds and Machines (NSF #1231216), Toyota Research Institute, and
   Samsung.
CR Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Barrow HG, 1978, COMPUTER VISION SYST
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Chang A.X., 2015, ARXIV151203012
   Chen Xi, 2016, NIPS
   Grosse R., 2009, ICCV
   He K., 2016, CVPR
   Hinton G. E., 2011, ICANN
   Hold-Geoffroy Y., 2017, CVPR
   HORN BKP, 1974, COMPUT GRAPH IMAGE P, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7
   Innamorati C, 2017, COMPUT GRAPH FORUM, V36, P15, DOI 10.1111/cgf.13220
   Ioffe S., 2015, ICML
   Kar Abhishek, 2015, CVPR
   Kingma D. P., 2015, ICLR
   Kulkarni Tejas D, 2015, NIPS
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318
   Lombardi Stephen, 2012, CVPR
   Nalbach Oliver, 2017, COMPUTER GRAPHICS FO, V36
   Narihira T., 2015, CVPR
   Narihira Takuya, 2015, ICCV
   Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734
   Queau Yvain, 2015, INT C SCAL SPAC VAR
   Rematas Konstantinos, 2016, CVPR
   Shi Jian, 2017, CVPR
   Shu Z, 2017, CVPR
   Tang Y., 2012, ICML
   Weiss Yair, 2001, ICCV
   Wu Jiajun, 2017, NIPS
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406002
DA 2019-06-15
ER

PT S
AU Jas, M
   La Tour, TD
   Simsekli, U
   Gramfort, A
AF Jas, Mainak
   La Tour, Tom Dupre
   Simsekli, Umut
   Gramfort, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning the Morphology of Brain Signals Using Alpha-Stable
   Convolutional Sparse Coding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OSCILLATIONS; ALGORITHM
AB Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such 'shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call alpha CSC, lies a family of heavy-tailed distributions called alpha-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, alpha CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series.
C1 [Jas, Mainak; La Tour, Tom Dupre; Simsekli, Umut; Gramfort, Alexandre] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
   [Gramfort, Alexandre] Univ Paris Saclay, INRIA, Saclay, France.
RP Jas, M (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.
RI Jeong, Yongwook/N-7413-2016
FU French National Research Agency [ANR-14-NEUC-0002-01,
   ANR-13-CORD-0008-02, ANR-16-CE23-0014]; ERC [SLAB ERC-YStG-676943]
FX The work was supported by the French National Research Agency grants
   ANR-14-NEUC-0002-01, ANR-13-CORD-0008-02, and ANR-16-CE23-0014
   (FBIMATRIX), as well as the ERC Starting Grant SLAB ERC-YStG-676943.
CR Agarwal Alekh, 2014, C LEARN THEOR, P123
   Barthelemy Q, 2013, J NEUROSCI METH, V215, P19, DOI 10.1016/j.jneumeth.2013.02.001
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Brockmeier AJ, 2016, IEEE T BIO-MED ENG, V63, P43, DOI 10.1109/TBME.2015.2499241
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   CHAMBERS JM, 1976, J AM STAT ASSOC, V71, P340, DOI 10.2307/2285309
   CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568
   Cohen MX, 2014, ISS CLIN COGN NEUROP, P1
   Cole S. R., 2017, TRENDS COGN SCI
   Dallerac G, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13920
   Gips B, 2017, J NEUROSCI METH, V275, P66, DOI 10.1016/j.jneumeth.2016.11.001
   Godsill S., 1999, P APPL HEAV TAIL DIS
   Gorski J, 2007, MATH METHOD OPER RES, V66, P373, DOI 10.1007/s00186-007-0161-1
   Grosse R, 2007, P 23 C UNC ART INT, P149
   Hari R., 2017, MEG EEG PRIMER
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hitziger S., 2017, IEEE T SIGNAL PROCES
   Huber P. J., 1981, ROBUST STAT
   Jensen O, 2007, TRENDS COGN SCI, V11, P267, DOI 10.1016/j.tics.2007.05.003
   Jones SR, 2016, CURR OPIN NEUROBIOL, V40, P72, DOI 10.1016/j.conb.2016.06.010
   Jost P., 2006, ACOUSTICS SPEECH SIG, V5
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kuruoglu E. E., 1999, THESIS
   Leglaive S, 2017, INT CONF ACOUST SPEE, P576, DOI 10.1109/ICASSP.2017.7952221
   Liu JS, 2008, MONTE CARLO STRATEGI
   Mailhe B., 2008, P 16 EUR SIGN PROC C, P1
   Mandelbrot B. B., 2013, FRACTALS SCALING FIN
   Mazaheri A, 2008, J NEUROSCI, V28, P7781, DOI 10.1523/JNEUROSCI.1631-08.2008
   MOULINES E, 1995, IEEE T SIGNAL PROCES, V43, P516, DOI 10.1109/78.348133
   Pachitariu M, 2013, ADV NEURAL INFORM PR, P1745
   Samorodnitsky G., 1994, STABLE NONGAUSSIAN R, V1
   Simsekli U, 2015, IEEE SIGNAL PROC LET, V22, P2289, DOI 10.1109/LSP.2015.2477535
   Sorel M., 2016, DIGITAL SIGNAL PROCE
   Tort ABL, 2010, J NEUROPHYSIOL, V104, P1195, DOI 10.1152/jn.00106.2010
   Wang YM, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/056009
   Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 37
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401014
DA 2019-06-15
ER

PT S
AU Ji, P
   Zhang, T
   Li, HD
   Salzmann, M
   Reid, I
AF Ji, Pan
   Zhang, Tong
   Li, Hongdong
   Salzmann, Mathieu
   Reid, Ian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Subspace Clustering Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SEGMENTATION
AB We present a novel deep neural network architecture for unsupervised subspace clustering. This architecture is built upon deep auto-encoders, which non-linearly map the input data into a latent space. Our key idea is to introduce a novel self-expressive layer between the encoder and the decoder to mimic the "self-expressiveness" property that has proven effective in traditional subspace clustering. Being differentiable, our new self-expressive layer provides a simple but effective way to learn pairwise affinities between all data points through a standard back-propagation procedure. Being nonlinear, our neural-network based method is able to cluster data points having complex (often nonlinear) structures. We further propose pre-training and fine-tuning strategies that let us effectively learn the parameters of our subspace clustering networks. Our experiments show that our method significantly outperforms the state-of-the-art unsupervised subspace clustering techniques.
C1 [Ji, Pan; Reid, Ian] Univ Adelaide, Adelaide, SA, Australia.
   [Zhang, Tong; Li, Hongdong] Australian Natl Univ, Canberra, ACT, Australia.
   [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland.
RP Ji, P (reprint author), Univ Adelaide, Adelaide, SA, Australia.
FU Australian Research Council (ARC) through the Centre of Excellence in
   Robotic Vision [CE140100016]; Australian Research Council (ARC) through
   Laureate Fellowship [FL130100102]; ARC [DP150104645]
FX This research was supported by the Australian Research Council (ARC)
   through the Centre of Excellence in Robotic Vision, CE140100016, and
   through Laureate Fellowship FL130100102 to IDR. TZ was supported by the
   ARC's Discovery Projects funding scheme (project DP150104645).
CR Abadi M., 2016, ARXIV160304467
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Cai D., 2007, P IEEE INT C COMP VI, P1
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9
   Ciptadi A, 2009, IEEE I CONF COMP VIS, P1765, DOI 10.1109/ICCV.2009.5459394
   Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999
   Dalal N, 2005, PROC CVPR IEEE, P886
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Favaro P, 2011, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2011.5995365
   Feng JS, 2014, PROC CVPR IEEE, P3818, DOI 10.1109/CVPR.2014.482
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ho J, 2003, PROC CVPR IEEE, P11
   Ji P, 2015, IEEE I CONF COMP VIS, P4687, DOI 10.1109/ICCV.2015.532
   Ji P, 2014, IEEE WINT CONF APPL, P461, DOI 10.1109/WACV.2014.6836065
   Kanatani K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P586, DOI 10.1109/ICCV.2001.937679
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li CG, 2015, PROC CVPR IEEE, P277, DOI 10.1109/CVPR.2015.7298624
   Liu G., 2010, P INT C MACH LEARN, V27, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26
   Ma Y., 2007, TPAMI, V29
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Mo QY, 2012, LECT NOTES COMPUT SC, V7578, P402, DOI 10.1007/978-3-642-33786-4_30
   Nene S.A., 1996, CUCS00696
   Nene S. K. N. S. A., 1996, CUCS00596
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ochs P., 2012, CVPR
   Patel VM, 2014, IEEE IMAGE PROC, P2849, DOI 10.1109/ICIP.2014.7025576
   Patel VM, 2013, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2013.35
   Peng X., 2016, IJCAI
   PURKAIT P, 2014, EUR C COMP VIS ECCV, V8692, P672
   Rao S. R., 2008, P IEEE C COMP VIS PA, P1
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z
   Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang  Y., 2013, ADV NEURAL INFORM PR, P64
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xie J., 2016, ICML
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
   Yin M, 2016, PROC CVPR IEEE, P5157, DOI 10.1109/CVPR.2016.557
   You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425
   You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400003
DA 2019-06-15
ER

PT S
AU Jiang, B
   Tang, J
   Ding, C
   Gong, YH
   Luo, B
AF Jiang, Bo
   Tang, Jin
   Ding, Chris
   Gong, Yihong
   Luo, Bin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Graph Matching via Multiplicative Update Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB As a fundamental problem in computer vision, graph matching problem can usually be formulated as a Quadratic Programming (QP) problem with doubly stochastic and discrete (integer) constraints. Since it is NP-hard, approximate algorithms are required. In this paper, we present a new algorithm, called Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update technique to solve the QP matching problem. MPGM has three main benefits: (1) theoretically, MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Empirically, MPGM generally returns a sparse solution and thus can also incorporate the discrete constraint approximately. (3) It is efficient and simple to implement. Experimental results show the benefits of MPGM algorithm.
C1 [Jiang, Bo; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
   [Ding, Chris] Univ Texas Arlington, CSE Dept, Arlington, TX 76019 USA.
   [Gong, Yihong] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian, Shaanxi, Peoples R China.
RP Jiang, B (reprint author), Anhui Univ, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
EM jiangbo@ahu.edu.cn; tj@ahu.edu.cn; chqding@uta.edu;
   ygong@mail.xjtu.edu.cn; luobin@ahu.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU NBRPC 973 Program [2015CB351705]; National Natural Science Foundation of
   China [61602001, 61671018, 61572030]; Natural Science Foundation of
   Anhui Province [1708085QF139]; Natural Science Foundation of Anhui
   Higher Education Institutions of China [KJ2016A020]; Co-Innovation
   Center for Information Supply & Assurance Technology, Anhui University;
   Open Projects Program of National Laboratory of Pattern Recognition
FX This work is supported by the NBRPC 973 Program (2015CB351705); National
   Natural Science Foundation of China (61602001,61671018, 61572030);
   Natural Science Foundation of Anhui Province (1708085QF139); Natural
   Science Foundation of Anhui Higher Education Institutions of China
   (KJ2016A020); Co-Innovation Center for Information Supply & Assurance
   Technology, Anhui University; The Open Projects Program of National
   Laboratory of Pattern Recognition.
CR Adamczewski K, 2015, IEEE I CONF COMP VIS, P109, DOI 10.1109/ICCV.2015.21
   Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228
   Cour T., 2006, P ADV NEUR INF PROC, V1, P313
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Ding C, 2008, IEEE DATA MINING, P183, DOI 10.1109/ICDM.2008.130
   Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619
   Jiang B., 2015, P 29 AAAI C ART INT, P3790
   Jiang B., 2017, AAAI
   Jiang B, 2014, PATTERN RECOGN, V47, P736, DOI 10.1016/j.patcog.2013.08.024
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Leordeanu M., 2009, NEURAL INFORM PROCES, P1114
   Leordeanu M., 2011, INT J COMPUT VISION, V95, P1
   Luo B, 2003, PATTERN RECOGN, V36, P2213, DOI 10.1016/S0031-3203(03)00084-0
   Julian JM, 2012, PATTERN RECOGN, V45, P563, DOI 10.1016/j.patcog.2011.05.008
   van Wyk BJ, 2004, IEEE T PATTERN ANAL, V26, P1526, DOI 10.1109/TPAMI.2004.95
   Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245
   Zass R., 2006, P ADV NEUR INF PROC, P1569
   Zhang Z, 2016, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2016.135
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403025
DA 2019-06-15
ER

PT S
AU Jiang, H
AF Jiang, Heinrich
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On the Consistency of Quick Shift
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MEAN SHIFT
AB Quick Shift is a popular mode-seeking and clustering algorithm. We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions. We then apply our results to construct a consistent modal regression algorithm.
C1 [Jiang, Heinrich] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
RP Jiang, H (reprint author), Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM heinrich.jiang@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Aleksandr Borisovich Tsybakov, 1990, PROBL PEREDACHI INF, V26, P38
   Arias-Castro Ery, 2015, J MACHINE LEARNING R
   Bharath Sriperumbudur and Ingo Steinwart, 2012, ARTIF INTELL, P1090
   CHAUDHURI K., 2010, ADV NEURAL INFORM PR, P343
   Chen YC, 2016, ANN STAT, V44, P489, DOI 10.1214/15-AOS1373
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dasgupta Sanjoy, 2014, ADV NEURAL INFORM PR, P2555
   ELDRIDGE J., 2015, P 28 C LEARN THEOR, P588
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840
   Jiang Heinrich, 2017, INT C ART INT STAT, P1197
   Jiang Heinrich, 2017, INT C MACH LEARN, P1684
   Jiang Heinrich, 2017, INT C MACH LEARN, P1694
   KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Wang Daren, 2017, ARXIV170603113
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400005
DA 2019-06-15
ER

PT S
AU Jiang, Z
   Balu, A
   Hegde, C
   Sarkar, S
AF Jiang, Zhanhong
   Balu, Aditya
   Hegde, Chinmay
   Sarkar, Soumik
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Collaborative Deep Learning in Fixed Topology Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DISTRIBUTED OPTIMIZATION
AB There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.
C1 [Jiang, Zhanhong; Balu, Aditya; Sarkar, Soumik] Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.
   [Hegde, Chinmay] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA USA.
RP Jiang, Z (reprint author), Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.
EM zhjiang@iastate.edu; baditya@iastate.edu; chinmay@iastate.edu;
   soumiks@iastate.edu
RI Jeong, Yongwook/N-7413-2016
FU USDA-NIFA [2017-67021-25965]; National Science Foundation [CNS-1464279,
   CCF-1566281]
FX This paper is based upon research partially supported by the USDA-NIFA
   under Award no. 2017-67021-25965, the National Science Foundation under
   Grant No. CNS-1464279 and No. CCF-1566281. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of the funding
   agencies.
CR Abadi M., 2016, ARXIV160304467
   Blot Michael, 2016, ARXIV161109726
   Bottou  L., 2016, ARXIV160604838
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   Choi HL, 2010, AUTOMATICA, V46, P1266, DOI 10.1016/j.automatica.2010.05.004
   De S, 2016, IEEE DATA MINING, P111, DOI [10.1109/ICDM.2016.0022, 10.1109/ICDM.2016.177]
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Gupta Suyog, 2015, ARXIV150904210
   Hajinezhad Davood, ZENITH ZEROTH ORDER
   Jha DK, 2016, INT J CONTROL, V89, P984, DOI 10.1080/00207179.2015.1110754
   Jin Peter H, 2016, ARXIV161104581
   Lan  G., 2017, ARXIV170103961
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu C, 2017, MEAS SCI TECHNOL, V28, DOI 10.1088/1361-6501/28/1/014011
   McMahan H. B., 2016, ARXIV160205629
   Mukherjee K, 2011, AUTOMATICA, V47, P185, DOI 10.1016/j.automatica.2010.10.031
   Nedic A, 2016, IEEE T AUTOMAT CONTR, V61, P3936, DOI 10.1109/TAC.2016.2529285
   Nedic A, 2015, IEEE T AUTOMAT CONTR, V60, P601, DOI 10.1109/TAC.2014.2364096
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Ram SS, 2012, OPTIM METHOD SOFTW, V27, P71, DOI 10.1080/10556788.2010.511669
   Scaman  K., 2017, ARXIV170208704
   Strom N., 2015, INTERSPEECH, V7, P10
   Su H, 2015, ARXIV150701239
   Yuan K., 2013, ARXIV13107063
   Zeng Jinshan, 2016, ARXIV160805766
   Zhang C., 2016, CORR
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
   Zhang W., 2015, ARXIV151105950
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405095
DA 2019-06-15
ER

PT S
AU Jidling, C
   Wahlstrom, N
   Wills, A
   Schon, TB
AF Jidling, Carl
   Wahlstrom, Niklas
   Wills, Adrian
   Schon, Thomas B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Linearly constrained Gaussian processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FUNCTIONAL REGRESSION
AB We consider a modification of the covariance function in Gaussian processes to correctly account for known linear operator constraints. By modeling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples.
C1 [Jidling, Carl; Wahlstrom, Niklas; Schon, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.
   [Wills, Adrian] Univ Newcastle, Sch Engn, Callaghan, NSW, Australia.
RP Jidling, C (reprint author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.
EM carl.jidling@it.uu.se; niklas.wahlstrom@it.uu.se;
   adrian.wills@newcastle.edu.au; thomas.schon@it.uu.se
RI Jeong, Yongwook/N-7413-2016
FU Swedish Foundation for Strategic Research (SSF) via the project ASSEMBLE
   [RIT 15-0012]; Swedish Research Council (VR) [621-2013-5524]
FX This research is financially supported by the Swedish Foundation for
   Strategic Research (SSF) via the project ASSEMBLE (Contract number: RIT
   15-0012). The work is also supported by the Swedish Research Council
   (VR) via the project Probabilistic modeling of dynamical systems
   (Contract number: 621-2013-5524). We are grateful for the help and
   equipment provided by the UAS Technologies Lab, Artificial Intelligence
   and Integrated Computer Systems Division (AIICS) at the Department of
   Computer and Information Science (IDA), Linkoping University, Sweden.
   The real data set used in this paper has been collected by some of the
   authors together with Manon Kok, Arno Solin, and Simo Sarkka. We thank
   them for allowing us to use this data. We also thank Manon Kok for
   supporting us with the data processing. Furthermore, we would like to
   thank Carl Rasmussen and Marc Deisenroth for fruitful discussions on
   constrained GPs.
CR Abrahamsen P, 2001, MATH GEOL, V33, P719, DOI 10.1023/A:1011078716252
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Andrade-Pacheco Ricardo, 2016, MONITORING SHORT TER, P95
   Constantinescu EM, 2013, INT J UNCERTAIN QUAN, V3, P47, DOI 10.1615/Int.J.UncertaintyQuantification.2012003722
   Da Veiga S., 2012, ANN FAC SCI TOULOUSE, V21, P529
   Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541
   Ginsbourger D, 2016, J STAT PLAN INFER, V170, P117, DOI 10.1016/j.jspi.2015.10.002
   Graepel T., 2003, P 20 INT C MACH LEAR
   Kiraly Franz J., 2014, TECHNICAL REPORT
   Koyejo O, 2013, INT CONF DAT MIN WOR, P72, DOI 10.1109/ICDMW.2013.150
   Luenberger D. G., 1969, OPTIMIZATION VECTOR
   Maatouk H, 2017, MATH GEOSCI, V49, P557, DOI 10.1007/s11004-017-9673-2
   Navarro Alexandre K. W., 2016, TECHNICAL REPORT
   Nguyen NC, 2016, J COMPUT PHYS, V309, P52, DOI 10.1016/j.jcp.2015.12.035
   Nguyen NC, 2015, COMPUT METHOD APPL M, V287, P69, DOI 10.1016/j.cma.2015.01.008
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ross J., 2013, P 30 INT C MACH LEAR, V28, P1346
   Rudovic Ognjen, 2011, P INT C COMP VIS ICC
   Salzmann Mathieu, 2010, NEURAL INFORM PROCES
   Sarkka S, 2011, LECT NOTES COMPUT SC, V6792, P151, DOI 10.1007/978-3-642-21738-8_20
   Tran Cuong, 2015, TECHNICAL REPORT
   Wahlstrom N., 2015, THESIS
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401025
DA 2019-06-15
ER

PT S
AU Jin, L
   Lazarow, J
   Tu, ZW
AF Jin, Long
   Lazarow, Justin
   Tu, Zhuowen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Introspective Classification with Convolutional Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative - being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.
C1 [Jin, Long; Lazarow, Justin; Tu, Zhuowen] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Jin, L (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM longjin@ucsd.edu; jlazarow@ucsd.edu; ztu@ucsd.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1618477, IIS-1717431]; Northrop Grumman Contextual Robotics
   grant
FX This work is supported by NSF IIS-1618477, NSF IIS-1717431, and a
   Northrop Grumman Contextual Robotics grant. We thank Saining Xie,
   Weijian Xu, Fan Fan, Kwonjoon Lee, Shuai Tang, and Sanjoy Dasgupta for
   helpful discussions.
CR Baldi P., 2012, J MACH LEARN RES P T, V27, P37
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brock A., 2017, ICLR
   Carreira-Perpignan M. A., 2005, P 10 INT WORKSH ART, V5, P33
   Chen T., 2014, ICML
   DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2015, ICLR
   Goodfellow I., 2014, NIPS
   He  K., 2016, DEEP RESIDUAL LEARNI
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jebara T, 2012, MACHINE LEARNING DIS, V755
   Kim T., DCGAN TENSORFLOW
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Krizhevsky Alex, 2010, CONVOLUTIONAL UNPUB
   Lazarow J., 2017, ICCV
   Leake D. B., 2012, ENCY SCI LEARNING, P1638
   LeCun  Y., 1998, MNIST DATABASE HANDW
   LeCun Y., 1989, NEURAL COMPUTATION
   Lee C. Y., 2015, AISTATS
   Lee Chen-Yu, 2016, AISTATS
   Liang P., 2008, ICML
   Liu JS, 2008, MONTE CARLO STRATEGI
   Maas A. L., 2013, ICML
   Mandt S., 2017, ARXIV170404289
   Mooney C. Z., 1993, BOOTSTRAPPING NONPAR
   Mordvintsev A., 2015, GOOGLE RES
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77, DOI 10.1613/jair.279
   Radford A., 2016, ICLR
   Salimans T., 2016, NIPS
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Sinha A., 2017, ARXIV170404959
   Szegedy C., 2016, CVPR
   Tu Z., 2007, CVPR
   Tu Z, 2008, IEEE T MED IMAGING, V27, P495, DOI 10.1109/TMI.2007.908121
   Vapnik VN, 1995, NATURE STAT LEARNING
   Welling M., 2011, ICML
   Welling M., 2002, NIPS
   Wu Y., TENSORPACK TOOLBOX
   Wu Y. N., 2000, INT J COMPUTER VISIO, V38
   Xie J., 2016, ICML
   Xie J., 2016, ARXIV160909408
   Zhao J., 2017, ICLR
   Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627
   Zhu X, 2005, 1530 U WISC
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400079
DA 2019-06-15
ER

PT S
AU Jin, XJ
   Xiao, HX
   Shen, XH
   Yang, JM
   Lin, Z
   Chen, Y
   Jie, ZQ
   Feng, JS
   Yan, SC
AF Jin, Xiaojie
   Xiao, Huaxin
   Shen, Xiaohui
   Yang, Jimei
   Lin, Zhe
   Chen, Yunpeng
   Jie, Zequn
   Feng, Jiashi
   Yan, Shuicheng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Predicting Scene Parsing and Motion Dynamics in the Future
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The ability of predicting the future is important for intelligent systems, e.g. autonomous vehicles and robots to plan early and make decisions accordingly. Future scene parsing and optical flow estimation are two key tasks that help agents better understand their environments as the former provides dense semantic information, i.e. what objects will be present and where they will appear, while the latter provides dense motion information, i.e. how the objects will move. In this paper, we propose a novel model to simultaneously predict scene parsing and optical flow in unobserved future video frames. To our best knowledge, this is the first attempt in jointly predicting scene parsing and motion dynamics. In particular, scene parsing enables structured motion prediction by decomposing optical flow into different groups while optical flow estimation brings reliable pixel-wise correspondence to scene parsing. By exploiting this mutually beneficial relationship, our model shows significantly better parsing and motion prediction results when compared to well-established baselines and individual prediction models on the large-scale Cityscapes dataset. In addition, we also demonstrate that our model can be used to predict the steering angle of the vehicles, which further verifies the ability of our model to learn latent representations of scene dynamics.
C1 [Jin, Xiaojie] NUS, Grad Sch Integrat Sci & Engn NGS, Singapore, Singapore.
   [Xiao, Huaxin; Chen, Yunpeng; Feng, Jiashi; Yan, Shuicheng] NUS, Dept ECE, Singapore, Singapore.
   [Shen, Xiaohui; Yang, Jimei; Lin, Zhe] Adobe Res, Seattle, WA USA.
   [Jie, Zequn] Tencent AI Lab, Bellevue, WA USA.
   [Yan, Shuicheng] Qihoo AI 360 Inst, Beijing, Peoples R China.
RP Jin, XJ (reprint author), NUS, Grad Sch Integrat Sci & Engn NGS, Singapore, Singapore.
RI Jeong, Yongwook/N-7413-2016
FU National University of Singapore startup grant [R-263-000-C08-133];
   Ministry of Education of Singapore AcRF Tier One grant
   [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]
FX The work of Jiashi Feng was partially supported by National University
   of Singapore startup grant R-263-000-C08-133, Ministry of Education of
   Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant
   R-263-000-C67-646.
CR Argyriou  A., 2007, NIPS
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Chao Y.-W., 2017, CVPR
   Chen L. C., 2015, ICLR
   Cordts M., 2016, ARXIV160401685
   Evgeniou T., 2004, SIGKDD
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fischer P, 2015, ARXIV150406852
   Fouhey D. F., 2014, CVPR
   Garcia-Garcia A., 2017, ARXIV170406857
   Goodfellow I., 2014, NIPS
   He K., 2016, CVPR
   Jaeger H., 2002, TUTORIAL TRAINING RE, V5
   Jin X., 2017, ICCV
   Jin Xiaojie, 2017, AAAI
   Long  J., 2015, CVPR
   Lotter W., 2015, ARXIV151106380
   Luo Zelun, 2017, ARXIV170101821
   Mathieu M, 2015, ARXIV151105440
   Mester R, 2014, IEEE SW SYMP IMAG, P113, DOI 10.1109/SSIAI.2014.6806042
   Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3
   Neverova Natalia, 2017, ARXIV170307684
   Pathak D., 2016, CVPR
   Patraucean V, 2015, ARXIV151106309
   Revaud J., 2015, CVPR
   Roy Anirban, 2014, CVPR
   Santana E., 2016, CORR
   Schwing A. G., 2015, ARXIV150302351
   Sevilla-Lara L., 2016, CVPR
   Socher R., 2011, ICML
   Srivastava N, 2015, ICML
   Villegas R., 2017, ICML
   Villegas R., 2017, ICLR
   Vondrick C., 2016, NIPS
   Walker  Jacob, 2014, CVPR, P2
   Walker Jacob, 2015, ICCV 2015
   Yuen J, 2010, ECCV
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407001
DA 2019-06-15
ER

PT S
AU Jitkrittum, W
   Xu, WK
   Szabo, Z
   Fukumizu, K
   Gretton, A
AF Jitkrittum, Wittawat
   Xu, Wenkai
   Szabo, Zoltan
   Fukumizu, Kenji
   Gretton, Arthur
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Linear-Time Kernel Goodness-of-Fit Test
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.
C1 [Jitkrittum, Wittawat; Xu, Wenkai; Gretton, Arthur] UCL, Gatsby Unit, London, England.
   [Szabo, Zoltan] Ecole Polytech, CMAP, Palaiseau, France.
   [Fukumizu, Kenji] Inst Stat Math, Tachikawa, Tokyo, Japan.
RP Jitkrittum, W (reprint author), UCL, Gatsby Unit, London, England.
EM wittawatj@gmail.com; wenkaix@gatsby.ucl.ac.uk;
   zoltan.szabo@polytechnique.edu; fukumizu@ism.ac.jp;
   arthur.gretton@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU Gatsby Charitable Foundation; Data Science Initiative; KAKENHI
   Innovative Areas [25120012]
FX WJ, WX, and AG thank the Gatsby Charitable Foundation for the financial
   support. ZSz was financially supported by the Data Science Initiative.
   KF has been supported by KAKENHI Innovative Areas 25120012.
CR BAHADUR RR, 1960, ANN MATH STAT, V31, P276, DOI 10.1214/aoms/1177705894
   Baringhaus L., 1988, METRIKA, V35, P339, DOI [10.1007/BF02613322, DOI 10.1007/BF02613322]
   BEIRLANT J, 1994, CAN J STAT, V22, P309, DOI 10.2307/3315594
   Bhatia  R., 2013, MATRIX ANAL, V169
   BOWMAN AW, 1993, J AM STAT ASSOC, V88, P529, DOI 10.2307/2290333
   Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503
   Chwialkowski K., 2014, ADV NEURAL INFORM PR, P3608
   Chwialkowski K., 2016, P INT C MACH LEARN, P2606
   Chwialkowski K. P, 2015, P ADV NEUR INF PROC, V28, P1981
   Epps T. W., 1986, J STATISTICAL COMPUT, V26, P177, DOI DOI 10.1080/00949658608810963
   Gleser L. J., 1964, MEASURE TEST EFFICIE, V35, P1537
   Gleser L. J., 1966, COMP MULTIVARIATE TE, V28, P157
   Gorham J., 2015, ADV NEURAL INFORM PR, P226
   Gorham J., 2017, ICML, P1292
   Gretton  A., 2012, ADV NEURAL INFORM PR, P1205
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gyorfi L., 1990, NONPARAMETRIC FUNCTI, P631
   Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181
   Jitkrittum  Wittawat, 2017, INT C MACH LEARN ICM, P1742
   Ley C, 2017, PROBAB SURV, V14, P1, DOI 10.1214/16-PS278
   Lin BQ, 2017, STAT COMPUT, V27, P1401, DOI 10.1007/s11222-016-9694-6
   Liu Qiang, 2016, INT C MACH LEARN, P276
   Lloyd JR, 2015, ADV NEURAL INFORM PR, P829
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Mityagin  B., 2015, ARXIV151207276
   Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185
   Rizzo ML, 2009, ASTIN BULL, V39, P691, DOI 10.2143/AST.39.2.2044654
   Serfling RJ, 2009, APPROXIMATION THEORE
   Steinwart I, 2008, INFORM SCI STAT, P1
   Sutherland D. J., 2016, ICLR
   Szekely GJ, 2005, J MULTIVARIATE ANAL, V93, P58, DOI 10.1016/j.jmva.2003.12.002
   van der Vaart A. W., 2000, ASYMPTOTIC STAT
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400025
DA 2019-06-15
ER

PT S
AU Joshi, B
   Amini, MR
   Partalas, I
   Iutzeler, F
   Maximov, Y
AF Joshi, Bikash
   Amini, Massih-Reza
   Partalas, Ioannis
   Iutzeler, Franck
   Maximov, Yury
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Aggressive Sampling for Multi-class to Binary Reduction with
   Applications to Text Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.
C1 [Joshi, Bikash; Amini, Massih-Reza] Univ Grenoble Alps, LIG, Grenoble, France.
   [Partalas, Ioannis] Expedia EWE, Geneva, Switzerland.
   [Iutzeler, Franck] Univ Grenoble Alps, LJK, Grenoble, France.
   [Maximov, Yury] Los Alamos Natl Lab, Los Alamos, NM USA.
   [Maximov, Yury] Skolkovo IST, Moscow, Russia.
RP Joshi, B (reprint author), Univ Grenoble Alps, LIG, Grenoble, France.
EM bikash.joshi@imag.fr; massih-reza.amini@imag.fr; ipartalas@expedia.com;
   franck.iutzeler@imag.fr; yury@lanl.gov
RI Jeong, Yongwook/N-7413-2016
FU LabEx PERSYVAL-Lab - French program Investissement d'avenir
   [ANR-11-LABX-0025-01]; U.S. Department of Energy's Office of Electricity
   as part of the DOE Grid Modernization Initiative
FX This work has been partially supported by the LabEx PERSYVAL-Lab
   (ANR-11-LABX-0025-01) funded by the French program Investissement
   d'avenir, and by the U.S. Department of Energy's Office of Electricity
   as part of the DOE Grid Modernization Initiative.
CR Abe N., 2004, P 10 ACM SIGKDD INT, P3
   Babbar Rohit, 2014, SIGKDD EXPLORATIONS, V16
   Bengio S., 2010, ADV NEURAL INFORM PR, P163
   Beygelzimer A, 2009, LECT NOTES ARTIF INT, V5809, P247
   Bhatia K., 2015, ADV NEURAL INFORM PR, P730
   Choromanska A., 2013, NIPS WORKSH EX UNPUB
   Choromanska Anna, 2014, CORR
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Daume H., 2016, ARXIV160604988
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Hsu D., 2009, ADV NEURAL INFORM PR, V22, P772
   Hullermeier E, 2007, LECT NOTES ARTIF INT, V4701, P583
   Jain H., 2016, P 22 ACM SIGKDD INT, P935
   Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008
   Jasinska Kalina, 2016, ARXIV161101964
   Joshi B, 2015, LECT NOTES COMPUT SC, V9385, P132, DOI 10.1007/978-3-319-24465-5_12
   Lorena AC, 2008, ARTIF INTELL REV, V30, P19, DOI 10.1007/s10462-009-9114-9
   Mineiro P, 2015, LECT NOTES ARTIF INT, V9284, P37, DOI 10.1007/978-3-319-23528-8_3
   Partalas I., 2015, ARXIV E PRINTS
   Prabhu Y., 2014, P 20 ACM SIGKDD INT, P263
   Qin T., 2007, P SIGIR 2007 WORKSH, P3
   Ralaivola Liva, 2015, P 32 INT C MACH LEAR, P2436
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Usunier N, 2005, ADV NEURAL INFORM PR, V18, P1369
   Vapnik V. N., 1998, STAT LEARNING THEORY
   WESTON J, 1998, CSDTR9804 U LOND ROY
   Weston Jason, 2011, P INT JOINT C ARTIFI
   Yen I. E., 2016, P 33 INT C MACH LEAR
   Yu H., 2014, P 31 INT C MACH LEAR, P593
   Zemel R. S., 2012, ADV NEURAL INFORM PR, P2294
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404023
DA 2019-06-15
ER

PT S
AU Jun, KS
   Bhargava, A
   Nowak, R
   Willett, R
AF Jun, Kwang-Sung
   Bhargava, Aniruddha
   Nowak, Robert
   Willett, Rebecca
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Generalized Linear Bandits: Online Computation and Hashing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION; FEEDBACK
AB Generalized Linear Bandits (GLBs), a natural extension of the stochastic linear bandits, has been popular and successful in recent years. However, existing GLBs scale poorly with the number of rounds and the number of arms, limiting their utility in practice. This paper proposes new, scalable solutions to the GLB problem in two respects. First, unlike existing GLBs, whose per-time-step space and time complexity grow at least linearly with time t, we propose a new algorithm that performs online computations to enjoy a constant space and time complexity. At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes any online learning algorithm and turns it into a GLB algorithm. As a special case, we apply GLOC to the online Newton step algorithm, which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work. Second, for the case where the number N of arms is very large, we propose new algorithms in which each next arm is selected via an inner product search. Such methods can be implemented via hashing algorithms (i.e., "hash-amenable") and result in a time complexity sublinear in N. While a Thompson sampling extension of GLOC is hash-amenable, its regret bound for d-dimensional arm sets scales with d(3/2), whereas GLOC's regret bound scales with d. Towards closing this gap, we propose a new hash-amenable algorithm whose regret bound scales with d(5/4). Finally, we propose a fast approximate hash-key computation (inner product) with a better accuracy than the state-of-the-art, which can be of independent interest. We conclude the paper with preliminary experimental results confirming the merits of our methods.
C1 [Jun, Kwang-Sung; Bhargava, Aniruddha; Nowak, Robert; Willett, Rebecca] UW Madison, Madison, WI 53706 USA.
RP Jun, KS (reprint author), UW Madison, Madison, WI 53706 USA.
EM kjun@discovery.wisc.edu; aniruddha@wisc.edu; rdnowak@wisc.edu;
   willett@discovery.wisc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1447449]; MURI grant [2015-05174-04]
FX This work was partially supported by the NSF grant IIS-1447449 and the
   MURI grant 2015-05174-04. The authors thank Yasin Abbasi-Yadkori and
   Anshumali Shrivastava for providing constructive feedback and Xin Hunt
   for her contribution at the initial stage.
CR Abbasi- Yadkori Yasin, 2012, P INT C ART INT STAT
   Abbasi- Yadkori Yasin, 2011, ADV NEURAL INFORM PR, P1
   Abeille M, 2017, P 20 INT C ATR INT S, V54, P176
   Agrawal S., 2013, P 30 INT C MACH LEAR, P127
   Agrawal Shipra, 2012, ABS12093 CORR
   Ahukorala K., 2015, P 24 ACM INT C INF K, P1703, DOI DOI 10.1145/2806416.2806609
   Auer P., 2002, J MACHINE LEARNING R, V3, p397 , DOI DOI 10.1162/153244303321897663
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Crammer K, 2013, MACH LEARN, V90, P347, DOI 10.1007/s10994-012-5321-8
   Dani V., 2008, COLT, P355
   Datar M., 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Dekel O, 2012, J MACH LEARN RES, V13, P2655
   Dekel Ofer, 2010, P C LEARN THEOR COL
   Filippi S., 2010, ADV NEURAL INFORM PR, V22, P586
   Gentile C, 2014, J MACH LEARN RES, V15, P2451
   Guo Ruiqi, 2016, J MACHINE LEARNING R, P482
   Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI [DOI 10.4086/T0C.2012.V008A014, 10.4086/toc.2012.v008a014]
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan Elad, 2016, J MACHINE LEARNING R, V17, P1
   Hofmann Katja, 2011, NIPS WORKSH BAYES OP
   Jain Prateek, 2010, NIPS, P928
   Kalai Adam Tauman, 2009, P C LEARN THEOR COL
   Kannan R, 2008, FOUND TRENDS THEOR C, V4, P157, DOI 10.1561/0400000025
   Konyushkova Ksenia, 2013, 21 EUR S ART NEUR NE
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Li L., 2012, P WORKSH ON LIN TRAD, V26, P19
   Li Lihong, 2017, ABS17030 CORR
   McCullagh P., 1989, GEN LINEAR MODELS
   Neyshabur B., 2015, ICML, P1926
   Orabona F., 2012, INT C ART INT STAT A, V22, P823
   Qin L, 2014, P 2014 SIAM INT C DA, P461
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446
   Shrivastava A, 2014, NIPS, V27, P2321
   Shrivastava Anshumali, 2015, P 31 C UNC ART INT U, P812
   Slaney M, 2012, P IEEE, V100, P2604, DOI 10.1109/JPROC.2012.2193849
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang Jingdong, 2014, ABS14082 CORR
   Yue Yisong, 2012, P INT C MACH LEARN I, P1895
   Zhang  L., 2016, P INT C MACH LEARN, P392
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400010
DA 2019-06-15
ER

PT S
AU Jung, YH
   Goetz, J
   Tewari, A
AF Jung, Young Hun
   Goetz, Jack
   Tewari, Ambuj
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Multiclass Boosting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recent work has extended the theoretical analysis of boosting algorithms to multiclass problems and to online settings. However, the multiclass extension is in the batch setting and the online extensions only consider binary classification. We fill this gap in the literature by defining, and justifying, a weak learning condition for online multiclass boosting. This condition leads to an optimal boosting algorithm that requires the minimal number of weak learners to achieve a certain accuracy. Additionally, we propose an adaptive algorithm which is near optimal and enjoys an excellent performance on real data due to its adaptive property.
C1 [Jung, Young Hun; Goetz, Jack; Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
RP Jung, YH (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
EM yhjung@umich.edu; jrgoetz@umich.edu; tewaria@umich.edu
FU NSF [CAREER IIS-1452099, CIF-1422157]
FX We acknowledge the support of NSF under grants CAREER IIS-1452099 and
   CIF-1422157.
CR Beygelzimer Alina, 2015, ICML
   BLAKE CL, 1998, UCI MACHINE LEARNING
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chen Shang-Tse, 2014, P 31 ICML, P342
   Chen Shang-Tse, 2012, ICML
   Daniely  A., 2011, J MACHINE LEARNING R, P207
   Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Higuera C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129126
   Hu Hanzhang, 2017, P 20 INT C ART INT S, P595
   Korytkowski M, 2016, INFORM SCIENCES, V327, P175, DOI 10.1016/j.ins.2015.08.030
   LITTLESTONE N, 1989, ANN IEEE SYMP FOUND, P256, DOI 10.1109/SFCS.1989.63487
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   Mukherjee I, 2013, J MACH LEARN RES, V14, P437
   Oza NC, 2005, IEEE SYS MAN CYBERN, P2340
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Schapire RE, 2001, MACH LEARN, V43, P265, DOI 10.1023/A:1010800213066
   Slud Eric V, 1977, ANN PROBAB, P404
   Ugulino Wallace, 2012, Advances in Artificial Intelligence - SBIA 2012. Proceedings 21th Brazilian Symposium on Artificial Intelligence, P52, DOI 10.1007/978-3-642-34459-6_6
   Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371
   Zhang, 2014, P INTERSPEECH, P1534
   Zinkevich Martin, 2003, P 20 ICML
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400088
DA 2019-06-15
ER

PT S
AU Kamp, M
   Boley, M
   Missura, O
   Gartner, T
AF Kamp, Michael
   Boley, Mario
   Missura, Olana
   Gartner, Thomas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Effective Parallelisation for Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPLEXITY; ALGORITHMS
AB We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on the efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.
C1 [Kamp, Michael] Univ Bonn, Bonn, Germany.
   [Kamp, Michael] Fraunhofer IAIS, St Augustin, Germany.
   [Boley, Mario] Max Planck Inst Informat, Saarbrucken, Germany.
   [Boley, Mario] Saarland Univ, Saarbrucken, Germany.
   [Missura, Olana] Google Inc, Mountain View, CA USA.
   [Gartner, Thomas] Univ Nottingham, Nottingham, England.
RP Kamp, M (reprint author), Univ Bonn, Bonn, Germany.; Kamp, M (reprint author), Fraunhofer IAIS, St Augustin, Germany.
EM kamp@cs.uni-bonn.de; mboley@mpi-inf.mpg.de; olanam@google.com;
   thomas.gaertner@nottingham.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU German Science Foundation (DFG) [GA 1615/1-1, GA 1615/2-1]
FX Part of this work was conducted while Mario Boley, Olana Missura, and
   Thomas Gartner were at the University of Bonn and partially funded by
   the German Science Foundation (DFG, under ref. GA 1615/1-1 and GA
   1615/2-1). The authors would like to thank Dino Oglic, Graham Hutton,
   Roderick MacKenzie, and Stefan Wrobel for valuable discussions and
   comments.
CR Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090
   Balcan M., 2016, P ACM SIGKDD INT C K, P725
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   BLUM M, 1967, J ACM, V14, P322, DOI 10.1145/321386.321395
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Chandra A.K., 1976, 17TH P IEEE S F COMP, P98
   Clarkson KL, 1996, INT J COMPUT GEOM AP, V6, P357, DOI 10.1142/S021819599600023X
   Cook S., 1979, 11TH P ANN ACM S THE, P338
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Freund Yoav, 2001, P 8 INT WORKSH ART I
   Greenlaw Raymond, 1995, LIMITS PARALLEL COMP
   Hanneke S, 2016, J MACH LEARN RES, V17
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   KAY DC, 1971, PAC J MATH, V38, P471, DOI 10.2140/pjm.1971.38.471
   KRUSKAL CP, 1990, THEOR COMPUT SCI, V71, P95, DOI 10.1016/0304-3975(90)90192-K
   Kumar V, 1994, INTRO PARALLEL COMPU
   Lichman M., 2013, UCI MACHINE LEARNING
   Lin Shao-Bo, 2017, J MACHINE LEARNING R, V18, P1
   Long PM, 2013, J MACH LEARN RES, V14, P3105
   Ma C, 2017, OPTIM METHOD SOFTW, V32, P813, DOI 10.1080/10556788.2016.1278445
   Mcdonald Ryan, 2009, ADV NEURAL INFORM PR, P1231
   McMahan H. B., 2017, ARTIF INTELL, P1273
   Meng XR, 2016, J MACH LEARN RES, V17
   Moler Cleve, 1986, HYPERCUBE MULTIPROCE, V86, P31
   Nouretdinov I, 2011, NEUROIMAGE, V56, P809, DOI 10.1016/j.neuroimage.2010.05.023
   Oglic D., 2017, P 34 INT C MACH LEAR, V70, P2652
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Radon J, 1921, MATH ANN, V83, P113, DOI 10.1007/BF01464231
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rosenblatt JD, 2016, INF INFERENCE, V5, P379, DOI 10.1093/imaiai/iaw013
   Rubinov Alexander M., 2013, ABSTRACT CONVEXITY G, V44
   Shamir O., 2014, INT C MACH LEARN, P1000
   Shamir O, 2014, ANN ALLERTON CONF, P850, DOI 10.1109/ALLERTON.2014.7028543
   Sommer R, 2010, P IEEE S SECUR PRIV, P305, DOI 10.1109/SP.2010.25
   Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1
   Tukey JW, 1975, P INT C MATH, P523
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vanschoren J., 2013, ACM SIGKDD EXPLORATI, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   VITTER JS, 1992, INFORM COMPUT, V96, P179, DOI 10.1016/0890-5401(92)90047-J
   von Luxburg U, 2011, HBK HIST LOGIC, V10, P651
   Witten IH, 2017, DATA MINING: PRACTICAL MACHINE LEARNING TOOLS AND TECHNIQUES, 4TH EDITION, P1
   Zhang YC, 2013, J MACH LEARN RES, V14, P3321
   Zinkevich M., 2010, ADV NEURAL INFORM PR, V23, P2595
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406053
DA 2019-06-15
ER

PT S
AU Kanai, S
   Fujiwara, Y
   Iwamura, S
AF Kanai, Sekitoshi
   Fujiwara, Yasuhiro
   Iwamura, Sotetsu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Preventing Gradient Explosions in Gated Recurrent Units
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABILITY
AB A gated recurrent unit (GRU) is a successful recurrent neural network architecture for time-series data. The GRU is typically trained using a gradient-based method, which is subject to the exploding gradient problem in which the gradient increases significantly. This problem is caused by an abrupt change in the dynamics of the GRU due to a small variation in the parameters. In this paper, we find a condition under which the dynamics of the GRU changes drastically and propose a learning method to address the exploding gradient problem. Our method constrains the dynamics of the GRU so that it does not drastically change. We evaluated our method in experiments on language modeling and polyphonic music modeling. Our experiments showed that our method can prevent the exploding gradient problem and improve modeling accuracy.
C1 [Kanai, Sekitoshi; Fujiwara, Yasuhiro; Iwamura, Sotetsu] NTT Software Innovat Ctr, 3-9-11 Midori Cho, Musashino, Tokyo, Japan.
RP Kanai, S (reprint author), NTT Software Innovat Ctr, 3-9-11 Midori Cho, Musashino, Tokyo, Japan.
EM kanai.sekitoshi@lab.ntt.co.jp; fujiwara.yasuhiro@lab.ntt.co.jp;
   iwamura.sotetsu@lab.ntt.co.jp
RI Jeong, Yongwook/N-7413-2016
CR Amodei D., 2016, INT C MACH LEARN, P173
   Arjovsky  M., 2016, INT C MACH LEARN, P1120
   Baldi P, 1996, ADV NEUR IN, V8, P451
   Barabanov NE, 2002, IEEE T NEURAL NETWOR, V13, P292, DOI 10.1109/72.991416
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chilali M, 1996, IEEE T AUTOMAT CONTR, V41, P358, DOI 10.1109/9.486637
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J., 2014, CORR
   Collins Jasmine, 2017, P ICLR
   DOYA K, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P2777, DOI 10.1109/ISCAS.1992.230622
   Doyon Bernard, 1993, P NIPS, P549
   Graves A., 2009, ADV NEURAL INFORM PR, P545
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Haschke R, 2005, NEUROCOMPUTING, V64, P25, DOI 10.1016/j.neucom.2004.11.030
   Hermans M., 2013, ADV NEURAL INFORM PR, P190
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jaeger H., 2002, TUTORIAL TRAINING RE
   Jozefowicz R., 2015, P 32 INT C MACH LEAR, V37, P2342, DOI DOI 10.1109/CVPR.2015.72987
   Kingma D. P., 2015, P ICLR
   Krueger David, 2016, P ICLR
   KUAN CM, 1994, NEURAL COMPUT, V6, P420, DOI 10.1162/neco.1994.6.3.420
   Laurent Thomas, 2017, P ICLR
   Lewandowski N. B., 2012, P 29 INT C MACH LEAR, P1159
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Mikolov T., 2012, THESIS
   Nakahara H, 1996, ADV NEUR IN, V8, P38
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Saxe Andrew M, 2014, P ICLR
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Suykens JAK, 2000, IEEE T NEURAL NETWOR, V11, P222, DOI 10.1109/72.822525
   Talathi S. S., 2015, ARXIV151103771
   Tang ZY, 2017, INT CONF ACOUST SPEE, P2736, DOI 10.1109/ICASSP.2017.7952654
   TOKER O, 1995, PROCEEDINGS OF THE 1995 AMERICAN CONTROL CONFERENCE, VOLS 1-6, P2525
   Tropp J. A., 2009, ARXIV09094061
   Vorontsov Eugene, 2017, P ICML
   Wiggins Stephen, 2003, INTRO APPL NONLINEAR, V2
   Yu W, 2004, INFORM SCIENCES, V158, P131, DOI 10.1016/j.ins.2003.08.002
   Zaremba W, 2014, ARXIV14092329
NR 38
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400042
DA 2019-06-15
ER

PT S
AU Kang, D
   Dhar, D
   Chan, AB
AF Kang, Di
   Dhar, Debarun
   Chan, Antoni B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Incorporating Side Information by Adaptive Convolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned "filter manifold" sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information.
C1 [Kang, Di; Dhar, Debarun; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
RP Kang, D (reprint author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM dkang5-c@my.cityu.edu.hk; ddhar2-c@my.cityu.edu.hk; abchan@cityu.edu.hk
RI Jeong, Yongwook/N-7413-2016
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [T32-101/15-R]; City University of Hong Kong [7004682]; NVIDIA
   Corporation
FX The work described in this paper was supported by a grant from the
   Research Grants Council of the Hong Kong Special Administrative Region,
   China (Project No. [T32-101/15-R]), and by a Strategic Research Grant
   from City University of Hong Kong (Project No. 7004682). We gratefully
   acknowledge the support of NVIDIA Corporation with the donation of the
   Tesla K40 GPU used for this research.
CR Arteta C., 2014, ECCV
   Burger H. C., 2012, CVPR
   Chan A. B., 2012, IEEE T IMAGE PROCESS
   Chan A. B., 2009, ICCV
   Chan A. B., 2008, P IEEE C COMP VIS PA, P1, DOI [DOI 10.1109/CVPR.2008.4587569, 10.1109/CVPR.2008.4587569]
   Ciregan D., 2012, PROC CVPR IEEE, P3642, DOI [10.1109/CVPR.2012.6248110, DOI 10.1109/CVPR.2012.6248110]
   De Brabandere  B., 2016, NIPS
   Eigen D, 2013, ICCV
   Fiaschi L., 2012, ICPR
   Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399
   Ha D., 2017, ICLR
   He K., 2016, CVPR
   Hodosh M., 2013, J ARTIFICIAL INTELLI
   Idrees H., 2013, CVPR
   Ioffe S., 2015, ICML
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Kang D, 2017, ARXIV170510118
   Klein Benjamin, 2015, CVPR
   Krizhevsky A., 2012, NIPS
   Lempitsky V, 2010, NIPS
   Li S., 2015, IJCV
   Ma Ziyang, 2015, CVPR
   Maas A. L., 2013, ICML
   Onoro- Rubio D., 2016, ECCV
   Rodriguez M, 2011, ICCV
   Simonyan Karen, 2015, ICLR
   Sun Yi, 2014, NIPS
   Xu  L., 2014, NIPS
   Zhang Cong, 2015, CVPR
   Zhang Y., 2016, CVPR
   Zhang Z., 2014, ECCV
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403090
DA 2019-06-15
ER

PT S
AU Kanitscheider, I
   Fiete, I
AF Kanitscheider, Ingmar
   Fiete, Ila
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Training recurrent networks to generate hypotheses about how the brain
   solves hard navigation problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID HIPPOCAMPAL PLACE CELLS; SPATIAL MAP; DYNAMICS; CONTEXT
AB Self-localization during navigation with noisy sensors in an ambiguous world is computationally challenging, yet animals and humans excel at it. In robotics, Simultaneous Location and Mapping (SLAM) algorithms solve this problem through joint sequential probabilistic inference of their own coordinates and those of external spatial landmarks. We generate the first neural solution to the SLAM problem by training recurrent LSTM networks to perform a set of hard 2D navigation tasks that require generalization to completely novel trajectories and environments. Our goal is to make sense of how the diverse phenomenology in the brain's spatial navigation circuits is related to their function. We show that the hidden unit representations exhibit several key properties of hippocampal place cells, including stable tuning curves that remap between environments. Our result is also a proof of concept for end-to-end-learning of a SLAM algorithm using recurrent networks, and a demonstration of why this approach may have some advantages for robotic SLAM.
C1 [Kanitscheider, Ingmar; Fiete, Ila] Univ Texas Austin, Dept Neurosci, Austin, TX 78712 USA.
RP Kanitscheider, I (reprint author), Univ Texas Austin, Dept Neurosci, Austin, TX 78712 USA.
EM ikanitscheider@mail.clm.utexas.edu; ilafiete@mail.clm.utexas.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CRCNS 26-1004-04xx]; HFSP award [26-6302-87]; Simons Foundation
   through the Simons Collaboration on the Global Brain
FX This work is supported by the NSF (CRCNS 26-1004-04xx), an HFSP award to
   IRF (26-6302-87), and the Simons Foundation through the Simons
   Collaboration on the Global Brain. The authors acknowledge the Texas
   Advanced Computing Center (TACC) at The University of Texas at Austin
   (URL: http://www.tacc.utexas.edu) for providing HPC resources that have
   contributed to the research results reported within this paper.
CR Bengio Y., 2015, ARXIV150204156
   Burak Y, 2012, P NATL ACAD SCI USA, V109, P17645, DOI 10.1073/pnas.1117386109
   Buzsaki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   Chen Z., 2014, CORR
   Cheung A, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002651
   Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003
   Forster Alexander, 2007, ESANN 2007, P537
   Graves A, 2013, ARXIV13080850
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   Hayman RM, 2008, HIPPOCAMPUS, V18, P1301, DOI 10.1002/hipo.20513
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Marblestone A. H., 2016, ARXIV160603813
   Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592
   Muller Robert U., 1991, SPATIAL FIRING CORRE, P296
   MULLER RU, 1987, J NEUROSCI, V7, P1951
   MULLER RU, 1994, J NEUROSCI, V14, P7235
   OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1
   OKEEFE J, 1979, BEHAV BRAIN SCI, V2, P487, DOI 10.1017/S0140525X00063949
   OKEEFE J, 1978, EXP BRAIN RES, V31, P573
   Rubin A, 2014, J NEUROSCI, V34, P1067, DOI 10.1523/JNEUROSCI.5393-12.2014
   Samsonovich A, 1997, J NEUROSCI, V17, P5900
   Save E, 2000, HIPPOCAMPUS, V10, P64, DOI 10.1002/(SICI)1098-1063(2000)10:1<64::AID-HIPO7>3.3.CO;2-P
   Smith DM, 2006, HIPPOCAMPUS, V16, P716, DOI 10.1002/hipo.20208
   Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986
   Thrun S., 2005, PROBABILISTIC ROBOTI
   WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zhang J., 2017, ARXIV170609520
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404058
DA 2019-06-15
ER

PT S
AU Kar, A
   Hane, C
   Malik, J
AF Kar, Abhishek
   Hane, Christian
   Malik, Jitendra
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning a Multi-View Stereo Machine
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.
C1 [Kar, Abhishek; Hane, Christian; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Kar, A (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM akar@berkeley.edu; chaene@berkeley.edu; malik@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1212798]; ONR [MURI-N00014-10-1-0933]; Swiss National Science
   Foundation [165245]
FX This work was supported in part by NSF Award IIS-1212798 and ONR
   MURI-N00014-10-1-0933. Christian Hane is supported by an "Early
   Postdoc.Mobility" fellowship No. 165245 from the Swiss National Science
   Foundation. The authors would like to thank David Fouhey, Saurabh Gupta
   and Shubham Tulsiani for valuable discussions and Fyusion Inc. for
   providing GPU hours for the work.
CR Bao S. Yingze, 2013, C COMP VIS PATT REC
   Becker S., 1992, NATURE
   Blanz V., 1999, C COMP GRAPH INT TEC
   Chang A.X., 2015, ARXIV151203012
   Cho  K., 2014, C EMP METH NAT LANG
   Choy C. B., 2016, EUR C COMP VIS ECCV
   Collins R. T., 1996, C COMP VIS PATT REC
   Cremers D, 2011, IEEE T PATTERN ANAL, V33, P1161, DOI 10.1109/TPAMI.2010.174
   Curless B., 1996, C COMP GRAPH INT TEC
   Dame A., 2013, C COMP VIS PATT REC
   Eigen D., 2014, NEURAL INFORM PROCES
   Fan H., 2017, C COMP VIS PATT REC
   Flynn J., 2016, C COMP VIS PATT REC
   Fu MY, 2016, NEURAL PLAST, DOI 10.1155/2016/3512098
   Furukawa Y., 2010, T PATTERN ANAL MACHI
   Garg R., 2016, EUR C COMP VIS ECCV
   Gargallo P, 2007, IEEE I CONF COMP VIS, P1364
   Girdhar Rohit, 2016, EUR C COMP VIS ECCV
   Haene C., 2016, T PATTERN ANAL MACHI
   Han X., 2015, C COMP VIS PATT REC
   Hane C., 2014, INT C 3D VIS 3DV
   Hane C., 2014, C COMP VIS PATT REC
   Hane C., 2017, INT C 3D VIS 3DV
   Hane C., 2013, C COMP VIS PATT REC
   Hochreiter S., 1997, NEURAL COMPUTATION
   Jaderberg  M., 2015, NEURAL INFORM PROCES
   Kanade T., 1995, INT C INT ROB SYST I
   Kar A., 2015, C COMP VIS PATT REC
   Kendall A., 2017, INT C COMP VIS ICCV
   Klein J, 2017, INT CONF IMAG VIS
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   Labatut P., 2007, INT C COMP VIS ICCV
   Ladicky L., 2014, C COMP VIS PATT REC
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lempitsky V., 2007, C COMP VIS PATT REC
   Lhuillier M., 2005, T PATTERN ANAL MACHI
   Liu SB, 2014, IEEE T PATTERN ANAL, V36, P2074, DOI 10.1109/TPAMI.2014.2315820
   Marr D., 1976, RETINA NEOCORTEX, P239, DOI DOI 10.1007/978-1-4684-6775-8_9
   Mayer N., 2016, C COMP VIS PATT REC
   Merrell P., 2007, INT C COMP VIS ICCV
   Pollard T., 2007, C COMP VIS PATT REC
   Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4
   Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a
   Rezende D. J., 2016, NEURAL INFORM PROCES
   Riegler G., 2017, INT C 3D VIS 3DV
   Ronneberger O, 2015, MED IMAGE COMPUTING
   Savinov N., 2016, C COMP VIS PATT REC
   Saxena A., 2005, NEURAL INFORM PROCES
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Seitz S. M., 2006, C COMP VIS PATT REC
   Sethi A, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3010048
   Sinha S. N., 2007, INT C COMP VIS ICCV
   Tatarchenko M., 2016, EUR C COMP VIS ECCV
   Tulsiani S., 2016, T PATTERN ANAL MACHI
   Tulsiani S., 2017, C COMP VIS PATT REC
   Ulusoy A. O., 2015, INT C 3D VIS 3DV
   Vogiatzis G., 2005, C COMP VIS PATT REC
   Yang R., 2003, COMPUTER GRAPHICS FO
   Zach C., 2007, INT C COMP VIS ICCV
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhou T., 2017, C COMP VIS PATT REC
NR 61
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400035
DA 2019-06-15
ER

PT S
AU Karakus, C
   Sun, YF
   Diggavi, S
   Yin, WT
AF Karakus, Can
   Sun, Yifan
   Diggavi, Suhas
   Yin, Wotao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Straggler Mitigation in Distributed Optimization Through Data Encoding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Slow running or straggler tasks can significantly reduce computation speed in distributed computation. Recently, coding-theory-inspired approaches have been applied to mitigate the effect of straggling, through embedding redundancy in certain linear computational steps of the optimization algorithm, thus completing the computation without waiting for the stragglers. In this paper, we propose an alternate approach where we embed the redundancy directly in the data itself, and allow the computation to proceed completely oblivious to encoding. We propose several encoding schemes, and demonstrate that popular batch algorithms, such as gradient descent and L-BFGS, applied in a coding-oblivious manner, deterministically achieve sample path linear convergence to an approximate solution of the original problem, using an arbitrarily varying subset of the nodes at each iteration. Moreover, this approximation can be controlled by the amount of redundancy and the number of nodes used in each iteration. We provide experimental results demonstrating the advantage of the approach over uncoded and data replication strategies.
C1 [Karakus, Can; Diggavi, Suhas; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
   [Sun, Yifan] Technicolor Res, Los Altos, CA USA.
RP Karakus, C (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
EM karakus@ucla.edu; Yifan.Sun@technicolor.com; suhasdiggavi@ucla.edu;
   wotaoyin@math.ucla.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [1314937, 1423271]
FX This work was supported in part by NSF grants 1314937 and 1423271.
CR Ananthanarayanan G., 2013, P 10 USENIX C NETW S, V13, P185
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Berahas Albert S, 2016, ADV NEURAL INFORM PR, P1055
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223
   Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6
   Dutta Sanghamitra, 2016, ADV NEURAL INFORM PR, P2092
   Fickus M, 2012, LINEAR ALGEBRA APPL, V436, P1014, DOI 10.1016/j.laa.2011.06.027
   GEMAN S, 1980, ANN PROBAB, V8, P252, DOI 10.1214/aop/1176994775
   Goethals J. M., 1967, CANAD J MATH
   Karakus C., 2017, STRAGGLER MITIGATION
   Karakus C, 2017, IEEE INT SYMP INFO, P2890, DOI 10.1109/ISIT.2017.8007058
   Lee K, 2016, IEEE INT SYMP INFO, P1143, DOI 10.1109/ISIT.2016.7541478
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Mokhtari A, 2015, J MACH LEARN RES, V16, P3151
   Paley R. E. A. C., 1933, J MATH PHYS, V12, P311, DOI DOI 10.1002/SAPM1933121311
   Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722
   Riedl J, 1998, MOVIELENS DATASET
   SILVERSTEIN JW, 1985, ANN PROBAB, V13, P1364, DOI 10.1214/aop/1176992819
   Szollosi F, 2013, LINEAR ALGEBRA APPL, V438, P1962, DOI 10.1016/j.laa.2011.05.034
   Tandon Rashish, 2016, ML SYST WORKSH MLSYS
   WELCH LR, 1974, IEEE T INFORM THEORY, V20, P397, DOI 10.1109/TIT.1974.1055219
   Yadwadkar NJ, 2016, J MACH LEARN RES, V17
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405050
DA 2019-06-15
ER

PT S
AU Karami, M
   White, M
   Schuurmans, D
   Szepesvari, C
AF Karami, Mahdi
   White, Martha
   Schuurmans, Dale
   Szepesvari, Csaba
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-view Matrix Factorization for Linear Dynamical System Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider maximum likelihood estimation of linear dynamical systems with generalized-linear observation models. Maximum likelihood is typically considered to be hard in this setting since latent states and transition parameters must be inferred jointly. Given that expectation-maximization does not scale and is prone to local minima, moment-matching approaches from the subspace identification literature have become standard, despite known statistical efficiency issues. In this paper, we instead reconsider likelihood maximization and develop an optimization based strategy for recovering the latent states and transition parameters. Key to the approach is a two-view reformulation of maximum likelihood estimation for linear dynamical systems that enables the use of global optimization algorithms for matrix factorization. We show that the proposed estimation strategy outperforms widely-used identification algorithms such as subspace identification methods, both in terms of accuracy and runtime.
C1 [Karami, Mahdi; White, Martha; Schuurmans, Dale; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
RP Karami, M (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM karami1@ualberta.ca; whitem@ualberta.ca; daes@ualberta.ca;
   szepesva@ualberta.ca
RI Jeong, Yongwook/N-7413-2016
FU Alberta Machine Intelligence Institute; NSERC
FX This work was supported in part by the Alberta Machine Intelligence
   Institute and NSERC. During this work, M. White was with the Department
   of Computer Science, Indiana University.
CR Andersson S., 2009, ANN STAT
   ASTROM KJ, 1980, AUTOMATICA, V16, P551, DOI 10.1016/0005-1098(80)90078-3
   Bach F., 2008, ARXIV08121869V1
   Banerjee  A., 2005, J MACHINE LEARNING R
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boots B., 2012, INT C MACH LEARN
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buesing L., 2012, NETWORK COMPUTATION
   Buesing L., 2012, ADV NEURAL INFORM PR
   Cramer H., 1946, MATH METHODS STAT
   Foster D., 2012, ARXIV12036130V1
   Gelfand AE, 2010, CH CRC HANDB MOD STA, P1, DOI 10.1201/9781420072884
   Ghahramani Z., 1996, TECHNICAL REPORT
   Haeffele B., 2014, INT C MACH LEARN
   Hsu D., 2012, J COMPUTER SYSTEM SC
   Katayama T., 2006, SUBSPACE METHODS SYS
   Ljung L, 1999, SYSTEM IDENTIFICATIO
   Macke JH, 2015, ADV STATE SPACE METH
   Moonen M., 1993, IEEE T AUTOMATIC CON
   Parikh N., 2013, FDN TRENDS OPTIMIZAT
   Roweis S., 1999, NEURAL COMPUTATION
   Siddiqi SM, 2007, ADV NEURAL INFORM PR
   Song L., 2010, INT C MACH LEARN
   Van Overschee P., 1994, AUTOMATICA
   Viberg M., 1995, AUTOMATICA
   White M., 2015, AAAI C ART INT
   White M., 2012, ADV NEURAL INFORM PR
   Yu  H.-F., 2015, ARXIV150908333
   Yu Y., 2014, ARXIV14104828
   Zhang X., 2012, ADV NEURAL INFORM PR
   Zhao H., 2014, ARXIV14064631
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407018
DA 2019-06-15
ER

PT S
AU Karimi, MR
   Lucic, M
   Hassani, N
   Krause, A
AF Karimi, Mohammad Reza
   Lucic, Mario
   Hassani, Named
   Krause, Andreas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Stochastic Submodular Maximization: The Case of Coverage Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FUNCTION SUBJECT; APPROXIMATIONS; ALGORITHM
AB Stochastic optimization of continuous objectives is at the heart of modern machine learning. However, many important problems are of discrete nature and often involve submodular objectives. We seek to unleash the power of stochastic continuous optimization, namely stochastic gradient descent and its variants, to such discrete problems. We first introduce the problem of stochastic submodular optimization, where one needs to optimize a submodular objective which is given as an expectation. Our model captures situations where the discrete objective arises as an empirical risk (e.g., in the case of exemplar-based clustering), or is given as an explicit stochastic model (e.g., in the case of influence maximization in social networks). By exploiting that common extensions act linearly on the class of submodular functions, we employ projected stochastic gradient ascent and its variants in the continuous domain, and perform rounding to obtain discrete solutions. We focus on the rich and widely used family of weighted coverage functions. We show that our approach yields solutions that are guaranteed to match the optimal approximation guarantees, while reducing the computational cost by several orders of magnitude, as we demonstrate empirically.
C1 [Karimi, Mohammad Reza; Lucic, Mario; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Hassani, Named] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.
RP Karimi, MR (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM mkarimi@ethz.ch; lucic@inf.ethz.ch; hassani@seas.upenn.edu;
   krausea@ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU ERC [StG 307036]
FX The research was partially supported by ERC StG 307036. We would like to
   thank Yaron Singer for helpful comments and suggestions.
CR Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Bach Francis R., 2010, ABS10104207 CORR
   Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497, DOI DOI 10.1137/1.9781611973402.110
   BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5
   Calinescu G, 2007, LECT NOTES COMPUT SC, V4513, P182
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Duchi  J., 2011, J MACHINE LEARNING R
   Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73
   Glance N., 2005, P 11 ACM SIGKDD INT, P419, DOI DOI 10.1145/1081870.1081919
   Gomez Ryan, 2010, P 27 INT C MACH LEAR
   Hassidim  A., 2016, ABS160103095 CORR
   Horel Thibaut, 2016, NIPS
   Iyer R. K., 2013, ADV NEURAL INFORM PR, P2436
   Iyer Rishabh K., 2015, ABS150607329 ARXIV C
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kingma D. P., 2015, ICLR
   Krause A., 2012, TRACTABILITY PRACT A, V3, P8
   Krause A, 2005, P 21 C UNC ART INT U, P324
   Krause Andreas, 2005, C UNC ART INT UAI JU
   Kulik A, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P545
   Kumar K. S. Sesh, 2016, HAL01161759V3
   Lin H., 2011, P 49 ANN M ASS COMP, V1, P510
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Mirzasoleiman Baharan, 2015, LAZIER LAZY GREEDY
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748
   Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Singla Adish, 2016, P C ART INT AAAI FEB
   Streeter  Matthew, 2008, NIPS
   Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318
   Vondrak J, 2008, ACM S THEORY COMPUT, P67
   Vondrak Jan, 2007, SUBMODULARITY COMBIN
   Wei Kai, 2014, INT C MACH LEARN ICM
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406088
DA 2019-06-15
ER

PT S
AU Karkus, P
   Hsu, D
   Lee, WS
AF Karkus, Peter
   Hsu, David
   Lee, Wee Sun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI QMDP-Net: Deep Learning for Planning under Partial Observability
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VALUE-ITERATION
AB This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and "transfer" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.
C1 [Karkus, Peter; Hsu, David] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore, Singapore.
   [Karkus, Peter; Hsu, David; Lee, Wee Sun] Natl Univ Singapore, Sch Comp, Singapore, Singapore.
RP Karkus, P (reprint author), Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore, Singapore.; Karkus, P (reprint author), Natl Univ Singapore, Sch Comp, Singapore, Singapore.
EM karkus@comp.nus.edu.sg; dyhsu@comp.nus.edu.sg; leews@comp.nus.edu.sg
RI Jeong, Yongwook/N-7413-2016
FU Singapore Ministry of Education AcRF grant [MOE2016-T2-2-068]; National
   University of Singapore AcRF grant [R-252-000-587-112]
FX We thank Leslie Kaelbling and Tomas Lozano-Perez for insightful
   discussions that helped to improve our understanding of the problem. The
   work is supported in part by Singapore Ministry of Education AcRF grant
   MOE2016-T2-2-068 and National University of Singapore AcRF grant
   R-252-000-587-112.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Bagnell J. A., 2003, ADV NEURAL INFORM PR, P831
   Bai HY, 2010, SPRINGER TRAC ADV RO, V68, P175
   Bakker B, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P430
   Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Gupta Saurabh, 2017, ARXIV170203920
   Haarnoja T., 2016, ADV NEURAL INFORM PR, P4376
   Hausknecht Matthew, 2015, DEEP RECURRENT Q LEA
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Howard A, 2003, ROBOTICS DATA SET RE
   Hsiao K, 2007, IEEE INT CONF ROBOT, P4685, DOI 10.1109/ROBOT.2007.364201
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jonschkowski R., 2016, WORKSH DEEP LEARN AC
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kurniawati H., 2008, ROBOTICS SCI SYSTEMS, V2008
   Littman M. L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P362
   Littman ML, 2002, ADV NEUR IN, V14, P1555
   Mirowski P., 2016, ARXIV161103673
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   Okada M., 2017, ARXIV170609597
   PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441
   Pineau J., 2003, ADV NEURAL INFORM PR
   Shani G, 2005, LECT NOTES ARTIF INT, V3720, P353
   Shani G, 2013, AUTON AGENT MULTI-AG, V27, P1, DOI 10.1007/s10458-012-9200-2
   Shankar T, 2016, INT C PATT RECOG, P2592, DOI 10.1109/ICPR.2016.7900026
   Shi X., 2015, ADV NEURAL INFORM PR, V9199, P802
   Silver D., 2016, PREDICTRON END TO EN
   Silver D, 2010, ADV NEURAL INFORM PR, V23, P2164
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spaan MTJ, 2005, J ARTIF INTELL RES, V24, P195, DOI 10.1613/jair.1659
   Stachniss C., ROBOTICS 2D LASER DA
   Tamar A, 2016, ADV NEURAL INFORM PR, P2146
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26
   Ye N, 2017, J ARTIF INTELL RES, V58, P231, DOI 10.1613/jair.5328
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404074
DA 2019-06-15
ER

PT S
AU Kaufmann, E
   Koolen, WM
AF Kaufmann, Emilie
   Koolen, Wouter M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Monte-Carlo Tree Search by Best Arm Identification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MULTIARMED BANDIT
AB Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem, where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth, that operate by summarizing all deeper levels of the tree into confidence intervals at depth one, and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match previous special-purpose methods for depth-two trees.
C1 [Kaufmann, Emilie] CNRS, Lille, France.
   [Kaufmann, Emilie] Univ Lille, UMR CRIStAL 9189, Inria SequeL, Lille, France.
   [Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.
RP Kaufmann, E (reprint author), CNRS, Lille, France.; Kaufmann, E (reprint author), Univ Lille, UMR CRIStAL 9189, Inria SequeL, Lille, France.
EM emilie.kaufmann@univ-lille1.fr; wmkoolen@cwi.nl
RI Jeong, Yongwook/N-7413-2016
FU French Agence Nationale de la Recherche (ANR) [ANR-16-CE40-0002];
   Netherlands Organization for Scientific Research (NWO) under Veni grant
   [639.021.439]
FX Emilie Kaufmann acknowledges the support of the French Agence Nationale
   de la Recherche (ANR), under grant ANR-16-CE40-0002 (project BADASS).
   Wouter Koolen acknowledges support from the Netherlands Organization for
   Scientific Research (NWO) under Veni grant 639.021.439.
CR Audibert J-Y., 2010, P 23 C LEARN THEOR
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Borsoniu L., 2014, ADPRL14
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Cazenave T, 2015, IEEE T COMP INTEL AI, V7, P102, DOI 10.1109/TCIAIG.2014.2317737
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Gabillon V., 2012, ADV NEURAL INFORM PR
   Garivier A., 2016, P 29 C LEARN THEOR C
   Garivier Aurelien, 2016, P 29 C LEARN THEOR
   Huang Ruitong, 2017, 28 INT C ALG LEARN T
   Jamieson K., 2014, P 27 C LEARN THEOR
   Kalyanakrishnan S., 2012, INT C MACH LEARN ICM
   Karnin Z., 2013, INT C MACH LEARN ICM
   Kaufmann E., 2013, P 26 C LEARN THEOR
   Kaufmann E, 2016, J MACH LEARN RES, V17
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Pepels T., 2014, COMP GAM WORKSH ECAI
   Plaat A, 1996, ARTIF INTELL, V87, P255, DOI 10.1016/0004-3702(95)00126-3
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404094
DA 2019-06-15
ER

PT S
AU Kazemitabar, SJ
   Amini, AA
   Bloniarz, A
   Talwalkar, A
AF Kazemitabar, S. Jalil
   Amini, Arash A.
   Bloniarz, Adam
   Talwalkar, Ameet
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variable Importance using Decision Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SPARSITY RECOVERY
AB Decision trees and random forests are well established models that not only offer good predictive performance, but also provide rich feature importance information. While practitioners often employ variable importance methods that rely on this impurity-based information, these methods remain poorly characterized from a theoretical perspective. We provide novel insights into the performance of these methods by deriving finite sample performance guarantees in a high-dimensional setting under various modeling assumptions. We further demonstrate the effectiveness of these impurity-based methods via an extensive set of simulations.
C1 [Kazemitabar, S. Jalil; Amini, Arash A.] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
   [Bloniarz, Adam] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Talwalkar, Ameet] CMU, Mt Pleasant, MI USA.
   [Bloniarz, Adam] Google, Mountain View, CA USA.
RP Kazemitabar, SJ (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
EM sjalilk@ucla.edu; aaamini@ucla.edu; adam@stat.berkeley.edu;
   talwalkar@cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Fan J., 2008, J ROYAL STAT SOC B, V70
   Fan JQ, 2008, J ROY STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Ishwaran H, 2007, ELECTRON J STAT, V1, P519, DOI 10.1214/07-EJS039
   Lafferty J, 2008, ANN STAT, V36, P28, DOI 10.1214/009053607000000811
   Louppe G., 2013, ADV NEURAL INFORM PR, V26
   Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x
   Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P5728, DOI 10.1109/TIT.2009.2032816
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400041
DA 2019-06-15
ER

PT S
AU Kazerouni, A
   Ghavamzadeh, M
   Abbasi-Yadkori, Y
   Van Roy, B
AF Kazerouni, Abbas
   Ghavamzadeh, Mohammad
   Abbasi-Yadkori, Yasin
   Van Roy, Benjamin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Conservative Contextual Linear Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.
C1 [Kazerouni, Abbas; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA.
   [Ghavamzadeh, Mohammad] DeepMind, London, England.
   [Abbasi-Yadkori, Yasin] Adobe Res, New York, NY USA.
RP Kazerouni, A (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM abbask@stanford.edu; ghavamza@google.com; abbasiya@adobe.com;
   byr@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Dani V., 2008, COLT, P355
   Ghavamzadeh Mohammad, 2016, ADV NEURAL INFORM PR, P2298
   Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Swaminathan A., 2015, P 32 INT C MACH LEAR
   Swaminathan A, 2015, J MACH LEARN RES, V16, P1731
   Thomas P., 2015, P 29 C ART INT
   Thomas Philip S., 2015, P 32 INT C MACH LEAR, P2380
   Wu Y., 2016, P 33 INT C MACH LEAR, P1254
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403094
DA 2019-06-15
ER

PT S
AU Ke, GL
   Meng, Q
   Finley, T
   Wang, TF
   Chen, W
   Ma, WD
   Ye, QW
   Liu, TY
AF Ke, Guolin
   Meng, Qi
   Finley, Thomas
   Wang, Taifeng
   Chen, Wei
   Ma, Weidong
   Ye, Qiwei
   Liu, Tie-Yan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI LightGBM: A Highly Efficient Gradient Boosting Decision Tree
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LOGISTIC-REGRESSION
AB Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.
C1 [Ke, Guolin; Wang, Taifeng; Chen, Wei; Ma, Weidong; Ye, Qiwei; Liu, Tie-Yan] Microsoft Res, Redmond, WA 98052 USA.
   [Meng, Qi] Peking Univ, Beijing, Peoples R China.
   [Finley, Thomas] Microsoft Redmond, Redmond, WA USA.
RP Ke, GL (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM guolin.ke@microsoft.com; qimeng13@pku.edu.cn; tfinely@microsoft.com;
   taifengw@microsoft.com; wche@microsoft.com; weima@microsoft.com;
   qiwye@microsoft.com; tie-yan.liu@microsoft.com
CR Alsabti K., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P2
   Appel R., 2013, JMLR WORKSHOP C P, P594
   Burges Christopher JC, 2010, LEARNING, V11, P81
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   Dubout Charles, 2011, ADV NEURAL INFORM PR, P1332
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2
   Jensen T. R., 2011, GRAPH COLORING PROBL, V39
   Jimenez LO, 1999, IEEE T GEOSCI REMOTE, V37, P2653, DOI 10.1109/36.803413
   Jin RM, 2003, SIAM PROC S, P119
   Jolliffe IT, 2002, PRINCIPAL COMPONENT
   Li P., 2007, NEURAL INFORM PROCES, V7, P845
   Li Ping, 2012, ARXIV12033491
   Mehta M., 1996, Advances in Database Technology - EDBT '96. 5th International Conference on Extending Database Technology. Proceedings, P18
   Meng Q., 2016, ADV NEURAL INFORM PR, P1271
   Mitchell R, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.127
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Qin T., 2013, CORR
   Richardson M., 2007, P 16 INT C WORLD WID, P521, DOI DOI 10.1145/1242572.1242643
   Ridgeway G., 2007, GENERALIZED BOOSTED, V1, P1, DOI DOI 10.1111/J.1467-9752.1996.TB00390.X
   Shafer J, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P544
   Shi H, 2007, THESIS
   Tyree Stephen, 2011, P 20 INT C WORLD WID, P387
   Wu Kuan-Wei, 2012, KDDCUP
   Yu H.-F., 2010, KDD CUP
   Zhang Huan, 2017, ARXIV170608359
   Zhou Z. - H., 2012, ENSEMBLE METHODS FDN
NR 29
TC 0
Z9 0
U1 15
U2 15
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403021
DA 2019-06-15
ER

PT S
AU Kendall, A
   Gal, Y
AF Kendall, Alex
   Gal, Yarin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI What Uncertainties Do We Need in Bayesian Deep Learning for Computer
   Vision?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.
C1 [Kendall, Alex; Gal, Yarin] Univ Cambridge, Cambridge, England.
RP Kendall, A (reprint author), Univ Cambridge, Cambridge, England.
EM agk34@cam.ac.uk; yg279@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, P 12 USENIX S OP SYS
   [Anonymous], 2017, 16007 NHTSA PE US DE
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL
   BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225
   Blundell C., 2015, ICML
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chen L.-C., 2014, ARXIV14127062
   David J. C. MacKay, 1992, NEURAL COMPUT, V4, P448
   Denker John, 1991, ADV NEURAL INFORM PR, V3
   Eigen D., 2014, ADV NEURAL INFORM PR, P2366
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Gal Y., 2016, ICLR WORKSH TRACK
   Gal Y., 2016, THESIS
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Guynn J., 2015, US TODAY
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   He  X., 2004, COMP VIS PATT REC 20, V2, pII
   Hernandez-Lobato J. M., 2016, P INT C MACH LEARN, V48, P1511
   Huang G, 2016, ARXIV160806993
   Jegou S., 2016, ARXIV161109326
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56
   Kendall A., 2015, ARXIV151102680
   Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020
   Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Le Quoc V, 2005, P 22 INT C MACH LEAR, P489
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Neal R M, 1995, THESIS
   NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Shelhamer E., 2016, IEEE T PATTERN ANAL
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Yu F., 2015, ARXIV151107122
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405064
DA 2019-06-15
ER

PT S
AU Khetan, A
   Oh, S
AF Khetan, Ashish
   Oh, Sewoong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Matrix Norm Estimation from a Few Entries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering various spectral properties of the underlying matrix from a sampling of its entries. We propose a framework of first estimating the Schatten k-norms of a matrix for several values of k, and using these as surrogates for estimating spectral properties of interest, such as the spectrum itself or the rank. This paper focuses on the technical challenges in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performances. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.
C1 [Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61801 USA.
RP Khetan, A (reprint author), Univ Illinois, Dept ISE, Champaign, IL 61801 USA.
EM khetan2@illinois.edu; swoh@illinois.edu
FU NSF [CNS-1527754, CCF-1553452, CCF-1705007]; GOOGLE Faculty Research
   Award
FX This work was partially supported by NSF grants CNS-1527754,
   CCF-1553452, CCF-1705007 and GOOGLE Faculty Research Award.
CR Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611
   Alon N, 1997, ALGORITHMICA, V17, P209, DOI 10.1007/BF02523189
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   ELBASSIONI K., 2015, J GRAPH ALGOR APPL, V19, P273
   Feige U, 2005, RANDOM STRUCT ALGOR, V27, P251, DOI 10.1002/rsa.20089
   Friedman J., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P587, DOI 10.1145/73007.73063
   Han I, 2015, ICML, P908
   Han I., 2016, ARXIV160600942
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Kloks T, 2000, INFORM PROCESS LETT, V74, P115, DOI 10.1016/S0020-0190(00)00047-8
   Kong W., 2016, ARXIV160200061
   Le C. M., 2015, ARXIV150203049
   Li Y., 2016, ARXIV160408679
   Li Y, 2014, P 25 ANN ACM SIAM S, P1562
   Mason J. C, 2002, CHEBYSHEV POLYNOMIAL
   Polizzi E., 2016, NUMERICAL LINEAR ALG
   Schudy W., 2011, ARXIV11095193
   Uehara Ryuhei, 1999, NUMBER CONNECT UNPUB
   Zhang Y., 2015, ARXIV150201403
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406048
DA 2019-06-15
ER

PT S
AU Killian, T
   Daulton, S
   Konidaris, G
   Doshi-Velez, F
AF Killian, Taylor
   Daulton, Samuel
   Konidaris, George
   Doshi-Velez, Finale
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust and Efficient Transfer Learning with Hidden Parameter Markov
   Decision Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.
C1 [Killian, Taylor; Daulton, Samuel; Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA.
   [Daulton, Samuel] Facebook, Menlo Pk, CA USA.
   [Konidaris, George] Brown Univ, Providence, RI 02912 USA.
RP Killian, T (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM taylorkillian@g.harvard.edu; sdaulton@g.harvard.edu; gdk@cs.brown.edu;
   finale@seas.harvard.edu
FU MIT Lincoln Laboratory Lincoln Scholars Program; NIH [R01MH109177]
FX We thank Mike Hughes, Andrew Miller, Jessica Forde, and Andrew Ross for
   their helpful conversations. TWK was supported by the MIT Lincoln
   Laboratory Lincoln Scholars Program. GDK is supported in part by the NIH
   R01MH109177. The content of this work is solely the responsibility of
   the authors and does not necessarily represent the official views of the
   NIH.
CR Adams BM, 2004, MATH BIOSCI ENG, V1, P223
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Bai HY, 2013, IEEE INT CONF ROBOT, P2853, DOI 10.1109/ICRA.2013.6630972
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Bonilla E., 2008, NIPS, V20, P153
   Broekens  Joost, 2017, ARXIV170500470
   Brunskill E., 2013, C UNC ART INT
   Caruana R, 1998, LEARNING TO LEARN, P95
   Chen M, 2016, IEEE INT CONF ROBOT, P5427, DOI 10.1109/ICRA.2016.7487754
   Delhaisse B, 2017, INT JOINT C NEUR NET
   Depeweg S, 2017, ARXIV170608495
   Depeweg S, 2017, INT C LEARN REPR
   Dietrich CR, 1997, SIAM J SCI COMPUT, V18, P1088, DOI 10.1137/S1064827592240555
   Doshi-Velez Finale, 2016, IJCAI (U S), V2016, P1432
   Ernst D, 2006, P 45 IEEE C DEC CONT
   Fern A., 2010, ADV NEURAL INFORM PR, P577
   Gal Y., 2016, P 33 INT C MACH LEAR
   Gal  Yarin, 2016, DAT EFF MACH LEARN W
   Genton MG, 2015, STAT SCI, V30, P147, DOI 10.1214/14-STS487
   Gupta A, 2017, INT C LEARN REPR
   Hernandez-Lobato JM, 2016, P 33 INT C MACH LEAR
   Jaques N., 2015, P NIPS WORKSH MULT M
   Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X
   Kendall A, 2017, ARXIV170304977
   Kingma D. P., 2015, INT C LEARN REPR
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Marivate VN, 2014, WORKSH 28 AAAI C ART
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104
   Moore BL, 2014, J MACH LEARN RES, V15, P655
   Neal RM, 1992, TECHNICAL REPORT
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen C., 2011, P INT C MACH LEARN
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rasmussen CE, 2003, ADV NEURAL INFORM PR, V15
   Rosman B, 2016, MACH LEARN, V104, P99, DOI 10.1007/s10994-016-5547-y
   Schaul Tom, 2016, INT C LEARN REPR
   Sehulam P, 2016, J MACH LEARN RES, V17
   Shortreed SM, 2011, MACH LEARN, V84, P109, DOI 10.1007/s10994-010-5229-0
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Tenenbaum M, 2010, WORKSH NIPS
   Van Hasselt H., 2016, AAAI, P2094
   Williams JD, 2006, AAAI WORKSH STAT EMP, P37
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406032
DA 2019-06-15
ER

PT S
AU Kim, H
   Gao, WH
   Kannan, S
   Oh, S
   Viswanath, P
AF Kim, Hyeji
   Gao, Weihao
   Kannan, Sreeram
   Oh, Sewoong
   Viswanath, Pramod
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Discovering Potential Correlations via Hypercontractivity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INEQUALITIES; INFORMATION
AB Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.
C1 [Kim, Hyeji; Gao, Weihao; Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Champaign, IL 61820 USA.
   [Kim, Hyeji; Gao, Weihao; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA.
   [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Champaign, IL 61820 USA.
   [Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
RP Kim, H (reprint author), Univ Illinois, Coordinated Sci Lab, Champaign, IL 61820 USA.; Kim, H (reprint author), Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA.
EM hyejikim@illinois.edu; wgao9@illinois.edu; ksreeram@uw.edu;
   swoh@illinois.edu; pramodv@illinois.edu
FU NSF [CNS-1527754, CNS-1718270, CCF-1553452, CCF-1617745, CCF-1651236,
   CCF-1705007]; GOOGLE Faculty Research Award
FX This work was partially supported by NSF grants CNS-1527754,
   CNS-1718270, CCF-1553452, CCF-1617745, CCF-1651236, CCF-1705007, and
   GOOGLE Faculty Research Award.
CR Achille A., 2016, 161101353 ARXIV
   AHLSWEDE R, 1976, ANN PROBAB, V4, P925, DOI 10.1214/aop/1176995937
   Alemi Alexander A, 2017, ICLR
   Anantharam V., 2013, CORR
   Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   BECKNER W, 1975, ANN MATH, V102, P159, DOI 10.2307/1970980
   Bekkerman R., 2003, Journal of Machine Learning Research, V3, P1183, DOI 10.1162/153244303322753625
   BELL CB, 1962, ANN MATH STAT, V33, P587, DOI 10.1214/aoms/1177704583
   BONAMI A, 1970, ANN I FOURIER, V20, P335, DOI 10.5802/aif.357
   Chechik G, 2005, J MACH LEARN RES, V6, P165
   Davies E B, 1992, IDEAS METHODS QUANTU, P370
   Dhillon I. S., 2003, J MACHINE LEARNING R, V3
   Gao Weihao, 2016, P 33 INT C MACH LEAR, P2780
   Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604
   Gorfine M., 2012, COMMENT DETECT UNPUB
   GROSS L, 1975, DUKE MATH J, V42, P383, DOI 10.1215/S0012-7094-75-04237-4
   Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520
   Kahn J., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P68, DOI 10.1109/SFCS.1988.21923
   Kim H., DISCOVERING PO UNPUB
   Krishnaswamy S., 2014, SCIENCE
   Michaeli T., 2016, P ICML 16 33 INT C I, P1967
   Mossel E, 2013, GEOM FUNCT ANAL, V23, P1062, DOI 10.1007/s00039-013-0229-4
   Nair C., 2014, INF THEOR APPL WORKS
   Nair C., 2016, COMMUNICATION
   Nelson E., 1973, J FUNC ANALYT, V12, P97
   Ngiam J, 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.1145/2647868
   ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO
   Pearson K., 1895, Proceedings of the Royal Society London, Vlviii, P240
   Renyi  A., 1959, ACTA MATH ACAD SCI H, V10, P441, DOI [10.1007/BF02024507, DOI 10.1007/BF02024507]
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Simon N., 2014, ARXIV E PRINTS
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505
   Tishby N., 1999, P 37 ANN ALL C COMM, P368
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404063
DA 2019-06-15
ER

PT S
AU Kiryo, R
   Niu, G
   du Plessis, MC
   Sugiyama, M
AF Kiryo, Ryuichi
   Niu, Gang
   du Plessis, Marthinus C.
   Sugiyama, Masashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Positive-Unlabeled Learning with Non-Negative Risk Estimator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning, in which the state of the art is unbiased PU learning. However, if its model is very flexible, empirical risks on training data will go negative, and we will suffer from serious overfitting. In this paper, we propose a non-negative risk estimator for PU learning: when getting minimized, it is more robust against overfitting, and thus we are able to use very flexible models (such as deep neural networks) given limited P data. Moreover, we analyze the bias, consistency, and mean-squared-error reduction of the proposed risk estimator, and bound the estimation error of the resulting empirical risk minimizer. Experiments demonstrate that our risk estimator fixes the overfitting problem of its unbiased counterparts.
C1 [Kiryo, Ryuichi; Niu, Gang; Sugiyama, Masashi] Univ Tokyo, 7-3-1 Hongo, Tokyo 1130033, Japan.
   [Kiryo, Ryuichi; Niu, Gang; Sugiyama, Masashi] RIKEN, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.
RP Kiryo, R (reprint author), Univ Tokyo, 7-3-1 Hongo, Tokyo 1130033, Japan.; Kiryo, R (reprint author), RIKEN, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.
EM kiryo@ms.k.u-tokyo.ac.jp; gang@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU JST CREST [JPMJCR1403]; Microsoft Research Asia
FX GN and MS were supported by JST CREST JPMJCR1403 and GN was also
   partially supported by Microsoft Research Asia.
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Chung K.-L., 1968, COURSE PROBABILITY T
   De Comite F., 1999, ALT
   Denis F., 1998, ALT
   du Plessis  M.C., 2014, NIPS
   du Plessis M. C., 2015, ICML
   du Plessis MC, 2017, MACH LEARN, V106, P463, DOI 10.1007/s10994-016-5604-6
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elkan  C., 2008, KDD
   Glorot X., 2010, AISTATS
   Hsieh Cho-Jui, 2015, ICML
   Ioffe S., 2015, ICML
   Jain  S., 2016, NIPS
   James W., 1961, P 4 BERK S MATH STAT
   Kingma D. P., 2015, ICLR
   Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lang K., 1995, ICML
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledoux M., 1991, PROBABILITY BANACH S
   Lee W. S., 2003, ICML
   Letouzey F., 2000, ALT
   Li X., 2003, IJCAI
   Li X., 2009, SDM
   Liu B, 2003, ICDM
   Liu B., 2002, ICML
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Menon A., 2015, ICML
   Mohri M., 2012, FDN MACHINE LEARNING
   Nair V., 2010, ICML
   Natarajan N., 2013, NIPS
   Nguyen M. N., 2011, IJCAI
   Niu  G., 2016, NIPS
   Patrini G., 2016, ICML
   Pennington Jeffrey, 2014, EMNLP
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Ramaswamy H. G., 2016, ICML
   Sakai  T., 2017, ICML
   Sansone E., 2016, ARXIV160806807
   Scott  C., 2009, AISTATS
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Springenberg J. T., 2015, ICLR
   Stein C., 1956, P 3 BERK S MATH STAT
   Tokui S., 2015, MACH LEARN SYST WORK
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x
   Yuan GX, 2012, J MACH LEARN RES, V13, P1999
   Yuille A. L., 2001, NIPS
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401069
DA 2019-06-15
ER

PT S
AU Klambauer, G
   Unterthiner, T
   Mayr, A
   Hochreiter, S
AF Klambauer, Guenter
   Unterthiner, Thomas
   Mayr, Andreas
   Hochreiter, Sepp
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Self-Normalizing Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION
AB Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance-even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep.
C1 [Klambauer, Guenter] Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.
   Johannes Kepler Univ Linz, Inst Bioinformat, A-4040 Linz, Austria.
RP Klambauer, G (reprint author), Johannes Kepler Univ Linz, LIT AI Lab, A-4040 Linz, Austria.
EM klambauer@bioinf.jku.at; unterthiner@bioinf.jku.at; mayr@bioinf.jku.at;
   hochreit@bioinf.jku.at
RI Jeong, Yongwook/N-7413-2016; Unterthiner, Thomas/K-7231-2018
OI Unterthiner, Thomas/0000-0001-5361-3087
CR Ba J. L., 2016, ARXIV160706450
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   BRADLEY RC, 1981, J MULTIVARIATE ANAL, V11, P1, DOI 10.1016/0047-259X(81)90128-7
   Ciresan D. C., 2015, NEUR NETW IJCNN 2015, DOI [DOI 10.1109/IJCNN.2015.7280516, 10.1109/ijcnn.2015.7280516]
   Clevert D.-A., 2015, 5 INT C LEARN REPR
   Desjardins G., 2015, ADV NEURAL INFORM PR, V28, P2071
   Dugan P., 2016, ARXIV160500982
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   Graves A., 2009, ADV NEURAL INFORM PR, P545
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huval Brody, 2015, ARXIV150401716
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Korolev V, 2012, SCAND ACTUAR J, P81, DOI 10.1080/03461238.2010.485370
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lyon RJ, 2016, MON NOT R ASTRON SOC, V459, P1104, DOI 10.1093/mnras/stw656
   Mayr A, 2016, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00080
   Rao Kanishka, 2015, ARXIV150706947
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Srivastava R.K., 2015, ADV NEURAL INFORM PR, P2377
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Wainberg M, 2016, J MACH LEARN RES, V17
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401002
DA 2019-06-15
ER

PT S
AU Kleindessner, M
   Von Luxburg, U
AF Kleindessner, Matthaeus
   Von Luxburg, Ulrike
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Kernel functions based on triplet comparisons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Given only information in the form of similarity triplets "Object A is more similar to object B than to object C" about a data set, we propose two ways of defining a kernel function on the data set. While previous approaches construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining kernel functions that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply any kernel method to the data set.
C1 [Kleindessner, Matthaeus] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Von Luxburg, Ulrike] Univ Tubingen, Dept Comp Sci, Tubingen, Germany.
   [Von Luxburg, Ulrike] Max Planck Inst Intelligent Syst, Tubingen, Germany.
   [Kleindessner, Matthaeus] Univ Tubingen, Tubingen, Germany.
RP Kleindessner, M (reprint author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
EM mk1572@cs.rutgers.edu; luxburg@informatik.uni-tuebingen.de
FU Institutional Strategy of the University of Tubingen (DFG) [ZUK 63]
FX This work has been supported by the Institutional Strategy of the
   University of Tubingen (DFG, ZUK 63).
CR Agarwal S., 2007, INT C ART INT STAT A
   Amid E., 2015, INT C MACH LEARN ICM
   Amid E., 2016, ARXIV161109957V1CSAI
   De Silva  Vin, 2004, TECHNICAL REPORT
   Dhillon I., 2001, INT C KNOWL DISC DAT
   Greene D., 2006, INT C MACH LEARN ICM
   Gustavson F. G., 1978, ACM Transactions on Mathematical Software, V4, P250, DOI 10.1145/355791.355796
   Haghiri S., 2017, ARTIFICIAL INTELLIGE
   Heikinheimo H., 2013, C HUM COMP CROWDS HC
   Heim E., 2015, SIAM INT C DAT MIN S
   HIGHAM NJ, 1990, ACM T MATH SOFTWARE, V16, P352, DOI 10.1145/98267.98290
   Jain L., 2016, NEURAL INFORM PROCES
   Jamieson K., 2011, ALL C COMM CONTR COM
   Jiao Y., 2015, INT C MACH LEARN ICM
   Kaplan H., 2006, S COMP GEOM SOCG
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.1093/biomet/30.1-2.81
   Kleindessner M., 2014, C LEARN THEOR COLT
   Kleindessner M, 2017, J MACH LEARN RES, V18
   Manning C. D., 2008, INTRO INFORM RETRIEV
   McFee B, 2011, J MACH LEARN RES, V12, P491
   Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327
   Scholkopf B., 2002, EUR C MACH LEARN ECM
   Schultz M., 2003, NEURAL INFORM PROCES
   Stewart N, 2005, PSYCHOL REV, V112, P881, DOI 10.1037/0033-295X.112.4.881
   Tamuz O., 2011, INT C MACH LEARN ICM
   Terada Y., 2014, INT C MACH LEARN ICM
   Ukkonen A., 2017, INT C DAT MIN SER IC
   Ukkonen A., 2015, C HUM COMP CROWDS HC
   van der Maaten L, 2012, IEEE INT WORKS MACH
   Wilber M., 2014, C HUM COMP CROWDS HC
   Wilber M., 2015, INT C COMP VIS ICCV
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406084
DA 2019-06-15
ER

PT S
AU Kocaoglu, M
   Shanmugam, K
   Bareinboim, E
AF Kocaoglu, Murat
   Shanmugam, Karthikeyan
   Bareinboim, Elias
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Experimental Design for Learning Causal Graphs with Latent Variables
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INTERVENTIONS; DISCOVERY; INFERENCE; SELECTION
AB We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(dlog(2) n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d(2)log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n(2)) interventions, our combined algorithm can learn the causal graph with latents using O(dlog(2) n + d(2) log (n)) interventions.
C1 [Kocaoglu, Murat] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
   [Shanmugam, Karthikeyan] IBM Res NY, New York, NY USA.
   [Bareinboim, Elias] Purdue Univ, Dept Comp Sci & Stat, W Lafayette, IN 47907 USA.
RP Kocaoglu, M (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM mkocaoglu@utexas.edu; karthikeyan.shanmugam2@ibm.com; eb@purdue.edu
CR Aho A. V., 1972, SIAM Journal on Computing, V1, P131, DOI 10.1137/0201008
   Ali Ayesha R., 2005, P UNC ART INT
   ALON N, 1986, COMBINATORICA, V6, P201, DOI 10.1007/BF02579381
   Bareinboim E, 2012, P 28 C UNC ART INT, P113
   Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113
   Bensmail J., 2015, ELECT NOTE DISCRETE, V49, P773
   Borboudakis Sofia, 2012, 6 EUR WORKSH PROB GR
   Claassen Tom, 2010, ADV NEURAL INFORM PR, P415
   Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638
   Eberhardt Frederick, 2007, THESIS
   Hauser A., 2012, P 6 EUR WORKSH PROB
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   Heinze-Deml Christina, 2017, ANN REV STAT ITS APP
   Hoyer Patrik O, 2008, P NIPS 2008
   Hyttinen A, 2013, J MACH LEARN RES, V14, P3041
   Hyttinen Antti, 2013, ARXIV13096836
   Katona Gyula, 1966, J COMB THEORY, V1, P174
   Kocaoglu Murat, 2017, ICML 17
   Kocaoglu Murat, 2017, AAAI 17
   Kocaoglu Murat, 2017, R28 PURD U AI LAB
   Loh PL, 2014, J MACH LEARN RES, V15, P3065
   Magliacane Sara, 2016, ARXIV161110351
   Meganck Stijn, 2006, P 3 EUR WORKSH PROB
   Parviainen Pekka, 2011, JOINT EUR C MACH LEA
   Pearl J, 2009, CAUSALITY MODELS REA
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043
   Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Scholkopf Bernhard, 2015, P 32 INT C MACH LEAR
   Shanmugam Karthikeyan, 2015, NIPS 2015
   Shimizu S, 2006, J MACH LEARN RES, V7, P2003
   Silva R, 2006, J MACH LEARN RES, V7, P191
   Spirtes P., 2001, CAUSATION PREDICTION
   Triantafillou S, 2015, J MACH LEARN RES, V16, P2147
   Verma Thomas, 1992, P 8 INT C UNC ART IN
   Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001
   Zhang JJ, 2008, J MACH LEARN RES, V9, P1437
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407011
DA 2019-06-15
ER

PT S
AU Komiyama, J
   Honda, J
   Takeda, A
AF Komiyama, Junpei
   Honda, Junya
   Takeda, Akiko
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Position-based Multiple-play Bandit Problem with Unknown Position Bias
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMAL ADAPTIVE POLICIES
AB Motivated by online advertising, we study a multiple-play multi-armed bandit problem with position bias that involves several slots and the latter slots yield fewer rewards. We characterize the hardness of the problem by deriving an asymptotic regret bound. We propose the Permutation Minimum Empirical Divergence (PMED) algorithm and derive its asymptotically optimal regret bound. Because of the uncertainty of the position bias, the optimal algorithm for such a problem requires non-convex optimizations that are different from usual partial monitoring and semi-bandit problems. We propose a cutting-plane method and related bi-convex relaxation for these optimizations by using auxiliary variables.
C1 [Komiyama, Junpei; Honda, Junya] Univ Tokyo, Tokyo, Japan.
   [Honda, Junya; Takeda, Akiko] RIKEN, Wako, Saitama, Japan.
   [Takeda, Akiko] Inst Stat Math, Tachikawa, Tokyo, Japan.
RP Komiyama, J (reprint author), Univ Tokyo, Tokyo, Japan.
EM junpei@komiyama.info; honda@stat.t.u-tokyo.ac.jp; atakeda@ism.ac.jp
FU JSPS KAKENHI [17K12736, 16H00881, 15K00031]; Inamori Foundation Research
   Grant
FX The authors gratefully acknowledge Kohei Komiyama for discussion on a
   permutation matrix and sincerely thank the anonymous reviewers for their
   useful comments. This work was supported in part by JSPS KAKENHI Grant
   Number 17K12736, 16H00881, 15K00031, and Inamori Foundation Research
   Grant.
CR Agarwal D., 2009, P 18 INT C WORLD WID, P21, DOI DOI 10.1145/1526709.1526713
   ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491
   Auer Peter, 2002, SIAM J COMPUTING
   Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663
   Bubeck S, 2010, THESIS
   Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007
   Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222
   Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P231
   Combes  R., 2015, ADV NEURAL INFORM PR, V28, P2116
   Craswell Nick, 2008, P 2008 INT C WEB SEA, P87, DOI DOI 10.1145/1341531.1341545
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Garivier A., 2016, P MACH LEARN RES, P998
   Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440
   Guo F., 2009, P 2 ACM INT C WEB SE, P124, DOI DOI 10.1145/1498759.1498818
   Hall P., 1935, J LOND MATH SOC, V10, P26, DOI [DOI 10.1112/JLMS/S1-10.37.26, 10.1112/jlms/s1-10.37.26]
   HOGAN WW, 1973, SIAM REV, V15, P591, DOI 10.1137/1015073
   Honda J., 2010, P COLT 2010 HAIF ISR, P67
   Hopcroft J. E., 1973, SIAM Journal on Computing, V2, P225, DOI 10.1137/0202019
   IAB, 2017, IAB INT ADV REV REP
   Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054
   Katariya S., 2016, INT C MACH LEARN, P1215
   KEMPE D, 2008, WINE, V5385, P585
   Komiyama J., 2015, NIPS, P1792
   Komiyama J., 2015, 32 INT C MACH LEARN, P1152
   Kveton B, 2015, P 32 INT C MACH LEAR, P767
   Lagree P, 2016, ADV NEURAL INFORM PR, P1597
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Mutapcic A, 2009, OPTIM METHOD SOFTW, V24, P381, DOI 10.1080/10556780802712889
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Vanchinathan H. P., 2014, NIPS, P1691
   Yuan Shuai, 2013, P 7 INT WORKSH DAT M, V3, P1, DOI DOI 10.1145/2501040.2501980
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405008
DA 2019-06-15
ER

PT S
AU Kontorovich, A
   Sabato, S
   Weiss, R
AF Kontorovich, Aryeh
   Sabato, Sivan
   Weiss, Roi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite
   Dimensions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION; CONVERGENCE; BOUNDS; ERROR; RATES; CURSE
AB We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension - the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.
C1 [Kontorovich, Aryeh; Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.
   [Weiss, Roi] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.
RP Kontorovich, A (reprint author), Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.
EM karyeh@cs.bgu.ac.il; sabatos@bgu.ac.il; roiw@weizmann.ac.il
RI Jeong, Yongwook/N-7413-2016
FU Israel Science Foundation [555/15, 755/15]; IBM; Paypal
FX We thank Frederic Cerou for the numerous fruitful discussions and
   helpful feedback on an earlier draft. Aryeh Kontorovich was supported in
   part by the Israel Science Foundation (grant No. 755/15), Paypal and
   IBM. Sivan Sabato was supported in part by the Israel Science Foundation
   (grant No. 555/15).
CR Abraham C, 2006, ANN I STAT MATH, V58, P619, DOI 10.1007/s10463-006-0032-1
   Berend D, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2359
   Berend D, 2012, STAT PROBABIL LETT, V82, P1102, DOI 10.1016/j.spl.2012.02.014
   Biau G, 2005, IEEE T INFORM THEORY, V51, P2163, DOI 10.1109/TIT.2005.847705
   Biau G, 2010, IEEE T INFORM THEORY, V56, P2034, DOI 10.1109/TIT.2010.2040857
   Bogachev V. I., 2007, MEASURE THEORY, VII
   Bogachev V. I., 2007, MEASURE THEORY, VI
   Boiman  O., 2008, CVPR
   Cerou F, 2006, ESAIM-PROBAB STAT, V10, P340, DOI DOI 10.1051/PS:2006014
   Chaudhuri K., 2014, NIPS
   Clment K., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   DEVROYE L, 1981, IEEE T PATTERN ANAL, V3, P75, DOI 10.1109/TPAMI.1981.4767052
   Devroye L, 1985, WILEY SERIES PROBABI
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Federer H., 1969, GRUNDLEHREN MATH WIS, V153
   FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797
   Floyd S, 1995, MACH LEARN, V21, P269
   Gottlieb LA, 2016, THEOR COMPUT SCI, V620, P105, DOI 10.1016/j.tcs.2015.10.040
   Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840
   Gottlieb Lee-Ad, 2014, NEURAL INFORM PROCES
   Gottlieb Lee-Ad, 2017, J MACHINE LEARNING R
   Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7
   Hall P, 2005, ANN STAT, V33, P284, DOI 10.1214/009053604000000959
   Kallenberg O., 2002, FDN MODERN PROBABILI
   Kontorovich Aryeh, 2014, ARTIFICIAL INTELLIGE
   Kontorovich Aryeh, 2016, P ADV NEUR INF PROC, P856
   Kontorovich Aryeh, 2017, ABS170508184 CORR
   Kontorovich Aryeh, 2014, INT C MACH LEARN ICM
   Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P791
   KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248
   Littlestone  Nick, 1986, RELATING DATA UNPUB, P1
   Munkres J. R., 1975, TOPOLOGY 1 COURSE
   Pestov V, 2000, INFORM PROCESS LETT, V73, P47, DOI 10.1016/S0020-0190(99)00156-8
   Pestov V, 2013, COMPUT MATH APPL, V65, P1427, DOI 10.1016/j.camwa.2012.09.011
   Preiss D., 1981, COMMENT MATH U CAROL, V22, P181
   Preiss David, 1979, ABSTRACTA 7 WINTER S, P58
   PSALTIS D, 1994, IEEE T INFORM THEORY, V40, P820, DOI 10.1109/18.335893
   Rudin W, 1976, INT SERIES PURE APPL
   Rudin W., 1987, REAL COMPLEX ANAL
   Samworth RJ, 2012, ANN STAT, V40, P2733, DOI 10.1214/12-AOS1049
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Snapp RR, 1998, ANN STAT, V26, P850
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Tiser J, 2003, T AM MATH SOC, V355, P3277, DOI 10.1090/S0002-9947-03-03296-3
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   ZHAO LC, 1987, J MULTIVARIATE ANAL, V21, P168, DOI 10.1016/0047-259X(87)90105-9
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401059
DA 2019-06-15
ER

PT S
AU Konyushkova, K
   Raphael, S
   Fua, P
AF Konyushkova, Ksenia
   Raphael, Sznitman
   Fua, Pascal
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Active Learning from Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can be learnt either from simple synthetic 2D datasets or from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.
C1 [Konyushkova, Ksenia; Fua, Pascal] Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland.
   [Raphael, Sznitman] Univ Bern, ARTORG Ctr, Bern, Switzerland.
RP Konyushkova, K (reprint author), Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland.
EM ksenia.konyushkova@epfl.ch; raphael.sznitman@artorg.unibe.ch;
   pascal.fua@epfl.ch
RI Jeong, Yongwook/N-7413-2016
FU European Union's Horizon 2020 Research and Innovation Programme [720270]
FX This project has received funding from the European Union's Horizon 2020
   Research and Innovation Programme under Grant Agreement No. 720270 (HBP
   SGA1). We would like to thank Carlos Becker and Helge Rhodin for their
   comments on the text, and Lucas Maystre for his discussions and
   attention to details.
CR Adam-Bourdarios C., 2015, NIPS 2014 WORKSH HIG
   Baram Y., 2004, J MACHINE LEARNING R
   Chu H.-M., 2016, ARXIV160800667
   Dal Pozzolo A, 2015, IEEE IJCNN
   Ebert S., 2012, C COMP VIS PATT REC
   Everingham M., 2010, INT J COMPUTER VISIO
   Gilad-Bachrach R., 2005, ADV NEURAL INFORM PR
   Gordillo N., 2013, MAGNETIC RESONANCE M
   Hoi S., 2006, INT C MACH LEARN
   Houlsby N., 2011, ARXIV11125745
   Hsu W.-N., 2015, AM ASS ART INT C
   Huang S.-J., 2010, ADV NEURAL INFORM PR
   Iglesias J., 2011, INFORM PROCESSING ME
   Joshi A., 2009, C COMP VIS PATT REC
   Joshi A. J., 2012, IEEE T PATTERN ANAL
   Kapoor A., 2007, INT C COMP VIS
   Konyushkova K., 2015, INT C COMP VIS
   Long J., 2015, C COMP VIS PATT REC
   Lorena A. C., 2002, BRAZ WORKSH BIOINF
   Lucchi A., 2012, EUR C COMP VIS
   Luo T, 2004, INT C PATT RECOG, P478, DOI 10.1109/ICPR.2004.1334570
   Maystre L., 2017, INT C MACH LEARN
   Menza B., 2014, IEEE T MED IMAGING
   Mosinska A., 2016, C COMP VIS PATT REC
   Olsson F., 2009, LIT SURVEY ACTIVE MA
   Santoro A., 2016, INT C MACH LEARN
   Settles B., 2008, C EMP METH NAT LANG
   Settles B., 2010, TECHNICAL REPORT
   Singla A., 2016, INT C MACH LEARN
   Sznitman R., 2010, IEEE T PATTERN ANAL
   Tamar A., 2016, ADV NEURAL INFORM PR
   Tong S., 2002, MACHINE LEARNING
   Vezhnevets A., 2012, C COMP VIS PATT REC
   Yang Y., 2015, INT J COMPUTER VISIO
NR 34
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404029
DA 2019-06-15
ER

PT S
AU Koren, T
   Livni, R
   Mansour, Y
AF Koren, Tomer
   Livni, Roi
   Mansour, Yishay
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Armed Bandits with Metric Movement Costs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MARKOV DECISION-PROCESSES; SPACES; RATES
AB We consider the non-stochastic Multi-Armed Bandit problem in a setting where there is a fixed and known metric on the action space that determines a cost for switching between any pair of actions. The loss of the online learner has two components: the first is the usual loss of the selected actions, and the second is an additional loss due to switching between actions. Our main contribution gives a tight characterization of the expected minimax regret in this setting, in terms of a complexity measure C of the underlying metric which depends on its covering numbers. In finite metric spaces with k actions, we give an efficient algorithm that achieves regret of the form (O) over tilde (max{(CT2/3)-T-1/3, root kT}), and show that this is the best possible. Our regret bound generalizes previous known regret bounds for some special cases: (i) the unit-switching cost regret (Theta) over tilde (max{k(1/3)T(2/3), root kT}) where C = Theta(k), and (ii) the interval metric with regret (Theta) over tilde (max{T-2/3, root kT})where C = Theta(1). For infinite metrics spaces with Lipschitz loss functions, we derive a tight regret bound of (Theta) over tilde (Td+1/d+2) where d >= 1 is the Minkowski dimension of the space, which is known to be tight even when there are no switching costs.
C1 [Koren, Tomer] Google Brain, Mountain View, CA 94043 USA.
   [Livni, Roi] Princeton Univ, Princeton, NJ 08544 USA.
   [Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel.
   [Mansour, Yishay] Google, Mountain View, CA USA.
RP Koren, T (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM tkoren@google.com; rlivni@cs.princeton.edu; mansour@cs.tau.ac.il
RI Jeong, Yongwook/N-7413-2016
FU Eric and Wendy Schmidt Foundation; Israel Science Foundation; United
   States-Israel Binational Science Foundation (BSF); Israeli Centers of
   Research Excellence (I-CORE) program [4/11]
FX RL is supported in funds by the Eric and Wendy Schmidt Foundation for
   strategic innovations. YM is supported in part by a grant from the
   Israel Science Foundation, a grant from the United States-Israel
   Binational Science Foundation (BSF), and the Israeli Centers of Research
   Excellence (I-CORE) program (Center No. 4/11).
CR AGRAWAL R, 1988, IEEE T AUTOMAT CONTR, V33, P899, DOI 10.1109/9.7243
   Arora R., 2012, P ICML, P1503
   Asawa M, 1996, IEEE T AUTOMAT CONTR, V41, P328, DOI 10.1109/9.486316
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2007, LECT NOTES COMPUT SC, V4539, P454, DOI 10.1007/978-3-540-72927-3_33
   BANKS JS, 1994, ECONOMETRICA, V62, P687, DOI 10.2307/2951664
   Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477
   BORODIN A, 1992, J ACM, V39, P745, DOI 10.1145/146585.146588
   Borodin A., 1998, ONLINE COMPUTATION C
   Bubeck S., 2011, J MACHINE LEARNING R, V12, P1587
   Cope EW, 2009, IEEE T AUTOMAT CONTR, V54, P1243, DOI 10.1109/TAC.2009.2019797
   Dekel O., 2014, P STOC, P459
   Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396
   Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01
   Feldman M., 2016, ANN C NEUR INF PROC
   Geulen S., 2010, P 23 ANN C LEARN THE, P132
   Gittins J., 2011, MULTIARMED BANDIT AL
   Guha S, 2009, LECT NOTES COMPUT SC, V5556, P496, DOI 10.1007/978-3-642-02930-1_41
   Gyorgy A, 2014, IEEE T INFORM THEORY, V60, P2823, DOI 10.1109/TIT.2014.2307062
   Jun TS, 2004, ECONOMIST-NETHERLAND, V152, P513, DOI 10.1007/s10645-004-2477-z
   Kleinberg R. D., 2004, ADV NEURAL INFORM PR, P697
   Kleinberg R, 2010, PROC APPL MATH, V135, P827
   Kleinberg R, 2008, ACM S THEORY COMPUT, P681
   Koren T., 2017, COLT
   Koren T., 2017, ARXIV171008997
   Magureanu  S., 2014, P C LEARN THEOR COLT, P975
   Neu G, 2014, IEEE T AUTOMAT CONTR, V59, P676, DOI 10.1109/TAC.2013.2292137
   Ortner R, 2010, THEOR COMPUT SCI, V411, P2684, DOI 10.1016/j.tcs.2010.04.005
   Slivkins A., 2011, ADV NEURAL INFORM PR, P1602
   Slivkins A, 2013, J MACH LEARN RES, V14, P399
   Tao T., 2009, 245C NOTES 5 HAUSDOR
   Yu J., 2011, P 28 INT C MACH LEAR
   Yu JY, 2009, MATH OPER RES, V34, P737, DOI 10.1287/moor.1090.0397
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404019
DA 2019-06-15
ER

PT S
AU Koren, T
   Livni, R
AF Koren, Tomer
   Livni, Roi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Affine-Invariant Online Optimization and the Low-rank Experts Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS; BOUNDS
AB We present a new affine-invariant optimization algorithm called Online Lazy Newton. The regret of Online Lazy Newton is independent of conditioning: the algorithm's performance depends on the best possible preconditioning of the problem in retrospect and on its intrinsic dimensionality. As an application, we show how Online Lazy Newton can be used to achieve an optimal regret of order root rT for the low-rank experts problem, improving by a root r factor over the previously best known bound and resolving an open problem posed by Hazan et al. [15].
C1 [Koren, Tomer] Google Brain, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Livni, Roi] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA.
RP Koren, T (reprint author), Google Brain, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM tkoren@google.com; rlivni@cs.princeton.edu
RI Jeong, Yongwook/N-7413-2016
FU Eric and Wendy Schmidt Fund for Strategic Innovations
FX The authors would like to thank Elad Hazan for helpful discussions. RL
   is supported by the Eric and Wendy Schmidt Fund for Strategic
   Innovations.
CR Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157
   Barman S., 2017, ARXIV170604125
   Cesa-Bianchi N, 2005, SIAM J COMPUT, V34, P640, DOI 10.1137/S0097539703432542
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Chiang Chao- Kai, 2012, COLT, P6
   Cohen A., 2017, ARXIV170207870
   Cutkosky Ashok, 2017, ARXIV170302629
   de Rooij S, 2014, J MACH LEARN RES, V15, P1281
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Foster D. J., 2017, ARXIV170404010
   Hazan E., 2016, 29 ANN C LEARN THEOR, P1096
   Hazan E., 2009, P ANN C NEUR INF PRO, P709
   Hazan E, 2006, LECT NOTES ARTIF INT, V4005, P499, DOI 10.1007/11776420_37
   Hazan E, 2011, J MACH LEARN RES, V12, P1287
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Koren T., 2015, ADV NEURAL INFORM PR, P1477
   Luo H., 2016, ADV NEURAL INFORM PR, P902
   Rakhlin A., 2013, P 16 INT C ART INT S, P516
   Rakhlin Alexander, 2013, COLT, P993
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Streeter M., 2010, TECHNICAL REPORT
   van Erven Tim, 2016, ADV NEURAL INFORM PR, V29, P3666
   Vovk V, 2001, INT STAT REV, V69, P213
   Zinkevich  M., 2003, ONLINE CONVEX PROGRA
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404079
DA 2019-06-15
ER

PT S
AU Kosiorek, AR
   Bewley, A
   Posner, I
AF Kosiorek, Adam R.
   Bewley, Alex
   Posner, Ingmar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hierarchical Attentive Recurrent Tracking
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.
C1 [Kosiorek, Adam R.; Bewley, Alex; Posner, Ingmar] Univ Oxford, Dept Engn Sci, Oxford, England.
RP Kosiorek, AR (reprint author), Univ Oxford, Dept Engn Sci, Oxford, England.
EM adamk@robots.ox.ac.uk; bewley@robots.ox.ac.uk; ingmar@robots.ox.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU UK's Engineering and Physical Sciences Research Council (EPSRC)
   [EP/M019918/1]; Doctoral Training Award (DTA)
FX We would like to thank Oiwi Parker Jones and Martin Engelcke for
   discussions and valuable insights and Neil Dhir for his help with
   editing the paper. Additionally, we would like to acknowledge the
   support of the UK's Engineering and Physical Sciences Research Council
   (EPSRC) through the Programme Grant EP/M019918/1 and the Doctoral
   Training Award (DTA). The donation from Nvidia of the Titan Xp GPU used
   in this work is also gratefully acknowledged.
CR Bengio Y., 2009, ICML
   Cheung Brian, 2017, ICLR
   Cheung Brian, 2016, GPU TECHN C
   Dayan P, 2001, THEORETICAL NEUROSCI
   De Brabandere  B., 2016, NIPS
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geoffrey Hinton, 2012, OVERVIEW MINIBATCH G
   Gordon D, 2017, ARXIV170506368
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Gregor K., 2015, ICML
   Held David, 2016, ECCV WORK
   Hinton G. E., 2016, NIPS
   Jaderberg M., 2015, NIPS
   Jaderberg M., 2016, ARXIV161105397
   Kahou Samira Ebrahimi, 2017, CVPR WORK
   Karl Maximilian, 2017, ICLR
   Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315
   Kendall A, 2017, ARXIV170507115
   Kristan Matej, 2016, ECCV WORK
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Krueger David, 2017, ICLR
   Mnih V., 2014, NIPS
   Ning Guanghan, 2016, ARXIV160705781
   Schuldt C., 2004, ICPR
   Stollenga Marijn, 2014, ARXIV PREPR ARXIV, P13
   Valmadre J., 2017, CVPR
   Vinyals O., 2015, NIPS
   Yu J., 2016, P 2016 ACM MULT C, P516, DOI DOI 10.1145/2964284.2967274
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403012
DA 2019-06-15
ER

PT S
AU Koster, U
   Webb, TJ
   Wang, X
   Nassar, M
   Bansal, AK
   Constable, WH
   Elibol, OH
   Gray, S
   Hall, S
   Hornof, L
   Khosrowshahi, A
   Kloss, C
   Pai, RJ
   Rao, N
AF Koster, Urs
   Webb, Tristan J.
   Wang, Xin
   Nassar, Marcel
   Bansal, Arjun K.
   Constable, William H.
   Elibol, Oguz H.
   Gray, Scott
   Hall, Stewart
   Hornof, Luke
   Khosrowshahi, Amir
   Kloss, Carey
   Pai, Ruby J.
   Rao, Naveen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep
   Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet [1], a deep residual network [2, 3] and a generative adversarial network [4], using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.
C1 [Koster, Urs; Webb, Tristan J.; Wang, Xin; Nassar, Marcel; Bansal, Arjun K.; Constable, William H.; Elibol, Oguz H.; Gray, Scott; Hall, Stewart; Hornof, Luke; Khosrowshahi, Amir; Kloss, Carey; Pai, Ruby J.; Rao, Naveen] Intel Corp, Artificial Intelligence Prod Grp, Santa Clara, CA 95051 USA.
   [Koster, Urs; Hall, Stewart] Nervana Syst, Cerebras Syst, San Diego, CA 92121 USA.
   [Koster, Urs; Hall, Stewart] Intel Corp, Santa Clara, CA 95051 USA.
   [Gray, Scott] Nervana Syst, OpenAI, San Diego, CA 92121 USA.
RP Koster, U (reprint author), Intel Corp, Artificial Intelligence Prod Grp, Santa Clara, CA 95051 USA.; Koster, U (reprint author), Nervana Syst, Cerebras Syst, San Diego, CA 92121 USA.; Koster, U (reprint author), Intel Corp, Santa Clara, CA 95051 USA.
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Arjovsky M, 2017, ARXIV170107875
   Courbariaux M, 2016, ARXIV160202830
   Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, P3123
   Courbariaux Matthieu, 2014, INT C LEARN REPR WOR
   Gupta S., 2015, ARXIV150202551
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Heusel Martin, 2017, ABS170608500 CORR
   Hubara I., 2016, ARXIV160907061
   Hwang Kyuyeon, 2014, SIGN PROC SYST SIPS, P1, DOI DOI 10.1109/SIPS.2014.6986082
   Jouppi Norman P., 2017, ARXIV170404760
   Kim  M., 2016, ARXIV160106071
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lin D. D., 2016, INT C MACH LEARN, P2849
   Lin  Zhouhan, 2015, ARXIV151003009
   Mellempudi N., 2017, ARXIV170108978
   Miyashita  D., 2016, ARXIV160301025
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Seide F, 2014, INTERSPEECH, P1058
   Vanhoucke Vincent, 2011, ADV NEURAL INFORM PR
   Venkatesh G., 2016, ARXIV161000324
   WILLIAMSON D, 1991, IEEE PACIF, P315, DOI 10.1109/PACRIM.1991.160742
   Yu F., 2015, ARXIV150603365
   Zhou S., 2016, ARXIV160606160
NR 25
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401075
DA 2019-06-15
ER

PT S
AU Kotlowski, W
   Koolen, WM
   Malek, A
AF Kotlowski, Wojciech
   Koolen, Wouter M.
   Malek, Alan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Random Permutation Online Isotonic Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID BOUNDS
AB We revisit isotonic regression on linear orders, the problem of fitting monotonic functions to best explain the data, in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model, which lead to its study in the fixed design model. Here, we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for algorithms for online isotonic regression on partial orders.
C1 [Kotlowski, Wojciech] Poznan Univ Tech, Poznan, Poland.
   [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Malek, Alan] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Kotlowski, W (reprint author), Poznan Univ Tech, Poznan, Poland.
EM wkotlowski@cs.put.poznan.pl; wmkoolen@cwi.nl; amalek@mit.edu
FU Polish National Science Centre [2016/22/E/ST6/00299]; Netherlands
   Organization for Scientific Research (NWO) [639.021.439]
FX Wojciech Kotlowski acknowledges support from the Polish National Science
   Centre (grant no. 2016/22/E/ST6/00299). Wouter Koolen acknowledges
   support from the Netherlands Organization for Scientific Research (NWO)
   under Veni grant 639.021.439. This work was done in part while Koolen
   was visiting the Simons Institute for the Theory of Computing.
CR AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157
   BIRGE L, 1993, PROBAB THEORY REL, V97, P113, DOI 10.1007/BF01199316
   BRUNK HD, 1955, ANN MATH STAT, V26, P607, DOI 10.1214/aoms/1177728420
   Cesa-Bianchi N, 2001, MACH LEARN, V43, P247, DOI 10.1023/A:1010848128995
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   de Leeuw J, 2009, J STAT SOFTW, V32, P1
   Fawcett T, 2007, MACH LEARN, V68, P97, DOI 10.1007/s10994-007-5011-0
   Forster J, 2002, J COMPUT SYST SCI, V64, P76, DOI 10.1006/jcss.2001.1798
   Gaillard Pierre, 2015, C LEARN THEOR COLT, P764
   Kakade S., 2011, ADV NEURAL INFORM PR, P927
   Kalai Adam Tauman, 2009, COLT
   Kotlowski W., 2009, P 26 ANN INT C MACH, P537
   Kotlowski Wojciech, 2016, P 29 C LEARN THEOR C, P1165
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Kyng Rasmus, 2015, NEURAL INFORM PROCES
   Luss R, 2012, ANN APPL STAT, V6, P253, DOI 10.1214/11-AOAS504
   Menon Aditya Krishna, 2012, INT C MACH LEARN ICM
   Moon T., 2010, P 3 ACM INT C WEB SE, P151, DOI DOI 10.1145/1718487.1718507
   Narasimhan H., 2013, ADV NEURAL INFORM PR, P2913
   Niculescu-Mizil A., 2005, P 22 INT C MACH LEAR, P625, DOI DOI 10.1145/1102351.1102430
   Obozinski G, 2008, GENOME BIOL, V9, DOI 10.1186/gb-2008-9-s1-s6
   Rakhlin Alexander, 2014, C LEARN THEOR COLT, P1232
   Robertson T., 1998, ORDER RESTRICTED STA
   Stylianou M, 2002, BIOMETRICS, V58, P171, DOI 10.1111/j.0006-341X.2002.00171.x
   VANDEGEER S, 1990, ANN STAT, V18, P907, DOI 10.1214/aos/1176347632
   Vovk Vladimir, 2015, NEURAL INFORM PROCES, V28, P892
   Zadrozny B, 2002, P 8 ACM SIGKDD INT C, P694, DOI DOI 10.1007/S10994-013-5343-X
   Zhang CH, 2002, ANN STAT, V30, P528, DOI 10.1214/aos/1021379864
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404025
DA 2019-06-15
ER

PT S
AU Krichene, W
   Bartlett, P
AF Krichene, Walid
   Bartlett, Peter
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Acceleration and Averaging In Stochastic Descent Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MIRROR DESCENT; OPTIMIZATION
AB We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions.
   Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.
C1 [Krichene, Walid] Google Inc, Mountain View, CA 94043 USA.
   [Bartlett, Peter] Univ Calif Berkeley, Berkeley, CA USA.
RP Krichene, W (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM walidk@google.com; bartlett@cs.berkeley.edu
FU NSF [IIS-1619362]; Australian Research Council through an Australian
   Laureate Fellowship [FL110100281]; Australian Research Council through
   Australian Research Council Centre of Excellence for Mathematical and
   Statistical Frontiers (ACEMS)
FX We gratefully acknowledge the support of the NSF through grant
   IIS-1619362 and of the Australian Research Council through an Australian
   Laureate Fellowship (FL110100281) and through the Australian Research
   Council Centre of Excellence for Mathematical and Statistical Frontiers
   (ACEMS). We thank the anonymous reviewers for their insightful comments
   and suggestions.
CR Attouch H., 2015, ABS150704782 CORR
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Ben-Tal A, 2001, SIAM J OPTIMIZ, V12, P79, DOI 10.1137/S1052623499354564
   Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617
   Benaim M, 1999, LECT NOTES MATH, V1709, P1
   BLACK F, 1973, J POLIT ECON, V81, P637, DOI 10.1086/260062
   Bloch A., 1994, HAMILTONIAN GRADIENT
   Bottou L., 2016, ABS160604838 CORR
   Bubeck S., 2015, ADV NEURAL INFORM PR, V28, P1243
   Cabot A, 2009, T AM MATH SOC, V361, P5983, DOI 10.1090/S0002-9947-09-04785-0
   Cheng X., 2017, ABS170703663 CORR
   Cheng X., 2017, ABS170509048 CORR
   CHIANG TS, 1987, SIAM J CONTROL OPTIM, V25, P737, DOI 10.1137/0325042
   Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183
   Duchi John C., 2010, SIAM J OPTIMIZ, V22, P1549
   Durmus A., 2016, CORR
   Eberle A., 2017, CORR
   Errami M, 2002, PROBAB THEORY REL, V122, P191, DOI 10.1007/s004400100168
   Helmke U., 1994, COMMUNICATIONS CONTR
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Krichene W., 2016, NIPS
   Krichene W., 2015, NIPS
   Lan GH, 2012, MATH PROGRAM, V133, P365, DOI 10.1007/s10107-010-0434-y
   Lyapunov A., 1892, THESIS
   Mertikopoulos P., 2016, ABS161106730 CORR
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nemirovsky A. S., 1983, WILEY INTERSCIENCE S
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3
   Oksendal B., 2003, STOCHASTIC DIFFERENT
   Pavliotis G, 2014, TEXTS APPL MATH
   Raginsky M., 2017, ABS170203849 CORR
   Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639
   Rockafellar R T, 1970, CONVEX ANAL
   Su Weijie, 2014, NIPS
   Wibisono A., 2016, ABS160304245 CORR
   Wilson A. C., 2016, ABS161102635 CORR
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406083
DA 2019-06-15
ER

PT S
AU Kuleshov, V
   Ermon, S
AF Kuleshov, Volodymyr
   Ermon, Stefano
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural Variational Inference and Learning in Undirected Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHM
AB Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.
C1 [Kuleshov, Volodymyr; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA.
RP Kuleshov, V (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM kuleshov@cs.stanford.edu; ermon@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Intel Corporation; Toyota; NSF [1651565, 1649208, 1522054]; Future of
   Life Institute [2016-158687]
FX This work is supported by the Intel Corporation, Toyota, NSF (grants
   1651565, 1649208, 1522054) and by the Future of Life Institute (grant
   2016-158687).
CR Alimoglu F., 1996, COMBINING MULTIPLE C
   Bastien F, 2012, ARXIV12115590
   Bilmes JA, 2004, IMA VOL MATH APPL, V138, P191
   Burda Yuri, 2015, ABS150900519 CORR
   Desjardins G., 2011, ADV NEURAL INFORM PR, V24, P2501
   Dieng A. B., 2017, ADV NEURAL INFORM PR
   Gershman S., 2012, P 29 INT C MACH LEAR
   Gopal S., 2013, ICML, P289
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koller D., 2009, PROBABILISTIC GRAPHI
   Larochelle H., 2011, AISTATS, V1, P2
   Maaloe Lars, 2016, ARXIV160205473
   Minka T., 2005, DIVERGENCE MEASURES
   Mnih Andriy, 2014, ARXIV14020030
   Mnih Andriy, 2016, ARXIV160206725
   Ranganath  R., 2014, AISTATS, P814
   Ranganath Rajesh, 2016, INT C MACH LEARN, P324
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Rolfe J. T., 2016, ARXIV160902200
   Ryu Ernest K., 2014, UNPUB
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   Scott J., 2012, SOCIAL NETWORK ANAL
   Smolensky P, 1986, INFORM PROCESSING DY
   Srinivasan Rajan, 2013, IMPORTANCE SAMPLING
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Tran D., 2016, ARXIV161009787
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
NR 33
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406077
DA 2019-06-15
ER

PT S
AU Kumagai, W
AF Kumagai, Wataru
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Regret Analysis for Continuous Dueling Bandit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The dueling bandit is a learning framework wherein the feedback information in the learning process is restricted to a noisy comparison between a pair of actions. In this research, we address a dueling bandit problem based on a cost function over a continuous space. We propose a stochastic mirror descent algorithm and show that the algorithm achieves an O(root T log T)-regret bound under strong convexity and smoothness assumptions for the cost function. Subsequently, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function. Moreover, when considering a lower bound in convex optimization, our algorithm is shown to achieve the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor.
C1 [Kumagai, Wataru] RIKEN, Ctr Adv Intelligence Project, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.
RP Kumagai, W (reprint author), RIKEN, Ctr Adv Intelligence Project, Chuo Ku, 1-4-1 Nihonbashi, Tokyo 1030027, Japan.
EM wataru.kumagai@riken.jp
RI Jeong, Yongwook/N-7413-2016
FU JSPS KAKENHI [17K12653]
FX We would like to thank Professor Takafumi Kanamori for helpful comments.
   This work was supported by JSPS KAKENHI Grant Number 17K12653.
CR Agarwal A, 2010, P 23 ANN C LEARN THE, P28
   Ailon N., 2014, ARXIV14053396
   Bubeck S., 2014, ARXIV14121587
   Busa-Fekete R., 2014, P 31 INT C MACH LEAR, V32, P1071
   Busa-Fekete R., 2013, ICML, V28, P1094
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Griva I, 2009, OTHER TITL APPL MATH, V108, P1, DOI 10.1137/1.9780898717730
   Hazan E., 2014, ADV NEURAL INFORM PR, P784
   Jamieson K. G., 2015, AISTATS
   Jamieson K. G., 2012, ADV NEURAL INFORM PR, P2672
   Matsui K., 2016, J GLOBAL OPTIM, P1
   Nesterov Y., 1994, INTERIOR POINT POLYN, V13
   Shamir O., 2013, COLT 2013, P3
   Shamir O, 2017, J MACH LEARN RES, V18
   Urvoy T., 2013, P 30 INT C MACH LEAR, V28, P91
   Yue Y., 2009, P 26 ANN INT C MACH, P1201, DOI DOI 10.1145/1553374.1553527
   Yue Y., 2011, P 28 INT C MACH LEAR, P241
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Zhang  L., 2016, P INT C MACH LEARN, P392
   Zoghi M, 2014, P INT C MACH LEARN I, P10
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401051
DA 2019-06-15
ER

PT S
AU Kumar, A
   Sattigeri, P
   Fletcher, PT
AF Kumar, Abhishek
   Sattigeri, Prasanna
   Fletcher, P. Thomas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Semi-supervised Learning with GANs: Manifold Invariance with Improved
   Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.
C1 [Kumar, Abhishek; Sattigeri, Prasanna] IBM Res AI, Yorktown Hts, NY 10598 USA.
   [Fletcher, P. Thomas] Univ Utah, Salt Lake City, UT USA.
RP Kumar, A (reprint author), IBM Res AI, Yorktown Hts, NY 10598 USA.
EM abhishk@us.ibm.com; psattig@us.ibm.com; fletcher@sci.utah.edu
CR Arjovsky M., 2017, ARXIV170107875
   Bernstein Alexander V, 2012, ARXIV12126031
   Bernstein AV, 2014, ICML 2014 WORKSH TOP, V25, P1
   Berthelot D., 2017, ARXIV170310717
   Canas Guillermo, 2012, ADV NEURAL INFORM PR, P2465
   Chen GL, 2011, APPL NUMER HARMON AN, P199, DOI 10.1007/978-0-8176-8095-4_10
   Clevert D.A., 2015, ARXIV151107289
   Donahue J., 2016, ARXIV160509782
   dos Santos Cicero Nogueira, 2017, ARXIV170702198
   Dumoulin V., 2016, ARXIV160600704
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow IJ, 2014, ARXIV14126572
   Gulrajani Ishaan, 2017, ARXIV170400028
   Jia K, 2015, NEUROCOMPUTING, V160, P250, DOI 10.1016/j.neucom.2015.02.023
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Laine S, 2016, ARXIV161002242
   Lerman G, 2010, PREPRINT
   Maaloe Lars, 2016, ARXIV160205473
   Menon Aditya Krishna, 2016, INT C MACH LEARN, P304
   Miyato  T., 2015, ARXIV150700677
   Mohamed S., 2016, ARXIV161003483
   Mroueh Y., 2017, NIPS
   Mroueh Youssef, 2017, ICML
   Mroueh Youssef, 2015, ADV NEURAL INFORM PR, P1558
   Niyogi P, 2011, SIAM J COMPUT, V40, P646, DOI 10.1137/090762932
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Odena A., 2016, ARXIV160601583
   Radford A., 2015, ARXIV151106434
   Raj Anant, 2017, AISTATS
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Rifai  S., 2011, NIPS
   Salimans  T., 2016, ADV NEURAL INFORM PR
   Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239
   Spivak M., 1999, COMPREHENSIVE INTRO, V1
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Tu Z, 2007, PROC CVPR IEEE, P500
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   Zhao Junbo, 2015, ARXIV150602351
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405060
DA 2019-06-15
ER

PT S
AU Kusner, M
   Loftus, J
   Russell, C
   Silva, R
AF Kusner, Matt
   Loftus, Joshua
   Russell, Chris
   Silva, Ricardo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Counterfactual Fairness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RISK
AB Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.
C1 [Kusner, Matt; Russell, Chris; Silva, Ricardo] Alan Turing Inst, London, England.
   [Kusner, Matt] Univ Warwick, Coventry, W Midlands, England.
   [Loftus, Joshua] NYU, New York, NY 10003 USA.
   [Russell, Chris] Univ Surrey, Guildford, Surrey, England.
   [Silva, Ricardo] UCL, London, England.
RP Kusner, M (reprint author), Alan Turing Inst, London, England.; Kusner, M (reprint author), Univ Warwick, Coventry, W Midlands, England.
EM mkusner@turing.ac.uk; loftus@nyu.edu; crussell@turing.ac.uk;
   ricardo@stats.ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Alan Turing Institute under the EPSRC [EP/N510129/1]; EPSRC Platform
   Grant [EP/P022529/1]
FX This work was supported by the Alan Turing Institute under the EPSRC
   grant EP/N510129/1. CR acknowledges additional support under the EPSRC
   Platform Grant EP/P022529/1. We thank Adrian Weller for insightful
   feedback, and the anonymous reviewers for helpful comments.
CR Berk R., 2017, ARXIV170309207V1
   Bollen K., 1989, STRUCTURAL EQUATIONS, V3, P7
   Bollen K. A., 1993, TESTING STRUCTURAL E
   Bolukbasi T, 2016, ADV NEURAL INFORM PR, V29, P4349
   Brennan T, 2009, CRIM JUSTICE BEHAV, V36, P21, DOI 10.1177/0093854808326545
   Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x
   Chouldechova A, 2017, BIG DATA-US, V5, P153, DOI 10.1089/big.2016.0047
   Dawid AP, 2000, J AM STAT ASSOC, V95, P407, DOI 10.2307/2669377
   DeDeo Simon, 2014, ARXIV14124643
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Glymour C, 2014, EPIDEMIOLOGY, V25, P488, DOI 10.1097/EDE.0000000000000122
   Grgic-Hlaca Nina, 2016, NIPS S MACH LEARN LA
   Halpern Joseph Y, 2016, ACTUAL CAUSALITY
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Johnson Kory D, 2016, ARXIV160800528
   Joseph Matthew, 2016, ARXIV161009559
   Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8
   Kamiran Faisal, 2009, INT C COMP CONTR COM, P1, DOI DOI 10.1109/IC4.2009.4909197
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Khandani AE, 2010, J BANK FINANC, V34, P2767, DOI 10.1016/j.jbankfin.2010.06.001
   Kilbertus N, 2017, ADV NEUR IN, V30
   Kleinberg Jon, 2017, P 8 INN THEOR COMP S
   Lewis David, 1973, COUNTERFACTUALS
   Louizos  C., 2015, ARXIV151100830
   Mahoney JF, 2007, Patent No. [US7287008 B1, 7287008]
   Mooij J., 2009, P 26 ANN INT C MACH, P745
   Nabi R., 2017, ARXIV170510378V1
   Pearl J., 2000, CAUSALITY MODELS REA
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057
   Peters J, 2014, J MACH LEARN RES, V15, P2009
   Russell C., 2017, ADV NEURAL INFORM PR, V31
   Silva R., 2016, J MACHINE LEARNING R, V17, P1
   Stan Development Team, 2016, RSTAN R INT STAN
   Wightman Linda F, 1998, LSAC RES REPORT SERI
   Zafar Muhammad Bilal, 2016, ARXIV161008452
   Zafar Muhammad Bilal, 2015, ARXIV150705259
   Zemel R., 2013, JMLR P, P325
   Zliobaite  I., 2015, ARXIV151100148
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404014
DA 2019-06-15
ER

PT S
AU Kuzborskij, I
   Cesa-Bianchi, N
AF Kuzborskij, Ilja
   Cesa-Bianchi, Nicolo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Nonparametric Online Regression while Learning the Metric
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRADIENTS
AB We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix G of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret -on the same data sequence- in terms of the spectrum of G. As a preliminary step in our analysis, we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.
C1 [Kuzborskij, Ilja] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Cesa-Bianchi, Nicolo] Univ Milan, Dipartimento Informat, I-20135 Milan, Italy.
RP Kuzborskij, I (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM ilja.kuzborskij@gmail.com; nicolo.cesa-bianchi@unimi.it
RI Jeong, Yongwook/N-7413-2016
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation programme [637076]
FX Authors would like to thank Sebastien Gerchinovitz and Samory Kpotufe
   for useful discussions on this work. IK would like to thank Google for
   travel support. This work also was in parts funded by the European
   Research Council (ERC) under the European Union's Horizon 2020 research
   and innovation programme (grant agreement no 637076).
CR Allez R, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.046202
   Bellet A., 2013, ARXIV PREPRINT ARXIV
   Dong XM, 2008, J MATH ANAL APPL, V341, P1018, DOI 10.1016/j.jmaa.2007.10.044
   Gaillard P., 2015, C LEARN THEOR COLT
   Guo ZC, 2017, ADV COMPUT MATH, V43, P127, DOI 10.1007/s10444-016-9479-7
   Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36
   Jin R., 2009, C NEUR INF PROC SYST
   Kpotufe S., 2013, C NEUR INF PROC SYST
   Kpotufe S, 2016, J MACH LEARN RES, V17
   Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P798
   Mukherjee S, 2006, J MACH LEARN RES, V7, P519
   Mukherjee S, 2006, J MACH LEARN RES, V7, P2481
   Rakhlin A., 2014, C LEARN THEOR COLT
   Trivedi S., 2014, C UNC ART INT UAI
   Vovk V., 2006, INT C THEOR APPL MOD
   Vovk V., 2006, CS0609045 ARXIV
   Vovk V, 2007, MACH LEARN, V69, P193, DOI 10.1007/s10994-007-5021-y
   Wang Y., 2012, C LEARN THEOR COLT
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wilkinson J. H., 1965, ALGEBRAIC EIGENVALUE, V87
   Wu QA, 2010, J MACH LEARN RES, V11, P2175
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400064
DA 2019-06-15
ER

PT S
AU Kuznetsov, V
   Mohri, M
AF Kuznetsov, Vitaly
   Mohri, Mehryar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Discriminative State-Space Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. We also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.
C1 [Kuznetsov, Vitaly; Mohri, Mehryar] Google Res, New York, NY 10011 USA.
   [Mohri, Mehryar] Courant Inst, New York, NY 10011 USA.
RP Kuznetsov, V (reprint author), Google Res, New York, NY 10011 USA.
EM vitaly@cims.nyu.edu; mohri@cims.nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1618662, CCF-1535987]; Google Research Award
FX This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662, as
   well as a Google Research Award.
CR Barve Rakesh D., 1996, COLT
   Bollerslev Tim, 1986, J ECONOMETRICS
   Box G. E. P., 1990, TIME SERIES ANAL FOR
   Brockwell P. J., 1986, TIME SERIES THEORY M
   Commandeur J. J, 2007, INTRO STATE SPACE TI
   Cortes Corinna, 2017, ABS171010657 CORR
   de la Pena V. H., 1999, PROBABILITY ITS APPL
   Durbin J., 2012, OXFORD STAT SCI SERI
   ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773
   Hamilton J, 1994, TIME SERIES ANAL
   Kalman Rudolph Emil, 1960, T ASME J BASIC ENG, V82
   Kuznetsov V., 2015, ADV NEURAL INFORM PR, V28, P541
   Kuznetsov V, 2017, MACH LEARN, V106, P93, DOI 10.1007/s10994-016-5588-2
   Kuznetsov Vitaly, 2016, P 29 C LEARN THEOR C
   Kuznetsov Vitaly, 2014, ALT
   Ledoux M., 1991, ERGEBNISSE MATH IHRE
   Littlestone Nick, 1987, MACHINE LEARNING
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   Rakhlin Alexander, 2011, NIPS
   Rakhlin Alexander, 2015, JMLR, V16
   Rakhlin Alexander, 2015, PROBABILITY THEORY R
   Rakhlin Alexander, 2010, NIPS
   Zimin Alexander, 2017, AISTAT
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405073
DA 2019-06-15
ER

PT S
AU Lai, WS
   Huang, JB
   Yang, MH
AF Lai, Wei-Sheng
   Huang, Jia-Bin
   Yang, Ming-Hsuan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Semi-Supervised Learning for Optical Flow with Generative Adversarial
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large amounts of labeled data, existing approaches resort to synthetic, unrealistic datasets. On the other hand, unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods, however, rely on the fundamental assumptions of brightness constancy and spatial smoothness priors that do not hold near motion boundaries. In this paper, we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and baseline semi-supervised learning schemes.
C1 [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA.
   [Huang, Jia-Bin] Virginia Tech, Blacksburg, VA USA.
   [Yang, Ming-Hsuan] Nvidia Res, Santa Clara, CA USA.
RP Lai, WS (reprint author), Univ Calif Merced, Merced, CA 95343 USA.
EM wlai24@ucmerced.edu; jbhuang@vt.edu; mhyang@ucmerced.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER [1149783]
FX This work is supported in part by the NSF CAREER Grant #1149783, gifts
   from Adobe and NVIDIA.
CR Ahmadi Aria, 2016, ICIP
   [Anonymous], 2017, CVPR
   Bailer C., 2015, ICCV
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Butler D., 2012, ECCV
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Denton E. L., 2015, NIPS
   Fischer P., 2015, ICCV
   Ganin Y, 2016, J MACH LEARN RES, V17
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A., 2012, CVPR
   Goodfellow I., 2014, NIPS
   He K., 2016, CVPR
   Hoffman J., 2016, FCNS WILD PIXEL LEVE
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Isola  Phillip, 2017, CVPR
   Jaderberg M., 2015, NIPS
   Kingma D. P., 2015, ICLR
   Kozinski  M., 2017, ADVERSARIAL REGULARI
   Kuznietsov Y., 2017, CVPR
   Ledig C., 2017, CVPR
   Li Y., 2017, CVPR
   Luc Pauline, 2016, NIPS WORKSH ADV TRAI
   Lucas B. D., 1981, INT JOINT C ART INT
   Mayer Nikolaus, 2016, CVPR
   Menze Moritz, 2015, CVPR
   Pathak D., 2016, CVPR
   Perazzi  F., 2016, CVPR
   Ranjan A., 2017, CVPR
   Rasmus A., 2015, NIPS
   Revaud J., 2015, CVPR
   Rosenbaum D., 2013, NIPS
   Rosenbaum D., 2016, BRIGHTNESS CONSTANCY
   Soomro K., 2012, CRCVTR1201
   Sun D., 2008, ECCV
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Wang X., 2016, ECCV
   Weinzaepfel P., 2013, ICCV
   Yu J. J., 2016, ECCV WORKSH
   Zhang Y., 2016, ICML
   Zhu J.-Y., 2017, ICCV
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400034
DA 2019-06-15
ER

PT S
AU Lai, YA
   Hsu, CC
   Chen, WH
   Yeh, MY
   Lin, SD
AF Lai, Yi-An
   Hsu, Chin-Chi
   Chen, Wen-Hao
   Yeh, Mi-Yen
   Lin, Shou-De
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI PRUNE: Preserving Proximity and Global Ranking for Network Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We investigate an unsupervised generative approach for network embedding. A multi-task Siamese neural network structure is formulated to connect embedding vectors and our objective to preserve the global node ranking and local proximity of nodes. We provide deeper analysis to connect the proposed proximity objective to link prediction and community detection in the network. We show our model can satisfy the following design properties: scalability, asymmetry, unity and simplicity. Experiment results not only verify the above design properties but also demonstrate the superior performance in learning-to-rank, classification, regression, and link prediction tasks.
C1 [Lai, Yi-An; Chen, Wen-Hao; Lin, Shou-De] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Hsu, Chin-Chi; Yeh, Mi-Yen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
RP Lai, YA (reprint author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM b99202031@ntu.edu.tw; chinchi@iis.sinica.edu.tw; b02902023@ntu.edu.tw;
   miyen@iis.sinica.edu.tw; sdlin@csie.ntu.edu.tw
FU Ministry of Science and Technology (MOST) of Taiwan, R.O.C.
   [105-2628-E-001-002-MY2, 106-2628-E-006-005-MY3, 104-2628-E-002 -015
   -MY3, 106-2218-E-002 -014 -MY4]; Air Force Office of Scientific
   Research; Asian Office of Aerospace Research and Development (AOARD)
   [FA2386-17-1-4038]; Microsoft [FY16-RES-THEME-021]
FX This study was supported in part by the Ministry of Science and
   Technology (MOST) of Taiwan, R.O.C., under Contracts
   105-2628-E-001-002-MY2, 106-2628-E-006-005-MY3, 104-2628-E-002 -015 -MY3
   & 106-2218-E-002 -014 -MY4, Air Force Office of Scientific Research,
   Asian Office of Aerospace Research and Development (AOARD) under award
   number No.FA2386-17-1-4038, and Microsoft under Contracts
   FY16-RES-THEME-021. All opinions, findings, conclusions, and
   recommendations in this paper are those of the authors and do not
   necessarily reflect the views of the funding agencies.
CR Ahmed Amr, WWW 13
   Cao Shaosheng, CIKM 15
   Cao Shaosheng, AAAI 16
   Clevert Djork-Arne, 2015, CORR
   Glorot Xavier, AISTATS 11
   Grover Aditya, KDD 16
   Heidemann Julia, ICIS 10
   Huang Xiao, WSDM 17
   Kingma D. P., 2014, CORR
   Kleinberg Jon, 1999, J ACM
   Leicht EA, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.118703
   Levy Omer, NIPS 14
   Menon Aditya Krishna, ECML PKDD 11
   Mikolov T., NIPS 13
   Ou Mingdong, KDD 16
   Page  L, 1999, TECHNICAL REPORT
   Pan Shirui, IJCAI 16
   Perozzi B., KDD 14
   Song H. H., IMC 09
   Tang Jian, WWW 15
   Tang Lei, KDD 09
   Tu Cunchao, IJCAI 16
   Wang Daixin, KDD 16
   Wang Xiao, AAAI 17
   Wang Yujing, AAAI 13
   Wei Xiaokai, WWW 17
   Wu Zhizheng, 2015, ICASSP 15
   Yang Cheng, IJCAI 15
   Yang Jaewon, WSDM 13
   Zhang D., ICDM 16
   Zhou Chang, AAAI 17
   Zhu Shenghuo, SIGIR 07
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405033
DA 2019-06-15
ER

PT S
AU Lakshminarayanan, B
   Pritzel, A
   Blundell, C
AF Lakshminarayanan, Balaji
   Pritzel, Alexander
   Blundell, Charles
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Simple and Scalable Predictive Uncertainty Estimation using Deep
   Ensembles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.
C1 [Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles] DeepMind, London, England.
RP Lakshminarayanan, B (reprint author), DeepMind, London, England.
EM balajiln@google.com; apritzel@google.com; cblundell@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abbasi Mahdieh, 2017, ARXIV170206856
   Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300
   Amodei Dario, 2016, ARXIV160606565
   Bernardo J. M., 2009, BAYESIAN THEORY, V405
   Bishop C. M., 1994, MIXTURE DENSITY NETW
   Blundell C., 2015, ICML
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brier G. W., 1950, MONTHLY WEATHER REV
   Bucila C., 2006, KDD
   Clarke B., 2003, J MACHINE LEARNING R, V4, P683, DOI [10.1162/153244304773936090, DOI 10.1162/153244304773936090]
   Dawid A. P., 1982, J AM STAT ASS
   DeGroot M. H., 1983, STATISTICIAN
   Dietterich T., 2000, MULTIPLE CLASSIFIER
   Gal  Yarin, 2016, ICML
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Goodfellow I., 2015, ICLR
   Graves A., 2011, NIPS
   Guo C, 2017, ARXIV170604599
   Hasenclever L., 2015, ARXIV151209327
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hendrycks Dan, 2016, ARXIV161002136
   Hernandez-Lobato J. M., 2015, ICML
   Hinton G., 2015, ARXIV150302531
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang G., 2017, ICLR SUBMISSION
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Kingma D. P., 2015, NIPS
   Korattikara A., 2015, NIPS
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Kurakin  A., 2016, ARXIV161101236
   Lakshminarayanan B., 2016, THESIS
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee S., 2016, NIPS
   Lee S., 2015, ARXIV151106314
   Li Y., 2015, NIPS
   Louizos C., 2016, ARXIV160304733
   Mackay D. J. C., 1992, THESIS
   Maeda S., 2014, ARXIV14127003
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Minka TP, 2000, BAYESIAN MODEL AVERA
   Miyato T., 2016, ICLR
   Nair V., 2010, ICML
   Neal RM, 1996, BAYESIAN LEARNING NE
   Nix D. A., 1994, IEEE INT C NEUR NETW
   Osband I., 2016, NIPS
   Quinonero-Candela J., 2006, MACHINE LEARNING CHA
   Rasmussen C. E., 2005, ICML
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Singh S., 2016, NIPS
   Springenberg Jost Tobias, 2016, ADV NEURAL INFORM PR, V29, P4134, DOI DOI 10.1152/JN.00333.2004
   Srivastava N., 2014, JMLR
   Szegedy C., 2014, ICLR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tramer F., 2017, ARXIV170507204
   Vinyals O., 2016, NIPS
   Welling M., 2011, ICML
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/NMETH.3547, 10.1038/nmeth.3547]
NR 60
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406046
DA 2019-06-15
ER

PT S
AU Lam, RR
   Willcox, KE
AF Lam, Remi R.
   Willcox, Karen E.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Lookahead Bayesian Optimization with Inequality Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GLOBAL OPTIMIZATION
AB We consider the task of optimizing an objective function subject to inequality constraints when both the objective and the constraints are expensive to evaluate. Bayesian optimization (BO) is a popular way to tackle optimization problems with expensive objective function evaluations, but has mostly been applied to unconstrained problems. Several BO approaches have been proposed to address expensive constraints but are limited to greedy strategies maximizing immediate reward. To address this limitation, we propose a lookahead approach that selects the next evaluation in order to maximize the long-term feasible reduction of the objective function. We present numerical experiments demonstrating the performance improvements of such a lookahead approach compared to several greedy BO algorithms, including constrained expected improvement (EIC) and predictive entropy search with constraint (PESC).
C1 [Lam, Remi R.; Willcox, Karen E.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Lam, RR (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM rlam@mit.edu; kwillcox@mit.edu
FU AFOSR MURI [FA9550-15-1-0038]
FX This work was supported in part by the AFOSR MURI on multi-information
   sources of multi-physics systems under Award Number FA9550-15-1-0038,
   program manager Dr. Jean-Luc Cambier.
CR Audet C., 2000, 4891 AIAA
   Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1
   Bjorkman M, 2000, OPTIM ENG, V1, P373, DOI 10.1023/A:1011584207202
   Buffoni M., 2010, 40 FLUID DYN C EXH, P5008, DOI DOI 10.2514/6.2010-5008
   Feliot P, 2017, J GLOBAL OPTIM, V67, P97, DOI 10.1007/s10898-016-0427-3
   Gardner J.R., 2014, ICML, P937
   Gelbart M, 2014, ARXIV14035607
   Ginsbourger D, 2010, CONTRIB STAT, P89, DOI 10.1007/978-3-7908-2410-0_12
   Gonzalez-Hernandez J, 2016, J PEDIATR SURG, V51, P790, DOI 10.1016/j.jpedsurg.2016.02.024
   Gramacy R. B., 2010, ARXIV10044027
   Gramacy RB, 2016, TECHNOMETRICS, V58, P1, DOI 10.1080/00401706.2015.1014065
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J. M., 2015, ARXIV151109422
   Hernandez-Lobato J. M, 2015, P 32 INT C MACH LEAR, P499
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Jones DR, 2001, J GLOBAL OPTIM, V21, P345, DOI 10.1023/A:1012771025575
   Lam R., 2016, ADV NEURAL INFORM PR, P883
   Ling C. K., 2015, ARXIV151106890
   Mockus J., 1978, GLOBAL OPTIMIZATION, V2, P2
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Picheny V., 2014, P 17 INT C ART INT S, P787
   Picheny  V., 2016, ADV NEURAL INFORM PR, P1435
   Powell W. B., 2011, APPROXIMATE DYNAMIC, V842
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Regis RG, 2014, ENG OPTIMIZ, V46, P218, DOI 10.1080/0305215X.2013.765000
   Sasena M.J., 2001, CONSTRAINTS, V2, P5
   Schonlau M., 1998, NEW DEV APPL EXPT DE, V34, P11, DOI DOI 10.1214/LNMS/1215456182
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401089
DA 2019-06-15
ER

PT S
AU Lamb, A
   Hjelm, RD
   Ganin, Y
   Cohen, JP
   Courville, A
   Bengio, Y
AF Lamb, Alex
   Hjelm, R. Devon
   Ganin, Yaroslav
   Cohen, Joseph Paul
   Courville, Aaron
   Bengio, Yoshua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI GibbsNet: Iterative Adversarial Inference for Deep Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Directed latent variable models that formulate the joint distribution as p(x, z) = p(z) p(x vertical bar z) have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify p (z), often with a simple fixed prior that limits the expressiveness of the model. Undirected latent variable models discard the requirement that p(z) be specified with a prior, yet sampling from them generally requires an iterative procedure such as blocked Gibbs-sampling that may require many steps to draw samples from the joint distribution p(x, z). We propose a novel approach to learning the joint distribution between the data and a latent code which uses an adversarially learned iterative procedure to gradually refine the joint distribution, p(x, z), to better match with the data distribution on each step. GibbsNet is the best of both worlds both in theory and in practice. Achieving the speed and simplicity of a directed latent variable model, it is guaranteed (assuming the adversarial game reaches the virtual training criteria global minimum) to produce samples from p(x, z) with only a few sampling iterations. Achieving the expressiveness and flexibility of an undirected latent variable model, GibbsNet does away with the need for an explicit p(z) and has the ability to do attribute prediction, class-conditional generation, and joint image-attribute modeling in a single model which is not trained for any of these specific tasks. We show empirically that GibbsNet is able to learn a more complex p(z) and show that this leads to improved inpainting and iterative refinement of p(x, z) for dozens of steps and stable generation without collapse for thousands of steps, despite being trained on only a few steps.
CR Bengio Y., 2012, ABS12074404 CORR
   Bengio Yoshua, 2013, ABS13061091 CORR
   Bornschein J., 2015, ABS150603877 CORR
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Donahue J., 2017, P INT C LEARN REPR I
   Dumoulin V., 2017, P INT C LEARN REPR I
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian, 2016, ARXIV170100160
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hjelm Devon, 2016, ADV NEURAL INFORM PR, P4691
   Hjelm RD, 2017, ARXIV170208431
   Huszar F., 2017, ARXIV E PRINTS
   Kingma D.P., 2013, ARXIV13126114
   Lamb A., 2016, NEURAL INFORM PROCES, V2016
   Larsen A. B. L., 2015, ABS151209300 CORR
   Liu  Z., 2015, P INT C COMP VIS ICC
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Radford  A., 2015, ABS151106434 CORR
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salimans T., 2016, ABS160603498 CORR
   Sohl-Dickstein J., 2015, ABS150303585 CORR
   Song J., 2017, ICLR WORKSH TRACK
   Theis L., 2015, ARXIV E PRINTS
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405017
DA 2019-06-15
ER

PT S
AU Lample, G
   Zeghidour, N
   Usunier, N
   Bordes, A
   Denoyer, L
   Ranzato, M
AF Lample, Guillaume
   Zeghidour, Neil
   Usunier, Nicolas
   Bordes, Antoine
   Denoyer, Ludovic
   Ranzato, Marc'Aurelio
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fader Networks: Manipulating Images by Sliding Attributes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.
C1 [Lample, Guillaume; Zeghidour, Neil; Usunier, Nicolas; Bordes, Antoine; Ranzato, Marc'Aurelio] Facebook AI Res, Menlo Pk, CA 94025 USA.
   [Lample, Guillaume; Denoyer, Ludovic] Sorbonne Univ, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France.
   [Zeghidour, Neil] PSL Res Univ, INRIA, CNRS, EHESS,LSCP,ENS, Paris, France.
RP Lample, G (reprint author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Lample, G (reprint author), Sorbonne Univ, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France.
EM gl@fb.com; neilz@fb.com; usunier@fb.com; abordes@fb.com;
   ludovic.denoyer@lip6.fr; ranzato@fb.com
RI Jeong, Yongwook/N-7413-2016
CR Antipov G., 2017, ARXIV170201983
   Bowman S. R., 2015, ARXIV151106349
   Brock  A., 2016, ARXIV160907093
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Edwards Harrison, 2015, ARXIV151105897
   Ganin Y, 2016, J MACH LEARN RES, V17
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Higgins Irina, 2017, P ICLR 2017
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Isola  P., 2016, ARXIV161107004
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2539
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Liu  Z., 2015, P INT C COMP VIS ICC
   Louppe  G., 2016, ARXIV161101046
   Mathieu M. F., 2016, ADV NEURAL INFORM PR, P5041
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Perarnau G, 2016, ARXIV161106355
   Reed S.E., 2015, ADV NEURAL INFORM PR, V28, P1252
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Taigman Yaniv, 2016, ARXIV161102200
   Upchurch Paul, 2016, ARXIV161105507
   Wolf L., 2017, ARXIV170405693
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yang J., 2015, ADV NEURAL INFORM PR, P1099
   Zhu J Y, 2017, ARXIV170310593
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406005
DA 2019-06-15
ER

PT S
AU Lanctot, M
   Zambaldi, V
   Gruslys, A
   Lazaridou, A
   Tuyls, K
   Perolat, J
   Silver, D
   Graepel, T
AF Lanctot, Marc
   Zambaldi, Vinicius
   Gruslys, Audrunas
   Lazaridou, Angeliki
   Tuyls, Karl
   Perolat, Julien
   Silver, David
   Graepel, Thore
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LEARNERS; BEHAVIOR; GO
AB To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.
C1 [Lanctot, Marc; Zambaldi, Vinicius; Gruslys, Audrunas; Lazaridou, Angeliki; Tuyls, Karl; Perolat, Julien; Silver, David; Graepel, Thore] DeepMind, London, England.
RP Lanctot, M (reprint author), DeepMind, London, England.
EM lanctot@google.com; vzambaldi@google.com; audrunas@google.com;
   angeliki@google.com; karltuyls@google.com; perolat@google.com;
   davidsilver@google.com; thore@google.com
RI Jeong, Yongwook/N-7413-2016
CR Al-Shedivat Maruan, 2017, CORR
   Amato C., 2015, AAAI15, P1995
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Baker C. L., 2011, P 33 ANN C COGN SCI, P2469
   Bansal Trapit, 2017, CORR
   Barreto Andre, 2017, P 31 ANN C NEUR INF
   Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818
   Blum A, 2007, ALGORITHMIC GAME THEORY, P79
   Bosansky B, 2016, ARTIF INTELL, V237, P1, DOI 10.1016/j.artint.2016.03.005
   Bosansky Branislav, 2013, P 23 INT JOINT C ART
   Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2
   Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433
   Brown G. W., 1951, ACTIVITY ANAL PRODUC, P374
   Brown N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P7
   Brown Noam, 2017, CORR
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Burch N., 2007, P 27 ART INT AAAI 07
   Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Camerer Colin F., 2004, Q J EC
   Claus C, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P746
   Costa-Gomes MA, 2006, AM ECON REV, V96, P1737, DOI 10.1257/aer.96.5.1737
   Finn  C., 2017, P 34 INT C MACH LEAR, P1126
   Foerster Jakob, 2017, P 34 INT C MACH LEAR
   Foerster Jakob N., 2016, 30 C NEUR INF PROC S
   Ganzfried Sam, 2015, ACM T EC COMPUTATION, V3, P1
   Gatti N., 2013, 27 AAAI C ART INT, P335
   Gibson Richard, 2013, CORR
   Gilpin  A., 2009, THESIS
   Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579
   Greenwald A., 2003, 20 INT C MACH LEARN, P242
   Greenwald Amy, 2017, OUTSTANDING CONTRIBU, V11, P185
   Gruslys Audrunas, 2017, CORR
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   Hart Sergiu, 2001, EC ESSAYS FESTSCHRIF
   Hartford Jason S., 2016, P 30 C NEUR INF PROC
   Hausknecht Matthew, 2016, P INT C LEARN REPR I
   Hausknecht Matthew John, 2016, THESIS
   He H, 2016, P 33 INT C MACH LEAR, P1804
   Heess N., 2015, ADV NEURAL INFORM PR, P2944
   Heinrich Johannes, 2016, CORR
   Heinrich Johannes, 2015, P 32 INT C MACH LEAR
   Hoang T. N., 2013, INT JOINT C ART INT, P2298
   Hofbauer J, 2002, ECONOMETRICA, V70, P2265, DOI 10.1111/j.1468-0262.2002.00440.x
   Jaderberg M., 2016, CORR
   Johanson M., 2008, ADV NEURAL INFORM PR, V20, P1128
   Johanson  M., 2013, P 12 INT C AUT AG MU
   Johanson M., 2011, P 22 INT JOINT C ART, P258, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-054
   Johanson M B, 2016, THESIS
   Kaisers Michael, 2010, P 9 INT C AUT AG MUL, P309
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kleiman-Weiner Max, 2016, P 38 ANN C COGN SCI
   Koller D., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P750, DOI 10.1145/195058.195451
   Kouvaris Kostas, 2017, PLOS COMPUTATIONAL B, V13, P1
   Kuhn H., 1953, ANN MATH STUD, P193
   Lanctot M, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1257
   Lanctot Marc, 2017, CORR
   Lauer M., 2004, P AAMAS 04 NEW YORK
   Laurent GJ, 2011, INT J KNOWL-BASED IN, V15, P55, DOI 10.3233/KES-2010-0206
   Lazaridou Angeliki, 2017, P INT C LEARN REPR I
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leibo Joel Z., 2017, P 16 INT C AUT AG MU
   Leslie DS, 2006, GAME ECON BEHAV, V56, P285, DOI 10.1016/j.geb.2005.08.005
   Littman M., 1994, P 11 INT C MACH LEAR, V157, P157
   Littman M. L., 2001, P 18 INT C MACH LEAR, V1, P322
   Littman ML, 2015, NATURE, V521, P445, DOI 10.1038/nature14540
   Long J, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P134
   Lowe Ryan, 2017, CORR
   Marecki Janusz, 2008, P 7 INT JOINT C AUT
   Marivate Vukosi N., 2015, THESIS
   Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057
   McMahan H. B., 2003, P 20 INT C MACH LEAR
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moravcik Matej, 2017, SCIENCE, V358
   Munos R., 2016, ADV NEURAL INFORM PR
   Nair Ranjit, 2004, THESIS
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Oliehoek F. A., 2006, P GEN EV COMP C GECC
   Oliehoek Frans A., 2016, SPRINGER BRIEFS INTE
   Omidshafiei Shayegan, 2017, P 34 INT C MACH LEAR
   Panait L, 2008, J MACH LEARN RES, V9, P423
   Parkes DC, 2015, SCIENCE, V349, P267, DOI 10.1126/science.aaa8403
   Ponsen Marc, 2009, ENTERTAINMENT COMPUT
   Sailer F., 2007, IEEE S COMP INT GAM, P37
   Samothrakis Spyridon, 2013, IEEE T EVOLUTIONARY
   Schmid Martin, 2014, P 28 AAAI C ART INT
   Schvartzman L. Julian, 2009, P 8 INT C AUT AG MUL, P249
   Shang  Wenling, 2016, P INT C MACH LEARN I
   Shoham Y., 2009, MULTIAGENT SYSTEMS A
   Shoham Y, 2007, ARTIF INTELL, V171, P365, DOI 10.1016/j.artint.2006.02.006
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sukhbaatar S., 2016, 30 C NEUR INF PROC S
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395
   Tavares Anderson, 2016, 12 AAAI C ART INT IN
   TAYLOR PD, 1978, MATH BIOSCI, V40, P145, DOI 10.1016/0025-5564(78)90077-9
   Tuyls K., 2008, AGENTS SIMULATION AP, P218
   Tuyls K, 2012, AI MAG, V33, P41, DOI 10.1609/aimag.v33i3.2426
   Walsh William E., 2002, AAAI 02 WORKSH GAM T
   Wellman Michael P., 2006, P NAT C ART INT AAAI
   Whiteson S., 2011, 2011 IEEE S AD DYN P, P120, DOI DOI 10.1109/ADPRL.2011.5967363
   Wright JR, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P901
   Wright Mason, 2016, CORR
   Yakovenko Nikolai, 2016, P 30 AAAI C ART INT
   Yoshida W., 2008, PLOS COMPUTATIONAL B, V4, P1
   Zinkevich  M., 2007, ADV NEURAL INFORM PR, V20
NR 107
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404026
DA 2019-06-15
ER

PT S
AU Lattimore, T
AF Lattimore, Tor
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan et al. [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.
C1 [Lattimore, Tor] DeepMind, London, England.
RP Lattimore, T (reprint author), DeepMind, London, England.
EM tor.lattimore@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Alon N., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P20, DOI 10.1145/237814.237823
   Audibert J.-Y., 2009, COLT, P217
   Bubeck S, 2012, REGRET ANAL STOCHAST
   Bubeck S, 2013, IEEE T INFORM THEORY, V59, P7711, DOI 10.1109/TIT.2013.2277869
   Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454
   Cowan Wesley, 2015, ARXIV150405823V2
   Cowan Wesley, 2015, ARXIV150501918
   DeCarlo LT, 1997, PSYCHOL METHODS, V2, P292, DOI 10.1037//1082-989X.2.3.292
   Honda J., 2010, P COLT 2010 HAIF ISR, P67
   Honda J, 2015, J MACH LEARN RES, V16, P3721
   KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lattimore Tor, 2015, ARXIV150707880
   Pena V. H., 2008, SELF NORMALIZED PROC
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401060
DA 2019-06-15
ER

PT S
AU Law, HCL
   Yau, C
   Sejdinovic, D
AF Law, Ho Chung Leon
   Yau, Christopher
   Sejdinovic, Dino
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Testing and Learning on Distributions with Symmetric Noise Invariance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID EMBEDDINGS
AB Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rare that all possible differences between samples are of interest - discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise.
C1 [Law, Ho Chung Leon; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England.
   [Yau, Christopher] Univ Birmingham, Ctr Computat Biol, Birmingham, W Midlands, England.
RP Law, HCL (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM hlaw@statsox.ac.uk; c.yau@bham.ac.uk; dino.sejdinovic@stats.ox.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; MRC
   [MR/L001411/1]; Spanish MultiDark Consolider Project [CSD2009-00064];
   Gauss Centre for Supercomputing e.V.; Partnership for Advanced
   Supercomputing in Europe (PRACE)
FX We thank Dougal Sutherland for suggesting the use of of the dark matter
   dataset, Michelle Ntampaka for providing the catalog, as well as Ricardo
   Silva, Hyunjik Kim and Kaspar Martens for useful discussions. This work
   was supported by the EPSRC and MRC through the OxWaSP CDT programme
   (EP/L016710/1). C.Y. and H.C.L.L. also acknowledge the support of the
   MRC Grant No. MR/L001411/1.; The CosmoSim database used in this paper is
   a service by the Leibniz-Institute for Astrophysics Potsdam (AIP). The
   MultiDark database was developed in cooperation with the Spanish
   MultiDark Consolider Project CSD2009-00064. The authors gratefully
   acknowledge the Gauss Centre for Supercomputing e.V.
   (www.gauss-centre.eu) and the Partnership for Advanced Supercomputing in
   Europe (PRACE, www.prace-ri.eu) for funding the MultiDark simulation
   project by providing computing time on the GCS Supercomputer SuperMUC at
   Leibniz Supercomputing Centre (LRZ, www.lrz.de).
CR Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Chwialkowski K. P, 2015, P ADV NEUR INF PROC, V28, P1981
   Delaigle A, 2016, J R STAT SOC B, V78, P231, DOI 10.1111/rssb.12109
   Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Klypin A, 2014, ARXIV14114001
   Law Ho Chung Leon, 2017, ARXIV170504293
   Lichman M., 2013, UCI MACHINE LEARNING
   LINNIK Y, 1977, DECOMPOSITION RANDOM
   Mitrovic J, 2016, P ICML 2016 JMLR W C, P1482
   Muandet K., 2012, ADV NEURAL INFORM PR, V25, P10
   Muandet Krikamol, 2016, ARXIV160509522
   Ntampaka M, 2016, ASTROPHYS J, V831, DOI 10.3847/0004-637X/831/2/135
   Ntampaka M, 2015, ASTROPHYS J, V803, DOI 10.1088/0004-637X/803/2/50
   Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rossberg H. - J., 1995, J MATH SCI, V76, P2181
   Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Sutherland Dougal J., 2016, AAAI C ART INT AAAI, P2073
   Szabo Zoltan, 2015, P INT C ART INT STAT
   Wang Z, 2012, IEEE T GEOSCI REMOTE, V50, P2226, DOI 10.1109/TGRS.2011.2171691
   Wendland H., 2004, SCATTERED DATA APPRO
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401037
DA 2019-06-15
ER

PT S
AU Lee, J
   Carlson, D
   Shokri, H
   Yao, WC
   Goetz, G
   Hagen, E
   Batty, E
   Chichilnisky, EJ
   Einevoll, G
   Paninski, L
AF Lee, JinHyung
   Carlson, David
   Shokri, Hooshmand
   Yao, Weichi
   Goetz, Georges
   Hagen, Espen
   Batty, Eleanor
   Chichilnisky, E. J.
   Einevoll, Gaute
   Paninski, Liam
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI YASS: Yet Another Spike Sorter
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Spike sorting is a critical first step in extracting neural signals from large-scale electrophysiological data. This manuscript describes an efficient, reliable pipeline for spike sorting on dense multi-electrode arrays (MEAs), where neural signals appear across many electrodes and spike sorting currently represents a major computational bottleneck. We present several new techniques that make dense MEA spike sorting more robust and scalable. Our pipeline is based on an efficient multi-stage "triage-then-cluster-then-pursuit" approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or "collided" events (representing two neurons firing synchronously). This is accomplished by developing a neural network detection method followed by efficient outlier triaging. The clean waveforms are then used to infer the set of neural spike waveform templates through nonparametric Bayesian clustering. Our clustering approach adapts a "coreset" approach for data reduction and uses efficient inference methods in a Dirichlet process mixture model framework to dramatically improve the scalability and reliability of the entire pipeline. The "triaged" waveforms are then finally recovered with matching-pursuit deconvolution techniques. The proposed methods improve on the state-of-the-art in terms of accuracy and stability on both real and biophysically-realistic simulated MEA data. Furthermore, the proposed pipeline is efficient, learning templates and clustering faster than real-time for a similar or equal to 500-electrode dataset, largely on a single CPU core.
C1 [Lee, JinHyung; Shokri, Hooshmand; Yao, Weichi; Batty, Eleanor; Paninski, Liam] Columbia Univ, New York, NY 10027 USA.
   [Carlson, David] Duke Univ, Durham, NC 27706 USA.
   [Goetz, Georges; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA.
   [Hagen, Espen] Univ Oslo, Oslo, Norway.
   [Einevoll, Gaute] Norwegian Univ Life Sci, As, Norway.
RP Lee, J (reprint author), Columbia Univ, New York, NY 10027 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1546296, IIS-1430239]; DARPA [N66001-17-C-4002]
FX This work was partially supported by NSF grants IIS-1546296 and
   IIS-1430239, and DARPA Contract No. N66001-17-C-4002.
CR Arthur D., 2007, ACM SIAM S DISCR ALG
   Bachem O., 2015, ICML
   Bahmani B., 2012, P VLDB ENDOWMENT
   Bankman I. N., 1993, IEEE T BIOMED ENG
   Barnett A. H., 2016, J NEURO METHODS
   Buzsaki G., 2004, NATURE NEUROSCIENCE
   Campbell T., 2015, NIPS
   Carlson D., 2013, NIPS
   Carlson D. E., 2014, IEEE TBME
   Chen B., 2011, NIPS
   Dacey D. M., 2003, NEURON
   Ekanadham C., 2014, J NEURO METHODS
   Feldman D., 2011, NIPS
   Fournier J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0160494
   Franke F., 2010, J COMP NEURO
   Gibson S., 2012, IEEE SIGNAL PROCESSI
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hagen E., 2015, J NEURO METHODS
   Har-Peled S., 2004, ACM THEORY COMPUTING
   Hilgen G., 2017, CELL REPORTS
   Hughes M. C., 2013, NIPS
   Ishwaran H., 2001, JASA
   Jun J. J., 2017, BIORXIV, DOI [10.1101/, DOI 10.1101/]
   Kadir Shabnam N, 2014, NEURAL COMPUTATION
   Kim K. H., 2000, IEEE TBME
   Kingma D. P., 2015, ICLR
   Knox E. M., 1998, VLDB
   Knudson K. C., 2014, NIPS
   Lewicki M. S., 1998, NETWORK COMPUTATION
   Litke A., 2004, IEEE T NUCL SCI
   Magland J. F., 2015, ARXIV150804841
   Mukhopadhyay S., 1998, IEEE TBME
   Muthmann JO, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00028
   Neal R. M., 2000, J COMPUTATIONAL GRAP
   Ng Andrew Y, SPECTRAL CLUSTERING
   Pachitariu M., 2016, NIPS
   Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123
   Quiroga R. Q., 2004, NEURAL COMPUTATION
   Rey H. G., 2015, BRAIN RES B
   Rodriguez A., 2014, SCIENCE
   Schmidt E. M., 1984, J NEURO METHODS
   Tarjan R., 1972, SIAM J COMPUTING
   Thorbergsson P. T., 2010, IEEE EMBC
   Ventura V., 2009, NEURAL COMPUTATION
   Vogelstein R. J., 2004, IEEE EMBS, V1
   Wang L., 2011, J COMP GRAPHICAL STA
   Wiltschko A. B., 2008, J NEURO METHODS
   Wood F., 2008, J NEURO METHODS
   Wood F., 2004, IEEE TBME
   Yang X., 1988, IEEE T BIOMED ENG
   Yger P., 2016, BIORXIV
   Zelnik-Manor L., 2004, NIPS, V17
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404008
DA 2019-06-15
ER

PT S
AU Lee, SW
   Kim, JH
   Jun, J
   Ha, JW
   Zhang, BT
AF Lee, Sang-Woo
   Kim, Jin-Hwa
   Jun, Jaehyun
   Ha, Jung-Woo
   Zhang, Byoung-Tak
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Overcoming Catastrophic Forgetting by Incremental Moment Matching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSDBirds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.
C1 [Lee, Sang-Woo; Kim, Jin-Hwa; Jun, Jaehyun; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea.
   [Ha, Jung-Woo] NAVER Corp, Clova AI Res, Seongnam, South Korea.
   [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea.
RP Lee, SW (reprint author), Seoul Natl Univ, Seoul, South Korea.
EM slee@bi.snu.ac.kr; jhkim@bi.snu.ac.kr; jhjun@bi.snu.ac.kr;
   jungwoo.ha@navercorp.com; btzhang@bi.snu.ac.kr
FU Naver Corp.; Korean government [IITP-R0126-16-1072-SW. StarLab,
   IITP-2017-0-01772-VTT, KEIT-10044009-HRI. MESSI, KEIT-10060086-RISF]
FX The authors would like to thank Jiseob Kim, Min-Oh Heo, Donghyun Kwak,
   Insu Jeon, Christina Baek, and Heidi Tessmer for helpful comments and
   editing. This work was supported by the Naver Corp. and partly by the
   Korean government (IITP-R0126-16-1072-SW. StarLab,
   IITP-2017-0-01772-VTT, KEIT-10044009-HRI. MESSI, KEIT-10060086-RISF).
   Byoung-Tak Zhang is the corresponding author.
CR Amendola Carlos, 2017, ARXIV170205066
   Baldi P, 2013, ADV NEURAL INFORM PR, P2814
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   Fernando C, 2017, ARXIV170108734
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Ghahramani Z., 2000, NIPS WORKSH ONL LEAR
   Goldberger J, 2005, ADV NEURAL INFORM PR, P505
   Goodfellow I. J., 2013, ARXIV13126211
   Goodfellow Ian J, 2014, ARXIV14126544
   Huang Z, 2014, INTERSPEECH, P1214
   Huang Zhen, 2015, 16 ANN C INT SPEECH
   Kienzle W., 2006, P 23 INT C MACH LEAR, P457, DOI DOI 10.1145/1143844.1143902
   Kingma D.P., 2013, ARXIV13126114
   Kirkpatrick J, 2017, P NATL ACAD SCI
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lee Sang- Woo, 2017, NEURAL NETWORKS
   Lee Sang-Woo, 2016, P INT JOINT C ART IN, P1669
   Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37
   Louizos C., 2016, ARXIV160304733
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Mccloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P104, DOI DOI 10.1016/S0079-7421(08)60536-8
   Pascanu R., 2013, ARXIV13013584
   Pathak  M., 2010, ADV NEURAL INFORM PR, P1876
   Rashwan Abdullah, 2016, P 19 INT C ART INT S, P1469
   Ray S, 2005, ANN STAT, V33, P2042, DOI 10.1214/00905360500000417
   Ray S, 2012, J MULTIVARIATE ANAL, V108, P41, DOI 10.1016/j.jmva.2012.02.006
   Rusu A. A., 2016, ARXIV160604671
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Wah C., 2011, CNSTR2011001
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zhang K, 2010, IEEE T NEURAL NETWOR, V21, P644, DOI 10.1109/TNN.2010.2040835
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404070
DA 2019-06-15
ER

PT S
AU Lehrmann, AM
   Sigal, L
AF Lehrmann, Andreas M.
   Sigal, Leonid
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Non-parametric Structured Output Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep neural networks (DNNs) and probabilistic graphical models (PGMs) are the two main tools for statistical modeling. While DNNs provide the ability to model rich and complex relationships between input and output variables, PGMs provide the ability to encode dependencies among the output variables themselves. End-to-end training methods for models with structured graphical dependencies on top of neural predictions have recently emerged as a principled way of combining these two paradigms. While these models have proven to be powerful in discriminative settings with discrete outputs, extensions to structured continuous spaces, as well as performing efficient inference in these spaces, are lacking. We propose non-parametric structured output networks (NSON), a modular approach that cleanly separates a non-parametric, structured posterior representation from a discriminative inference scheme but allows joint end-to-end training of both components. Our experiments evaluate the ability of NSONs to capture structured posterior densities (modeling) and to compute complex statistics of those densities (inference). We compare our model to output spaces of varying expressiveness and popular variational and sampling-based inference algorithms.
C1 [Lehrmann, Andreas M.; Sigal, Leonid] Disney Res, Pittsburgh, PA 15213 USA.
RP Lehrmann, AM (reprint author), Disney Res, Pittsburgh, PA 15213 USA.
EM andreas.lehrmann@disneyresearch.com; lsigal@disneyresearch.com
RI Jeong, Yongwook/N-7413-2016
CR Adams A., 2010, COMPUTER GRAPHICS FO
   Bandanau D., 2015, ICLR
   Bishop C. M., 1994, TECHNICAL REPORT
   Campbell N., 2013, CVPR
   Chen L. C., 2015, ICLR
   Chen L.-C., 2015, ICML
   Collobert  R., 2008, ICML
   Deng J., 2009, CVPR
   Deng Z., 2015, CVPR
   Ghahramani Z., 2001, NIPS
   Girshick R., 2015, ICCV
   He K., 2016, CVPR
   Hoffman M. D., 2013, JMLR
   Ihler A., 2009, AISTATS
   Ioffe S., 2015, ICML
   Isard M., 2003, CVPR
   Jain A., 2016, CVPR
   Kingma D. P., 2015, ICLR
   Koller D., 2009, PROBABILISTIC GRAPHI
   Koller D., 1999, UAI
   Kothapa R., 2011, TECHNICAL REPORT
   Kraehenbuehl P., 2012, NIPS
   Krizhevsky A., 2012, NIPS
   Lehrmann A., 2013, ICCV
   Lin T.Y., 2014, ARXIV14050312CSCV
   Pacheco J., 2014, ICML
   Park M., 2008, CVPR
   Pearl J, 1988, PROBABILISTIC REASON
   Ranganath R., 2014, JMLR W CP
   Rena S., 2015, ARXIV150601497CSCV
   Ross S., 2011, CVPR
   Schwing A., 2015, ARXIV150302351CSCV
   Scott DW, 1992, MULTIVARIATE DENSITY
   Shelhamer E., 2016, PAMI
   Simonyan Karen, 2015, ICLR
   Srivastava N., 2014, JMLR
   Sudderth E. B., 2003, CVPR, P2
   Sun W., 2017, ARXIV170301030CSLG
   Wang W., 2013, ARXIV13091541CSLG
   Weiss Y., 2001, NEURAL COMPUTATION
   Yedidia J.S., 2001, TECHNICAL REPORT
   Zheng S., 2015, ICCV
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404028
DA 2019-06-15
ER

PT S
AU Lei, LH
   Ju, C
   Chen, JB
   Jordan, MI
AF Lei, Lihua
   Ju, Cheng
   Chen, Jianbo
   Jordan, Michael, I
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Non-Convex Finite-Sum Optimization Via SCSG Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We develop a class of algorithms, as variants of the stochastically controlled stochastic gradient (SCSG) methods [21], for the smooth non-convex finite-sum optimization problem. Assuming the smoothness of each component, the complexity of SCSG to reach a stationary point with E parallel to del f(x)parallel to(2) <= epsilon is O (min{epsilon(-5/3), epsilon(-1)n(2/3)}), which strictly outperforms the stochastic gradient descent. Moreover, SCSG is never worse than the state-of-the-art methods based on variance reduction and it significantly outperforms them when the target accuracy is low. A similar acceleration is also achieved when the functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments demonstrate that SCSG outperforms stochastic gradient methods on training multi-layers neural networks in terms of both training and validation loss.
C1 [Lei, Lihua; Ju, Cheng; Chen, Jianbo; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Lei, LH (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM lihua.lei@berkeley.edu; cju@berkeley.edu; jianbochen@berkeley.edu;
   jordan@stat.berkeley.edu
RI Jeong, Yongwook/N-7413-2016; Ju, Cheng/O-8988-2019
CR Agarwal Alekh, 2014, ABS14100723 ARXIV
   Agarwal Naman, 2016, ARXIV161101146
   Allen-Zhu Z., 2014, ARXIV14071537
   Allen-Zhu  Z., 2017, ARXIV170200763
   Allen-Zhu Zeyuan, 2016, ABS160305643 ARXIV
   Allen-Zhu Zeyuan, 2015, ABS150601972 ARXIV
   Babanezhad R., 2015, ADV NEURAL INFORM PR, P2242
   Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022
   Carmon Yair, 2016, ARXIV161100756
   Carmon Yair, 2017, ARXIV170502766
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gaivoronski A. A., 1994, OPTIMIZATION METHODS, V4, P117
   Ghadimi S, 2016, MATH PROGRAM, V156, P59, DOI 10.1007/s10107-015-0871-8
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lei Lihua, 2016, ARXIV160903261
   McCullagh P., 1989, GEN LINEAR MODELS
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Polyak Boris Teodorovich, 1963, ZH VYCH MAT MAT FIZ, V3, P643
   Reddi S. J., 2016, ARXIV160306160
   Reddi Sashank J, 2016, ARXIV160306159
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402039
DA 2019-06-15
ER

PT S
AU Levine, N
   Zahavy, T
   Mankowitz, DJ
   Tamar, A
   Mannor, S
AF Levine, Nir
   Zahavy, Tom
   Mankowitz, Daniel J.
   Tamar, Aviv
   Mannor, Shie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Shallow Updates for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ITERATION; GAME
AB Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach - the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.
C1 [Levine, Nir; Zahavy, Tom; Mankowitz, Daniel J.; Mannor, Shie] Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel.
   [Tamar, Aviv] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Levine, N (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel.
EM levin.nirl@gmail.com; tomzahavy@campus.technion.ac.il;
   danielm@tx.technion.ac.il; avivt@berkeley.edu; shie@ee.technion.ac.il
RI Jeong, Yongwook/N-7413-2016
FU European Community's Seventh Framework Program (FP7/2007-2013) [306638];
   Siemens; Viterbi Scholarship, Technion
FX This research was supported by the European Community's Seventh
   Framework Program (FP7/2007-2013) under grant agreement 306638 (SUPREL).
   A. Tamar is supported in part by Siemens and the Viterbi Scholarship,
   Technion.
CR Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bertsekas D. P., 2008, APPROXIMATE DYNAMIC
   Blundell Charles, 2016, STAT, V1050, P14
   Box G. E, 2011, BAYESIAN INFERENCE S
   Crites RH, 1996, ADV NEUR IN, V8, P1017
   Desai VV, 2012, OPER RES, V60, P655, DOI 10.1287/opre.1120.1044
   Dinh L., 2017, ARXIV170304933
   Donahue J., 2013, DECAF DEEP CONVOLUTI, P647
   Ernst D, 2005, J MACH LEARN RES, V6, P503
   Farahmand A. M., 2009, ADV NEURAL INFORM PR, P441
   Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049
   Hester Todd, 2017, ARXIV170403732
   Hinton G, 2012, NEURAL NETWORKS MACH
   Hoffer Elad, 2017, ARXIV170508741
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Keskar N. S., 2016, ARXIV160904836
   Kingma D. P., 2014, ARXIV14126980
   Kirkpatrick James, 2017, P NATL ACAD SCI USA
   Kolter J Zico, 2009, P 26 ANN INT C MACH
   Lagoudakis M., 2003, J MACHINE LEARNING R, V4, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107
   Liang TY, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON APPLIED SYSTEM INNOVATION (ICASI)
   Lin L.-J., 1993, REINFORCEMENT LEARNI
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317
   Scherrer B, 2015, J MACH LEARN RES, V16, P1629
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Song Zhao, 2016, ADV NEURAL INFORM PR, P4224
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tessler Chen, 2017, P NAT C ART INT AAAI
   Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874
   Van Hasselt Hado, 2016, P NAT C ART INT AAAI
   Wang Z., 2016, SER P MACHINE LEARNI, P1995
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Zahavy Tom, 2016, INT C MACH LEARN, P1899
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403020
DA 2019-06-15
ER

PT S
AU Levine, N
   Crammer, K
   Mannor, S
AF Levine, Nir
   Crammer, Koby
   Mannor, Shie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Rotting Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Exploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.
C1 [Levine, Nir; Crammer, Koby; Mannor, Shie] Technion, Elect Engn Dept, IL-32000 Haifa, Israel.
RP Levine, N (reprint author), Technion, Elect Engn Dept, IL-32000 Haifa, Israel.
EM levin.nir1@gmail.com; koby@ee.technion.ac.il; shie@ee.technion.ac.il
RI Jeong, Yongwook/N-7413-2016
FU European Research Council under the European Union's Seventh Framework
   Program (FP/2007-2013)/ERC [306638]
FX The research leading to these results has received funding from the
   European Research Council under the European Union's Seventh Framework
   Program (FP/2007-2013)/ERC Grant Agreement n. 306638
CR Agarwal D., 2009, P 18 INT C WORLD WID, P21, DOI DOI 10.1145/1526709.1526713
   Agrawal S, 2013, P 16 INT C ART INT S, P99
   Arora R., 2012, ARXIV12066400
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Awerbuch  B., 2004, P 36 ANN ACM S THEOR, P45
   Besbes O., 2014, ADV NEURAL INFORM PR, P199
   Bouneffouf D, 2016, NEUROCOMPUTING, V205, P16, DOI 10.1016/j.neucom.2016.02.052
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chakrabarti D., 2009, ADV NEURAL INFORM PR, V20, P273
   Du S, 2013, IEEE T CIRC SYST VID, V23, P322, DOI 10.1109/TCSVT.2012.2203741
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Garivier Aurelien, 2008, ARXIV08053415
   GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148
   Gopalan A., 2014, P 31 INT C MACH LEAR, P100
   Hazan E, 2011, J MACH LEARN RES, V12, P1287
   Heidari H., 1972, DYNAMIC ALLOCATION I
   Jones D. M., 1972, DYNAMIC ALLOCATION I
   Kaspi H, 1998, ANN APPL PROBAB, V8, P1270
   Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   Kocsis L., 2006, 2 PASCAL CHAL WORKSH, P784
   Komiyama J, 2014, LECT NOTES COMPUT SC, V8877, P460, DOI 10.1007/978-3-319-13129-0_40
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Maillard Odalric- Ambrym, 2011, COLT, P497
   MANDELBAUM A, 1987, ANN PROBAB, V15, P1527, DOI 10.1214/aop/1176991992
   Pandey S, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P216
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Slivkins A., 2008, COLT, P343
   Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Tran-Thanh L, 2012, FRONT ARTIF INTEL AP, V242, P768, DOI 10.3233/978-1-61499-098-7-768
   WHITTLE P, 1981, ANN PROBAB, V9, P284, DOI 10.1214/aop/1176994469
   Whittle P., 1988, J APPL PROB A, P287, DOI [10.2307/3214163, DOI 10.2307/3214163]
   Yu J. Y., 2009, P 26 ANN INT C MACH, P1177
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403014
DA 2019-06-15
ER

PT S
AU Levy, KY
AF Levy, Kfir Y.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online to Offline Conversions, Universality and Adaptive Minibatch Sizes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present an approach towards convex optimization that relies on a novel scheme which converts adaptive online algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on the harmonic sum of the queried gradients. We further show that our methods implicitly adapt to the objective's structure: in the smooth case fast convergence rates are ensured without any prior knowledge of the smoothness parameter, while still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD (stochastic GD), where minibathces are chosen adaptively depending on the magnitude of the gradients. Thus providing a principled approach towards choosing minibatch sizes.
C1 [Levy, Kfir Y.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Levy, KY (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM yehuda.levy@inf.ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU ETH Zurich Postdoctoral Fellowship; Marie Curie Actions for People
   COFUND program
FX This work was supported by the ETH Zurich Postdoctoral Fellowship and
   Marie Curie Actions for People COFUND program.
CR Bullen Peter S, 2013, MEANTHEIR INEQUALI, V31
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658
   Cotter A., 2011, ADV NEURAL INFORM PR, P1647
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hazan E., 2012, P 29 INT C MACH LEAR, P807
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Hazan Elad, 2015, ADV NEURAL INFORM PR, P1594
   Jain P., 2016, ARXIV161003774
   Juditsky Anatoli B, 2008, ARXIV08090813
   Kakade Sham, 2010, LECT NOTES MULTIVARI
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Levy Kfir Y, 2016, ARXIV161104831
   Li Mu, 2014, P 20 ACM SIGKDD INT, P661, DOI DOI 10.1145/2623330.2623612
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   McMahan H. B, 2010, P 23 C LEARN THEOR, P244
   Nemirovskii A, 1983, PROBLEM COMPLEXITY M
   NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Nesterov Yurii, 1984, MATEKON, V29, P519
   Shalev-Shwartz S., 2013, ADV NEURAL INF PROCE, P378
   Takac M., 2015, ARXIV150708322
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Wright S., 1999, SPRINGER SCI, V35, P67
   Zeiler M.D., 2012, ARXIV12125701
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401063
DA 2019-06-15
ER

PT S
AU Li, C
   Wong, FMF
   Liu, ZM
   Kanade, V
AF Li, Cheng
   Wong, Felix M. F.
   Liu, Zhenming
   Kanade, Varun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI From which world is your graph?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NETWORKS; MODELS
AB Discovering statistical structure from links is a fundamental problem in the analysis of social networks. Choosing a misspecified model, or equivalently, an incorrect inference algorithm will result in an invalid analysis or even falsely uncover patterns that are in fact artifacts of the model. This work focuses on unifying two of the most widely used link-formation models: the stochastic blockmodel (SBM) and the small world (or latent space) model (SWM). Integrating techniques from kernel learning, spectral graph theory, and nonlinear dimensionality reduction, we develop the first statistically sound polynomial-time algorithm to discover latent patterns in sparse graphs for both models. When the network comes from an SBM, the algorithm outputs a block structure. When it is from an SWM, the algorithm outputs estimates of each node's latent position.
C1 [Li, Cheng; Liu, Zhenming] Coll William & Mary, Williamsburg, VA 23187 USA.
   [Kanade, Varun] Univ Oxford, Oxford, England.
   [Wong, Felix M. F.] Google, Menlo Pk, CA USA.
RP Li, C (reprint author), Coll William & Mary, Williamsburg, VA 23187 USA.
RI Jeong, Yongwook/N-7413-2016
CR Abbe Emmanuel, 2016, OMMUNITY DETECTION S
   Abbe Emmanuel, 2015, P 56 ANN IEEE FDN CO, P18
   Abraham I, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1853
   Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   Airoldi EM, 2013, ADV NEURAL INFORM PR, V26, P692
   BADOIU  M., 2005, P 37 ANN ACM S THEOR, P225
   Barbera P, 2015, PSYCHOL SCI, V26, P1531, DOI 10.1177/0956797615594620
   Barbera Pablo, 2012, BIRDS SAME FEATHER T
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Borg I., 2005, MODERN MULTIDIMENSIO
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Choi D, 2014, ANN STAT, V42, P29, DOI 10.1214/13-AOS1173
   Clinton J, 2004, AM POLIT SCI REV, V98, P355, DOI 10.1017/S0003055404001194
   DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001
   Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269
   Gerrish S., 2012, P NIPS
   Gerrish S., 2011, P ICML
   Grimmer J., 2013, POLITICAL ANAL
   Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Holme P, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.056108
   Indyk P., 2004, HDB DISCRETE COMPUTA, V273, P177
   Kanade V, 2016, IEEE T INFORM THEORY, V62, P5906, DOI 10.1109/TIT.2016.2516564
   Kleinberg J., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P163, DOI 10.1145/335305.335325
   Konig H., 1986, OPERATOR THEORY ADV
   Lang K. J., 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Laver Michael, 2003, AM POLITICAL SCI REV, V97
   Leskovec J., 2008, P 17 INT C WORLD WID, P695, DOI DOI 10.1145/1367497.1367591
   Li Cheng, 2017, WHICH WORLD IS YOUR
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mihail M., 2002, Randomization and Approximation Techniques in Computer Science. 6th International Workshop, RANDOM 2002. Proceedings (Lecture Notes in Computer Science Vol.2483), P254
   Mossel E, 2013, ARXIV13114115
   Mossel E, 2015, PROBAB THEORY REL, V162, P431, DOI 10.1007/s00440-014-0576-6
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.026113
   Newman MEJ, 2002, P NATL ACAD SCI USA, V99, P2566, DOI 10.1073/pnas.012582999
   POOLE KT, 1985, AM J POLIT SCI, V29, P357, DOI 10.2307/2111172
   Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106
   ROHE K., 2013, ADV NEURAL INFORM PR, V26, P3120
   Rohe K, 2016, P NATL ACAD SCI USA, V113, P12679, DOI 10.1073/pnas.1525793113
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Rosasco L, 2010, J MACH LEARN RES, V11, P905
   Sandon C, 2015, ARXIV151209080
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Silva VD, 2003, ADV NEURAL INFORM PR, P705
   Sofia C., 2013, NONPARAMETRIC GRAPHO
   Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112
   Tauberer Joshua, 2012, LAW VIA THE INTERNET
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wong FMF, 2016, IEEE T KNOWL DATA EN, V28, P2158, DOI 10.1109/TKDE.2016.2553667
   Xu Jiaming, 2014, P 27 C LEARN THEOR J, P903
   Yun Se- Young, 2016, ADV NEUR INF PROC SY, P965
   Zhao YP, 2012, ANN STAT, V40, P2266, DOI 10.1214/12-AOS1036
   Zhou T, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046115
NR 56
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401049
DA 2019-06-15
ER

PT S
AU Li, C
   Jegelka, S
   Sra, S
AF Li, Chengtao
   Jegelka, Stefanie
   Sra, Suvrit
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Polynomial time algorithms for dual volume sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study dual volume sampling, a method for selecting k columns from an n x m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013), who showed it to be a promising method for column subset selection and its multiple applications. However, its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter, we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the "Strong Rayleigh" property. This result has numerous consequences, including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.
C1 [Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit] MIT, Cambridge, MA 02139 USA.
RP Li, C (reprint author), MIT, Cambridge, MA 02139 USA.
EM ctli@mit.edu; stefje@csail.mit.edu; suvrit@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER award [1553284]; NSF [IIS-1409802]; DARPA [N66001-17-1-4039];
   DARPA FunLoL grant [W911NF-16-1-0551]; Siebel Scholar Fellowship
FX This research was supported by NSF CAREER award 1553284, NSF grant
   IIS-1409802, DARPA grant N66001-17-1-4039, DARPA FunLoL grant
   (W911NF-16-1-0551) and a Siebel Scholar Fellowship. The views, opinions,
   and/or findings contained in this article are those of the author and
   should not be interpreted as representing the official views or
   policies, either expressed or implied, of the Defense Advanced Research
   Projects Agency or the Department of Defense.
CR Anari N., 2014, ARXIV14121143
   Anari N., 2016, COLT, P23
   Anari N., 2015, IEEE S FDN COMP SCI
   Arioli M., 2015, SIAM J SCI COMPUT
   Arthur D., 2007, P 18 ANN ACM SIAM S
   Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287
   Borcea J, 2008, DUKE MATH J, V143, P205, DOI 10.1215/00127094-2008-018
   Borcea J, 2009, J AM MATH SOC, V22, P521
   Borodin A., 2009, ARXIV09111153
   Boutsidis C., 2011, ARXIV11102897
   Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X
   Boutsidis C, 2013, IEEE T INFORM THEORY, V59, P6099, DOI 10.1109/TIT.2013.2255021
   Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968
   Chen SH, 2015, IEEE T SIGNAL PROCES, V63, P6510, DOI 10.1109/TSP.2015.2469645
   Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018
   Derezinski M., 2017, ADV NEURAL INFORM PR
   Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38
   Elkin M., 2008, SIAM J COMPUTING
   Feder T., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P26, DOI 10.1145/129712.129716
   Fedorov V., 1969, PREPRINT
   Fedorov V. V., 1972, THEORY OPTIMAL EXPT
   Frieze A, 2014, SIAM J COMPUT, V43, P497, DOI 10.1137/120890971
   Gharan SO, 2011, ANN IEEE SYMP FOUND, P550, DOI 10.1109/FOCS.2011.80
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095
   Kulesza A, 2012, FDN TRENDS MACHINE L, V5
   Li C., 2016, ADV NEURAL INFORM PR
   Li C., 2016, ICML, P1766
   Lyons R, 2003, PUBL MATH IHES, P167
   Ma P., 2015, J MACHINE LEARNING R
   Macchi O., 1975, ADV APPL PROBABILITY, V7
   Magen A, 2008, LECT NOTES COMPUT SC, V5171, P523
   Miller A. J., 1994, J ROYAL STAT SOC
   MORRIS MD, 1995, J STAT PLAN INFER, V43, P381, DOI 10.1016/0378-3758(94)00035-T
   NGUYEN NK, 1992, COMPUT STAT DATA AN, V14, P489, DOI 10.1016/0167-9473(92)90064-M
   Pemantle R, 2000, J MATH PHYS, V41, P1371, DOI 10.1063/1.533200
   Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345
   Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109
   Spielman D. A., 2004, STOC
   Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029
   Tsitsvero M, 2016, IEEE T SIGNAL PROCES, V64, P4845, DOI 10.1109/TSP.2016.2573748
   Wagner DG, 2011, B AM MATH SOC, V48, P53, DOI 10.1090/S0273-0979-2010-01321-5
   Wang Y, 2016, ARXIV E PRINTS
   Zhao YB, 2016, IEEE DECIS CONTR P, P1859, DOI 10.1109/CDC.2016.7798535
   Zhu R., 2015, ARXIV150905111
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405012
DA 2019-06-15
ER

PT S
AU Li, CX
   Xu, K
   Zhu, J
   Zhang, B
AF Li, Chongxuan
   Xu, Kun
   Zhu, Jun
   Zhang, Bo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Triple Generative Adversarial Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players-a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.
C1 [Li, Chongxuan; Xu, Kun; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech, TNList Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech, TNList Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China.
EM licx14@mails.tsinghua.edu.cn; xu-k16@mails.tsinghua.edu.cn;
   dcszj@mail.tsinghua.edu.cn; dcszb@mail.tsinghua.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant
   of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support
   Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA
   NVAIL Program; Project from Siemens
FX The work is supported by the National NSF of China (Nos. 61620106010,
   61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No.
   2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua
   Tiangong Institute for Intelligent Computing, the NVIDIA NVAIL Program
   and a Project from Siemens.
CR Burt Peter, 1983, IEEE T COMMUNICATION
   Chen Xi, 2016, NIPS
   Denton E. L., 2015, NIPS
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Dziugaite G. K., 2015, ARXIV150503906
   Goodfellow I., 2014, NIPS
   Ioffe S., 2015, ARXIV150203167
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma D. P., 2014, NIPS
   Krizhevsky Alex, 2009, CITESEER
   Laine S, 2016, ARXIV161002242
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Chongxuan, 2016, ARXIV161107119
   Li Y., 2015, ICML
   Maaloe Lars, 2016, ARXIV160205473
   Miyato  T., 2015, ARXIV150700677
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Odena A., 2016, ARXIV161009585
   Odena A., 2016, ARXIV160601583
   Radford A., 2015, ARXIV151106434
   Rasmus A., 2015, NIPS
   Rezende D. J, 2014, ARXIV14014082
   Salimans T., 2016, NIPS
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Theano Development Team, 2016, ABS160502688 ARXIV T
   Theis L., 2015, ARXIV151101844
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Yang Jimei, 2015, NIPS
NR 30
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404016
DA 2019-06-15
ER

PT S
AU Li, CJ
   Wang, MD
   Liu, H
   Zhang, T
AF Li, Chris Junchi
   Wang, Mengdi
   Liu, Han
   Zhang, Tong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Diffusion Approximations for Online Principal Component Estimation and
   Global Convergence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID EIGENVALUE; POWER; PCA
AB In this paper, we propose to adopt the diffusion approximation tools to study the dynamics of Oja's iteration which is an online stochastic gradient descent method for the principal component analysis. Oja's iteration maintains a running estimate of the true principal component from streaming data and enjoys less temporal and spatial complexities. We show that the Oja's iteration for the top eigenvector generates a continuous-state discrete-time Markov chain over the unit sphere. We characterize the Oja's iteration in three phases using diffusion approximation and weak convergence tools. Our three-phase analysis further provides a finite-sample error bound for the running estimate, which matches the minimax information lower bound for principal component analysis under the additional assumption of bounded samples.
C1 [Li, Chris Junchi; Wang, Mengdi; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
   [Zhang, Tong] Tencent AI Lab, Shennan Ave, Shenzhen 518057, Guangdong, Peoples R China.
RP Li, CJ (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
EM junchil@princeton.edu; mengdiw@princeton.edu; hanliu@princeton.edu;
   tongzhang@tongzhang-ml.org
RI Jeong, Yongwook/N-7413-2016
CR Aldous D., 1989, PROBABILITY APPROXIM, V77
   Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664
   Anandkumar A., 2016, ARXIV160205908
   Arora R., 2013, ADV NEURAL INFORM PR, V26, P1815
   Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308
   Balsubramani A., 2013, ADV NEURAL INFORM PR, V26, P3174
   Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   Darken C., 1991, NIPS, P1009
   DE SA C, 2015, P 32 INT C MACH LEAR, P2332
   Ethier S. N., 2005, MARKOV PROCESSES CHA, V282
   Garber D., 2015, ARXIV150905647
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Helmke U., 1994, OPTIMIZATION DYNAMIC
   Jain Prateek, 2016, ARXIV160206929
   Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121
   Krasulina TP, 1969, USSR COMP MATH MATH, V9, P189
   KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066
   Lee J. D., 2016, C LEARN THEOR, V49, P1246
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Li C. J., 2016, ARXIV160305305
   Li Q., 2015, ARXIV151106251
   LIU H., 2014, ARXIV14085352
   Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097
   Mandt Stephan, 2016, ARXIV160202666
   Mitliagkas I., 2013, P ADV NEUR INF PROC, V26, P2886
   Musco C., 2015, ARXIV150405477
   Nadler B, 2008, ANN STAT, V36, P2791, DOI 10.1214/08-AOS618
   OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Oksendal B., 2003, STOCHASTIC DIFFERENT
   Panageas  I., 2016, ARXIV160500405
   Shamir O., 2015, ARXIV150909002
   Shamir O., 2015, P 32 INT C MACH LEAR, P144
   Shamir Ohad, 2015, ARXIV150708788
   Su WJ, 2016, J MACH LEARN RES, V17
   Sun J., 2015, ARXIV151006096
   Sun  J., 2015, ARXIV151104777
   Sun J., 2016, ARXIV160206664
   Sun J., 2015, ARXIV151103607
   Vu V. Q., 2012, P 15 INT C ART INT S, P1278
   Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400062
DA 2019-06-15
ER

PT S
AU Li, CY
   Liu, H
   Chen, CY
   Pu, YC
   Chen, LQ
   Henao, R
   Carin, L
AF Li, Chunyuan
   Liu, Hao
   Chen, Changyou
   Pu, Yunchen
   Chen, Liqun
   Henao, Ricardo
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI ALICE: Towards Understanding Adversarial Learning for Joint Distribution
   Matching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.
C1 [Li, Chunyuan; Pu, Yunchen; Chen, Liqun; Henao, Ricardo; Carin, Lawrence] Duke Univ, Durham, NC 27708 USA.
   [Liu, Hao] Nanjing Univ, Nanjing, Jiangsu, Peoples R China.
   [Chen, Changyou] Univ Buffalo, Buffalo, NY USA.
RP Li, CY (reprint author), Duke Univ, Durham, NC 27708 USA.
EM cl319@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO; DARPA; ONR; NSF; DOE; NGA
FX We acknowledge Shuyang Dai, Chenyang Tao and Zihang Dai for helpful
   feedback/editing. This research was supported in part by ARO, DARPA,
   DOE, NGA, ONR and NSF.
CR Arjovsky Martin, 2017, ICLR
   Chen Xi, 2016, NIPS
   Donahue J., 2017, ICLR
   Dumoulin V., 2017, ICLR
   Fidler S., 2012, NIPS
   Gan Z., 2017, NIPS
   Goodfellow I., 2014, NIPS
   Isola  Phillip, 2017, CVPR
   Kim Taeksoo, 2017, ICML
   Kingma Diederik P, 2014, ICLR
   Larsen Anders Boesen Lindbo, 2016, ICML
   LI  Chongxuan, 2017, NIPS
   Liu  Z., 2015, ICCV
   Makhzani A., 2015, ARXIV151105644
   Mescheder Lars, 2017, ICML
   Mirza Mehdi, 1784, ARXIV14111784
   Pu Y., 2016, NIPS
   Pu Y., 2017, NIPS
   Reed S., 2016, ICML
   Salimans T., 2016, NIPS
   Vincent P., 2008, ICML
   Wang Z., 2004, IEEE T IMAGE PROCESS
   Yi Z, 2017, ICCV
   Yu Aron, 2014, CVPR
   Zhao J., 2017, ICLR
   Zhu J.-Y., 2017, ICCV
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405056
DA 2019-06-15
ER

PT S
AU Li, DS
   Chen, C
   Liu, W
   Lu, T
   Gu, N
   Chu, SM
AF Li, Dongsheng
   Chen, Chao
   Liu, Wei
   Lu, Tun
   Gu, Ning
   Chu, Stephen M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Mixture-Rank Matrix Approximation for Collaborative Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Low-rank matrix approximation (LRMA) methods have achieved excellent accuracy among today's collaborative filtering (CF) methods. In existing LRMA methods, the rank of user/item feature matrices is typically fixed, i.e., the same rank is adopted to describe all users/items. However, our studies show that submatrices with different ranks could coexist in the same user-item rating matrix, so that approximations with fixed ranks cannot perfectly describe the internal structures of the rating matrix, therefore leading to inferior recommendation accuracy. In this paper, a mixture-rank matrix approximation (MRMA) method is proposed, in which user-item ratings can be characterized by a mixture of LRMA models with different ranks. Meanwhile, a learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to MRMA. Experimental studies on MovieLens and Netflix datasets demonstrate that MRMA can outperform six state-of-the-art LRMA-based CF methods in terms of recommendation accuracy.
C1 [Li, Dongsheng; Chen, Chao; Chu, Stephen M.] IBM Res, Beijing, Peoples R China.
   [Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China.
   [Lu, Tun; Gu, Ning] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
   [Lu, Tun; Gu, Ning] Fudan Univ, Shanghai Key Lab Data Sci, Shanghai, Peoples R China.
RP Li, DS (reprint author), IBM Res, Beijing, Peoples R China.
EM ldsli@cn.ibm.com; cshchen@cn.ibm.com; wliu@ee.columbia.edu;
   lutun@fudan.edu.cn; ninggu@fudan.edu.cn; schu@cn.ibm.com
RI Jeong, Yongwook/N-7413-2016
FU National Natural Science Foundation of China [61332008]; NSAF [U1630115]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 61332008 and NSAF under Grant No.
   U1630115.
CR BESAG J, 1986, J ROY STAT SOC B MET, V48, P259
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Chen C., 2016, P 25 INT JOINT C ART, P1382
   Chen C, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P303, DOI 10.1145/2766462.2767718
   Dueck D., 2004, PSI200423 U TOR
   Hardt M., 2015, ARXIV150901240
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Koren Y., 2008, P 14 ACM SIGKDD INT, V08, P426, DOI [DOI 10.1145/1401890.1401944, 10.1145/1401890.1401944]
   Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lee J., 2013, INT C MACH LEARN, V28, P82
   Li D., 2016, P INT C MACH LEARN, P295
   Ma H., 2008, P 17 ACM C INF KNOWL, P931, DOI DOI 10.1145/1458082.1458205
   Mackey L. W., 2011, ADV NEURAL INFORM PR, V24, P1134
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.1145/1390156.1390267
   Schmidt M, 2007, LECT NOTES ARTIF INT, V4701, P286
   Toh KC, 2010, PAC J OPTIM, V6, P615
   Yuan Ting, 2014, P 28 AAAI C ART INT, P222
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400046
DA 2019-06-15
ER

PT S
AU Li, H
   De, S
   Xu, Z
   Studer, C
   Samet, H
   Goldstein, T
AF Li, Hao
   De, Soham
   Xu, Zheng
   Studer, Christoph
   Samet, Hanan
   Goldstein, Tom
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Training Quantized Nets: A Deeper Understanding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.
C1 [Li, Hao; De, Soham; Xu, Zheng; Samet, Hanan; Goldstein, Tom] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Studer, Christoph] Cornell Univ, Sch Elect & Comp Engn, Ithaca, NY 14853 USA.
RP De, S (reprint author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM haoli@cs.umd.edu; sohamde@cs.umd.edu; xuzh@cs.umd.edu;
   studer@cornell.edu; hjs@cs.umd.edu; tomg@cs.umd.edu
RI Jeong, Yongwook/N-7413-2016
FU US National Science Foundation (NSF) [CCF-1535902]; US Office of Naval
   Research [N00014-17-1-2078]; Sloan Foundation; Xilinx, Inc.; US NSF
   [ECCS-1408006, CCF-1535897, CAREER CCF-1652065, IIS-13-20791]
FX T. Goldstein was supported in part by the US National Science Foundation
   (NSF) under grant CCF-1535902, by the US Office of Naval Research under
   grant N00014-17-1-2078, and by the Sloan Foundation. C. Studer was
   supported in part by Xilinx, Inc. and by the US NSF under grants
   ECCS-1408006, CCF-1535897, and CAREER CCF-1652065. H. Samet was
   supported in part by the US NSF under grant IIS-13-20791.
CR Anwar S., 2015, ICASSP
   Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101
   Cheng Z., 2015, ARXIV150303562
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Courbariaux M, 2016, ARXIV160202830
   David J., 2015, NIPS
   De Soham, 2016, ARXIV161005792
   Goyal Priya, 2017, ARXIV170602677
   Gupta S., 2015, ICML
   He K., 2016, CVPR
   HOHFELD M, 1992, NEUROCOMPUTING, V4, P291, DOI 10.1016/0925-2312(92)90014-G
   Hubara I., 2016, ARXIV160907061
   Hwang Kyuyeon, 2014, IEEE WORKSH SIGN PRO
   Ioffe S., 2015, BATCH NORMALIZATION
   Kim M., 2015, ICML WORKSH RES EFF
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lan GH, 2012, MATH PROGRAM, V134, P425, DOI 10.1007/s10107-011-0442-6
   Lax PD, 2007, LINEAR ALGEBRA ITS A
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Li  F., 2016, ARXIV160504711
   Lin D., 2016, ICML
   Lin Z., 2016, ICLR
   MARCHESI M, 1993, IEEE T NEURAL NETWOR, V4, P53, DOI 10.1109/72.182695
   Martens J., 2015, INT C MACH LEARN, P2408
   Miyashita  D., 2016, ARXIV160301025
   Rastegari M., 2016, ECCV
   Russakovsky Olga, 2015, IJCV
   Simonyan Karen, 2015, ICLR
   Soudry D., 2014, NIPS
   Zagoruyko S, 2016, ARXIV160507146
   Zhou  A., 2017, ICLR
   Zhou S., 2016, ARXIV160606160
   Zhu  C., 2017, ICLR
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405086
DA 2019-06-15
ER

PT S
AU Li, P
   Milenkovic, O
AF Li, Pan
   Milenkovic, Olgica
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inhomogeneous Hypergraph Clustering with Applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ORGANIZATION
AB Hypergraph partitioning is an important problem in machine learning, computer vision and network analytics. A widely used method for hypergraph partitioning relies on minimizing a normalized sum of the costs of partitioning hyperedges across clusters. Algorithmic solutions based on this approach assume that different partitions of a hyperedge incur the same cost. However, this assumption fails to leverage the fact that different subsets of vertices within the same hyperedge may have different structural importance. We hence propose a new hypergraph clustering technique, termed inhomogeneous hypergraph partitioning, which assigns different costs to different hyperedge cuts. We prove that inhomogeneous partitioning produces a quadratic approximation to the optimal solution if the inhomogeneous costs satisfy submodularity constraints. Moreover, we demonstrate that inhomogenous partitioning offers significant performance improvements in applications such as structure learning of rankings, subspace segmentation and motif clustering.
C1 [Li, Pan; Milenkovic, Olgica] UIUC, Dept ECE, Champaign, IL 61820 USA.
RP Li, P (reprint author), UIUC, Dept ECE, Champaign, IL 61820 USA.
EM panli2@illinois.edu; milenkov@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF 1527636]
FX The authors gratefully acknowledge many useful suggestions by the
   reviewers. They are also indebted to the reviewers for providing many
   additional and relevant references. This work was supported in part by
   the NSF grant CCF 1527636.
CR Agarwal S, 2005, PROC CVPR IEEE, P838
   Agarwal S, 2006, P 23 INT C MACH LEAR, P17
   Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513
   Allesina S, 2005, OIKOS, V110, P164, DOI 10.1111/j.0030-1299.2005.13082.x
   Awasthi P., 2014, ADV NEURAL INFORM PR, P2609
   Bansal N, 2002, ANN IEEE SYMP FOUND, P238, DOI 10.1109/SFCS.2002.1181947
   Benson AR, 2016, SCIENCE, V353, P163, DOI 10.1126/science.aad9029
   Benson Austin R, 2015, Proc SIAM Int Conf Data Min, V2015, P118
   Bu Y., 2016, ARXIV160702653
   Chung F. R., 2007, P ICCM, V2, P378
   Devanur N. R., 2013, ARXIV13044948
   Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732
   GHOSHDASTIDAR D., 2014, ADV NEURAL INFORM PR, P397
   Ghoshdastidar D., 2015, ARXIV150501582
   Gormley IC, 2007, LECT NOTES COMPUT SC, V4503, P90
   Hein M., 2013, ADV NEURAL INFORM PR, P2427
   Huang J, 2012, ELECTRON J STAT, V6, P199, DOI 10.1214/12-EJS670
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jeong H, 2000, NATURE, V407, P651, DOI 10.1038/35036627
   Jiao JT, 2017, IEEE T INFORM THEORY, V63, P6774, DOI 10.1109/TIT.2017.2733537
   Kim S., 2011, ADV NEURAL INFORM PR, P1530
   Knyazev A. V., 2017, ARXIV170101394
   Kunegis J, 2010, P SIAM INT C DAT MIN, P559, DOI DOI 10.1137/1.9781611972801.49
   Leordeanu M, 2012, AISTATS, P676
   Li GY, 2013, NUMER LINEAR ALGEBR, V20, P1001, DOI 10.1002/nla.1877
   Li P., 2017, IEEE C COMP COMM INF, P109
   Liu H., 2010, ADV NEURAL INFORM PR, V23, P1414
   Louis A., 2015, P 47 ANN ACM S THEOR, P713, DOI DOI 10.1145/2746539.2746555
   Meek C., 2014, ADV NEURAL INFORM PR, P631
   Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Pelillo M., 2009, ADV NEURAL INFORM PR, V22, P1571
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tsourakakis  C., 2016, ARXIV160606235
   Yin HB, 2017, AER ADV ENG RES, V112, P555
   Zhang  C., 2017, INT C MACH LEARN ICM, P4026
   Zhou D., 2007, P ADV NEUR INF PROC, P1601
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402035
DA 2019-06-15
ER

PT S
AU Li, P
   Slawski, M
AF Li, Ping
   Slawski, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Simple Strategies for Recovering Inner Products from Coarsely Quantized
   Random Projections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID JOHNSON-LINDENSTRAUSS
AB Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning, we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection, the loss in accuracy tends to range from "negligible" to "moderate". One implication is that in most scenarios of practical interest, there is no need for a sophisticated recovery approach like maximum likelihood estimation as considered in previous work on the subject. What we propose herein also yields considerable improvements in terms of accuracy over the Hamming distance-based approach in Li et al. (ICML 2014) which is comparable in terms of simplicity.
C1 [Li, Ping] Baidu Res, Sunnyvale, CA 94089 USA.
   [Li, Ping] Rutgers State Univ, New Brunswick, NJ 08901 USA.
   [Slawski, Martin] George Mason Univ, Dept Stat, Fairfax, VA 22030 USA.
RP Li, P (reprint author), Baidu Res, Sunnyvale, CA 94089 USA.; Li, P (reprint author), Rutgers State Univ, New Brunswick, NJ 08901 USA.
EM pingli98@gmail.com; mslawsk3@gmu.edu
RI Jeong, Yongwook/N-7413-2016
FU  [NSF-Bigdata-1419210];  [NSF-III-1360971]
FX The work was partially supported by NSF-Bigdata-1419210,
   NSF-III-1360971. Ping Li also thanks Michael Mitzenmacher for helpful
   discussions.
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P557
   Ailon N, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483701
   Anderson T. W, 2003, INTRO MULTIVARIATE S
   Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245
   Boufounos P., 2008, INFORM SCI SYSTEMS
   Boutsidis C., 2010, ADV NEURAL INFORM PR, V23, P298
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   Datar M., 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Gersho A., 1991, VECTOR QUANTIZATION
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jacques L, 2015, IEEE T INFORM THEORY, V61, P5012, DOI 10.1109/TIT.2015.2453355
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kenthapadi K., 2013, J PRIVACY CONFIDENTI, V5
   KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622
   Krahmer F, 2011, SIAM J MATH ANAL, V43, P1269, DOI 10.1137/100810447
   Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710
   Li M, 2012, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2012.6343406
   Li P, 2014, P ICML 14, P676
   Li P., 2016, ADV NEURAL INFORM PR, P2756
   Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46
   Madigan D., 2003, P 9 ACM SIGKDD INT C, P517
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Maillard O., 2009, ADV NEURAL INFORM PR, V22, P1213
   Matousek J, 2008, RANDOM STRUCT ALGOR, V33, P142, DOI 10.1002/rsa.20218
   Rane S., 2013, SPIE OPTICAL ENG APP
   Rane S, 2013, IEEE SIGNAL PROC MAG, V30, P18, DOI 10.1109/MSP.2012.2230221
   Vempala S.S., 2005, RANDOM PROJECTION ME
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404062
DA 2019-06-15
ER

PT S
AU Li, Q
   Sun, ZA
   He, R
   Tan, T
AF Li, Qi
   Sun, Zhenan
   He, Ran
   Tan, Tieniu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Supervised Discrete Hashing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.
C1 [Li, Qi; Sun, Zhenan; He, Ran; Tan, Tieniu] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp,Natl Lab Patte, Beijing, Peoples R China.
RP Li, Q (reprint author), Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp,Natl Lab Patte, Beijing, Peoples R China.
EM qli@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn; rhe@nlpr.ia.ac.cn;
   tnt@nlpr.ia.ac.cn
FU National Key Research and Development Program of China [2016YFB1001000];
   Natural Science Foundation of China [61622310]
FX This work was partially supported by the National Key Research and
   Development Program of China (Grant No. 2016YFB1001000) and the Natural
   Science Foundation of China (Grant No. 61622310).
CR Cao Y, 2016, AAAI, P3457
   Chatfield K., 2014, BMVC
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   J Ji, 2012, P ADV NEUR INF PROC, P108
   Kulis B., 2009, ADV NEURAL INFORM PR, P1042
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li W.-J., 2016, P INT JOINT C ART IN, P1711
   Lin G., 2014, P IEEE C COMP VIS PA, P1963
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Mu YD, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P539
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Wang J., 2010, P 27 INT C MACH LEAR, P1127
   Wang J. F., 2013, P 21 ACM INT C MULT, P133
   Wang X, 2016, P AS C COMP VIS, P70
   Weiss Y., 2009, ADV NEURAL INFORM PR, P1753
   Xia R., 2014, P AAAI C ART INT, P2156
   Yang H. F., 2017, IEEE T PATTERN ANAL, P1
   Yao T, 2016, P INT JOINT C ART IN, P3931
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu H., 2016, AAAI, P2415
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402052
DA 2019-06-15
ER

PT S
AU Li, Q
   Chen, W
   Sun, XM
   Zhang, JL
AF Li, Qiang
   Chen, Wei
   Sun, Xiaoming
   Zhang, Jialin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Influence Maximization with epsilon-Almost Submodular Threshold
   Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Influence maximization is the problem of selecting k nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them epsilon-almost submodular). We first show a strong hardness result: there is no 1/n(gamma/c) approximation for influence maximization (unless P = NP) for all networks with up to n(gamma) epsilon-almost submodular nodes, where gamma is in (0,1) and c is a parameter depending on epsilon. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide (1 - epsilon)(l) (1 - 1/e) approximation algorithms when the number of epsilon-almost submodular nodes is f. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms.
C1 [Li, Qiang; Sun, Xiaoming; Zhang, Jialin] Chinese Acad Sci, Inst Comp Technol, CAS Key Lab Network Data Sci & Technol, Beijing, Peoples R China.
   [Li, Qiang; Sun, Xiaoming; Zhang, Jialin] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Chen, Wei] Microsoft Res, Beijing, Peoples R China.
RP Li, Q (reprint author), Chinese Acad Sci, Inst Comp Technol, CAS Key Lab Network Data Sci & Technol, Beijing, Peoples R China.; Li, Q (reprint author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM liqiang01@ict.ac.cn; weic@microsoft.com; sunxiaoming@ict.ac.cn;
   zhangjialin@ict.ac.cn
FU National Natural Science Foundation of China [61433014, 61502449,
   61602440]; 973 Program of China [2016YFB1000201]
FX This work was supported in part by the National Natural Science
   Foundation of China Grant 61433014, 61502449, 61602440, the 973 Program
   of China Grants No. 2016YFB1000201.
CR Aslay C, 2015, PROC VLDB ENDOW, V8, P814, DOI 10.14778/2752939.2752950
   Backstrom L., 2006, ACM SIGKDD, P44, DOI DOI 10.1145/1150402.1150412
   Borgs C, 2014, P 25 ANN ACM SIAM S, P946, DOI DOI 10.1137/1.9781611973402.70
   Cheng W, 2016, LECT NOTES COMPUT SC, V9977, P307, DOI 10.1007/978-3-319-50011-9_24
   Du DZ, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P167
   Ebrahimi Roozbeh, 2015, ITCS 15
   Gao Jie, 2016, ACM C EC COMP
   Ghasemiesfeh Golnaz, 2013, ACM C EL COMM
   Goyal A., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P211, DOI 10.1109/ICDM.2011.132
   Goyal Amit, 2012, SOCIAL NETWORK ANAL, P1, DOI DOI 10.1007/S11760-012-0353-X
   Harathi S, 2007, LECT NOTES COMPUT SC, V4858, P306
   Horel T., 2016, NIPS, V29, P3045
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Lu W, 2015, PROC VLDB ENDOW, V9, P60
   Mossel E, 2007, ACM S THEORY COMPUT, P128
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Nguyen HT, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P695, DOI 10.1145/2882903.2915207
   Ning Chen, 2008, SODA 08
   Subramani MR, 2003, COMMUN ACM, V46, P300, DOI 10.1145/953460.953514
   Tang YZ, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1539, DOI 10.1145/2723372.2723734
   Tang Youze, 2014, SIGMOD 14
   Wang B., 2016, P 30 AAAI INT C ART, P791
   Wei Chen, 2010, KDD 10
   Wei Chen, 2015, ACM C EC COMP
   Wei Chen, 2009, P 15 ACM SIGKDD
   Yang Y., 2016, P C EMP METH NAT LAN, P65
   Zhang P., 2014, P 20 ACM SIGKDD INT, P1306
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403084
DA 2019-06-15
ER

PT S
AU Li, S
   Fu, Y
AF Li, Sheng
   Fu, Yun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Matching on Balanced Nonlinear Representations for Treatment Effects
   Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PROPENSITY SCORE; CAUSAL INFERENCE; BIAS
AB Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.
C1 [Li, Sheng] Adobe Res, San Jose, CA 95110 USA.
   [Fu, Yun] Northeastern Univ, Boston, MA 02115 USA.
RP Li, S (reprint author), Adobe Res, San Jose, CA 95110 USA.
EM sheli@adobe.com; yunfu@ece.neu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF IIS [1651902]; ONR Young Investigator Award [N00014-14-1-0484]; U.S.
   Army Research Office Award [W911NF-17-1-0367]
FX This research is supported in part by the NSF IIS award 1651902, ONR
   Young Investigator Award N00014-14-1-0484, and U.S. Army Research Office
   Award W911NF-17-1-0367.
CR Abadie A, 2006, ECONOMETRICA, V74, P235, DOI 10.1111/j.1468-0262.2006.00655.x
   Agarwal D., 2011, INT C ART INT STAT, P93
   Athey S, 2016, P NATL ACAD SCI USA, V113, P7353, DOI 10.1073/pnas.1510489113
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Brodersen KH, 2015, ANN APPL STAT, V9, P247, DOI 10.1214/14-AOAS788
   Chan D., 2010, P 16 ACM SIGKDD INT, P7, DOI DOI 10.1145/1835804.1835809
   Chang  Y., 2017, P 31 AAAI C ART INT, P1770
   Dehejia RH, 2002, REV ECON STAT, V84, P151, DOI 10.1162/003465302317331982
   Diamond A, 2013, REV ECON STAT, V95, P932, DOI 10.1162/REST_a_00318
   Entner Doris, 2013, P 16 INT C ART INT S, P256
   Galles D., 1998, FOUND SCI, V3, P151, DOI DOI 10.1023/A:1009602825894
   Glass TA, 2013, ANNU REV PUBL HEALTH, V34, P61, DOI 10.1146/annurev-publhealth-031811-124606
   He XF, 2004, ADV NEUR IN, V16, P153
   Heckman JJ, 1998, REV ECON STUD, V65, P261, DOI 10.1111/1467-937X.00044
   Hill D. N., 2015, P 21 ACM SIGKDD INT, P1839, DOI 10.1145/2783258.2788622
   Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162
   Iacus SM, 2012, POLIT ANAL, V20, P1, DOI 10.1093/pan/mpr013
   Imai K, 2014, J R STAT SOC B, V76, P243, DOI 10.1111/rssb.12027
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Jin H, 2008, J AM STAT ASSOC, V103, P101, DOI 10.1198/016214507000000347
   Johansson F. D., 2016, INT C MACH LEARN, V48, P3020
   Jolliffe IT, 2002, PRINCIPAL COMPONENT
   LALONDE RJ, 1986, AM ECON REV, V76, P604
   Lee BK, 2010, STAT MED, V29, P337, DOI 10.1002/sim.3782
   Li  S., 2016, P 25 INT JOINT C ART, P3768
   Liu QS, 2006, IEEE T NEURAL NETWOR, V17, P1081, DOI 10.1109/TNN.2006.875970
   Neyman J. S., 1990, STAT SCI, V5, P465, DOI DOI 10.2307/224538210.1214/SS/1177012031
   Pan S.J., 2008, AAAI, V2, P677
   PANE J. F., 2016, P INT C ED DAT MIN, P207
   Pearl J., 2009, CAUSALITY
   Peikes DN, 2008, AM STAT, V62, P222, DOI 10.1198/000313008X332016
   ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41
   RUBIN DB, 1979, J AM STAT ASSOC, V74, P318, DOI 10.2307/2286330
   RUBIN DB, 1973, BIOMETRICS, V29, P159, DOI 10.2307/2529684
   RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350
   Rubin DB, 2000, J AM STAT ASSOC, V95, P573, DOI 10.2307/2669400
   Shalit U., 2016, ARXIV160603976
   Silva Ricardo, 2014, ADV NEURAL INFORM PR, P298
   Spirtes P, 2010, J MACH LEARN RES, V11, P1643
   Stuart EA, 2010, STAT SCI, V25, P1, DOI 10.1214/09-STS313
   Sun Wei, 2015, P 29 AAAI C ART INT, P297
   Wager S., 2015, ARXIV151004342
   Wang  P., 2015, P 8 ACM INT C WEB SE, P67
   Wang PY, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P133, DOI 10.1145/2740908.2742758
   Zhang K., 2015, P 29 AAAI C ART INT, P3150
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400089
DA 2019-06-15
ER

PT S
AU Li, XG
   Yang, LF
   Ge, JS
   Haupt, J
   Zhang, T
   Zhao, T
AF Li, Xingguo
   Yang, Lin F.
   Ge, Jason
   Haupt, Jarvis
   Zhang, Tong
   Zhao, Tuo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex
   Sparse Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MULTISTAGE CONVEX RELAXATION; GENERALIZED LINEAR-MODELS; VARIABLE
   SELECTION; LASSO; RATES
AB We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.
C1 [Li, Xingguo; Haupt, Jarvis] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Yang, Lin F.; Ge, Jason] Princeton Univ, Princeton, NJ 08544 USA.
   [Zhang, Tong] Tencent Lab, Bellevue, WA USA.
   [Li, Xingguo; Zhao, Tuo] Georgia Tech, Atlanta, GA 30332 USA.
RP Li, XG (reprint author), Univ Minnesota, Minneapolis, MN 55455 USA.; Li, XG; Zhao, T (reprint author), Georgia Tech, Atlanta, GA 30332 USA.
EM lixx1661@umn.edu; tuo.zhao@isye.gatech.edu
FU DARPA [YFA N66001-14-1-4047]; NSF [IIS-1447639]; Doctoral Dissertation
   Fellowship from University of Minnesota
FX The authors acknowledge support from DARPA YFA N66001-14-1-4047, NSF
   Grant IIS-1447639, and Doctoral Dissertation Fellowship from University
   of Minnesota.
CR ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cristiano A, 2015, P HIPEAC WAPCO AMST
   Eloyan A, 2012, FRONT SYST NEUROSCI, V6, DOI 10.3389/fnsys.2012.00061
   Fan J., 2015, ARXIV150701037
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Guyon I, 2005, ADV NEURAL INFORM PR, V17, P545
   Lee JD, 2014, SIAM J OPTIMIZ, V24, P1420, DOI 10.1137/130921428
   Li X., 2016, P INT C MACH LEARN, V48, P917
   Li Xingguo, 2015, PICASSO PACKAGE NONC
   Li Xingguo, 2016, ARXIV160507950
   Loh Po-Ling, 2015, J MACHINE LEARNING R
   LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025
   Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058
   MCCULLAGH P, 1984, EUR J OPER RES, V16, P285, DOI 10.1016/0377-2217(84)90282-0
   Neale BM, 2012, NATURE, V485, P242, DOI 10.1038/nature11011
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Ning Yang, 2014, ARXIV14122295
   Pfanzagl J., 1994, PARAMETRIC STAT THEO
   Raginsky M, 2010, IEEE T SIGNAL PROCES, V58, P3990, DOI 10.1109/TSP.2010.2049997
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Schraudolph NN, 1999, NEURAL COMPUT, V11, P853, DOI 10.1162/089976699300016467
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997
   Yang Y, 2013, J COMPUT GRAPH STAT, V22, P396, DOI 10.1080/10618600.2012.680324
   Yen I. E. H., 2014, ADV NEURAL INFORM PR, V27, P1008
   Yue Man-Chung, 2016, ARXIV160507522
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   Zhang T, 2013, BERNOULLI, V19, P2277, DOI 10.3150/12-BEJ452
   Zhang T, 2010, J MACH LEARN RES, V11, P1081
   Zhao T, 2012, J MACH LEARN RES, V13, P1059
   Zhao Tuo, 2014, ARXIV14127477
   Zhou Shuheng, 2009, ARXIV09124045
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402077
DA 2019-06-15
ER

PT S
AU Li, YJ
   Fang, C
   Yang, JM
   Wang, ZW
   Lu, X
   Yang, MH
AF Li, Yijun
   Fang, Chen
   Yang, Jimei
   Wang, Zhaowen
   Lu, Xin
   Yang, Ming-Hsuan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Universal Style Transfer via Feature Transforms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.
C1 [Li, Yijun; Yang, Ming-Hsuan] UC Merced, Merced, CA 95343 USA.
   [Fang, Chen; Yang, Jimei; Wang, Zhaowen; Lu, Xin] Adobe Res, San Jose, CA USA.
   [Yang, Ming-Hsuan] NVIDIA Res, Santa Clara, CA USA.
RP Li, YJ (reprint author), UC Merced, Merced, CA 95343 USA.
EM yli62@ucmerced.edu; cfang@adobe.com; jimyang@adobe.com;
   zhawang@adobe.com; xinl@adobe.com; mhyang@ucmerced.edu
FU NSF CAREER [1149783]
FX This work is supported in part by the NSF CAREER Grant #1149783, gifts
   from Adobe and NVIDIA.
CR Champandard A. J., 2016, ARXIV160301768
   Chen D., 2017, CVPR
   Chen T. Q., 2016, ARXIV161204337
   Cimpoi M., 2014, CVPR
   Dosovitskiy Alexey, 2016, NIPS
   Dumoulin V., 2017, ICLR
   Frigo O., 2016, CVPR
   Gatys L., 2015, NIPS
   Gatys L. A., 2016, CVPR
   Gatys L. A., 2017, CVPR
   Ghiasi G., 2017, BMVC
   Gonzalez R., 2008, DIGITAL IMAGE PROCES
   Hertzmann A., 2001, SIGGRAPH
   Hossain M., 2016, WHITENING COLORING T
   Huang Xun, 2017, ICCV
   Johnson J., 2016, ECCV
   Karayev S., 2014, BMVC
   Li C., 2016, CVPR
   Li C., 2016, ECCV
   Li Y., 2017, CVPR
   Liao J., 2017, ARXIV170501088
   Lin T.-Y., 2014, ECCV
   Luan F., 2017, CVPR
   Shih Y., 2013, SIGGRAPH
   Shih Y., 2014, SIGGRAPH
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Ulyanov D., 2016, ARXIV160708022
   Ulyanov D., 2017, CVPR
   Ulyanov D., 2016, ICML
   Wang H., 2017, ARXIV170307255
   Wang Xiaolong, 2017, CVPR
   Wilmot  P., 2017, ARXIV170108893
NR 32
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400037
DA 2019-06-15
ER

PT S
AU Li, YT
   Murias, M
   Major, S
   Dawson, G
   Dzirasa, K
   Carin, L
   Carlson, DE
AF Li, Yitong
   Murias, Michael
   Major, Samantha
   Dawson, Geraldine
   Dzirasa, Kafui
   Carin, Lawrence
   Carlson, David E.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Targeting EEG/LFP Synchrony with Neural Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the analysis of Electroencephalography (EEG) and Local Field Potential (LFP) datasets, which are "big" in terms of the size of recorded data but rarely have sufficient labels required to train complex models (e. g., conventional deep learning methods). Furthermore, in many scientific applications, the goal is to be able to understand the underlying features related to the classification, which prohibits the blind application of deep networks. This motivates the development of a new model based on parameterized convolutional filters guided by previous neuroscience research; the filters learn relevant frequency bands while targeting synchrony, which are frequency-specific power and phase correlations between electrodes. This results in a highly expressive convolutional neural network with only a few hundred parameters, applicable to smaller datasets. The proposed approach is demonstrated to yield competitive (often state-of-the-art) predictive performance during our empirical tests while yielding interpretable features. Furthermore, a Gaussian process adapter is developed to combine analysis over distinct electrode layouts, allowing the joint processing of multiple datasets to address overfitting and improve generalizability. Finally, it is demonstrated that the proposed framework effectively tracks neural dynamics on children in a clinical trial on Autism Spectrum Disorder.
C1 [Li, Yitong; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
   [Murias, Michael; Major, Samantha; Dawson, Geraldine; Dzirasa, Kafui] Duke Univ, Dept Psychiat, Durham, NC 27706 USA.
   [Murias, Michael; Major, Samantha; Dawson, Geraldine; Dzirasa, Kafui] Duke Univ, Dept Behav Sci, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA.
   [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA.
RP Li, YT (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM yitong.li@duke.edu; michael.murias@duke.edu; samantha.major@duke.edu;
   geraldine.dawson@duke.edu; kafui.dzirasa@duke.edu; lcarin@duke.edu;
   david.carlson@duke.edu
FU DARPA HIST program; National Institutes of Health [R01MH099192-05S2];
   W.M. Keck Foundation; Marcus Foundation; Perkin Elmer; Stylli
   Translational Neuroscience Award; NICHD [1P50HD093074]
FX In working on this project L.C. received funding from the DARPA HIST
   program; K.D., L.C., and D.C. received funding from the National
   Institutes of Health by grant R01MH099192-05S2; K.D received funding
   from the W.M. Keck Foundation; G.D. received funding from Marcus
   Foundation, Perkin Elmer, Stylli Translational Neuroscience Award, and
   NICHD 1P50HD093074.
CR Aghaei A. S., 2016, IEEE TBME
   Bashivan P., 2015, ARXIV151106448
   Bastos A. M., 2015, FRONTIERS SYSTEMS NE
   Bonilla E. V., 2007, NIPS, V20
   Bosl W., 2011, BMC MED
   Bruna J., 2013, IEEE PAMI
   Dawson G., 2017, STEM CELLS TRANSLATI
   Delorme A., 2004, J NEUROSCIENCE METHO
   Duan R.-N., 2013, IEEE EMBS C NEUR ENG
   Gallagher N., 2017, NIPS
   Hultman R., 2016, NEURON
   Jirsa V., 2013, FRONTIERS COMPUTATIO
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koelstra S., 2012, IEEE T AFFECTIVE COM
   Lawhern V.J., 2016, ARXIV161108024
   Li BB, 2016, I C NETWORK PROTOCOL
   Li S. C.-X., 2016, NIPS
   Lotte F, 2007, J NEURAL ENG
   Muller K.-R., 2008, J NEUROSCIENCE METHO
   Murias M., 2007, BIOL PSYCHIAT
   Novoselov E, 2016, INT CONF INFRA MILLI
   Nurse E., 2016, ACM INT C COMP FRONT
   Oostenveld R, 2011, COMPUTATIONAL INTELL
   OReilly D., 2012, CLIN NEUROPHYSIOLOGY
   Page A., 2015, IEEE CIRCUITS SYSTEM
   Pu Y., 2016, NIPS
   Qi Y, 2014, BIOMED RES INT, DOI 10.1155/2014/703816
   Rasmus A., 2015, NIPS
   Tsinalis O., 2016, ARXIV161001683
   Ulrich K. R., 2015, NIPS
   van Putten M. J., 2007, CLIN NEUROPHYSIOLOGY
   Yang H., 2015, EMBC
   Yang Y., 2016, NIPS
   Zheng W. - L., 2015, IEEE T AUTONOMOUS ME
   Zheng W.-L., 2014, IEEE ICME
   Zheng  Y., 2014, INT C WEB AG INF MAN
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404067
DA 2019-06-15
ER

PT S
AU Li, YZ
   Yuan, Y
AF Li, Yuanzhi
   Yuan, Yang
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convergence Analysis of Two-layer Neural Networks with ReLU Activation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing.
   In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that, if input follows from Gaussian distribution, with standard O(1/root d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.
   Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.
C1 [Li, Yuanzhi] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.
   [Yuan, Yang] Cornell Univ, Comp Sci Dept, Ithaca, NY 14853 USA.
RP Li, YZ (reprint author), Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.
EM yuanzhil@cs.princeton.edu; yangyuan@cs.cornell.edu
RI Jeong, Yongwook/N-7413-2016
CR Andoni A., 2014, P 31 INT C MACH LEAR, P1908
   Arora S., 2014, P 31 INT C MACH LEAR, P584
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   BREIMAN L, 1993, IEEE T INFORM THEORY, V39, P999, DOI 10.1109/18.256506
   Choromanska A., 2015, P 28 C LEARN THEOR C, P1756
   Choromanska Anna, 2015, AISTATS
   CYBENKO G, 1992, MATH CONTROL SIGNAL, V5, P455, DOI 10.1007/BF02134016
   Daniely Amit, 2016, NIPS, P2253
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goel Surbhi, 2017, ARXIV E PRINTS
   Goel Surbhi, 2016, CORR
   Goel Surbhi, 2017, NIPS 2017
   Goodfellow I. J., 2014, CORR
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hardt Moritz, 2016, CORR
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Janzamin Majid, 2015, ARXIV150608473
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Klusowski J. M., 2016, ARXIV E PRINTS
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pan Xingyuan, 2016, INT C MACH LEARN, P2427
   Pascanu Razvan, 2013, CORR
   Rudelson M., 2010, ARXIV E PRINTS
   Saad D, 1996, ADV NEUR IN, V8, P302
   Safran I., 2016, INT C MACH LEARN, P774
   Saxe Andrew M., 2013, CORR
   Sedghi Hanie, 2015, ICLR
   Shamir Ohad, 2016, CORR
   Sima J, 2002, NEURAL COMPUT, V14, P2709, DOI 10.1162/089976602760408035
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Tian Yuandong, 2016, ICLR 2017 UNPUB
   Xie Bo, 2017, AISTATS
   Zhang Yi, 2015, CORR
   Zhong Kai, 2017, ICML 2017
NR 44
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400057
DA 2019-06-15
ER

PT S
AU Li, YJ
   Schwing, A
   Wang, KC
   Zemel, R
AF Li, Yujia
   Schwing, Alexander
   Wang, Kuan-Chieh
   Zemel, Richard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dualing GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its saddle point formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this 'dualing GAN' act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.
C1 [Li, Yujia; Wang, Kuan-Chieh; Zemel, Richard] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
   [Wang, Kuan-Chieh; Zemel, Richard] Vector Inst, Toronto, ON, Canada.
   [Schwing, Alexander] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA.
   [Li, Yujia] DeepMind, London, England.
RP Li, YJ (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.; Li, YJ (reprint author), DeepMind, London, England.
EM yujiali@cs.toronto.edu; aschwing@illinois.edu; wangkua1@cs.toronto.edu;
   zemel@cs.toronto.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1718221]; NSERC; Samsung; CIFAR
FX This material is based upon work supported in part by the National
   Science Foundation under Grant No. 1718221, and grants from NSERC,
   Samsung and CIFAR.
CR Arjovsky M., 2017, ARXIV170107875
   Chen X., 2016, ARXIV160603657
   Dziugaite G. K., 2015, ARXIV150503906
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, ARXIV14062661
   Gretton A., 2012, JMLR
   Huang X., 2016, STACKED GENERATIVE A
   Im D. J., 2016, GENERATING IMAGES RE
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, P ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LeCun Yann, 1998, IEEE
   Li Y., 2015, ABS150202761
   London B., 2016, P NIPS WORKSH ADV TR
   Metz  L., 2016, UNROLLED GENERATIVE
   Nowozin S., 2016, ARXIV160600709
   Radford A., 2015, ARXIV151106434
   Salimans T., 2016, IMPROVED TECHNIQUES
   van den Oord A., 2016, CONDITIONAL IMAGE GE
   Wachter A., 2006, MATH PROGRAMMING
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Zhang Han, 2016, STACKGAN TEXT PHOTOR
   Zhao J., 2017, P ICLR
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405067
DA 2019-06-15
ER

PT S
AU Li, YZ
   Song, JM
   Ermon, S
AF Li, Yunzhu
   Song, Jiaming
   Ermon, Stefano
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.
C1 [Li, Yunzhu] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Song, Jiaming; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA.
RP Li, YZ (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM liyunzhu@mit.edu; tsong@cs.stanford.edu; ermon@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Toyota Research Institute (TRI); Intel Corporation; NSF [1651565,
   1522054, 1733686]; FLI
FX We thank Shengjia Zhao and Neal Jean for their assistance and advice.
   Toyota Research Institute (TRI) provided funds to assist the authors
   with their research but this article solely reflects the opinions and
   conclusions of its authors and not TRI or any other Toyota entity. This
   research was also supported by Intel Corporation, FLI and NSF grants
   1651565, 1522054, 1733686.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Aly Mohamed, 2008, 2008 IEEE Intelligent Vehicles Symposium (IV), P7, DOI 10.1109/IVS.2008.4621152
   Arjovsky M., 2017, ARXIV170107875
   Arora S., 2017, ARXIV170300573
   Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156
   Bojarski M., 2016, ARXIV160407316
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Englert P., 2015, P INT S ROB RES
   Ermon S., 2015, P 29 AAAI C ART INT, P644
   Finn  C., 2016, P 33 INT C MACH LEAR, V48
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ho J., 2016, P 33 INT C MACH LEAR
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Kuefler Alex, 2017, ARXIV170106699
   Lenz P, 2011, IEEE INT VEH SYM, P926, DOI 10.1109/IVS.2011.5940558
   Levine  S., 2013, P 30 INT C MACH LEAR, P1
   Lillicrap T P, 2015, ARXIV150902971
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pomerleau D. A., 1989, TECH REP
   Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ross S., 2011, AISTATS, P6
   Ross S., 2010, AISTATS, P3
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman J., 2015, ARXIV150602438
   Sermanet Pierre, 2016, ARXIV161206699
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stadie Bradly C, 2017, ICLR
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Tamar A, 2016, ADV NEURAL INFORM PR, P2146
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zhang J, 2016, ARXIV160506450
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403085
DA 2019-06-15
ER

PT S
AU Lian, XR
   Zhang, C
   Zhang, H
   Hsieh, CJ
   Zhang, W
   Liu, J
AF Lian, Xiangru
   Zhang, Ce
   Zhang, Huan
   Hsieh, Cho-Jui
   Zhang, Wei
   Liu, Ji
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Can Decentralized Algorithms Outperform Centralized Algorithms? A Case
   Study for Decentralized Parallel Stochastic Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONSENSUS
AB Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart?
   Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.
C1 [Lian, Xiangru; Liu, Ji] Univ Rochester, Rochester, NY 14627 USA.
   [Zhang, Ce] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Zhang, Huan; Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA.
   [Zhang, Wei] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA.
   [Liu, Ji] Tencent AI Lab, Bellevue, WA USA.
RP Lian, XR (reprint author), Univ Rochester, Rochester, NY 14627 USA.
EM xiangru@yandex.com; ce.zhang@inf.ethz.ch; victzhang@gmail.com;
   chohsieh@ucdavis.edu; weiz@us.ibm.com; ji.liu.uwisc@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF1718513, IIS-1719097]; Swiss National Science Foundation [NRP 75
   407540_167266]; IBM Zurich; Mercedes-Benz Research & Development North
   America; Oracle Labs; Swisscom; Chinese Scholarship Council; Department
   of Computer Science at ETH Zurich; TACC
FX Xiangru Lian and Ji Liu are supported in part by NSF CCF1718513. Ce
   Zhang gratefully acknowledge the support from the Swiss National Science
   Foundation NRP 75 407540_167266, IBM Zurich, Mercedes-Benz Research &
   Development North America, Oracle Labs, Swisscom, Chinese Scholarship
   Council, the Department of Computer Science at ETH Zurich, the GPU
   donation from NVIDIA Corporation, and the cloud computation resources
   from Microsoft Azure for Research award program. Huan Zhang and Cho-Jui
   Hsieh acknowledge the support of NSF IIS-1719097 and the TACC
   computation resources.
CR Agarwal A., 2011, NIPS
   Aybat N. S., 2015, ARXIV151208122
   Aysal TC, 2009, IEEE T SIGNAL PROCES, V57, P2748, DOI 10.1109/TSP.2009.2016247
   Bianchi P, 2013, IEEE T INFORM THEORY, V59, P7405, DOI 10.1109/TIT.2013.2275131
   Boyd S, 2005, IEEE INFOCOM SER, P1653
   Carli R, 2010, AUTOMATICA, V46, P70, DOI 10.1016/j.automatica.2009.10.032
   Chen J., 2016, ARXIV160400981
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Fagnani Fabio, 2008, IEEE J SELECTED AREA, V26
   Feng MW, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2413, DOI 10.1145/2983323.2983377
   Feyzmahdavian H. R., 2015, ASYNCHRONOUS MINIBAT
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   He K, 2015, ARXIV E PRINTS
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lan  G., 2017, ARXIV170103961
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li M, 2014, P 11 USENIX S OP SYS, P583
   Lian X., 2015, ADV NEURAL INFORM PR, P2737
   Lian  X., 2016, ADV NEURAL INFORM PR, P3054
   Lin Z., 2017, 5 INT C LEARN REPR
   Lu J, 2010, P AMER CONTR CONF, P301, DOI 10.1109/ITSC.2010.5625002
   Mokhtari A, 2016, J MACH LEARN RES, V17
   Moulines E., 2011, NIPS
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Nvidia, NCCL OPT PRIM COLL M
   Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293
   Ram SS, 2010, J OPTIMIZ THEORY APP, V147, P516, DOI 10.1007/s10957-010-9737-7
   Ram SS, 2009, IEEE DECIS CONTR P, P3581, DOI 10.1109/CDC.2009.5399485
   Ram SS, 2009, INT CONF ACOUST SPEE, P3653, DOI 10.1109/ICASSP.2009.4960418
   Schenato L., 2007, P 46 IEEE C DEC CONT, P2289, DOI DOI 10.1109/CDC.2007.4434671
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shi W., LINEAR CONVERGENCE A
   Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X
   Sirb B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P76, DOI 10.1109/BigData.2016.7840591
   Srivastava K, 2011, IEEE J-STSP, V5, P772, DOI 10.1109/JSTSP.2011.2118740
   Sundhar Ram S., 2010, RECENT ADV OPTIMIZAT, P51, DOI DOI 10.1007/978-3-642-12598-0_5
   Wu T., 2016, ARXIV161200150
   Yan F, 2013, IEEE T KNOWL DATA EN, V25, P2483, DOI 10.1109/TKDE.2012.191
   Yang TB, 2014, MACH LEARN, V95, P183, DOI 10.1007/s10994-013-5418-8
   Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170
   Zhang H., 2016, HOGWILD NEW MECH DEC
   Zhang R., 2014, P 31 INT C MACH LEAR, P1701
   Zhang S., 2015, ADV NEURAL INFORM PR, P685
   Zhang W., 2017, P 25 ACM INT C INF K
   Zhang W, 2016, IEEE INT CONF COMMUN
   Zhang Wei, 2016, P 25 INT JOINT C ART
   Zhuang Yong, 2013, P 7 ACM C REC SYST, P249, DOI DOI 10.1145/2507157.2507164
   Zinkevich M., 2010, ADV NEURAL INFORM PR, V23, P2595
NR 53
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405040
DA 2019-06-15
ER

PT S
AU Ligett, K
   Neel, S
   Roth, A
   Waggoner, B
   Wu, ZS
AF Ligett, Katrina
   Neel, Seth
   Roth, Aaron
   Waggoner, Bo
   Wu, Zhiwei Steven
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Accuracy First: Selecting a Differential Privacy Level for
   Accuracy-Constrained ERM
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Traditional approaches to differential privacy assume a fixed privacy requirement epsilon for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general "noise reduction" framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to "search" the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, and incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objective functions, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.
C1 [Ligett, Katrina] CALTECH, Pasadena, CA 91125 USA.
   [Ligett, Katrina] Hebrew Univ Jerusalem, Jerusalem, Israel.
   [Neel, Seth; Roth, Aaron; Waggoner, Bo] Univ Penn, Philadelphia, PA 19104 USA.
   [Wu, Zhiwei Steven] Microsoft Res, Redmond, WA USA.
RP Ligett, K (reprint author), CALTECH, Pasadena, CA 91125 USA.; Ligett, K (reprint author), Hebrew Univ Jerusalem, Jerusalem, Israel.
RI Jeong, Yongwook/N-7413-2016
CR [Anonymous], 1999, KDD 99 KDD CUP 1999
   Bassily R., 2014, CORR
   Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24
   Chaudhuri K., 2008, NIPS, V21, P289
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Duchi JC, 2013, ANN ALLERTON CONF, P1592, DOI 10.1109/Allerton.2013.6736718
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Fanti Giulia, 2016, POPETS, V3, P2016
   Greenberg Andy, 2016, WIRED MAGAZINE
   Jain P., 2012, COLT
   Kifer D., 2012, COLT
   Koufogiannis Fragkiskos, 2017, J PRIVACY CONFIDENTI, V7
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   Rogers Ryan M, 2016, ADV NEURAL INFORM PR, P1921
   Rubinstein Benjamin I. P., 2009, CORR
   Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35
   Song S, 2013, IEEE GLOB CONF SIG, P245, DOI 10.1109/GlobalSIP.2013.6736861
   The AMA Team at Laboratoire d'Informatique de Grenoble, 2017, BUZZ PRED ONL SOC ME
   Williams Oliver, 2010, ADV NEURAL INFORM PR, P2451
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402060
DA 2019-06-15
ER

PT S
AU Lim, CH
   Wright, SJ
AF Lim, Cong Han
   Wright, Stephen J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness
   and Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VARIABLE SELECTION; MODEL SELECTION; REGRESSION
AB The k-support and OWL norms generalize the l(1) norm, providing better prediction accuracy and better handling of correlated variables. We study the norms obtained from extending the k-support norm and OWL norms to the setting in which there are overlapping groups. The resulting norms are in general NP-hard to compute, but they are tractable for certain collections of groups. To demonstrate this fact, we develop a dynamic program for the problem of projecting onto the set of vectors supported by a fixed number of groups. Our dynamic program utilizes tree decompositions and its complexity scales with the treewidth. This program can be converted to an extended formulation which, for the associated group structure, models the k-group support norms and an overlapping group variant of the ordered weighted l(1) norm. Numerical results demonstrate the efficacy of the new penalties.
C1 [Lim, Cong Han; Wright, Stephen J.] Univ Wisconsin, Madison, WI 53706 USA.
RP Lim, CH (reprint author), Univ Wisconsin, Madison, WI 53706 USA.
EM clim9@wisc.edu; swright@cs.wisc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CMMI-1634597]; ONR Award [N00014-13-1-0129]; AFOSR
   [FA9550-13-1-0138]
FX This work was supported by NSF award CMMI-1634597, ONR Award
   N00014-13-1-0129, and AFOSR Award FA9550-13-1-0138.
CR Argyriou Andreas, 2012, ADV NEURAL INFORM PR, P1457
   AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Baldassarre L, 2016, IEEE T INFORM THEORY, V62, P6508, DOI 10.1109/TIT.2016.2602222
   Bogdan M, 2015, ANN APPL STAT, V9, P1103, DOI 10.1214/15-AOAS842
   Bondell HD, 2008, BIOMETRICS, V64, P115, DOI 10.1111/j.1541-0420.2007.00843.x
   Dell H., 2016, 11 INT S PAR EX COMP
   Downey R.G., 1999, MG COMP SCI, P530
   Downey RG, 2013, TEXTS COMPUTER SCI
   Halabi M. E., 2015, P INT C ART INT STAT, P223
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Obozinski G., 2012, TECHNICAL REPORT
   Obozinski G., 2016, TECHNICAL REPORT
   Oswal U., 2016, P 33 INT C MACH LEAR, P1041
   Rao N., 2012, P 15 INT C ART INT S, P942
   Rao N., 2013, ADV NEURAL INFORM PR, P2202
   Rao N. S., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1917, DOI 10.1109/ICIP.2011.6115845
   Rao N, 2017, INT CONF ACOUST SPEE, P2402, DOI 10.1109/ICASSP.2017.7952587
   Sankaran R., 2017, P 20 INT C ART INT S, P1123
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zeng X., 2014, ARXIV14094271
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400027
DA 2019-06-15
ER

PT S
AU Lin, J
   Rao, YM
   Lu, JW
   Zhou, J
AF Lin, Ji
   Rao, Yongming
   Lu, Jiwen
   Zhou, Jie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Runtime Neural Pruning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. Unlike existing neural pruning methods which produce a fixed pruned model for deployment, our method preserves the full ability of the original network and conducts pruning according to the input image and current feature maps adaptively. The pruning is performed in a bottom-up, layer-by-layer manner, which we model as a Markov decision process and use reinforcement learning for training. The agent judges the importance of each convolutional kernel and conducts channel-wise pruning conditioned on different samples, where the network is pruned more when the image is easier for the task. Since the ability of network is fully preserved, the balance point is easily adjustable according to the available resources. Our method can be applied to off-the-shelf network structures and reach a better tradeoff between speed and accuracy, especially with a large pruning rate.
C1 [Lin, Ji; Rao, Yongming; Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing, Peoples R China.
RP Lin, J (reprint author), Tsinghua Univ, Dept Automat, Beijing, Peoples R China.
EM lin-j14@mails.tsinghua.edu.cn; raoyongming95@gmail.com;
   lujiwen@tsinghua.edu.cn; zhou@tsnghua.edu.cn
FU National Natural Science Foundation of China [61672306]; National 1000
   Young Talents Plan Program
FX We would like to thank Song Han, Huazhe (Harry) Xu, Xiangyu Zhang and
   Jian Sun for their generous help and insightful advice. This work is
   supported by the National Natural Science Foundation of China under
   Grants 61672306 and the National 1000 Young Talents Plan Program. The
   corresponding author of this work is Jiwen Lu.
CR Almahairi Amjad, 2015, ARXIV151107838
   Anwar Sajid, 2015, ARXIV151208571
   BELLMAN R, 1956, P NATL ACAD SCI USA, V42, P767, DOI 10.1073/pnas.42.10.767
   Benbouzid Djalel, 2012, ARXIV12066387
   Bengio E., 2015, ARXIV151106297
   Bengio Y., 2013, ARXIV13083432
   Bolukbasi Tolga, 2017, ARXIV170207811
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chetlur S., 2014, ARXIV14100759
   Figurnov M., 2016, ARXIV161202297
   Han S., 2015, ARXIV151000149
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   HANSON SJ, 1989, ADV NEURAL INFORMATI, V1, P177
   Hassibi B., 1993, ADV NEURAL INFORMATI, V5, P164
   He He, 2012, ADV NEURAL INFORM PR, P3149
   Howard Andrew G., 2017, ARXIV170404861
   Hu H., 2016, ARXIV160703250
   Huang G. B., 2007, 0749 U MASS
   Jaderberg M., 2014, ARXIV14053866
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Karayev S., 2012, ADV NEURAL INFORM PR, V25, P890
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leroux Sam, 2017, KNOWL INF SYST, P1
   Li Hao, 2016, ARXIV160808710
   LI HX, 2015, PROC CVPR IEEE, P5325, DOI DOI 10.1109/CVPR.2015.7299170
   Littman ML, 2015, NATURE, V521, P445, DOI 10.1038/nature14540
   Liu L., 2017, ARXIV170100299
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Murray K., 2015, ARXIV150805051
   Odena Augustus, 2017, ARXIV170207780
   Puterman M. L., 2014, MARKOV DECISION PROC
   Ren S, 2015, ADV NEURAL INFORM PR, P91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Strom N., 1997, FREE SPEECH J, V5, P1
   Sun C, 2016, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2016.379
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy C., 2016, ARXIV160207261
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Zhang  F., 2015, ARXIV151103791
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402023
DA 2019-06-15
ER

PT S
AU Lin, K
   Sharpnack, J
   Rinaldo, A
   Tibshirani, RJ
AF Lin, Kevin
   Sharpnack, James
   Rinaldo, Alessandro
   Tibshirani, Ryan J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Sharp Error Analysis for the Fused Lasso, with Application to
   Approximate Changepoint Screening
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In the 1-dimensional multiple changepoint detection problem, we derive a new fast error rate for the fused lasso estimator, under the assumption that the mean vector has a sparse number of changepoints. This rate is seen to be suboptimal (compared to the minimax rate) by only a factor of log log n. Our proof technique is centered around a novel construction that we call a lower interpolant. We extend our results to misspecified models and exponential family distributions. We also describe the implications of our error analysis for the approximate screening of changepoints.
C1 [Lin, Kevin; Rinaldo, Alessandro; Tibshirani, Ryan J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Sharpnack, James] Univ Calif Davis, Davis, CA 95616 USA.
RP Lin, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kevin11@andrew.cmu.edu; jsharpna@ucdavis.edu; arinaldo@stat.cmu.edu;
   ryantibs@stat.cmu.edu
FU NSF [DMS-1554123, DMS-1712996]
FX JS was supported by NSF Grant DMS-1712996. RT was supported by NSF Grant
   DMS-1554123.
CR Aston JAD, 2012, ANN APPL STAT, V6, P1906, DOI 10.1214/12-AOAS565
   Boysen L, 2009, ANN STAT, V37, P157, DOI 10.1214/07-AOS558
   Brodsky B. E., 1993, NONPARAMETRIC METHOD
   Chan NH, 2014, J AM STAT ASSOC, V109, P590, DOI 10.1080/01621459.2013.866566
   Chen J., 2000, PARAMETRIC STAT CHAN
   Dalalyan AS, 2017, BERNOULLI, V23, P552, DOI 10.3150/15-BEJ756
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Dumbgen L, 2008, ANN STAT, V36, P1758, DOI 10.1214/07-AOS521
   Eckley I. A., 2011, BAYESIAN TIME SERIES, P205, DOI 10.1017/CBO9780511984679.011
   Fryzlewicz P, 2007, J AM STAT ASSOC, V102, P1318, DOI 10.1198/016214507000000860
   Fryzlewicz Piotr, 2016, TAIL GREEDY BOTTOM U
   Guntuboyina Adityanand, 2017, ARXIV1702050113
   Hernan Oscar, 2016, ARXIV160803384
   JOHNSTONE I, 2015, GAUSSIAN ESTIMATION
   Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274
   Mammen E, 1997, ANN STAT, V25, P387
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Steidl G, 2006, INT J COMPUT VISION, V70, P241, DOI 10.1007/s11263-006-8066-7
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R, 2008, BIOSTATISTICS, V9, P18, DOI 10.1093/biostatistics/kxm013
   Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406091
DA 2019-06-15
ER

PT S
AU Lin, K
   Li, DQ
   He, XD
   Zhang, ZY
   Sun, MT
AF Lin, Kevin
   Li, Dianqi
   He, Xiaodong
   Zhang, Zhengyou
   Sun, Ming-Ting
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adversarial Ranking for Language Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.
C1 [Lin, Kevin; Li, Dianqi; Sun, Ming-Ting] Univ Washington, Seattle, WA 98195 USA.
   [He, Xiaodong; Zhang, Zhengyou] Microsoft Res, Redmond, WA USA.
RP Lin, K (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM kvlin@uw.edu; dianqili@uw.edu; xiaohe@microsoft.com;
   zhang@microsoft.com; mts@uw.edu
RI Jeong, Yongwook/N-7413-2016
CR Bahdanau D., 2014, ARXIV14090473
   Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65
   Bo Dai, 2017, ARXIV170306029
   Bowman S.R., 2016, P 20 SIGNLL C COMP N, P10, DOI [10.18653/v1/K16-1002, DOI 10.18653/V1/K16-1002]
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gehring J., 2017, ARXIV170503122
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian J, 2014, ARXIV14126515
   Graves A, 2013, ARXIV13080850
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huang P.-S., 2013, P 22 ACM INT C C INF, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   Huszar Ferenc, 2015, ARXIV151105101
   Isola P., 2017, P CVPR
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Kusner M. J., 2016, ARXIV161104051
   Ledig C, 2016, ARXIV160904802
   Li J., 2017, ARXIV170106547
   Liu Siqi, IMPROVED IMAGE CAPTI
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Radford A., 2015, ARXIV151106434
   Reed Scott, 2016, P NIPS
   Reschke Kevin, 2013, ACL
   Salimans T., 2016, ARXIV160603498
   Shakespeare William, 2014, COMPLETE WORKS W SHA
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   SUTTON R.S., 1999, NIPS, V99, P1057
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wu Y., 2016, ARXIV160908144
   Yang Z., 2017, ARXIV170304887
   Yu Lantao, 2017, P AAAI
   Zhang X., 2015, ARXIV150201710
   Zhang Xingxing, 2014, P EMNLP
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403022
DA 2019-06-15
ER

PT S
AU Lin, XF
   Zhao, C
   Pan, W
AF Lin, Xiaofan
   Zhao, Cong
   Pan, Wei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Towards Accurate Binary Convolutional Neural Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a novel scheme to train binary convolutional neural networks (CNNs) - CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.
C1 [Lin, Xiaofan; Zhao, Cong; Pan, Wei] DJI Innovat Inc, Shenzhen, Peoples R China.
RP Pan, W (reprint author), DJI Innovat Inc, Shenzhen, Peoples R China.
EM xiaofan.lin@dji.com; cong.zhao@dji.com; wei.pan@dji.com
RI Jeong, Yongwook/N-7413-2016
CR Bengio Y., 2013, ARXIV13083432
   CAI Z., 2017, ARXIV170200953
   Courbariaux M, 2016, ARXIV160202830
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Esser S. K., 2016, P NATL ACAD SCI USA
   Giusti A., 2016, IEEE ROBOTICS AUTOMA
   Govindu G., PAR DISTR PROC S 200, P149
   Grabbe C, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P268
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hubara I., 2016, ARXIV160907061
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Liu SL, 2016, CONF PROC INT SYMP C, P393, DOI 10.1109/ISCA.2016.42
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren S, 2015, ADV NEURAL INFORM PR, P91
   TOMS DJ, 1990, ELECTRON LETT, V26, P1745, DOI 10.1049/el:19901121
   Venkatesh G., 2016, ARXIV161000324
   Zhou S., 2016, ARXIV160606160
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400033
DA 2019-06-15
ER

PT S
AU Ling, H
   Fidler, S
AF Ling, Huan
   Fidler, Sanja
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Teaching Machines to Describe Images via Natural Language Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. In particular, we first train a captioning model on a subset of images paired with human written captions. We then let the model describe new images and collect human feedback on the generated descriptions. We propose a hierarchical phrase-based captioning model, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback on new images our model learns to perform better than when given human written captions on these images.
C1 [Ling, Huan; Fidler, Sanja] Univ Toronto, Toronto, ON, Canada.
   [Fidler, Sanja] Vector Inst, Toronto, ON, Canada.
RP Ling, H (reprint author), Univ Toronto, Toronto, ON, Canada.
EM linghuan@cs.toronto.edu; fidler@cs.toronto.edu
RI Jeong, Yongwook/N-7413-2016
FU NSERC
FX We gratefully acknowledge the support from NVIDIA for their donation of
   the GPUs used for this research. This work was partially supported by
   NSERC. We also thank Relu Patrascu for infrastructure support.
CR Dai B., 2017, ARXIV170306029
   Das A., 2016, ARXIV161108669
   Griffith Shane, 2013, NIPS
   Judah K., 2010, AAAI
   Kaplan R, 2017, ARXIV170405539
   Karpathy A., 2015, CVPR
   Kingma D. P., 2014, ARXIV14126980
   Kiros R., 2014, CORR
   Knox W. Bradley, 2012, INT C AUT AG MULT SY
   Knox W. Bradley, 2013, INT C SOC ROB
   Krause J., 2017, CVPR
   Kuhlmann G., 2004, AAAI WORKSH SUP CONT
   Lebret R., 2015, ARXIV150203671
   Levine S, 2016, J MACH LEARN RES, V17
   Li J., 2016, ARXIV160601541
   Li Jiwei, 2016, ARXIV161109823
   Liu S., 2016, ARXIV161200370
   MACLIN R, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P694
   Manning Christopher D., 2014, ICLR
   Matari Maja J., 2017, SCI ROBOTICS, V2
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   Parkash Amar, 2012, EUR C COMP VIS ECCV
   Pascanu Razvan, 2014, ASS COMPUTATIONAL LI, P55
   Ranzato M., 2015, ARXIV151106732
   Rennie S. J., 2016, ARXIV161200563
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simo-Serra E., 2015, CVPR
   Simonyan Karen, 2015, VERY DEEP CONVOLUTIO, P23
   Tan Ying Hua, 2016, ACCV
   Thomaz A., 2006, AAAI
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Vinyals O, 2015, ARXIV150605869
   Westlund J. Kory, 2016, INT C HUM ROB INT
   Weston J., 2016, ARXIV160406045
   Williams Ronald J, 1992, MACHINE LEARNING
   Xu K, 2015, ICML
   Yu Yanchao, 2016, P SIGDIAL
   Yu Yanchao, 2017, WORKSH VIS LANG
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405015
DA 2019-06-15
ER

PT S
AU Liu, GC
   Liu, QS
   Yuan, XT
AF Liu, Guangcan
   Liu, Qingshan
   Yuan, Xiao-Tong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A New Theory for Matrix Completion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LOW-RANK MATRIX; ALGORITHMS
AB Prevalent matrix completion theories reply on an assumption that the locations of the missing data are distributed uniformly and randomly (i.e., uniform sampling). Nevertheless, the reason for observations being missing often depends on the unseen observations themselves, and thus the missing data in practice usually occurs in a nonuniform and deterministic fashion rather than randomly. To break through the limits of random sampling, this paper introduces a new hypothesis called isomeric condition, which is provably weaker than the assumption of uniform sampling and arguably holds even when the missing data is placed irregularly. Equipped with this new tool, we prove a series of theorems for missing data recovery and matrix completion. In particular, we prove that the exact solutions that identify the target matrix are included as critical points by the commonly used nonconvex programs. Unlike the existing theories for nonconvex matrix completion, which are built upon the same condition as convex programs, our theory shows that nonconvex programs have the potential to work with a much weaker condition. Comparing to the existing studies on nonuniform sampling, our setup is more general.
C1 [Liu, Guangcan; Liu, Qingshan; Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, B DAT, Sch Informat & Control, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
RP Liu, GC (reprint author), Nanjing Univ Informat Sci & Technol, B DAT, Sch Informat & Control, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
EM gcliu@cse.ohio-state.edu; qsliu@cse.ohio-state.edu; xtyuan@nuist.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU national Natural Science Foundation of China (NSFC) [61622305,
   61502238]; Natural Science Foundation of Jiangsu Province of China
   (NSFJPC) [BK20160040]; NSFC [61402232, 61522308]; NSFJPC [BK20141003]
FX The work of Guangcan Liu is supported in part by national Natural
   Science Foundation of China (NSFC) under Grant 61622305 and Grant
   61502238, in part by Natural Science Foundation of Jiangsu Province of
   China (NSFJPC) under Grant BK20160040.; The work of Xiao-Tong Yuan is
   supported in part by NSFC under Grant 61402232 and Grant 61522308, in
   part by NSFJPC under Grant BK20141003.
CR Bishop WE, 2014, ADV NEURAL INFORM PR, V27, P2762
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen YD, 2015, J MACH LEARN RES, V16, P2999
   CHISTOV AL, 1984, LECT NOTES COMPUT SC, V176, P17
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Heiman E, 2014, RANDOM STRUCT ALGOR, V45, P306, DOI 10.1002/rsa.20483
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Kiraly FJ, 2015, J MACH LEARN RES, V16, P1391
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR, P836
   Lee Troy, 2013, ADV NEURAL INFORM PR, P1781
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2017, IEEE T PATTERN ANAL, V39, P47, DOI 10.1109/TPAMI.2016.2539946
   Liu GC, 2016, IEEE T SIGNAL PROCES, V64, P5623, DOI 10.1109/TSP.2016.2586753
   Liu GC, 2016, IEEE T PATTERN ANAL, V38, P417, DOI 10.1109/TPAMI.2015.2453969
   Liu  Guangcan, 2014, P ADV NEUR INF PROC, V27, P1206
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Meka Raghu, 2009, ADV NEURAL INFORM PR, P1258
   Mohan K, 2010, IEEE INT SYMP INFO, P1573, DOI 10.1109/ISIT.2010.5513471
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Netrapalli P., 2014, P ADV NEUR INF PROC, P1107
   Ni Yuzhao, 2013, INT C DAT MIN WORKSH, P1179
   Pimentel-Alarcon  D., 2016, INT C MACH LEARN, P802
   Recht B., 2008, TECHNICAL REPORT
   Rockafellar R T, 1970, CONVEX ANAL
   Salakhutdinov R., 2010, ADV NEURAL INFORM PR, P2056
   Shang F., 2016, P 30 AAAI C ART INT, P2016
   Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574
   Weimer Markus, 2007, NEURAL INFORM PROCES
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Zhang Yin, 2006, TR0615 CAAM
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400075
DA 2019-06-15
ER

PT S
AU Liu, LP
   Ruiz, FJR
   Athey, S
   Blei, DM
AF Liu, Li-Ping
   Ruiz, Francisco J. R.
   Athey, Susan
   Blei, David M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Context Selection for Embedding Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Word embeddings are an effective tool to analyze language. They have been recently extended to model other types of data beyond text, such as items in recommendation systems. Embedding models consider the probability of a target observation (a word or an item) conditioned on the elements in the context (other words or items). In this paper, we show that conditioning on all the elements in the context is not optimal. Instead, we model the probability of the target conditioned on a learned subset of the elements in the context. We use amortized variational inference to automatically choose this subset. Compared to standard embedding models, this method improves predictions and the quality of the embeddings.
C1 [Liu, Li-Ping] Tufts Univ, Medford, MA 02155 USA.
   [Ruiz, Francisco J. R.; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England.
   [Athey, Susan] Stanford Univ, Stanford, CA 94305 USA.
RP Liu, LP (reprint author), Tufts Univ, Medford, MA 02155 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [PPAML
   FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan
   Foundation; John Simon Guggenheim Foundation; EU [706760]; NVIDIA
   Corporation
FX This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA
   PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, the Alfred P.
   Sloan Foundation, and the John Simon Guggenheim Foundation. Francisco J.
   R. Ruiz is supported by the EU H2020 programme (Marie Sklodowska-Curie
   grant agreement 706760). We also acknowledge the support of NVIDIA
   Corporation with the donation of two GPUs used for this research.
CR Arora S., 2016, T ASS COMPUTATIONAL, V4
   Bamler R., 2017, INT C MACH LEARN
   Barkan O., 2016, IEEE INT WORKSH MACH
   Bengio Y, 2006, INNOVATIONS MACHINE
   Bishop C. M., 2006, PATTERN RECOGNITION
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Firth J. R., 1957, STUDIES LINGUISTIC A, V1952-1959
   Gershman Samuel J, 2014, P 36 ANN C COGN SCI
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   Korattikara A., 2015, ADV NEURAL INFORM PR
   Levy O., 2014, ADV NEURAL INFORM PR
   Liang D., 2016, ACM C REC SYST
   Mikolov T., 2013, C N AM CHAPT ASS COM
   Mikolov T., 2013, INT C LEARN REPR
   Mikolov Tomas, 2013, ADV NEURAL INFORM PR
   Mnih A., 2013, ADV NEURAL INFORM PR
   Mnih Andriy, 2014, INT C MACH LEARN
   Munson M. A., 2015, EBIRD REFERENCE DATA
   Murphy KP, 2012, MACHINE LEARNING PRO
   Paisley  J., 2012, INT C MACH LEARN
   Pennington J., 2014, C EMP METH NAT LANG
   Ranganath R, 2015, ARTIFICIAL INTELLIGE
   Ranganath Rajesh, 2014, ARTIFICIAL INTELLIGE
   Rezende D. J., 2014, INT C MACH LEARN
   Rudolph M., 2016, ADV NEURAL INFORM PR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006
   Vilnis Luke, 2015, INT C LEARN REPR
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404086
DA 2019-06-15
ER

PT S
AU Liu, LX
   Li, DN
   Wong, WH
AF Liu, Linxi
   Li, Dangna
   Wong, Wing Hung
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convergence rates of a partition based Bayesian multivariate density
   estimation method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DISTRIBUTIONS
AB We study a class of non-parametric density estimators under Bayesian settings. The estimators are obtained by adaptively partitioning the sample space. Under a suitable prior, we analyze the concentration rate of the posterior distribution, and demonstrate that the rate does not directly depend on the dimension of the problem in several special cases. Another advantage of this class of Bayesian density estimators is that it can adapt to the unknown smoothness of the true density function, thus achieving the optimal convergence rate without artificial conditions on the density. We also validate the theoretical results on a variety of simulated data sets.
C1 [Liu, Linxi] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Li, Dangna] Stanford Univ, ICME, Stanford, CA 94305 USA.
   [Wong, Wing Hung] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Liu, Linxi] Stanford Univ, Stanford, CA 94305 USA.
RP Liu, LX (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.
EM ll3098@columbia.edu; dangna@stanford.edu; whwong@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074
   Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   de Jonge R, 2012, ELECTRON J STAT, V6, P1984, DOI 10.1214/12-EJS735
   DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733
   FARRELL RH, 1967, ANN MATH STAT, V38, P471, DOI 10.1214/aoms/1177698962
   FERGUSON TS, 1974, ANN STAT, V2, P615, DOI 10.1214/aos/1176342752
   FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766
   Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228
   Grenander U., 1981, PROBABILITY STAT SER
   Kruijer W, 2010, ELECTRON J STAT, V4, P1225, DOI 10.1214/10-EJS584
   Li Dangna, 2016, 30 C NEUR INF PROC S
   Lu L, 2013, J AM STAT ASSOC, V108, P1402, DOI 10.1080/01621459.2013.813389
   Ma L, 2011, J AM STAT ASSOC, V106, P1553, DOI 10.1198/jasa.2011.tm10003
   Rivoirard V, 2012, BAYESIAN ANAL, V7, P311, DOI 10.1214/12-BA710
   Rousseau J, 2010, ANN STAT, V38, P146, DOI 10.1214/09-AOS703
   Shen WN, 2015, SCAND J STAT, V42, P1194, DOI 10.1111/sjos.12159
   Shen WN, 2013, BIOMETRIKA, V100, P623, DOI 10.1093/biomet/ast015
   SHEN XT, 1994, ANN STAT, V22, P580, DOI 10.1214/aos/1176325486
   Shen XT, 2001, ANN STAT, V29, P687
   Soriano J, 2017, J R STAT SOC B, V79, P547
   Wong WH, 2010, ANN STAT, V38, P1433, DOI 10.1214/09-AOS755
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404078
DA 2019-06-15
ER

PT S
AU Liu, MR
   Yang, TB
AF Liu, Mingrui
   Yang, Tianbao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Accelerated Gradient Converging Method under Holderian Error
   Bound Condition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DESCENT METHODS; CONVEX
AB Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the Holderian error bound (HEB) condition. The key technique for our development is a novel synthesis of adaptive regularization and a conditional restarting scheme, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG's convergence speed is essentially o(1/t), where t is the total number of iterations; (ii) if the objective function consists of an l(1), l(infinity), l(1,infinity) or huber norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a faster linear convergence than PG without any other assumptions (e.g., restricted eigen-value condition). It is notable that our linear convergence results for the aforementioned problems are global instead of local. To the best of our knowledge, these improved results are first shown in this work.
C1 [Liu, Mingrui; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
RP Liu, MR (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1463988, IIS-1545995]
FX We thank the anonymous reviewers for their helpful comments. M. Liu and
   T. Yang are partially supported by National Science Foundation
   (IIS-1463988, IIS-1545995).
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BIERSTONE E, 1988, PUBL MATH-PARIS, P5
   Bolte J., 2015, ABS151008234 CORR
   Drusvyatskiy D., 2016, ARXIV160206661
   Fan R. E., 2011, LIBSVM DATA CLASSIFI
   Fercoq Olivier, 2016, ARXIV160907358
   Gong P., 2014, ABS14061102 CORR
   Hou K., 2013, ADV NEURAL INFORM PR, P710
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z
   Lin Q., 2014, ICML, P73
   LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   Necoara I., 2015, ABS150406298 CORR
   Nesterov Y., 2012, OPTIMA, V88
   Nesterov Y., 2004, APPL OPTIMIZATION
   Nesterov  Y., 2007, GRADIENT METHODS MIN
   NYQUIST H, 1983, COMMUN STAT-THEOR M, V12, P2511, DOI 10.1080/03610928308828618
   So A. M., 2013, ABS13090113 CORR
   Tseng P., 2008, SIAM J OPTIMIZATION
   Tyrrell Rockafellar R., 1976, SIAM J CONTROL OPTIM, V14
   Wang PW, 2014, J MACH LEARN RES, V15, P1523
   Xu Y., 2016, ADV NEURAL INFORM PR, P1208
   Xu Yi, 2017, P 34 INT C MACH LEAR, P3821
   Yang T., 2016, ABS151203107 CORR
   Yang WH, 2009, SIAM J OPTIMIZ, V19, P1633, DOI 10.1137/070689838
   Zadorozhnyi  O., 2016, MACH LEARN KNOWL DIS, P714
   Zhang H., 2016, ABS160600269 CORR
   Zhang H., 2016, OPTIMIZATION LETT, P1
   Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157
   Zhou Z., 2015, P 32 INT C MACH LEAR, P1501
   Zhou Z., 2015, ABS151203518 CORR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403017
DA 2019-06-15
ER

PT S
AU Liu, S
   Bousquet, O
   Chaudhuri, K
AF Liu, Shuang
   Bousquet, Olivier
   Chaudhuri, Kamalika
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Approximation and Convergence Properties of Generative Adversarial
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a "two-player game" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence.
   In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.
C1 [Liu, Shuang; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA.
   [Bousquet, Olivier] Google Brain, Mountain View, CA USA.
RP Liu, S (reprint author), Univ Calif San Diego, San Diego, CA 92103 USA.
EM shuangliu@ucsd.edu; obousquet@google.com; kamalika@cs.ucsd.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS 1617157]
FX We thank Iliya Tolstikhin, Sylvain Gelly, and Robert Williamson for
   helpful discussions. The work of KC and SL were partially supported by
   NSF under IIS 1617157.
CR Aliprantis CD, 1998, PRINCIPLES REAL ANAL
   Arjovsky  M., 2017, ABS170107875 CORR
   Arora S., 2017, ABS170300573 CORR
   Dales HG, 2016, CMS BOOKS MATH, P1, DOI 10.1007/978-3-319-32349-7
   Dziugaite G. K., 2015, UAI
   Genevay A., 2016, NIPS
   Goodfellow I., 2014, NIPS
   Gulrajani I., 2017, ABS170400028 CORR
   Li Y., 2015, ICML
   Nowozin Sebastian, 2016, NIPS
   Rudin W., 1991, INT SERIES PURE APPL
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Sutherland D., 2017, ICLR
   Villani  C., 2009, GRUNDLEHREN MATH WIS
   Wu Y., 2017, LECT NOTES INFORM TH
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405061
DA 2019-06-15
ER

PT S
AU Liu, SF
   De Mello, S
   Gu, JW
   Zhong, GY
   Yang, MH
   Kautz, J
AF Liu, Sifei
   De Mello, Shalini
   Gu, Jinwei
   Zhong, Guangyu
   Yang, Ming-Hsuan
   Kautz, Jan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Affinity via Spatial Propagation Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be outputs from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, such as image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of deep CNNs. We validate the framework on the task of refinement of image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.
C1 [Liu, Sifei; Yang, Ming-Hsuan] UC Merced, Merced, CA 95343 USA.
   [Liu, Sifei; De Mello, Shalini; Gu, Jinwei; Yang, Ming-Hsuan; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA.
   [Zhong, Guangyu] Dalian Univ Technol, Dalian, Peoples R China.
RP Liu, SF (reprint author), UC Merced, Merced, CA 95343 USA.; Liu, SF (reprint author), NVIDIA, Santa Clara, CA 95051 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER [1149783]
FX This work is supported in part by the NSF CAREER Grant #1149783, gifts
   from Adobe and NVIDIA.
CR Arnab  A., 2016, ECCV
   Bertasius G., 2016, ARXIV160507681
   Byeon W., 2015, P IEEE C COMP VIS PA
   Chen L., 2015, ARXIV151103328
   Chen L. C., 2016, CORR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gersgorin S., 1931, B ACAD SCI URSS SM
   GRAVES A, 2007, ICANN, V4668, P549
   He K., 2015, CORR
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Jia Y., 2014, ARXIV14085093
   Kalchbrenner  N., 2015, ARXIV150701526
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, V24, P109
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lin G., 2015, ARXIV150602108
   Liu RS, 2016, IEEE T PATTERN ANAL, V38, P2457, DOI 10.1109/TPAMI.2016.2522415
   Liu S., 2016, EUR C COMP VIS
   Liu S, 2015, CVPR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maire M., 2015, CORR
   Oord  A.v.d., 2016, ARXIV160106759
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Schwing A. G., 2015, ARXIV150302351
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smith B. M., 2013, CVPR
   Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0
   Tomasi C., 1998, ICCV
   Visin F., 2015, ARXIV150500393
   Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1
   Yamashita T., 2015, IPSJ T COMPUTER VISI, V7, P99
   Yu F., 2015, ARXIV151107122
   Zhang Z., 2014, ECCV
   Zheng  S., 2015, IEEE INT C COMP VIS
   Zilly J. G., 2016, ARXIV160703474
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401054
DA 2019-06-15
ER

PT S
AU Liu, S
   Takeda, A
   Suzuki, T
   Fukumizu, K
AF Liu, Song
   Takeda, Akiko
   Suzuki, Taiji
   Fukumizu, Kenji
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Trimmed Density Ratio Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID EXPONENTIAL-FAMILIES; REGRESSION
AB Density ratio estimation is a vital tool in both machine learning and statistical community. However, due to the unbounded nature of density ratio, the estimation procedure can be vulnerable to corrupted data points, which often pushes the estimated ratio toward infinity. In this paper, we present a robust estimator which automatically identifies and trims outliers. The proposed estimator has a convex formulation, and the global optimum can be obtained via subgradient descent. We analyze the parameter estimation error of this estimator under high-dimensional settings. Experiments are conducted to verify the effectiveness of the estimator.
C1 [Liu, Song] Univ Bristol, Bristol, Avon, England.
   [Takeda, Akiko] RIKEN, AIP, Inst Stat Math, Tokyo, Japan.
   [Suzuki, Taiji] Univ Tokyo, Sakigake PRESTO, JST, AIP,RIKEN, Tokyo, Japan.
   [Liu, Song; Fukumizu, Kenji] Inst Stat Math, Tokyo, Japan.
RP Liu, S (reprint author), Univ Bristol, Bristol, Avon, England.
EM song.liu@bristol.ac.uk; atakeda@ism.ac.jp; taiji@mist.i.u-tokyo.ac.jp;
   fukumizu@ism.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU MEXT KAKENHI [25730013, 25120012, 26280009, 15H05707]; JST-PRESTO;
   JST-CREST; MEXT [25120012];  [15K00031]
FX We thank three anonymous reviewers for their detailed and helpful
   comments. Akiko Takeda thanks Grant-in-Aid for Scientific Research (C),
   15K00031. Taiji Suzuki was partially supported by MEXT KAKENHI
   (25730013, 25120012, 26280009 and 15H05707), JST-PRESTO and JST-CREST.
   Song Liu and Kenji Fukumizu have been supported in part by MEXT
   Grant-in-Aid for Scientific Research on Innovative Areas (25120012).
CR Azmandian F., 2012, P 4 AS C MACH LEARN, P49
   Boyd S., 2014, TECHNICAL REPORT
   CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Efron B, 1996, ANN STAT, V24, P2431
   Fazayeli F., 2016, P 33 INT C MACH LEAR, P2281
   Fithian W, 2015, BIOMETRIKA, V102, P486, DOI 10.1093/biomet/asu065
   Fokianos K, 2004, J R STAT SOC B, V66, P941, DOI 10.1111/j.1467-9868.2004.05480.x
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hadi AS, 1997, COMPUT STAT DATA AN, V25, P251, DOI 10.1016/S0167-9473(97)00011-X
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Kawahara Yoshinobu, 2012, Statistical Analysis and Data Mining, V5, P114, DOI 10.1002/sam.10124
   Liu S, 2017, ANN STAT, V45, P959, DOI 10.1214/16-AOS1470
   Loh PL, 2015, J MACH LEARN RES, V16, P559
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7
   Neykov N., 1990, COMPSTAT 90 SHORT CO, P99
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Pitman EJG, 1936, P CAMB PHILOS SOC, V32, P567
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201
   Scholkopf B, 2000, ADV NEUR IN, V12, P582
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Smola A. J., 2009, 12 INT C ART INT STA, P536
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Sugiyama M, 2008, ANN I STAT MATH, V60, P699, DOI 10.1007/s10463-008-0197-x
   Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0
   Tsuboi Y., 2009, J INFORM PROCESSING, V17, P138, DOI DOI 10.2197/IPSJIIP.17.138
   Vandev DL, 1998, STATISTICS, V32, P111, DOI 10.1080/02331889808802657
   Wornowizki M, 2016, COMPUTATION STAT, V31, P291, DOI 10.1007/s00180-015-0633-3
   Yamada M, 2013, NEURAL COMPUT, V25, P1324, DOI 10.1162/NECO_a_00442
   Yang E., 2015, ADV NEURAL INFORM PR, P2602
   Yang E., 2016, ARXIV160508299
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404057
DA 2019-06-15
ER

PT S
AU Liu, WW
   Shen, XB
   Tsang, IW
AF Liu, Weiwei
   Shen, Xiaobo
   Tsang, Ivor W.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sparse Embedded k-Means Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DIMENSIONALITY REDUCTION
AB The k-means clustering algorithm is a ubiquitous tool in data mining and machine learning that shows promising performance. However, its high computational cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently, [1] develop a state-of-the-art random projection (RP) method for faster k-means clustering. Their method delivers many improvements over other dimensionality reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach, [1] reduce the running time by a factor of min{n, d}epsilon(2)log(d)/k for data matrix X is an element of R-nxd with n data points and d features, while losing only a factor of one in approximation accuracy. Unfortunately, they still require O(ndk/epsilon(2)log(d)) for matrix multiplication and this cost will be prohibitive for large values of n and d. To break this bottleneck, we carefully build a sparse embedded 1-means clustering algorithm which requires O(nnz (X)) (nnz(X) denotes the number of non-zeros in X) for fast matrix multiplication. Moreover, our proposed algorithm improves on [1]'s results for approximation accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate k-means clustering, while achieving satisfactory clustering performance.
C1 [Liu, Weiwei] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.
   [Shen, Xiaobo] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Liu, Weiwei; Tsang, Ivor W.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.
RP Liu, WW (reprint author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.; Liu, WW (reprint author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.
EM liuweiwei863@gmail.com; njust.shenxiaobo@gmail.com;
   ivor.tsang@uts.edu.au
FU ARC Future Fellowship [FT130100746]; ARC [LP150100671, DP170101628,
   DP150102728, DP150103071]; NSFC [61232006, 61672235]
FX We would like to thank the area chairs and reviewers for their valuable
   comments and constructive suggestions on our paper. This project is
   supported by the ARC Future Fellowship FT130100746, ARC grant
   LP150100671, DP170101628, DP150102728, DP150103071, NSFC 61232006 and
   NSFC 61672235.
CR Boutsidis C., 2009, ADV NEURAL INFORM PR, P153
   Boutsidis C, 2015, IEEE T INFORM THEORY, V61, P1045, DOI 10.1109/TIT.2014.2375327
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cohen M. B., 2016, P 43 INT C AUT LANG
   Drineas P, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P291
   Fan JQ, 2009, J MACH LEARN RES, V10, P2013
   Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434
   FOLEY DH, 1975, IEEE T COMPUT, VC 24, P281, DOI 10.1109/T-C.1975.224208
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   He X, 2005, P ADV NEUR INF PROC, P507, DOI DOI 10.1155/2014/803919
   Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902
   Liu WW, 2017, J MACH LEARN RES, V18
   Liu X., 2017, P 31 AAAI C ART INT, P2259
   Mirsky L, 1960, Q J MATH, V11, P50, DOI 10.1093/qmath/11.1.50
   Mitchell TM, 2004, MACH LEARN, V57, P145, DOI 10.1023/B:MACH.0000035475.85309.1b
   Pourkamali-Anaraki F, 2017, IEEE T INFORM THEORY, V63, P2954, DOI 10.1109/TIT.2017.2672725
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shen  Xiaobo, 2017, P AAAI C ART INT, P2527
   Wang R, 2015, IEEE T CYBERNETICS, V45, P1108, DOI 10.1109/TCYB.2014.2341575
   Zhai YT, 2014, IEEE COMPUT INTELL M, V9, P14, DOI 10.1109/MCI.2014.2326099
NR 21
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403038
DA 2019-06-15
ER

PT S
AU Liu, WY
   Zhang, YM
   Li, XG
   Yu, ZD
   Dai, B
   Zhao, T
   Song, L
AF Liu, Weiyang
   Zhang, Yan-Ming
   Li, Xingguo
   Yu, Zhiding
   Dai, Bo
   Zhao, Tuo
   Song, Le
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Hyperspherical Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.
C1 [Liu, Weiyang; Li, Xingguo; Dai, Bo; Zhao, Tuo; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Zhang, Yan-Ming] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
   [Li, Xingguo] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Yu, Zhiding] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Liu, WY (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM wyliu@gatech.edu; ymzhang@nlpr.ia.ac.cn; tourzhao@gatech.edu;
   lsong@cc.gatech.edu
FU NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701]; NIH BIGDATA
   [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340]; Intel
   ISTC; NVIDIA; Amazon AWS; University of Minnesota; National Natural
   Science Foundation of China [61773376]
FX We thank Zhen Liu (Georgia Tech) for helping with the experiments and
   providing suggestions. This project was supported in part by NSF
   IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF
   IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC,
   NVIDIA and Amazon AWS. Xingguo Li is supported by doctoral dissertation
   fellowship from University of Minnesota. Yan-Ming Zhang is supported by
   the National Natural Science Foundation of China under Grant 61773376.
CR Chen L. C., 2015, ICLR
   Clevert D.A., 2015, ARXIV151107289
   Girshick R., 2014, CVPR
   Glorot X., 2010, AISTATS
   He K., 2016, ARXIV160305027
   He K., 2016, CVPR
   He K., 2015, ICCV
   Ioffe S., 2015, ICML
   Krizhevsky A., 2012, NIPS
   LI X, 2016, ARXIV161209296
   Liu W., 2017, CVPR
   Liu Weiyang, 2016, ICML
   Long  J., 2015, CVPR
   Mishkin Dmytro, 2015, ARXIV151106422
   Nakatsukasa Y, 2012, APPL NUMER MATH, V62, P67, DOI 10.1016/j.apnum.2011.09.010
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2015, CVPR
   Veit A., 2016, NIPS
   Xie Di, 2017, ARXIV170301827
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404003
DA 2019-06-15
ER

PT S
AU Liu, Y
   Chen, JS
   Deng, L
AF Liu, Yu
   Chen, Jianshu
   Deng, Li
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised Sequence Classification using Sequential Output Statistics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.
C1 [Liu, Yu; Chen, Jianshu; Deng, Li] Microsoft Res, Redmond, WA 98052 USA.
   [Liu, Yu; Deng, Li] Citadel LLC, Chicago, IL USA.
RP Chen, JS (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM jianshuc@microsoft.com; Li.Deng@citadel.com
RI Jeong, Yongwook/N-7413-2016
CR Bengio Y., 2007, P ADV NEUR INF PROC, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berg-kirkpatrick Taylor, 2013, P 51 ANN M ASS COMP, P207
   Beutelspacher Albrecht, 1994, CRYPTOLOGY
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen Jianshu, 2016, ARXIV160604646
   Chintala Soumith, 2016, PATH UNSUPERVISED LE
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dai Andrew M., 2015, ADV NEURAL INFORM PR, P3079
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow Ian, 2016, TUTORIAL AT NIPS
   Graves A., 2012, ARXIV12113711
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kay Anthony, 2007, LINUX J
   Kingma D.P., 2013, ARXIV13126114
   Knight Kevin, 2006, P JOINT C INT COMM C, P499
   Le Q., 2012, INT C MACH LEARN
   Luciano Dennis, 1987, COLL MATH J, V18, P2, DOI DOI 10.2307/2686311
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Minka T. P, 2005, TECHNICAL REPORT
   Parker Robert, 2009, PHILADELPHIA LINGUIS
   Smolensky P., 1986, PARALLEL DISTRIBUTED, P194
   Stewart Russell, 2017, P AAAI
   Sutskever Ilya, 2015, ARXIV151106440
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang CY, 2015, INT CONF GEOINFORM
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403060
DA 2019-06-15
ER

PT S
AU Liu, YY
   Shang, FH
   Cheng, J
   Cheng, H
   Jiao, LC
AF Liu, Yuanyuan
   Shang, Fanhua
   Cheng, James
   Cheng, Hong
   Jiao, Licheng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Accelerated First-order Methods for Geodesically Convex Optimization on
   Riemannian Manifolds
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FRAMEWORK
AB In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate from O((1 - mu/L)(k) )to O((1 - root mu/L)(k)). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k(2)). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.
C1 [Liu, Yuanyuan; Shang, Fanhua; Cheng, James] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Cheng, Hong] Chinese Univ Hong Kong, Dept Syst Engn & Engn Management, Hong Kong, Peoples R China.
   [Jiao, Licheng] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Sch Artificial Intelligence, Xian, Shaanxi, Peoples R China.
RP Shang, FH (reprint author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM yyliu@cse.cuhk.edu.hk; fhshang@cse.cuhk.edu.hk; jcheng@cse.cuhk.edu.hk;
   hcheng@se.cuhk.edu.hk; lchjiao@mail.xidian.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU Hong Kong RGC [CUHK 14206715, 14222816]; Major Research Plan of the
   National Natural Science Foundation of China [91438201, 91438103];
   National Natural Science Foundation of China [61573267]
FX This research is supported in part by Grants (CUHK 14206715 & 14222816)
   from the Hong Kong RGC, the Major Research Plan of the National Natural
   Science Foundation of China (Nos. 91438201 and 91438103), and the
   National Natural Science Foundation of China (No. 61573267).
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Attouch H, 2016, SIAM J OPTIMIZ, V26, P1824, DOI 10.1137/15M1046095
   Azagra D., 2006, REV MAT COMPLUT
   Bacak M, 2014, CONVEX ANAL OPTIMIZA
   Barbaresco F., 2009, RADAR
   Batchelor PG, 2005, MAGN RESON MED, V53, P221, DOI 10.1002/mrm.20334
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bhatia R, 2007, PRINC SER APPL MATH, P1
   Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052
   Boyd S, 2007, OPTIM ENG, V8, P67, DOI 10.1007/s11081-007-9001-7
   Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423
   Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lapuyade-Lahorgue J., 2008, RADAR
   Liu Y., 2017, AAAI, P2287
   Meyer G, 2011, J MACH LEARN RES, V12, P593
   Moakher M, 2006, J ELASTICITY, V82, P273, DOI 10.1007/s10659-005-9035-z
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Petersen P, 2016, RIEMANNIAN GEOMETRY
   Shang F., 2017, ARXIV170404966
   Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168
   Su WJ, 2016, J MACH LEARN RES, V17
   Tseng P., 2008, AACELERATED PROXIMAL
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xinru Yuan, 2016, Procedia Computer Science, V80, P2147, DOI 10.1016/j.procs.2016.05.534
   Zhang H., 2016, NIPS, P4592
   Zhang  Hongyi, 2016, COLT, P1617
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404091
DA 2019-06-15
ER

PT S
AU Locatello, F
   Tschannen, M
   Ratsch, G
   Jaggi, M
AF Locatello, Francesco
   Tschannen, Michael
   Raetsch, Gunnar
   Jaggi, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Greedy Algorithms for Cone Constrained Optimization with Convergence
   Guarantees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NONNEGATIVE MATRIX; SPARSE SOLUTIONS
AB Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e(-t))) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.
C1 [Locatello, Francesco] Swiss Fed Inst Technol, MPI Intelligent Syst, Zurich, Switzerland.
   [Tschannen, Michael; Raetsch, Gunnar] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Locatello, F (reprint author), Swiss Fed Inst Technol, MPI Intelligent Syst, Zurich, Switzerland.
EM locatelf@ethz.ch; michaelt@nari.ee.ethz.ch; raetsch@inf.ethz.ch;
   martin.jaggi@epfl.ch
RI Jeong, Yongwook/N-7413-2016
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8
   Behr J, 2013, BIOINFORMATICS, V29, P2529, DOI 10.1093/bioinformatics/btt442
   Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006
   Bruckstein AM, 2008, IEEE T INFORM THEORY, V54, P4813, DOI 10.1109/TIT.2008.929920
   Buhlmann P, 2005, SEM STAT ETH ZUR
   Buhlmann P, 2010, WIRES COMPUT STAT, V2, P69, DOI 10.1002/wics.55
   Burger Martin, 2003, INFINITE DIMENSIONAL
   CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472
   Cichocki A, 2009, IEICE T FUND ELECTR, VE92A, P708, DOI 10.1587/transfun.E92.A.708
   Esser E, 2013, SIAM J IMAGING SCI, V6, P2010, DOI 10.1137/13090540X
   Frank Marguerite, 1956, NAVAL RES LOGISTICS
   Gillis N, 2015, IEEE T GEOSCI REMOTE, V53, P2066, DOI 10.1109/TGRS.2014.2352857
   Gillis N, 2014, SIAM J IMAGING SCI, V7, P1420, DOI 10.1137/130946782
   Gillis N, 2012, NEURAL COMPUT, V24, P1085, DOI 10.1162/NECO_a_00256
   Gillis Nicolas, 2016, ARXIV161001349
   Grubb Alexander, 2011, ARXIV11052054
   Guo Xiawei, 2017, AAAI C ART INT
   Harchaoui Z, 2015, MATH PROGRAM, V152, P75, DOI 10.1007/s10107-014-0778-9
   Hsieh C.-J., 2011, P 17 ACM SIGKDD INT, P1064
   Jaggi Martin, 2013, ICML 2013 P 30TH INT
   Kim H, 2007, PROCEEDINGS OF THE 7TH IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING, VOLS I AND II, P1147
   Kim J.G., 2012, HIGH PERFORMANCE SCI, P311, DOI [10.1007/978-1-4471-2437-5_16., DOI 10.1007/978-1-4471-2437-5_16]
   Kim J, 2014, J GLOBAL OPTIM, V58, P285, DOI 10.1007/s10898-013-0035-4
   Kopriva I, 2010, LECT NOTES COMPUT SC, V6365, P490, DOI 10.1007/978-3-642-15995-4_61
   Kumar A., 2013, INT C MACH LEARN ICM, P231
   Lacoste-Julien S., 2013, NIPS 2013 WORKSH GRE
   Lacoste-Julien Simon, 2015, ADV NEURAL INFORM PR, V28, P496
   Laue Soren, 2012, ICML
   Lawson C. L., 1995, SOLVING LEAST SQUARE, V15
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Locatello Francesco, 2017, P INT C ART INT STAT
   Makalic E, 2011, LECT NOTES ARTIF INT, V7106, P82, DOI 10.1007/978-3-642-25832-9_9
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Meir R., 2003, Advanced Lectures on Machine Learning. Machine Learning Summer School 2002. Revised Lectures. (Lecture Notes in Artificial Intelligence Vol.2600), P118
   Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293
   Nguyen Hao, 2014, CALCOLO, P1
   Peharz Robert, 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P83, DOI 10.1109/MLSP.2010.5589219
   Pena J, 2017, MATH PROGRAM, V166, P87, DOI 10.1007/s10107-016-1105-4
   Pena Javier, 2015, ARXIV151206142
   Pogorelov AV., 1973, EXTRINSIC GEOMETRY C, V35
   Ratsch Gunnar, 2001, NIPS, P487
   Sha F., 2002, ADV NEURAL INFORM PR, V15
   Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574
   Shashua A., 2005, P 22 INT C MACH LEAR, V119
   Temlyakov VN, 2015, CONSTR APPROX, V41, P269, DOI 10.1007/s00365-014-9272-0
   Temlyakov V, 2014, CONF REC ASILOMAR C, P1331, DOI 10.1109/ACSSC.2014.7094676
   Temlyakov Vladimir, 2013, CHEBUSHEV GREEDY ALG
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Welling M, 2001, PATTERN RECOGN LETT, V22, P1255, DOI 10.1016/S0167-8655(01)00070-8
   Yaghoobi M, 2015, IEEE SIGNAL PROC LET, V22, P1229, DOI 10.1109/LSP.2015.2393637
   Yang Yuning, 2015, HIGHER ORDER MATCHIN
   Yao Quanming, 2016, IJCAI
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400074
DA 2019-06-15
ER

PT S
AU London, B
AF London, Ben
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A PAC-Bayesian Analysis of Randomized Learning with Application to
   Stochastic Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABILITY
AB We study the generalization error of randomized learning algorithms-focusing on stochastic gradient descent (SGD)-using a novel combination of PAC-Bayes and algorithmic stability. Importantly, our generalization bounds hold for all posterior distributions on an algorithm's random hyperparameters, including distributions that depend on the training data. This inspires an adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We analyze this algorithm in the context of our generalization bounds and evaluate it on a benchmark dataset. Our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform sampling while also improving out-of-sample accuracy.
C1 [London, Ben] Amazon AI, Seattle, WA 98109 USA.
RP London, B (reprint author), Amazon AI, Seattle, WA 98109 USA.
EM blondon@amazon.com
RI Jeong, Yongwook/N-7413-2016
CR Bottou L., 2008, NEURAL INFORM PROCES
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Catoni O., 2007, I MATH STAT LECT NOT, V56
   Collins M, 2008, J MACH LEARN RES, V9, P1775
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elisseeff A, 2005, J MACH LEARN RES, V6, P55
   Feng J., 2016, CORR
   Freund Y, 1995, COMPUTATIONAL LEARNI
   Germain P., 2009, INT C MACH LEARN
   Hardt Moritz, 2016, INT C MACH LEARN
   Kontorovich A., 2014, INT C MACH LEARN
   Krizhevsky A., 2009, TECHNICAL REPORT
   Kuzborskij I., 2017, CORR
   Langford J., 2002, NEURAL INFORM PROCES
   Lin J., 2016, NEURAL INFORM PROCES
   Lin J, 2016, INT CONF EUR ENERG
   London B, 2016, J MACH LEARN RES, V17
   McAllester D., 1999, COMPUTATIONAL LEARNI
   Rosasco L., 2015, NEURAL INFORM PROCES
   Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386
   Shalev-Shwartz S., 2016, INT C MACH LEARN
   Shalev-Shwartz S., 2014, CORR
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Wang YQ, 2016, BMC GENET, V17, DOI 10.1186/s12863-016-0364-7
   Zhao  P, 2015, INT C MACH LEARN
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402096
DA 2019-06-15
ER

PT S
AU Long, MS
   Cao, ZJ
   Wang, JM
   Yu, PS
AF Long, Mingsheng
   Cao, Zhangjie
   Wang, Jianmin
   Yu, Philip S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Multiple Tasks with Multilinear Relationship Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.
C1 [Long, Mingsheng; Cao, Zhangjie; Wang, Jianmin; Yu, Philip S.] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
RP Long, MS (reprint author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM mingsheng@tsinghua.edu.cn; caozhangjie14@gmail.com;
   jimwang@tsinghua.edu.cn; psyu@uic.edu
RI Jeong, Yongwook/N-7413-2016
FU National Key R&D Program of China [2016YFB1000701]; National Natural
   Science Foundation of China [61772299, 61325008, 61502265, 61672313];
   TNList Fund
FX This work was supported by the National Key R&D Program of China
   (2016YFB1000701), National Natural Science Foundation of China
   (61772299, 61325008, 61502265, 61672313) and TNList Fund.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen J., 2011, KDD
   Chen JH, 2013, IEEE T PATTERN ANAL, V35, P1025, DOI 10.1109/TPAMI.2012.189
   Chu X, 2015, ICCV
   Ciliberto C., 2015, ICML
   Donahue J., 2014, ICML
   Evgeniou T., 2004, KDD
   Glorot X., 2011, ICML
   Gong B., 2012, CVPR
   Gupta A. K., 2000, MATRIX VARIATE DISTR
   Jacob L., 2009, NIPS
   Kang Z., 2011, ICML
   Krizhevsky A., 2012, NIPS
   Kumar A., 2012, ICML
   Long M., 2015, ICML
   Maurer A, 2016, J MACH LEARN RES, V17
   Misra  Ishan, 2016, CVPR
   Ohlson M, 2013, J MULTIVARIATE ANAL, V113, P37, DOI 10.1016/j.jmva.2011.05.015
   Ouyang W, 2014, CVPR
   Romera- Paredes B., 2013, ICML
   Simonyan Karen, 2015, ICLR
   Srivastava N., 2013, NIPS
   Venkateswara H., 2017, CVPR
   Yang Yingzhen, 2017, ICLR
   Yosinski J., 2014, NIPS
   Zhang Y., 2010, UAI
   Zhang Y., 2010, NIPS
   Zhang Yu, 2017, ARXIV170708114
   Zhang Z., 2014, ECCV
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401061
DA 2019-06-15
ER

PT S
AU Lopez-Paz, D
   Ranzato, M
AF Lopez-Paz, David
   Ranzato, Marc'Aurelio
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gradient Episodic Memory for Continual Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.
C1 [Lopez-Paz, David; Ranzato, Marc'Aurelio] Facebook Artificial Intelligence Res, Menlo Pk, CA 94025 USA.
RP Lopez-Paz, D (reprint author), Facebook Artificial Intelligence Res, Menlo Pk, CA 94025 USA.
EM dlp@fb.com; ranzato@fb.com
RI Jeong, Yongwook/N-7413-2016
CR Aljundi R., 2016, CVPR
   Balcan M.-F., 2015, COLT
   Baroni M., 2017, COMMAI EVALUATING 1
   Baxter J., 2000, JAIR
   Ben-David Shai, 2010, MACHINE LEARNING J
   Bengio Y., 2009, ICML
   Bertinetto L., 2016, NIPS
   Carlson  Andrew, 2010, AAAI
   Caruana R, 1998, LEARNING TO LEARN, P95
   Denoyer L., 2015, DEEP SEQUENTIAL NEUR
   Dorn W. S., 1960, Q APPL MATH
   Eigen D., 2014, ICLR
   Fei-Fei L., 2003, ICCV
   Fernando C., 2017, PATHNET EVOLUTION CH
   French R. M., 1999, TRENDS COGNITIVE SCI
   Goodfellow I. J., 2013, EMPIRICAL INVESTIGAT
   He K, 2015, DEEP RESIDUAL LEARNI
   Hinton Geoffrey, 2015, DISTILLING KNOWLEDGE
   Jung H., 2016, LESS FORGETTING LEAR
   Kirkpatrick James, 2017, PNAS
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lampert C. H., 2009, CVPR
   Li Z., 2016, ECCV
   Lucic M., 2017, TRAINING MIXTURE MOD
   McClelland J. L., 1995, PSYCHOL REV
   McCloskey Michael, 1989, PSYCHOL LEARNING MOT
   Mikolov T., 2015, ROADMAP MACHINE INTE
   Oquab M., 2014, CVPR
   Palatucci M., 2009, NIPS
   Pan S. J., 2010, TKDE
   Pentina A., 2016, NIPS
   Pentina A., 2015, CVPR
   Peters J., 2016, J ROYAL STAT SOC
   Ratcliff R., 1990, PSYCHOL REV
   Rebuffi Sylvestre-Alvise, 2017, CVPR
   Ring M. B., 1994, THESIS
   Ring M. B., 1997, MACHINE LEARNING
   Rusu A. A., 2016, NIPS
   Ruvolo P., 2013, ICML
   Santoro  A., 2016, ONE SHOT LEARNING ME
   Schaul  T., 2015, ICML
   Scholkopf B., 2016, LEARNING THEORY APPR
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Sutton R. S., 2011, 10 INT C AUT AG MULT
   Thrun S., 1994, P IEEE RSJ GI C INT
   Thrun S., 1998, LEARNING LEARN
   Thrun S., 2012, LEARNING LEARN
   Thrun S., 1996, NIPS
   Triki A. Rannen, 2017, ENCODER BASED LIFELO
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vinyals O., 2016, NIPS
   Zenke F., 2017, IMPROVED MULTITASK L
NR 52
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406052
DA 2019-06-15
ER

PT S
AU Lou, Q
   Dechter, R
   Ihler, A
AF Lou, Qi
   Dechter, Rina
   Ihler, Alexander
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dynamic Importance Sampling for Anytime Bounds of the Partition Function
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Computing the partition function is a key inference task in many graphical models. In this paper, we propose a dynamic importance sampling scheme that provides anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies, heuristic search, variational bounds, and Monte Carlo methods, blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search [16] and probabilistic bounds [15] of the partition function. By using an intelligently chosen weighted average over the samples, we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling and the long-term benefits of an improved proposal from search. This gives significantly improved anytime behavior, and more flexible trade-offs between memory, time, and solution quality. We demonstrate the effectiveness of our approach empirically on real-world problem instances taken from recent UAI competitions.
C1 [Lou, Qi; Dechter, Rina; Ihler, Alexander] Univ Calif Irvine, Comp Sci, Irvine, CA 92697 USA.
RP Lou, Q (reprint author), Univ Calif Irvine, Comp Sci, Irvine, CA 92697 USA.
EM qlou@ics.uci.edu; dechter@ics.uci.edu; ihler@ics.uci.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1526842, IIS-1254071]; United States Air Force
   [FA8750-14-C-0011, FA9453-16-C-0508]
FX This work is sponsored in part by NSF grants IIS-1526842, IIS-1254071,
   and by the United States Air Force under Contract No. FA8750-14-C-0011
   and FA9453-16-C-0508.
CR Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Chakraborty S., 2014, P 28 AAAI C ART INT, P1722
   Chakraborty S., IJCAI16
   Dagum P, 1997, ARTIF INTELL, V93, P1, DOI 10.1016/S0004-3702(97)00013-1
   Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357
   Dechter R, 2003, J ACM, V50, P107, DOI 10.1145/636865.636866
   Dechter R., 2010, HEURISTICS PROBABILI
   Dechter R., 2013, SYNTH LECT ARTIF INT, V7, P1, DOI DOI 10.2200/S00529ED1V01Y201308AIM023
   Ermon S., 2014, P 31 INT C MACH LEAR, P271
   Ermon S., 2013, J MACHINE LEARNING R, P334
   Gogate V, 2011, INTELL ARTIF, V5, P171, DOI 10.3233/IA-2011-0026
   Henrion M, 1991, P 7 ANN C UNC ART IN, P142
   Liu Q, 2014, THESIS
   Liu Q., 2015, ADV NEURAL INFORM PR, P1432
   Liu Q., 2011, P 28 INT C MACH LEAR
   Lou Q., 2017, P 31 AAAI C ART INT
   Maurer Andreas, 2009, COLT
   Oh M.-S., 1992, J STAT COMPUT SIM, V41, P143
   Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6
   Viricel C, 2016, LECT NOTES COMPUT SC, V9892, P733, DOI 10.1007/978-3-319-44953-1_46
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Yanover C., 2002, ADV NEURAL INFORM PR, P1457
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403026
DA 2019-06-15
ER

PT S
AU Louizos, C
   Ullrich, K
   Welling, M
AF Louizos, Christos
   Ullrich, Karen
   Welling, Max
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bayesian Compression for Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SHRINKAGE; SELECTION; NETWORKS
AB Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.
C1 [Louizos, Christos] Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.
   [Ullrich, Karen] Univ Amsterdam, Amsterdam, Netherlands.
   [Welling, Max] Univ Amsterdam, CIFAR, Amsterdam, Netherlands.
RP Louizos, C (reprint author), Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.
EM c.louizos@uva.nl; k.ullrich@uva.nl; m.welling@uva.nl
RI Jeong, Yongwook/N-7413-2016
FU TNO; NWO; Google
FX We would like to thank Dmitry Molchanov, Dmitry Vetrov, Klamer Schutte
   and Dennis Koelma for valuable discussions and feedback. This research
   was supported by TNO, NWO and Google.
CR Abadi M., 2016, ARXIV160304467
   ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99
   Armagan Artin, 2011, Adv Neural Inf Process Syst, V24, P523
   Azarkhish Erfan, 2017, ARXIV170106420
   Ba J, 2014, ADV NEURAL INFORM PR, V1, P2654
   BEALE EML, 1959, ANN MATH STAT, V30, P1145, DOI 10.1214/aoms/1177706099
   Blundell C., 2015, P 32 INT C MACH LEAR
   Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017
   Chai S., 2017, ARXIV170308595
   Chen W., 2015, ARXIV150604449
   Courbariaux M, 2016, ARXIV160202830
   Courbariaux M., 2014, ARXIV14127024
   Courbariaux M., 2015, ADV NEURAL INFORM PR, P3105
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Dong X., 2017, ARXIV170308651
   Figueiredo MAT, 2002, ADV NEUR IN, V14, P697
   Gal  Yarin, 2016, ICML
   Gong Y, 2015, ICLR
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Guo Y., 2016, ADV NEURAL INFORM PR, P1379
   Gupta S., 2015, CORR, P392
   Gysel P, 2016, THESIS
   Han S, 2016, ICLR
   Han Song, 2015, P ADV NEUR INF PROC, V2015, P1135
   Hinton G., 2015, ARXIV150302531
   Hinton G. E, 2012, ARXIV12070580
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762
   Howard Andrew G., 2017, ARXIV170404861
   Iandola F. N., 2017, ICLR
   Ingraham J. B., 2016, ARXIV160203807
   Kaae Sonderby C, 2016, ARXIV160202282
   Karaletsos T., 2015, ARXIV150507765
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P., 2015, ADV NEURAL INFORM PR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lawrence ND, 2002, PERSP NEURAL COMP, P128
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lecun  Y., 1989, NEURAL INFORM PROCES, P598
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lin D. D., 2015, ARXIV151106393
   Lin D. D., 2016, WORKSH ICML
   Louizos C., 2017, ARXIV E PRINTS
   Louizos Christos, 2015, THESIS
   MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011
   Mellempudi  N., 2017, ARXIV170501462
   Merolla P., 2016, ARXIV160601981
   MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129
   Molchanov Dmitry, 2017, ARXIV170105369
   Nalisnick E., 2015, ARXIV150603208
   Neal R M, 1995, THESIS
   Neville SE, 2014, ELECTRON J STAT, V8, P1113, DOI 10.1214/14-EJS910
   Papaspiliopoulos O, 2007, STAT SCI, V22, P59, DOI 10.1214/088342307000000014
   Peterson C., 1987, Complex Systems, V1, P995
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   RISSANEN J, 1986, ANN STAT, V14, P1080, DOI 10.1214/aos/1176350051
   Scardapane S., 2016, ARXIV160700485
   Shi S., 2017, ARXIV170407724
   Simonyan Karen, 2015, ICLR
   Srinivas  Suraj, 2016, ARXIV161106791
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sze V., 2017, ARXIV170309039
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Ullrich  Karen, 2017, ICLR
   Venkatesh G., 2016, ARXIV161000324
   Wallace C. S., 1990, ICCI 90 ADV COMPUTIN, P72
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Yang T.-J., 2017, CVPR
   Zhu  C., 2017, ICLR
NR 72
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403035
DA 2019-06-15
ER

PT S
AU Louizos, C
   Shalit, U
   Mooij, J
   Sontag, D
   Zemel, R
   Welling, M
AF Louizos, Christos
   Shalit, Uri
   Mooij, Joris
   Sontag, David
   Zemel, Richard
   Welling, Max
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Causal Effect Inference with Deep Latent-Variable Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PROXY VARIABLES; ERRORS; BIAS
AB Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.
C1 [Louizos, Christos] Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.
   [Shalit, Uri] NYU, CIMS, New York, NY 10003 USA.
   [Mooij, Joris] Univ Amsterdam, Amsterdam, Netherlands.
   [Sontag, David] MIT, CSAIL, Cambridge, MA 02139 USA.
   [Sontag, David] MIT, IMES, Cambridge, MA 02139 USA.
   [Zemel, Richard] Univ Toronto, CIFAR, Toronto, ON, Canada.
   [Welling, Max] Univ Amsterdam, CIFAR, Amsterdam, Netherlands.
RP Louizos, C (reprint author), Univ Amsterdam, TNO Intelligent Imaging, Amsterdam, Netherlands.
EM c.louizos@uva.nl; uas1@nyu.edu; j.m.mooij@uva.nl; dsontag@mit.edu;
   zemel@cs.toronto.edu; m.welling@uva.nl
RI Jeong, Yongwook/N-7413-2016
FU TNO; NWO; Google; European Research Council (ERC) under the European
   Union's Horizon 2020 research and innovation programme [639466]
FX We would like to thank Fredrik D. Johansson for valuable discussions,
   feedback and for providing the data for IHDP and Jobs. We would also
   like to thank Maggie Makar for helping with the Twins dataset. Christos
   Louizos and Max Welling were supported by TNO, NWO and Google. Joris
   Mooij was supported by the European Research Council (ERC) under the
   European Union's Horizon 2020 research and innovation programme (grant
   agreement 639466).
CR Abadi M., 2016, ARXIV160304467
   Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689
   Almond D, 2005, Q J ECON, V120, P1031, DOI 10.1162/003355305774268228
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Anandkumar Animashree, 2012, COLT, V1, P4
   Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1
   Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/10505160404000000512
   Arora S., 2016, CORR
   Cai Z., 2008, P 24 ANN C UNC ART I, P62
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Clevert D.A., 2015, ARXIV151107289
   Edwards JK, 2015, INT J EPIDEMIOL, V44, P1452, DOI 10.1093/ije/dyu272
   Filmer D, 2001, DEMOGRAPHY, V38, P115, DOI 10.2307/3088292
   FROST PA, 1979, REV ECON STAT, V61, P323, DOI 10.2307/1924606
   FULLER WA, 1987, WILEY SERIES PROBABI
   GOODMAN LA, 1974, BIOMETRIKA, V61, P215, DOI 10.1093/biomet/61.2.215
   GREENLAND S, 1983, INT J EPIDEMIOL, V12, P93, DOI 10.1093/ije/12.1.93
   Greenland S, 2008, MODERN EPIDEMIOLOGY, P345
   Gregor K., 2015, ARXIV E PRINTS
   GRILICHES Z, 1986, J ECONOMETRICS, V31, P93, DOI 10.1016/0304-4076(86)90058-8
   Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Jernite Y, 2013, ADV NEURAL INFORM PR, V26, P2355
   Johansson F., 2016, INT C MACH LEARN ICM
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kingma D. P., 2016, ARXIV160604934
   Kolenikov S, 2009, REV INCOME WEALTH, V55, P128, DOI 10.1111/j.1475-4991.2008.00309.x
   KRUSKAL JB, 1976, PSYCHOMETRIKA, V41, P281, DOI 10.1007/BF02293554
   Kuroki M., 2011, MEASUREMENT BIAS EFF
   Kuroki M, 2014, BIOMETRIKA, V101, P423, DOI 10.1093/biomet/ast066
   LALONDE RJ, 1986, AM ECON REV, V76, P604
   Louizos  C., 2016, INT C LEARN REPR ICL
   Maaloe Lars, 2016, ARXIV160205473
   Maddala G. S, 1992, INTRO ECONOMETRICS, V2
   Miao Wang, 2016, ARXIV160908816
   Montgomery MR, 2000, DEMOGRAPHY, V37, P155, DOI 10.2307/2648118
   Morgan SL, 2015, ANAL METHOD SOC RES, P1
   Pearl J., 2009, CAUSALITY
   Pearl J., 2012, ARXIV12033504
   Pearl  Judea, 2015, SOCIOLOGICAL METHODS
   Peysakhovich A., 2016, ARXIV161102385
   Ranganath Rajesh, 2016, ADV NEURAL INFORM PR, P496
   Rezende D. Jimenez, 2016, ARXIV E PRINTS
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   SELEN J, 1986, J AM STAT ASSOC, V81, P75, DOI 10.2307/2287969
   Shalit U., 2016, ARXIV E PRINTS
   Smith JA, 2005, J ECONOMETRICS, V125, P305, DOI 10.1016/j.jeconom.2004.04.011
   Thiesson B., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P504
   Tran D., 2016, ARXIV161009787
   Tran D., 2015, INT C LEARN REPR ICL
   Wager S., 2015, ARXIV151004342
   Wickens Michael R, 1972, ECONOMETRICA J ECONO, P759
   Wooldridge JM, 2009, ECON LETT, V104, P112, DOI 10.1016/j.econlet.2009.04.026
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406050
DA 2019-06-15
ER

PT S
AU Louppe, G
   Kagan, M
   Cranmer, K
AF Louppe, Gilles
   Kagan, Michael
   Cranmer, Kyle
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning to Pivot with Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot - a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.
C1 [Louppe, Gilles; Cranmer, Kyle] NYU, New York, NY 10003 USA.
   [Kagan, Michael] SLAC Natl Accelerator Lab, Menlo Pk, CA USA.
RP Louppe, G (reprint author), NYU, New York, NY 10003 USA.
EM g.louppe@nyu.edu; makagan@slac.stanford.edu; kyle.cranmer@nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [ACI-1450310]; US Department of Energy (DOE) [DE-AC02-76SF00515];
   SLAC Panofsky Fellowship;  [PHY-1505463];  [PHY-1205376]
FX We would like to thank the authors of (Baldi et al., 2016a) for sharing
   the data used in their studies. KC and GL are both supported through NSF
   ACI-1450310, additionally KC is supported through PHY-1505463 and
   PHY-1205376. MK is supported by the US Department of Energy (DOE) under
   grant DE-AC02-76SF00515 and by the SLAC Panofsky Fellowship.
CR Adam-Bourdarios C., 2014, NIPS 2014 WORKSH HIG, V42, P37
   Ajakan H., 2014, ARXIV14124446
   ATLAS collaboration, 2015, ATLPHYSPUB2015033 CE
   ATLAS Collaboration, 2014, ATLPHYSPUB2014004 CE
   Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Baldi P, 2016, PHYS REV D, V93, DOI 10.1103/PhysRevD.93.094034
   Baldi P, 2016, EUR PHYS J C, V76, DOI 10.1140/epjc/s10052-016-4099-4
   Bishop C. M., 1994, MIXTURE DENSITY NETW
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Cranmer K., 2015, ARXIV150602169
   DEGROOT MH, 1975, PROBABILITY STAT
   Edwards Harrison, 2015, ARXIV151105897
   Evans L, 2008, J INSTRUM, V3, DOI 10.1088/1748-0221/3/08/S08001
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Ganin Y., 2014, ARXIV14097495
   Gong B., 2013, P 30 INT C MACH LEAR, V28, P222
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3
   Khachatryan V, 2014, J HIGH ENERGY PHYS, DOI 10.1007/JHEP12(2014)017
   Louizos  C., 2015, ARXIV151100830
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Shimmin C., 2017, DECORRELATED JET SUB
   Zafar Muhammad Bilal, 2015, ARXIV150705259
   Zemel R., 2013, JMLR P, P325
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401003
DA 2019-06-15
ER

PT S
AU Lu, JS
   Kannan, A
   Yang, JW
   Parikh, D
   Batra, D
AF Lu, Jiasen
   Kannan, Anitha
   Yang, Jianwei
   Parikh, Devi
   Batra, Dhruv
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Best of Both Worlds: Transferring Knowledge from Discriminative Learning
   to a Generative Visual Dialog Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (I don't know', 'I can't tell'). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users.
   Our work aims to achieve the best of both worlds - the practical usefulness of G and the strong performance of D - via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution - specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).
C1 [Lu, Jiasen; Yang, Jianwei; Parikh, Devi; Batra, Dhruv] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Kannan, Anitha] Curai, Palo Alto, CA USA.
   [Parikh, Devi; Batra, Dhruv] Facebook AI Res, Menlo Pk, CA USA.
RP Lu, JS (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM jiasenlu@gatech.edu; jw2yang@gatech.edu; parikh@gatech.edu;
   dbatra@gatech.edu
RI Jeong, Yongwook/N-7413-2016
CR Antol S., 2015, ICCV
   Bengio Y., 2013, ABS13083432 CORR
   Bordes Antoine, 2016, ARXIV160507683
   Chen K, 2015, ARXIV151105960
   Chen T., 2015, ARXIV151105641
   Dai B., 2017, ARXIV170306029
   Das A., 2017, CVPR
   Das Abhishek, 2017, ARXIV170306585
   de Vries H, 2016, ARXIV161108481
   Denton E. L., 2015, NIPS
   Donahue J., 2015, CVPR
   Dosovitskiy Alexey, 2016, NIPS
   Fang H, 2015, CVPR
   Gao H., 2015, NIPS
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, NIPS
   Hinton G., 2015, ARXIV150302531
   Jang Eric, 2016, ARXIV161101144
   Johnson J., 2016, ECCV
   Karpathy A., 2015, CVPR
   Kusner Matt J., 2016, ABS161104051 CORR
   Ledig  C., 2016, ABS160904802 CORR, V2, P3
   Li J., 2017, ARXIV170106547
   Lin Chin-Yew, 2004, ACL 2004 WORKSH
   Lin T.-Y., 2014, ECCV
   Liu Chia- Wei, 2016, ARXIV160308023
   Liu S., 2016, ARXIV161200370
   Lu J, 2016, NIPS
   Lu Jiasen, 2016, CVPR
   Maddison Chris J, 2016, ARXIV161100712
   Malinowski M., 2015, ICCV
   Mei Hongyuan, 2016, ARXIV161106997
   Mostafazadeh N., 2017, ARXIV170108251
   Papineni K., 2002, ACL
   Radford A., 2015, ARXIV151106434
   Ranzato M., 2015, ARXIV151106732
   Ren M., 2015, NIPS
   Serban I. V., 2016, ARXIV160506069
   Serban I. V., 2017, AAAI
   Serban I. V., 2015, ARXIV150704808
   Shetty Rakshith, 2017, ABS170310476 CORR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sohn K., 2016, NIPS
   Sordoni A, 2015, ARXIV150606714
   Strub Florian, 2017, ARXIV170305423
   Sukhbaatar S., 2015, NIPS
   Sutskever  I., 2014, NIPS
   Vedantam Ramakrishna, 2015, CVPR
   Vinyals O, 2015, CVPR
   Xu H., 2016, ECCV
   Xu K., 2015, ABS150203044 CORR
   Yang Z, 2016, CVPR
   Yu Lantao, 2017, AAAI
   Zhao Junbo Jake, 2016, ABS160903126 CORR
   Zhu J Y, 2017, ARXIV170310593
NR 55
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400030
DA 2019-06-15
ER

PT S
AU Lu, XY
   Van Roy, B
AF Lu, Xiuyuan
   Van Roy, Benjamin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Ensemble Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.
C1 [Lu, Xiuyuan; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA.
RP Lu, XY (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM lxy@stanford.edu; bvr@stanford.edu
FU Boeing; Adobe
FX This work was generously supported by a research grant from Boeing and a
   Marketing Research Award from Adobe.
CR Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Dietterich TG, 2002, HDB BRAIN THEORY NEU, V2, P110
   Gal Y., 2016, INT C MACH LEARN, P1050
   Gomez-Uribe Carlos, 2016, ARXIV160505697V1
   Osband Ian, 2016, ADV NEURAL INFORM PR, P4026
   Papandreou G., 2010, ADV NEURAL INF PROCE, P1858
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
NR 8
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403032
DA 2019-06-15
ER

PT S
AU Lu, Z
   Pu, HM
   Wang, FC
   Hu, ZQ
   Wang, LW
AF Lu, Zhou
   Pu, Hongming
   Wang, Feicheng
   Hu, Zhiqiang
   Wang, Liwei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Expressive Power of Neural Networks: A View from the Width
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.
C1 [Lu, Zhou; Pu, Hongming; Wang, Feicheng] Peking Univ, Dept Math, Beijing, Peoples R China.
   [Hu, Zhiqiang; Wang, Liwei] Peking Univ, Key Lab Machine Percept, MOE, Sch EECS, Beijing, Peoples R China.
   [Lu, Zhou; Wang, Feicheng; Wang, Liwei] Peking Univ, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China.
RP Lu, Z (reprint author), Peking Univ, Dept Math, Beijing, Peoples R China.; Lu, Z (reprint author), Peking Univ, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China.
EM 1400010739@pku.edu.cn; 1400010621@pku.edu.cn; 1400010604@pku.edu.cn;
   huzq@pku.edu.cn; wanglw@cis.pku.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National Basic Research Program of China (973 Program) [2015CB352502];
   NSFC [61573026]; Center for Data Science, Beijing Institute of Big Data
   Research in Peking University
FX This work was partially supported by National Basic Research Program of
   China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and Center
   for Data Science, Beijing Institute of Big Data Research in Peking
   University. We would like to thank the anonymous reviewers for their
   valuable comments on our paper.
CR BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902
   Cohen N., 2016, C LEARN THEOR, P698
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Delalleau O., 2011, ADV NEURAL INFORM PR, P666
   Eldan  R., 2016, C LEARN THEOR, P907
   FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8
   Harvey Nick, 2017, COLT 2017
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Nguyen  Quynh, 2017, P 34 INT C MACH LEAR, P2603
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srikant R., 2017, ICLR 2017
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Telgarsky Matus, 2016, P 29 ANN C LEARN THE, V49, P1517
   Yarotsky D., 2016, ARXIV161001145
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406030
DA 2019-06-15
ER

PT S
AU Lueckmann, JM
   Goncalves, PJ
   Bassetto, G
   Ocal, K
   Nonnenmacher, M
   Macke, JH
AF Lueckmann, Jan-Matthis
   Goncalves, Pedro J.
   Bassetto, Giacomo
   Oecal, Kaan
   Nonnenmacher, Marcel
   Macke, Jakob H.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Flexible statistical inference for mechanistic models of neural dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SIMULATION
AB Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.
C1 [Lueckmann, Jan-Matthis; Goncalves, Pedro J.; Bassetto, Giacomo; Oecal, Kaan; Nonnenmacher, Marcel; Macke, Jakob H.] Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.
   [Oecal, Kaan] Univ Bonn, Math Inst, Bonn, Germany.
   [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany.
RP Lueckmann, JM (reprint author), Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.
EM jan-matthis.lueckmann@caesar.de; pedro.goncalves@caesar.de;
   giacomo.bassetto@caesar.de; kaan.oecal@caesar.de;
   marcel.nonnenmacher@caesar.de; jakob.macke@caesar.de
RI Jeong, Yongwook/N-7413-2016
FU German Research Foundation (DFG) [SFB 1089, SFB 1233]; caesar foundation
FX We thank Maneesh Sahani, David Greenberg and Balaji Lakshminarayanan for
   useful comments on the manuscript. This work was supported by SFB 1089
   (University of Bonn) and SFB 1233 (University of Tubingen) of the German
   Research Foundation (DFG) to JHM and by the caesar foundation.
CR Beaumont M, 2002, GENETICS, V162
   Beaumont M A, 2009, BIOMETRIKA
   Blum MGB, 2013, STAT SCI, V28, P189, DOI 10.1214/12-STS406
   Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0
   Bonassi FV, 2015, BAYESIAN ANAL, V10, P171, DOI 10.1214/14-BA891
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Carnevale N. T., 2009, NEURON BOOK
   Cho K, 2014, ARXIV14061078
   Chung J, 2014, ARXIV14123555
   Daly AC, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150499
   De Nicolao G, 1997, AUTOMATICA, V33
   Diggle P J, 1984, J R STAT SOC B
   Druckmann S, 2007, FRONT NEUROSCI, V1
   Fan Y, 2013, STAT, V2, P34, DOI 10.1002/sta4.15
   Friedrich P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00063
   Gerhard F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005390
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Graves A, 2011, ADV NEUR IN
   Gu Shixiang, 2015, ADV NEURAL INFORM PR, P2629
   Hartig F, 2011, ECOL LETT, V14, P816, DOI 10.1111/j.1461-0248.2011.01640.x
   Hay E, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002107
   Hinton G. E., 1993, P 6 ANN C COMP LEARN
   Hodgkin A., 1952, J PHYSL, V117
   Huys QJM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000379
   Jarvenpaa Marko, 2017, ARXIV170400520
   Jiang B., 2015, ARXIV151002175
   Kingma D P, 2015, VARIATIONAL DROPOUT, P2575
   Linderman S, 2016, ADV NEURAL INFORM PR
   Linderman S W, 2017, BIORXIV
   Lintusaari J, 2016, SYST BIOL
   Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100
   Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029
   Meeds E, 2014, UAI
   Meeds E, 2015, ARXIV150301916
   Meliza C D, 2014, BIOL CYBERN, V108
   Meng L, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/6/065006
   Ong V. M., 2016, ARXIV160803069
   Papamakarios G, 2017, ADV NEUR IN
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Pospischil M, 2008, BIOL CYBERN, V99, P427, DOI 10.1007/s00422-008-0263-8
   Price L F, 2017, J COMPUT GRAPH STAT
   Prinz AA, 2003, J NEUROPHYSIOL, V90, P3998, DOI 10.1152/jn.00641.2003
   Pritchard JK, 1999, MOL BIOL EVOL, V16, P1791, DOI 10.1093/oxfordjournals.molbev.a026091
   Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00009
   Stringer C, 2016, ELIFE, V5, DOI 10.7554/eLife.19695
   Turner B M, 2014, PSYCHONOMIC B REV, V21
   Van Geit W, 2016, FRONT NEUROINFORM, V10, DOI 10.3389/fninf.2016.00017
   vanVreeswijk C, 1996, SCIENCE, V274, P1724
   Wilkinson R, 2014, AISTATS
   Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319
NR 51
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401032
DA 2019-06-15
ER

PT S
AU Luo, ZL
   Zou, YL
   Hoffman, J
   Fei-Fei, L
AF Luo, Zelun
   Zou, Yuliang
   Hoffman, Judy
   Fei-Fei, Li
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Label Efficient Learning of Transferable Representations across Domains
   and Tasks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.
C1 [Luo, Zelun; Fei-Fei, Li] Stanford Univ, Stanford, CA 94305 USA.
   [Zou, Yuliang] Virginia Tech, Blacksburg, VA USA.
   [Hoffman, Judy] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Luo, ZL (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM zelunluo@stanford.edu; ylzou@vt.edu; jhoffman@eecs.berkeley.edu;
   feifeili@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Stanford Computer Science Department; Stanford Program in AI-assisted
   Care (PAC)
FX We would like to start by thanking our sponsors: Stanford Computer
   Science Department and Stanford Program in AI-assisted Care (PAC). Next,
   we specially thank De-An Huang, Kenji Hata, Serena Yeung, Ozan Sener and
   all the members of Stanford Vision and Learning Lab for their insightful
   discussion and feedback. Lastly, we thank all the anonymous reviewers
   for their valuable comments.
CR Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Bousmalis Konstantinos, 2016, ARXIV161205424
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Csurka  G., 2017, ARXIV170205374
   Darrell T., 2014, ARXIV14123474
   de Sa Virginia R, 1994, ADV NEURAL INFORM PR, P112
   Deng J., 2009, CVPR09
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue J., 2016, ARXIV160509782
   Donahue J., 2014, P INT C MACH LEARN, P647
   Dumoulin V., 2016, ARXIV160600704
   Finn C, 2017, ARXIV170303400
   Ganin Y., 2014, ARXIV14097495
   Ganin Y, 2016, J MACH LEARN RES, V17
   Girshick R., 2014, COMPUTER VISION PATT
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grandvalet Y., 2004, NIPS, V17, P529
   Gretton A, 2009, NEURAL INF PROCESS S, P131
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 1986, PARALLEL DISTRILMTED, V1
   Hoffman J., 2016, ARXIV161202649
   Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96
   Hoffman J, 2016, IEEE INT CONF ROBOT, P5032, DOI 10.1109/ICRA.2016.7487708
   Kalogeiton V, 2016, IEEE T PATTERN ANAL, V38, P2327, DOI 10.1109/TPAMI.2016.2551239
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koch G., 2015, THESIS
   Krizhevsky Alex, 2012, NEURAL INFORM PROCES
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Y, 2016, ARXIV160304779
   Lim J. J., 2011, ADV NEURAL INFORM PR, P118
   Liu M.-Y., 2016, ADV NEURAL INFORM PR, P469
   Liu Ming Yu, 2017, ARXIV170300848
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long M., 2016, ARXIV160506636
   Long M, 2016, ADV NEURAL INFORM PR, V2016, P136
   Luo Zelun, 2017, ARXIV170101821
   Mingsheng L., 2015, INT C MACH LEARN, P97
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   Oord  A.v.d., 2016, ARXIV160106759
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Palubinskas Gintautas, 1999, AAAI
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pathak D., 2016, ARXIV161206370
   Ravi Sachin, 2017, INT C LEARN REPR, V1, P6
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rozantsev A., 2016, ARXIV160306432
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Snell Jake, 2017, ARXIV170305175
   Soomro K., 2012, ARXIV12120402
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Taigman Yaniv, 2016, ARXIV161102200
   Tang K., 2012, ADV NEURAL INFORM PR, P638
   Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064
   Tzeng E., 2015, INT C COMP VIS ICCV
   Tzeng E., 2017, COMPUTER VISION PATT
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang X., 2015, ARXIV150300591
NR 73
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400016
DA 2019-06-15
ER

PT S
AU Ma, LQ
   Jia, X
   Sun, QR
   Schiele, B
   Tuytelaars, T
   Van Gool, L
AF Ma, Liqian
   Jia, Xu
   Sun, Qianru
   Schiele, Bernt
   Tuytelaars, Tinne
   Van Gool, Luc
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Pose Guided Person Image Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper proposes the novel Pose Guided Person Generation Network (PG(2)) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG(2) utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128x64 re-identification images and 256x256 fashion photos show that our model generates high-quality person images with convincing details.
C1 [Ma, Liqian; Van Gool, Luc] Katholieke Univ Leuven, PSI, TRACE, Toyota Res Europe, Leuven, Belgium.
   [Jia, Xu; Tuytelaars, Tinne] Katholieke Univ Leuven, PSI, IMEC, Leuven, Belgium.
   [Sun, Qianru; Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.
   [Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Ma, LQ (reprint author), Katholieke Univ Leuven, PSI, TRACE, Toyota Res Europe, Leuven, Belgium.
EM liqian.ma@esat.kuleuven.be; xu.jia@esat.kuleuven.be;
   qsun@mpi-inf.mpg.de; schiele@mpi-inf.mpg.de;
   tuytrlaars@esat.kuleuven.be; luc.vangool@esat.kuleuven.be
RI Jeong, Yongwook/N-7413-2016; Tuytelaars, Tinne/B-4319-2015
OI Tuytelaars, Tinne/0000-0003-3307-9723
FU Toyota Motors Europe; FWO Structure from Semantics project; KU Leuven
   GOA project CAMETRON; German Research Foundation (DFG) [CRC 1223]
FX We gratefully acknowledge the support of Toyota Motors Europe, FWO
   Structure from Semantics project, KU Leuven GOA project CAMETRON, and
   German Research Foundation (DFG CRC 1223). We would like to thank Bo
   Zhao for his helpful discussions.
CR Arjovsky M., 2017, ARXIV170107875
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Goodfellow I., 2014, NIPS
   Grauman K., 2014, P IEEE C COMP VIS PA, P2003
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Isola  Phillip, 2017, CVPR
   Johnson J., 2016, ECCV
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lassner C., 2017, ARXIV170504098
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Mathieu M., 2016, ICLR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Quan Tran Minh, 2016, 161205360 ARXIV
   Radford A., 2015, ARXIV151106434
   Reed S., 2016, NIPS
   Reed S., 2016, TECHNICAL REPORT
   Reed S., 2016, ICML
   Rezende D. J., 2014, ICML
   Rui Huang, 2017, 170404086 ARXIV
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Shi Wenzhe, REAL TIME SINGLE IMA
   Uria Benigno, 2016, 160502226 ARXIV
   van den Oord A., 2016, P 33 INT C MACH LEAR, P1747
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu Jia, 2016, BMVC
   Xun Huang, 2016, ARXIV161204357
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yang Jimei, 2015, NIPS
   Yim J., 2015, CVPR
   Zhang H., 2016, ARXIV161203242
   Zhao Bo, 2017, 170404886 ARXIV
   Zhe Cao, 2016, 161108050 ARXIV
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400039
DA 2019-06-15
ER

PT S
AU Ma, SY
   Belkin, M
AF Ma, Siyuan
   Belkin, Mikhail
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Diving into the shallows: a computational perspective on large-scale
   shallow learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture.
   In this paper we identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data.
   To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a significant performance boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.
C1 [Ma, Siyuan; Belkin, Mikhail] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
RP Ma, SY (reprint author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM masi@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1422830, IIS-1550757]
FX We thank Adam Stiff, Eric Fosler-Lussier, Jitong Chen, and Deliang Wang
   for providing TIMIT and HINT datasets. This work is supported by NSF
   IIS-1550757 and NSF CCF-1422830. Part of this work was completed while
   the second author was at the Simons Institute at Berkeley. In
   particular, he thanks Suvrit Sra, Daniel Hsu, Peter Bartlett, and
   Stefanie Jegelka for many discussions and helpful suggestions.
CR Agarwal N., 2016, ARXIV160203943
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Avron Haim, 2016, ARXIV161103220
   Bishop C. M, 2006, MACHINE LEARNING, V128
   Bordes A, 2009, J MACH LEARN RES, V10, P1737
   Boyd S., 2004, CONVEX OPTIMIZATION
   Braun Mikio Ludwig, 2005, THESIS
   Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362
   Camoriano Raffaello, 2016, P 19 INT C ART INT S, P1403
   Cheng CC, 2011, INT CONF ACOUST SPEE, P5200
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   Dennis J.E., 1996, NUMERICAL METHODS UN
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Erdogdu M. A., 2015, NIPS
   Gonen Alon, 2016, ICML, P1397
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hsieh C.-J., 2008, P 25 INT C MACH LEAR, V951, P408, DOI DOI 10.1145/1390156.1390208
   Jie Chen, 2016, ARXIV160800860
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Le Q., 2013, P INT C MACH LEARN
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Lu Zhiyun, 2014, ARXIV14114000
   May Avner, 2017, ARXIV170103577
   Minsker Stanislav, 2017, STAT PROBABILITY LET
   Moritz P., 2016, AISTATS
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587
   Povey D., 2011, ASRU
   Que Q., 2016, AISTATS, P1375
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Raskutti G, 2014, J MACH LEARN RES, V15, P335
   Richardson LF, 1911, PHILOS T R SOC LOND, V210, P307, DOI 10.1098/rsta.1911.0009
   Rosasco L, 2010, J MACH LEARN RES, V11, P905
   Rosenberg  S., 1997, LAPLACIAN RIEMANNIAN
   Schraudolph N. N., 2007, P 11 INT C ART INT S, P436
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Shewchuk J. R., 1994, INTRO CONJUGATE GRAD
   Sindhwani V., 2005, P 22 INT C MACH LEAR, V22, P824
   Steinwart I, 2008, INFORM SCI STAT, P1
   Takac M., 2013, CORNELL U LIBR, P1022
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Tropp Joel A., 2015, ARXIV150101571
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Tu Stephen, 2016, ARXIV160205310
   WILLIAMS CKI, 2001, NIPS, V13, P682
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403082
DA 2019-06-15
ER

PT S
AU Maddison, CJ
   Lawson, D
   Tucker, G
   Heess, N
   Norouzi, M
   Mnih, A
   Doucet, A
   Teh, YW
AF Maddison, Chris J.
   Lawson, Dieterich
   Tucker, George
   Heess, Nicolas
   Norouzi, Mohammad
   Mnih, Andriy
   Doucet, Arnaud
   Teh, Yee Whye
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Filtering Variational Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.
C1 [Maddison, Chris J.; Heess, Nicolas; Mnih, Andriy; Teh, Yee Whye] DeepMind, London, England.
   [Lawson, Dieterich; Tucker, George; Norouzi, Mohammad] Google Brain, Mountain View, CA USA.
   [Maddison, Chris J.; Doucet, Arnaud] Univ Oxford, Oxford, England.
RP Maddison, CJ (reprint author), DeepMind, London, England.; Maddison, CJ (reprint author), Univ Oxford, Oxford, England.
EM cmaddis@google.com; dieterichl@google.com; gjt@google.com
RI Jeong, Yongwook/N-7413-2016
FU EPSRC [EP/K000276/1]; European Research Council under the European
   Union's Seventh Framework Programme (FP7/2007-2013) ERC [617071]
FX We thank Matt Hoffman, Matt Johnson, Danilo J. Rezende, Jascha
   Sohl-Dickstein, and Theophane Weber for helpful discussions and support
   in this project. A. Doucet was partially supported by the EPSRC grant
   EP/K000276/1. Y. W. Teh's research leading to these results has received
   funding from the European Research Council under the European Union's
   Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no.
   617071.
CR Abadi M., 2016, ARXIV160304467
   Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Beal M. J., 2003, VARIATIONAL ALGORITH
   Bengio Yoshua, 2013, NIPS
   Berard J, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-3428
   Bornschein Jorg, 2015, ICLR
   Boulanger-Lewandowski Nicolas, 2012, ICML
   Bowman S. R., 2015, ARXIV151106349
   Burda Yuri, 2015, AISTATS
   Cerou F, 2011, ANN I H POINCARE-PR, V47, P629, DOI 10.1214/10-AIHP358
   Chung J, 2015, NIPS
   Del Moral P, 2004, PROB APPL S
   DelMoral P, 2013, MONOGR STAT APPL PRO, V126, P1
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dinh L., 2016, ARXIV160508803
   Doucet A., 2011, OXFORD HDB NONLINEAR, V12, P656
   Fraccaro Marco, 2016, NIPS
   Gal Y., 2016, THESIS
   Goodfellow I., 2014, NIPS
   Grosse RB, 2015, ARXIV151102543
   Gu Shixiang, 2015, NIPS
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma Diederik P, 2014, ICML
   Kingma Diederik P, 2016, NIPS
   Kucukelbir A., 2016, ARXIV160300788
   Le T. A., 2017, ARXIV170510306
   Mnih Andriy, 2014, ARXIV14020030
   Mnih Andriy, 2016, ARXIV160206725
   Mohamed S., 2016, ARXIV161003483
   Naesseth Christian A, 2017, ARXIV170511140
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Nowozin S., 2016, ARXIV160600709
   Pitt MK, 2012, J ECONOMETRICS, V171, P134, DOI 10.1016/j.jeconom.2012.06.004
   Ranganath Rajesh, 2014, AISTATS
   Ranganath Rajesh, 2016, NIPS
   Rezende D. J., 2014, ICML
   Rezende D. J., 2015, ICML
   Salakhutdinov R., 2016, ICLR
   Salimans T., 2015, ICML
   Tran  Dustin, 2017, ARXIV170208896
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406062
DA 2019-06-15
ER

PT S
AU Makhzani, A
   Frey, B
AF Makhzani, Alireza
   Frey, Brendan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI PixelGAN Autoencoders
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we describe the "PixelGAN autoencoder", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.
C1 [Makhzani, Alireza; Frey, Brendan] Univ Toronto, Toronto, ON, Canada.
RP Makhzani, A (reprint author), Univ Toronto, Toronto, ON, Canada.
EM makhzani@psi.toronto.edu; frey@psi.toronto.edu
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Barber D., 2003, NIPS, V16, P201
   Barone Antonio Valerio Miceli, 2016, ARXIV160802996
   Bowman S. R., 2015, ARXIV151106349
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chen X., 2016, ARXIV161102731
   Donahue J., 2016, ARXIV160509782
   Dumoulin V., 2016, ARXIV160600704
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani Ishaan, 2016, ARXIV161105013
   Hoffman Matthew D, 2016, NIPS 2016 WORKSH ADV
   Huszar F., 2017, ARXIV170208235
   Huszar Ferenc, IS MAXIMUM LIKELIHOO
   Im Daniel Jiwoong, 2015, ARXIV151106406
   Ioffe S., 2015, ARXIV150203167
   Kalchbrenner N., 2016, ARXIV161000527
   Kim T, 2017, ARXIV170305192
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Laine S, 2016, ARXIV161002242
   Maaloe Lars, 2016, ARXIV160205473
   Makhzani A., 2015, ARXIV151105644
   Mescheder L., 2017, ARXIV170104722
   Miyato Takeru, 2015, STAT, V1050, P25
   Mohamed S., 2016, ARXIV161003483
   Oord  A.v.d., 2016, ARXIV160106759
   Ranganath Rajesh, 2016, ADV NEURAL INFORM PR, P496
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Rezende D. J., 2014, INT C MACH LEARN
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Salimans Tim, 2017, ARXIV170105517
   Sonderby C. K., 2016, ARXIV161004490
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Sutskever Ilya, 2015, ARXIV151106440
   Theis L., 2015, ARXIV151101844
   Tran  Dustin, 2017, ARXIV170208896
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Zhang Meng, ADVERSARIAL TRAINING
   Zhu J Y, 2017, ARXIV170310593
NR 40
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402003
DA 2019-06-15
ER

PT S
AU Malach, E
   Shalev-Shwartz, S
AF Malach, Eran
   Shalev-Shwartz, Shai
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Decoupling "when to update" from "how to update"
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NOISE
AB Deep learning requires data. A useful approach to obtain data is to be creative and mine data from various sources, that were created for different purposes. Unfortunately, this approach often leads to noisy labels. In this paper, we propose a meta algorithm for tackling the noisy labels problem. The key idea is to decouple "when to update" from "how to update". We demonstrate the effectiveness of our algorithm by mining data for gender classification by combining the Labeled Faces in the Wild (LFW) face recognition dataset with a textual genderizing service, which leads to a noisy dataset. While our approach is very simple to implement, it leads to state-of-the-art results. We analyze some convergence properties of the proposed algorithm.
C1 [Malach, Eran; Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel.
RP Malach, E (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel.
EM eran.malach@mail.huji.ac.il; shais@cs.huji.ac.il
FU European Research Council (TheoryDL project)
FX This research is supported by the European Research Council (TheoryDL
   project).
CR Ando R. K., 2007, P 24 INT C MACH LEAR, P25
   Atlas L.E., 1989, ADV NEURAL INFORMATI, P566
   Awasthi P., 2014, STOC, P449
   Barandela R, 2000, LECT NOTES COMPUT SC, V1876, P621
   Bekker AJ, 2016, INT CONF ACOUST SPEE, P2682, DOI 10.1109/ICASSP.2016.7472164
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15
   Bootkrajang Jakramate, 2013, ARXIV13096818
   Brodersen Kay H, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3121, DOI 10.1109/ICPR.2010.764
   Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Flatow David, 2017, ROBUSTNESS CONVNETS
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Goldberger J., 2017, ICLR
   Grandvalet Y., 2004, NIPS, V17, P529
   Grandvalet  Y., 2006, SEMISUPERVISED LEARN, P151
   Huang G. B., 2007, 0749 U MASS
   Ipeirotis P. G., 2010, P ACM SIGKDD WORKSH, P64, DOI DOI 10.1145/1837885.1837906
   Kakar Pravin, 2015, MULT EXP ICME 2015 I, P1
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Larsen J, 1998, INT CONF ACOUST SPEE, P1205, DOI 10.1109/ICASSP.1998.675487
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Masek Philip, 2015, THESIS
   McDonald RA, 2003, LECT NOTES COMPUT SC, V2709, P35
   Menon Aditya Krishna, 2016, ARXIV160500751
   Mnih V., 2012, P 29 INT C MACH LEAR, P567
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Nigam K., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P86, DOI 10.1145/354756.354805
   Patrini G., 2016, ARXIV160202450
   Patrini Giorgio, 2016, ARXIV160903683
   Reed S., 2014, ARXIV14126596
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417
   Sukhbaatar S., 2014, ARXIV14062080
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Zhu Xiaojin, 2005, 1530 TR
   Zhuang Bohan, 2016, ARXIV161109960
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401001
DA 2019-06-15
ER

PT S
AU Mallasto, A
   Feragen, A
AF Mallasto, Anton
   Feragen, Aasa
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning from uncertain curves: The 2-Wasserstein metric for Gaussian
   processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID WASSERSTEIN; TRACTOGRAPHY; BARYCENTERS; DISTANCE
AB We introduce a novel framework for statistical analysis of populations of non-degenerate Gaussian processes (GPs), which are natural representations of uncertain curves. This allows inherent variation or uncertainty in function-valued data to be properly incorporated in the population analysis. Using the 2-Wasserstein metric we geometrize the space of GPs with L-2 mean and covariance functions over compact index spaces. We prove uniqueness of the barycenter of a population of GPs, as well as convergence of the metric and the barycenter of their finite-dimensional counterparts. This justifies practical computations. Finally, we demonstrate our framework through experimental validation on GP datasets representing brain connectivity and climate development. A MATLAB library for relevant computations will be published at https://sites.google.com/view/antonmallasto/software.
C1 [Mallasto, Anton; Feragen, Aasa] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
RP Mallasto, A (reprint author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
EM mallasto@di.ku.dk; aasa@di.ku.dk
RI Jeong, Yongwook/N-7413-2016
FU Centre for Stochastic Geometry and Advanced Bioimaging - Villum
   Foundation; McDonnell Center for Systems Neuroscience at Washington
   University
FX This research was supported by Centre for Stochastic Geometry and
   Advanced Bioimaging, funded by a grant from the Villum Foundation. Data
   were provided [in part] by the Human Connectome Project, WU-Minn
   Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil;
   1U54MH091657) funded by the 16 NIH Institutes and Centers that support
   the NIH Blueprint for Neuroscience Research; and by the McDonnell Center
   for Systems Neuroscience at Washington University. The authors would
   also like to thank Mads Nielsen for valuable discussions and
   supervision. Finally, the authors would like to thank Victor Panaretos
   for valuable discussions and, in particular, for pointing out an error
   in an earlier version of the manuscript.
CR Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741
   Aliprantis C., 1999, STUDIES EC THEORY, V4
   Alvarez-Esteban PC, 2011, ANN I H POINCARE-PR, V47, P358, DOI 10.1214/09-AIHP354
   Alvarez-Esteban PC, 2016, J MATH ANAL APPL, V441, P744, DOI 10.1016/j.jmaa.2016.04.045
   Ambrosio L., 2008, LECT MATH
   Ambrosio L, 2013, LECT NOTES MATH, V2062, P1, DOI 10.1007/978-3-642-32160-3_1
   Arveson W., 2006, SHORT COURSE SPECTRA, V209
   Berman J, 2009, MAGN RESON IMAGING C, V17, P205, DOI 10.1016/j.mric.2009.02.002
   Bulygina ON, 2012, DAILY TEMPERATURE PR
   CuestaAlbertos JA, 1996, J THEOR PROBAB, V9, P263, DOI 10.1007/BF02214649
   DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X
   Faraki M, 2015, INT CONF ACOUST SPEE, P1364, DOI 10.1109/ICASSP.2015.7178193
   GELBRICH M, 1990, MATH NACHR, V147, P185, DOI 10.1002/mana.19901470121
   GIVENS CR, 1984, MICH MATH J, V31, P231
   Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127
   Minh HQ, 2016, ADV COMPUT VIS PATT, P115, DOI 10.1007/978-3-319-45026-1_5
   Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132
   Hauberg S, 2015, LECT NOTES COMPUT SC, V9349, P597, DOI 10.1007/978-3-319-24553-9_73
   KNOTT M, 1984, J OPTIMIZ THEORY APP, V43, P39, DOI 10.1007/BF00934745
   Le M, 2015, LECT NOTES COMPUT SC, V9351, P38, DOI 10.1007/978-3-319-24574-4_5
   Masarotto  V., 2018, ARXIV180101990
   Masci J., 2015, P IEEE INT C COMP VI, P37
   Minh H.Q., 2014, ADV NEURAL INFORM PR, V27, P388
   OLKIN I, 1982, LINEAR ALGEBRA APPL, V48, P257, DOI 10.1016/0024-3795(82)90112-4
   Pigoli D, 2014, BIOMETRIKA, V101, P409, DOI 10.1093/biomet/asu008
   Pujol S, 2015, J NEUROIMAGING, V25, P875, DOI 10.1111/jon.12283
   Rajput B. S., 1972, J MULTIVARIATE ANAL, V2, P382
   Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550
   Schober M., 2014, ADV NEURAL INFORM PR, P739
   Schober M, 2014, LECT NOTES COMPUT SC, V8675, P265, DOI 10.1007/978-3-319-10443-0_34
   Seguy V, 2015, ADV NEURAL INFORM PR, P3312
   Sotiropoulos SN, 2013, MAGN RESON MED, V70, P1682, DOI 10.1002/mrm.24623
   Takatsu A, 2011, OSAKA J MATH, V48, P1005
   Tatusko RL, 1990, COOPERATION CLIMATE
   Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041
   Villani C., 2003, TOPICS OPTIMAL TRANS, V58
   Wassermann D, 2010, NEUROIMAGE, V51, P228, DOI 10.1016/j.neuroimage.2010.01.004
   Yang X, 2015, LECT NOTES COMPUT SC, V9350, P289, DOI 10.1007/978-3-319-24571-3_35
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405072
DA 2019-06-15
ER

PT S
AU Mariet, Z
   Sra, S
AF Mariet, Zelda
   Sra, Suvrit
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Elementary Symmetric Polynomials for Optimal Experimental Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHM; PERFORMANCE; DERIVATIVES
AB We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture "partial volumes" and offer a graded interpolation between the widely used A-optimal design and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy method. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.
C1 [Mariet, Zelda; Sra, Suvrit] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Mariet, Z (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM zelda@csail.mit.edu; suvrit@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1409802]; DARPA Fundamental Limits of Learning grant
   [W911NF-16-1-0551]
FX Suvrit Sra acknowledges support from NSF grant IIS-1409802 and DARPA
   Fundamental Limits of Learning grant W911NF-16-1-0551.
CR Atkinson  A., 2007, OXFORD STAT SCI SERI
   Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287
   BARNES ER, 1982, IBM J RES DEV, V26, P759, DOI 10.1147/rd.266.0759
   Bauschke HH, 2001, CAN J MATH, V53, P470, DOI 10.4153/CJM-2001-020-6
   Bhatia R, 2007, PRINC SER APPL MATH, P1
   Bhatia R., 1997, MATRIX ANAL
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939
   Cohn D, 1994, P ADV NEUR INF PROC, V6, P679
   Cominetti R, 2014, MATH PROGRAM COMPUT, V6, P151, DOI 10.1007/s12532-014-0066-y
   Davenport MA, 2016, IEEE T SIGNAL PROCES, V64, P5437, DOI 10.1109/TSP.2016.2597130
   Davis TA, 2016, ACM T MATH SOFTWARE, V42, DOI 10.1145/2828635
   Dette H, 2006, STAT SINICA, V16, P789
   Dolia AN, 2006, LECT NOTES COMPUT SC, V4212, P630
   Dolia E. N., 2004, P 7 INT C SIGN IM PR, P73
   ELFVING G, 1952, ANN MATH STAT, V23, P255, DOI 10.1214/aoms/1177729442
   Fedorov Valerii V., 1972, PROBABILITY MATH STA
   Gu YJ, 2013, NEURAL COMPUT APPL, V23, P2085, DOI 10.1007/s00521-012-1155-3
   He XF, 2010, IEEE T IMAGE PROCESS, V19, P254, DOI 10.1109/TIP.2009.2032342
   Horel T., 2014, BUDGET FEASIBLE MECH, P719
   Horn R. A., 1985, MATRIX ANAL
   Jackson DA, 2004, ENVIRONMETRICS, V15, P129, DOI 10.1002/env.628
   Jain T, 2011, LINEAR ALGEBRA APPL, V435, P1111, DOI 10.1016/j.laa.2011.02.026
   Jozsa R, 2015, J MATH PHYS, V56, DOI 10.1063/1.4922317
   Khuri AI, 2006, STAT SCI, V21, P376, DOI 10.1214/088342306000000105
   KIEFER J, 1975, BIOMETRIKA, V62, P277, DOI 10.2307/2335363
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Lewis AS, 1996, MATH OPER RES, V21, P576, DOI 10.1287/moor.21.3.576
   Liu ZK, 2016, COMPUTATIONAL THERMODYNAMICS OF MATERIALS, DOI 10.1017/CBO9781139018265
   Macdonald I. G., 1998, SYMMETRIC FUNCTIONS
   MILLER AJ, 1994, APPL STAT-J ROY ST C, V43, P669, DOI 10.2307/2986264
   MUIR WW, 1974, P EDINBURGH MATH SOC, V19, P109, DOI 10.1017/S001309150001021X
   Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109
   Rousseeuw P. J., 1987, ROBUST REGRESSION OU
   Sagnol G., 2010, THESIS
   Lasheras FS, 2010, MATH COMPUT MODEL, V52, P1169, DOI 10.1016/j.mcm.2010.03.007
   Schein A., 2004, A OPTIMALITY ACTIVE
   Shpilka A, 2001, COMPUT COMPLEX, V10, P1, DOI 10.1007/PL00001609
   SILVEY SD, 1978, COMMUN STAT A-THEOR, V7, P1379, DOI 10.1080/03610927808827719
   Smith J. D., 2017, CORR
   Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168
   Sun P, 2004, OPER RES, V52, P690, DOI 10.1287/opre.1040.0115
   Todd MJ, 2016, MOS-SIAM SER OPTIMIZ, P1, DOI 10.1137/1.9781611974386
   Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430
   Wang ZF, 2016, J COMB OPTIM, V31, P29, DOI 10.1007/s10878-014-9707-3
   Xygkis T. C., 2016, IEEE T SMART GRID
   Yeh IC, 1998, CEMENT CONCRETE RES, V28, P1797, DOI 10.1016/S0008-8846(98)00165-3
   Yu YM, 2010, ANN STAT, V38, P1593, DOI 10.1214/09-AOS761
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402019
DA 2019-06-15
ER

PT S
AU Mattila, R
   Rojas, CR
   Krishnamurthy, V
   Wahlberg, B
AF Mattila, Robert
   Rojas, Cristian R.
   Krishnamurthy, Vikram
   Wahlberg, Bo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inverse Filtering for Hidden Markov Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper considers a number of related inverse filtering problems for hidden Markov models (HMMs). In particular, given a sequence of state posteriors and the system dynamics; i) estimate the corresponding sequence of observations, ii) estimate the observation likelihoods, and iii) jointly estimate the observation likelihoods and the observation sequence. We show how to avoid a computationally expensive mixed integer linear program (MILP) by exploiting the algebraic structure of the HMM filter using simple linear algebra operations, and provide conditions for when the quantities can be uniquely reconstructed. We also propose a solution to the more general case where the posteriors are noisily observed. Finally, the proposed inverse filtering algorithms are evaluated on real-world polysomnographic data used for automatic sleep segmentation.
C1 [Mattila, Robert; Rojas, Cristian R.; Wahlberg, Bo] KTH Royal Inst Technol, Dept Automat Control, Stockholm, Sweden.
   [Krishnamurthy, Vikram] Cornell Univ, Cornell Tech, Ithaca, NY 14853 USA.
RP Mattila, R (reprint author), KTH Royal Inst Technol, Dept Automat Control, Stockholm, Sweden.
EM rmattila@kth.se; crro@kth.se; vikramk@cornell.edu; bo@kth.se
RI Jeong, Yongwook/N-7413-2016
FU Swedish Research Council [2016-06079]; U.S. Army Research Office
   [12346080]; National Science Foundation [1714180]
FX This work was partially supported by the Swedish Research Council under
   contract 2016-06079, the U.S. Army Research Office under grant 12346080
   and the National Science Foundation under grant 1714180. The authors
   would like to thank Alexandre Proutiere for helpful comments during the
   preparation of this work.
CR Anderson B. D. O., 1979, OPTIMAL FILTERING
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Caplin A, 2015, AM ECON REV, V105, P2183, DOI 10.1257/aer.20140117
   Cappe O., 2005, SPR S STAT
   Chen  Jie, 1999, ROBUST MODEL BASED F
   Chen Y, 2015, IEEE ENG MED BIO, P530, DOI 10.1109/EMBC.2015.7318416
   Choi Jaedeug, 2012, ADV NEURAL INFORM PR
   Elliott RJ, 1995, HIDDEN MARKOV MODELS
   Flexer A, 2002, APPL ARTIF INTELL, V16, P199, DOI 10.1080/088395102753559271
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Gustafsson  F., 2000, ADAPTIVE FILTERING C
   Hadfield-Menell D., 2016, ADV NEURAL INFORM PR
   Hornik K, 2012, J STAT SOFTW, V50, P1
   Kalman R.K., 1964, Transactions of the ASME. Series D, Journal of Basic Engineering, V86, P51
   Klein E., 2012, ADV NEURAL INFORM PR
   Koller D., 2009, PROBABILISTIC GRAPHI
   Krishnamurthy V., 2016, PARTIALLY OBSERVED M
   Levine Sergey, 2011, ADV NEURAL INFORM PR
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Pan ST, 2012, BIOMED ENG ONLINE, V11, DOI 10.1186/1475-925X-11-52
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Terzano MG, 2001, SLEEP MED, V2, P537, DOI 10.1016/S1389-9457(01)00149-6
   Varian H.R., 1992, MICROECONOMIC ANAL
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404027
DA 2019-06-15
ER

PT S
AU Mazumdar, A
   Saha, B
AF Mazumdar, Arya
   Saha, Barna
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Query Complexity of Clustering with Side Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Suppose, we are given a set of n elements to be clustered into k (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, "do two elements u and v belong to the same cluster?". The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. However, obtaining an ideal similarity function is extremely challenging due to ambiguity in data representation, poor data quality etc., and this is one of the primary reasons that makes clustering hard. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution f(+) when the underlying pair of elements belong to the same cluster, and from some f(-) otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from circle minus(nk) (no similarity matrix) to O(k(2)log n/H-2(f(+)parallel to f(-)) where H-2 denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an O(log n) factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of k, f(+) and f(-), and only depend logarithmically with n. Our lower bounds could be of independent interest, and provide a general framework for proving lower bounds for classification problems in the interactive setting. Along the way, our work also reveals intriguing connection to popular community detection models such as the stochastic block model and opens up many avenues for interesting future research.
C1 [Mazumdar, Arya; Saha, Barna] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Mazumdar, A (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM arya@cs.umass.edu; barna@cs.umass.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303]; Yahoo ACE
   Award; Google Faculty Research Award
FX This work is supported in part by NSF awards CCF 1642658, CCF 1642550,
   CCF 1464310, CCF 1652303, a Yahoo ACE Award and a Google Faculty
   Research Award. We are particularly thankful to an anonymous reviewer
   whose comments led to notable improvement of the presentation of the
   paper.
CR Abbe E., 2015, ADV NEURAL INFORM PR, P676
   Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Ajtai M, 1986, P ACM S THEORY COMPU, P188
   Ashtiani H., 2016, NIPS
   Awasthi P., 2014, ICML, P550
   Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27
   BOLLOBAS B, 1990, SIAM J DISCRETE MATH, V3, P21, DOI 10.1137/0403003
   Chaudhuri K., 2012, J MACHINE LEARNING R, V23, P35
   Chen Y., 2016, P 33 INT C MACH LEAR, V48, P689
   Chin  P., 2015, ARXIV150105021
   Dalvi N. N., 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414
   Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1
   FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877
   FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061
   Firmani D, 2016, PROC VLDB ENDOW, V9, P384
   Gadde A, 2016, IEEE INT SYMP INFO, P1889, DOI 10.1109/ISIT.2016.7541627
   Getoor L, 2012, PROC VLDB ENDOW, V5, P2018, DOI 10.14778/2367502.2367564
   Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599
   Gokhale C, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P601
   Guntuboyina A, 2011, IEEE T INFORM THEORY, V57, P2386, DOI 10.1109/TIT.2011.2110791
   Hajek B. E., 2015, P 28 C LEARN THEOR C, P899
   Hajek B, 2016, IEEE T INFORM THEORY, V62, P2788, DOI 10.1109/TIT.2016.2546280
   HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943
   Hassibi Babak, 2016, ADV NEURAL INFORM PR, P1316
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Karger D. R., 2011, NIPS, V24, P1953
   Kopcke H, 2010, PROC VLDB ENDOW, V3, P484
   Lim Shiau Hong, 2014, ADV NEURAL INFORM PR, P1188
   Mazumdar A., 2016, ARXIV160401839
   Mazumdar A., 2017, ADV NEURAL INFORM PR, V31
   Mazumdar A., 2017, 31 AAAI C ART INT AA
   Mossel E., 2015, P 47 ANN ACM S THEOR, P69
   Polyanskiy Y, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1327, DOI 10.1109/ALLERTON.2010.5707067
   Sason I, 2016, IEEE T INFORM THEORY, V62, P5973, DOI 10.1109/TIT.2016.2603151
   Shi F, 2013, 2013 INTERNATIONAL CONFERENCE ON MANAGEMENT AND INFORMATION TECHNOLOGY, P299
   Verroios Vasilis, 2015, 2015 IEEE 31st International Conference on Data Engineering (ICDE), P219, DOI 10.1109/ICDE.2015.7113286
   Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982
   Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404073
DA 2019-06-15
ER

PT S
AU Mazumdar, A
   Pal, S
AF Mazumdar, Arya
   Pal, Soumyabrata
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Semisupervised Clustering, AND-Queries and Locally Encodable Source
   Coding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECOVERY
AB Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number Delta of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise 'same cluster' queries - and propose pairwise AND queries, that provably performs better in many situations.
C1 [Mazumdar, Arya; Pal, Soumyabrata] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Mazumdar, A (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM arya@cs.umass.edu; soumyabratap@umass.edu
FU NSF [CCF-BSF 1618512, CCF 1642550]; NSF CAREER Award [CCF 1642658]
FX This research is supported in parts by NSF Awards CCF-BSF 1618512, CCF
   1642550 and an NSF CAREER Award CCF 1642658. The authors thank Barna
   Saha for many discussions on the topics of this paper. The authors also
   thank the volunteers who participated in the crowdsourcing experiments
   for this paper.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Ahn K, 2016, ANN ALLERTON CONF, P657, DOI 10.1109/ALLERTON.2016.7852294
   Alon N, 2004, PROBABILISTIC METHOD
   Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216
   Buhrman H, 2002, SIAM J COMPUT, V31, P1723, DOI 10.1137/S0097539702405292
   Chandar V. B., 2010, THESIS
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Firmani D, 2016, PROC VLDB ENDOW, V9, P384
   Gruenheid A., 2015, ARXIV151200537
   Gruenheid A., 2015, CORR
   Hajek B, 2016, IEEE T INFORM THEORY, V62, P5918, DOI 10.1109/TIT.2016.2594812
   Hassibi Babak, 2016, ADV NEURAL INFORM PR, P1316
   Karger D. R., 2011, NIPS, V24, P1953
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Lahouti F., 2016, ADV NEURAL INFORM PR, P5059
   Liu Q., 2012, ADV NEURAL INFORM PR, P692
   Makhdoumi A, 2015, IEEE ICC, P4394, DOI 10.1109/ICC.2015.7249014
   Massey J. L., 1977, TECHNICAL REPORT
   Mazumdar A., 2017, THIRT 1 AAAI C ART I
   Mazumdar A., 2017, ADV NEURAL INFORM PR
   Mazumdar A, 2015, IEEE INT SYMP INFO, P2984, DOI 10.1109/ISIT.2015.7283004
   Mazumdar A, 2014, IEEE J SEL AREA COMM, V32, P976, DOI 10.1109/JSAC.2014.140517
   Montanari A, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-6, P2474, DOI 10.1109/ISIT.2008.4595436
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Pananjady A, 2015, IEEE INT SYMP INFO, P2979, DOI 10.1109/ISIT.2015.7283003
   Patrascu M, 2008, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2008.83
   Prelec D, 2017, NATURE, V541, P532, DOI 10.1038/nature21054
   Vempaty A, 2014, IEEE J-STSP, V8, P667, DOI 10.1109/JSTSP.2014.2316116
   Verroios Vasilis, 2015, 2015 IEEE 31st International Conference on Data Engineering (ICDE), P219, DOI 10.1109/ICDE.2015.7113286
   Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982
   Viola E, 2012, SIAM J COMPUT, V41, P1593, DOI 10.1137/090766619
   Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263
   Zhou D., 2012, ADV NEURAL INFORM PR, P2195
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406054
DA 2019-06-15
ER

PT S
AU Mazumdar, A
   Saha, B
AF Mazumdar, Arya
   Saha, Barna
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Clustering with Noisy Queries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECOVERY
AB In this paper, we provide a rigorous theoretical study of clustering with noisy queries. Given a set of n elements, our goal is to recover the true clustering by asking minimum number of pairwise queries to an oracle. Oracle can answer queries of the form "do elements u and v belong to the same cluster?"-the queries can be asked interactively (adaptive queries), or non-adaptively up-front, but its answer can be erroneous with probability p. In this paper, we provide the first information theoretic lower bound on the number of queries for clustering with noisy oracle in both situations. We design novel algorithms that closely match this query complexity lower bound, even when the number of clusters is unknown. Moreover, we design computationally efficient algorithms both for the adaptive and non-adaptive settings. The problem captures/generalizes multiple application scenarios. It is directly motivated by the growing body of work that use crowdsourcing for entity resolution, a fundamental and challenging data mining task aimed to identify all records in a database referring to the same entity. Here crowd represents the noisy oracle, and the number of queries directly relates to the cost of crowdsourcing. Another application comes from the problem of sign edge prediction in social network, where social interactions can be both positive and negative, and one must identify the sign of all pair-wise interactions by querying a few pairs. Furthermore, clustering with noisy oracle is intimately connected to correlation clustering, leading to improvement therein. Finally, it introduces a new direction of study in the popular stochastic block model where one has an incomplete stochastic block model matrix to recover the clusters.
C1 [Mazumdar, Arya; Saha, Barna] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Mazumdar, A (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM arya@cs.umass.edu; barna@cs.umass.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303]; Yahoo ACE
   Award; Google Faculty Research Award
FX This work is supported in parts by NSF awards CCF 1642658, CCF 1642550,
   CCF 1464310, CCF 1652303, a Yahoo ACE Award and a Google Faculty
   Research Award. The authors are thankful to an anonymous reviewer whose
   comments led to many improvements in the presentation. The authors would
   also like to thank Sanjay Subramanian for his help with the experiments.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Ailon N., 2013, P 30 INT C MACH LEAR, P995
   Ashtiani H., 2016, NIPS
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95
   BRAVERMAN M., 2009, CORR
   Braverman M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P268
   Brzozowski MJ, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P817
   Burke M, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P27
   CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277, DOI 10.1037/h0046049
   Cesa-Bianchi N., 2012, ANN C LEARN THEOR MI, P34
   Chaudhuri K., 2012, J MACHINE LEARNING R, V23, P35
   Chen Y., 2016, P 33 INT C MACH LEAR, V48, P689
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Chen Yudong, 2012, ADV NEURAL INFORM PR, V25, P2204
   Chiang KY, 2014, J MACH LEARN RES, V15, P1177
   Chin  P., 2015, ARXIV150105021
   Christen  P., 2012, DATA MATCHING CONCEP
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dalvi N. N., 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1
   Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581
   FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877
   FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061
   Firmani D, 2016, PROC VLDB ENDOW, V9, P384
   Getoor L, 2012, PROC VLDB ENDOW, V5, P2018, DOI 10.14778/2367502.2367564
   Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599
   Gokhale C, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P601
   Gruenheid A., 2015, CORR
   Hajek B, 2016, IEEE T INFORM THEORY, V62, P5918, DOI 10.1109/TIT.2016.2594812
   HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943
   Harary F., 1953, MICH MATH J, V2, P143, DOI DOI 10.1307/MMJ/1028989917
   Hassibi Babak, 2016, ADV NEURAL INFORM PR, P1316
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Karger D. R., 2011, NIPS, V24, P1953
   Kleinberg R., 2007, LECT NOTES LEARNING
   Lampe C. A., 2007, P SIGCHI C HUM FACT, P1253
   Lampe C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1253
   Larsen MD, 2001, J AM STAT ASSOC, V96, P32, DOI 10.1198/016214501750332956
   Leskovec J., 2010, P 19 INT C WORLD WID, P641, DOI DOI 10.1145/1772690.1772756
   Lim Shiau Hong, 2014, ADV NEURAL INFORM PR, P1188
   Makarychev K., 2015, COLT, P1321
   Mathieu C, 2010, PROC APPL MATH, V135, P712
   Mazumdar A., 2016, ARXIV160401839
   Mazumdar A., 2017, ADV NEURAL INFORM PR, P31
   Mazumdar A., 2017, 31 AAAI C ART INT AA
   Mitzenmacher M., 2016, CORR
   Mossel E., 2015, P 47 ANN ACM S THEOR, P69
   Polyanskiy Y, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1327, DOI 10.1109/ALLERTON.2010.5707067
   Prelec D, 2017, NATURE, V541, P532, DOI 10.1038/nature21054
   Verroios Vasilis, 2015, 2015 IEEE 31st International Conference on Data Engineering (ICDE), P219, DOI 10.1109/ICDE.2015.7113286
   Verroios V., 2017, SIGMOD, P219
   Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982
   Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263
NR 57
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405084
DA 2019-06-15
ER

PT S
AU McAllister, RT
   Rasmussen, CE
AF McAllister, Rowan Thomas
   Rasmussen, Carl Edward
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Data-Efficient Reinforcement Learning in Continuous State-Action
   Gaussian-POMDPs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present a data-efficient reinforcement learning method for continuous state-action systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.
C1 [McAllister, Rowan Thomas; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.
RP McAllister, RT (reprint author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.
EM rtm26@cam.ac.uk; cer54@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Candela JQ, 2003, INT CONF ACOUST SPEE, P701
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Dallaire P, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2604, DOI 10.1109/IROS.2009.5354013
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Deisenroth Marc, 2012, EUR WORKSH REINF LEA
   Duff M., 2002, THESIS
   Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x
   Lillicrap T P, 2015, ARXIV150902971
   McHutchon A., 2014, THESIS
   Poupart P., 2006, P 23 INT C MACH LEAR, V148, P697, DOI DOI 10.1145/1143844.1143932
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ross S, 2008, IEEE INT CONF ROBOT, P2845, DOI 10.1109/ROBOT.2008.4543641
   van den Berg Jur, 2012, ASS ADVANCEMENT ARTI
   Webb DJ, 2014, IEEE INT CONF ROBOT, P5998, DOI 10.1109/ICRA.2014.6907743
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402009
DA 2019-06-15
ER

PT S
AU McCann, B
   Bradbury, J
   Xiong, CM
   Socher, R
AF McCann, Bryan
   Bradbury, James
   Xiong, Caiming
   Socher, Richard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learned in Translation: Contextualized Word Vectors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.
EM bmccann@salesforce.com; james.bradbury@salesforce.com;
   cxiong@salesforce.com; rsocher@salesforce.com
RI Jeong, Yongwook/N-7413-2016
CR Agirre E., 2014, SEMEVAL COLING
   Bandanau D., 2015, ICLR
   Bowman S. R., 2014, CORR
   Bowman S. R., 2015, P 2015 C EMP METH NA
   Cettolo M., 2015, IWSLT
   Chen Q., 2016, CORR
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Conneau  A., 2017, ARXIV170502364
   Dai A. M., 2015, NIPS
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dieng A. B., 2016, CORR
   Dong L., 2016, CORR
   Fukui A., 2016, ARXIV160601847
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I, 2013, ICML
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guo H., 2017, CORR
   Hashimoto K., 2016, CORR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hill F., 2016, HLT NAACL
   Hill F, 2017, MACH TRANSL, V31, P3, DOI 10.1007/s10590-017-9194-2
   Huang ML, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3052770
   Ioffe S., 2015, ICML
   Johnson R., 2016, ICML
   Kiros Ryan, 2015, NIPS
   Klein G., 2017, ARXIV E PRINTS
   Koehn P., 2007, ACL
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kumar A., 2016, ICML
   Loni B., 2011, TSD
   Looks M., 2017, CORR
   Lu J., 2016, ARXIV161201887
   Luong  T., 2015, EMNLP
   Maas A. L., 2011, P 49 ANN M ASS COMP, V1, P142
   Madabushi H. T., 2016, COLING
   Mikolov Tomas, 2013, ICLR WORKSH
   Min S., 2017, QUESTION ANSWERING T
   Miyato T., 2017, ADVERSARIAL TRAINING
   Mou L., 2015, EMNLP
   Munkhdalai Tsendsuren, 2016, CORR
   Nair V., 2010, ICML
   Nallapati Ramesh, 2016, CONLL
   Paria B., 2016, CORR
   Parikh Ankur P., 2016, EMNLP
   Pennington Jeffrey, 2014, EMNLP
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Radford A., 2017, CORR
   Rajpurkar P., 2016, ARXIV160605250
   Ramachandran P., 2016, CORR
   Saenko K., 2010, ECCV
   Seo M., 2017, ICLR
   Sha L., 2016, COLING
   Silva J, 2011, ARTIF INTELL REV, V35, P137, DOI 10.1007/s10462-010-9188-4
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Socher R., 2014, ACL
   Socher R., 2013, EMNLP
   Specia L., 2016, WMT
   Sutskever  I., 2014, NIPS
   Van-Tu N., 2016, INDIAN J SCI TECHNOL, V9
   Voorhees E. M., 1999, TEXT RETR C, V1999, P82
   Wang S., 2017, MACHINE COMPREHENSIO
   Wang W., 2017, GATED SELF MATCHING
   Wieting John, 2016, ICLR
   Xin Li, 2006, Natural Language Engineering, P229, DOI 10.1017/S1351324905003955
   Xiong, 2016, P INT C MACH LEARN, P2397
   Xiong C., 2017, ICRL
   Yu Y., 2017, ICLR
   Zhang R., 2016, HLT NAACL
   Zhou Peng, 2016, COLING
   Zhu Y., 2011, AAAI
NR 70
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406036
DA 2019-06-15
ER

PT S
AU McInerney, J
AF McInerney, James
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI An Empirical Bayes Approach to Optimizing Machine Learning Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB There is rapidly growing interest in using Bayesian optimization to tune model and inference hyperparameters for machine learning algorithms that take a long time to run. For example, Spearmint is a popular software package for selecting the optimal number of layers and learning rate in neural networks. But given that there is uncertainty about which hyperparameters give the best predictive performance, and given that fitting a model for each choice of hyperparameters is costly, it is arguably wasteful to "throw away" all but the best result, as per Bayesian optimization. A related issue is the danger of overfitting the validation data when optimizing many hyperparameters. In this paper, we consider an alternative approach that uses more samples from the hyperparameter selection procedure to average over the uncertainty in model hyperparameters. The resulting approach, empirical Bayes for hyperparameter averaging (EB-Hyp) predicts held-out data better than Bayesian optimization in two experiments on latent Dirichlet allocation and deep latent Gaussian models. EB-Hyp suggests a simpler approach to evaluating and deploying machine learning algorithms that does not require a separate validation data set and hyperparameter selection procedure.
C1 [McInerney, James] Spotify Res, 45 W 18th St,7th Floor, New York, NY 10011 USA.
RP McInerney, J (reprint author), Spotify Res, 45 W 18th St,7th Floor, New York, NY 10011 USA.
EM jamesm@spotify.com
RI Jeong, Yongwook/N-7413-2016
CR Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brochu E., 2010, ARXIV10122599
   Carlin BP, 2000, J AM STAT ASSOC, V95, P1286, DOI 10.2307/2669771
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Choi T., 2004, POSTERIOR CONSISTENC
   EFRON B, 1972, J AM STAT ASSOC, V67, P130, DOI 10.2307/2284711
   Freund Y., 1999, J JAPANESE SOC ARTIF, V14, P1612
   GPy, 2012, GPY GAUSS PROC FRAM
   Hensman J, 2013, ARXIV13096835
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Huang G. B., 2007, 0749 U MASS
   Li C., 2015, P INT C ADV NEUR INF, P1837
   Murphy KP, 2012, MACHINE LEARNING PRO
   Osborne M, 2012, ADV NEURAL INFORM PR, V25, P46
   Osborne M, 2010, THESIS
   Rasmussen C., 2006, GAUSSIAN PROCESSES M, V2, P4
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Robbins H., 1955, H ROBBINS SELECTED P, P49
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Stein M. L., 1999, INTERPOLATION SPATIA
   Swersky K, 2014, ARXIV14063896
   Van Der Wart AW, 2008, ANN STAT, V36, P1435, DOI 10.1214/009053607000000613
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402074
DA 2019-06-15
ER

PT S
AU Medina, AM
   Vassilvitskii, S
AF Medina, Andres Munoz
   Vassilvitskii, Sergei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Revenue Optimization with Approximate Bid Predictions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REGRET
AB In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types, and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.
C1 [Medina, Andres Munoz; Vassilvitskii, Sergei] Google Res, 76 9th Ave, New York, NY 10011 USA.
RP Medina, AM (reprint author), Google Res, 76 9th Ave, New York, NY 10011 USA.
RI Jeong, Yongwook/N-7413-2016
CR Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772
   Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243
   Cole Richard, 2015, ABS150200963 CORR
   Devanur NR, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P426, DOI 10.1145/2897518.2897553
   Dhangwatnotai P, 2015, GAME ECON BEHAV, V91, P318, DOI 10.1016/j.geb.2014.03.011
   Goldberg AV, 2001, SIAM PROC S, P735
   Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   Leme RP, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1093, DOI 10.1145/2872427.2883071
   Medina Andres Munoz, 2014, P 31 INT C MACH LEAR, P262
   Mohri M., 2012, FDN MACHINE LEARNING
   Morgenstern Jamie, 2016, P 29 ANN C LEARN THE, P1298
   Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR, P136
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P601
   Rudolph MR, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1113, DOI 10.1145/2872427.2883051
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401086
DA 2019-06-15
ER

PT S
AU Mei, HY
   Eisner, J
AF Mei, Hongyuan
   Eisner, Jason
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point
   Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Many events occur in the world. Some event types are stochastically excited or inhibited-in the sense of having their probabilities elevated or decreased-by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.
C1 [Mei, Hongyuan; Eisner, Jason] Johns Hopkins Univ, Dept Comp Sci, 3400 N Charles St, Baltimore, MD 21218 USA.
RP Mei, HY (reprint author), Johns Hopkins Univ, Dept Comp Sci, 3400 N Charles St, Baltimore, MD 21218 USA.
EM hmei@cs.jhu.edu; jason@cs.jhu.edu
RI Jeong, Yongwook/N-7413-2016
CR Chelba Ciprian, 2013, ARXIV13123005 COMP R
   Choi E, 2015, IEEE DATA MINING, P721, DOI 10.1109/ICDM.2015.144
   Du N., 2015, ADV NEURAL INFORM PR, P3492
   Du N., 2016, P 22 ACM SIGKDD INT, P1555, DOI DOI 10.1145/2939672.2939875
   Du Nan, 2015, P 21 ACM SIGKDD INT, P219, DOI DOI 10.1145/2783258.2783411
   Etesami Jalal, 2016, ARXIV160304319
   Gomez-Rodriguez M., 2013, P 6 ACM INT C WEB SE, P23, DOI DOI 10.1145/2433396.2433402
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI 10.1007/978-3-642-24797-2
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Guo F., 2015, P 18 INT C ART INT S, P315
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   He X., 2015, P 32 INT C MACH LEAR, P871
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Karpathy A., 2015, ARXIV150602078
   Kingma D., 2015, P INT C LEARN REPR I
   Lee Young, 2016, P INT C MACH LEARN I
   LEWIS PAW, 1979, NAV RES LOG, V26, P403, DOI 10.1002/nav.3800260304
   Liniger Thomas Josef, 2009, THESIS EIDGENOSSISCH
   Lukasik M., 2016, P 54 ANN M ASS COMP, P393
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Palm Conny, 1943, ERICSSON TECHNICS
   Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057
   Sundermeyer  M., 2012, P INTERSPEECH
   Wang Yichen, 2016, P INT C MACH LEARN I
   Xiao S, 2017, ADV NEUR IN, V30
   Xiao Shuai, 2017, ARXIV170308524
   Xu H, 2016, 2016 IEEE/ACES INTERNATIONAL CONFERENCE ON WIRELESS INFORMATION TECHNOLOGY AND SYSTEMS (ICWITS) AND APPLIED COMPUTATIONAL ELECTROMAGNETICS (ACES)
   Yang S. H., 2013, P 30 INT C MACH LEAR, V28, P1
   Zaidan O. F., 2008, P C EMP METH NAT LAN, P31
   Zhao Q., 2015, P 21 ACM SIGKDD INT, P1513, DOI DOI 10.1145/2783258.2783401
   Zhou Ke, 2013, P 30 INT C MACH LEAR, V28, P1301
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406079
DA 2019-06-15
ER

PT S
AU Mensch, A
   Mairal, J
   Bzdok, D
   Thirion, B
   Varoquaux, G
AF Mensch, Arthur
   Mairal, Julien
   Bzdok, Danilo
   Thirion, Bertrand
   Varoquaux, Gael
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Neural Representations of Human Cognition across Many fMRI
   Studies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PARCELLATION; FUTURE; BRAIN
AB Cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets. It opens the door to large-scale statistical models. Finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge: how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks? We cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies. For this, we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli. Our multi-dataset classification model achieves the best prediction performance on several large reference datasets, compared to models without cognitive-aware low-dimension representations; it brings a substantial performance boost to the analysis of small datasets, and can be introspected to identify universal template cognitive concepts.
C1 [Mensch, Arthur; Mairal, Julien; Thirion, Bertrand; Varoquaux, Gael] INRIA, Rocquencourt, France.
   [Bzdok, Danilo] Rhein Westfal TH Aachen, Dept Psychiat, Aachen, Germany.
   [Mensch, Arthur; Thirion, Bertrand; Varoquaux, Gael] Univ Paris Saclay, CEA, INRIA, F-91191 Gif Sur Yvette, France.
   [Mairal, Julien] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LJK, F-38000 Grenoble, France.
RP Mensch, A (reprint author), INRIA, Rocquencourt, France.; Mensch, A (reprint author), Univ Paris Saclay, CEA, INRIA, F-91191 Gif Sur Yvette, France.
EM arthur.mensch@m4x.org; julien.mairal@inria.fr;
   danilo.bzdok@rwth-aachen.de; bertrand.thirion@inria.fr;
   gael.varoquaux@inria.fr
RI Jeong, Yongwook/N-7413-2016
FU European Union's Horizon 2020 Framework Programme for Research and
   Innovation [720270]; ERC grant SOLARIS [714381]; ANR
   [ANR-14-CE23-0003-01]
FX This project has received funding from the European Union's Horizon 2020
   Framework Programme for Research and Innovation under grant agreement No
   720270 (Human Brain Project SGA1). Julien Mairal was supported by the
   ERC grant SOLARIS (No 714381) and a grant from ANR (MACARON project
   ANR-14-CE23-0003-01). We thank Olivier Grisel for his most helpful
   insights.
CR Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], 2013, ADV NEURAL INF PROCE
   Barrett LF, 2009, PERSPECT PSYCHOL SCI, V4, P326, DOI 10.1111/j.1745-6924.2009.01134.x
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Blumensath T, 2013, NEUROIMAGE, V76, P313, DOI 10.1016/j.neuroimage.2013.03.024
   Bugden S, 2012, DEV COGN NEUROS-NETH, V2, P448, DOI 10.1016/j.dcn.2012.04.001
   Bzdok D., 2015, ADV NEURAL INFORM PR, P3348
   Bzdok D, 2017, NEUROIMAGE, V155, P549, DOI 10.1016/j.neuroimage.2017.04.061
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Donahue J., 2014, P INT C MACH LEARN, P647
   Eickhoff SB, 2015, HUM BRAIN MAPP, V36, P4771, DOI 10.1002/hbm.22933
   Gramfort A, 2013, INT WORKSHOP PATTERN, P17, DOI 10.1109/PRNI.2013.14
   Kingma D. P., 2015, INT C LEARN REPR
   Koyejo Oluwasanmi, 2013, NIPS WORKSH MACH LEA, P5
   Laird AR, 2005, NEUROINFORMATICS, V3, P65, DOI 10.1385/NI:3:1:065
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Medaglia JD, 2015, J COGNITIVE NEUROSCI, V27, P1471, DOI 10.1162/jocn_a_00810
   Mensch A., 2016, P INT C MACH LEARN, P1737
   Mensch Arthur, 2017, IEEE T SIGNAL PROCES, V99
   Newell A., 1973, YOU CANT PLAY 20 QUE
   Orfanos DP, 2017, NEUROIMAGE, V144, P309, DOI 10.1016/j.neuroimage.2015.09.052
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pinel P, 2007, BMC NEUROSCI, V8, DOI 10.1186/1471-2202-8-91
   Poldrack RA, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.110
   Poldrack RA, 2017, NAT REV NEUROSCI, V18, P115, DOI 10.1038/nrn.2016.167
   Rubin Timothy, 2016, ADV NEURAL INF PROCE, P1118
   Salimi-Khorshidi G, 2009, NEUROIMAGE, V45, P810, DOI 10.1016/j.neuroimage.2008.12.039
   Shafto MA, 2014, BMC NEUROL, V14, DOI 10.1186/s12883-014-0204-1
   Srebro N., 2004, ADV NEURAL INFORM PR, V17, P1329
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Thirion B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00167
   Turner JA, 2012, NEUROINFORMATICS, V10, P57, DOI 10.1007/s12021-011-9126-x
   Van Essen DC, 2012, NEUROIMAGE, V62, P2222, DOI 10.1016/j.neuroimage.2012.02.018
   Wager Stefan, 2013, ADV NEURAL INFORM PR, P351
   Wager TD, 2013, NEW ENGL J MED, V368, P1388, DOI 10.1056/NEJMoa1204471
   Xue Y, 2007, J MACH LEARN RES, V8, P35
   Yarkoni T, 2011, NAT METHODS, V8, P665, DOI [10.1038/NMETH.1635, 10.1038/nmeth.1635]
   2009, PSYCHOL SCI, V20, P1364
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405093
DA 2019-06-15
ER

PT S
AU Merdivan, E
   Loghmani, MR
   Geist, M
AF Merdivan, Erinc
   Loghmani, Mohammad Reza
   Geist, Matthieu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Reconstruct & Crush Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This article introduces an energy-based model that is adversarial regarding data: it minimizes the energy for a given data distribution (the positive samples) while maximizing the energy for another given data distribution (the negative or unlabeled samples). The model is especially instantiated with autoencoders where the energy, represented by the reconstruction error, provides a general distance measure for unknown data. The resulting neural network thus learns to reconstruct data from the first distribution while crushing data from the second distribution. This solution can handle different problems such as Positive and Unlabeled (PU) learning or covariate shift, especially with imbalanced data. Using autoencoders allows handling a large variety of data, such as images, text or even dialogues. Our experiments show the flexibility of the proposed approach in dealing with different types of data in different settings: images with CIFAR-10 and CIFAR-100 (not-in-training setting), text with Amazon reviews (PU learning) and dialogues with Facebook bAbI (next response classification and dialogue completion).
C1 [Merdivan, Erinc] AIT Austrian Inst Technol GmbH, Vienna, Austria.
   [Merdivan, Erinc] Univ Lorraine, LORIA, F-57070 Metz, France.
   [Merdivan, Erinc] Univ Paris Saclay, Cent Supelec, CNRS, F-57070 Metz, France.
   [Loghmani, Mohammad Reza] TU Wien, Vision4Robot Lab, ACIN, Vienna, Austria.
   [Geist, Matthieu] Univ Lorraine, F-57070 Metz, France.
   [Geist, Matthieu] CNRS, LIEC, UMR 7360, F-57070 Metz, France.
RP Merdivan, E (reprint author), AIT Austrian Inst Technol GmbH, Vienna, Austria.; Merdivan, E (reprint author), Univ Lorraine, LORIA, F-57070 Metz, France.; Merdivan, E (reprint author), Univ Paris Saclay, Cent Supelec, CNRS, F-57070 Metz, France.
EM erinc.merdivan@ait.ac.at; loghmani@acin.tuwien.ac.at;
   matthieu.geist@univ-lorraine.fr
FU European Union Horizon2020 MSCA ITN ACROSSING project [616757]
FX This work has been funded by the European Union Horizon2020 MSCA ITN
   ACROSSING project (GA no. 616757). The authors would like to thank the
   members of the project's consortium for their valuable inputs.
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bordes Antoine, 2016, ARXIV160507683
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Cho K., 2014, ARXIV14091259
   Denis F, 1998, LECT NOTES ARTIF INT, V1501, P112
   Geli F., 2015, EMNLP
   GREENE WH, 1981, ECONOMETRICA, V49, P795, DOI 10.2307/1911523
   He K., 2016, CVPR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1994, NIPS
   Japkowicz N., 1995, IJCAI
   Kingma D. P., 2013, ICLR
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   LeCun Y., 2006, TECHNICAL REPORT
   Li X., 2005, ECML
   Li X., 2003, IJCAI
   Liu B, 2003, ICDM
   Liu C.-W., 2016, EMNLP
   McAuley Julian, 2013, RECSYS
   Papineni K., 2002, ACL
   Pennington Jeffrey, 2014, EMNLP
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2015, CVPR
   Vincent P., 2008, ACM
   Yu H., 2002, KDD
   Zhao J., 2017, ICLR
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404060
DA 2019-06-15
ER

PT S
AU Meshi, O
   Schwing, AG
AF Meshi, Ofer
   Schwing, Alexander G.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Asynchronous Parallel Coordinate Minimization for MAP Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE
AB Finding the maximum a-posteriori (MAP) assignment is a central task for structured prediction. Since modern applications give rise to very large structured problem instances, there is increasing need for efficient solvers. In this work we propose to improve the efficiency of coordinate-minimization-based dual-decomposition solvers by running their updates asynchronously in parallel. In this case message-passing inference is performed by multiple processing units simultaneously without coordination, all reading and writing to shared memory. We analyze the convergence properties of the resulting algorithms and identify settings where speedup gains can be expected. Our numerical evaluations show that this approach indeed achieves significant speedups in common computer vision tasks.
C1 [Meshi, Ofer] Google, Mountain View, CA 94043 USA.
   [Schwing, Alexander G.] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA.
RP Meshi, O (reprint author), Google, Mountain View, CA 94043 USA.
EM meshi@google.com; aschwing@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1718221]
FX This material is based upon work supported in part by the National
   Science Foundation under Grant No. 1718221. This work utilized computing
   resources provided by the Innovative Systems Lab (ISL) at NCSA.
CR Asuncion A., 2011, DISTRIBUTED GIBBS SA
   Avron H, 2015, J ACM, V62, DOI 10.1145/2814566
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Chen Liang Chieh, 2015, P ICML
   Choi J., 2012, FIELD PROGRAMMABLE L
   Davis D., 2016, P NIPS, V29, P226
   Desmaison A, 2016, LECT NOTES COMPUT SC, V9906, P818, DOI 10.1007/978-3-319-46475-6_50
   Globerson A., 2008, NIPS
   Gonzalez J., 2011, PARALLEL INFERENCE L
   Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014
   Hsieh C.-J., 2015, P 32 INT C MACH LEAR, V15, P2370
   Hurkat Skand, 2015, 2015 25th International Conference on Field Programmable Logic and Applications (FPL), P1, DOI 10.1109/FPL.2015.7293934
   Johnson J. S., 2008, THESIS
   Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   Komodakis N., 2007, MRF OPTIMIZATION VIA
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, V24, P109
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Meshi O., 2014, ADV STRUCTURED PREDI
   Meshi O., 2015, NEURAL INFORM PROCES
   Nam Ma, 2011, 2011 Proceedings of 23rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD 2011), P56, DOI 10.1109/SBAC-PAD.2011.34
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950
   Piatkowski N., 2011, INT WORKSH ECML PKDD
   Recht B., 2011, ADV NEURAL INFORM PR, V24
   Savchynskyy B., 2011, CVPR
   Schwing A. G., 2014, P ICML
   Schwing A. G., 2012, P NIPS
   Schwing A. G., 2011, P CVPR
   Shimony Y., 1994, ARITIFICAL INTELLIGE, V68, P399
   Singh S., 2010, NEUR INF PROC SYST N
   Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219
   Tseng P, 1991, SIAM J OPTIMIZ, V1, P603, DOI 10.1137/0801036
   Wainwright M, 2008, GRAPHICAL MODELS EXP
   Wang Y.-X., 2016, P 33 INT C MACH LEAR, P1548
   Werner T., 2009, CTUCMP200906
   Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036
   Werner T, 2010, IEEE T PATTERN ANAL, V32, P1474, DOI 10.1109/TPAMI.2009.134
   Wick M, 2010, PROC VLDB ENDOW, V3, P794, DOI 10.14778/1920841.1920942
   Yang You, 2016, ADV NEURAL INFORM PR, P4682
   Zhang J., 2014, P NIPS
   Zhang R., 2014, P 31 INT C MACH LEAR, P1701
   Zhou B., 2016, ARXIV160805442
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405079
DA 2019-06-15
ER

PT S
AU Messias, JV
   Whiteson, S
AF Messias, Joao, V
   Whiteson, Shimon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dynamic-Depth Context Tree Weighting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Reinforcement learning (RL) in partially observable settings is challenging because the agent's observations are not Markov. Recently proposed methods can learn variable-order Markov models of the underlying process but have steep memory requirements and are sensitive to aliasing between observation histories due to sensor noise. This paper proposes dynamic-depth context tree weighting (D2-CTW), a model-learning method that addresses these limitations. D2-CTW dynamically expands a suffix tree while ensuring that the size of the model, but not its depth, remains bounded. We show that D2-CTW approximately matches the performance of state-of-the-art alternatives at stochastic time-series prediction while using at least an order of magnitude less memory. We also apply D2-CTW to model-based RL, showing that, on tasks that require memory of past observations, D2-CTW can learn without prior knowledge of a good state representation, or even the length of history upon which such a representation should depend.
C1 [Messias, Joao, V] Morpheus Labs, Oxford, England.
   [Whiteson, Shimon] Univ Oxford, Oxford, England.
RP Messias, JV (reprint author), Morpheus Labs, Oxford, England.
EM jmessias@morpheuslabs.co.uk; shimon.whiteson@cs.ox.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU European Commission [FP7-ICT-611153]
FX This work was supported by the European Commission under the grant
   agreement FP7-ICT-611153 (TERESA).
CR Bakker B, 2001, NIPS, P1475
   Begleiter R, 2004, J ARTIF INTELL RES, V22, P385, DOI 10.1613/jair.1491
   BELL T, 1989, COMPUT SURV, V21, P557
   Bellemare M., 2014, P 31 INT C MACH LEAR, P1458
   Bellemare MG, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3337
   Cleary JG, 1997, COMPUT J, V40, P67, DOI 10.1093/comjnl/40.2_and_3.67
   Farias VF, 2010, IEEE T INFORM THEORY, V56, P2441, DOI 10.1109/TIT.2010.2043762
   Hamilton W. L., 2013, P 30 INT C MACH LEAR, P178
   Hausknecht Matthew, 2015, AAAI FALL S SERIES
   Holmes M. P., 2006, P 23 INT C MACH LEAR, P409
   Hutter M., 2013, J MACHINE LEARNING R, V30
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   McCallum  A., 1995, THESIS
   Ron D, 1996, MACH LEARN, V25, P117, DOI 10.1023/A:1026490906255
   Singh S., 2004, P 20 C UNC ART INT, P512
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tjalkens T. J., 1993, Proceedings of the Fourteenth Symposium on Information Theory in the Benelux, P128
   Veness J, 2012, IEEE DATA COMPR CONF, P327, DOI 10.1109/DCC.2012.39
   Veness J, 2011, J ARTIF INTELL RES, V40, P95, DOI 10.1613/jair.3125
   Volf P. A. J., 2002, WEIGHTING TECHNIQUES
   Wierstra D, 2007, LECT NOTES COMPUT SC, V4668, P697
   Willems FMJ, 1998, IEEE T INFORM THEORY, V44, P792, DOI 10.1109/18.661523
   WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012
   Wood F, 2011, COMMUN ACM, V54, P91, DOI 10.1145/1897816.1897842
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403039
DA 2019-06-15
ER

PT S
AU Metelli, AM
   Pirotta, M
   Restelli, M
AF Metelli, Alberto Maria
   Pirotta, Matteo
   Restelli, Marcello
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Compatible Reward Inverse Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. This paper is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert's reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert's policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is empirically compared to other IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian (LQG) and Car on the Hill environments.
C1 [Metelli, Alberto Maria; Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy.
   [Pirotta, Matteo] Inria Lille, SequeL Team, Lille, France.
RP Metelli, AM (reprint author), Politecn Milan, DEIB, Milan, Italy.
EM albertomaria.metelli@polimi.it; matteo.pirotta@inria.fr;
   marcello.restelli@polimi.it
RI Jeong, Yongwook/N-7413-2016
FU French Ministry of Higher Education and Research; Nord-Pasde-Calais
   Regional Council; French National Research Agency (ANR)
   [ANR-14-CE24-0010-01]
FX This research was supported in part by French Ministry of Higher
   Education and Research, Nord-Pasde-Calais Regional Council and French
   National Research Agency (ANR) under project ExTra-Learn
   (n.ANR-14-CE24-0010-01).
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Audiffren J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3315
   Bohmer W, 2013, J MACH LEARN RES, V14, P2067
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   Dorato P., 2000, LINEAR QUADRATIC CON
   Englert P., 2015, P INT S ROB RES
   Ernst D, 2005, J MACH LEARN RES, V6, P503
   Farahmand Amir-massoud, 2012, ADV NEURAL INFORM PR, P1349
   Finn C., 2016, INT C MACH LEARN, P49
   Furmston Thomas, 2012, ADV NEURAL INFORM PR, P2717
   Hester Todd, 2017, ABS170403732 CORR
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Ho Jonathan, 2016, JMLR WORKSHOP C P, P2760
   Hwang C. L., 2012, MULTIPLE OBJECTIVE D, V164
   Kakade S., 2001, ADV NEURAL INFORM PR, V14, P1531
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Klein Edouard, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P1, DOI 10.1007/978-3-642-40988-2_1
   Levine S, 2010, ADV NEURAL INFORM PR, P1342
   Mahadevan S., 2005, P 22 INT C MACH LEAR, P553, DOI DOI 10.1145/1102351.1102421
   Mahadevan S, 2007, J MACH LEARN RES, V8, P2169
   Mahadevan Sridhar, 2006, P 21 NAT C ART INT, V6, P1194
   Manganini Giorgio, 2015, P IJCNN, P1
   Mengi E, 2014, SIAM J MATRIX ANAL A, V35, P699, DOI 10.1137/130933472
   Neu G, 2009, MACH LEARN, V77, P303, DOI 10.1007/s10994-009-5110-1
   Ng A. Y., 2000, P 17 INT C MACH LEAR, P663, DOI DOI 10.2460/AJVR.67.2.323
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Parisi S, 2016, J ARTIF INTELL RES, V57, P187, DOI 10.1613/jair.4961
   Parr R., 2007, ICML, P737
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Piot B, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1249
   Pirotta Matteo, 2016, P AAAI, P1993
   Puterman M. L., 1994, MARKOV DECISION PROC
   Ratliff ND, 2009, AUTON ROBOT, V27, P25, DOI 10.1007/s10514-009-9121-3
   Ratliff ND, 2006, P 23 INT C MACH LEAR, P729
   SUTTON R.S., 1999, NIPS, V99, P1057
   Syed U., 2008, P 25 INT C MACH LEAR, P1032
   Vidal JM, 2006, FUNDAMENTALS MULTIAG
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402010
DA 2019-06-15
ER

PT S
AU Metzler, CA
   Mousavi, A
   Baraniuk, RG
AF Metzler, Christopher A.
   Mousavi, Ali
   Baraniuk, Richard G.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learned D-AMP: Principled Neural Network Based Compressive Image
   Recovery
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS; MODEL
AB Compressive image recovery is a challenging problem that requires fast and accurate algorithms. Recently, neural networks have been applied to this problem with promising results. By exploiting massively parallel GPU processing architectures and oodles of training data, they can run orders of magnitude faster than existing techniques. However, these methods are largely unprincipled black boxes that are difficult to train and often-times specific to a single measurement matrix.
   It was recently demonstrated that iterative sparse-signal-recovery algorithms can be "unrolled" to form interpretable deep networks. Taking inspiration from this work, we develop a novel neural network architecture that mimics the behavior of the denoising-based approximate message passing (D-AMP) algorithm. We call this new network Learned D-AMP (LDAMP).
   The LDAMP network is easy to train, can be applied to a variety of different measurement matrices, and comes with a state-evolution heuristic that accurately predicts its performance. Most importantly, it outperforms the state-of-the-art BM3D-AMP and NLR-CS algorithms in terms of both accuracy and run time. At high resolutions, and when used with sensing matrices that have fast implementations, LDAMP runs over 50x faster than BM3D-AMP and hundreds of times faster than NLR-CS.
C1 [Metzler, Christopher A.; Mousavi, Ali; Baraniuk, Richard G.] Rice Univ, Houston, TX 77251 USA.
RP Metzler, CA (reprint author), Rice Univ, Houston, TX 77251 USA.
EM chris.metzler@rice.edu; ali.mousavi@rice.edu; richb@rice.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA REVEAL [HR0011-16-C-0028]; DARPA OMNI-SCIENT [G001534-7500]; ONR
   [N00014-17-1-2551, N00014-15-1-2735]; ARO [W911NF-15-1-0316]; NSF
   [CCF-1527501]; NSF GRFP
FX This work was supported in part by DARPA REVEAL grant HR0011-16-C-0028,
   DARPA OMNI-SCIENT grant G001534-7500, ONR grant N00014-15-1-2735, ARO
   grant W911NF-15-1-0316, ONR grant N00014-17-1-2551, and NSF grant
   CCF-1527501. In addition, C. Metzler was supported in part by the NSF
   GRFP.
CR Alain G, 2014, J MACH LEARN RES, V15, P3563
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Beygi S., 2017, ARXIV170401992
   Borgerding M., 2016, ARXIV161201183
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I., 2004, COMMUNICATIONS PURE, V75, P1412
   Dave A., 2016, ARXIV161204229
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Gulrajani Ishaan, 2017, ARXIV170400028
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hershey J. R., 2014, ARXIV14092574
   Ioffe S., 2015, ARXIV150203167
   Kamilov US, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2548245
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li C., 2009, CAAM REPORT, V20, P46
   MALEKI A., 2010, THESIS
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mezard M., 2008, INFORM PHYS COMPUTAT
   Mousavi A, 2017, INT CONF ACOUST SPEE, P2272, DOI 10.1109/ICASSP.2017.7952561
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Ramani S, 2008, IEEE T IMAGE PROCESS, V17, P1540, DOI 10.1109/TIP.2008.2001404
   Rangan  Sundeep, 2016, ARXIV161003082
   Schniter P., 2016, ARXIV161101376
   SMIEJA FJ, 1993, CIRC SYST SIGNAL PR, V12, P331, DOI 10.1007/BF01189880
   Sonderby C. K., 2017, P INT C LEARN REPR I
   Theis L., 2015, ADV NEURAL INFORM PR, V2, P1927
   THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992
   Tramel Eric W., 2016, 2016 IEEE Information Theory Workshop (ITW), P265, DOI 10.1109/ITW.2016.7606837
   Tramel E. W., 2017, ARXIV170203260
   Tramel EW, 2016, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2016/07/073401
   Vedaldi A., 2015, P ACM INT C MULT
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2621478
   Yakar T. B., 2013, ISMIR, P65
   Yang Y, 2016, P ADV NEUR INF PROC, P10, DOI DOI 10.1007/978-3-662-48681-8
   Yao H., 2017, ARXIV170205743
NR 49
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401078
DA 2019-06-15
ER

PT S
AU Miller, AC
   Foti, NJ
   D'Amour, A
   Adams, RP
AF Miller, Andrew C.
   Foti, Nicholas J.
   D'Amour, Alexander
   Adams, Ryan P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Reducing Reparameterization Gradient Variance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to generate more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on a non-conjugate hierarchical model and a Bayesian neural net where our method attained orders of magnitude (20-2,000x) reduction in gradient variance resulting in faster and more stable optimization.
C1 [Miller, Andrew C.] Harvard Univ, Cambridge, MA 02138 USA.
   [Foti, Nicholas J.] Univ Washington, Seattle, WA 98195 USA.
   [D'Amour, Alexander] Univ Calif Berkeley, Berkeley, CA USA.
   [Adams, Ryan P.] Google Brain, Mountain View, CA USA.
   [Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA.
RP Miller, AC (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM acm@seas.harvard.edu; nfoti@uw.edu; alexdamour@berkeley.edu;
   rpa@princeton.edu
RI Jeong, Yongwook/N-7413-2016
FU Applied Mathematics Program within the Office of Science Advanced
   Scientific Computing Research of the U.S. Department of Energy
   [DE-AC02-05CH11231]; Washington Research Foundation Innovation
   Postdoctoral Fellowship in Neuroengineering and Data Science; NSF
   [IIS-1421780]; Alfred P. Sloan Foundation
FX The authors would like to thank Finale Doshi-Velez, Mike Hughes, Taylor
   Killian, Andrew Ross, and Matt Hoffman for helpful conversations and
   comments on this work. ACM is supported by the Applied Mathematics
   Program within the Office of Science Advanced Scientific Computing
   Research of the U.S. Department of Energy under contract No.
   DE-AC02-05CH11231. NJF is supported by a Washington Research Foundation
   Innovation Postdoctoral Fellowship in Neuroengineering and Data Science.
   RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation.
CR Abadi M., 2016, ARXIV160304467
   Arjovsky M., 2017, ARXIV170107875
   Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003
   Blei D. M., 2012, P 29 INT C INT C MAC, P1363
   Blei  David, 2017, J AM STAT ASS
   Gelman A., 2006, DATA ANAL USING REGR
   Gelman A, 2007, J AM STAT ASSOC, V102, P813, DOI 10.1198/016214506000001040
   Glasserman P., 2013, MONTE CARLO METHODS, V53
   Glasserman P., 2004, APPL MATH, V53
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hoffman Matthew D, 2016, ELBO SURG YET ANOTHE
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kingma D. P., 2015, P INT C LEARN REPR
   Kingma  Diederik, 2014, P INT C LEARN REPR
   Maclaurin Dougal, 2015, AUTOGRAD REVERSE MOD
   Martens James, 2012, P INT C MACH LEARN
   Mnih A, 2016, INT C MACH LEARN, P2188
   Mohamed S., 2016, ARXIV161003483
   Paszke Adam, 2017, PYTORCH
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Ranganath R., 2016, INT C MACH LEARN
   Ranganath  R., 2014, AISTATS, P814
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Rezende D. J., 2014, INT C MACH LEARN
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roeder Geoffrey, 2017, ARXIV170309194
   Ruiz Francisco, 2016, ADV NEURAL INFORM PR, P460
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Tran Dustin, 2017, P INT C LEARN REPR
   Wang C., 2013, ADV NEURAL INFORM PR, V26, P181
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403075
DA 2019-06-15
ER

PT S
AU Milstein, DJ
   Pacheco, JL
   Hochberg, LR
   Simeral, JD
   Jarosiewicz, B
   Sudderth, EB
AF Milstein, Daniel J.
   Pacheco, Jason L.
   Hochberg, Leigh R.
   Simeral, John D.
   Jarosiewicz, Beata
   Sudderth, Erik B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer
   Interfaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TETRAPLEGIA; MOVEMENTS
AB Intracortical brain-computer interfaces (iBCIs) have allowed people with tetraplegia to control a computer cursor by imagining the movement of their paralyzed arm or hand. State-of-the-art decoders deployed in human iBCIs are derived from a Kalman filter that assumes Markov dynamics on the angle of intended movement, and a unimodal dependence on intended angle for each channel of neural activity. Due to errors made in the decoding of noisy neural data, as a user attempts to move the cursor to a goal, the angle between cursor and goal positions may change rapidly. We propose a dynamic Bayesian network that includes the on-screen goal position as part of its latent state, and thus allows the person's intended angle of movement to be aggregated over a much longer history of neural activity. This multiscale model explicitly captures the relationship between instantaneous angles of motion and long-term goals, and incorporates semi-Markov dynamics for motion trajectories. We also introduce a multimodal likelihood model for recordings of neural populations which can be rapidly calibrated for clinical applications. In offline experiments with recorded neural data, we demonstrate significantly improved prediction of motion directions compared to the Kalman filter. We derive an efficient online inference algorithm, enabling a clinical trial participant with tetraplegia to control a computer cursor with neural activity in real time. The observed kinematics of cursor movement are objectively straighter and smoother than prior iBCI decoding models without loss of responsiveness.
C1 [Milstein, Daniel J.; Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   [Pacheco, Jason L.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Hochberg, Leigh R.; Simeral, John D.] Brown Univ, Sch Engn, Providence, RI 02912 USA.
   [Hochberg, Leigh R.; Simeral, John D.] Massachusetts Gen Hosp, Dept Neurol, Boston, MA 02114 USA.
   [Hochberg, Leigh R.; Simeral, John D.; Jarosiewicz, Beata] Dept Vet Affairs Med Ctr, Rehabil R&D Serv, Providence, RI USA.
   [Hochberg, Leigh R.; Simeral, John D.; Jarosiewicz, Beata] Brown Univ, Brown Inst Brain Sci, Providence, RI 02912 USA.
   [Hochberg, Leigh R.] Harvard Med Sch, Dept Neurol, Boston, MA USA.
   [Jarosiewicz, Beata] Brown Univ, Dept Neurosci, Providence, RI 02912 USA.
   [Jarosiewicz, Beata] Stanford Univ, Dept Neurosurg, Stanford, CA 94305 USA.
   [Sudderth, Erik B.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA USA.
RP Milstein, DJ (reprint author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
EM daniel_milstein@alumni.brown.edu; pachecoj@mit.edu;
   leigh_hochberg@brown.edu; john_simeral@brown.edu; beataj@stanford.edu;
   sudderth@uci.edu
RI Jeong, Yongwook/N-7413-2016
FU Office of Research and Development, Rehabilitation R&D Service,
   Department of Veterans Affairs [B4853C, B6453R, N9228C]; National
   Institute on Deafness and Other Communication Disorders of National
   Institutes of Health (NIDCD-NIH) [R01DC009899]; MGH-Deane Institute;
   Executive Committee on Research (ECOR) of Massachusetts General Hospital
FX The authors thank Participants T9 and T10 and their families, Brian
   Franco, Tommy Hosman, Jessica Kelemen, Dave Rosler, Jad Saab, and Beth
   Travers for their contributions to this research. Support for this study
   was provided by the Office of Research and Development, Rehabilitation
   R&D Service, Department of Veterans Affairs (B4853C, B6453R, and
   N9228C), the National Institute on Deafness and Other Communication
   Disorders of National Institutes of Health (NIDCD-NIH: R01DC009899),
   MGH-Deane Institute, and The Executive Committee on Research (ECOR) of
   Massachusetts General Hospital. The content is solely the responsibility
   of the authors and does not necessarily represent the official views of
   the National Institutes of Health, or the Department of Veterans Affairs
   or the United States Government. CAUTION: Investigational Device.
   Limited by Federal Law to Investigational Use.
CR Ajiboye AB, 2017, LANCET, V389, P1821, DOI 10.1016/S0140-6736(17)30601-3
   Amirikian B, 2000, NEUROSCI RES, V36, P73, DOI 10.1016/S0168-0102(99)00112-1
   Bacher D, 2015, NEUROREHAB NEURAL RE, V29, P462, DOI 10.1177/1545968314554624
   Boyen X., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P33
   Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9
   Donoghue J. A., 2011, YOUMANS NEUROLOGICAL
   Fraser GW, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/5/055004
   GEORGOPOULOS AP, 1982, J NEUROSCI, V2, P1527
   Gilja V, 2015, NAT MED, V21, P1142, DOI 10.1038/nm.3953
   Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Jarosiewicz B, 2015, SCI TRANSL MED, V7, DOI 10.1126/scitranslmed.aac7328
   Kim SP, 2008, J NEURAL ENG, V5, P455, DOI 10.1088/1741-2560/5/4/010
   Koyama S, 2010, J COMPUT NEUROSCI, V29, P73, DOI 10.1007/s10827-009-0196-9
   Levy P., 1954, P INT C MATH AMSTERD, V3, P416
   MacKenzie I. S., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P9
   Malik WQ, 2015, IEEE T BIO-MED ENG, V62, P570, DOI 10.1109/TBME.2014.2360393
   Murphy K., 2001, P UAI, P378
   Murphy K. P., 2002, THESIS
   Pandarinath C, 2017, ELIFE, V6, DOI [10.7554/eLife.48554, 10.7554/eLife.18554]
   Simeral JD, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/2/025027
   SMITH WL, 1955, PROC R SOC LON SER-A, V232, P6, DOI 10.1098/rspa.1955.0198
   Willett FR, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2560/14/1/016001
   Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400083
DA 2019-06-15
ER

PT S
AU Minsker, S
   Wei, XH
AF Minsker, Stanislav
   Wei, Xiaohan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Estimation of the covariance structure of heavy-tailed distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID HIGH-DIMENSIONAL COVARIANCE; MATRICES OPTIMAL RATES; ADAPTIVE ESTIMATION
AB We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution, such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood, much less in known in the case of heavy-tailed data. As K. Balasubramanian and M. Yuan write(1), "data from real-world experiments oftentimes tend to be corrupted with outliers and/or exhibit heavy tails. In such cases, it is not clear that those covariance matrix estimators .. remain optimal" and "..what are the other possible strategies to deal with heavy tailed distributions warrant further studies." We make a step towards answering this question and prove tight deviation inequalities for the proposed estimator that depend only on the parameters controlling the "intrinsic dimension" associated to the covariance matrix (as opposed to the dimension of the ambient space); in particular, our results are applicable in the case of high-dimensional observations.
C1 [Minsker, Stanislav] Univ Southern Calif, Dept Math, Los Angeles, CA 90007 USA.
   [Wei, Xiaohan] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA.
RP Minsker, S (reprint author), Univ Southern Calif, Dept Math, Los Angeles, CA 90007 USA.
EM minsker@usc.edu; xiaohanw@usc.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [NSF DMS-1712956]
FX Research of S. Minsker and X. Wei was partially supported by the
   National Science Foundation grant NSF DMS-1712956.
CR Allard WK, 2012, APPL COMPUT HARMON A, V32, P435, DOI 10.1016/j.acha.2011.08.001
   Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101
   Balasubramanian K, 2016, ELECTRON J STAT, V10, P71, DOI 10.1214/15-EJS1006
   Bhatia  R., 2013, MATRIX ANAL, V169
   Boucheron S., 2013, CONCENTRATION INEQUA
   Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081
   Catoni O., 2016, ARXIV160305229
   Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454
   Chen M., 2015, ARXIV150600691
   DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001
   Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P655, DOI 10.1109/FOCS.2016.85
   Fan J., 2017, J AM STAT ASS
   Fan J., 2016, ARXIV160308315
   Fan JQ, 2017, J R STAT SOC B, V79, P247, DOI 10.1111/rssb.12166
   Fan JQ, 2016, ECONOMET J, V19, pC1, DOI 10.1111/ectj.12061
   Fan  Jianqing, 2016, ARXIV160303516
   Fang K. T., 1990, SYMMETRIC MULTIVARIA
   Giulini I., 2015, ARXIV151106263
   Han F., 2017, J AM STAT ASS
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hubert M, 2008, STAT SCI, V23, P92, DOI 10.1214/088342307000000087
   Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4
   Ledoit O, 2012, ANN STAT, V40, P1024, DOI 10.1214/12-AOS989
   LEPSKII OV, 1991, THEOR PROBAB APPL+, V36, P682, DOI 10.1137/1136085
   Lounici K, 2014, BERNOULLI, V20, P1029, DOI 10.3150/12-BEJ487
   Minsker S., 2016, ARXIV160507129
   Minsker S, 2015, BERNOULLI, V21, P2308, DOI 10.3150/14-BEJ645
   Novembre J, 2008, NATURE, V456, P98, DOI 10.1038/nature07331
   Saal LH, 2007, P NATL ACAD SCI USA, V104, P7564, DOI 10.1073/pnas.0702507104
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Tropp Joel A., 2015, ARXIV150101571
   Tukey JW, 1975, P INT C MATH, P523
   TYLER DE, 1987, ANN STAT, V15, P234, DOI 10.1214/aos/1176350263
   Wegkamp M, 2016, BERNOULLI, V22, P1184, DOI 10.3150/14-BEJ690
   Zwald Laurent, 2006, ADV NEURAL INFORM PR, P1649
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402088
DA 2019-06-15
ER

PT S
AU Mishchuk, A
   Mishkin, D
   Radenovic, F
   Matas, J
AF Mishchuk, Anastasiya
   Mishkin, Dmytro
   Radenovic, Filip
   Matas, Jiri
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Working hard to know your neighbor's margins: Local descriptor learning
   loss
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SCALE; GEOMETRY
AB We introduce a loss for metric learning, which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss, that maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks.
C1 [Mishchuk, Anastasiya] Szkocka Res Grp, Lvov, Ukraine.
   [Mishkin, Dmytro; Radenovic, Filip; Matas, Jiri] CTU, Visual Recognit Grp, Prague, Czech Republic.
RP Mishchuk, A (reprint author), Szkocka Res Grp, Lvov, Ukraine.
EM anastasiya.mishchuk@gmail.com; mishkdmy@cmp.felk.cvut.cz;
   filip.radenovic@cmp.felk.cvut.cz; matas@cmp.felk.cvut.cz
RI Jeong, Yongwook/N-7413-2016
FU Czech Science Foundation Project GACR [P103/12/G084]; Austrian Ministry
   for Transport, Innovation and Technology; Federal Ministry of Science,
   Research and Economy; Province of Upper Austria in the frame of the
   COMET center; CTU student grant [SGS17/185/OHK3/3T/13]; MSMT ERC-CZ
   grant [LL1303]; Szkocka Research Group Grant
FX The authors were supported by the Czech Science Foundation Project GACR
   P103/12/G084, the Austrian Ministry for Transport, Innovation and
   Technology, the Federal Ministry of Science, Research and Economy, and
   the Province of Upper Austria in the frame of the COMET center, the CTU
   student grant SGS17/185/OHK3/3T/13, and the MSMT LL1303 ERC-CZ grant.
   Anastasiya Mishchuk was supported by the Szkocka Research Group Grant.
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Balntas Vassileios, 2016, BRIT MACH VIS C BMVC
   Balntas Vassileios, 2017, C COMP VIS PATT REC
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Bursuc Andrei, 2015, ACM INT C MULT RETR
   Choy C. B., 2016, ADV NEURAL INFORM PR, P2414
   Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145
   Fernando B, 2015, COMPUT VIS IMAGE UND, V139, P21, DOI 10.1016/j.cviu.2015.05.016
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   Ioffe S., 2015, ARXIV150203167
   Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jegou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285
   Kendall Alex, 2015, INT C COMP VIS ICCV
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lee C. Y., 2015, ARTIF INTELL, P562
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, P BRIT MACH VIS C, P384
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1
   Mishkin D, 2015, COMPUT VIS IMAGE UND, V141, P81, DOI 10.1016/j.cviu.2015.08.005
   Mishkin Dmytro, 2015, ARXIV150406603
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Philbin J., 2007, COMPUT VIS PATTERN R, DOI [10.1109/CVPR.2007.383172, DOI 10.1109/CVPR.2007.383172]
   Philbin J., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587635
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schonberger J. L., 2017, C COMP VIS PATT REC
   Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schonberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Simonyan K, 2012, LECT NOTES COMPUT SC, V7572, P243, DOI 10.1007/978-3-642-33718-5_18
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tian Bin Fan Yurun, 2017, C COMP VIS PATT REC
   Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007
   Wilson DR, 2003, NEURAL NETWORKS, V16, P1429, DOI 10.1016/S0893-6080(03)00138-2
   Yang GH, 2007, IEEE T PATTERN ANAL, V29, P1973, DOI 10.1109/TPAMl.2007.1116.
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zagoruyko Sergey, 2015, C COMP VIS PATT REC
   Zitnick CL, 2011, IEEE I CONF COMP VIS, P359, DOI 10.1109/ICCV.2011.6126263
NR 45
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404087
DA 2019-06-15
ER

PT S
AU Mitrovic, S
   Bogunovic, I
   Norouzi-Fard, A
   Tarnawski, J
   Cevher, V
AF Mitrovic, Slobodan
   Bogunovic, Ilija
   Norouzi-Fard, Ashkan
   Tarnawski, Jakub
   Cevher, Volkan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Streaming Robust Submodular Maximization: A Partitioned Thresholding
   Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the classical problem of maximizing a monotone submodular function subject to a cardinality constraint k, with two additional twists: (i) elements arrive in a streaming fashion, and (ii) m items from the algorithm's memory are removed after the stream is finished. We develop a robust submodular algorithm STAR-T. It is based on a novel partitioning structure and an exponentially decreasing thresholding rule. STAR-T makes one pass over the data and retains a short but robust summary. We show that after the removal of any m elements from the obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the remaining elements achieves a constant-factor approximation guarantee. In two different data summarization tasks, we demonstrate that it matches or outperforms existing greedy and streaming methods, even if they are allowed the benefit of knowing the removed subset in advance.
C1 [Mitrovic, Slobodan; Bogunovic, Ilija; Norouzi-Fard, Ashkan; Tarnawski, Jakub; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Mitrovic, S (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM slobodan.mitrovic@epfl.ch; ilija.bogunovic@epfl.ch;
   ashkan.norouzifard@epfl.ch; jakub.tarnawski@epfl.ch;
   volkan.cevher@epfl.ch
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation program [725594]; Swiss National Science
   Foundation (SNF) [407540_167319/1]; NCCR MARVEL - Swiss National Science
   Foundation; Hasler Foundation Switzerland [16066]; Office of Naval
   Research (ONR) [N00014-16-R-BA01]; ERC [335288-OptApprox]
FX IB and VC's work was supported in part by the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   program (grant agreement number 725594), in part by the Swiss National
   Science Foundation (SNF), project 407540_167319/1, in part by the NCCR
   MARVEL, funded by the Swiss National Science Foundation, in part by
   Hasler Foundation Switzerland under grant agreement number 16066 and in
   part by Office of Naval Research (ONR) under grant agreement number
   N00014-16-R-BA01. JT's work was supported by ERC Starting Grant
   335288-OptApprox.
CR Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Bogunovic I., 2017, INT C MACH LEARN ICM
   Chen W, 2016, P 22 ACM SIGKDD INT, P795, DOI DOI 10.1145/ 2939672.2939745
   El-Arini Khalid, 2011, P 17 ACM SIGKDD INT, P439
   Golovin D., 2011, J ARTIFICIAL INTELLI, V42
   Gomes Ryan, 2010, P 27 INT C MACH LEAR, P391
   Guillory A., 2010, ARXIV10023345
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Kempe D, 2003, INT C KNOWL DISC DAT
   Krause A, 2008, J MACH LEARN RES, V9, P2761
   Kumar  Ravi, 2015, ACM T PARALLEL COMPU, V2, P14
   Lin H., 2011, ASS COMP LING HUMAN, V1
   Lindgren Erik, 2016, ADV NEURAL INFORM PR, P3414
   Mcauley J., 2014, ACM T KNOWL DISCOV D
   Mirzasoleiman  Baharan, 2017, INT C MACH LEARN, P2449
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Norouzi-Fard A., 2016, ADV NEUR INF P SYS N
   Orlin J. B., 2016, INT C INT PROGR COMB
   Staib M., 2017, INT C MACH LEARN ICM
   Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520
   Tschiatschek S., 2014, ADV NEURAL INFORM PR, P1413
   Yang J, 2015, KNOWL INF SYST, V42, P181, DOI 10.1007/s10115-013-0693-z
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404061
DA 2019-06-15
ER

PT S
AU Mohri, M
   Yang, S
AF Mohri, Mehryar
   Yang, Scott
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Learning with Transductive Regret
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study online learning with the general notion of transductive regret, that is regret with modification rules applying to expert sequences (as opposed to single experts) that are representable by weighted finite-state transducers. We show how transductive regret generalizes existing notions of regret, including: (1) external regret; (2) internal regret; (3) swap regret; and (4) conditional swap regret. We present a general and efficient online learning algorithm for minimizing transductive regret. We further extend that to design efficient algorithms for the time-selection and sleeping expert settings. A by-product of our study is an algorithm for swap regret, which, under mild assumptions, is more efficient than existing ones, and a substantially more efficient algorithm for time selection swap regret.
C1 [Mohri, Mehryar] Courant Inst, New York, NY 10012 USA.
   [Mohri, Mehryar] Google Res, New York, NY 10011 USA.
   [Yang, Scott] DE Shaw & Co, New York, NY USA.
   [Yang, Scott] Courant Inst Math Sci, New York, NY USA.
RP Mohri, M (reprint author), Courant Inst, New York, NY 10012 USA.; Mohri, M (reprint author), Google Res, New York, NY 10011 USA.
EM mohri@cims.nyu.edu; yangs@cims.nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1535987, IIS-1618662]
FX We thank Avrim Blum for informing us of an existing lower bound for swap
   regret proven by Auer [2017]. This work was partly funded by NSF
   CCF-1535987 and NSF IIS-1618662.
CR Adamskiy Dmitry, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P290, DOI 10.1007/978-3-642-34106-9_24
   Auer P., 2017, COMMUNICATION
   Blum A, 1997, MACH LEARN, V26, P5, DOI 10.1023/A:1007335615132
   Blum A, 2007, J MACH LEARN RES, V8, P1307
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi Nicolo, 2012, ADV NEURAL INFORM PR, P980
   COHEN W, 1996, AAAI WORKSH INT BAS
   Cohen WW, 1999, ACM T INFORM SYST, V17, P141, DOI 10.1145/306686.306688
   Daniely A., 2015, P 32 INT C MACH LEAR, P1405
   Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595
   Freund Y, 1997, P 29 ANN ACM S THEOR, P334, DOI DOI 10.1145/258533.258616
   Greenwald A.R., 2008, COLT, P239
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   Hazan Elad, 2009, P 26 ANN INT C MACH, V382, P393
   Hazan Elad, 2008, NIPS, P625
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Khot S., 2008, 21 ANN C LEARN THEOR, V2008
   Koolen WM, 2013, IEEE T INFORM THEORY, V59, P7168, DOI 10.1109/TIT.2013.2273353
   Lehrer E, 2003, GAME ECON BEHAV, V42, P101, DOI 10.1016/S0899-8256(03)00032-0
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Mohri M., 2017, 170500132 ARXIV
   Mohri M., 2018, AISTATS
   Mohri Mehryar, 2014, NIPS, P1314
   Monteleoni C., 2003, NIPS
   Munos  R., 2011, P 14 INT C ART INT S, P570
   Nesterov Y, 2015, APPL MATH COMPUT, V255, P58, DOI 10.1016/j.amc.2014.04.053
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   Stoltz G, 2005, MACH LEARN, V59, P125, DOI 10.1007/s10994-005-0465-4
   Vovk V, 1999, MACH LEARN, V35, P247, DOI 10.1023/A:1007595032382
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405029
DA 2019-06-15
ER

PT S
AU Mokhtari, A
   Ribeiro, A
AF Mokhtari, Aryan
   Ribeiro, Alejandro
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI First-Order Adaptive Sample Size Methods to Reduce Complexity of
   Empirical Risk Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper studies empirical risk minimization (ERM) problems for large-scale datasets and incorporates the idea of adaptive sample size methods to improve the guaranteed convergence bounds for first-order stochastic and deterministic methods. In contrast to traditional methods that attempt to solve the ERM problem corresponding to the full dataset directly, adaptive sample size schemes start with a small number of samples and solve the corresponding ERM problem to its statistical accuracy. The sample size is then grown geometrically - e.g., scaling by a factor of two - and use the solution of the previous ERM as a warm start for the new ERM. Theoretical analyses show that the use of adaptive sample size methods reduces the overall computational cost of achieving the statistical accuracy of the whole dataset for a broad range of deterministic and stochastic first-order methods. The gains are specific to the choice of method. When particularized to, e.g., accelerated gradient descent and stochastic variance reduce gradient, the computational cost advantage is a logarithm of the number of training samples. Numerical experiments on various datasets confirm theoretical claims and showcase the gains of using the proposed adaptive sample size scheme.
C1 [Mokhtari, Aryan; Ribeiro, Alejandro] Univ Penn, Philadelphia, PA 19104 USA.
RP Mokhtari, A (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
EM aryanm@seas.upenn.edu; aribeiro@seas.upenn.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF 1717120]; ARO [W911NF1710438]
FX This research was supported by NSF CCF 1717120 and ARO W911NF1710438.
CR Allen-Zhu Z., 2017, STOC
   BACH F., 2012, ADV NEURAL INF PROCE, V25, P2672
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bottou L., 2007, ADV NEURAL INFORM PR, V20, P161
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bousquet O., 2002, THESIS
   Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Defazio Aaron, 2016, ADV NEURAL INFORM PR, P676
   Frostig R., 2015, P C LEARN THEOR PAR, P728
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Mokhtari A., 2016, P ADV NEUR INF PROC, P4062
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov  Y., 2007, GRADIENT METHODS MIN
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Vapnik V., 2013, NATURE STAT LEARNING
   Woodworth B., 2016, ADV NEURAL INFORM PR, P3639
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402011
DA 2019-06-15
ER

PT S
AU Monti, F
   Bronstein, MM
   Bresson, X
AF Monti, Federico
   Bronstein, Michael M.
   Bresson, Xavier
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationary structures on user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix. Our neural network system is computationally attractive as it requires a constant number of parameters independent of the matrix size. We apply our method on several standard datasets, showing that it outperforms state-of-the-art matrix completion techniques.
C1 [Monti, Federico; Bronstein, Michael M.] Univ Svizzera Italiana, Lugano, Switzerland.
   [Bresson, Xavier] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
RP Monti, F (reprint author), Univ Svizzera Italiana, Lugano, Switzerland.
EM federico.monti@usi.ch; michael.bronstein@usi.ch; xbresson@ntu.edu.sg
RI Jeong, Yongwook/N-7413-2016
FU ERC [724228, 307047]; Google Faculty Research Award; Nvidia equipment
   grant; Harvard Institute for Advanced Study; TU Munich Institute for
   Advanced Study - German Excellence Initiative; European Union Seventh
   Framework Programme [291763]; NRF Fellowship [NRFF2017-10]
FX FM and MB are supported in part by ERC Starting Grant No. 307047
   (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty
   Research Award, Nvidia equipment grant, Radcliffe fellowship from
   Harvard Institute for Advanced Study, and TU Munich Institute for
   Advanced Study, funded by the German Excellence Initiative and the
   European Union Seventh Framework Programme under grant agreement No.
   291763. XB is supported in part by NRF Fellowship NRFF2017-10.
CR Benzi K., 2016, P ICASSP
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini  D., 2016, P NIPS
   Breese J., 1998, P UNC ART INT
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Bruna J., 2013, SPECTRAL NETWORKS LO
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Defferrard  M., 2016, P NIPS
   Dror Gideon, 2012, KDD CUP
   Duvenaud D. K., 2015, P NIPS
   Gori  M., 2005, P IJCNN
   He X., 2017, P WWW
   Henaff Mikael, 2015, ARXIV150605163
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   JAIN P, 2013, ARXIV13060626
   Jamali M., 2010, P REC SYST
   Kalofolias V., 2014, MATRIX COMPLETION GR
   Kingma D. P., 2015, ADAM METHOD STOCHAST
   Kipf T. N., 2017, SEMISUPERVISED CLASS
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Ktena S. I., 2017, P MICCAI
   Kuang D., 2016, CORR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Y., 2016, GATED GRAPH SEQUENCE
   Ma H., 2011, P WEB SEARCH DAT MIN
   Masci  J., 2015, P 3DRR
   Miller B. N., 2003, P INT US INT
   Monti  F., 2017, P CVPR
   Parisot  S., 2017, P MICCAI
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Rao N., 2015, P NIPS
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Sedhain S., 2015, P WWW
   Seo Y., 2016, ARXIV161207659
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Srebro N., 2004, P NIPS
   Suhara Y., 2016, P INT C COMP SOC SCI
   Sukhbaatar S, 2016, ADV NEURAL INFORM PR, V29, P2244
   Vestner M., 2017, P CVPR
   Xu M., 2013, P NIPS
   Yanez F, 2017, INT CONF ACOUST SPEE, P2257, DOI 10.1109/ICASSP.2017.7952558
   Zheng Y., 2016, P ICML
NR 44
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403074
DA 2019-06-15
ER

PT S
AU Morrison, RE
   Baptista, R
   Marzouk, Y
AF Morrison, Rebecca E.
   Baptista, Ricardo
   Marzouk, Youssef
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Beyond normality: Learning sparse probabilistic graphical models in the
   non-Gaussian setting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INFERENCE; SELECTION
AB We present an algorithm to identify sparse dependence structure in continuous and non-Gaussian probability distributions, given a corresponding set of data. The conditional independence structure of an arbitrary distribution can be represented as an undirected graph (or Markov random field), but most algorithms for learning this structure are restricted to the discrete or Gaussian cases. Our new approach allows for more realistic and accurate descriptions of the distribution in question, and in turn better estimates of its sparse Markov structure. Sparsity in the graph is of interest as it can accelerate inference, improve sampling methods, and reveal important dependencies between variables. The algorithm relies on exploiting the connection between the sparsity of the graph and the sparsity of transport maps, which deterministically couple one probability measure to another.
C1 [Morrison, Rebecca E.; Baptista, Ricardo; Marzouk, Youssef] MIT, Cambridge, MA 02139 USA.
RP Morrison, RE (reprint author), MIT, Cambridge, MA 02139 USA.
EM rmorriso@mit.edu; rsb@mit.edu; ymarz@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU AFOSR MURI [FA9550-15-1-0038]
FX This work has been supported in part by the AFOSR MURI on "Managing
   multiple information sources of multi-physics systems," program officer
   Jean-Luc Cambier, award FA9550-15-1-0038. We would also like to thank
   Daniele Bigoni for generous help with code implementation and execution.
CR Bigoni D., COMPUTATION MO UNPUB
   Bogachev VI, 2005, SB MATH+, V196, P309, DOI 10.1070/SM2005v196n03ABEH000882
   Cai T, 2011, J AM STAT ASSOC, V106, P672, DOI 10.1198/jasa.2011.tm10560
   El Moselhy TA, 2012, J COMPUT PHYS, V231, P7815, DOI 10.1016/j.jcp.2012.07.022
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Ghosh SK, 2016, NEW J PHYS, V18, DOI 10.1088/1367-2630/18/1/013027
   Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050
   Knothe H., 1957, MICHIGAN MATH J, V1957
   Koller D., 2009, PROBABILISTIC GRAPHI
   Liu CL, 2004, MAGN RESON MED, V51, P924, DOI 10.1002/mrm.20071
   Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Loh P.-L., 2012, NIPS, P2096
   Marzouk Y., 2016, HDB UNCERTAINTY QUAN
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   OEHLERT GW, 1992, AM STAT, V46, P27, DOI 10.2307/2684406
   Parno M., 2014, ARXIV14125492
   Parno M, 2016, SIAM-ASA J UNCERTAIN, V4, P1160, DOI 10.1137/15M1032478
   PENG CK, 1993, PHYS REV LETT, V70, P1343, DOI 10.1103/PhysRevLett.70.1343
   Perron M, 2013, J CLIMATE, V26, P1063, DOI 10.1175/JCLI-D-11-00504.1
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394
   Rue H., 2005, GAUSSIAN MARKOV RAND
   SAAD Y., 2003, ITERATIVE METHODS SP
   Sengupta A, 2016, AUST NZ J STAT, V58, P15, DOI 10.1111/anzs.12148
   Spantini A, 2017, ARXIV170306131
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402040
DA 2019-06-15
ER

PT S
AU Moseley, B
   Wang, JR
AF Moseley, Benjamin
   Wang, Joshua R.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Approximation Bounds for Hierarchical Clustering: Average Linkage,
   Bisecting K-means, and Local Search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use, the method has an underdeveloped analytical foundation. Having a well understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta [Das16]. The main result is that one of the most popular algorithms used in practice, average linkage agglomerative clustering, has a small constant approximation ratio for this objective. Furthermore, this paper establishes that using bisecting k-means divisive clustering has a very poor lower bound on its approximation ratio for the same objective. However, we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function. This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well.
C1 [Moseley, Benjamin] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Wang, Joshua R.] Stanford Univ, Dept Comp Sci, 353 Serra Mall, Stanford, CA 94305 USA.
RP Moseley, B (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM moseleyb@andrew.cmu.edu; joshua.wang@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Google Research Award; Yahoo Research Award; NSF [CCF-1524062,
   CCF-1617724, CCF-1733873, CCF-1725661]
FX Benjamin Moseley was supported in part by a Google Research Award, a
   Yahoo Research Award and NSF Grants CCF-1617724, CCF-1733873 and
   CCF-1725661. This work was partially done while the author was working
   at Washington University in St. Louis.; Joshua R. Wang was supported in
   part by NSF Grant CCF-1524062.
CR Ackerman M, 2016, J MACH LEARN RES, V17
   Ackerman Margareta, 2012, P 26 AAAI C ART INT
   Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794
   Awasthi P., 2015, P 2015 C INN THEOR C, P191
   Ben-David Shai, 2008, P 22 ANN C NEUR INF, P121
   Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841
   Cohen-Addad Vincent, 2017, CORR
   Dasgupta S, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P118, DOI 10.1145/2897518.2897527
   Ghahramani Z., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389
   Hastie T., 2009, UNSUPERVISED LEARNIN, P485
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Krishnamurthy Akshay, 2012, P 29 INT C MACH LEAR
   Murtagh F, 2012, WIRES DATA MIN KNOWL, V2, P86, DOI 10.1002/widm.53
   Roy A., 2016, ADV NEURAL INFORM PR, P2316
   Zadeh R. B., 2009, P 25 C UNC ART INT, P639
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403016
DA 2019-06-15
ER

PT S
AU Motiian, S
   Jones, Q
   Iranmanesh, SM
   Doretto, G
AF Motiian, Saeid
   Jones, Quinn
   Iranmanesh, Seyed Mehdi
   Doretto, Gianfranco
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Few-Shot Adversarial Domain Adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SVM
AB This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high "speed" of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.
C1 [Motiian, Saeid; Jones, Quinn; Iranmanesh, Seyed Mehdi; Doretto, Gianfranco] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
RP Motiian, S (reprint author), West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
EM samotiian@mix.wvu.edu; qjones1@mix.wvu.edu; seiranmanesh@mix.wvu.edu;
   gidoretto@mix.wvu.edu
RI Jeong, Yongwook/N-7413-2016
CR Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504
   Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Becker C, 2013, ADV NEURAL INFORM PR, P485
   Bergamo Alessandro, 2010, P ADV NEUR INF PROC, P181
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Chen L, 2014, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2014.184
   Chopra S, 2005, PROC CVPR IEEE, P539
   Darrell T., 2014, ARXIV14123474
   Daume H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Donahue J., 2013, ARXIV13101531
   Duan LX, 2009, PROC CVPR IEEE, P1375, DOI [10.1109/CVPR.2009.5206747, 10.1109/CVPRW.2009.5206747]
   Fernando B., 2015, PATTERN RECOGITION L
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Ganin Y., 2014, ARXIV14097495
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian, 2016, ARXIV170100160
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gretton A., 2006, NIPS
   Guo Y., 2012, P 29 INT C MACH LEAR
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Isola  P., 2016, ARXIV161107004
   Kingma D. P., 2014, CORR
   Koniusz P., 2016, ARXIV161108195
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu M.-Y., 2016, ADV NEURAL INFORM PR, P469
   Long MS, 2013, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2013.59
   Mingsheng L., 2015, INT C MACH LEARN, P97
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Motiian S, 2016, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR.2016.166
   Motiian S, 2016, LECT NOTES COMPUT SC, V9911, P630, DOI 10.1007/978-3-319-46478-7_39
   Muandet K., 2013, P 30 INT C MACH LEAR, P10
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Reed S., 2016, P 33 INT C MACH LEAR, P1060
   Reed S. E., 2016, ADV NEURAL INFORM PR, P217
   Rozantsev A., 2016, ARXIV160306432
   Russakovsky Olga, 2015, IJCV
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Salimans T., 2016, ADV NEURAL INFORM PR, P2234
   Sankaranarayanan  S., 2017, ARXIV170401705
   Sarafianos N., 2017, ARXIV170809083
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shrivastava A., 2017, IEEE C COMP VIS PATT
   Simonyan K., 2014, CORR
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tommasi T, 2016, LECT NOTES COMPUT SC, V9915, P475, DOI 10.1007/978-3-319-49409-8_39
   Tommasi T, 2015, LECT NOTES COMPUT SC, V9358, P504, DOI 10.1007/978-3-319-24947-6_42
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tzeng E., 2015, ICCV
   Tzeng  Eric, 2017, IEEE C COMP VIS PATT
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Yang J., 2007, 7 IEEE INT C DAT MIN, P69
   Yao T., 2015, IEEE C COMP VIS PATT
   Zhang H., 2016, ARXIV161203242
NR 64
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406071
DA 2019-06-15
ER

PT S
AU Mourtada, J
   Gaiffas, S
   Scornet, E
AF Mourtada, Jaouad
   Gaiffas, Stephane
   Scornet, Erwan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Universal consistency and minimax rates for online Mondrian Forests
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION; AGGREGATION
AB We establish the consistency of an algorithm of Mondrian Forests [LRT14, LRT16], a randomized classification algorithm that can be implemented online. First, we amend the original Mondrian Forest algorithm proposed in [LRT14], that considers a fixed lifetime parameter. Indeed, the fact that this parameter is fixed hinders the statistical consistency of the original procedure. Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters lambda(n), and uses an alternative updating rule, allowing to work also in an online fashion. Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results [AG14] to an arbitrary dimension.
C1 [Mourtada, Jaouad; Gaiffas, Stephane; Scornet, Erwan] Ecole Polytech, Ctr Math Appl, Palaiseau, France.
RP Mourtada, J (reprint author), Ecole Polytech, Ctr Math Appl, Palaiseau, France.
EM jaouad.mourtada@polytechnique.edu; stephane.gaiffas@polytechnique.edu;
   erwan.scornet@polytechnique.edu
CR Arlot S, 2014, ARXIV14073939
   Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217
   Balog Matej, 2016, 32 C UNC ART INT UAI
   Biau G, 2016, TEST-SPAIN, V25, P197, DOI 10.1007/s11749-016-0481-7
   Biau G, 2012, J MACH LEARN RES, V13, P1063
   Biau G, 2008, J MACH LEARN RES, V9, P2015
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 2004, 670 U CAL BERK
   Breiman Leo, 2000, 577 U CAL BERK STAT
   Denil M., 2014, P 31 INT C MACH LEAR, P665
   Denil M., 2013, P 30 INT C MACH LEAR, P1256
   Devroye L., 1996, APPL MATH, V31
   Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107
   Emery M, 2000, LECT NOTES MATH, V1738, P5
   Freund Y, 1997, P 29 ANN ACM S THEOR, P334, DOI DOI 10.1145/258533.258616
   Genuer R, 2012, J NONPARAMETR STAT, V24, P543, DOI 10.1080/10485252.2012.677843
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Helmbold DP, 1997, MACH LEARN, V27, P51, DOI 10.1023/A:1007396710653
   Lakshminarayanan Balaji, 2016, P 19 INT WORKSH ART
   Lakshminarayanan Balaji, 2014, ADV NEURAL INFORM PR, V27, P3140
   Lecue G, 2007, BERNOULLI, V13, P1000, DOI 10.3150/07-BEJ6044
   Mammen E, 1999, ANN STAT, V27, P1808
   Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Roy Daniel M., 2011, THESIS
   Roy Daniel M., 2009, ADV NEURAL INF PROCE, P1377
   Saffari A., 2009, 3 IEEE ICCV WORKSH O
   Scornet E, 2015, ANN STAT, V43, P1716, DOI 10.1214/15-AOS1321
   Taddy MA, 2011, J AM STAT ASSOC, V106, P109, DOI 10.1198/jasa.2011.ap09769
   Tsybakov AB, 2004, ANN STAT, V32, P135
   WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012
   Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403080
DA 2019-06-15
ER

PT S
AU Mujika, A
   Meier, F
   Steger, A
AF Mujika, Asier
   Meier, Florian
   Steger, Angelika
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fast-Slow Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LONG-TERM DEPENDENCIES
AB Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to 1.1 9 and 1.2 5 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks.
C1 [Mujika, Asier; Meier, Florian; Steger, Angelika] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Mujika, A (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM asierm@ethz.ch; meierflo@inf.ethz.ch; steger@inf.ethz.ch
CR Ba J. L., 2016, ARXIV160706450
   Bahdanau Dzmitry, 2016, ACOUSTICS SPEECH SIG
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chung J., 2015, INT C MACH LEARN, p2067 , DOI DOI 10.1145/2661829.2661935
   Chung J., 2016, ARXIV160901704
   DiPietro Robert, 2017, REVISITING NARX RECU
   El Hihi Salah, 1995, NIPS, V409
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Graves A, 2013, ARXIV13080850
   Graves A., 2014, ARXIV14105401
   Graves A., 2016, ARXIV160308983
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Ha David, 2016, ARXIV161101578
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1991, THESIS
   Hutter  M., 2012, HUMAN KNOWLEDGE COMP
   Jaeger Herbert, 2007, TECHNICAL REPORT
   Jing Li, 2016, TUNABLE EFFICIENT UN
   Kalchbrenner N., 2016, ARXIV161010099
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koutnik Jan, 2016, ARXIV160308983
   Lin TN, 1996, IEEE T NEURAL NETWOR, V7, P1329, DOI 10.1109/72.548162
   Mahoney Matt, 2017, LARGE TEXT COMPRESSI
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   Mikolov  T., 2012, SUBWORD LANGUAGE MOD
   Pascanu R., 2013, ARXIV13126026
   Robinson  A., 1987, UTILITY DRIVEN DYNAM
   Rocki Kamil, 2016, ARXIV161007675
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Sordoni A., 2015, P 24 ACM INT C INF K, V19, P553, DOI DOI 10.1145/2806416.2806493
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2015, ARXIV150500387
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Weston J., 2014, ARXIV14103916
   Williams Ronald J., 1989, NUCCS8927
   Zaremba W, 2014, ARXIV14092329
   Zilly J. G., 2016, ARXIV160703474
   Zoph B., 2016, ARXIV161101578
NR 42
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405096
DA 2019-06-15
ER

PT S
AU Mukherjee, SS
   Sarkar, P
   Lin, LZ
AF Mukherjee, Soumendu Sundar
   Sarkar, Purnamrita
   Lin, Lizhen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On clustering network-valued data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LIKELIHOOD
AB Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to cluster multiple networks. This is largely motivated by the routine collection of network data that are generated from potentially different populations. These networks may or may not have node correspondence. When node correspondence is present, we cluster networks by summarizing a network by its graphon estimate, whereas when node correspondence is not present, we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical justifications are provided in terms of consistency.
C1 [Mukherjee, Soumendu Sundar] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
   [Sarkar, Purnamrita] Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA.
   [Lin, Lizhen] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
RP Mukherjee, SS (reprint author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
EM soumendu@berkeley.edu; purna.sarkar@austin.utexas.edu; lizhen.lin@nd.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF-FRG [DMS-1160319]; Loeve Fellowship; NSF [IIS 1663870, DMS 1654579,
   DMS 1713082]; DARPA [N-66001-17-1-4041]
FX We thank Professor Peter J. Bickel for helpful discussions. SSM was
   partially supported by NSF-FRG grant DMS-1160319 and a Loeve Fellowship.
   PS was partially supported by NSF grant DMS 1713082. LL was partially
   supported by NSF grants IIS 1663870, DMS 1654579 and a DARPA grant
   N-66001-17-1-4041.
CR ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3
   Aliakbary S, 2015, CHAOS, V25, DOI 10.1063/1.4908605
   Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138
   Amini Arash A, 2014, ARXIV14065647
   [Anonymous], 2017, SUPPLEMENT CLUSTERIN
   Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103
   Bickel P, 2013, ANN STAT, V41, P1922, DOI 10.1214/13-AOS1124
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Chen H, 2015, ANN STAT, V43, P139, DOI 10.1214/14-AOS1269
   Choi David S, 2012, BIOMETRIKA
   Erdos P, 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.1234/12345678
   Gao C, 2015, ANN STAT, V43, P2624, DOI 10.1214/15-AOS1354
   Ginestet C. E., 2014, ARXIV E PRINTS
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Hoover D. N., 1979, TECHNICAL REPORT
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274
   Mahadevan P, 2006, ACM SIGCOMM COMP COM, V36, P135, DOI 10.1145/1151659.1159930
   Maugis Pierre-Andre G, 2017, ARXIV170505677
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Newman MEJ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208701
   Olhede SC, 2014, P NATL ACAD SCI USA, V111, P14722, DOI 10.1073/pnas.1400374111
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Shervashidze N., 2009, P 12 INT C ART INT S, V2009, P488
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Wolfe P. J., 2013, ARXIV E PRINTS
   Zhang Y., 2015, ARXIV E PRINTS
   Zhou S, 2004, IEEE COMMUN LETT, V8, P180, DOI 10.1109/LCOMM.2004.823426
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407016
DA 2019-06-15
ER

PT S
AU Murata, T
   Suzuki, T
AF Murata, Tomoya
   Suzuki, Taiji
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for
   Regularized Empirical Risk Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches has become a golden standard in the machine learning community, because the mini-batch techniques stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new "double acceleration" technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size root n mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where n is the training set size. Further, we show that even in non-mini-batch settings, our method achieves the best known convergence rate for non-strongly convex and strongly convex objectives.
C1 [Murata, Tomoya] NTT DATA Math Syst Inc, Tokyo, Japan.
   [Suzuki, Taiji] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Math Informat, Tokyo, Japan.
   [Suzuki, Taiji] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.
   [Suzuki, Taiji] RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan.
RP Murata, T (reprint author), NTT DATA Math Syst Inc, Tokyo, Japan.
EM murata@msi.co.jp; taiji@mist.i.u-tokyo.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU MEXT kakenhi [25730013, 25120012, 26280009, 15H05707]; JST-PRESTO;
   JST-CREST
FX This work was partially supported by MEXT kakenhi (25730013, 25120012,
   26280009 and 15H05707), JST-PRESTO and JST-CREST.
CR Allen-Zhu Z., 2017, 48 ANN ACM S THEOR C, P19
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Duchi J., 2009, ADV NEURAL INFORM PR, V22, P495
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Le Roux N., 2012, ADV NEURAL INFORM PR, V25, P2663
   Li H, 2015, ADV NEURAL INFORM PR, P379
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Lin Q., 2014, ADV NEURAL INFORM PR, V27, P3059
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   Nitanda A., 2016, ARTIF INTELL, P195
   O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3
   Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6
   Shalev-Shwartz S., 2007, TECHNICAL REPORT
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Tseng P., 2008, TECHNICAL REPORT
   Xiao L., 2009, P ADV NEUR INF PROC, P2116
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P1080
   Zhang Yuchen, 2015, P 32 INT C MACH LEAR, P353
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400058
DA 2019-06-15
ER

PT S
AU Murugesan, K
   Carbonell, J
AF Murugesan, Keerthiram
   Carbonell, Jaime
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Active Learning from Peers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION
AB This paper addresses the challenge of learning from peers in an online multitask setting. Instead of always requesting a label from a human oracle, the proposed method first determines if the learner for each task can acquire that label with sufficient confidence from its peers either as a task-similarity weighted sum, or from the single most similar task. If so, it saves the oracle query for later use in more difficult cases, and if not it queries the human oracle. The paper develops the new algorithm to exhibit this behavior and proves a theoretical mistake bound for the method compared to the best linear predictor in hindsight. Experiments over three multitask learning benchmark datasets show clearly superior performance over baselines such as assuming task independence, learning only from the oracle and not learning from peer tasks.
C1 [Murugesan, Keerthiram; Carbonell, Jaime] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Murugesan, K (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM kmuruges@cs.cmu.edu; jgc@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Abernethy J, 2007, LECT NOTES COMPUT SC, V4539, P484, DOI 10.1007/978-3-540-72927-3_35
   Agarwal A., 2013, JMLR P, P1220
   Agarwal Alekh, 2008, UCBEECS2008138
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bartlett PL, 2008, J MACH LEARN RES, V9, P1823
   Cavallanti G, 2009, ADV NEURAL INFORM PR, P249
   Cavallanti G, 2010, J MACH LEARN RES, V11, P2901
   Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P1205
   Cohen Haim, 2014, ADV NEURAL INFORM PR, P1170
   Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5
   Crammer K., 2012, ADV NEURAL INFORM PR, P1475
   Dekel O, 2007, J MACH LEARN RES, V8, P2233
   Dekel O, 2012, J MACH LEARN RES, V13, P2655
   Donmez P., 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165
   Donmez P, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P259
   Lugosi Gabor, 2009, ARXIV09023526
   Murugesan K., 2016, ADV NEURAL INFORM PR, P4296
   Orabona F., 2011, P 28 INT C MACH LEAR, P433
   Saha Avishek, 2011, INT C ART INT STAT, P643
   Urner Ruth, 2012, AISTATS, P4
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Xue Y, 2007, J MACH LEARN RES, V8, P35
   Yan Y, 2011, P 28 INT C MACH LEAR, P1161
   Zhang C, 2015, ADV NEURAL INFORM PR, V28, P703
   Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407010
DA 2019-06-15
ER

PT S
AU Musco, C
   Musco, C
AF Musco, Cameron
   Musco, Christopher
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Recursive Sampling for the Nystrom Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MATRIX; APPROXIMATION
AB We give the first algorithm for kernel Nystrom approximation that runs in linear time in the number of training points and is provably accurate for all kernel matrices, without dependence on regularity or incoherence conditions. The algorithm projects the kernel onto a set of s landmark points sampled by their ridge leverage scores, requiring just O(ns) kernel evaluations and O(ns(2)) additional runtime. While leverage score sampling has long been known to give strong theoretical guarantees for Nystrom approximation, by employing a fast recursive sampling scheme, our algorithm is the first to make the approach scalable. Empirically we show that it finds more accurate kernel approximations in less time than popular techniques such as classic Nystrom approximation and the random Fourier features method.
C1 [Musco, Cameron; Musco, Christopher] MIT, EECS, Cambridge, MA 02139 USA.
RP Musco, C (reprint author), MIT, EECS, Cambridge, MA 02139 USA.
EM cnmusco@mit.edu; cpmusco@mit.edu
CR Achlioptas Dimitris, 2001, ADV NEURAL INFORM PR
   Alaoui Ahmed, 2015, ADV NEURAL INFORM PR, V28, P775
   Avron Haim, 2014, ADV NEURAL INFORM PR, V27, P2258
   Bach F, 2002, J MACHINE LEARNING R, V3, P1
   Bach Francis, 2013, P 26 ANN C COMP LEAR
   Balcan MF, 2006, MACH LEARN, V65, P79, DOI 10.1007/s10994-006-7550-1
   Belabbas MA, 2009, P NATL ACAD SCI USA, V106, P369, DOI 10.1073/pnas.0810600105
   Boutsidis C., 2009, ADV NEURAL INFORM PR, P153
   Boutsidis Christos, 2016, P 48 ANN ACM S THEOR
   Calandriello Daniele, 2017, P 20 INT C ART INT S
   Calandriello Daniele, 2016, P 32 ANN C UNC ART I, P62
   Chen Shouyuan, 2015, P 31 C UNC ART INT, P201
   Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cohen M. B., 2015, P 2015 C INN THEOR C, P181
   Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758
   Cortes C., 2010, JMLR P TRACK, P113
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   Gittens A., 2011, ARXIV11105305
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI [DOI 10.1145/1656274.1656278, 10.1145/1656274.1656278]
   Hastie T., 2002, ELEMENTS STAT LEARNI
   Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1
   IBM Reseach Division Skylark Team, 2014, LIBSK SKETCH BAS DIS
   Kumar S, 2012, J MACH LEARN RES, V13, P981
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Li Chengtao, 2016, P 33 INT C MACH LEAR
   Li M, 2015, IEEE T NEUR NET LEAR, V26, P152, DOI 10.1109/TNNLS.2014.2359798
   Lichman M., 2013, UCI MACHINE LEARNING
   Mitzenmacher M., 2017, PROBABILITY COMPUTIN
   Paul S, 2016, NEURAL COMPUT, V28, P716, DOI 10.1162/NECO_a_00816
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Platt John, 2005, P 8 INT C ART INT ST
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1648
   Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Silva V. D., 2003, ADV NEURAL INFORM PR, P721
   Smola A.J., 2000, P 17 INT C MACH LEAR, P911
   Tong Zhang, 2006, LEARNING, V17
   Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048
   Tu Stephen, 2016, ARXIV160205310
   Uzilov AV, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-173
   Wainwright Martin, 2013, P 26 ANN C COMP LEAR
   Wang SS, 2013, J MACH LEARN RES, V14, P2729
   Wang Weiran, 2016, ARXIV160202172
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Yang T., 2012, ADV NEURAL INFORM PR, P476
   Yun Yang, 2015, ANN STAT
   Zhang K., 2008, P 25 INT C MACH LEAR, P1232, DOI DOI 10.1145/1390156.1390311
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403087
DA 2019-06-15
ER

PT S
AU Musco, C
   Woodruff, DP
AF Musco, Cameron
   Woodruff, David P.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NYSTROM METHOD; MATRIX
AB Low-rank approximation is a common tool used to accelerate kernel methods: the n x n kernel matrix K is approximated via a rank-k matrix (K) over tilde which can be stored in much less space and processed more quickly. In this work we study the limits of computationally efficient low-rank kernel approximation. We show that for a broad class of kernels, including the popular Gaussian and polynomial kernels, computing a relative error k-rank approximation to K is at least as difficult as multiplying the input data matrix A is an element of R-nxd by an arbitrary matrix C is an element of R-dxk. Barring a breakthrough in fast matrix multiplication, when k is not too large, this requires Omega(nnz(A)k) time where nnz(A) is the number of non-zeros in A. This lower bound matches, in many parameter regimes, recent work on subquadratic time algorithms for low-rank approximation of general kernels [MM16, MW17], demonstrating that these algorithms are unlikely to be significantly improved, in particular to O(nnz(A)) input sparsity runtimes. At the same time there is hope: we show for the first time that O(nnz(A)) time approximation is possible for general radial basis function kernels (e.g., the Gaussian kernel) for the closely related problem of low-rank approximation of the kernelized dataset.
C1 [Musco, Cameron] MIT, Cambridge, MA 02139 USA.
   [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Musco, C (reprint author), MIT, Cambridge, MA 02139 USA.
EM cnmusco@mit.edu; dwoodruf@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Achlioptas Dimitris, 2001, ADV NEURAL INFORM PR
   Alaoui Ahmed, 2015, ADV NEURAL INFORM PR, V28, P775
   Avron Haim, 2017, P 34 INT C MACH LEAR
   Avron Haim, 2014, ADV NEURAL INFORM PR, V27, P2258
   Bach F, 2002, J MACHINE LEARNING R, V3, P1
   Backurs Arturs, 2017, ADV NEURAL INFORM PR
   Belabbas MA, 2009, P NATL ACAD SCI USA, V106, P369, DOI 10.1073/pnas.0810600105
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758
   Cotter Andrew, 2011, ARXIV11094603
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619
   Friedland S, 2007, SIAM J MATRIX ANAL A, V29, P656, DOI 10.1137/06065551
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   Hedenfalk I, 2001, NEW ENGL J MED, V344, P539, DOI 10.1056/NEJM200102223440801
   Javed A, 2011, ANN HUM GENET, V75, P707, DOI 10.1111/j.1469-1809.2011.00673.x
   Kapralov Michael, 2016, P 33 INT C MACH LEAR, P2101
   Le Gall F., 2014, P INT S SYMB ALG COM, P296, DOI DOI 10.1145/2608628.2608664
   Le Gall F, 2012, ANN IEEE SYMP FOUND, P514, DOI 10.1109/FOCS.2012.80
   Le Gall Francois, 2017, ARXIV170805622
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Musco Cameron, 2016, ADV NEURAL INFORM PR
   Musco Cameron, 2017, P 58 ANN IEEE S FDN
   Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Smola A.J., 2000, P 17 INT C MACH LEAR, P911
   Wang SS, 2013, J MACH LEARN RES, V14, P2729
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Zhang K., 2008, P 25 INT C MACH LEAR, P1232, DOI DOI 10.1145/1390156.1390311
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404049
DA 2019-06-15
ER

PT S
AU Nachum, O
   Norouzi, M
   Xu, K
   Schuurmans, D
AF Nachum, Ofir
   Norouzi, Mohammad
   Xu, Kelvin
   Schuurmans, Dale
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bridging the Gap Between Value and Policy Based Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.(2)
C1 [Nachum, Ofir; Norouzi, Mohammad; Xu, Kelvin; Schuurmans, Dale] Google Brain, Mountain View, CA 94043 USA.
RP Nachum, O (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM ofirnachum@google.com; mnorouzi@google.com; kelvinxx@google.com;
   daes@ualberta.ca
RI Jeong, Yongwook/N-7413-2016
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2
   Asadi K., 2016, ARXIV161205628
   Azar M. G., 2011, AISTATS
   Azar M. G., 2012, MACH LEARN J, V87
   Azar M. G., 2012, JMLR, V13
   Fox R., 2016, UAI
   Gruslys Audrunas, 2017, ARXIV170404651
   Gu S., 2017, ICLR
   Gu S., 2016, ICRA
   Haarnoja T., 2017, ARXIV170208165
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Huang D.-A., 2015, APPROXIMATE MAXENT I
   Kakade S., 2001, NIPS
   Kappen HJ, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11011
   Kober J., 2013, IJRR
   Levine  S., 2016, JMLR, V17
   Li L., 2010, CONTEXTUAL BANDIT AP
   Lillicrap T. P., 2016, ICLR
   Littman M. L., 1996, THESIS
   Mnih V., 2016, ICML
   Mnih V., 2015, NATURE
   Munos R., 2016, NIPS
   Nachum Ofir, 2017, ICLR
   O'Donoghue Brendan, 2017, ICLR
   Peng J, 1996, MACH LEARN, V22, P283, DOI 10.1007/BF00114731
   Peters J., 2010, AAAI
   Precup D., 2001, OFF POLICY TEMPORAL
   Precup D., 2000, COMPUTER SCI DEP FAC, P80
   Schaul T., 2016, ICLR
   Schulman J., 2017, ARXIV170406440
   Schulman J., 2016, ICLR
   Schulman John, 2015, ICML
   Silver D., 2014, ICML
   Sutton R., 2017, INTRO REINFORCEMENT
   Sutton R. S., 1999, NIPS
   Tesauro G., 1995, CACM
   Theocharous G., 2015, IJCAI
   Todorov E., 2006, NIPS
   Todorov E., 2010, NIPS
   Wang Z., 2016, ICLR
   Wang Ziyu, 2017, ICLR
   Watkins C. J. C. H., 1989, THESIS
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Williams R. J., 1992, MACH LEARN J
   Williams R. J., 1991, CONNECTION SCI
   Ziebart B. D., 2010, THESIS
   Ziebart Brian D, 2008, AAAI
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402080
DA 2019-06-15
ER

PT S
AU Nam, J
   Mencia, EL
   Kim, HJ
   Furnkranz, J
AF Nam, Jinseok
   Mencia, Eneldo Loza
   Kim, Hyunwoo J.
   Fuernkranz, Johannes
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label
   Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Multi-label classification is the task of predicting a set of labels for a given input instance. Classifier chains are a state-of-the-art method for tackling such problems, which essentially converts this problem into a sequential prediction problem, where the labels are first ordered in an arbitrary fashion, and the task is to predict a sequence of binary values for these labels. In this paper, we replace classifier chains with recurrent neural networks, a sequence-to-sequence prediction algorithm which has recently been successfully applied to sequential prediction tasks in many domains. The key advantage of this approach is that it allows to focus on the prediction of the positive labels only, a much smaller set than the full set of possible labels. Moreover, parameter sharing across all classifiers allows to better exploit information of previous decisions. As both, classifier chains and recurrent neural networks depend on a fixed ordering of the labels, which is typically not part of a multi-label problem specification, we also compare different ways of ordering the label set, and give some recommendations on suitable ordering strategies.
C1 [Nam, Jinseok; Mencia, Eneldo Loza; Fuernkranz, Johannes] Tech Univ Darmstadt, Knowledge Engn Grp, Darmstadt, Germany.
   [Kim, Hyunwoo J.] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
RP Nam, J (reprint author), Tech Univ Darmstadt, Knowledge Engn Grp, Darmstadt, Germany.
RI Jeong, Yongwook/N-7413-2016
FU German Institute for Educational Research (DIPF) under the Knowledge
   Discovery in Scientific Literature (KDSL) program; German Research
   Foundation as part of the Research Training Group Adaptive Preparation
   of Information from Heterogeneous Sources (AIPHES) [GRK 1994/1]
FX The authors would like to thank anonymous reviewers for their thorough
   feedback. Computations for this research were conducted on the
   Lichtenberg high performance computer of the Technische Universitat
   Darmstadt. The Titan X used for this research was donated by the NVIDIA
   Corporation. This work has been supported by the German Institute for
   Educational Research (DIPF) under the Knowledge Discovery in Scientific
   Literature (KDSL) program, and the German Research Foundation as part of
   the Research Training Group Adaptive Preparation of Information from
   Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.
CR Bahdanau D., 2015, P INT C LEARN REPR
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Dembczynski K, 2012, FRONT ARTIF INTEL AP, V242, P294, DOI 10.3233/978-1-61499-098-7-294
   Dembczynski Krzysztof, 2010, P 27 INT C MACH LEAR, P279
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Doppa J. R., 2014, P AAAI C ART INT
   Furnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Gehring J., 2017, P 34 INT C MACH LEAR, P1243
   Ghamrawi N., 2005, P 14 ACM INT C INF K, P195, DOI [DOI 10.1145/1099554.1099591, 10.1145/1099554.1099591]
   Grave E., 2017, P ICML PMLR AUG, P1302
   Jasinska Kalina, 2016, P 33 INT C MACH LEAR, P1435
   Kingma D. P., 2015, P INT C LEARN REPR
   Kumar A., 2016, INT C MACH LEARN, V48, P1378
   Kumar A, 2013, MACH LEARN, V92, P65, DOI 10.1007/s10994-013-5371-6
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li  C., 2016, P 33 INT C MACH LEAR, P2482
   Liu W., 2015, ADV NEURAL INFORM PR, P712
   Mena D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3707
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Nam Jinseok, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P437, DOI 10.1007/978-3-662-44851-9_28
   Nan Li, 2013, Multiple Classifier Systems. 11th International Workshop, MCS 2013. Proceedings, P146, DOI 10.1007/978-3-642-38067-9_13
   Prabhu Y., 2014, P 20 ACM SIGKDD INT, P263
   Read J, 2014, PATTERN RECOGN, V47, P1535, DOI 10.1016/j.patcog.2013.10.006
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Senge R, 2014, STUD CLASS DATA ANAL, P163, DOI 10.1007/978-3-319-01595-8_18
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sucar LE, 2014, PATTERN RECOGN LETT, V41, P14, DOI 10.1016/j.patrec.2013.11.007
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tsoumakas G, 2011, IEEE T KNOWL DATA EN, V23, P1079, DOI 10.1109/TKDE.2010.164
   Waegeman W, 2014, J MACH LEARN RES, V15, P3333
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405048
DA 2019-06-15
ER

PT S
AU Namkoong, H
   Duchi, JC
AF Namkoong, Hongseok
   Duchi, John C.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variance-based Regularization with Convex Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID INEQUALITIES
AB We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.
C1 [Namkoong, Hongseok; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA.
RP Namkoong, H (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM hnamk@stanford.edu; jduchi@stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU SAIL-Toyota Center for AI Research; Samsung Fellowship; National Science
   Foundation [NSF-CAREER-1553086]; Sloan Foundation
FX We thank Feng Ruan for pointing out a much simpler proof of Theorem 1
   than in our original paper. JCD and HN were partially supported by the
   SAIL-Toyota Center for AI Research and HN was partially supported
   Samsung Fellowship. JCD was also partially supported by the National
   Science Foundation award NSF-CAREER-1553086 and the Sloan Foundation.
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641
   Bertsimas D., 2014, ARXIV14084445MATHOC
   Boucheron S., 2013, CONCENTRATION INEQUA
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Boyd S., 2004, CONVEX OPTIMIZATION
   Duchi J. C., 2016, ARXIV161003425STAT M
   Duchi J. C., 2008, P 25 INT C MACH LEAR
   Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA
   Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lichman M., 2013, UCI MACHINE LEARNING
   Mammen E, 1999, ANN STAT, V27, P1808
   Maurer A., 2009, P 22 ANN C COMP LEAR
   Mendelson S., 2014, P 27 ANN C COMP LEAR
   Namkoong H., 2016, ADV NEURAL INFORM PR, P29
   Owen A. B., 2001, EMPIRICAL LIKELIHOOD
   Samson PM, 2000, ANN PROBAB, V28, P416
   Shapiro A, 2009, LECT STOCHASTIC PROG
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
   Zubkov AM, 2013, THEOR PROBAB APPL+, V57, P539, DOI 10.1137/S0040585X97986138
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403004
DA 2019-06-15
ER

PT S
AU Nan, F
   Saligrama, V
AF Nan, Feng
   Saligrama, Venkatesh
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Classification for Prediction Under a Budget
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.
C1 [Nan, Feng] Boston Univ, Syst Engn, Boston, MA 02215 USA.
   [Saligrama, Venkatesh] Boston Univ, Elect Engn, Boston, MA 02215 USA.
RP Nan, F (reprint author), Boston Univ, Syst Engn, Boston, MA 02215 USA.
EM fnan@bu.edu; srv@bu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF: 1320566, CNS: 1330008, CCF: 1527618, DHS 2013-ST-061-ED0001];
   NGA Grant [HM1582-09-1-0037]; ONR [N00014-13-C-0288]
FX Feng Nan would like to thank Dr Ofer Dekel for ideas and discussions on
   resource constrained machine learning during an internship in Microsoft
   Research in summer 2016. Familiarity and intuition gained during the
   internship contributed to the motivation and formulation in this paper.
   We also thank Dr Joseph Wang and Tolga Bolukbasi for discussions and
   helps in experiments. This material is based upon work supported in part
   by NSF Grants CCF: 1320566, CNS: 1330008, CCF: 1527618, DHS
   2013-ST-061-ED0001, NGA Grant HM1582-09-1-0037 and ONR Grant
   N00014-13-C-0288.
CR Bolukbasi Tolga, 2017, P MACHINE LEARNING R, P527
   Breiman L., 1984, CLASSIFICATION REGRE
   Busa-Fekete Robert, 2012, P 29 INT C MACH LEAR
   Chapelle O., 2011, P YAH LEARN RANK CHA
   Chen M., 2012, P INT C ART INT STAT, V22, P218
   Frank A., 2010, UCI MACHINE LEARNING
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gao  T., 2011, ADV NEURAL INFORM PR
   Graca J, 2008, P OF NIPS 2007, V20, P569
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Krizhevsky A., 2009, THESIS
   Kumar Ashish, 2017, 34 INT C MACH LEARN, P1935
   Kusner M, 2014, AAAI C ART INT
   Lopez-Paz D., 2016, INT C LEARN REPR
   Nan F., 2016, ADV NEURAL INFORM PR, P2334
   Nan F., 2015, P 32 INT C MACH LEAR, P1983
   Nan F, 2014, INT CONF ACOUST SPEE
   Robinson Daniel P., 2016, P 25 INT JOINT C ART, P1974
   Trapeznikov K., 2013, P 16 INT C ART INT S, P581
   Wang Joseph, 2015, ADV NEURAL INFORM PR, V28, P2143
   Wang Joseph, 2014, MODEL SELECTION LINE, P647
   Weiss D, 2013, IEEE I CONF COMP VIS, P2656, DOI 10.1109/ICCV.2013.330
   Xu Z, 2013, INT C MACH LEARN
   Xu Z., 2012, P 29 INT C MACH LEAR
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404077
DA 2019-06-15
ER

PT S
AU Neklyudov, K
   Molchanov, D
   Ashukha, A
   Vetrov, D
AF Neklyudov, Kirill
   Molchanov, Dmitry
   Ashukha, Arsenii
   Vetrov, Dmitry
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Structured Bayesian Pruning via Log-Normal Multiplicative Noise
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.
C1 [Neklyudov, Kirill; Molchanov, Dmitry; Ashukha, Arsenii; Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   [Neklyudov, Kirill; Ashukha, Arsenii; Vetrov, Dmitry] Yandex, Moscow, Russia.
   [Molchanov, Dmitry] Skolkovo Inst Sci & Technol, Moscow, Russia.
RP Neklyudov, K (reprint author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.; Neklyudov, K (reprint author), Yandex, Moscow, Russia.
EM k.necludov@gmail.com; dmolchanov@hse.ru; aashukha@hse.ru; dvetrov@hse.ru
RI Jeong, Yongwook/N-7413-2016
FU HSE International lab of Deep Learning and Bayesian Methods - Russian
   Academic Excellence Project '5-100'; Ministry of Education and Science
   of the Russian Federation [14.756.31.0001]; Russian Science Foundation
   [17-11-01027]
FX We would like to thank Christos Louizos and Max Welling for valuable
   discussions. Kirill Neklyudov and Arsenii Ashukha were supported by HSE
   International lab of Deep Learning and Bayesian Methods which is funded
   by the Russian Academic Excellence Project '5-100'. Dmitry Molchanov was
   supported by the Ministry of Education and Science of the Russian
   Federation (grant 14.756.31.0001). Dmitry Vetrov was supported by the
   Russian Science Foundation grant 17-11-01027.
CR Abadi M., 2016, ARXIV160304467
   Figurnov M., 2016, ADV NEURAL INFORM PR, P947
   Figurnov M., 2016, ARXIV161202297
   Garipov T., 2016, ARXIV161103214
   Han S., 2015, ARXIV151000149
   Jaderberg M., 2014, ARXIV14053866
   James B, P PYTH SCI COMP C SC
   Kingma D. P., 2015, ADV NEURAL INFORM PR, P2575
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P9
   Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lobacheva E., 2017, ARXIV170800077
   Louizos Christos, 2015, THESIS
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Molchanov Dmitry, 2016, BAYES DEEP LEARN WOR
   Molchanov Dmitry, 2017, ARXIV170105369
   Novikov A., 2015, ADV NEURAL INFORM PR, P442
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Ullrich K., 2017, ARXIV170204008
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang S., 2013, P 30 INT C MACH LEAR, p[118, 126]
   Wen  W., 2016, ADV NEURAL INFORM PR, P2074
   Zagoruyko S., 2015, 92 45 CIFAR 10 TORCH
   Zhang C, 2016, ARXIV161103530
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406081
DA 2019-06-15
ER

PT S
AU Neubig, G
   Goldberg, Y
   Dyer, C
AF Neubig, Graham
   Goldberg, Yoav
   Dyer, Chris
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On-the-fly Operation Batching in Dynamic Computation Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits-both static and dynamic-require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.(2)
C1 [Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
   [Goldberg, Yoav] Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel.
   [Dyer, Chris] DeepMind, London, England.
RP Neubig, G (reprint author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
EM gneubig@cs.cmu.edu; yogo@cs.biu.ac.il; cdyer@google.com
RI Jeong, Yongwook/N-7413-2016
FU Israeli Science Foundation [1555/15]; Intel Collaborative Research
   Institute for Computational Intelligence (ICRI-CI)
FX The work of YG is supported by the Israeli Science Foundation (grant
   number 1555/15) and by the Intel Collaborative Research Institute for
   Computational Intelligence (ICRI-CI).
CR Abadi M., 2016, ARXIV160304467
   Ballesteros M., 2016, C EMP METH NAT LANG, P2005
   Bartholomew-Biggs M, 2000, J COMPUT APPL MATH, V124, P171, DOI 10.1016/S0377-0427(00)00422-2
   Battaglia P., 2016, NEURAL INFORM PROCES
   Bengio Samy, 2015, ABS150603099 CORR
   Bowman S. R., 2016, P 54 ANN M ASS COMP, V1, P1466
   Breuleux Olivier, 2010, P 9 PYTH SCI C, P1
   Dyer C., 2015, P 53 ANN M ASS COMP, V1, P334, DOI DOI 10.3115/V1/P15-1033
   Dyer C, 2016, NAACL HLT, P199
   Goldberg Yoav, 2013, T ASS COMPUTATIONAL, V1, P403
   Hadjis Stefan, 2015, P 4 WORKSH DAT AN SC
   Huang Z., 2015, ARXIV150801991
   Kiperwasser E., 2016, T ASS COMPUTATIONAL, V4, P313, DOI DOI 10.1162/tacl_a_00101
   Kiperwasser E., 2016, T ASS COMPUTATIONAL, V4, P445
   Ladhak Faisal, 2016, P INTERSPEECH
   Li Chengtao, 2017, INT C LEARN REPR ICL
   Liang Xiaodan, 2016, P ECCV
   Ling Wang, 2015, P 2015 C EMP METH NA, P1520
   Looks Moshe, 2017, INT C LEARN REPR ICL
   Louppe  G., 2017, ARXIV170200748
   Neubig  G., 2017, ARXIV170103980
   Nothman J, 2013, ARTIF INTELL, V194, P151, DOI 10.1016/j.artint.2012.03.006
   Plank  B., 2016, P 54 ANN M ASS COMP, P412
   Potts CN, 2000, EUR J OPER RES, V120, P228, DOI 10.1016/S0377-2217(99)00153-8
   Reed Scott, 2016, INT C LEARN REPR ICL
   Rohou E, 2013, ACM T ARCHIT CODE OP, V9, DOI 10.1145/2400682.2400685
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shazeer Noam, 2017, INT C LEARN REPR ICL
   Socher R., 2013, C EMP METH NAT LANG
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Tai Kai Sheng, 2015, ANN C ASS COMP LING
   Tokui  S., 2015, P WORKSH MACH LEARN
   Yogatama Dani, 2017, INT C LEARN REPR ICL
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404005
DA 2019-06-15
ER

PT S
AU Newell, A
   Huang, Z
   Deng, J
AF Newell, Alejandro
   Huang, Zhiao
   Deng, Jia
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Associative Embedding: End-to-End Learning for Joint Detection and
   Grouping
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to multi-person pose estimation and report state-of-the-art performance on the MPII and MS-COCO datasets.
C1 [Newell, Alejandro; Deng, Jia] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
   [Huang, Zhiao] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.
RP Newell, A (reprint author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
EM alnewell@umich.edu; hza14@mails.tsinghua.edu.cn; jiadeng@umich.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1734266]; Institute for Interdisciplinary
   Information Sciences, Tsinghua University
FX This work is partially supported by the National Science Foundation
   under Grant No. 1734266. ZH is partially supported by the Institute for
   Interdisciplinary Information Sciences, Tsinghua University.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64
   Bulat A., 2016, ECCV
   Cao Zhe, 2017, COMP VIS PATT REC CV
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chu X., 2017, MULTICONTEXT ATTENTI
   Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740
   Fang H., 2017, ICCV
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Frome A, 2007, IEEE I CONF COMP VIS, P94
   Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Han F, 2009, IEEE T PATTERN ANAL, V31, P59, DOI [10.1109/TPAMI.2008.65, 10.1109/TPAMI.2008.55]
   Harley Adam W, 2016, INT C LEARN REPR WOR
   He K., 2017, MASK R CNN
   Hu PY, 2016, PROC CVPR IEEE, P5600, DOI 10.1109/CVPR.2016.604
   Insafutdinov E., 2016, EUR C COMP VIS ECCV
   Insafutdinov E, 2016, ARXIV161201465
   Iqbal Umar, 2016, ARXIV160808526
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Koltun V, 2011, ADV NEURAL INF PROCE
   Le Q., 2014, P 31 INT C MACH LEAR, P1188, DOI DOI 10.1007/978-1-4614-3223-4
   Levinkov Evgeny, 2017, IEEE C COMP VIS PATT
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Maire M, 2011, IEEE I CONF COMP VIS, P2142, DOI 10.1109/ICCV.2011.6126490
   Maire M, 2010, LECT NOTES COMPUT SC, V6312, P450, DOI 10.1007/978-3-642-15552-9_33
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Newell Alejandro, 2016, ECCV
   Ning G., 2017, ARXIV170502407
   Papandreou G, 2017, ARXIV170101779
   Pishchulin L., 2016, IEEE C COMP VIS PATT
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tompson JJ, 2014, ADV NEURAL INFORM PR, P1799
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wei S.E., 2016, COMPUTER VISION PATT
   Weinberger K. Q., 2005, ADV NEURAL INFORM PR, P1473
   Yu SX, 2009, PROC CVPR IEEE, P2302, DOI 10.1109/CVPRW.2009.5206673
NR 42
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402032
DA 2019-06-15
ER

PT S
AU Newell, A
   Deng, J
AF Newell, Alejandro
   Deng, Jia
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Pixels to Graphs by Associative Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation.
C1 [Newell, Alejandro; Deng, Jia] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
RP Newell, A (reprint author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.
EM alnewell@umich.edu; jiadeng@umich.edu
RI Jeong, Yongwook/N-7413-2016
FU King Abdullah University of Science and Technology (KAUST) Office of
   Sponsored Research (OSR) [OSR-2015-CRG4-2639]
FX This publication is based upon work supported by the King Abdullah
   University of Science and Technology (KAUST) Office of Sponsored
   Research (OSR) under Award No. OSR-2015-CRG4-2639.
CR Abadi M., 2016, ARXIV160304467
   Atzmon Yuval, 2016, ARXIV160807639
   Chao Y.-W., 2017, ARXIV170205448
   Dai B., 2017, ARXIV170403114
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Frome A, 2007, IEEE I CONF COMP VIS, P94
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Hadsell R., 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He K., 2017, ARXIV170306870
   Hu R, 2016, ARXIV161109978
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishna R, 2016, VISUAL GENOME CONNEC
   Li Y, 2017, ARXIV170207191
   Liang X., 2017, ARXIV170303054
   Liao Wentong, 2016, ARXIV160905834
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu Cewu, 2016, ARXIV161207310
   Newell A, 2016, ARXIV161105424
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Plummer B. A., 2016, ARXIV161106641
   Raposo David, 2017, ARXIV170205068
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Weinberger K. Q., 2005, ADV NEURAL INFORM PR, P1473
   Xu D., 2017, P IEEE C COMP VIS PA
   Zhang H., 2017, ARXIV170208319
   Zhuang Bohan, 2017, ARXIV170306246
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402022
DA 2019-06-15
ER

PT S
AU Newling, J
   Fleuret, F
AF Newling, James
   Fleuret, Francois
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI K-Medoids for K-Means Seeding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We show experimentally that the algorithm clarans of Ng and Han (1994) finds better K-medoids solutions than the Voronoi iteration algorithm of Hastie et al. (2001). This finding, along with the similarity between the Voronoi iteration algorithm and Lloyd's K-means algorithm, motivates us to use clarans as a K-means initializer. We show that clarans outperforms other algorithms on 23/23 datasets with a mean decrease over k-means-++ (Arthur and Vassilvitskii, 2007) of 30% for initialization mean squared error (MSE) and 3% for final MSE. We introduce algorithmic improvements to clarans which improve its complexity and runtime, making it a viable initialization scheme for large datasets.
C1 [Newling, James; Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland.
   [Newling, James; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Newling, J (reprint author), Idiap Res Inst, Martigny, Switzerland.; Newling, J (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM james.newling@idiap.ch; francois.fleuret@idiap.ch
RI Jeong, Yongwook/N-7413-2016
FU Hasler Foundation [13018 MASH2]
FX James Newling was funded by the Hasler Foundation under the grant 13018
   MASH2.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bachem O., 2016, NEURAL INFORM PROCES
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P91
   Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021
   Elkan C., 2003, ICML, V3, P147
   HARTIGAN J. A., 1975, CLUSTERING ALGORITHM
   Hastie T., 2001, SPRINGER SERIES STAT
   Kanungo T., 2002, P 18 ANN ACM S COMP, P10
   Kaufman L., 1990, WILEY SERIES PROBABI
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Li YJ, 2007, IEEE T PATTERN ANAL, V29, P1091, DOI 10.1109/TPAMI.2007.1070
   Newling  J., 2016, P 33 INT C MACH LEAR, P936
   Ng R.T., 1994, P 20 INT C VER LARG, P144
   Ng RT, 2002, IEEE T KNOWL DATA EN, V14, P1003, DOI 10.1109/TKDE.2002.1033770
   Park HS, 2009, EXPERT SYST APPL, V36, P3336, DOI 10.1016/j.eswa.2008.01.039
   Telgarsky M., 2010, J MACHINE LEARNING R, P820
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405027
DA 2019-06-15
ER

PT S
AU Neyshabur, B
   Bhojanapalli, S
   McAllester, D
   Srebro, N
AF Neyshabur, Behnam
   Bhojanapalli, Srinadh
   McAllester, David
   Srebro, Nathan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Exploring Generalization in Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CLASSIFICATION; NETWORKS
AB With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.
C1 [Neyshabur, Behnam; Bhojanapalli, Srinadh; McAllester, David; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
RP Neyshabur, B (reprint author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
EM bneyshabur@ttic.edu; srinadh@ttic.edu; mcallester@ttic.edu;
   nati@ttic.edu
CR Anthony  M., 2009, NEURAL NETWORK LEARN
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett P. L., 2017, PREPRINT
   Bartlett Peter, 2017, ARXIV170608498
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Bartlett PL, 1998, NEURAL COMPUT, V10, P2159, DOI 10.1162/089976698300017016
   Chaudhari Pratik, 2016, ARXIV161101838
   Dziugaite Gintare Karolina, 2017, ARXIV170311008
   Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316
   Hardt M., 2016, ICML
   Harvey Nick, 2017, ARXIV170302930
   Keskar N. S., 2016, ARXIV160904836
   Langford J., 2001, P 14 INT C NEUR INF, P809
   McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16
   McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   Neyshabur  B., 2015, P INT C LEARN REPR W
   Neyshabur B., 2016, ADV NEURAL INFORM PR
   Neyshabur  Behnam, 2015, ADV NEURAL INFORM PR
   Neyshabur  Behnam, 2015, P 28 C LEARN THEOR C
   Neyshabur  Behnam, 2016, INT C LEARN REPR
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X
   Sokolic J., 2016, ARXIV161004574
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Srebro N., 2005, ADV NEURAL INFORM PR, P1329
   von Luxburg U, 2004, J MACH LEARN RES, V5, P669
   Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1
   Zhang Chiyuan, 2017, INT C LEARN REPR
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406003
DA 2019-06-15
ER

PT S
AU Niculae, V
   Blondel, M
AF Niculae, Vlad
   Blondel, Mathieu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Regularized Framework for Sparse and Structured Neural Attention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHM; SMOOTHNESS; SHRINKAGE
AB Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.
C1 [Niculae, Vlad] Cornell Univ, Ithaca, NY 14850 USA.
   [Blondel, Mathieu] NTT Commun Sci Labs, Kyoto, Japan.
RP Niculae, V (reprint author), Cornell Univ, Ithaca, NY 14850 USA.
EM vlad@cs.cornell.edu; mathieu@mblondel.org
RI Jeong, Yongwook/N-7413-2016
CR Amos B., 2017, P ICML
   Bahdanau D., 2015, P ICLR
   BALL K, 1994, INVENT MATH, V115, P463, DOI 10.1007/BF01231769
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bengio Y., 2013, P NIPS
   Bengio Y., 2005, P NIPS
   Bondell HD, 2008, BIOMETRICS, V64, P115, DOI 10.1111/j.1541-0420.2007.00843.x
   Bowman S., 2015, P EMNLP
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chorowski J. K., 2015, P NIPS
   Clarke F. H., 1990, OPTIMIZATION NONSMOO
   Cohn T., 2016, P NAACL HLT
   Condat L, 2013, IEEE SIGNAL PROC LET, V20, P1054, DOI 10.1109/LSP.2013.2278339
   Dagan I., 2009, NAT LANG ENG, V15, pi, DOI DOI 10.1017/S1351324909990209
   Dmmuchi J., 2008, P ICML
   Dugas C., 2001, P NIPS
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Graves A., 2014, P NIPS
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Irsoy O., 2017, THESIS
   Jang Eric, 2017, P ICLR
   Kakade SM, 2012, J MACH LEARN RES, V13, P1865
   Kim Y., 2017, P ICLR
   Klein G., 2017, ARXIV E PRINTS
   Koehn P., 2007, P ACL
   Lei Tao, 2016, P EMNLP
   Li J., 2016, P NAACL HLT
   Liu B., 2015, P ICCVPR
   Luong Minh-Thang, 2015, P EMNLP
   Maddison Chris J., 2017, P ICLR
   Martins A. F., 2017, P EMNLP
   Martins Andre F. T., 2016, P ICML
   Meshi O., 2015, P NIPS
   MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Rocktaschel Tim, 2016, P ICLR
   Rush Alexander M., 2015, P EMNLP
   Scardapane S, 2017, NEUROCOMPUTING, V241, P81, DOI 10.1016/j.neucom.2017.02.029
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Wen W., 2016, P NIPS
   Xu Kelvin, 2015, P ICML
   Yu Y., 2013, P NIPS
   Zalinescu  C., 2002, CONVEX ANAL GEN VECT
   Zeng X., 2014, ORDERED WEIGHTED L1
   Zeng XR, 2014, DIGIT SIGNAL PROCESS, V31, P124, DOI 10.1016/j.dsp.2014.03.010
   Zhong LW, 2012, IEEE T NEUR NET LEAR, V23, P1436, DOI 10.1109/TNNLS.2012.2200262
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403040
DA 2019-06-15
ER

PT S
AU Nie, FP
   Wang, XQ
   Deng, C
   Huang, H
AF Nie, Feiping
   Wang, Xiaoqian
   Deng, Cheng
   Huang, Heng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning A Structured Optimal Bipartite Graph for Co-Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Co-clustering methods have been widely applied to document clustering and gene expression analysis. These methods make use of the duality between features and samples such that the co-occurring structure of sample and feature clusters can be extracted. In graph based co-clustering methods, a bipartite graph is constructed to depict the relation between features and samples. Most existing co-clustering methods conduct clustering on the graph achieved from the original data matrix, which doesn't have explicit cluster structure, thus they require a post-processing step to obtain the clustering results. In this paper, we propose a novel co-clustering method to learn a bipartite graph with exactly k connected components, where k is the number of clusters. The new bipartite graph learned in our model approximates the original graph but maintains an explicit cluster structure, from which we can immediately get the clustering results without post-processing. Extensive empirical results are presented to verify the effectiveness and robustness of our model.
C1 [Nie, Feiping] Northwestern Polytech Univ, Ctr OPTIMAL, Sch Comp Sci, Xian, Shaanxi, Peoples R China.
   [Wang, Xiaoqian; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
   [Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.
RP Nie, FP (reprint author), Northwestern Polytech Univ, Ctr OPTIMAL, Sch Comp Sci, Xian, Shaanxi, Peoples R China.
EM feipingnie@gmail.com; xqwang1991@gmail.com; chdeng@mail.xidian.edu;
   heng.huang@pitt.edu
RI Jeong, Yongwook/N-7413-2016
FU U.S. NSF [IIS 1302675, IIS 1344152, IIS 1619308, IIS 1633753]; NSF [DBI
   1356628]; NIH [AG049371]
FX This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS
   1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH
   AG049371.
CR Alavi Y, 1991, GRAPH THEORY COMBINA, V2, P871
   Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998
   Chung F, 1997, CBMS REGIONAL C SERI, V92
   Cui X., 2005, J COMPUTER SCI, V27, P33
   Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269
   Ding C., 2006, P 12 ACM SIGKDD INT, P126, DOI DOI 10.1145/1150402.1150420
   Fan K., 1949, THEOREM WEYL EIGEN V, V35, P652
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Liao Zhongyue, 2004, BMC Urol, V4, P8, DOI 10.1186/1471-2490-4-8
   Nie F., 2014, P 20 ACM SIGKDD INT, P977, DOI DOI 10.1145/2623330.2623726
   Nie FP, 2016, P 30 AAAI C ART INT, P1969
   Nutzmann HW, 2014, CURR OPIN BIOTECH, V26, P91, DOI 10.1016/j.copbio.2013.10.009
   Petricoin EF, 2002, J NATL CANCER I, V94, P1576
   Piano F, 2002, CURR BIOL, V12, P1959, DOI 10.1016/S0960-9822(02)01301-5
   Shahnaz F, 2006, INFORM PROCESS MANAG, V42, P373, DOI 10.1016/j.ipm.2004.11.005
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Zelnik-Manor L., 2004, NIPS, P5
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404020
DA 2019-06-15
ER

PT S
AU Nock, R
   Cranko, Z
   Menon, AK
   Qu, LZ
   Williamson, RC
AF Nock, Richard
   Cranko, Zac
   Menon, Aditya Krishna
   Qu, Lizhen
   Williamson, Robert C.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI f-GANs in an Information Geometric Nutshell
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DIVERGENCE; RISK
AB Nowozin et al showed last year how to extend the GAN principle to all f - divergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator's design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens - namely, deformed exponential families, a wide superset of exponential families -. We show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the f-GAN game. This result holds given a sufficient condition on activation functions - which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator.
C1 [Nock, Richard; Cranko, Zac; Menon, Aditya Krishna; Qu, Lizhen; Williamson, Robert C.] Data61, Eveleigh, NSW, Australia.
   [Nock, Richard; Cranko, Zac; Menon, Aditya Krishna; Qu, Lizhen; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia.
   [Nock, Richard] Univ Sydney, Sydney, NSW, Australia.
RP Nock, R (reprint author), Data61, Eveleigh, NSW, Australia.
EM richard.nock@data61.csiro.au; zac.cranko@data61.csiro.au;
   aditya.menon@data61.csiro.au; lizhen.qu@data61.csiro.au;
   bob.williamson@data61.csiro.au
RI Jeong, Yongwook/N-7413-2016
CR ALI SM, 1966, J ROY STAT SOC B, V28, P131
   Amari S., 2000, METHODS INFORM GEOME
   Amari S., 1985, DIFFERENTIAL GEOMETR
   Amari S, 2016, INFORM GEOMETRY ITS
   Amari S, 2012, PHYSICA A, V391, P4308, DOI 10.1016/j.physa.2012.04.016
   Arjovsky M., 2017, ARXIV170107875
   Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157
   BENTAL A, 1991, J MATH ANAL APPL, V157, P211, DOI 10.1016/0022-247X(91)90145-P
   Boissonnat JD, 2010, DISCRETE COMPUT GEOM, V44, P281, DOI 10.1007/s00454-010-9256-1
   Boyd S., 2004, CONVEX OPTIMIZATION
   Clevert D.-A., 2016, 4 ICLR
   Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299
   Frongillo R, 2014, AIP CONF PROC, V1636, P11, DOI 10.1063/1.4903703
   Genevay A., 2017, ABS170600292 CORR
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gulrajani I., 2017, ABS170400028 CORR
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2017, ABS170207028 CORR
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Liu S., 2017, ABS170508991 CORR
   Maas A.-L., 2013, 30 ICML
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Naudts J, 2011, GENERALISED THERMOSTATISTICS, P1, DOI 10.1007/978-0-85729-355-8
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nock R., 2008, NIPS 21, P1201
   Nock R., 2017, ABS170704385 CORR
   Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Radford A., 2016, 4 ICLR
   Reid M.-D., 2010, JMLR, V11
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Telgarsky M., 2012, 29 ICML
   Vigelis R., 2011, J THEOR PROBAB, V21, P1
   Wolf L., 2017, ABS170405693 CORR
   Yu F., 2015, ARXIV150603365
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400044
DA 2019-06-15
ER

PT S
AU Noh, H
   You, T
   Mun, J
   Han, B
AF Noh, Hyeonwoo
   You, Tackgeun
   Mun, Jonghwan
   Han, Bohyung
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Regularizing Deep Neural Networks by Noise: Its Interpretation and
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Overfitting is one of the most critical challenges in deep neural networks, and there are various types of regularization methods to improve generalization performance. Injecting noises to hidden units during training, e.g., dropout, is known as a successful regularizer, but it is still not clear enough why such training techniques work well in practice and how we can maximize their benefit in the presence of two conflicting objectives-optimizing to true data distribution and preventing overfitting by regularization. This paper addresses the above issues by 1) interpreting that the conventional training methods with regularization by noise injection optimize the lower bound of the true objective and 2) proposing a technique to achieve a tighter lower bound using multiple noise samples per training example in a stochastic gradient descent iteration. We demonstrate the effectiveness of our idea in several computer vision applications.
C1 [Noh, Hyeonwoo; You, Tackgeun; Mun, Jonghwan; Han, Bohyung] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
RP Noh, H (reprint author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.
EM shgusdngogo@postech.ac.kr; tackgeun.you@postech.ac.kr;
   choco1916@postech.ac.kr; bhhan@postech.ac.kr
RI Jeong, Yongwook/N-7413-2016
FU IITP grant - Korea government (MSIT) [2017-0-01778, 2017-0-01780]
FX This work was supported by the IITP grant funded by the Korea government
   (MSIT) [2017-0-01778, Development of Explainable Human-level Deep
   Machine Learning Inference Framework; 2017-0-01780, The Technology
   Development for Event Recognition/Relational Reasoning and Learning
   Knowledge based System for Video Understanding].
CR Andreas J., 2016, CVPR
   Antol S., 2015, ICCV
   Ba J., 2013, NIPS
   Ba J., 2015, NIPS
   Bornschein Jorg, 2015, ICLR
   Bulo S. R., 2016, ICML
   Feichtenhofer C, 2016, CVPR
   Gal  Yarin, 2016, ICML
   Han Bohyung, 2017, CVPR
   Han D., 2017, CVPR
   He K., 2016, ECCV
   He K., 2016, CVPR
   Huang G., 2016, ECCV
   Huang G, 2017, CVPR
   Jain P., 2015, ARXIV150302031
   Kingma D. P., 2015, NIPS
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, NIPS
   Larsson G., 2017, ICLR
   Li Z., 2016, NIPS
   Long  J., 2015, CVPR
   Ma X., 2016, ICLR
   Mnih A., 2016, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nam  H., 2016, CVPR
   Noh H., 2016, CVPR
   Noh H., 2015, ICCV
   Raiko T., 2015, ICLR
   Ren S., 2015, NIPS
   Salakhutdinov R., 2016, ICLR
   Simonyan Karen, 2015, ICLR
   Soomro K., 2012, ARXIV12120402
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vinyals O, 2015, CVPR
   Wager  S., 2013, NIPS
   Wan L., 2013, ICML
   Wu Y., 2016, ARXIV160908144
   Yang Z, 2016, CVPR
   Zagoruyko S., 2016, BMVC
NR 40
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405019
DA 2019-06-15
ER

PT S
AU Noh, YK
   Sugiyama, M
   Kim, KE
   Park, FC
   Lee, DD
AF Noh, Yung-Kyun
   Sugiyama, Masashi
   Kim, Kee-Eung
   Park, Frank C.
   Lee, Daniel D.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Generative Local Metric Learning for Kernel Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DENSITY-FUNCTION
AB DThis paper shows how metric learning can be used with Nadaraya-Watson (NW) kernel regression. Compared with standard approaches, such as bandwidth selection, we show how metric learning can significantly reduce the mean square error (MSE) in kernel regression, particularly for high-dimensional data. We propose a method for efficiently learning a good metric function based upon analyzing the performance of the NW estimator for Gaussian-distributed data. A key feature of our approach is that the NW estimator with a learned metric uses information from both the global and local structure of the training data. Theoretical and empirical results confirm that the learned metric can considerably reduce the bias and MSE for kernel regression even when the data are not confined to Gaussian.
C1 [Noh, Yung-Kyun; Park, Frank C.] Seoul Natl Univ, Seoul, South Korea.
   [Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan.
   [Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Lee, Daniel D.] Univ Penn, Philadelphia, PA 19104 USA.
RP Noh, YK (reprint author), Seoul Natl Univ, Seoul, South Korea.
EM nohyung@snu.ac.kr; sugi@k.u-tokyo.ac.jp; kekim@cs.kaist.ac.kr;
   fcp@snu.ac.kr; ddlee@seas.upenn.edu
FU BK21Plus in Korea; KAKENHI in Japan [17H01760]; IITP/MSIT in Korea
   [2017-0-01778]; BK21Plus in Korea [MITIP-10048320]; NSF in US; ONR in
   US; ARL in US; AFOSR in US; DOT in US; DARPA in US; 
   [NRF/MSIT-2017R1E1A1A03070945]
FX YKN acknowledges support from NRF/MSIT-2017R1E1A1A03070945, BK21Plus in
   Korea, MS from KAKENHI 17H01760 in Japan, KEK from IITP/MSIT
   2017-0-01778 in Korea, FCP from BK21Plus, MITIP-10048320 in Korea, and
   DDL from the NSF, ONR, ARL, AFOSR, DOT, DARPA in US.
CR Alcala-Fdez J, 2011, J MULT-VALUED LOG S, V17, P255
   Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014
   Bellet A., 2013, ABS13066709 CORR
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Choi E, 2000, ANN STAT, V28, P1339
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Goldberger J., 2005, ADV NEURAL INFORM PR, P513
   HALL P, 1991, BIOMETRIKA, V78, P263
   Hastie T., 2001, SPRINGER SERIES STAT
   Haykin S., 2008, NEURAL NETWORKS LEAR
   Huang RQ, 2013, J INTELL FUZZY SYST, V24, P775, DOI 10.3233/IFS-2012-0597
   KELLER PW, 2006, P 23 INT C MACH LEAR, P449
   Kpotufe S, 2016, J MACH LEARN RES, V17
   Lazaro-Gredilla M, 2010, IEEE T NEURAL NETWOR, V21, P1345, DOI 10.1109/TNN.2010.2049859
   Nadaraya E., 1964, THEOR PROBAB APPL, V9, P141, DOI DOI 10.1137/1109020
   Nguyen B, 2016, NEUROCOMPUTING, V214, P805, DOI 10.1016/j.neucom.2016.07.005
   Noh YK, 2018, IEEE T PATTERN ANAL, V40, P106, DOI 10.1109/TPAMI.2017.2666151
   Nosofsky RM, 1997, PSYCHOL REV, V104, P266, DOI 10.1037//0033-295X.104.2.266
   Park B. U., 1992, Computational Statistics, V7, P251
   PARK BU, 1990, J AM STAT ASSOC, V85, P66, DOI 10.2307/2289526
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632
   SCHUCANY WR, 1977, J AM STAT ASSOC, V72, P420, DOI 10.2307/2286810
   Shi L, 2010, PSYCHON B REV, V17, P443, DOI 10.3758/PBR.17.4.443
   Watson G., 1964, SANKHYA A, V26, P359, DOI DOI 10.2307/25049340
   Weinberger K., 2006, ADV NEURAL INFORM PR, V18, P1473, DOI DOI 10.1007/978-3-319-13168-9_
   Weinberger K., 2007, INT C ART INT STAT, P608
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402049
DA 2019-06-15
ER

PT S
AU Nonnenmacher, M
   Turaga, SC
   Macke, JH
AF Nonnenmacher, Marcel
   Turaga, Srinivas C.
   Macke, Jakob H.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Extracting low-dimensional dynamics from multiple large-scale neural
   population recordings by learning to predict correlations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.
C1 [Nonnenmacher, Marcel; Macke, Jakob H.] Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.
   [Turaga, Srinivas C.] HHMI Janelia Res Campus, Ashburn, VA USA.
   [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany.
RP Nonnenmacher, M (reprint author), Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.
EM marcel.nonnenmacher@caesar.de; turagas@janelia.hhmi.org;
   jakob.macke@caesar.de
RI Jeong, Yongwook/N-7413-2016
FU caesar foundation
FX We thank M. Ahrens for the larval zebrafish data. Our work was supported
   by the caesar foundation.
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   Aoki M., 1990, STATE SPACE MODELING
   Balzano L, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976
   Bishop WE, 2014, ADV NEURAL INFORM PR, V27, P2762
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Briggman KL, 2005, SCIENCE, V307, P896, DOI 10.1126/science.1103736
   Buesing L., 2014, ADV NEURAL INFORM PR, V27, P3500
   Buesing L, 2012, ADV NEURAL INFORM PR, V25, P1682
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dhawale AK, 2017, ELIFE, V6, DOI 10.7554/eLife.27702
   Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003
   Gao Y, 2015, ADV NEURAL INFORM PR, V2, P2044
   Ghahramani  Z., 1996, CRGTR962 U TOTR DEP
   He J., 2011, ARXIV11093827
   Huys QJM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000379
   Katayama T., 2006, SUBSPACE METHODS SYS
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Li N, 2016, NATURE, V532, P459, DOI 10.1038/nature17643
   Liu Z, 2013, SYST CONTROL LETT, V62, P605, DOI 10.1016/j.sysconle.2013.04.005
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Markovsky I., 2016, SYSTEMS CONTROL LETT
   Markovsky I., 2016, IEEE T AUTOMATIC CON
   Mazor O, 2005, NEURON, V48, P661, DOI 10.1016/j.neuron.2005.09.032
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Pnevmatikakis EA, 2014, J COMPUT GRAPH STAT, V23, P316, DOI 10.1080/10618600.2012.760461
   Shenoy KV, 2013, ANNU REV NEUROSCI, V36, P337, DOI 10.1146/annurev-neuro-062111-150509
   Sofroniew NJ, 2016, ELIFE, V5, DOI 10.7554/eLife.14472
   Soudry D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004464
   Turaga S., 2013, ADV NEURAL INFORM PR, V26, P539
   Van Overschee P, 2012, SUBSPACE IDENTIFICAT
   Yu B. M., 2009, ADV NEURAL INFORM PR, P1881, DOI [10.1152/jn.90941.2008, DOI 10.1152/JN.90941]
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405076
DA 2019-06-15
ER

PT S
AU Oates, CJ
   Niederer, S
   Lee, A
   Briol, FX
   Girolami, M
AF Oates, Chris J.
   Niederer, Steven
   Lee, Angela
   Briol, Francois-Xavier
   Girolami, Mark
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Probabilistic Models for Integration Error in the Assessment of
   Functional Cardiac Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper studies the numerical computation of integrals, representing estimates or predictions, over the output f (x) of a computational model with respect to a distribution p(dx) over uncertain inputs x to the model. For the functional cardiac models that motivate this work, neither f nor p possess a closed-form expression and evaluation of either requires approximate to 100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function f and the a priori unknown distribution p. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.
C1 [Oates, Chris J.] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.
   [Niederer, Steven; Lee, Angela] Kings Coll London, London, England.
   [Briol, Francois-Xavier] Univ Warwick, Coventry, W Midlands, England.
   [Girolami, Mark] Imperial Coll London, London, England.
   [Oates, Chris J.; Girolami, Mark] Alan Turing Inst, London, England.
RP Oates, CJ (reprint author), Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.; Oates, CJ (reprint author), Alan Turing Inst, London, England.
RI Jeong, Yongwook/N-7413-2016
FU Lloyds Register Foundation Programme on Data-Centric Engineering; EPSRC
   Intermediate Career Fellowship; EPSRC [EP/K034154/1, EP/R018413/1,
   EP/P020720/1, EP/L014165/1]; EPSRC Established Career Fellowship
   [EP/J016934/1]; National Science Foundation (NSF) [DMS-1127914]
FX CJO and MG were supported by the Lloyds Register Foundation Programme on
   Data-Centric Engineering. SN was supported by an EPSRC Intermediate
   Career Fellowship. FXB was supported by the EPSRC grant [EP/L016710/1].
   MG was supported by the EPSRC grants [EP/K034154/1, EP/R018413/1,
   EP/P020720/1, EP/L014165/1], and an EPSRC Established Career Fellowship,
   [EP/J016934/1]. This material was based upon work partially supported by
   the National Science Foundation (NSF) under Grant DMS-1127914 to the
   Statistical and Applied Mathematical Sciences Institute. Opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the author(s) and do not necessarily reflect the views of
   the NSF.
CR BACH F, 2017, J MACH LEARN RES, V18
   BRIOL F. -X., 2015, ADV NEURAL INFORM PR, P1162
   Briol F.-X., 2015, ARXIV151200933
   Briol F-X, 2017, P MACHINE LEARNING R, P586
   Cockayne J., 2017, ARXIV170203673
   Cohen SN, 2016, ARXIV160906545
   Craig PS, 2001, J AM STAT ASSOC, V96, P717, DOI 10.1198/016214501753168370
   Delyon B, 2016, BERNOULLI, V22, P2177, DOI 10.3150/15-BEJ725
   DIACONIS P, 1986, ANN STAT, V14, P1, DOI 10.1214/aos/1176349830
   Diaconis P., 1988, STATISTICAL DECISION, V1, P163
   Dick J, 2013, ACTA NUMER, V22, P133, DOI 10.1017/S0962492913000044
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Ghosal S, 2001, ANN STAT, V29, P1233
   Gunter T, 2014, ADV NEURAL INFORM PR, V27, P2789
   Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142
   HUSZAR F., 2012, P 28 C ANN C UNC ART, P377
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   Ishwaran H, 2002, CAN J STAT, V30, P269, DOI 10.2307/3315951
   Kadane J. B., 1985, BAYESIAN STAT, P361
   Kanagawa M, 2016, ADV NEURAL INFORM PR, V30
   Karvonen T., 2017, ARXIV170306359
   Kennedy MC, 2001, J ROY STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294
   Lee AWC, 2017, J CARDIOVASC ELECTR, V28, P208, DOI 10.1111/jce.13134
   Mirams GR, 2016, J PHYSIOL-LONDON, V594, P6833, DOI 10.1113/JP271671
   Novak E, 2010, EMS TRACTS MATH, V12, P1
   OHAGAN A, 1987, J ROY STAT SOC D-STA, V36, P247
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   Osborne M, 2012, P 15 INT C ART INT S, P832
   Osborne MA, 2012, ADV NEURAL INFORM PR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rizvi H., 1983, RECENT ADV STAT, P287
   Robert C, 2013, MONTE CARLO STAT MET
   Sarkka S., 2016, J ADV INFORM FUSION, V11, P31
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Von Mises R, 1974, MATH THEORY PROBABIL
   Wand M. P., 1994, KERNEL SMOOTHING
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400011
DA 2019-06-15
ER

PT S
AU Oh, J
   Singh, S
   Lee, H
AF Oh, Junhyuk
   Singh, Satinder
   Lee, Honglak
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Value Prediction Network
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.
C1 [Oh, Junhyuk; Singh, Satinder; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Lee, Honglak] Google Brain, Mountain View, CA USA.
RP Oh, J (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM junhyuk@umich.edu; baveja@umich.edu; honglak@umich.edu
FU NSF [IIS-1526059]
FX This work was supported by NSF grant IIS-1526059. Any opinions,
   findings, conclusions, or recommendations expressed here are those of
   the authors and do not necessarily reflect the views of the sponsor.
CR Abadi M., 2016, ARXIV160304467
   Bellemare M. G., 2012, ARXIV12074708
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Chiappa S., 2017, ICLR
   Finn  C., 2017, ICRA
   Finn Chelsea, 2016, NIPS
   Gu S., 2016, ICML
   Guo X., 2016, IJCAI
   Hausknecht M.J., 2015, ARXIV150706527
   He K., 2016, CVPR
   Heess Nicolas, 2015, NIPS
   Jaderberg  Max, 2017, ICLR
   Kalchbrenner N., 2016, ARXIV161000527
   Kingma D. P., 2015, ICLR
   Kocsis L., 2006, ECML
   Kulkarni T., 2016, ARXIV160602396
   Lakshminarayanan A. S., 2017, AAAI
   Lenz I., 2015, RSS
   Mishra N., 2017, ICML
   Mnih V., 2016, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Oh  J., 2015, NIPS
   Oh J., 2016, ICML
   Parisotto E., 2017, ARXIV170208360
   Precup Doina, 2000, THESIS
   Raiko T, 2009, NEUROCOMPUTING, V72, P3704, DOI 10.1016/j.neucom.2009.06.009
   Silver D., 2017, ICML
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Silver D, 2012, MACH LEARN, V87, P183, DOI 10.1007/s10994-012-5280-0
   Stadie BC, 2015, ARXIV150700814
   Stolle M., 2002, SARA
   Sutton R. S., 2008, UAI
   Sutton R. S., 1990, ICML
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Tamar A., 2016, NIPS
   Vezhnevets A., 2016, NIPS
   Wang Z., 2016, ICML
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Yao Hengshuai, 2009, NIPS
NR 39
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406019
DA 2019-06-15
ER

PT S
AU Ohama, I
   Sato, I
   Kida, T
   Arimura, H
AF Ohama, Iku
   Sato, Issei
   Kida, Takuya
   Arimura, Hiroki
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On the Model Shrinkage Effect of Gamma Process Edge Partition Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process (Gamma P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal Gamma P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM. incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the Gamma P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM. (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.
C1 [Ohama, Iku] Panasonic Corp, Kadoma, Osaka, Japan.
   [Sato, Issei] Univ Tokyo, Tokyo, Japan.
   [Ohama, Iku; Kida, Takuya; Arimura, Hiroki] Hokkaido Univ, Sapporo, Hokkaido, Japan.
RP Ohama, I (reprint author), Panasonic Corp, Kadoma, Osaka, Japan.; Ohama, I (reprint author), Hokkaido Univ, Sapporo, Hokkaido, Japan.
EM ohama.iku@jp.panasonic.com; sato@k.u-tokyo.ac.jp;
   kida@ist.hokudai.ac.jp; arim@ist.hokudai.ac.jp
RI Jeong, Yongwook/N-7413-2016
CR Airoldi EM, 2008, J MACH LEARN RES, V9, P1981
   BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Geyer Charles J., 2007, LOWER TRUNCATED POIS
   Griffiths T., 2005, NEURAL INFORM PROCES, P475
   Griffiths TL, 2011, J MACH LEARN RES, V12, P1185
   Hu Changwei, 2015, P UAI, P375
   KEMP C., 2006, P NAT C ART INT, V21, P381, DOI DOI 10.1145/1837026.1837061
   Klimt B, 2004, LECT NOTES COMPUT SC, V3201, P217
   LIU JS, 1994, J AM STAT ASSOC, V89, P958, DOI 10.2307/2290921
   Morup M., 2011, P IEEE INT WORKSH MA, P1
   Newman D, 2009, J MACH LEARN RES, V10, P1801
   Palla Konstantina, 2012, P 29 INT C MACH LEAR, P1607
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Zhou  M., 2015, NIPS, P3043
   Zhou M, 2015, P MACH LEARN RES, P1135
   Zhou M., 2014, P NIPS, P3455
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400038
DA 2019-06-15
ER

PT S
AU Orabona, F
   Tommasi, T
AF Orabona, Francesco
   Tommasi, Tatiana
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Training Deep Networks without Learning Rates Through Coin Betting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.
C1 [Orabona, Francesco] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Tommasi, Tatiana] Sapienza Rome Univ, Dept Comp Control & Management Engn, Rome, Italy.
RP Orabona, F (reprint author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM francesco@orabona.com; tommasi@dis.uniroma1.it
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1531492]; ERC [637076 - RoboExNovo]; Google
   Research Award
FX The authors thank the Stony Brook Research Computing and
   Cyberinfrastructure, and the Institute for Advanced Computational
   Science at Stony Brook University for access to the high-performance
   SeaWulf computing system, which was made possible by a $1.4M National
   Science Foundation grant (#1531492). The authors also thank Akshay Verma
   for the help with the TensorFlow implementation and Matej Kristan for
   reporting a bug in the pseudocode in the previous version of the paper.
   T.T. was supported by the ERC grant 637076 - RoboExNovo. F.O. is partly
   supported by a Google Research Award.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Cutkosky A., 2016, ADV NEURAL INFORM PR, V29, P748
   Cutkosky Ashok, 2017, P 30 C LEARN THEOR C, V65, P643
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hardt Moritz, 2016, ARXIV160905191
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1991, THESIS
   Jia Y., 2014, ARXIV14085093
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Krizhevsky A., 2009, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan H. B., 2014, P 27 C LEARN THEOR C, P1020
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Orabona F., 2016, ADV NEURAL INFORM PR, V29, P577
   Orabona F, 2015, LECT NOTES ARTIF INT, V9355, P287, DOI 10.1007/978-3-319-24486-0_19
   Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Orabona Francesco, 2013, ADV NEURAL INFORM PR, P1806
   Ross S., 2013, P 29 C UNC ART INT U
   Schaul T, 2013, P 30 INT C MACH LEAR, P343
   Streeter M., 2010, ARXIV10024862
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Wright S, 1999, NUMERICAL OPTIMIZATI
   Zaremba W, 2014, ARXIV14092329
   Zeiler M.D., 2012, ARXIV12125701
   Zhang T., 2004, P 21 INT C MACH LEAR, P919, DOI DOI 10.1145/1015330.1015332
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402021
DA 2019-06-15
ER

PT S
AU Osokin, A
   Bach, F
   Lacoste-Julien, S
AF Osokin, Anton
   Bach, Francis
   Lacoste-Julien, Simon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Structured Prediction Theory with Calibrated Convex Surrogate Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STATISTICAL-ANALYSIS; CLASSIFICATION; MULTICLASS; CONSISTENCY; RANKING
AB We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.
C1 [Osokin, Anton; Bach, Francis] PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.
   [Osokin, Anton] Natl Res Univ, HSE, Moscow, Russia.
   [Lacoste-Julien, Simon] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada.
RP Osokin, A (reprint author), PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.
RI Osokin, Anton/D-7398-2012; Jeong, Yongwook/N-7413-2016
OI Osokin, Anton/0000-0002-8807-5132; 
FU ERC grant Activia [307574]; NSERC [RGPIN-2017-06936]; MSR-INRIA Joint
   Center
FX We would like to thank Pascal Germain for useful discussions. This work
   was partly supported by the ERC grant Activia (no. 307574), the NSERC
   Discovery Grant RGPIN-2017-06936 and the MSR-INRIA Joint Center.
CR Agarwal S, 2014, J MACH LEARN RES, V15, P1653
   Avila Pires Bernardo, 2016, ARXIV160906385V1
   Avila Pires Bernardo, 2013, ICML
   Bakir G., 2007, PREDICTING STRUCTURE
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Bousquet Olivier, 2008, NIPS
   Brouard C, 2016, J MACH LEARN RES, V17
   Calauzenes Clement, 2011, ICML
   Calauzenes Clement, 2012, NIPS
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Collins M., 2002, EMNLP
   Cortes Corinna, 2016, NIPS
   Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939
   Crammer K., 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628
   Do Chuong B., 2009, NIPS
   Dogan U, 2016, J MACH LEARN RES, V17
   Duchi John C., 2010, ICML
   Durbin R., 1998, BIOL SEQUENCE ANAL P
   Gao Wei, 2011, COLT
   Gimpel K., 2010, NAACL
   Hazan T., 2010, NIPS
   Keshet Joseph, 2014, ADV STRUCTURED PREDI
   Kotlowski W., 2011, ICML
   Lafferty J. O., 2001, ICML
   Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098
   Lin Y, 2004, STAT PROBABIL LETT, V68, P73, DOI 10.1016/j.spl.2004.03.002
   London B, 2016, J MACH LEARN RES, V17
   Long Phil, 2013, ICML
   McAllester  D., 2007, PREDICTING STRUCTURE
   McAllester D. A., 2011, NIPS
   Narasimhan H., 2015, ICML
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Nowozin Sebastian, 2014, ADV STRUCTURED PREDI
   Orabona Francesco, 2014, NIPS
   Pedregosa F, 2017, J MACH LEARN RES, V18, P1
   Pletscher Patrick, 2010, ECML PKDD
   Ramaswamy HG, 2016, J MACH LEARN RES, V17
   Ramaswamy Harish G., 2013, NIPS
   Rosasco Lorenzo, 2016, NIPS
   Shi QF, 2015, IEEE T PATTERN ANAL, V37, P2, DOI 10.1109/TPAMI.2014.2306414
   Smith Noah A., 2011, SYNTHESIS LECT HUMAN, V4, P1
   Steinwart I, 2007, CONSTR APPROX, V26, P225, DOI 10.1007/s00365-006-0662-3
   Taskar B, 2003, NIPS
   Taskar B., 2005, ICML
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Williamson RC, 2016, J MACH LEARN RES, V17, P1
   Zhang T, 2004, ANN STAT, V32, P56
   Zhang T, 2004, J MACH LEARN RES, V5, P1225
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400029
DA 2019-06-15
ER

PT S
AU Ouyang, Y
   Gagrani, M
   Nayyar, A
   Jain, R
AF Ouyang, Yi
   Gagrani, Mukul
   Nayyar, Ashutosh
   Jain, Rahul
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Unknown Markov Decision Processes: A Thompson Sampling Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish (O) over tilde (HS root AT) bounds on expected regret under a Bayesian setting, where S and A are the sizes of the state and action spaces, T is time, and H is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.
C1 [Ouyang, Yi] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Gagrani, Mukul; Nayyar, Ashutosh; Jain, Rahul] Univ Southern Calif, Los Angeles, CA USA.
RP Ouyang, Y (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM ouyangyi@berkeley.edu; mgagrani@usc.edu; ashutosn@usc.edu;
   rahul.jain@usc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [1611574, 1446901]
FX Yi Ouyang would like to thank Yang Liu from Harvard University for
   helpful discussions. Rahul Jain and Ashutosh Nayyar were supported by
   NSF Grants 1611574 and 1446901.
CR Abbasi-Yadkori Y., 2015, UAI
   Bartlett P. L., 2009, UAI
   Bertsekas D. P., 2012, DYNAMIC PROGRAMMING, V2
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222
   Chapelle Olivier, 2011, NIPS
   Dann C., 2015, NIPS
   Filippi S, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P115, DOI 10.1109/ALLERTON.2010.5706896
   Fonteneau R., 2013, BAYESOPT2013
   Gopalan A., 2015, COLT
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Osband I., 2016, EWRL
   Osband I., 2013, NIPS
   Osband Ian, 2016, ARXIV160802731
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Strens M., 2000, ICML
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Varaiya P, 2015, STOCHASTIC SYSTEMS E
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401036
DA 2019-06-15
ER

PT S
AU Pal, DK
   Kannan, AA
   Arakalgud, G
   Savvides, M
AF Pal, Dipan K.
   Kannan, Ashwin A.
   Arakalgud, Gautam
   Savvides, Marios
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Max-Margin Invariant Features from Transformed Unlabeled Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to a unitary group while having theoretical guarantees in addressing the important practical issue of unavailability of transformed versions of labelled data. A problem we call the Unlabeled Transformation Problem which is a special form of semi-supervised learning and one-shot learning. We present a theoretically motivated alternate approach to the invariant kernel SVM based on which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As an illustration, we design an framework for face recognition and demonstrate the efficacy of our approach on a large scale semi-synthetic dataset with 153,000 images and a new challenging protocol on Labelled Faces in the Wild (LFW) while out-performing strong baselines.
C1 [Pal, Dipan K.; Kannan, Ashwin A.; Arakalgud, Gautam; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
RP Pal, DK (reprint author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
EM dipanp@cmu.edu; aalapakk@cmu.edu; garakalgud@cmu.edu; marioss@cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Anselmi F., 2013, CORR
   Anselmi F., 2013, MAGIC MAT THEORY DEE
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Haasdonk B, 2002, INT C PATT RECOG, P864, DOI 10.1109/ICPR.2002.1048439
   Haasdonk B, 2007, MACH LEARN, V68, P35, DOI 10.1007/s10994-007-5009-7
   HINTON GE, 1987, LECT NOTES COMPUT SC, V258, P1
   Leibo J. Z., 2014, INT JOINT C COMP VIS
   Liao Q., 2013, ADV NEURAL INFORM PR
   Niyogi P, 1998, P IEEE, V86, P2196, DOI 10.1109/5.726787
   Pal DK, 2016, PROC CVPR IEEE, P5590, DOI 10.1109/CVPR.2016.603
   Park SW, 2010, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2010.5539980
   Parkhi O. M., 2015, DEEP FACE RECOGNITIO
   Poggio  T., 1992, RECOGNITION STRUCTUR
   Raj A., 2017, P 20 INT C ART INT S, V54, P1225
   Reisert M., 2008, THESIS
   Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21
   Scholkopf B., 1998, ADV NEURAL INFORM PR
   Scholkopf B., 1996, INT C ART NEUR NETW, P47
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Walder C., 2007, ADV NEURAL INFORM PR, P1561
   Zhang X., 2013, ADV NEURAL INFORM PR, P2031
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401046
DA 2019-06-15
ER

PT S
AU Palaiopanos, G
   Panageas, I
   Piliouras, G
AF Palaiopanos, Gerasimos
   Panageas, Ioannis
   Piliouras, Georgios
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multiplicative Weights Update with Constant Step-Size in Congestion
   Games: Convergence, Limit Cycles and Chaos
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to action gamma is multiplied by (1 - epsilon C(gamma)) > 0 where C(gamma) is the "cost" of action gamma and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use arbitrary admissible constants as learning rates epsilon and prove convergence to exact Nash equilibria. Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to action gamma is multiplied by (1 - epsilon)(C(gamma)) even for the simplest case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior.
C1 [Palaiopanos, Gerasimos; Piliouras, Georgios] SUTD, Singapore, Singapore.
   [Panageas, Ioannis] MIT, Cambridge, MA 02139 USA.
   [Panageas, Ioannis] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Panageas, Ioannis; Piliouras, Georgios] Simons Inst Theory Comp, Berkeley, CA USA.
RP Palaiopanos, G (reprint author), SUTD, Singapore, Singapore.
EM gerasimosath@yahoo.com; ioannis@csail.mit.edu; georgios@sutd.edu.sg
RI Jeong, Yongwook/N-7413-2016
FU SUTD Presidential fellowship; MIT-SUTD postdoctoral fellowship; SUTD
   grant [SRG ESD 2015 097]; MOE AcRF Tier 2 Grant [2016-T2-1-170]; NRF
   Fellowship
FX Gerasimos Palaiopanos would like to acknowledge a SUTD Presidential
   fellowship.; Ioannis Panageas would like to acknowledge a MIT-SUTD
   postdoctoral fellowship. Part of this work was completed while Ioannis
   Panageas was a PhD student at Georgia Institute of Technology and a
   visiting scientist at the Simons Institute for the Theory of Computing.;
   Georgios Piliouras would like to acknowledge SUTD grant SRG ESD 2015
   097, MOE AcRF Tier 2 Grant 2016-T2-1-170 and a NRF Fellowship. Part of
   this work was completed while Georgios Piliouras was a visiting
   scientist at the Simons Institute for the Theory of Computing.
CR Ackermann H, 2009, PODC'09: PROCEEDINGS OF THE 2009 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P63, DOI 10.1145/1582716.1582732
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Avramopoulos I., 2016, CORR
   Balcan M. - F., 2012, ICML WORKSH MARK MEC
   BAUM LE, 1967, B AM MATH SOC, V73, P360, DOI 10.1090/S0002-9904-1967-11751-8
   Berenbrink P, 2007, SIAM J COMPUT, V37, P1163, DOI 10.1137/060660345
   Berenbrink P, 2014, ACM T ALGORITHMS, V11, DOI 10.1145/2629671
   Bilmes J. A., 1998, INT COMPUTER SCI I, V4, P126
   Blum A, 2008, ACM S THEORY COMPUT, P373
   Caragiannis I., 2011, FOCS
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Chen PA, 2016, ARTIF INTELL, V241, P217, DOI 10.1016/j.artint.2016.09.002
   Chien S, 2011, GAME ECON BEHAV, V71, P315, DOI 10.1016/j.geb.2009.05.004
   Cohen J., 2017, P 31 INT C NEUR INF
   Daskalakis C., 2017, ARXIV E PRINTS
   Daskalakis C., 2006, COMPLEXITY COMPUTING, P71
   Daskalakis C, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P790
   Daskalakis C, 2010, LECT NOTES COMPUT SC, V6386, P114, DOI 10.1007/978-3-642-16170-4_11
   Engelberg R., 2013, P 14 ACM C EL COMM, P379
   Fabrikant A., 2004, P 36 ANN ACM S THEOR, P604
   Fearnley J., 2017, ARXIV E PRINTS
   Foster Dylan J., 2016, ADV NEURAL INFORM PR, P4727
   Fotakis D, 2008, LECT NOTES COMPUT SC, V4997, P121, DOI 10.1007/978-3-540-79309-0_12
   Fudenberg D, 1998, THEORY LEARNING GAME
   Jaggard A. D, 2011, ICS
   Jaggard AD, 2017, ACM T ECON COMPUT, V5, DOI 10.1145/3107182
   Kleinberg R., 2011, S INN COMP SCI ICS
   Kleinberg R., 2009, ACM S THEOR COMP STO
   Kleinberg R, 2011, DISTRIB COMPUT, V24, P21, DOI 10.1007/s00446-011-0129-5
   LI TY, 1975, AM MATH MON, V82, P985, DOI 10.2307/2318254
   Mertikopoulos P, 2010, ANN APPL PROBAB, V20, P1359, DOI 10.1214/09-AAP651
   Monderer D, 1996, J ECON THEORY, V68, P258, DOI 10.1006/jeth.1996.0014
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Nisan N, 2008, LECT NOTES COMPUT SC, V5385, P531, DOI 10.1007/978-3-540-92185-1_59
   Piliouras G., 2014, SODA
   Rosenthal R. W., 1973, International Journal of Game Theory, V2, P65, DOI 10.1007/BF01737559
   Roughgarden T, 2009, ACM S THEORY COMPUT, P513
   Sarkovs'kii O. M., 1964, UKR MAT ZH, V16, P61
   Strogatz S.H., 2000, NONLINEAR DYNAMICS C
   Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989
   Welch L. R., 2003, IEEE INFORM THEORY S, V53, P10
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405092
DA 2019-06-15
ER

PT S
AU Pan, JW
   Zhang, BQ
   Rao, V
AF Pan, Jiangwei
   Zhang, Boqian
   Rao, Vinayak
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Collapsed variational Bayes for Markov jump processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SIMULATION; INFERENCE; CHAINS
AB Markov jump processes are continuous-time stochastic processes widely used in statistical applications in the natural sciences, and more recently in machine learning. Inference for these models typically proceeds via Markov chain Monte Carlo, and can suffer from various computational challenges. In this work, we propose a novel collapsed variational inference algorithm to address this issue. Our work leverages ideas from discrete-time Markov chains, and exploits a connection between these two through an idea called uniformization. Our algorithm proceeds by marginalizing out the parameters of the Markov jump process, and then approximating the distribution over the trajectory with a factored distribution over segments of a piecewise-constant function. Unlike MCMC schemes that marginalize out transition times of a piecewise-constant process, our scheme optimizes the discretization of time, resulting in significant computational savings. We apply our ideas to synthetic data as well as a dataset of check-in recordings, where we demonstrate superior performance over state-of-the-art MCMC methods.
C1 [Pan, Jiangwei] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.
   [Zhang, Boqian; Rao, Vinayak] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.
   [Pan, Jiangwei] Facebook, Menlo Pk, CA 94025 USA.
RP Pan, JW (reprint author), Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.; Pan, JW (reprint author), Facebook, Menlo Pk, CA 94025 USA.
EM panjiangwei@gmail.com; zhan1977@purdue.edu; varao@purdue.edu
RI Jeong, Yongwook/N-7413-2016
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Bladt M, 2005, J R STAT SOC B, V67, P395, DOI 10.1111/j.1467-9868.2005.00508.x
   Boys RJ, 2008, STAT COMPUT, V18, P125, DOI 10.1007/s11222-007-9043-x
   Cinlar E., 1975, INTRO STOCHASTIC PRO
   Fearnhead P, 2006, J ROY STAT SOC B, V68, P767, DOI 10.1111/j.1467-9868.2006.00566.x
   Gao H., 2012, P 21 ACM C INF KNOWL
   GILLESPIE DT, 1977, J PHYS CHEM-US, V81, P2340, DOI 10.1021/j100540a008
   Hajiaghayi M., 2014, INT C MACH LEARN 201, V31, P638
   Hobolth A, 2009, ANN APPL STAT, V3, P1204, DOI 10.1214/09-AOAS247
   Huggins J. H., 2015, P 32 INT C MACH LEAR, P693
   Hughes M.C., 2015, ADV NEURAL INFORM PR, P1198
   Jensen A, 1953, SKAND AKTUARIETIDSKR, V36, P87, DOI 10.1080/03461238.1953.10419459
   Metzner P, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066702
   Opper M., 2007, NIPS 20
   Pan J., 2016, P 33 INT C MACH LEAR, P2244
   Rao V., 2014, J MACHINE LEARNING R, V13
   Rao V. A., 2012, P ADV NEUR INF PROC, V25, P710
   Saeedi A., 2011, NIPS 24
   Wang P., 2013, AISTATS
   Xu J, 2010, J ARTIF INTELL RES, V39, P745, DOI 10.1613/jair.3050
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403079
DA 2019-06-15
ER

PT S
AU Panahi, A
   Hassibi, B
AF Panahi, Ashkan
   Hassibi, Babak
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Universal Analysis of Large-Scale Regularized Least Squares Solutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY; LASSO
AB A problem that has been of recent interest in statistical inference, machine learning and signal processing is that of understanding the asymptotic behavior of regularized least squares solutions under random measurement matrices (or dictionaries). The Least Absolute Shrinkage and Selection Operator (LASSO or least-squares with l(1) regularization) is perhaps one of the most interesting examples. Precise expressions for the asymptotic performance of LASSO have been obtained for a number of different cases, in particular when the elements of the dictionary matrix are sampled independently from a Gaussian distribution. It has also been empirically observed that the resulting expressions remain valid when the entries of the dictionary matrix are independently sampled from certain non-Gaussian distributions. In this paper, we confirm these observations theoretically when the distribution is sub-Gaussian. We further generalize the previous expressions for a broader family of regularization functions and under milder conditions on the underlying random, possibly non-Gaussian, dictionary matrix. In particular, we establish the universality of the asymptotic statistics (e.g., the average quadratic risk) of LASSO with non-Gaussian dictionaries.
C1 [Panahi, Ashkan] North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA.
   [Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.
RP Panahi, A (reprint author), North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA.
EM apanahi@ncsu.edu; hassibi@caltech.edu
RI Jeong, Yongwook/N-7413-2016
CR Bai ZD, 1999, STAT SINICA, V9, P611
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.90.9718
   Barbier J, 2016, ADV NEURAL INFORM PR, P424
   Barbier J, 2016, ANN ALLERTON CONF, P625, DOI 10.1109/ALLERTON.2016.7852290
   Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P7235, DOI 10.1109/TIT.2011.2161794
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Gordon Y., 1988, MILMANS INEQUALITY R
   Kabashima Y, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/09/L09003
   Karoui N. E, 2013, ARXIV13112445
   Lelarge M., 2016, ARXIV161103888
   Lindeberg JW, 1922, MATH Z, V15, P211, DOI 10.1007/BF01494395
   Liu Ji, 2013, P 30 INT C MACH LEAR, P91
   Montanari A., 2010, TECH REP
   Oymak S, 2013, ANN ALLERTON CONF, P1002, DOI 10.1109/Allerton.2013.6736635
   Oymak  Samet, 2015, ARXIV151109433
   Silverstein J. W., 2010, SPECTRAL ANAL LARGE, V20
   Stojnic  Mihailo, 2013, ARXIV13037291
   Thrampoulidis C., 2015, ADV NEURAL INFORM PR
   Thrampoulidis C., 2016, ARXIV160106233
   Thrampoulidis C., 2015, ARXIV150204977
   Thrampoulidis C, 2015, IEEE INT SYMP INFO, P2021
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R. J., 2011, SOLUTION PATH GEN LA
   Xin B, 2014, P AAAI, P2163
   Zerbib N., 2016, TECH REP
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403044
DA 2019-06-15
ER

PT S
AU Pang, HT
   Vanderbei, R
   Liu, H
   Zhao, T
AF Pang, Haotian
   Vanderbei, Robert
   Liu, Han
   Zhao, Tuo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Parametric Simplex Method for Sparse Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VARIABLE SELECTION; REGRESSION
AB High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a regularization factor, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.
C1 [Pang, Haotian; Vanderbei, Robert; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA.
   [Liu, Han] Tencent AI Lab, Bellevue, WA USA.
   [Pang, Haotian; Vanderbei, Robert; Liu, Han] Northwestern Univ, Evanston, IL 60208 USA.
   [Zhao, Tuo] Georgia Tech, Atlanta, GA 30332 USA.
RP Zhao, T (reprint author), Georgia Tech, Atlanta, GA 30332 USA.
EM tuo.zhao@isye.gatech.edu
CR Bandyopadhyay S, 2010, SCIENCE, V330, P1385, DOI 10.1126/science.1195618
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cai T, 2011, J AM STAT ASSOC, V106, P1566, DOI 10.1198/jasa.2011.tm11199
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523
   DANAHER P., 2013, J ROYAL STAT SOC B, V7, P373
   DANTZIG G., 1951, LINEAR PROGRAMMING E
   DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966
   Gai YJ, 2013, STAT SINICA, V23, P615, DOI 10.5705/ss.2012.061
   Hudson NJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000382
   Ideker T, 2012, MOL SYST BIOL, V8, DOI 10.1038/msb.2011.99
   Li XG, 2015, J MACH LEARN RES, V16, P553
   Murty K, 1983, LINEAR PROGRAMMING
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   VANDERBEI R., 1995, LINEAR PROGRAMMING F
   Wang HS, 2007, J BUS ECON STAT, V25, P347, DOI 10.1198/073500106000000251
   Yao YG, 2014, STAT COMPUT, V24, P885, DOI 10.1007/s11222-013-9408-2
   ZHAO S. D., 2013, BIOMETRIKA, V58, P253
   ZHU J., 2004, ADV NEURAL INFORM PR, V16
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400018
DA 2019-06-15
ER

PT S
AU Papamakarios, G
   Pavlakou, T
   Murray, I
AF Papamakarios, George
   Pavlakou, Theo
   Murray, Iain
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Masked Autoregressive Flow for Density Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.
C1 [Papamakarios, George; Pavlakou, Theo; Murray, Iain] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
RP Papamakarios, G (reprint author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.
EM g.papamakarios@ed.ac.uk; theo.pavlakou@ed.ac.uk; i.murray@ed.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Centre for Doctoral Training in Data Science - EPSRC [EP/L016427/1];
   University of Edinburgh; Microsoft Research
FX We thank Maria Gorinova for useful comments. George Papamakarios and
   Theo Pavlakou were supported by the Centre for Doctoral Training in Data
   Science, funded by EPSRC (grant EP/L016427/1) and the University of
   Edinburgh. George Papamakarios was also supported by Microsoft Research
   through its PhD Scholarship Programme.
CR Al-Rfou, 2016, ARXIV160502688 THEAN
   Balle J., 2016, P 4 INT C LEARN REPR
   Chen SSB, 2001, ADV NEUR IN, V13, P423
   Chu Z, 2017, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT 2017)
   Dinh L., 2014, ARXIV14108516
   Dinh L., 2017, P 5 INT C LEARN REPR
   Fan Y, 2013, STAT, V2, P34, DOI 10.1002/sta4.15
   Germain M., 2015, ICML, V37, P881
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gu Shixiang, 2015, ADV NEURAL INFORM PR, P2629
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kingma D., 2015, P 3 INT C LEARN REPR
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Kingma Diederik P, 2016, ADV NEURAL INFORM PR, P4743
   Krizhevsky A., 2009, TECHNICAL REPORT
   Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068
   Loaiza-Ganem G., 2017, P 5 INT C LEARN REPR
   Paige B., 2016, P 33 INT C MACH LEAR
   Papamakarios G., 2015, PROB INT WORKSH NEUR, P28
   Papamakarios G., 2016, ADV NEURAL INFORM PR, V29
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rippel O., 2013, ARXIV13025125
   Salimans Tim, 2017, ARXIV170105517
   Tang Y., 2012, P 29 INT C MACH LEAR, P505
   Theis L., 2015, ADV NEURAL INFORM PR, V2, P1927
   Theis L., 2016, P 4 INT C LEARN REPR
   Uria B, 2016, J MACH LEARN RES, V17
   Uria B, 2015, INT CONF ACOUST SPEE, P4465, DOI 10.1109/ICASSP.2015.7178815
   Uria Benigno, 2014, P 31 INT C MACH LEAR, V32, P467
   Uria Benigno, 2013, ADV NEURAL INFORM PR, P2175
   van den Oord A., 2016, P 33 INT C MACH LEAR, P1747
   Van Den Oord A., 2016, ARXIV160903499
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402038
DA 2019-06-15
ER

PT S
AU Papini, M
   Pirotta, M
   Restelli, M
AF Papini, Matteo
   Pirotta, Matteo
   Restelli, Marcello
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Batch Size for Safe Policy Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.
C1 [Papini, Matteo; Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy.
   [Pirotta, Matteo] Inria Lille, SequeL Team, Lille, France.
RP Papini, M (reprint author), Politecn Milan, DEIB, Milan, Italy.
EM matteo.papini@polimi.it; matteo.pirotta@inria.fr;
   marcello.restelli@polimi.it
RI Jeong, Yongwook/N-7413-2016
FU French Ministry of Higher Education and Research; French National
   Research Agency (ANR) [ANR-14-CE24-0010-01]; Nord-Pas-de-Calais Regional
   Council
FX This research was supported in part by French Ministry of Higher
   Education and Research, Nord-Pas-de-Calais Regional Council and French
   National Research Agency (ANR) under project ExTra-Learn (n.
   ANR-14-CE24-0010-01).
CR Abbasi-Yadkori Yasin, 2016, P 19 INT C ART INT S, P1338
   Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Bertsekas Dimitri P., 2011, Journal of Control Theory and Applications, V9, P310, DOI 10.1007/s11768-011-1005-3
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Ghavamzadeh Mohammad, 2016, SAFE POLICY IMPROVEM, P2298
   Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595
   Kakade  S., 2002, ICML, P267
   Kober J., 2008, NIPS 08 P 21 INT C N, P849
   Mnih V, 2008, P 25 INT C MACH LEAR, P672
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Peters J, 2008, NEUROCOMPUTING, V71, P1180, DOI 10.1016/j.neucom.2007.11.026
   Peters Jan, 2010, AAAI C ART INT, P24
   PINSKER M. S., 1960, INFORM INFORM STABIL
   Pirotta M., 2013, P INT C MACH LEARN I, P307
   Pirotta M., 2013, ADV NEURAL INFORM PR, P1394
   Pirotta M, 2015, MACH LEARN, V100, P255, DOI 10.1007/s10994-015-5484-1
   Scherrer B, 2014, P 31 INT C MACH LEAR, V32, P1314
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sehnke F, 2008, LECT NOTES COMPUT SC, V5163, P387, DOI 10.1007/978-3-540-87536-9_40
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Thomas Philip S., 2015, P 32 INT C MACH LEAR, P2380
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Zhao TT, 2012, NEURAL NETWORKS, V26, P118, DOI 10.1016/j.neunet.2011.09.005
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403064
DA 2019-06-15
ER

PT S
AU Park, S
   Min, S
   Choi, HS
   Yoon, S
AF Park, Seunghyun
   Min, Seonwoo
   Choi, Hyun-Soo
   Yoon, Sungroh
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Recurrent Neural Network-Based Identification of Precursor
   microRNAs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MIRNA GENES; CLASSIFICATION; REAL
AB MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.
C1 [Park, Seunghyun; Min, Seonwoo; Choi, Hyun-Soo; Yoon, Sungroh] Seoul Natl Univ, Elect & Comp Engn, Seoul 08826, South Korea.
   [Park, Seunghyun] Korea Univ, Sch Elect Engn, Seoul 02841, South Korea.
RP Yoon, S (reprint author), Seoul Natl Univ, Elect & Comp Engn, Seoul 08826, South Korea.
EM sryoon@snu.ac.kr
RI Jeong, Yongwook/N-7413-2016
FU Samsung Research Funding Center of the Samsung Electronics
   [SRFC-IT1601-05]; Institute for Information & communications Technology
   Promotion (IITP) - Korea government (MSIT) [2016-0-00087]; Future
   Flagship Program - Ministry of Trade, Industry & Energy (MOTIE, Korea)
   [10053249]; Basic Science Research Program through the National Research
   Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning
   [2016M3A7B4911115]; Brain Korea 21 Plus Project
FX This work was supported in part by the Samsung Research Funding Center
   of the Samsung Electronics [No. SRFC-IT1601-05], the Institute for
   Information & communications Technology Promotion (IITP) grant funded by
   the Korea government (MSIT) [No. 2016-0-00087], the Future Flagship
   Program funded by the Ministry of Trade, Industry & Energy (MOTIE,
   Korea) [No. 10053249], the Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT & Future Planning [No. 2016M3A7B4911115], and Brain Korea
   21 Plus Project in 2017.
CR Agarwal S, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-S1-S29
   Baldi P., 2001, BIOINFORMATICS MACHI
   Bartel DP, 2004, CELL, V116, P281, DOI 10.1016/S0092-8674(04)00045-5
   Batuwita R, 2009, BIOINFORMATICS, V25, P989, DOI 10.1093/bioinformatics/btp107
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bottou Leon, 1991, P NEUR, V91, P8
   Bu D., 2011, NUCLEIC ACIDS RES
   Chen J., 2016, SCI REPORTS, V6
   Chung J., 2014, CORR
   Friedlander MR, 2008, NAT BIOTECHNOL, V26, P407, DOI 10.1038/nbt1394
   Griffiths-Jones S, 2006, NUCLEIC ACIDS RES, V34, pD140, DOI 10.1093/nar/gkj112
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hofacker IL, 2003, NUCLEIC ACIDS RES, V31, P3429, DOI 10.1093/nar/gkg599
   Ioffe S., 2015, ARXIV150203167
   Jiang P, 2007, NUCLEIC ACIDS RES, V35, pW339, DOI 10.1093/nar/gkm368
   Kin T, 2007, NUCLEIC ACIDS RES, V35, pD145, DOI 10.1093/nar/gkl837
   KING G, 2001, POLITICAL ANAL, V0009
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kleftogiannis D, 2013, J BIOMED INFORM, V46, P563, DOI 10.1016/j.jbi.2013.02.002
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LEE RC, 1993, CELL, V75, P843, DOI 10.1016/0092-8674(93)90529-Y
   Lestrade L, 2006, NUCLEIC ACIDS RES, V34, pD158, DOI 10.1093/nar/gkj002
   LILLIEFORS HW, 1967, J AM STAT ASSOC, V62, P399, DOI 10.2307/2283970
   Lopes IDN, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-124
   Lorenz R, 2011, ALGORITHM MOL BIOL, V6, DOI 10.1186/1748-7188-6-26
   Lyngso RB, 2004, LECT NOTES COMPUT SC, V3142, P919
   Mathelier A, 2010, BIOINFORMATICS, V26, P2226, DOI 10.1093/bioinformatics/btq329
   Mendes ND, 2009, NUCLEIC ACIDS RES, V37, P2419, DOI 10.1093/nar/gkp145
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Rahman ME, 2012, GENOMICS, V99, P189, DOI 10.1016/j.ygeno.2012.02.001
   Rocktaschel  T., 2015, ARXIV150906664
   Starega-Roslan J, 2015, NUCLEIC ACIDS RES, V43, DOI 10.1093/nar/gkv968
   Thomas J, 2017, INT CONF BIG DATA, P96, DOI 10.1109/BIGCOMP.2017.7881722
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Tran VDT, 2015, RNA, V21, P775, DOI 10.1261/rna.043612.113
   Vinyals O., 2015, ADV NEURAL INFORM PR, P2773
   Wei LY, 2014, IEEE ACM T COMPUT BI, V11, P192, DOI 10.1109/TCBB.2013.146
   Xu K., 2015, ICML, V14, P77
   Xue CH, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-310
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402092
DA 2019-06-15
ER

PT S
AU Parra, G
   Tobar, F
AF Parra, Gabriel
   Tobar, Felipe
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Spectral Mixture Kernels for Multi-Output Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Early approaches to multiple-output Gaussian processes (MOGPs) relied on linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with the ability of single-output GPs to understand lengthscales, frequencies and magnitudes to name a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We extend this rationale and propose a parametric family of complex-valued cross-spectral densities and then build on Cramer's Theorem (the multivariate version of Bochner's Theorem) to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.
C1 [Parra, Gabriel] Univ Chile, Dept Math Engn, Santiago, Chile.
   [Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago, Chile.
RP Parra, G (reprint author), Univ Chile, Dept Math Engn, Santiago, Chile.
EM gparra@dim.uchile.cl; ftobar@dim.uchile.cl
RI Jeong, Yongwook/N-7413-2016
FU Conicyt Basal-CMM
FX We thank Cristobal Silva (Universidad de Chile) for useful
   recommendations about GPU implementation, Rasmus Bonnevie from the
   GPflow team for his assistance on the experimental MOGP module within
   GPflow, and the anonymous reviewers. This work was financially supported
   by Conicyt Basal-CMM.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Alvarez M, 2008, ADV NEURAL INFORM PR, V21, P57
   Alvarez M. A., 2010, INT C ART INT STAT, V9, P25
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Bochner S., 1959, ANN MATH STUDIES
   Boloix-Tortosa R, 2014, PR IEEE SEN ARRAY, P137, DOI 10.1109/SAM.2014.6882359
   Cramer H, 1940, ANN MATH, V41, P215, DOI 10.2307/1968827
   Duvenaud D, 2014, THESIS
   Genton M. G., 2015, I MATH STAT, V30
   Gneiting T, 2010, J AM STAT ASSOC, V105, P1167, DOI 10.1198/jasa.2010.tm09420
   Goovaerts P, 1997, GEOSTATISTICS NATURA
   Hensman J, 2016, ARXIV161106740
   Kay S. M., 1988, MODERN SPECTRAL ESTI
   Matthews A. G. d. G., 2016, GPFLOW GAUSSIAN PROC
   Pratt JW, 2012, CONCEPTS NONPARAMETR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Tobar F., 2016, P IEEE SAM, P2209
   Tobar F., 2015, NIPS 2015 TIM SER WO
   Tobar F, 2015, INT CONF ACOUST SPEE, P2209, DOI 10.1109/ICASSP.2015.7178363
   Tobar Felipe, 2015, NIPS, V29, P3501
   Ulrich K. R., 2015, ADV NEURAL INFORM PR, V28, P1999
   Wilson A., 2013, P 30 INT C MACH LEAR, P1067
   Yaglom A. M, 1987, CORRELATION THEORY S, VI
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406072
DA 2019-06-15
ER

PT S
AU Parthasarathy, N
   Batty, E
   Falcon, W
   Rutten, T
   Rajpal, M
   Chichilnisky, EJ
   Paninski, L
AF Parthasarathy, Nikhil
   Batty, Eleanor
   Falcon, William
   Rutten, Thomas
   Rajpal, Mohit
   Chichilnisky, E. J.
   Paninski, Liam
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural Networks for Efficient Bayesian Decoding of Natural Images from
   Retinal Neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECONSTRUCTION; RESPONSES
AB Decoding sensory stimuli from neural signals can be used to reveal how we sense our physical environment, and is valuable for the design of brain-machine interfaces. However, existing linear techniques for neural decoding may not fully reveal or exploit the fidelity of the neural signal. Here we develop a new approximate Bayesian method for decoding natural images from the spiking activity of populations of retinal ganglion cells (RGCs). We sidestep known computational challenges with Bayesian inference by exploiting artificial neural networks developed for computer vision, enabling fast nonlinear decoding that incorporates natural scene statistics implicitly. We use a decoder architecture that first linearly reconstructs an image from RGC spikes, then applies a convolutional autoencoder to enhance the image The resulting decoder, trained on natural images and simulated neural responses, significantly outperforms linear decoding, as well as simple point-wise nonlinear decoding. These results provide a tool for the assessment and optimization of retinal prosthesis technologies, and reveal that the retina may provide a more accurate representation of the visual scene than previously appreciated.
C1 [Parthasarathy, Nikhil; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA.
   [Batty, Eleanor; Falcon, William; Rutten, Thomas; Rajpal, Mohit; Paninski, Liam] Columbia Univ, New York, NY 10027 USA.
RP Parthasarathy, N (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM nikparth@gmail.com; erb2180@columbia.edu; waf2107@columbia.edu;
   tkr2112@columbia.edu; mr3522@columbia.edu; ej@stanford.edu;
   liam@stat.columbia.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF GRFP [DGE-16-44869]; NSF/NIH Collaborative Research in Computational
   Neuroscience Grant [IIS-1430348/1430239]; DARPA [N66001-17-C-4002,
   FA8650-16-1-7657]; Simons Foundation [SF-SCGB-365002]; IARPA MICRONS
   [D16PC00003]
FX NSF GRFP DGE-16-44869 (EB), NSF/NIH Collaborative Research in
   Computational Neuroscience Grant IIS-1430348/1430239 (EJC & LP), DARPA
   Contract FA8650-16-1-7657 (EJC), Simons Foundation SF-SCGB-365002 (LP);
   IARPA MICRONS D16PC00003 (LP); DARPA N66001-17-C-4002 (LP).
CR Anderson AG, 2016, CONF REC ASILOMAR C, P588, DOI 10.1109/ACSSC.2016.7869110
   Arsenault E, 2011, J VISION, V11, DOI 10.1167/11.10.14
   Batty E., 2017, INT C LEARN REPR
   Brouwer GJ, 2009, J NEUROSCI, V29, P13992, DOI 10.1523/JNEUROSCI.3577-09.2009
   Brown EN, 1998, J NEUROSCI, V18, P7411
   Burak Y, 2010, P NATL ACAD SCI USA, V107, P19525, DOI 10.1073/pnas.1006076107
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diaz-Tahoces Ariadna R., 2015, 6 INT WORK C INT NAT, V9107
   Frechette ES, 2005, J NEUROPHYSIOL, V94, P119, DOI 10.1152/jn.01175.2004
   Gondara Lovedeep, 2016, 160804667 ARXIV
   Heitman A., 2016, BIORXIV, DOI DOI 10.1101/045336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kamitani Y, 2005, NAT NEUROSCI, V8, P679, DOI 10.1038/nn1444
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koyama S, 2010, J AM STAT ASSOC, V105, P170, DOI 10.1198/jasa.2009.tm08326
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lalor EC, 2009, J OPT SOC AM A, V26, pB25, DOI 10.1364/JOSAA.26.000B25
   Laparra Valero, 2017, ARXIV170106641
   Ledig C, 2016, ARXIV160904802
   Liu  Z., 2015, P INT C COMP VIS ICC
   Mao  X., 2016, ADV NEURAL INFORM PR
   Marre O, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004304
   McIntosh Lane T., 2016, ADV NEURAL INFORM PR
   Naselaris T, 2009, NEURON, V63, P902, DOI 10.1016/j.neuron.2009.09.006
   Nirenberg Sheila, 2012, PNAS, V109
   Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031
   Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x
   Pasley BN, 2012, PLOS BIOL, V10, DOI 10.1371/journal.pbio.1001251
   Ramirez AD, 2011, J NEUROSCI, V31, P3828, DOI 10.1523/JNEUROSCI.3256-10.2011
   Rieke F, 1999, SPIKES EXPLORING NEU
   Shpigelman L, 2009, ADV NEURAL INFORM PR, P1489
   Stanley GB, 1999, J NEUROSCI, V19, P8036
   Sussillo D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13749
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Zhangyang, 2015, P IEEE C COMP VIS PA
   Warland DK, 1997, J NEUROPHYSIOL, V78, P2336
   Wen Haiguang, 2016, 160803425 ARXIV
   Xie  J., 2012, P INT C NEUR INF PRO, P341
   Xu Kai, 2011, ENG MED BIOL SOC EMB
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zhao H, 2015, ARXIV151108861
NR 42
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406049
DA 2019-06-15
ER

PT S
AU Peck, J
   Roels, J
   Goossens, B
   Saeys, Y
AF Peck, Jonathan
   Roels, Joris
   Goossens, Bart
   Saeys, Yvan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Lower bounds on the robustness to adversarial perturbations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific, hardly perceptible perturbations to the input, called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them, but a proven explanation remains elusive. In this work, we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The proposed bounds can be computed efficiently, requiring time at most linear in the number of parameters and hyperparameters of the model for any given sample. This makes them suitable for use in model selection, when one wishes to find out which of several proposed classifiers is most robust to adversarial perturbations. They may also be used as a basis for developing techniques to increase the robustness of classifiers, since they enjoy the theoretical guarantee that no adversarial perturbation could possibly be any smaller than the quantities provided by the bounds. We experimentally verify the bounds on the MNIST and CIFAR-10 data sets and find no violations. Additionally, the experimental results suggest that very small adversarial perturbations may occur with non-zero probability on natural samples.
C1 [Peck, Jonathan; Saeys, Yvan] Univ Ghent, Dept Appl Math Comp Sci & Stat, B-9000 Ghent, Belgium.
   [Peck, Jonathan; Roels, Joris; Saeys, Yvan] VIB Inflammat Res Ctr, Data Min & Modeling Biomed, B-9052 Ghent, Belgium.
   [Roels, Joris; Goossens, Bart] Univ Ghent, Dept Telecommun & Informat Proc, B-9000 Ghent, Belgium.
RP Peck, J (reprint author), Univ Ghent, Dept Appl Math Comp Sci & Stat, B-9000 Ghent, Belgium.; Peck, J (reprint author), VIB Inflammat Res Ctr, Data Min & Modeling Biomed, B-9052 Ghent, Belgium.
CR Fawzi Alhussein, 2016, ADV NEURAL INFORM PR, V29, P1632
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Goodfellow I., 2015, P 3 INT C LEARN REPR, V3
   Gu Shixiang, 2014, NIPS WORKSH DEEP LEA
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin M., 2014, P INT C LEARN REPR
   Lou Y., 2016, ARXIV151106292
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Rozsa A., 2016, ARXIV161200138
   Simonyan K., 2015, P 3 INT C LEARN REPR, V3
   Szegedy C., 2014, P 2 INT C LEARN REPR, V2
   Szegedy Christian, 2015, P IEEE C COMP VIS PA
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400077
DA 2019-06-15
ER

PT S
AU Pennington, J
   Worah, P
AF Pennington, Jeffrey
   Worah, Pratik
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Nonlinear random matrix theory for deep learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix (YY)-Y-T, Y = f(W X), where W is a random weight matrix, X is a random data matrix, and f is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature networks on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.
C1 [Pennington, Jeffrey] Google Brain, Mountain View, CA 94043 USA.
   [Worah, Pratik] Google Res, Mountain View, CA USA.
RP Pennington, J (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM jpennin@google.com; pworah@google.com
RI Jeong, Yongwook/N-7413-2016
CR AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007
   Cheng XY, 2013, RANDOM MATRICES-THEO, V2, DOI 10.1142/S201032631350010X
   Choromanska Anna, 2015, AISTATS
   Daniely A., 2016, ARXIV160205897
   Dupic Thomas, 2014, ARXIV14017802
   El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648
   GARDNER E, 1988, J PHYS A-MATH GEN, V21, P271, DOI 10.1088/0305-4470/21/1/031
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Louart Cosme, 2017, ARXIV170205419
   Marchenko V. A., 1967, MATH USSR SB, V72, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]
   Neal R N, 1994, THESIS
   Neal R N, 1994, CRGTR941 U TOR
   Pennington J., 2017, ADV NEURAL INFORM PR
   Poole B., 2016, ARXIV160605340
   Raghu M., 2016, ARXIV160605336
   Rahimi Ali, 2007, NEURAL INFOMRATION P
   Saxe  A.M., 2014, INT C LEARN REPR
   Schoenholz S. S., 2017, ARXIV E PRINTS
   Schoenholz S. S., 2016, ARXIV E PRINTS
   Shazeer N., 2017, ICLR
   Tao T, 2012, TOPICS RANDOM MATRIX, V132
   Van Den Oord A., 2016, ARXIV160903499
   Wu Y., 2016, ARXIV160908144
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402067
DA 2019-06-15
ER

PT S
AU Pennington, J
   Schoenholz, SS
   Ganguli, S
AF Pennington, Jeffrey
   Schoenholz, Samuel S.
   Ganguli, Surya
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Resurrecting the sigmoid in deep learning through dynamical isometry:
   theory and practice
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB It is well known that weight initialization in deep networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is O(1) is essential for avoiding exponentially vanishing or exploding gradients. Moreover, in deep linear networks, ensuring that all singular values of the Jacobian are concentrated near 1 can yield a dramatic additional speed-up in learning; this is a property known as dynamical isometry. However, it is unclear how to achieve dynamical isometry in nonlinear deep networks. We address this question by employing powerful tools from free probability theory to analytically compute the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.
C1 [Pennington, Jeffrey; Schoenholz, Samuel S.; Ganguli, Surya] Google Brain, Mountain View, CA 94043 USA.
   [Ganguli, Surya] Stanford Univ, Appl Phys, Stanford, CA 94305 USA.
RP Pennington, J (reprint author), Google Brain, Mountain View, CA 94043 USA.
RI Jeong, Yongwook/N-7413-2016
FU Simons Foundation; McKnight Foundation; James S. McDonnell Foundation;
   Burroughs Wellcome Foundation; Office of Naval Research
FX S.G. thanks the Simons, McKnight, James S. McDonnell, and Burroughs
   Wellcome Foundations and the Office of Naval Research for support.
CR Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Hinton  G., NEURAL NETWORKS MACH
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kingma D. P., 2014, ARXIV14126980
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lagrange Joseph Louis, 1770, NOUVELLE METHODE RES
   McIntosh Lane T, 2016, Adv Neural Inf Process Syst, V29, P1369
   Mishkin D., 2015, CORR
   Neuschel T, 2014, RANDOM MATRICES-THEO, V3, DOI 10.1142/S2010326314500038
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Piech C., 2015, ADV NEURAL INFORM PR, P505
   Poole B., 2016, NEURAL INFORM PROCES
   Saxe Andrew M, 2013, ICLR 2014
   Schoenholz S. S., 2017, INT C LEARN REPR ICL
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   SPEICHER R, 1994, MATH ANN, V298, P611, DOI 10.1007/BF01459754
   Tao T, 2012, TOPICS RANDOM MATRIX, V132
   Voiculescu D. V., 1992, FREE RANDOM VARIABLE
   Wu Y., 2016, CORR
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zoph B., 2016, CORR
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404083
DA 2019-06-15
ER

PT S
AU Perolat, J
   Leibo, JZ
   Zambaldi, V
   Beattie, C
   Tuyls, K
   Graepel, T
AF Perolat, Julien
   Leibo, Joel Z.
   Zambaldi, Vinicius
   Beattie, Charles
   Tuyls, Karl
   Graepel, Thore
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A multi-agent reinforcement learning model of common-pool resource
   appropriation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TERRITORIALITY; TRAGEDY
AB Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria-a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.
C1 [Perolat, Julien; Leibo, Joel Z.; Zambaldi, Vinicius; Beattie, Charles; Graepel, Thore] DeepMind, London, England.
   [Tuyls, Karl] Univ Liverpool, Liverpool, Merseyside, England.
RP Perolat, J (reprint author), DeepMind, London, England.
EM perolat@google.com; jzl@google.com; yzambaldi@google.com;
   cbeattie@google.com; karltuyls@google.com; thore@google.com
RI Jeong, Yongwook/N-7413-2016
CR Acheson JM, 2005, RATION SOC, V17, P309, DOI 10.1177/1043463105051634
   Axelrod R. M., 1984, EVOLUTION COOPERATIO
   Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919
   Camerer CF, 1997, J ECON PERSPECT, V11, P167, DOI 10.1257/jep.11.4.167
   Chalkiadakis G., 2003, P 2 INT JOINT C AUT, P709, DOI 10.1145/860575.860689
   Dietz T, 2003, SCIENCE, V302, P1907, DOI 10.1126/science.1091015
   Gardner R., 1990, RATION SOC, V2, P335, DOI DOI 10.1177/1043463190002003005
   GINI C., 1912, VARIABILITA MUTABILI
   Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579
   Gordon HS, 1954, J POLIT ECON, V62, P124, DOI 10.1086/257497
   Greenwald A., 2003, 20 INT C MACH LEARN, P242
   HARDIN G, 1968, SCIENCE, V162, P1243
   Janssen M., 2010, ECOLOGY SOC, V15
   Janssen Marco, 2008, INT J COMMONS, V2
   Janssen MA, 2008, RATION SOC, V20, P371, DOI 10.1177/1043463108096786
   Janssen MA, 2013, ECOL SOC, V18, DOI 10.5751/ES-05664-180404
   Janssen MA, 2010, SCIENCE, V328, P613, DOI 10.1126/science.1183532
   Junling Hu, 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P242
   Kleiman-Weiner Max, 2016, P 38 ANN C COGN SCI
   Laurent GJ, 2011, INT J KNOWL-BASED IN, V15, P55, DOI 10.3233/KES-2010-0206
   Leibo Joel Z., 2017, P 16 INT C AUT AG MU
   Littman M., 1994, P 11 INT C MACH LEAR, V157, P157
   Martin Kent O, 1979, N ATLANTIC MARITIME, P277
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   NOWAK M, 1993, NATURE, V364, P56, DOI 10.1038/364056a0
   Ostrom E, 1999, SCIENCE, V284, P278, DOI 10.1126/science.284.5412.278
   OSTROM E, 1993, J ECON PERSPECT, V7, P93, DOI 10.1257/jep.7.4.93
   Ostrom E., 1977, ALTERNATIVES DELIVER, P7
   Ostrom E, 1994, RULES GAMES COMMON P
   Ostrom Elinor, 1990, GOVERNING COMMONS EV
   Rankin DJ, 2007, TRENDS ECOL EVOL, V22, P643, DOI 10.1016/j.tree.2007.07.009
   SCHELLING TC, 1973, J CONFLICT RESOLUT, V17, P381, DOI 10.1177/002200277301700302
   Shapley L. S., 1953, P NATL ACAD SCI US
   Shoham Y, 2007, ARTIF INTELL, V171, P365, DOI 10.1016/j.artint.2006.02.006
   SMITH VL, 1968, AM ECON REV, V58, P409
   Turner RA, 2013, SOC NATUR RESOUR, V26, P491, DOI 10.1080/08941920.2012.709313
   Varakantham Pradeep, 2009, P 19 INT C AUT PLANN
   Yu C, 2015, IEEE T NEUR NET LEAR, V26, P3083, DOI 10.1109/TNNLS.2015.2403394
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403069
DA 2019-06-15
ER

PT S
AU Perrault-Joncas, DC
   Meila, M
   McQueen, J
AF Perrault-Joncas, Dominique C.
   Meila, Marina
   McQueen, James
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Improved Graph Laplacian via Geometric Consistency
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE; REDUCTION
AB In all manifold learning algorithms and tasks setting the kernel bandwidth epsilon used construct the graph Laplacian is critical. We address this problem by choosing a quality criterion for the Laplacian, that measures its ability to preserve the geometry of the data. For this, we exploit the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator. Experiments show that this principled approach is effective and robust.
C1 [Perrault-Joncas, Dominique C.] Google Inc, Mountain View, CA 94043 USA.
   [Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
   [McQueen, James] Amazon, Seattle, WA USA.
RP Perrault-Joncas, DC (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM dominiquep@google.com; mmp2@uw.edu; jmcq@amazon.com
CR [Anonymous], 2012, METRIC LEARNIN UNPUB
   Aswani A, 2011, ANN STAT, V39, P48, DOI 10.1214/10-AOS823
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M., 2007, NIPS PROCESSING, P129
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Berry Tyrus, 2016, CONSISTENT MANIFOLD
   Carter KM, 2007, 2007 IEEE/SP 14TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P601, DOI 10.1109/SSP.2007.4301329
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chazal Frederic, 2016, ADV NEURAL INFORM PR, V29, P3963
   Chen GL, 2011, APPL NUMER HARMON AN, P199, DOI 10.1007/978-0-8176-8095-4_10
   Chen LS, 2009, J AM STAT ASSOC, V104, P209, DOI 10.1198/jasa.2009.0111
   Coifman R. R., 2006, APPL COMPUT HARMON A, V21, p[6, 1]
   Dasgupta S, 2008, ACM S THEORY COMPUT, P537
   Gine E., 2006, IMS LECT NOTES MONOG, V51, P238, DOI DOI 10.1214/074921706000000888
   Goldberg Y, 2008, J MACH LEARN RES, V9, P1909
   Hein M, 2007, J MACH LEARN RES, V8, P1325
   Lee JM, 1997, RIEMANNIAN MANIFOLDS
   Lee JA, 2007, INFORM SCI STAT, P1
   LEVINA E, 2005, ADV NIPS, V17
   Rosenberg  S., 1997, LAPLACIAN RIEMANNIAN
   Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667
   Sindhwani V., 2007, P INT JOINT C ART IN
   Singer A, 2006, APPL COMPUT HARMON A, V21, P128, DOI 10.1016/j.acha.2006.03.004
   Smola A. J., 2003, P ANN C COMP LEARN T
   Ting D., 2010, P 27 INT C MACH LEAR, P1079
   von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640
   Wang Xu, 2015, SPECTRAL CONVERGENCE
   Yue HH, 2004, IEEE DECIS CONTR P, P4262, DOI 10.1109/CDC.2004.1429421
   Zhou X., 2011, AISTAT
   Zhu  Xiaojin, 2003, TECHNICAL REPORT
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404051
DA 2019-06-15
ER

PT S
AU Peter, S
   Kirschbaum, E
   Both, M
   Campbell, LA
   Harvey, BK
   Heins, C
   Durstewitz, D
   Andilla, FD
   Hamprecht, FA
AF Peter, Sven
   Kirschbaum, Elke
   Both, Martin
   Campbell, Lee A.
   Harvey, Brandon K.
   Heins, Conor
   Durstewitz, Daniel
   Andilla, Ferran Diego
   Hamprecht, Fred A.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sparse convolutional coding for neuronal assembly detection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CORTICAL ACTIVITY; SPIKING ACTIVITY; CELL ASSEMBLIES; MULTIPLE LEVELS;
   SYNFIRE CHAINS; UNITARY EVENTS; DECONVOLUTION; PATTERNS; REGULARIZATION;
   PROBABILITY
AB Cell assemblies, originally proposed by Donald Hebb (1949), are subsets of neurons firing in a temporally coordinated way that gives rise to repeated motifs supposed to underly neural representations and information processing. Although Hebb's original proposal dates back many decades, the detection of assemblies and their role in coding is still an open and current research topic, partly because simultaneous recordings from large populations of neurons became feasible only relatively recently. Most current and easy-to-apply computational techniques focus on the identification of strictly synchronously spiking neurons. In this paper we propose a new algorithm, based on sparse convolutional coding, for detecting recurrent motifs of arbitrary structure up to a given length. Testing of our algorithm on synthetically generated datasets shows that it outperforms established methods and accurately identifies the temporal structure of embedded assemblies, even when these contain overlapping neurons or when strong background noise is present. Moreover, exploratory analysis of experimental datasets from hippocampal slices and cortical neuron cultures have provided promising results.
C1 [Peter, Sven; Kirschbaum, Elke; Hamprecht, Fred A.] Interdisciplinary Ctr Sci Comp IWR, Heidelberg, Germany.
   [Both, Martin] Inst Physiol & Pathophysiol, Heidelberg, Germany.
   [Campbell, Lee A.; Harvey, Brandon K.; Heins, Conor] NIDA, Baltimore, MD USA.
   [Heins, Conor] Max Planck Inst Dynam & Self Org, Gottingen, Germany.
   [Durstewitz, Daniel] Cent Inst Mental Hlth, Dept Theoret Neurosci, Mannheim, Germany.
   [Andilla, Ferran Diego] Robert Bosch GmbH, Hildesheim, Germany.
RP Peter, S (reprint author), Interdisciplinary Ctr Sci Comp IWR, Heidelberg, Germany.
EM sven.peter@iwr.uni-heidelberg.de; elke.kirschbaum@iwr.uni-heidelberg.de;
   mboth@physiologie.uni-heidelberg.de; lee.campbell@nih.gov;
   bharvey@mail.nih.gov; conor.heins@ds.mpg.de;
   daniel.durstewitz@zi-mannheim.de; ferran.diegoandilla@de.bosch.com;
   fred.hamprecht@iwr.uni-heidelberg.de
RI Jeong, Yongwook/N-7413-2016
FU Intramural Research Program of the NIH, NIDA; DFG [SFB 1134, Du 354/8-1]
FX SP and EK thank Eleonora Russo for sharing her knowledge on generating
   synthetic data and Fynn Bachmann for his support. LAC, BKH and CH thank
   Lowella Fortuno for technical assistance with cortical cultures and
   acknowledge the support by the Intramural Research Program of the NIH,
   NIDA. DD acknowledges partial financial support by DFG Du 354/8-1. SP,
   EK, MB, DD, FD and FAH gratefully acknowledge partial financial support
   by DFG SFB 1134.
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   Andilla F. D., 2014, ADV NEURAL INFORM PR, P64
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Billeh YN, 2014, J NEUROSCI METH, V236, P92, DOI 10.1016/j.jneumeth.2014.08.011
   Buzsaki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233
   Buzsaki G, 1998, J SLEEP RES, V7, P17, DOI 10.1046/j.1365-2869.7.s1.3.x
   Carrillo-Reid L, 2015, J NEUROSCI, V35, P8813, DOI 10.1523/JNEUROSCI.5214-14.2015
   Cichocki A, 2006, ELECTRON LETT, V42, P947, DOI 10.1049/el:20060983
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Cossart P, 2004, SCIENCE, V304, P242, DOI 10.1126/science.1090124
   Diego F., 2013, NIPS
   Gerstein GL, 2012, J NEUROSCI METH, V206, P54, DOI 10.1016/j.jneumeth.2012.02.003
   Girardeau G, 2011, CURR OPIN NEUROBIOL, V21, P452, DOI 10.1016/j.conb.2011.02.005
   Girardeau G, 2009, NAT NEUROSCI, V12, P1222, DOI 10.1038/nn.2384
   Grun S, 2002, NEURAL COMPUT, V14, P81, DOI 10.1162/089976602753284464
   Grun S, 2002, NEURAL COMPUT, V14, P43, DOI 10.1162/089976602753284455
   Hansen PC, 2002, NUMER ALGORITHMS, V29, P323, DOI 10.1023/A:1015222829062
   Hebb D. O, 1949, ORG BEHAV NEUROPSYCH
   Howard DB, 2008, VIROLOGY, V372, P24, DOI 10.1016/j.virol.2007.10.007
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Lopes-dos-Santos V, 2013, J NEUROSCI METH, V220, P149, DOI 10.1016/j.jneumeth.2013.04.010
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Marr D., 1991, SIMPLE MEMORY THEORY
   Mokeichev A, 2007, NEURON, V53, P413, DOI 10.1016/j.neuron.2007.01.017
   Nicolelis MAL, 1997, NEURON, V19, P219, DOI 10.1016/S0896-6273(00)80932-0
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   O'Grady Paul D, 2006, Proceedings of the 2006 IEEE Signal Processing Society Workshop, P427
   Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775
   Pfeiffer T, 2014, NEUROIMAGE, V94, P239, DOI 10.1016/j.neuroimage.2014.03.030
   PIERSKALLA WP, 1968, OPER RES, V16, P422, DOI 10.1287/opre.16.2.422
   Pnevmatikakis E. A., ARXIV14092903QBIOSTA
   Pnevmatikakis E. A., 2013, NIPS
   Pnevmatikakis E. A., 2013, COMPUTATIONAL SYSTEM
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P27, DOI 10.1109/TIP.2008.2008065
   Quiroga-Lombard CS, 2013, J NEUROPHYSIOL, V110, P562, DOI 10.1152/jn.00186.2013
   Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477
   Russo E, 2017, ELIFE, V6, DOI 10.7554/eLife.19428
   Santos V. Lopes-dos, 2011, PLOS ONE, V6, P1
   SINGER W, 1993, ANNU REV PHYSIOL, V55, P349, DOI 10.1146/annurev.physiol.55.1.349
   Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494
   Smith AC, 2006, NEURAL COMPUT, V18, P1197, DOI 10.1162/neco.2006.18.5.1197
   Smith AC, 2010, NEURAL COMPUT, V22, P2522, DOI 10.1162/NECO_a_00020
   Staude B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00016
   Staude B, 2010, J COMPUT NEUROSCI, V29, P327, DOI 10.1007/s10827-009-0195-x
   Stevens CF, 2003, NEURON, V40, P381, DOI 10.1016/S0896-6273(03)00643-3
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Szlam A., 2010, COMPUTER RES REPOSIT
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Torre E, 2016, J NEUROSCI, V36, P8329, DOI 10.1523/JNEUROSCI.4375-15.2016
   Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009
   Weiss R. J., 2010, ISMIR
   Yuste R, 2005, NAT REV NEUROSCI, V6, P477, DOI 10.1038/nrn1686
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 54
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403072
DA 2019-06-15
ER

PT S
AU Peter, S
   Diego, F
   Hamprecht, FA
   Nadler, B
AF Peter, Sven
   Diego, Ferran
   Hamprecht, Fred A.
   Nadler, Boaz
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Cost efficient gradient boosting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ARTIFICIAL NEURAL-NETWORKS; CLASSIFICATION; TREES
AB Many applications require learning classifiers or regressors that are both accurate and cheap to evaluate. Prediction cost can be drastically reduced if the learned predictor is constructed such that on the majority of the inputs, it uses cheap features and fast evaluations. The main challenge is to do so with little loss in accuracy. In this work we propose a budget-aware strategy based on deep boosted regression trees. In contrast to previous approaches to learning with cost penalties, our method can grow very deep trees that on average are nonetheless cheap to compute. We evaluate our method on a number of datasets and find that it outperforms the current state of the art by a large margin. Our algorithm is easy to implement and its learning time is comparable to that of the original gradient boosting. Source code is made available at http://github.com/svenpeter42/LightGBM-CEGB.
C1 [Peter, Sven; Hamprecht, Fred A.] Heidelberg Univ, Interdisciplinary Ctr Sci Comp, Heidelberg Collab Image Proc, D-69115 Heidelberg, Germany.
   [Diego, Ferran] Robert Bosch GmbH, Robert Bosch Str 200, D-31139 Hildesheim, Germany.
   [Nadler, Boaz] Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel.
RP Peter, S (reprint author), Heidelberg Univ, Interdisciplinary Ctr Sci Comp, Heidelberg Collab Image Proc, D-69115 Heidelberg, Germany.
EM sven.peter@iwr.ani-heidelberg.de; ferran.diegoandilla@de.bosch.com;
   fred.hamprecht@iwr.uni-heidelberg.de; boaz.nadler@weizmann.ac.ir
RI Jeong, Yongwook/N-7413-2016
CR Amayeh G, 2009, LECT NOTES COMPUT SC, V5875, P243, DOI 10.1007/978-3-642-10331-5_23
   Baldi Pierre, 2016, ARXIV160107913
   Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Castelli V, 1996, INT CONF ACOUST SPEE, P2199, DOI 10.1109/ICASSP.1996.545857
   Chapelle O., 2011, JMLR P TRACK, V14, P1
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   DeSalvo G, 2015, LECT NOTES ARTIF INT, V9355, P254, DOI 10.1007/978-3-319-24486-0_17
   Dollar P., 2010, BMVC, V2, P7
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Getreuer P, 2013, IMAGE PROCESS ON LIN, V3, P286, DOI 10.5201/ipol.2013.87
   Han S., 2016, INT C LEARN REPR ICL
   Hancock T, 1996, INFORM COMPUT, V126, P114
   Hubara I., 2016, ADV NEURAL INFORM PR, V29, P4107
   Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8
   Ke G., 2017, ADV NEURAL INFORM PR, P3149
   Kusner M, 2014, AAAI, V3, P1939
   Lefakis L., 2010, ADV NEURAL INFORM PR, V23, P1315
   Lichman M., 2013, UCI MACHINE LEARNING
   Maska M, 2014, BIOINFORMATICS, V30, P1609, DOI 10.1093/bioinformatics/btu080
   Nan F., 2016, ADV NEURAL INFORM PR, P2334
   Nan F., 2015, P 32 INT C MACH LEAR, P1983
   Naumov G. E., 1991, Soviet Physics - Doklady, V36, P270
   Pedersoli M, 2015, PATTERN RECOGN, V48, P1844, DOI 10.1016/j.patcog.2014.11.006
   Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Shi H, 2007, THESIS
   Trapeznikov K., 2013, P 16 INT C ART INT S, P581
   Viola P., 2001, COMP VIS PATT REC 20
   Wang J., 2015, ADV NEURAL INFORM PR, P2152
   Xu Z., 2013, P 30 INT C MACH LEAR, P133
   Xu Zhixiang, 2012, P 29 INT C MACH LEAR, P1175
   Xu Z, 2014, J MACH LEARN RES, V15, P2113
   Zantema H., 2000, International Journal of Foundations of Computer Science, V11, P343, DOI 10.1142/S0129054100000193
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401057
DA 2019-06-15
ER

PT S
AU Pica, G
   Piasini, E
   Safaai, H
   Runyan, CA
   Diamond, ME
   Fellin, T
   Kayser, C
   Harvey, CD
   Panzeri, S
AF Pica, Giuseppe
   Piasini, Eugenio
   Safaai, Houman
   Runyan, Caroline A.
   Diamond, Mathew E.
   Fellin, Tommaso
   Kayser, Christoph
   Harvey, Christopher D.
   Panzeri, Stefano
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Quantifying how much sensory information in a neural code is relevant
   for behavior
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DISCRIMINATION; NEURONS; CHOICE; PERCEPTION
AB Determining how much of the sensory information carried by a neural code contributes to behavioral performance is key to understand sensory function and neural information flow. However, there are as yet no analytical tools to compute this information that lies at the intersection between sensory coding and behavioral readout. Here we develop a novel measure, termed the information-theoretic intersection information I-II(S; R; C), that quantifies how much of the sensory information carried by a neural response R is used for behavior during perceptual discrimination tasks. Building on the Partial Information Decomposition framework, we define I-II(S; R; C) as the part of the mutual information between the stimulus S and the response R that also informs the consequent behavioral choice C. We compute I-II(S; R; C) in the analysis of two experimental cortical datasets, to show how this measure can be used to compare quantitatively the contributions of spike timing and spike rates to task performance, and to identify brain areas or neural populations that specifically transform sensory information into choice.
C1 [Pica, Giuseppe; Piasini, Eugenio; Safaai, Houman; Panzeri, Stefano] Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Computat Lab, I-38068 Rovereto, TN, Italy.
   [Pica, Giuseppe; Fellin, Tommaso; Panzeri, Stefano] Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Coding Lab, I-38068 Rovereto, TN, Italy.
   [Safaai, Houman; Runyan, Caroline A.; Harvey, Christopher D.] Harvard Med Sch, Dept Neurobiol, Boston, MA 02115 USA.
   [Runyan, Caroline A.] Univ Pittsburgh, Dept Neurosci, Ctr Neural Basis Cognit, Pittsburgh, PA USA.
   [Diamond, Mathew E.] Int Sch Adv Studies SISSA, Tactile Percept & Learning Lab, Trieste, Italy.
   [Fellin, Tommaso] Ist Italiano Tecnol, Opt Approaches Brain Funct Lab, I-16163 Genoa, Italy.
   [Kayser, Christoph] Univ Glasgow, Inst Neurosci & Psychol, Glasgow, Lanark, Scotland.
   [Kayser, Christoph] Bielefeld Univ, Fac Biol, Dept Cognit Neurosci, Universitatsstr 25, D-33615 Bielefeld, Germany.
RP Pica, G (reprint author), Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Computat Lab, I-38068 Rovereto, TN, Italy.; Pica, G (reprint author), Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Coding Lab, I-38068 Rovereto, TN, Italy.
EM giuseppe.pica@iit.it; eugenio.piasini@iit.it;
   houman_safaai@hms.haryard.edu; runyan@pitt.edu; diamond@sissa.it;
   tommaso.fellin@iit.it; christoph.kayser@uni-bielefeld.de;
   Christopher_Haryey@hms.haryard.edu; stefano.panzeri@iit.it
RI Jeong, Yongwook/N-7413-2016
FU Seal of Excellence Fellowship CONISC; Fondation Bertarelli; NIH
   [1U01NS090576-01, MH107620, NS089521]; ERC (NEURO-PATTERNS); European
   Research Council (ERC-2014-CoG) [646657]
FX GP was supported by a Seal of Excellence Fellowship CONISC. SP was
   supported by Fondation Bertarelli. CDH was supported by grants from the
   NIH (MH107620 and NS089521). CDH is a New York Stem Cell Foundation
   Robertson Neuroscience Investigator. TF was supported by the grants ERC
   (NEURO-PATTERNS) and NIH (1U01NS090576-01). CK was supported by the
   European Research Council (ERC-2014-CoG; grant No 646657).
CR Barrett AB, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.052802
   Bertschinger N., 2012, P ECCS 2012 BRUSS BE
   Bertschinger N, 2014, ENTROPY-SWITZ, V16, P2161, DOI 10.3390/e16042161
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Britten KH, 1996, VISUAL NEUROSCI, V13, P87, DOI 10.1017/S095252380000715X
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Chicharro D., 2017, ARXIV170803845
   Engineer CT, 2008, NAT NEUROSCI, V11, P603, DOI 10.1038/nn.2109
   Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038
   Griffith V, 2014, EMERGENCE COMPLEX CO, V9, P159, DOI 10.1007/978-3-642-53734-9_6
   Haefner RM, 2013, NAT NEUROSCI, V16, P235, DOI 10.1038/nn.3309
   Harder M, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.012130
   Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918
   Harvey MA, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001558
   Jacobs AL, 2009, P NATL ACAD SCI USA, V106, P5936, DOI 10.1073/pnas.0900573106
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Luczak A, 2015, NAT REV NEUROSCI, V16, P745, DOI 10.1038/nrn4026
   Luna R, 2005, NAT NEUROSCI, V8, P1210, DOI 10.1038/nn1513
   Nakamura K, 1999, J NEUROPHYSIOL, V82, P2503
   NEWSOME WT, 1989, NATURE, V341, P52, DOI 10.1038/341052a0
   O'Connor DH, 2013, NAT NEUROSCI, V16, P958, DOI 10.1038/nn.3419
   Panzeri S, 2017, NEURON, V93, P491, DOI 10.1016/j.neuron.2016.12.036
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Panzeri S, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0467
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Pica G, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19090451
   Pitkow X, 2015, NEURON, V87, P411, DOI 10.1016/j.neuron.2015.06.033
   Quiroga RQ, 2009, NAT REV NEUROSCI, V10, P173, DOI 10.1038/nrn2578
   Raposo D, 2014, NAT NEUROSCI, V17, P1784, DOI 10.1038/nn.3865
   Rauschecker JP, 2000, P NATL ACAD SCI USA, V97, P11800, DOI 10.1073/pnas.97.22.11800
   Romo R, 2003, NAT REV NEUROSCI, V4, P203, DOI 10.1038/nrn1058
   Rossi-Pool R, 2016, P NATL ACAD SCI USA, V113, pE7966, DOI 10.1073/pnas.1618196113
   Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shamir M, 2014, CURR OPIN NEUROBIOL, V25, P140, DOI 10.1016/j.conb.2014.01.002
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Victor JD, 2008, NEURAL COMPUT, V20, P2895, DOI 10.1162/neco.2008.10-07-633
   Williams P. L., 2010, COMPUT RES REPOSITOR, V1004, P2515, DOI DOI 10.HTTPS://ARXIV.0RG/ABS/1004.2515
   Zuo YF, 2015, CURR BIOL, V25, P357, DOI 10.1016/j.cub.2014.11.065
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403073
DA 2019-06-15
ER

PT S
AU Platanios, EA
   Poon, H
   Mitchell, TM
   Horvitz, E
AF Platanios, Emmanouil A.
   Poon, Hoifung
   Mitchell, Tom M.
   Horvitz, Eric
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.
C1 [Platanios, Emmanouil A.; Mitchell, Tom M.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Poon, Hoifung; Horvitz, Eric] Microsoft Res, Redmond, WA USA.
RP Platanios, EA (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM e.a.platanios@cs.cmu.edu; hoifung@microsoft.com;
   tom.mitchell@cs.cmu.edu; horvitz@microsoft.com
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS1250956]; Carnegie Mellon University
FX We would like to thank Abulhair Saparov and Otilia Stretcu for the
   useful feedback they provided in early versions of this paper. This
   research was performed during an internship at Microsoft Research, and
   was also supported in part by NSF under award IIS1250956, and in part by
   a Presidential Fellowship from Carnegie Mellon University.
CR Bach S. H., 2015, CORR
   Bach S. H., 2013, C UNC ART INT
   Balcan N, 2013, P 30 INT C MACH LEAR, P1112
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Brocheler M., 2010, P 26 C UNC ART INT U, P73
   Collins J, 2014, STAT MED, V33, P4141, DOI 10.1002/sim.6218
   Collins M., 1999, JOINT C EMP METH NAT
   Dasgupta S., 2001, ADV NEURAL INFORM PR, P375
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Klir GJ, 1995, FUZZY SETS FUZZY LOG
   Madani O., 2004, NEURAL INFORM PROCES
   Mitchell T., 2015, NEVER ENDING LEARNIN
   Moreno P. G., 2015, J MACHINE LEARNING R, V16
   Niu F, 2011, PROC VLDB ENDOW, V4, P373, DOI 10.14778/1978665.1978669
   Parisi F., 2014, P NATL ACAD SCI
   Platanios E. A., 2014, C UNC ART INT
   Platanios Emmanouil Antonios, 2016, INT C MACH LEARN, P1416
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Tian T., 2015, NEURAL INFORM PROCES
   Wehbe L., 2014, PREDICTING BRAIN ACT
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404042
DA 2019-06-15
ER

PT S
AU Pleiss, G
   Raghavan, M
   Wu, FL
   Kleinberg, J
   Weinberger, KQ
AF Pleiss, Geoff
   Raghavan, Manish
   Wu, Felix
   Kleinberg, Jon
   Weinberger, Kilian Q.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Fairness and Calibration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.
C1 [Pleiss, Geoff; Raghavan, Manish; Wu, Felix; Kleinberg, Jon; Weinberger, Kilian Q.] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Pleiss, G (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
EM geoff@cs.cornell.edu; manish@cs.cornell.edu; fw245@cornell.edu;
   kleinber@cs.cornell.edu; kwq4@cornell.edu
FU National Science Foundation [III-1149882, III-1525919, III-1550179,
   III-1618134, III-1740822]; Office of Naval Research DOD
   [N00014-17-1-2175]; Bill and Melinda Gates Foundation; NSF Graduate
   Research Fellowship [DGE-1650441]; Simons Investigator Award; ARO MURI
   grant; Facebook Faculty Research Grant; Google Research Grant
FX GP, FW, and KQW are supported in part by grants from the National
   Science Foundation (III-1149882, III-1525919, III-1550179, III-1618134,
   and III-1740822), the Office of Naval Research DOD (N00014-17-1-2175),
   and the Bill and Melinda Gates Foundation. MR is supported by an NSF
   Graduate Research Fellowship (DGE-1650441). JK is supported in part by a
   Simons Investigator Award, an ARO MURI grant, a Google Research Grant,
   and a Facebook Faculty Research Grant.
CR Angwin J, 2016, PROPUBLICA
   Barocas S., 2016, CALIFORNIA LAW REV, V104
   Berk R., 2016, CRIMINOLOGY, V41, P6
   Berk R., 2017, ARXIV170309207
   Bolukbasi T, 2016, ADV NEURAL INFORM PR, V29, P4349
   Calders T., 2009, ICDM WORKSH
   Calders T., 2012, KDD
   Chouldechova Alexandra, 2017, ARXIV170300056
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   Crowson CS, 2016, STAT METHODS MED RES, V25, P1692, DOI 10.1177/0962280213497434
   DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720
   DIETERICH W., 2016, TECHNICAL REPORT
   Dwork Cynthia, 2012, INNOVATIONS THEORETI
   Edwards Harrison, 2016, ICLR
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Flores A., 2016, TECHNICAL REPORT
   Goh G., 2016, ADV NEURAL INFORM PR, P2415
   Guo  Chuan, 2017, ICML
   Hardt Moritz, 2016, ADV NEURAL INFORM PR
   Johndrow James E, 2017, ARXIV170304957
   Joseph M., 2016, NIPS
   Kamiran F., 2009, INT C COMP CONTR COM
   Kamishima Toshihiro, 2011, ICDM WORKSH
   Kearns M., 2017, INT C MACH LEARN, P1828
   Kilbertus Niki, 2017, NIPS
   Kleinberg Jon M., 2017, INNOVATIONS THEORETI
   Kusner Matt J, 2017, ARXIV170306856
   Lichman M., 2013, UCI MACHINE LEARNING
   Louizos Christos, 2016, ICLR
   Niculescu-Mizil Alexandru, 2005, ICML
   Platt J, 1999, ADV LARGE MARGIN CLA, V10, P61
   Romei A, 2014, KNOWL ENG REV, V29, P582, DOI 10.1017/S0269888913000039
   White-House, 2016, TECHNICAL REPORT
   Woodworth B., 2017, P 2017 C LEARN THEOR, P1920
   Zadrozny B., 2001, P 18 INT C MACH LEAR, P609
   Zafar M. B., 2017, WORLD WID WEB C
   Zafar Muhammad Bilal, 2015, ARXIV150705259
   Zemel R., 2013, ICML
   Zliobaite I., 2015, ICML WORKSH FAIRN AC
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405074
DA 2019-06-15
ER

PT S
AU Poloczek, M
   Wang, JL
   Frazier, PI
AF Poloczek, Matthias
   Wang, Jialei
   Frazier, Peter I.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Information Source Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID KNOWLEDGE-GRADIENT; PROBABILITY
AB We consider Bayesian methods for multi-information source optimization (MISO), in which we seek to optimize an expensive-to-evaluate black-box objective function while also accessing cheaper but biased and noisy approximations ("information sources"). We present a novel algorithm that outperforms the state of the art for this problem by using a Gaussian process covariance kernel better suited to MISO than those used by previous approaches, and an acquisition function based on a one-step optimality analysis supported by efficient parallelization. We also provide a novel technique to guarantee the asymptotic quality of the solution provided by this algorithm. Experimental evaluations demonstrate that this algorithm consistently finds designs of higher value at less cost than previous approaches.
C1 [Poloczek, Matthias] Univ Arizona, Dept Syst & Ind Engn, Tucson, AZ 85721 USA.
   [Wang, Jialei] IBM Corp, Analyt Off, Armonk, NY 10504 USA.
   [Frazier, Peter I.] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA.
RP Poloczek, M (reprint author), Univ Arizona, Dept Syst & Ind Engn, Tucson, AZ 85721 USA.
EM poloczek@email.arizona.edu; jw865@cornell.edu; pf98@cornell.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER [CMMI-1254298]; NSF [CMMI-1536895, IIS-1247696]; AFOSR
   [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]
FX This work was partially supported by NSF CAREER CMMI-1254298, NSF
   CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR
   FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.
CR Allaire D, 2014, INT J UNCERTAIN QUAN, V4, P1, DOI 10.1615/Int.J.UncertaintyQuantification.2013004121
   Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Bonilla E. V., 2007, NEURAL INFORM PROCES, P153
   Brynjarsdottir J., 2014, INVERSE PROBLEMS, V30
   Cinlar E, 2011, GRAD TEXTS MATH, V261, P1, DOI 10.1007/978-0-387-87859-1
   Forrester AIJ, 2007, P R SOC A, V463, P3251, DOI 10.1098/rspa.2007.1900
   Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314
   Frazier PI, 2008, SIAM J CONTROL OPTIM, V47, P2410, DOI 10.1137/070693424
   Ghosal S, 2006, ANN STAT, V34, P2413, DOI 10.1214/009053606000000795
   Goovaerts P, 1997, GEOSTATISTICS NATURA
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Hong LJ, 2006, OPER RES, V54, P115, DOI 10.1287/opre.1050.0237
   Huang D, 2006, STRUCT MULTIDISCIP O, V32, P369, DOI 10.1007/s00158-005-0587-0
   Kandasamy Kirthevasan, 2016, ADV NEURAL INFORM PR
   Kennedy MC, 2000, BIOMETRIKA, V87, P1, DOI 10.1093/biomet/87.1.1
   Klein A., 2016, CORR
   Lam  R., 2015, 56 AIAA ASCE AHS ASC
   Le Gratiet L, 2015, TECHNOMETRICS, V57, P418, DOI 10.1080/00401706.2014.928233
   LeGratiet L., 2014, INT J UNCERTAINTY QU, V4
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   Picheny V, 2013, TECHNOMETRICS, V55, P2, DOI 10.1080/00401706.2012.707580
   Poloczek M, 2016, WINT SIMUL C PROC, P770, DOI 10.1109/WSC.2016.7822140
   Qu HS, 2015, OPER RES, V63, P931, DOI 10.1287/opre.2015.1395
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275
   Shah  A., 2015, ADV NEURAL INFORM PR, P3330
   Snoek J., 2016, COMMUNICATION
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N, 2009, ARXIV09123995
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Teh Y.-W., 2005, ARTIFICIAL INTELLIGE, V10
   Theano, 2017, THEAN LOG REGR
   Toscano-Palmerin S., 2016, P 12 INT C MONT CARL
   Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2
   WINKLER RL, 1981, MANAGE SCI, V27, P479, DOI 10.1287/mnsc.27.4.479
   Wu Jiajun, 2017, ADV NEURAL INFORM PR
   Xie J., 2012, ASSEMBLE ORDER SIMUL
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404035
DA 2019-06-15
ER

PT S
AU Prasad, A
   Niculescu-Mizil, A
   Ravikumar, P
AF Prasad, Adarsh
   Niculescu-Mizil, Alexandru
   Ravikumar, Pradeep
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On Separability of Loss Functions, and Revisiting Discriminative Vs
   Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We revisit the classical analysis of generative vs discriminative models for general exponential families, and high-dimensional settings. Towards this, we develop novel technical machinery, including a notion of separability of general loss functions, which allow us to provide a general framework to obtain l(infinity )convergence rates for general M-estimators. We use this machinery to analyze l(infinity) and l(2) convergence rates of generative and discriminative models, and provide insights into their nuanced behaviors in high-dimensions. Our results are also applicable to differential parameter estimation, where the quantity of interest is the difference between generative model parameters.
C1 [Prasad, Adarsh; Ravikumar, Pradeep] CMU, Machine Learning Dept, Mt Pleasant, MI 48859 USA.
   [Niculescu-Mizil, Alexandru] NEC Labs Amer, Princeton, NJ USA.
RP Prasad, A (reprint author), CMU, Machine Learning Dept, Mt Pleasant, MI 48859 USA.
EM adarshp@andrew.cmu.edu; pradeepr@cs.cmu.edu
FU ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1447574, DMS-1264033];
   NIH, Joint DMS/NIGMS Initiative [R01 GM117594-01]
FX A.P. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and
   NSF via IIS-1149803, IIS-1447574, DMS-1264033, and NIH via R01
   GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
   Research at the Interface of the Biological and Mathematical Sciences.
CR de la Fuente A, 2010, TRENDS GENET, V26, P326, DOI 10.1016/j.tig.2010.05.001
   Giraud C, 2014, INTRO HIGH DIMENSION, V138
   Li Tianyang, 2015, ADV NEURAL INFORM PR, P1054
   Li Tianyang, 2017, ARTIF INTELL, P1
   Li Yen-Huan, 2015, AISTATS
   Liu S, 2014, NEURAL COMPUT, V26, P1169, DOI 10.1162/NECO_a_00589
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Ng AY, 2002, ADV NEUR IN, V14, P841
   Ortega J., 2000, ITERATIVE SOLUTION N
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   WAINWRIGHT J. M., 2015, HIGH DIMENSION UNPUB
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Zhao Sihai Dave, 2014, BIOMETRIKA
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407014
DA 2019-06-15
ER

PT S
AU Premont-Schwarz, I
   Ilin, A
   Hao, TH
   Rasmus, A
   Boney, R
   Valpola, H
AF Premont-Schwarz, Isabeau
   Ilin, Alexander
   Hao, Tele Hotloo
   Rasmus, Antti
   Boney, Rinu
   Valpola, Harri
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Recurrent Ladder Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a recurrent extension of the Ladder networks [22] whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.
C1 [Premont-Schwarz, Isabeau; Ilin, Alexander; Hao, Tele Hotloo; Rasmus, Antti; Boney, Rinu; Valpola, Harri] Curious AI Co, Helsinki, Finland.
RP Premont-Schwarz, I (reprint author), Curious AI Co, Helsinki, Finland.
EM isabeau@cai.fi; alexilin@cai.fi; hotloo@cai.fi; antti@cai.fi;
   rinu@cai.fi; harri@cai.fi
CR Alain G., 2012, ABS12114246 CORR
   Arponen H., 2017, ARXIV170902797
   Badrinarayanan Vijay, 2015, ARXIV151100561
   Berglund M., 2015, ADV NEURAL INFORM PR
   Bishop C. M., 2006, PATTERN RECOGNITION
   Brodatz P, 1966, TEXTURES PHOTOGRAPHI
   Cho K., 2014, ARXIV14091259
   Cricri F, 2016, ARXIV161201756
   Eyjolfsdottir E., 2016, ARXIV161100094
   Finn C., 2016, ADV NEURAL INFORM PR, V29
   Greff K., 2015, ABS151106418 CORR
   Greff K., 2016, ADV NEURAL INFORM PR, V29
   Greff K., 2017, ICLR WORKSH
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Johnson D. D., 2017, INT C EV BIOL INSP M
   Jozefowicz R., 2015, P 32 INT C MACH LEAR
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Laukien E., 2016, ARXIV160903971
   Lewandowski N. B., 2012, P 29 INT C MACH LEAR, P1159
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Ronneberger O., 2015, INT C MED IM COMP CO
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srivastava N., 2015, INT C MACH LEARN, P843
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR
   Tietz M., 2017, INT C ART NEUR NETW
   Valpola H., 2015, ADV INDEPENDENT COMP
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406009
DA 2019-06-15
ER

PT S
AU Pu, YC
   Wang, WY
   Henao, R
   Chen, LQ
   Gan, Z
   Li, CY
   Carin, L
AF Pu, Yunchen
   Wang, Weiyao
   Henao, Ricardo
   Chen, Liqun
   Gan, Zhe
   Li, Chunyuan
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adversarial Symmetric Variational Autoencoder
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.
C1 [Pu, Yunchen; Wang, Weiyao; Henao, Ricardo; Chen, Liqun; Gan, Zhe; Li, Chunyuan; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RP Pu, YC (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM yp42@duke.edu; ww109@duke.edu; r.henao@duke.edu; lc267@duke.edu;
   zg27@duke.edu; cl319@duke.edu; lcarin@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO; DARPA; DOE; NGA; ONR; NSF
FX This research was supported in part by ARO, DARPA, DOE, NGA, ONR and
   NSF.
CR Arjovsky M, 2017, WASSERSTEIN GAN
   Arjovsky Martin, 2017, ICLR
   Arora S., 2017, GEN EQUILIBRIUM GENE
   Chen L., 2017, SYMMETRIC VARIATIONA
   Chen Xi, 2016, NIPS
   Donahue J., 2017, ICLR
   Dumoulin V., 2017, ICLR
   Gan Z., 2017, NIPS
   Glorot X., 2010, AISTATS
   Goodfellow I., 2014, NIPS
   Hornik K., 1989, NEURAL NETWORKS
   Kim  T., 2017, LEARNING DISCOVER CR
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2016, NIPS
   Kingma Diederik P, 2014, ICLR
   Larsen Anders Boesen Lindbo, 2016, ICML
   Li  C., 2017, TRIPLE GENERATIVE AD
   LI  Chongxuan, 2017, NIPS
   Liu Q., 2016, NIPS
   Makhzani A., 2015, ADVERSARIAL AUTOENCO
   Mescheder L., 2016, ADVERSARIAL VARIATIO
   Oord A. v. d., 2016, ICML
   Pu Y., 2015, ICLR WORKSH
   Pu Y., 2016, NIPS
   Pu Y., 2016, ARTIFICIAL INTELLIGE
   Pu Y., 2017, NIPS
   Radford A., 2016, ICLR
   Reed S., 2016, ICML
   Rezende D. J., 2014, ICML
   Rezende D. J., 2015, ICML
   Salakhutdinov R., 2016, ICLR
   Salimans T., 2017, ICLR
   Salimans T., 2016, NIPS
   Shen D., 2017, DECONVOLUTIONAL LATE
   Srivastava N., 2014, JMLR
   Szegedy C., 2015, CVPR
   Thei L., 2016, ICLR
   Zhang Y., 2017, ICML
   Zhang Y., 2016, NIPS
   Zhang Yizhe, 2017, NIPS
   Zhao J., 2017, ICLR
   Zhu J. Y., 2017, UNPAIRED IMAGE TO IM
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404039
DA 2019-06-15
ER

PT S
AU Pu, YC
   Gan, Z
   Henao, R
   Li, CY
   Han, SB
   Carin, L
AF Pu, Yunchen
   Gan, Zhe
   Henao, Ricardo
   Li, Chunyuan
   Han, Shaobo
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI VAE Learning via Stein Variational Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.
C1 [Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Li, Chunyuan; Han, Shaobo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RP Pu, YC (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM yp42@duke.edu; zg27@duke.edu; r.henao@duke.edu; cl319@duke.edu;
   shaobo.han@duke.edu; lcarin@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO; DARPA; DOE; NGA; ONR; NSF
FX This research was supported in part by ARO, DARPA, DOE, NGA, ONR and
   NSF.
CR Chen L., 2017, SYMMETRIC VARIATIONA
   Feng Y., 2017, UAI
   Gan Zhe, 2015, ICML
   Gregor K., 2015, ICML
   Han J., 2017, UAI
   Han S., 2016, AISTATS
   He K., 2016, CVPR
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2016, NIPS
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, NIPS
   Larochelle H., 2012, NIPS
   Liu Q., 2016, NIPS
   Maas A. L., 2013, ICML
   Miao Yishu, 2016, ICML
   Mnih A., 2016, ICML
   Mnih A., 2014, ICML
   Oord A. v. d., 2016, ICML
   Pu Y., 2015, ICLR WORKSH
   Pu Y., 2016, NIPS
   Pu Y., 2016, ARTIFICIAL INTELLIGE
   Pu Y., 2017, NIPS
   Ranganath R., 2015, AISTATS
   Ranganath R., 2016, ICML
   Rasmus A., 2015, NIPS
   Rezende D. J., 2014, ICML
   Rezende D. J., 2015, ICML
   Russakovsky O., 2014, IJCV
   Salakhutdinov R., 2016, ICLR
   Shen D., 2017, DECONVOLUTIONAL LATE
   Springenberg J. T., 2015, ICLR WORKSH
   Srivastava N., 2014, JMLR
   Vincent Pascal, 2010, JMLR
   Zhang Yizhe, 2017, NIPS
   Zhou M., 2012, AISTATS
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404030
DA 2019-06-15
ER

PT S
AU Qian, C
   Shi, JC
   Yu, Y
   Tang, K
   Zhou, ZH
AF Qian, Chao
   Shi, Jing-Cheng
   Yu, Yang
   Tang, Ke
   Zhou, Zhi-Hua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Subset Selection under Noise
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The problem of selecting the best k-element subset from a universe is involved in many applications. While previous studies assumed a noise-free environment or a noisy monotone submodular objective function, this paper considers a more realistic and general situation where the evaluation of a subset is a noisy monotone function (not necessarily submodular), with both multiplicative and additive noises. To understand the impact of the noise, we firstly show the approximation ratio of the greedy algorithm and POSS, two powerful algorithms for noise-free subset selection, in the noisy environments. We then propose to incorporate a noise-aware strategy into POSS, resulting in the new PONSS algorithm. We prove that PONSS can achieve a better approximation ratio under some assumption such as i.i.d. noise distribution. The empirical results on influence maximization and sparse regression problems show the superior performance of PONSS.
C1 [Qian, Chao; Tang, Ke] USTC, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei, Anhui, Peoples R China.
   [Shi, Jing-Cheng; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Tang, Ke] SUSTech, Shenzhen Key Lab Computat Intelligence, Shenzhen, Peoples R China.
RP Qian, C (reprint author), USTC, Anhui Prov Key Lab Big Data Anal & Applicat, Hefei, Anhui, Peoples R China.
EM chaoqian@ustc.edu.cn; shijc@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn;
   tangk3@sustc.edu.cn; zhouzh@lamda.nju.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU NSFC [61333014, 61603367, 61672478]; YESS [2016QNRC001]; JiangsuSF
   [BK20160066, BK20170013]; Royal Society Newton Advanced Fellowship
   [NA150123]; Collaborative Innovation Center of Novel Software Technology
   and Industrialization
FX The authors would like to thank reviewers for their helpful comments and
   suggestions. C. Qian was supported by NSFC (61603367) and YESS
   (2016QNRC001). Y. Yu was supported by JiangsuSF (BK20160066,
   BK20170013). K. Tang was supported by NSFC (61672478) and Royal Society
   Newton Advanced Fellowship (NA150123). Z.-H. Zhou was supported by NSFC
   (61333014) and Collaborative Innovation Center of Novel Software
   Technology and Industrialization.
CR Bian A. A., 2017, ICML, P498
   Chen W., 2010, P 16 ACM SIGKDD INT, P1029, DOI DOI 10.1145/1835804.1835934
   Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047
   Chen  Y., 2015, P 28 C LEARN THEOR, P338
   Das A., 2011, P 28 INT C MACH LEAR, P1057
   Das A, 2008, ACM S THEORY COMPUT, P45
   Davis G, 1997, CONSTR APPROX, V13, P57, DOI 10.1007/BF02678430
   Diekhoff G., 1992, STAT SOCIAL BEHAV SC
   Elenberg E. R., 2016, ARXIV161200804
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Goldenberg J, 2001, MARKET LETT, V12, P211, DOI 10.1023/A:1011122126881
   Goyal A., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P211, DOI 10.1109/ICDM.2011.132
   Hassidim Avinatan, 2017, COLT, P1069
   Horel T., 2016, NIPS, V29, P3045
   Johnson R. A., 2007, APPL MULTIVARIATE ST
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Miller A, 2002, SUBSET SELECTION REG
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Qian C., 2017, EVOLUTIONARY COMPUTA
   Qian C., 2017, P 26 INT JOINT C ART, P2613
   Qian  C., 2015, P 29 AAAI C ART INT, P2935
   Qian C., 2017, IJCAI, P2606
   Qian  C., 2015, P ADV NEUR INF PROC, P1765
   Qian Chao, 2016, P 25 INT JOINT C ART, P1939
   Singla Adish, 2016, AAAI, P2037
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403061
DA 2019-06-15
ER

PT S
AU Qin, C
   Klabjan, D
   Russo, D
AF Qin, Chao
   Klabjan, Diego
   Russo, Daniel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Improving the Expected Improvement Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID EFFICIENT GLOBAL OPTIMIZATION; CONVERGENCE-RATES
AB The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.
C1 [Qin, Chao; Russo, Daniel] Columbia Business Sch, New York, NY 10027 USA.
   [Klabjan, Diego] Northwestern Univ, Evanston, IL 60208 USA.
RP Qin, C (reprint author), Columbia Business Sch, New York, NY 10027 USA.
EM cqin22@gsb.columbia.edu; d-klabjan@northwestern.edu;
   djr2174@gsb.columbia.edu
RI Jeong, Yongwook/N-7413-2016
CR Audibert J.-Y., 2010, P 23 ANN C LEARN THE, P41
   Bull AD, 2011, J MACH LEARN RES, V12, P2879
   Chen CH, 2000, DISCRETE EVENT DYN S, V10, P251, DOI 10.1023/A:1008349927281
   CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205
   Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255
   Frazier PI, 2008, SIAM J CONTROL OPTIM, V47, P2410, DOI 10.1137/070693424
   Garivier A., 2016, P MACH LEARN RES, P998
   Glynn P., 2004, SIM C 2004 P 2004 WI, V1
   Jamieson K, 2014, INF SCI SYST CISS 20, P1
   Jamieson K. G., 2014, P 27 C LEARN THEOR, V35, P423
   Jennison C., 1982, STAT DECISION THEO 3, V2, P55
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Karnin Z., 2013, P 30 INT C MACH LEAR, V28, P1238
   Kaufmann E., 2013, C LEARN THEOR, P228
   Kaufmann E, 2016, J MACH LEARN RES, V17
   Kaufmann Emilie, 2014, P MACHINE LEARNING R, V35, P461
   Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212
   Mannor S., 2004, J MACHINE LEARNING R, V5, P2004
   Russo D., 2016, C LEARN THEOR, P1417
   Ryzhov IO, 2016, OPER RES, V64, P1515, DOI 10.1287/opre.2016.1494
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Soare M., 2014, ADV NEURAL INFORM PR, P828
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405045
DA 2019-06-15
ER

PT S
AU Qu, Q
   Zhang, YQ
   Eldar, YC
   Wright, J
AF Qu, Qing
   Zhang, Yuqian
   Eldar, Yonina C.
   Wright, John
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convolutional Phase Retrieval
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABLE SIGNAL RECOVERY
AB We study the convolutional phase retrieval problem, which considers recovery of an unknown signal x is an element of C-n from m measurements consisting of the magnitude of its cyclic convolution with a known kernel a of length m. This model is motivated by applications to channel estimation, optics, and underwater acoustic communication, where the signal of interest is acted on by a given channel/filter, and phase information is difficult or impossible to acquire. We show that when a is random and m is sufficiently large, x can be efficiently recovered up to a global phase using a combination of spectral initialization and generalized gradient descent. The main challenge is coping with dependencies in the measurement operator; we overcome this challenge by using ideas from decoupling theory, suprema of chaos processes and the restricted isometry property of random circulant matrices, and recent analysis for alternating minimizing methods.
C1 [Qu, Qing; Zhang, Yuqian; Wright, John] Columbia Univ, New York, NY 10027 USA.
   [Eldar, Yonina C.] Technion, Haifa, Israel.
RP Qu, Q (reprint author), Columbia Univ, New York, NY 10027 USA.
EM qq2105@columbia.edu; yz2409@columbia.edu; yonina@ee.technion.ac.il;
   jw2966@columbia.edu
FU European Unions Horizon 2020 research and innovation program
   [646804-ERCCOGBNYQ]; Israel Science Foundation [335/14]; Microsoft
   graduate research fellowship;  [NSF CCF 1527809];  [NSF IIS 1546411]
FX This work was partially supported by the grants NSF CCF 1527809 and NSF
   IIS 1546411, the grants from the European Unions Horizon 2020 research
   and innovation program under grant agreement No. 646804-ERCCOGBNYQ, and
   the grant from the Israel Science Foundation under grant no. 335/14. QQ
   thanks the generous support of the Microsoft graduate research
   fellowship. We would like to thank Shan Zhong for the helpful discussion
   for real applications and providing the antenna data for experiments,
   and we thank Ju Sun and Han-wen Kuo for helpful discussion and input
   regarding the analysis of this work.
CR Arik SO, 2016, OPT LETT, V41, P4265, DOI 10.1364/OL.41.004265
   Bendory T., 2017, IEEE T INFORM THEORY, VPP, P1
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Candfes Emmanuel J., 2013, SIAM J IMAGING SCI, V6
   Chen Yuxin, 2015, ARXIV150505114
   de la Pena V., 1999, DECOUPLING DEPENDENC
   Eldar Y. C., 2012, COMPRESSED SENSING T
   Foucart S, 2013, MATH INTRO COMPRESSI
   Gagliardi R., 1976, OPTICAL COMMUNICATIO
   Ge R., 2015, P 28 C LEARN THEOR, P797
   GERCHBERG RW, 1972, OPTIK, V35, P237
   Gross David, 2013, ARXIV13102267
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Jaganathan Kishore, 2016, OPTICAL COMPRESSIVE
   Krahmer F., 2014, GAMM MITT, V37, P217
   Krahmer F, 2014, COMMUN PUR APPL MATH, V67, P1877, DOI 10.1002/cpa.21504
   Kreutz-Delgado K., 2009, ARXIV09064835
   KWAPIEN S, 1987, ANN PROBAB, V15, P1062, DOI 10.1214/aop/1176992081
   Mecozzi A, 2016, OPTICA, V3, P1220, DOI 10.1364/OPTICA.3.001220
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   Rauhut H., 2010, THEORET FDN NUMER ME, V9, P1, DOI DOI 10.1515/9783110226157.1
   Shahmansoori  A., 2015, 2015 IEEE GLOB WORKS, P1
   Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673
   Soltanolkotabi M., 2014, THESIS
   Soltanolkotabi Mahdi, 2017, ABS170206175 CORR
   STOJANOVIC M, 1994, IEEE J OCEANIC ENG, V19, P100, DOI 10.1109/48.289455
   Sun J., 2015, ARXIV151006096
   Sun J., 2016, ARXIV160206664
   Sun J., 2015, ARXIV150406785
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Waldspurger Irene, 2016, ARXIV160903088
   Walk P., 2015, ASILOMAR 2015
   Wang G., 2017, IEEE T INFORM THEORY, VPP, P1
   Zhang H., 2016, ARXIV160507719
   Zhang  Yinda, 2017, P IEEE C COMP VIS PA, P7
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406016
DA 2019-06-15
ER

PT S
AU Quadrianto, N
   Sharmanska, V
AF Quadrianto, Novi
   Sharmanska, Viktoriia
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Recycling Privileged Learning and Distribution Matching for Fairness
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.
C1 [Quadrianto, Novi] Univ Sussex, PAL, Brighton, E Sussex, England.
   [Sharmanska, Viktoriia] Imperial Coll London, Dept Comp, London, England.
   [Quadrianto, Novi] Natl Res Univ, Higher Sch Econ, Moscow, Russia.
RP Quadrianto, N (reprint author), Univ Sussex, PAL, Brighton, E Sussex, England.
EM n.quadrianto@sussex.ac.uk; sharmanska.v@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU UK EPSRC [EP/P03442X/1]; Russian Academic Excellence Project '5-100'; IC
   Research Fellowship
FX NQ is supported by the UK EPSRC project EP/P03442X/1 'EthicalML:
   Injecting Ethical and Legal Constraints into Machine Learning Models'
   and the Russian Academic Excellence Project '5-100'. VS is supported by
   the IC Research Fellowship. We thank NVIDIA for GPU donation and Amazon
   for AWS Cloud Credits. We thank Kristian Kersting and Oliver Thomas for
   discussions, Muhammad Bilal Zafar for his implementations of [4] and
   [5], and Sienna Quadrianto for supporting the work.
CR Adler Philip, 2016, ICDM
   Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83
   Collette Y., 2003, MULTIOBJECTIVE OPTIM
   Deb K., 2000, Parallel Problem Solving from Nature PPSN VI. 6th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1917), P849
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Executive Office of the President, 2016, TECHNICAL REPORT
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Feyereisl J., 2014, P NIPS, P208
   Fortin FA, 2012, J MACH LEARN RES, V13, P2171
   Goh G., 2016, ADV NEURAL INFORM PR, P2415
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Hernandez-lobato D., 2014, ADV NEURAL INFORM PR, V27, P837
   Hinton Geoffrey E., 2015, ABS150302531 CORR
   Joseph Matthew, 2016, ADV NEURAL INFORM PR, V29, P325
   Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Kleinberg Jon M., 2016, ABS160905807 CORR
   Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167
   Lipp T, 2016, OPTIM ENG, V17, P263, DOI 10.1007/s11081-015-9294-x
   Lopez-Paz D., 2016, INT C LEARN REPR ICL
   Louizos Christos, 2015, ABS151100830 CORR
   Luong B. T., 2011, P 17 ACM SIGKDD INT, P502, DOI DOI 10.1145/2020408.2020488
   Pan SJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1187
   Pechyony D., 2010, ADV NEURAL INFORM PR, P1894
   Quadrianto N., 2009, ADV NEURAL INFORM PR, P1500
   Romei A, 2014, KNOWL ENG REV, V29, P582, DOI 10.1017/S0269888913000039
   Ruggieri S, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754432
   Russell C, 2017, ADV NEUR IN, V30
   Sharmanska V, 2016, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2016.241
   Sharmanska V, 2016, PROC CVPR IEEE, P3967, DOI 10.1109/CVPR.2016.430
   Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107
   Song L., 2009, P 26 ANN INT C MACH, P961
   Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   The Royal Society Working Group, 2017, TECHNICAL REPORT
   Vapnik V, 2015, J MACH LEARN RES, V16, P2023
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Zafar MB, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1171, DOI 10.1145/3038912.3052660
   Zafar Muhammad Bilal, 2017, P MACHINE LEARNING R, P962
   Zemel R., 2013, JMLR P, P325
   Zhang Xu, 2015, ABS150300591 CORR
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400065
DA 2019-06-15
ER

PT S
AU Rabusseau, G
   Balle, B
   Pineau, J
AF Rabusseau, Guillaume
   Balle, Borja
   Pineau, Joelle
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multitask Spectral Learning of Weighted Automata
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider the problem of estimating multiple related functions computed by weighted automata (WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the novel model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.
C1 [Rabusseau, Guillaume; Pineau, Joelle] McGill Univ, Montreal, PQ, Canada.
   [Balle, Borja] Amazon Res Cambridge, Cambridge, England.
RP Rabusseau, G (reprint author), McGill Univ, Montreal, PQ, Canada.
EM guillaume.rabusseau@mail.mcgill.ca; pigem@amazon.co.uk;
   jpineau@cs.mcgill.ca
RI Jeong, Yongwook/N-7413-2016
FU NSERC; CIFAR; IVADO postdoctoral fellowship
FX G. Rabusseau acknowledges support of an IVADO postdoctoral fellowship.
   B. Balle completed this work while at Lancaster University. We thank
   NSERC and CIFAR for their financial support.
CR Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Bailly Raphael, 2009, P 26 INT C MACH LEAR, P33
   Balle  B., 2014, ICML, P1386
   Balle B, 2014, MACH LEARN, V96, P33, DOI 10.1007/s10994-013-5416-x
   Balle Borja, 2013, THESIS
   Baxter Jonathan, 2000, J ARTIFICAL INTELLIG, V12, P3
   Ben-David S, 2003, LECT NOTES ARTIF INT, V2777, P567, DOI 10.1007/978-3-540-45167-9_41
   Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092
   Carlyl J. W., 1971, Journal of Computer and System Sciences, V5, P26, DOI 10.1016/S0022-0000(71)80005-3
   Caruana R, 1998, LEARNING TO LEARN, P95
   Cohen S.B., 2013, P NAACL HLT, P148
   Denis F, 2008, FUND INFORM, V86, P41
   FLIESS M, 1974, J MATH PURE APPL, V53, P197
   Fox E., 2009, ADV NEURAL INFORM PR, V22, P549
   Girolami Mark A, 2003, NIPS, V16, P9
   Hsu Daniel J., 2009, COLT
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kulesza Alex, 2015, AISTATS
   Li RC, 1998, SIAM J MATRIX ANAL A, V20, P471, DOI 10.1137/S0895479896298506
   Liu P., 2016, P 25 INT JOINT C ART, P2873
   Luong  M.-T., 2015, ARXIV151106114
   Ni K., 2007, P 24 INT C MACH LEAR, P689
   Nivre  J., 2016, UNIVERSAL DEPENDENCI
   Petrov  S., 2011, ARXIV11042086
   Thon M, 2015, J MACH LEARN RES, V16, P103
   Tony Cai  T, 2016, ARXIV160500353
   Verwer S., 2012, 11 INT C GRAMM INF, V21, P243
   Wang B., 2016, AAAI, P2115
   Wedin P.-A., 1972, BIT (Nordisk Tidskrift for Informationsbehandling), V12, P99, DOI 10.1007/BF01932678
   Zwald Laurent, 2006, ADV NEURAL INFORM PR, P1649
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402062
DA 2019-06-15
ER

PT S
AU Racah, E
   Beckham, C
   Maharaj, T
   Kahou, SE
   Prabhat
   Pa, C
AF Racah, Evan
   Beckham, Christopher
   Maharaj, Tegan
   Kahou, Samira Ebrahimi
   Prabhat
   Pa, Christopher
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI ExtremeWeather: A large-scale climate dataset for semi-supervised
   detection, localization, and understanding of extreme weather events
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https: //github.com/eracah/hur-detect.
C1 [Racah, Evan; Beckham, Christopher; Maharaj, Tegan; Pa, Christopher] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Racah, Evan; Prabhat] Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.
   [Beckham, Christopher; Maharaj, Tegan; Pa, Christopher] Ecole Polytech Montreal, Montreal, PQ, Canada.
   [Kahou, Samira Ebrahimi] Microsoft Maluuba, Montreal, PQ, Canada.
RP Racah, E (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.; Racah, E (reprint author), Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.
EM evan.racah@umontreal.ca; christopher.beckham@polymtl.ca;
   tegan.maharaj@polymtl.ca; samira.ebrahimi@microsoft.com;
   Prabhat@lbl.gov; christopher.pal@polymtl.ca
RI Jeong, Yongwook/N-7413-2016
FU Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231];
   Samsung; Google
FX This research used resources of the National Energy Research Scientific
   Computing Center (NERSC), a DOE Office of Science User Facility
   supported by the Office of Science of the U.S. Department of Energy
   under Contract No. DE-AC02-05CH11231. Code relies on open-source deep
   learning frameworks Theano (Bergstra et al.; Team et al., 2016) and
   Lasagne (Team, 2016), whose developers we gratefully acknowledge. We
   thank Samsung and Google for support that helped make this research
   possible. We would also like to thank Yunjie Liu and Michael Wehner for
   providing access to the climate datasets; Alex Lamb and Thorsten Kurth
   for helpful discussions.
CR Ballas Nicolas, 2016, P ICLR
   Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI 10.1109/ICCV.2015.312
   Conley A. J., 2012, DESCRIPTION NCAR COM
   Dettinger MD, 2011, WATER-SUI, V3, P445, DOI 10.3390/w3020445
   Donahue J., 2015, IEEE C COMP VIS PATT
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goyal Raghav, 2017, ARXIV170604261
   Gu C., 2017, ARXIV170508421
   Hannun Awni Y., 2013, ICML WORKSH DEEP LEA
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hosseini-Asl Ehsan, 2016, ALZHEIMERS DIS DIAGN
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kahou SE, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2818346.2830596
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lavers DA, 2012, J GEOPHYS RES-ATMOS, V117, DOI 10.1029/2012JD018027
   Liu  W., 2015, ARXIV151202325
   Liu Y, 2016, APPL DEEP CONVOLUTIO
   Makhzani A, 2015, ABS151105644 CORR
   Misra Ishan, 2015, ABS150505769 CORR
   Monahan AH, 2009, J CLIMATE, V22, P6501, DOI 10.1175/2009JCLI3062.1
   Neu U, 2013, B AM METEOROL SOC, V94, P529, DOI 10.1175/BAMS-D-11-00154.1
   Parkhi O M, 2015, P BR MACH VIS, P6
   Prabhat Oliver Rubel, 2012, ICCS
   Prabhat Surendra Byna, 2015, CAIP
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Redmon  Joseph, 2015, ABS150602640 CORR
   Ren S., 2015, FASTER R CNN REAL TI
   Russakovsky Olga, 2017, IMAGENET LARGE SCALE
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salimans T., 2016, IMPROVED TECHNIQUES
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Shi X., 2015, ADV NEURAL INFORM PR, V9199, P802
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Srivastava Nitish, 2015, ABS150204681 CORR, V2
   Steinhaeuser Karsten, 2011, Advances in Spatial and Temporal Databases. Proceedings 12th International Symposium (SSTD 2011), P39, DOI 10.1007/978-3-642-22922-0_4
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tran D, 2014, LEARNING SPATIOTEMPO
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wehner M, 2015, J CLIMATE, V28, P3905, DOI 10.1175/JCLI-D-14-00311.1
   Whitney William F., 2016, UNDERSTANDING VISUAL
   Xie Jianwen, 2016, SYNTHESIZING DYNAMIC
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Zhang Yuting, 2016, ARXIV160606582V1
   Zhao Junbo, 2015, ARXIV150602351
NR 49
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403046
DA 2019-06-15
ER

PT S
AU Racaniere, S
   Weber, T
   Reichert, DP
   Buesing, L
   Guez, A
   Rezende, D
   Badia, AP
   Vinyals, O
   Heess, N
   Li, YJ
   Pascanu, R
   Battaglia, P
   Hassabis, D
   Silver, D
   Wierstra, D
AF Racaniere, Sebastien
   Weber, Theophane
   Reichert, David P.
   Buesing, Lars
   Guez, Arthur
   Rezende, Danilo
   Badia, Adria Puigdomenech
   Vinyals, Oriol
   Heess, Nicolas
   Li, Yujia
   Pascanu, Razvan
   Battaglia, Peter
   Hassabis, Demis
   Silver, David
   Wierstra, Daan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Imagination-Augmented Agents for Deep Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FUTURE
AB We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.
C1 [Racaniere, Sebastien; Weber, Theophane; Reichert, David P.; Buesing, Lars; Guez, Arthur; Rezende, Danilo; Badia, Adria Puigdomenech; Vinyals, Oriol; Heess, Nicolas; Li, Yujia; Pascanu, Razvan; Battaglia, Peter; Hassabis, Demis; Silver, David; Wierstra, Daan] DeepMind, London, England.
RP Racaniere, S; Weber, T; Reichert, DP (reprint author), DeepMind, London, England.
EM sracaniere@google.com; theophane@google.com; reichert@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abbeel P., 2005, P 22 INT C MACH LEAR, P1
   Baird III Leemon C, 1993, WLTR931146
   Bansal S., 2017, ARXIV170309260
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Chiappa Silvia, 2017, 5 INT C LEARN REPR
   Childs BE, 2008, 2008 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND GAMES, P389, DOI 10.1109/CIG.2008.5035667
   Christiano  P., 2016, ARXIV161003518
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Cutler M, 2015, IEEE T ROBOT, V31, P655, DOI 10.1109/TRO.2015.2419431
   Deisenroth M, 2011, P 28 INT C MACH LEAR, P465
   Dickinson A., 2002, ROLE LEARNING OPERAT
   Finn Chelsea, 2017, IEEE INT C ROB AUT I
   Gelly S, 2007, P 24 INT C MACH LEAR, P273, DOI DOI 10.1145/1273496.1273531
   Graves A., 2016, ARXIV160308983
   Gu  S., 2016, INT C MACH LEARN, P2829
   Hamrick Jessica B., 2017, P 5 INT C LEARN REPR
   Hassabis D, 2007, J NEUROSCI, V27, P14365, DOI 10.1523/JNEUROSCI.4549-07.2007
   Hassabis D, 2007, P NATL ACAD SCI USA, V104, P1726, DOI 10.1073/pnas.0610561104
   Henaff M, 2017, ARXIV170507177
   Jaderberg M., 2016, ARXIV161105397
   Junhyuk Oh, 2017, ARXIV170703497
   Kansky Ken, 2017, INT C MACH LEARN 201
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Lake B. M., 2016, ARXIV160400289
   Legg S, 2007, MIND MACH, V17, P391, DOI 10.1007/s11023-007-9079-x
   Leibfried Felix, 2016, ABS161107078 CORR
   Lenz  I., 2015, ROBOTICS SCI SYSTEMS
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Lillicrap T. P., 2016, ICLR
   Liu Yuxuan, 2017, ARXIV170703374
   Marco Alonso, 2017, ARXIV170301250
   Mirowski P., 2016, ARXIV161103673
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Murase Y., 1996, PRICAI'96: Topics in Artificial Intelligence. 4th Pacific Rim International Conference on Artificial Intelligence. Proceedings, P592
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Pascanu Razvan, 2017, LEARNING MODEL BASED
   Peng J., 1993, ADAPT BEHAV, V1, P437, DOI DOI 10.1177/105971239300100403
   Pfeiffer BE, 2013, NATURE, V497, P74, DOI 10.1038/nature12112
   Rosin C. D., 2011, IJCAI, P649
   Schacter DL, 2012, NEURON, V76, P677, DOI 10.1016/j.neuron.2012.11.001
   Schmidhuber J., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P253, DOI 10.1109/IJCNN.1990.137723
   Schmidhuber J., 2015, ARXIV151109249
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Schulman  J., 2015, ADV NEURAL INFORM PR, P3528
   Silver D., 2016, ARXIV161208810
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216
   Talvitie E., 2014, UAI, P780
   Talvitie Erik, 2015, AAAI, P2986
   Tamar A., 2016, ADV NEURAL INFORM PR, V29, P2154
   Taylor J, 2011, GAMEON-NA 2011: 6TH INTERNATIONAL NORTH- AMERICAN CONFERENCE ON INTELLIGENT GAMES AND AND SIMULATION / 3RD INTERNATIONAL NORTH AMERICAN SIMULATION TECHNOLOGY CONFERENCE, NASTEC 2011, P5
   Taylor ME, 2009, J MACH LEARN RES, V10, P1633
   Tesauro G, 1996, ADV NEURAL INFORM PR, P1068
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626
   Tzeng  E., 2015, ARXIV151107111
   Venkatraman Arun, 2016, INT S EXP ROB, P703
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
NR 59
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405075
DA 2019-06-15
ER

PT S
AU Raghu, M
   Gilmer, J
   Yosinski, J
   Sohl-Dickstein, J
AF Raghu, Maithra
   Gilmer, Justin
   Yosinski, Jason
   Sohl-Dickstein, Jascha
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning
   Dynamics and Interpretability
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.
C1 [Raghu, Maithra; Gilmer, Justin; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA 94043 USA.
   [Raghu, Maithra] Cornell Univ, Ithaca, NY 14853 USA.
   [Yosinski, Jason] Uber AI Labs, San Francisco, CA USA.
RP Raghu, M (reprint author), Google Brain, Mountain View, CA 94043 USA.; Raghu, M (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM maithrar@gmail.com; gilmer@google.com; yosinski@uber.com;
   jaschasd@google.com
CR Alain  G., 2016, ARXIV161001644
   Eigen D., 2013, ARXIV13121847
   Faruqui Manaal, 2014, IMPROVING VECTOR SPA
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He K., 2015, CORR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Horn R. A., 1985, MATRIX ANAL
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lenc K, 2015, PROC CVPR IEEE, P991, DOI 10.1109/CVPR.2015.7298701
   Li Y., 2015, FEATURE EXTRACTION M, P196
   Li Y., 2016, INT C LEARN REPR ICL
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Montavon G, 2011, J MACH LEARN RES, V12, P2563
   Simonyan K, 2013, ARXIV13126034
   Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042
   Szegedy C, 2013, ARXIV13126199
   Wu Y., 2016, ARXIV160908144
   Yosinski J., 2015, DEEP LEARN WORKSH IN
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhou B., 2014, CORR, V1412, P6856
NR 21
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406015
DA 2019-06-15
ER

PT S
AU Raghunathan, A
   Jain, P
   Krishnaswamy, R
AF Raghunathan, Aditi
   Jain, Prateek
   Krishnaswamy, Ravishankar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Mixture of Gaussians with Streaming Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of N points in d dimensions generated by an unknown mixture of k spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians accurately if they are sufficiently separated. Assuming each pair of centers are C sigma distant with C = Omega((k log k)(1/4)sigma) and where sigma(2) is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians [18]. For finite samples, we show that a bias term based on the initial estimate decreases at O(1/poly(N)) rate while variance decreases at nearly optimal rate of sigma(2) d/N. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal d . k while space complexity of our algorithm is O(dk log k).
   In addition to the bias and variance terms which tend to 0, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an approximation error that cannot be avoided. However, by using a streaming version of the classical (soft-thresholding-based) EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to 0 for N -> infinity.
C1 [Raghunathan, Aditi] Stanford Univ, Stanford, CA 94305 USA.
   [Jain, Prateek; Krishnaswamy, Ravishankar] Microsoft Res, Bengaluru, Karnataka, India.
RP Raghunathan, A (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM aditir@stanford.edu; prajain@microsoft.com; rakri@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Anandkumar A, 2015, LECT NOTES ARTIF INT, V9355, P19, DOI 10.1007/978-3-319-24486-0_2
   Ashtiani Hassan, 2017, ARXIV170601596
   Balakrishnan Sivaraman, 2014, ANN STATS, V45, P77
   Balcan MF, 2013, J ACM, V60, DOI 10.1145/2450142.2450144
   Dasgupta A, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1036
   Daskalakis C., 2016, ARXIV160900368
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   Jain P., 2016, 29 ANN C LEARN THEOR, P1147
   Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35
   Mitliagkas I., 2013, P ADV NEUR INF PROC, V26, P2886
   Raghunathan Aditi, 2017, CORR
   Shamir Ohad, 2011, ARXIV11102392
   Tang C., 2016, P 19 INT C ART INT S, P1280
   Tang Cheng, 2017, P AISTATS
   Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jess.2003.11.008
   Xu J., 2016, ADV NEURAL INFORM PR, P2676
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406065
DA 2019-06-15
ER

PT S
AU Rahmanian, H
   Warmuth, MK
AF Rahmanian, Holakou
   Warmuth, Manfred K.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Dynamic Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS
AB We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find a good binary search tree in a changing environment. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. The learner is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hindsight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a general methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge [16] and Component Hedge [25] to a significantly wider class of combinatorial objects than was possible before.
C1 [Rahmanian, Holakou; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95060 USA.
RP Rahmanian, H (reprint author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95060 USA.
EM holakou@ucsc.edu; manfred@ucsc.edu
FU National Science Foundation (NSF) [IIS-1619271]
FX We thank S.V.N. Vishwanathan for initiating and guiding much of this
   research. We also thank Michael Collins for helpful discussions and
   pointers to the literature on hypergraphs and PCFGs. This research was
   supported by the National Science Foundation (NSF grant IIS-1619271).
CR Adamskiy Dmitry, 2012, ADV NEURAL INFORM PR, P135
   Ailon  N., 2014, P 17 INT C ART INT S, P29
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Audibert Jean-Yves, 2011, COLT, V19, P107
   Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016
   Bauschke H. H., 1997, J CONVEX ANAL, V4, P27
   Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Cormen T H., 2009, INTRO ALGORITHMS
   Cortes Corinna, 2015, C LEARN THEOR, P424
   Dani V., 2008, ADV NEURAL INFORM PR, V20, P345
   DEUTSCH F, 1995, NATO ADV SCI INST SE, V454, P87
   Dick T., 2014, P 31 INT C INT C MAC, P512
   Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gupta  Swati, 2016, ARXIV160300522
   Gyorgy A, 2007, J MACH LEARN RES, V8, P2369
   Helmbold DP, 2009, J MACH LEARN RES, V10, P1705
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Kaibel V., 2011, ARXIV11041023
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Kleinberg J, 2006, ALGORITHM DESIGN
   Knight PA, 2008, SIAM J MATRIX ANAL A, V30, P261, DOI 10.1137/060659624
   Koolen Wouter M, 2010, C LEARN THEOR, P239
   Kuzmin D, 2005, LECT NOTES COMPUT SC, V3559, P684, DOI 10.1007/11503415_46
   Kveton B, 2015, P 18 INT C ART INT S, P535
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Loday Jean-Louis, 2005, P 2005 ACAD COLL SER
   MARTIN RK, 1990, OPER RES, V38, P127, DOI 10.1287/opre.38.1.127
   Mohri M, 2009, MONOGR THEOR COMPUT, P213, DOI 10.1007/978-3-642-01492-5_6
   Rahmanian Holakou, 2017, ARXIV160905374
   Rajkumar Arun, 2014, ADV NEURAL INFORM PR, P3482
   Suehiro Daiki, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P260, DOI 10.1007/978-3-642-34106-9_22
   Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328
   Warmuth MK, 2008, J MACH LEARN RES, V9, P2287
   Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402085
DA 2019-06-15
ER

PT S
AU Rajeswaran, A
   Lowrey, K
   Todorov, E
   Kakade, S
AF Rajeswaran, Aravind
   Lowrey, Kendall
   Todorov, Emanuel
   Kakade, Sham
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Towards Generalization and Simplicity in Continuous Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of widely studied continuous control tasks, including the gym-v1 benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, the standard training and testing scenarios for these tasks are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution induces more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.
C1 [Rajeswaran, Aravind; Lowrey, Kendall; Todorov, Emanuel; Kakade, Sham] Univ Washington, Seattle, WA 98195 USA.
RP Rajeswaran, A (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM aravraj@cs.washington.edu; klowrey@cs.washington.edu;
   todorov@cs.washington.edu; sham@cs.washington.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF
FX This work was supported in part by the NSF. The authors would like to
   thank Vikash Kumar, Igor Mordatch, John Schulman, and Sergey Levine for
   valuable comments.
CR Al Borno M., 2013, IEEE T VISUALIZATION
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   [Anonymous], 1995, ICML
   Bertsekas D. P., 2008, APPROXIMATE DYNAMIC
   Brockman G., 2016, OPENAI GYM
   Duan Y., 2016, ICML
   Erez T, 2013, IEEE-RAS INT C HUMAN, P292, DOI 10.1109/HUMANOIDS.2013.7029990
   Erez Tom, 2011, RSS
   Geramifard Alborz, 2013, Foundations and Trends in Machine Learning, V6, P375, DOI 10.1561/2200000042
   Gu S., 2017, ICLR
   Heess Nicolas, 2015, NIPS
   Kakade S., 2001, NIPS
   Kumar V., 2016, ICRA
   Kumar V., 2016, ARXIV E PRINTS
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2015, ARXIV E PRINTS
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mordatch I., 2015, IROS
   Mordatch I., 2012, ACM SIGGRAPH
   Peters J., 2007, THESIS
   Pinto Lerrel, 2016, ICRA
   Rahimi A., 2007, NIPS
   Rajeswaran A., 2017, ICLR
   Sadeghi Fereshteh, 2016, ARXIV E PRINTS
   Schulman J., 2016, ICLR
   Schulman John, 2015, ICML
   Si J, 2004, HDB LEARNING APPROXI, V2
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Tassa Y., 2012, INT C INT ROB SYST
   Tobin Josh, 2017, ARXIV E PRINTS
   Todorov E., 2012, INT C INT ROB SYST
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406060
DA 2019-06-15
ER

PT S
AU Ramdas, A
   Yang, F
   Wainwright, MJ
   Jordan, MI
AF Ramdas, Aaditya
   Yang, Fanny
   Wainwright, Martin J.
   Jordan, Michael, I
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online control of the false discovery rate with decaying memory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In the online multiple testing problem, p-values corresponding to different null hypotheses are observed one by one, and the decision of whether or not to reject the current hypothesis must be made immediately, after which the next p-value is observed. Alpha-investing algorithms to control the false discovery rate (FDR), formulated by Foster and Stine, have been generalized and applied to many settings, including quality-preserving databases in science and multiple A/B or multi-armed bandit tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways: (a) we show how to uniformly improve the power of the entire class of monotone GAI procedures by awarding more alpha-wealth for each rejection, giving a win-win resolution to a recent dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more important than others, (d) we define a new quantity called the decaying memory false discovery rate (mem-FDR) that may be more meaningful for truly temporal applications, and which alleviates problems that we describe and refer to as "piggybacking" and "alpha-death." Our GAI++ algorithms incorporate all four generalizations simultaneously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity. Finally, we also describe a simple method to derive new online FDR rules based on an estimated false discovery proportion.
C1 [Ramdas, Aaditya; Yang, Fanny; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Ramdas, A (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM aramdas@berkeley.edu; fanny-yang@berkeley.edu; wainwrig@berkeley.edu;
   jordan@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU Army Research Office [W911NF-17-1-0304]; National Science Foundation
   grant [NSF-DMS-1612948]
FX We thank A. Javanmard, R. F. Barber, K. Johnson, E. Katsevich, W.
   Fithian and L. Lei for related discussions, and A. Javanmard for sharing
   code to reproduce experiments in Javanmard and Montanari [9]. This
   material is based upon work supported in part by the Army Research
   Office under grant number W911NF-17-1-0304, and National Science
   Foundation grant NSF-DMS-1612948.
CR Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048
   Benjamini Y, 1997, SCAND J STAT, V24, P407, DOI 10.1111/1467-9469.00072
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Blanchard G, 2008, ELECTRON J STAT, V2, P963, DOI 10.1214/08-EJS180
   Foster DP, 2008, J R STAT SOC B, V70, P429, DOI 10.1111/j.1467-9868.2007.00643.x
   Genovese CR, 2006, BIOMETRIKA, V93, P509, DOI 10.1093/biomet/93.3.509
   Heesen Philipp, 2014, ARXIV14106296
   Javanmard A., 2017, ANN STAT
   Javanmard A., 2015, ARXIV150206197
   Li Ang, 2016, ARXIV160607926
   Ramdas A., 2017, ARXIV170306222
   Tukey JW, 1953, PROBLEM MULTIPLE COM
   Yang Fanny, 2017, ADV NEURAL INFORM PR
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405071
DA 2019-06-15
ER

PT S
AU Rantanen, K
   Hyttinen, A
   Jarvisalo, M
AF Rantanen, Kari
   Hyttinen, Antti
   Jarvisalo, Matti
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Chordal Markov Networks via Branch and Bound
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID BAYESIAN GRAPHICAL MODELS; CHAIN
AB We present a new algorithmic approach for the task of finding a chordal Markov network structure that maximizes a given scoring function. The algorithm is based on branch and bound and integrates dynamic programming for both domain pruning and for obtaining strong bounds for search-space pruning. Empirically, we show that the approach dominates in terms of running times a recent integer programming approach (and thereby also a recent constraint optimization approach) for the problem. Furthermore, our algorithm scales at times further with respect to the number of variables than a state-of-the-art dynamic programming algorithm for the problem, with the potential of reaching 20 variables and at the same time circumventing the tight exponential lower bounds on memory consumption of the pure dynamic programming approach.
C1 [Rantanen, Kari; Hyttinen, Antti; Jarvisalo, Matti] Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.
RP Rantanen, K (reprint author), Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.
RI Jeong, Yongwook/N-7413-2016
FU Academy of Finland [251170 COIN]; Centre of Excellence in Computational
   Inference Research [276412, 284591, 295673, 312662]; Research Funds of
   the University of Helsinki
FX The authors gratefully acknowledge financial support from the Academy of
   Finland under grants 251170 COIN Centre of Excellence in Computational
   Inference Research, 276412, 284591, 295673, and 312662; and the Research
   Funds of the University of Helsinki.
CR Abel HJ, 2011, STAT APPL GENET MOL, V10, DOI 10.2202/1544-6115.1615
   Bartlett M, 2017, ARTIF INTELL, V244, P258, DOI 10.1016/j.artint.2015.03.003
   Chickering D. M., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P87
   Chickering DM, 2002, J MACH LEARN RES, V2, P445, DOI 10.1162/153244302760200696
   Corander J., 2013, ADV NEURAL INFORM PR, P1349
   DAWID AP, 1993, ANN STAT, V21, P1272, DOI 10.1214/aos/1176349260
   de Campos CP, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P431
   de Campos CP, 2011, J MACH LEARN RES, V12, P663
   Dellaportas P, 1999, BIOMETRIKA, V86, P615, DOI 10.1093/biomet/86.3.615
   Giudici P, 1999, BIOMETRIKA, V86, P785, DOI 10.1093/biomet/86.4.785
   Green PJ, 2013, BIOMETRIKA, V100, P91, DOI 10.1093/biomet/ass052
   Janhunen T, 2017, STAT COMPUT, V27, P115, DOI 10.1007/s11222-015-9611-4
   Kangas K., 2014, P NIPS, V27, P2357
   Kangas Kustaa, 2015, P UAI, P415
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kumar K. S. Sesh, 2013, JMLR WORKSHOP C PROC, V28, P525
   Lauritzen SL, 1990, READINGS UNCERTAIN R, P415
   Letac G, 2007, ANN STAT, V35, P1278, DOI 10.1214/009053606000001235
   MADIGAN D, 1995, INT STAT REV, V63, P215, DOI 10.2307/1403615
   Malone B, 2014, LECT NOTES COMPUT SC, V8323, P111, DOI 10.1007/978-3-319-04534-4_8
   Rantanen Kari, 2017, THESIS
   Silander T., 2006, P 22 ANN C UNC ART I, P445
   Srebro N, 2003, ARTIF INTELL, V143, P123, DOI 10.1016/S0004-3702(02)00360-0
   Studeny M, 2017, INT J APPROX REASON, V88, P259, DOI 10.1016/j.ijar.2017.06.001
   Suzuki J., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P462
   Suzuki Joe, 2017, P UAI
   Tarantola C, 2004, STAT MODEL, V4, P39, DOI 10.1191/1471082X04st063oa
   Tian J., 2000, P 16 C UNC ART INT, P580
   van Beek P, 2015, LECT NOTES COMPUT SC, V9255, P429, DOI 10.1007/978-3-319-23219-5_31
   Verzilli CJ, 2006, AM J HUM GENET, V79, P100, DOI 10.1086/505313
   Wiesel A, 2010, IEEE T SIGNAL PROCES, V58, P1482, DOI 10.1109/TSP.2009.2037350
   Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401085
DA 2019-06-15
ER

PT S
AU Rashtchian, C
   Makarychev, K
   Racz, M
   Ang, SD
   Jevdjic, D
   Yekhanin, S
   Ceze, L
   Strauss, K
AF Rashtchian, Cyrus
   Makarychev, Konstantin
   Racz, Miklos
   Ang, Siena Dumas
   Jevdjic, Djordje
   Yekhanin, Sergey
   Ceze, Luis
   Strauss, Karin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Clustering Billions of Reads for DNA Data Storage
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SIMILARITY; ROBUST
AB Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.
C1 [Rashtchian, Cyrus; Makarychev, Konstantin; Racz, Miklos; Ang, Siena Dumas; Jevdjic, Djordje; Yekhanin, Sergey; Ceze, Luis; Strauss, Karin] Microsoft Res, Redmond, WA 98052 USA.
   [Rashtchian, Cyrus; Ceze, Luis] Univ Washington, CSE, Seattle, WA 98195 USA.
   [Makarychev, Konstantin] Northwestern Univ, EECS, Evanston, IL 60208 USA.
   [Racz, Miklos] Princeton Univ, ORFE, Princeton, NJ 08544 USA.
RP Rashtchian, C (reprint author), Microsoft Res, Redmond, WA 98052 USA.; Rashtchian, C (reprint author), Univ Washington, CSE, Seattle, WA 98195 USA.
RI Jeong, Yongwook/N-7413-2016
CR Abboud A., 2016, STOC
   Ackerman M., 2014, ADV NEURAL INFORM PR, P307
   Ackerman M., 2013, AISTATS
   Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513
   Andoni A., SIAM J COMPUT, V39
   Andoni A, 2012, ACM T ALGORITHMS, V8, DOI 10.1145/2344422.2344434
   Backurs A., 2015, STOC
   Balcan MF, 2014, J MACH LEARN RES, V15, P3831
   Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95
   Batu T., 2003, STOC
   Betancourt B., 2016, NIPS
   Bornholt J., 2016, ASPLOS
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7
   Buhler J, 2001, BIOINFORMATICS, V17, P419, DOI 10.1093/bioinformatics/17.5.419
   Chakraborty Diptarka, 2016, STOC
   CHARIKAR M., 2006, THEORY COMPUTING, V2, P207
   Chawla S., 2015, STOC
   Chen Jiecao, 2016, 29 ADV NEURAL INFORM, P3720
   Christen  P., 2012, DATA MATCHING CONCEP
   Deng D, 2014, PROC INT CONF DATA, P340, DOI 10.1109/ICDE.2014.6816663
   Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274
   Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581
   Erlich Y, 2017, SCIENCE, V355, P950, DOI 10.1126/science.aaj2038
   Ganguly S, 2016, IEEE INT SYMP INFO, P265, DOI 10.1109/ISIT.2016.7541302
   Goldman N, 2013, NATURE, V494, P77, DOI 10.1038/nature11875
   Gollapudi S., 2006, CIKM
   Gravano L., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P491
   Guha S., 2017, ARXIV170301539
   Hanada H., 2017, ARXIV170106134
   Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI [DOI 10.4086/T0C.2012.V008A014, 10.4086/toc.2012.v008a014]
   Hassanzadeh O., 2009, PVLDB, V2, P1282, DOI DOI 10.14778/1687627.1687771
   Hennig C., 2015, HDB CLUSTER ANAL
   Jiang Y., 2013, JOINT EDBT ICDT WORK
   Jiang Y, 2014, PROC VLDB ENDOW, V7, P625, DOI 10.14778/2732296.2732299
   Johnson J., 2017, ARXIV170208734
   Kobren A., 2017, KDD
   Krauthgamer R, 2009, SIAM J COMPUT, V38, P2487, DOI 10.1137/060660126
   Li H, 2009, BIOINFORMATICS, V25, P1754, DOI 10.1093/bioinformatics/btp324
   Li P., 2012, NIPS
   Li P., 2010, P 19 INT C WORLD WID, P671
   Malkomes G., 2015, NIPS
   Matousek J., 2002, LECT DISCRETE GEOMET, V212
   Meila M, 2001, MACH LEARN, V42, P9, DOI 10.1023/A:1007648401407
   Organick L., 2017, BIORXIV
   Ostrovsky R, 2007, J ACM, V54, DOI 10.1145/1284320.1284322
   Pan X., 2015, ADV NEURAL INFORM PR, P82
   Rasheed Z., 2012, P 2012 SIAM INT C DA, P1023
   Sundaram N, 2013, PROC VLDB ENDOW, V6, P1930, DOI 10.14778/2556549.2556574
   UKKONEN E, 1992, THEOR COMPUT SCI, V92, P191, DOI 10.1016/0304-3975(92)90143-4
   Yan CR, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172526
   Yazdi S. H. T., 2016, BIORXIV
   Yu MH, 2016, FRONT COMPUT SCI-CHI, V10, P399, DOI 10.1007/s11704-015-5900-5
   Yuan PS, 2014, LECT NOTES COMPUT SC, V8505, P217, DOI 10.1007/978-3-662-43984-5_16
   Zadeh RB, 2013, J MACH LEARN RES, V14, P1605
   Zhang H., 2017, KDD
   Zorita E. V., 2015, BIOINFORMATICS
NR 57
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403042
DA 2019-06-15
ER

PT S
AU Rebeschini, P
   Tatikonda, S
AF Rebeschini, Patrick
   Tatikonda, Sekhar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Accelerated consensus via Min-Sum Splitting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DISTRIBUTED OPTIMIZATION; ALGORITHMS; CONVERGENCE; WALK
AB We apply the Min-Sum message-passing protocol to solve the consensus problem in distributed optimization. We show that while the ordinary Min-Sum algorithm does not converge, a modified version of it known as Splitting yields convergence to the problem solution. We prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods. The acceleration scheme embodied by Min-Sum Splitting for the consensus problem bears similarities with lifted Markov chains techniques and with multi-step first order methods in convex optimization.
C1 [Rebeschini, Patrick] Univ Oxford, Dept Stat, Oxford, England.
   [Tatikonda, Sekhar] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.
RP Rebeschini, P (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM patrick.rebeschini@stats.ox.ac.uk; sekhar.tatikonda@yale.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [EECS-1609484]
FX This work was partially supported by the NSF under Grant EECS-1609484.
CR Aldous David, 2002, UNFINISHED MONOGRAPH
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516
   Cao  M., 2006, P 44 ANN ALL C COMM, P952
   Chen JS, 2012, IEEE T SIGNAL PROCES, V60, P4289, DOI 10.1109/TSP.2012.2198470
   Diaconis P, 2000, ANN APPL PROBAB, V10, P726
   DIACONIS P, 1994, GEOM FUNCT ANAL, V4, P1, DOI 10.1007/BF01898359
   Dimakis AG, 2010, P IEEE, V98, P1847, DOI 10.1109/JPROC.2010.2052531
   Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027
   Feng Chen, 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P275
   Forero PA, 2010, J MACH LEARN RES, V11, P1663
   Franca G, 2017, IEEE SIGNAL PROC LET, V24, P294, DOI 10.1109/LSP.2017.2654860
   Ghadimi E, 2013, IEEE T SIGNAL PROCES, V61, P5417, DOI 10.1109/TSP.2013.2278149
   Ghosh B., 1996, SPAA '96. 8th Annual ACM Symposium on Parallel Algorithms and Architectures, P72
   Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712
   Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X
   Jung K, 2010, IEEE T INFORM THEORY, V56, P634, DOI 10.1109/TIT.2009.2034777
   Kar S, 2008, IEEE T SIGNAL PROCES, V56, P2609, DOI 10.1109/TSP.2008.923536
   Lesser V, 2003, MU S ART SOC SIM ORG, V9, P1
   Li D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.985674
   Li WJ, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P2881, DOI 10.1109/ISIT.2007.4557655
   Li WJ, 2010, IEEE T INFORM THEORY, V56, P6208, DOI 10.1109/TIT.2010.2081030
   Liu J, 2013, AUTOMATICA, V49, P873, DOI 10.1016/j.automatica.2013.01.001
   Malioutov DM, 2006, J MACH LEARN RES, V7, P2031
   Mateos G, 2010, IEEE T SIGNAL PROCES, V58, P5262, DOI 10.1109/TSP.2010.2055862
   Moallemi CC, 2006, IEEE T INFORM THEORY, V52, P4753, DOI 10.1109/TIT.2006.883539
   Moallemi CC, 2010, IEEE T INFORM THEORY, V56, P2041, DOI 10.1109/TIT.2010.2040863
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Olshevsky A., 2014, 14114186 ARXIV
   Predd JB, 2009, IEEE T INFORM THEORY, V55, P1856, DOI 10.1109/TIT.2009.2012992
   Rabbat MG, 2005, 2005 IEEE 6th Workshop on Signal Processing Advances in Wireless Communications, P1088
   Ram SS, 2010, J OPTIMIZ THEORY APP, V147, P516, DOI 10.1007/s10957-010-9737-7
   Roch S, 2005, ELECTRON COMMUN PROB, V10, P282, DOI 10.1214/ECP.v10-1169
   Ruozzi N, 2013, J MACH LEARN RES, V14, P2287
   Ruozzi N, 2013, IEEE T INFORM THEORY, V59, P5860, DOI 10.1109/TIT.2013.2259576
   Scaman K., 2017, P 34 INT C MACH LEAR, P3027
   Shah D, 2008, FOUND TRENDS NETW, V3, P1, DOI 10.1561/1300000014
   Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X
   Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432
   Tsitsiklis J. N., 1984, THESIS
   TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412
   Varga R. S., 1961, NUMER MATH, V3, P147
   Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904
   Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022
   Young D. M., 1972, Journal of Approximation Theory, V5, P137, DOI 10.1016/0021-9045(72)90036-6
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401040
DA 2019-06-15
ER

PT S
AU Rebuffi, SA
   Bilen, H
   Vedaldi, A
AF Rebuffi, Sylvestre-Alvise
   Bilen, Hakan
   Vedaldi, Andrea
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning multiple visual domains with residual adapters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to perform well uniformly.
C1 [Rebuffi, Sylvestre-Alvise; Bilen, Hakan; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England.
   [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Rebuffi, SA (reprint author), Univ Oxford, Visual Geometry Grp, Oxford, England.
EM srebuffi@robots.ox.ac.uk; hbilen@robots.ox.ac.uk;
   vedaldi@robots.ox.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Mathworks/DTA [DFR02620]; ERC [677195-IDIU]
FX This work acknowledges the support of Mathworks/DTA DFR02620 and ERC
   677195-IDIU.
CR Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Bertinetto L., 2016, ADV NEURAL INFORM PR, P523
   Bilen H., 2016, P NIPS
   Bilen H., 2016, P CVPR
   Bilen H., 2017, ARXIV170107275
   Caruana R., 1997, MACHINE LEARNING, V28
   Cimpoi M., 2014, P CVPR
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Daume III H., 2007, P 45 ANN M ASS COMP, P256
   Evgeniou T, 2004, P 10 ACM SIGKDD INT, P109, DOI [10.1145/1014052.1014067, DOI 10.1145/1014052.1014067]
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Ganin Y., 2015, P ICML
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081
   Ioffe S., 2015, CORR
   Jia Xu, 2016, ADV NEURAL INFORM PR, V29, P667
   Kirkpatrick J., 2017, NATL ACAD SCI
   Kokkinos I., 2017, P CVPR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   LI ZZ, 2016, P ECCV, V9908, P614, DOI DOI 10.1007/978-3-319-46493-0_37
   Liu X., 2015, HLT NAACL, P912, DOI DOI 10.3115/V1/N15-1092
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Maji S., 2013, TECHNICAL REPORT
   Mitchell T., 2010, NEVER ENDING LEARNIN
   Munder S, 2006, IEEE T PATTERN ANAL, V28, P1863, DOI 10.1109/TPAMI.2006.217
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Nilsback  M.-E., 2008, ICCVGIP
   Razavian A. S., 2014, CVPR DEEPVISION WORK
   Rebuffi S. -A., 2017, P CVPR
   Rosenfeld A., 2017, ARXIV170504228
   Russakovsky  O., 2014, IMAGENET LARGE SCALE
   Rusu A. A., 2016, ARXIV160604671
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Soomro K., 2012, ARXIV12120402
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Terekhov AV, 2015, LECT NOTES ARTIF INT, V9222, P268, DOI 10.1007/978-3-319-22979-9_27
   Thrun S, 1998, LEARNING TO LEARN, P181
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Zagoruyko S, 2016, ARXIV160507146
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang Z., 2014, P ECCV
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400049
DA 2019-06-15
ER

PT S
AU Remes, S
   Heinonen, M
   Kaski, S
AF Remes, Sami
   Heinonen, Markus
   Kaski, Samuel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Non-Stationary Spectral Kernels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose non-stationary spectral kernels for Gaussian process regression by modelling the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially longrange, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics.
C1 [Remes, Sami; Heinonen, Markus; Kaski, Samuel] Aalto Univ, HIIT, Dept Comp Sci, Espoo, Finland.
RP Remes, S (reprint author), Aalto Univ, HIIT, Dept Comp Sci, Espoo, Finland.
EM sami.remes@aalto.fi; markus.o.heinonen@aalto.fi; samuel.kaski@aalto.fi
RI Jeong, Yongwook/N-7413-2016; Kaski, Samuel/B-6684-2008
OI Kaski, Samuel/0000-0003-1925-9154
FU Finnish Funding Agency for Innovation; Academy of Finland [299915,
   294238, 292334]
FX This work has been partly supported by the Finnish Funding Agency for
   Innovation (project Re: Know) and Academy of Finland (COIN CoE, and
   grants 299915, 294238 and 292334). We acknowledge the computational
   resources provided by the Aalto Science-IT project.
CR Flaxman S., 2015, ICML, V2015
   Genton MG, 2002, J MACH LEARN RES, V2, P299, DOI 10.1162/15324430260185646
   Gibbs M., 1997, THESIS
   Gramacy RB, 2008, J AM STAT ASSOC, V103, P1119, DOI 10.1198/016214508000000689
   Grzegorczyk M, 2008, BIOINFORMATICS, V24, P2071, DOI 10.1093/bioinformatics/btn367
   Heinonen M., 2016, ARTIF INTELL, P732
   Higdon D, 1999, BAYESIAN STATISTICS 6, P761
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Huang NE, 2008, REV GEOPHYS, V46, DOI 10.1029/2007RG000228
   KAKIHARA Y, 1985, J MULTIVARIATE ANAL, V16, P140, DOI 10.1016/0047-259X(85)90055-7
   Kom Samo Y.-L, 2015, TECHNICAL REPORT
   Kuss M, 2005, J MACH LEARN RES, V6, P1679
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   Loeve M., 1978, GRADUATE TEXTS MATH, V46
   Paciorek CJ, 2006, ENVIRONMETRICS, V17, P483, DOI 10.1002/env.785
   Paciorek CJ, 2004, ADV NEUR IN, V16, P273
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rioul O, 1991, IEEE SIGNAL PROC MAG, V8, P14, DOI 10.1109/79.91217
   Robinson J. W., 2009, ADV NEURAL INFORM PR, V21, P1369
   Saatci Y., 2011, THESIS
   Sampson P., 1992, J AM STAT ASS, V87
   SILVERMAN RA, 1957, IRE T INFORM THEOR, V3, P182, DOI 10.1109/TIT.1957.1057413
   Sinha A., 2016, NIPS
   Snoek J., 2014, INT C MACH LEARN, P1674
   Tolvanen V., 2014, 2014 IEEE INT WORKSH, P1
   Wilson A., 2014, NIPS
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson A. G., 2013, ICML
   Wilson Andrew Gordon, 2014, THESIS
   Yaglom AM., 1987, SPRINGER SERIES STAT, VI
   Yang Z., 2015, AISTATS
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404069
DA 2019-06-15
ER

PT S
AU Rocktaschel, T
   Riedel, S
AF Rocktaschel, Tim
   Riedel, Sebastian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI End-to-End Differentiable Proving
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.
C1 [Rocktaschel, Tim] Univ Oxford, Oxford, England.
   [Riedel, Sebastian] UCL, London, England.
   [Riedel, Sebastian] Bloomsbury AI, London, England.
RP Rocktaschel, T (reprint author), Univ Oxford, Oxford, England.
EM tim.rocktaschel@cs.ox.ac.uk; s.riedel@cs.ucl.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Google PhD Fellowship in Natural Language Processing; Allen
   Distinguished Investigator Award; Marie Curie Career Integration Award
FX We thank Pasquale Minervini, Tim Dettmers, Matko Bosnjak, Johannes
   Welbl, Naoya Inoue, Kai Arulkumaran, and the anonymous reviewers for
   very helpful comments on drafts of this paper. This work has been
   supported by a Google PhD Fellowship in Natural Language Processing, an
   Allen Distinguished Investigator Award, and a Marie Curie Career
   Integration Award.
CR Abadi M., 2016, CORR
   Andreas J., 2016, P NAACL HLT, P1545
   Andrychowicz M., 2016, ADV NEURAL INFORM PR, P3981
   Artur S., 2012, NEURAL SYMBOLIC LEAR
   Beltagy Islam, 2017, COMPUTATIONAL LINGUI
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Bosnjak Matko, 2017, INT C MACH LEARN ICM
   Bouchard Guillaume, 2015, P 2015 AAAI SPRING S
   Broomhead D. S., 1988, TECHNICAL REPORT
   Chang K., 2014, P 2014 C EMP METH NA, P1568
   Cohen William W., 2016, CORR
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Das Rajarshi, 2017, C EUR CHAPT ASS COMP
   Demeester T., 2016, P 2016 C EMP METH NA, P1389
   DING LY, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOLS 1-5, P3603, DOI 10.1109/ICSMC.1995.538347
   Franca MVM, 2014, MACH LEARN, V94, P81, DOI 10.1007/s10994-013-5392-1
   Furche T., 1978, ADV DATA BASE THEORY, P1
   Garcez ASA, 1999, APPL INTELL, V11, P59, DOI 10.1023/A:1008328630915
   Gardner  M., 2013, EMNLP, P833
   Gardner M., 2014, P 2014 C EMP METH NA, P397
   Getoor L, 2007, INTRO STAT RELATIONA
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Graves A., 2014, ARXIV14105401
   Grefenstette Edward, 2015, ADV NEURAL INFORM PR, P1828
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Holldobler S., 1990, AAAI-90 Proceedings. Eighth National Conference on Artificial Intelligence, P587
   Hu Zhiting, 2016, P 54 ANN M ASS COMP, V1
   Joulin A., 2015, ADV NEURAL INFORM PR, V28, P190
   Kaliszyk Cezary, 2017, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Kok S., 2007, P 24 INT C MACH LEAR, V227, P433
   Komendantskaya E, 2011, LOG J IGPL, V19, P821, DOI 10.1093/jigpal/jzq012
   Lao  N., 2011, P C EMP METH NAT LAN, P529
   Lao N, 2012, P 2012 JOINT C EMP M, P1017
   Loos S. M., 2017, EPIC SERIES COMPUTIN, V46, P85
   MUGGLETON S, 1990, NEW GENERAT COMPUT, V8, P295
   Muggleton SH, 2015, MACH LEARN, V100, P49, DOI 10.1007/s10994-014-5471-y
   Neelakantan Arvind, 2015, P 53 ANN M ASS COMP, V1, P156
   Neelakantan Arvind, 2016, INT C LEARN REPR ICL
   Nickel M., 2012, P 21 INT C WORLD WID, P271, DOI DOI 10.1145/2187836.2187874
   Nickel M., 2016, P 30 AAAI C ART INT, P1955
   Peng Baolin, 2015, CORR
   QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105
   Reed Scott, 2016, INT C LEARN REPR ICL
   Riedel S, 2013, P 2013 C N AM CHAPT, P74
   Rocktaschel T., 2015, P C N AM CHAPT ASS C, P1119
   Rocktaschel Tim, 2014, ACL WORKSH SEM PARS
   Russell S., 2010, ARTIFICIAL INTELLIGE
   Schoenmackers S., 2010, P 2010 C EMP METH NA, P1088
   Segler M. H. S., 2017, CORR
   Serafini Luciano, 2016, P 11 INT WORKSH NEUR
   Shastri Lokendra, 1992, P 14 ANN C COGN SCI, V14, P159
   Shavlik J. W., 1989, Connection Science, V1, P231, DOI 10.1080/09540098908915640
   Shen Yelong, 2016, P WORKSH COGN COMP L
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Socher R., 2013, ADV NEURAL INFORM PR, P926
   Sourek Gustav, 2015, P NIPS WORKSH COGN C
   STICKEL ME, 1984, NEW GENERAT COMPUT, V2, P371, DOI 10.1007/BF03037328
   Toutanova Kristina, 2015, EMNLP, V15, P1499
   TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8
   Trouillon  T., 2016, P 33 INT C MACH LEAR, P2071
   VANGELDER A, 1987, J LOGIC PROGRAM, V4, P23, DOI 10.1016/0743-1066(87)90020-3
   Vendrov I., 2016, INT C LEARN REPR ICL
   Wang William Yang, 2015, P 53 ANN M ASS COMP, V1, P355
   Weissenborn Dirk, 2016, CORR
   Weston J., 2014, ARXIV14103916
   Yang B., 2015, INT C LEARN REPR ICL
   Yang F., 2017, CORR
NR 69
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403083
DA 2019-06-15
ER

PT S
AU Roeder, G
   Wu, YH
   Duvenaud, D
AF Roeder, Geoffrey
   Wu, Yuhuai
   Duvenaud, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sticking the Landing: Simple, Lower-Variance Gradient Estimators for
   Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.
C1 [Roeder, Geoffrey; Wu, Yuhuai; Duvenaud, David] Univ Toronto, Toronto, ON, Canada.
RP Roeder, G (reprint author), Univ Toronto, Toronto, ON, Canada.
EM roeder@cs.toronto.edu; ywu@cs.toronto.edu; duvenaud@cs.toronto.edu
RI Jeong, Yongwook/N-7413-2016
CR Abadi A. A. M., 2015, TENSORFLOW LARGE SCA
   Breuleux Olivier, 2010, P 9 PYTH SCI C, P1
   Burda Y., 2015, ARXIV150900519
   Collobert R., 2002, TECHNICAL REPORT
   Dinh L., 2016, ARXIV160508803
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Han S., 2016, JMLR WORKSHOP C P, V51, P829
   Kingma D., 2015, P 3 INT C LEARN REPR
   Kingma D.P., 2013, ARXIV13126114
   Kingma Diederik P., 2016, ADV NEURAL INFORM PR, V29
   Lake Brenden M, 2014, THESIS
   LeCun Y., 1998, MNIST DATASET HANDWR
   Maclaurin Dougal, 2015, AUTOGRAD REVERSEMODE
   Ranganath  R., 2014, AISTATS, P814
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, 32 INT C MACH LEARN
   Ruiz Francisco JR, 2016, ARXIV161002287
   Tan Linda SL, 2017, STAT COMPUT, P1
   Tran D., 2016, ARXIV161009787
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407002
DA 2019-06-15
ER

PT S
AU Roth, K
   Lucchi, A
   Nowozin, S
   Hofmann, T
AF Roth, Kevin
   Lucchi, Aurelien
   Nowozin, Sebastian
   Hofmann, Thomas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Stabilizing Training of Generative Adversarial Networks through
   Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DIVERGENCE; RISK
AB Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.
C1 [Roth, Kevin; Lucchi, Aurelien; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Nowozin, Sebastian] Microsoft Res, Cambridge, England.
RP Roth, K (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM kevin.roth@inf.ethz.ch; aurelien.lucchi@inf.ethz.ch;
   sebastian.Nowozin@microsoft.com; thomas.hofmann@inf.ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU Microsoft Research through its PhD Scholarship Programme
FX We would like to thank Devon Hjelm for pointing out that the regularizer
   works well with ResNets. KR is thankful to Yannic Kilcher, Lars
   Mescheder and the dalab team for insightful discussions. Big thanks also
   to Ishaan Gulrajani and Taehoon Kim for their open-source GAN
   implementations. This work was supported by Microsoft Research through
   its PhD Scholarship Programme.
CR Amari S., 2007, METHODS INFORM GEOME
   An GZ, 1996, NEURAL COMPUT, V8, P643, DOI 10.1162/neco.1996.8.3.643
   Arjovsky Martin, 2017, ICLR
   Arjovsky Martin, 2017, P MACH LEARN RES PML
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   Bouchacourt D., 2016, ADV NEURAL INFORM PR, P352
   Che T., 2016, ARXIV161202136
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, V3, P2672
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gulrajani Ishaan, 2017, ADV NEURAL INFORM PR
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jimenez Rezende D., 2014, P 31 INT C MACH LEAR
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P, 2013, INT C LEARN REPR ICL
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   Liu  Z., 2015, P INT C COMP VIS ICC
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mescheder L., 2017, ADV NEURAL INFORM PR
   Metz Luke, 2016, INT C LEARN REPR ICL
   Minka T. P, 2005, TECHNICAL REPORT
   Muller A, 1997, ADV APPL PROBAB, V29, P429, DOI 10.2307/1428011
   Narayanan  Hariharan, 2010, ADV NEURAL INFORM PR, P1786
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Radford A., 2015, ARXIV151106434
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Scott D. W., 2015, MULTIVARIATE DENSITY
   Sonderby C. K., 2016, ARXIV161004490
   Sriperumbudur Bharath K, 2009, ARXIV09012698
   Yu F., 2015, ARXIV150603365
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402007
DA 2019-06-15
ER

PT S
AU Rothe, A
   Lake, BM
   Gureckis, TM
AF Rothe, Anselm
   Lake, Brenden M.
   Gureckis, Todd M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Question Asking as Program Generation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.
C1 [Rothe, Anselm; Lake, Brenden M.; Gureckis, Todd M.] NYU, Dept Psychol, New York, NY 10003 USA.
   [Lake, Brenden M.] NYU, Ctr Data Sci, New York, NY 10003 USA.
RP Rothe, A (reprint author), NYU, Dept Psychol, New York, NY 10003 USA.
EM anselm@nyu.edu; brenden@nyu.edu; todd.gureckis@nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [BCS-1255538]; John Templeton Foundation Varieties of Understanding
   project; John S. McDonnell Foundation Scholar Award; Moore-Sloan Data
   Science Environment at NYU
FX We thank Chris Barker, Sam Bowman, Noah Goodman, and Doug Markant for
   feedback and advice. This research was supported by NSF grant
   BCS-1255538, the John Templeton Foundation Varieties of Understanding
   project, a John S. McDonnell Foundation Scholar Award to TMG, and the
   Moore-Sloan Data Science Environment at NYU.
CR Bordes A., 2016, LEARNING END TO END
   Chouinard MM, 2007, MONOGR SOC RES CHILD, V72, P1
   FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5
   Fodor JA, 1975, LANGUAGE THOUGHT
   Goodman N. D., 2015, CONCEPTS NEW DIRECTI
   Gureckis T. M., 2009, P 31 ANN C COGN SCI
   Gureckis TM, 2012, PERSPECT PSYCHOL SCI, V7, P464, DOI 10.1177/1745691612454304
   Hawkins R. X. D., 2015, P 37 ANN C COGN SCI
   Jacobson P., 2014, COMPOSITIONAL SEMANT
   Jain U., 2017, CREATIVITY GENERATIN
   Johnson R, 2017, INT CONF PERVAS COMP
   Lake B. M., 2017, BEHAV BRAIN SCI
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Marcus GF, 2003, ALGEBRAIC MIND INTEG
   Mostafazadeh  N., 2016, P 54 ANN M ASS COMP, P1802
   Piantadosi ST, 2012, COGNITION, V123, P199, DOI 10.1016/j.cognition.2011.11.005
   Roberts C, 1996, OSU WORKING PAPERS L, V49, P91, DOI DOI 10.3765/SP.5.6
   Rothe A., 2016, P 38 ANN C COGN SCI
   Serban I, 2016, P 30 AAAI C ART INT
   Settles B., 2012, ACTIVE LEARNING
   Stokhof Martin, 1984, THESIS
   Strub Florian, 2017, INT JOINT C ART INT
   Vijayakumar  A.K., 2016, DIVERSE BEAM SEARCH
   Wang Y., 2015, ACL, V1, P1332
   Young S, 2013, P IEEE, V101, P1160, DOI 10.1109/JPROC.2012.2225812
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401009
DA 2019-06-15
ER

PT S
AU Roughgarden, T
   Schrijvers, O
AF Roughgarden, Tim
   Schrijvers, Okke
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Prediction with Selfish Experts
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SCORING RULES
AB We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions. First, it defines a model to reason formally about settings with selfish experts, and demonstrates that "incentive compatible" (IC) algorithms are closely related to the design of proper scoring rules. Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish versus honest experts by proving lower bounds for both IC and non-IC algorithms. In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction-IC or otherwise-with asymptotically vanishing regret.
C1 [Roughgarden, Tim; Schrijvers, Okke] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Roughgarden, T (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM tim@cs.stanford.edu; okkes@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12, DOI DOI 10.1145/2465769.2465777
   Babaio ff Moshe, 2010, P 11 ACM C EL COMM, P43, DOI DOI 10.1145/1807342.1807349
   BAYARRI MJ, 1989, J AM STAT ASSOC, V84, P214, DOI 10.2307/2289866
   Bickel JE, 2008, MON WEATHER REV, V136, P4867, DOI 10.1175/2008MWR2547.1
   BONIN JP, 1976, AM ECON REV, V66, P682
   Boutilier C., 2012, P 11 INT C AUT AG MU, V2, P737
   Brier GW., 1950, MONTHLY WEATHER REVI, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
   Bruckner M., 2011, P 17 ACM SIGKDD INT, P547
   Buja A, 2005, LOSS FUNCTIONS BINAR
   Cai Y, 2015, P 28 C LEARN THEOR, P280
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Chen YL, 2010, AI MAG, V31, P42, DOI 10.1609/aimag.v31i4.2313
   Dekel O, 2010, J COMPUT SYST SCI, V76, P759, DOI 10.1016/j.jcss.2010.03.003
   Frazier P., 2014, P 15 ACM C EC COMP, P5, DOI DOI 10.1145/2600057.2602897
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Goldberg AV, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P620
   Hardt M, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P111, DOI 10.1145/2840728.2840730
   Horn C., 2014, J PREDICTION MARKETS, V8, P89
   Jain K, 2007, ALGORITHMIC GAME THEORY, P385
   Jose VRR, 2008, OPER RES, V56, P1146, DOI 10.1287/opre.1070.0498
   Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Liu Y, 2016, ADV NEURAL INFORM PR, V29, P1813
   Mansour  Y., 2016, ARXIV160207570
   MCCARTHY J, 1956, P NATL ACAD SCI USA, V42, P654, DOI 10.1073/pnas.42.9.654
   Merkle EC, 2013, DECIS ANAL, V10, P292, DOI 10.1287/deca.2013.0280
   Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/ninsc.1050.0379
   Moulin H, 1999, SOC CHOICE WELFARE, V16, P279, DOI 10.1007/s003550050145
   Roughgarden T, 2007, ALGORITHMIC GAME THEORY, P443
   SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229
   SCHERVISH MJ, 1989, ANN STAT, V17, P1856, DOI 10.1214/aos/1176347398
   Shah N. B., 2015, ADV NEURAL INFORM PR, V1, P1
   THOMSON W, 1979, J ECON THEORY, V20, P360, DOI 10.1016/0022-0531(79)90042-5
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401033
DA 2019-06-15
ER

PT S
AU Roulet, V
   d'Aspremont, A
AF Roulet, Vincent
   d'Aspremont, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sharpness, Restart and Acceleration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MINIMIZATION; NONCONVEX; MINIMA
AB The Lojasiewicz inequality shows that sharpness bounds on the minimum of convex optimization problems hold almost generically. Sharpness directly controls the performance of restart schemes, as observed by Nemirovskii and Nesterov [1985]. The constants quantifying error bounds are of course unobservable, but we show that optimal restart strategies are robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.
C1 [Roulet, Vincent] ENS, INRIA, Paris, France.
   [d'Aspremont, Alexandre] ENS, CNRS, Paris, France.
RP Roulet, V (reprint author), ENS, INRIA, Paris, France.
EM vincent.roulet@inria.fr; aspremon@ens.fr
RI Jeong, Yongwook/N-7413-2016
FU chaire Economie des nouvelles donnees; fonds AXA pour la recherche; AMX
   fellowship
FX The authors would like to acknowledge support from the chaire Economie
   des nouvelles donnees with the data science joint research initiative
   with the fonds AXA pour la recherche, a gift from Societe Generale Cross
   Asset Quantitative Research and an AMX fellowship. The authors are
   affiliated to PSL Research University, Paris, France.
CR Arjevani Yossi, 2016, INT C MACH LEARN, P908
   Asuncion A., 2007, UCI MACHINE LEARNING
   Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449
   AUSLENDER AA, 1988, MATH OPER RES, V13, P243, DOI 10.1287/moor.13.2.243
   BIERSTONE E, 1988, PUBL MATH-PARIS, P5
   Bolte J., 2015, MATH PROGRAM, P1
   Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641
   Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9
   Burke JV, 2002, CONTROL CYBERN, V31, P439
   BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063
   Fercoq O, 2017, ARXIV170902300
   Fercoq Olivier, 2016, ARXIV160907358
   Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3
   Freund R. M., 2015, ARXIV151102974
   Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2
   Giselsson P, 2014, IEEE DECIS CONTR P, P5058, DOI 10.1109/CDC.2014.7040179
   Hoffman Alan J, 1952, J RES NBS, V49
   Juditski A., 2014, ARXIV14011792
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Lin Q., 2014, ICML, P73
   LOJASIEWICZ S, 1993, ANN I FOURIER, V43, P1575
   Lojasiewicz S., 1963, EQUATIONS DERIVEES P, V117, P87, DOI DOI 10.1006/JDEQ.1997.3393
   MANGASARIAN OL, 1985, MATH OPER RES, V10, P175, DOI 10.1287/moor.10.2.175
   NEMIROVSKII AS, 1985, USSR COMP MATH MATH+, V25, P21, DOI 10.1016/0041-5553(85)90100-4
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3
   Polyak B. T., 1987, INTRO OPTIMIZATION
   POLYAK B. T., 1979, IIASA WORKSH GEN LAG
   Renegar J, 2014, ARXIV14095832
   ROBINSON SM, 1975, SIAM J CONTROL, V13, P271, DOI 10.1137/0313015
   Roulet V., 2015, ARXIV150603295
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401016
DA 2019-06-15
ER

PT S
AU Rowland, M
   Weller, A
AF Rowland, Mark
   Weller, Adrian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Uprooting and Rerooting Higher-Order Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The idea of uprooting and rerooting graphical models was introduced specifically for binary pairwise models by Weller [19] as a way to transform a model to any of a whole equivalence class of related models, such that inference on any one model yields inference results for all others. This is very helpful since inference, or relevant bounds, may be much easier to obtain or more accurate for some model in the class. Here we introduce methods to extend the approach to models with higher-order potentials and develop theoretical insights. In particular, we show that the triplet-consistent polytope TRI is unique in being 'universally rooted'. We demonstrate empirically that rerooting can significantly improve accuracy of methods of inference for higher-order models at negligible computational cost.
C1 [Rowland, Mark; Weller, Adrian] Univ Cambridge, Cambridge, England.
   [Weller, Adrian] Alan Turing Inst, London, England.
RP Rowland, M (reprint author), Univ Cambridge, Cambridge, England.
EM mr504@cam.ac.uk; aw665@cam.ac.uk
FU UK Engineering and Physical Sciences Research Council (EPSRC)
   [EP/L016516/1]; Cambridge Centre for Analysis; Alan Turing Institute
   under the EPSRC [EP/N510129/1]; Leverhulme Trust via the CFI
FX We thank Aldo Pacchiano for helpful discussions, and the anonymous
   reviewers for helpful comments. MR acknowledges support by the UK
   Engineering and Physical Sciences Research Council (EPSRC) grant
   EP/L016516/1 for the University of Cambridge Centre for Doctoral
   Training, the Cambridge Centre for Analysis. AW acknowledges support by
   the Alan Turing Institute under the EPSRC grant EP/N510129/1, and by the
   Leverhulme Trust via the CFI.
CR BARAHONA F, 1988, OPER RES, V36, P493, DOI 10.1287/opre.36.3.493
   Deza M. M., 1997, GEOMETRY CUTS METRIC
   Djolonga  J., 2015, ICML, P1804
   Heskes T., 2003, P 19 ANN C UNC ART I, P313
   Jaimovich A, 2006, J COMPUT BIOL, V13, P145, DOI 10.1089/cmb.2006.13.145
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kolmogorov V, 2015, SIAM J COMPUT, V44, P1, DOI 10.1137/130945648
   Mooij JM, 2010, J MACH LEARN RES, V11, P2169
   Pearl J, 1988, PROBABILISTIC REASON
   Rowland M., 2017, ARTIFICAL INTELLIGEN
   SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036
   Sontag D., 2007, NIPS
   Sontag D., 2007, THESIS
   Wainwright M. J., 2006, IEEE T SIGNAL PROCES
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
   Weller A., 2015, AISTATS
   Weller A., 2014, NEURAL INFORM PROCES
   Weller A., 2014, UNCERTAINTY ARTIFICI
   Weller A., 2016, UAI
   Weller A., 2016, INT C MACH LEARN ICM
   Weller A., 2016, ARTIFICIAL INTELLIGE
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400020
DA 2019-06-15
ER

PT S
AU Roy, A
   Xu, H
   Pokutta, S
AF Roy, Aurko
   Xu, Huan
   Pokutta, Sebastian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Reinforcement Learning under Model Mismatch
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS
AB We study reinforcement learning under model misspecification, where we do not have access to the true environment but only to a reasonably close approximation to it. We address this problem by extending the framework of robust MDPs of [1, 15, 11] to the model free Reinforcement Learning setting, where we do not have access to the model parameters, but can only sample states from it. We define robust versions of Q-learning, SARSA, and TD-learning and prove convergence to an approximately optimal robust policy and approximate value function respectively. We scale up the robust algorithms to large MDPs via function approximation and prove convergence under two different settings. We prove convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures (under mild assumptions). We also define a robust loss function, the mean squared robust projected Bellman error and give stochastic gradient descent algorithms that are guaranteed to converge to a local minimum.
C1 [Roy, Aurko] Google, Mountain View, CA 94043 USA.
   [Xu, Huan; Pokutta, Sebastian] Georgia Inst Technol, ISyE, Atlanta, GA 30332 USA.
   [Roy, Aurko] Georgia Tech, Atlanta, GA USA.
RP Roy, A (reprint author), Google, Mountain View, CA 94043 USA.
EM aurkor@google.com; huan.xu@isye.gatech.edu;
   sebastian.pokutta@isye.gatech.edu
RI Jeong, Yongwook/N-7413-2016
CR Bagnell J. A., 2001, SOLVING UNCERTAIN MA
   Bertsekas Dimitri P., 2011, Journal of Control Theory and Applications, V9, P310, DOI 10.1007/s11768-011-1005-3
   Bertsekas D. P., 1996, LIDSP2349 MIT LAB IN
   Bertsekas DP, 2009, J COMPUT APPL MATH, V227, P27, DOI 10.1016/j.cam.2008.07.037
   Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953
   Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646
   Bradtke SJ, 1996, MACH LEARN, V22, P33, DOI 10.1023/A:1018056104778
   Brockman G, 2016, ARXIV160601540
   Delage E, 2010, OPER RES, V58, P203, DOI 10.1287/opre.1080.0685
   Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129
   Maei H. R., 2009, ADV NEURAL INFORM PR, P1204
   Morimoto J, 2005, NEURAL COMPUT, V17, P335, DOI 10.1162/0899766053011528
   Nedic A, 2003, DISCRETE EVENT DYN S, V13, P79, DOI 10.1023/A:1022192903948
   Nilim A., 2003, NIPS, P839
   Pinto L., 2017, ARXIV170302702
   Powell W. B., 2007, APPROXIMATE DYNAMIC, V703
   Puterman M. L., 2014, MARKOV DECISION PROC
   Shapiro A, 2002, OPTIM METHOD SOFTW, V17, P523, DOI 10.1080/1055678021000034008
   Sutton R., 2009, INT C MACH LEARN, P993, DOI DOI 10.1145/1553374.1553501
   Sutton R. S., 2009, ADV NEURAL INFORM PR, P1609
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Tamar A., 2014, ARXIV14043862
   Tamar A., 2014, ICML, V32, P2014
   Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566
   Xu Huan, 2013, ADV NEURAL INFORM PR, P701
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403011
DA 2019-06-15
ER

PT S
AU Roychowdhury, A
   Parthasarathy, S
AF Roychowdhury, Anirban
   Parthasarathy, Srinivasan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Bayesian Sampling with Monte Carlo EM
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MAXIMUM-LIKELIHOOD; CONVERGENCE; ALGORITHMS
AB We present a novel technique for learning the mass matrices in samplers obtained from discretized dynamics that preserve some energy function. Existing adaptive samplers use Riemannian preconditioning techniques, where the mass matrices are functions of the parameters being sampled. This leads to significant complexities in the energy reformulations and resultant dynamics, often leading to implicit systems of equations and requiring inversion of high-dimensional matrices in the leapfrog steps. Our approach provides a simpler alternative, by using existing dynamics in the sampling step of a Monte Carlo EM framework, and learning the mass matrices in the M step with a novel online technique. We also propose a way to adaptively set the number of samples gathered in the E step, using sampling error estimates from the leapfrog dynamics. Along with a novel stochastic sampler based on Nose-Poincare dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong performance on synthetic and real high-dimensional sampling scenarios; we achieve sampling accuracies comparable to Riemannian samplers while being significantly faster.
C1 [Roychowdhury, Anirban; Parthasarathy, Srinivasan] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
RP Roychowdhury, A (reprint author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM roychowdhury.7@osu.edu; srini@cse.ohio-state.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [DMS-1418265]
FX We thank the anonymous reviewers for their insightful comments and
   suggestions. This material is based upon work supported by the National
   Science Foundation under Grant No. DMS-1418265. Any opinions, findings,
   and conclusions or recommendations expressed in this material are those
   of the author(s) and do not necessarily reflect the views of the
   National Science Foundation.
CR Bond SD, 1999, J COMPUT PHYS, V151, P114, DOI 10.1006/jcph.1998.6171
   Booth JG, 1999, J ROY STAT SOC B, V61, P265, DOI 10.1111/1467-9868.00176
   Bottou L., 1998, ONLINE LEARNING NEUR, P9
   CHAN KS, 1995, J AM STAT ASSOC, V90, P242, DOI 10.2307/2291149
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   DIXON JD, 1982, NUMER MATH, V40, P137, DOI 10.1007/BF01459082
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Eberly W., 2006, P 2006 INT S SYMB AL, P63
   Fort G, 2003, ANN STAT, V31, P1220
   Frenkel D, 2001, UNDERSTANDING MOL SI
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   HOOVER WG, 1985, PHYS REV A, V31, P1695, DOI 10.1103/PhysRevA.31.1695
   Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941
   Leimkuhler B, 2015, MOL DYNAMICS DETERMI
   Levine RA, 2001, J COMPUT GRAPH STAT, V10, P422, DOI 10.1198/106186001317115045
   McCulloch CE, 1997, J AM STAT ASSOC, V92, P162, DOI 10.2307/2291460
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Reich S., 2004, SIMULATING HAMILTONI
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Robert CP, 1999, J STAT COMPUT SIM, V64, P327, DOI 10.1080/00949659908811984
   Roychowdhury A., 2016, P 33 INT C MACH LEAR, P2673
   Roychowdhury A., 2015, P 18 INT C ART INT S, P800
   Sherman R. P., 1999, ECONOMET J, V2, P248
   Srivastava N., 2013, UNC ART INT P 29 C, P616
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Uhler C, 2012, ANN STAT, V40, P238, DOI 10.1214/11-AOS957
   WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Yin L, 2006, J PHYS A-MATH GEN, V39, P8593, DOI 10.1088/0305-4470/39/27/003
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401029
DA 2019-06-15
ER

PT S
AU Royer, M
AF Royer, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive Clustering through Semidefinite Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We analyze the clustering problem through a flexible probabilistic model that aims to identify an optimal partition on the sample X-1 , ..., X-n. We perform exact clustering with high probability using a convex semidefinite estimator that interprets as a corrected, relaxed version of K-means. The estimator is analyzed through a non-asymptotic framework and showed to be optimal or near-optimal in recovering the partition. Furthermore, its performances are shown to be adaptive to the problem's effective dimension, as well as to K the unknown number of groups in this partition. We illustrate the method's performances in comparison to other classical clustering algorithms with numerical experiments on simulated high-dimensional data.
C1 [Royer, Martin] Univ Paris Sud, Univ Paris Saclay, Lab Math Orsay, CNRS, F-91405 Orsay, France.
RP Royer, M (reprint author), Univ Paris Sud, Univ Paris Saclay, Lab Math Orsay, CNRS, F-91405 Orsay, France.
EM martin.royer@math.u-psud.fr
RI Jeong, Yongwook/N-7413-2016
FU French National research Agency (ANR), "Investissement d'Avenir"
   program, through the "IDI 2015" project - IDEX Paris-Saclay
   [ANR-11-IDEX-0003-02]; CNRS PICS funding HighClust
FX This work is supported by a public grant overseen by the French National
   research Agency (ANR) as part of the "Investissement d'Avenir" program,
   through the "IDI 2015" project funded by the IDEX Paris-Saclay,
   ANR-11-IDEX-0003-02. It is also supported by the CNRS PICS funding
   HighClust. We thank Christophe Giraud for a shrewd, unwavering thesis
   direction.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   AZIZYAN M., 2013, ADV NEURAL INFORM PR, P2139
   Banks J., 2016, ARXIV160705222
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Bunea F., 2016, ARXIV160605100
   Bunea F., 2015, ARXIV150801939
   Chen YY, 2016, BMC MOL BIOL, V17, DOI 10.1186/s12867-016-0055-y
   Chretien S., 2016, ABS160609190 CORR
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   Fei Y., 2017, ARXIV170508391
   Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131
   Guedon O., 2014, ABS14114686 CORR
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mixon DG, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983
   Royer Martin, 2017, ADMM IMPLEMENTATION
   Steinhaus H., 1957, B ACAD POL SCI, V12, P801
   Vershynin R., 2012, COMPRESSED SENSING T
   Verzelen N., 2014, ARXIV14051478
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Yan Bowei, 2016, ARXIV160702675
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401080
DA 2019-06-15
ER

PT S
AU Rudi, A
   Carratino, L
   Rosasco, L
AF Rudi, Alessandro
   Carratino, Luigi
   Rosasco, Lorenzo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI FALKON: An Optimal Large Scale Kernel Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS
AB Kernel methods provide a principled way to perform non linear, nonparametric learning. They rely on solid functional analytic foundations and enjoy optimal statistical properties. However, at least in their basic form, they have limited applicability in large scale scenarios because of stringent computational requirements in terms of time and especially memory. In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points. FALKON is derived combining several algorithmic principles, namely stochastic subsampling, iterative solvers and preconditioning. Our theoretical analysis shows that optimal statistical accuracy is achieved requiring essentially O(n) memory and O (n root n) time. An extensive experimental analysis on large scale datasets shows that, even with a single machine, FALKON outperforms previous state of the art solutions, which exploit parallel/distributed architectures.
C1 [Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.
   [Carratino, Luigi; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy.
   [Rudi, Alessandro; Rosasco, Lorenzo] IIT, LCSL, Genoa, Italy.
   [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA.
RP Rudi, A (reprint author), Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.
EM alessandro.rudi@inria.fr
RI Jeong, Yongwook/N-7413-2016
FU Air Force project [FA9550-17-1-0390]; FIRB project [RBFR12M3AC]
FX The authors would like to thank Mikhail Belkin, Benjamin Recht and
   Siyuan Ma, Eric Fosler-Lussier, Shivaram Venkataraman, Stephen L. Tu,
   for providing their features of the TIMIT and YELP datasets, and NVIDIA
   Corporation for the donation of the Tesla K40c GPU used for this
   research. This work is funded by the Air Force project FA9550-17-1-0390
   (European Office of Aerospace Research and Development) and by the FIRB
   project RBFR12M3AC (Italian Ministry of Education, University and
   Research).
CR Alaoui Ahmed, 2015, ADV NEURAL INFORM PR, V28, P775
   Alves Alexandre, 2016, ABS161207725 CORR
   Avron Haim, 2016, ARXIV161103220
   BACH F, 2017, J MACH LEARN RES, V18
   Bach F., 2013, J MACH LEARN RES, V30, P185
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Bertin-Mahieux T., 2011, ISMIR
   Boucheron Stephane, 2004, ADV LECT MACHINE LEA
   Camoriano Raffaello, 2016, P 19 INT C ART INT S, P1403
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564
   Chen Jie, 2016, ABS160800860 CORR
   Cohen M. B., 2015, P 2015 C INN THEOR C, P181
   Cutajar Kurt, 2016, P 33 INT C MACH LEAR, P2529
   Dai B., 2014, ADV NEURAL INFORM PR, P3041
   De Vito E, 2005, J MACH LEARN RES, V6, P883
   Dieuleveut  A., 2014, ARXIV14080361
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Fasshauer GE, 2012, SIAM J SCI COMPUT, V34, pA737, DOI 10.1137/110824784
   Gonen Alon, 2016, ARXIV160202350
   Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517
   Ma Siyuan, 2017, ARXIV170310622
   May Avner, 2017, ABS170103577 CORR
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   REED M., 1980, FUNCTIONAL ANAL, VI
   Rudi  A., 2016, ARXIV160204474
   Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1648
   Rudi  A., 2013, ADV NEURAL INFORM PR, P2067
   SAAD Y., 2003, ITERATIVE METHODS SP
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Smola A.J., 2000, P 17 INT C MACH LEAR, P911
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart Ingo, 2009, COLT
   Szegedy C., 2017, AAAI, V4, P4278
   Tu Stephen, 2016, ARXIV160205310
   Williams Christopher K.I., 2000, ADV NEURAL INFORM PR, V13, P682
   Yang Yun, 2015, ARXIV150106195
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Zhang Y., 2013, C LEARN THEOR, P592
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403092
DA 2019-06-15
ER

PT S
AU Rudi, A
   Rosasco, L
AF Rudi, Alessandro
   Rosasco, Lorenzo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Generalization Properties of Learning with Random Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID APPROXIMATION
AB ( )We study the generalization properties of ridge regression with random features in the statistical learning framework. We show for the first time that O(1/root n) learning bounds can be achieved with only O(root n log n) random features rather than O(n) as suggested by previous results. Further, we prove faster learning rates and show that they might require more random features, unless they are sampled according to a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.
C1 [Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, F-75012 Paris, France.
   [Rosasco, Lorenzo] Univ Genoa, Ist Italiano Tecnol, Genoa, Italy.
   [Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Rudi, A (reprint author), Ecole Normale Super, INRIA, Sierra Project Team, F-75012 Paris, France.
EM alessandro.rudi@inria.fr; lrosasco@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU Air Force project (European Office of Aerospace Research and
   Development) [FA9550-17-1-0390]; FIRB project (Italian Ministry of
   Education, University and Research) [RBFR12M3AC]
FX The authors gratefully acknowledge the contribution of Raffaello
   Camoriano who was involved in the initial phase of this project. These
   preliminary result appeared in the 2016 NIPS workshop "Adaptive and
   Scalable Nonparametric Methods in ML". This work is funded by the Air
   Force project FA9550-17-1-0390 (European Office of Aerospace Research
   and Development) and by the FIRB project RBFR12M3AC (Italian Ministry of
   Education, University and Research).
CR Alaoui A., 2015, NIPS
   ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Bach F., 2015, ARXIV E PRINTS
   Bach F., 2013, COLT
   Bhatia  R., 2013, MATRIX ANAL, V169
   Bishop C. M., 2006, PATTERN RECOGNITION
   Boucheron Stephane, 2004, ADV LECT MACHINE LEA
   Caponnetto A., 2007, OPTIMAL RATES REGULA
   Caponnetto Andrea, 2006, ADAPTATION REGULARIZ
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Cortes C., 2010, AISTATS
   Cucker F, 2002, B AM MATH SOC, V39, P1
   De Vito E, 2005, J MACH LEARN RES, V6, P883
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   FUJII J, 1993, P AM MATH SOC, V118, P827, DOI 10.2307/2160128
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hamid Raffay, 2014, P 31 INT C MACH LEAR, P19
   Kar Purushottam, 2012, AISTATS
   KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089
   Le  Q., 2013, ICML
   Minsker S., 2011, SOME EXTENSIONS BERN
   Pham N., 2013, P 19 ACM SIGKDD INT, P239
   Pinkus A., 1999, Acta Numerica, V8, P143, DOI 10.1017/S0962492900002919
   Plan Y, 2014, DISCRETE COMPUT GEOM, V51, P438, DOI 10.1007/s00454-013-9561-6
   Poggio T., 1990, P IEEE
   Raginsky M., 2009, NIPS
   Rahimi A., 2007, NIPS
   Rahimi Ali, 2009, NIPS
   Rudi A., 2015, NIPS
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y
   Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089
   Smola A. J., 2000, ICML
   Sriperumbudur B. K., 2015, ARXIV E PRINTS
   Steinwart I, 2006, IEEE T INFORM THEORY, V52, P4635, DOI 10.1109/TIT.2006.881713
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tropp J. A., 2012, USER FRIENDLY TOOLS
   Vapnik V. N., 1998, STAT LEARNING THEORY, V1
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wahba G, 1990, SPLINE MODELS OBSERV, V59
   Williams Christopher K. I., 2000, NIPS
   Yang J., 2014, P 31 INT C MACH LEAR, P485
   Yang JY, 2014, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2014.129
   Yang T., 2012, NIPS, P485
   Yurinsky V. V., 1995, SUMS GAUSSIAN VECTOR
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403028
DA 2019-06-15
ER

PT S
AU Rudolph, M
   Ruiz, F
   Athey, S
   Blei, D
AF Rudolph, Maja
   Ruiz, Francisco
   Athey, Susan
   Blei, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Structured Embedding Models for Grouped Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S - EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S - EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.
C1 [Rudolph, Maja; Blei, David] Columbia Univ, New York, NY 10027 USA.
   [Ruiz, Francisco] Univ Cambridge, Cambridge, England.
   [Athey, Susan] Stanford Univ, Stanford, CA 94305 USA.
RP Rudolph, M (reprint author), Columbia Univ, New York, NY 10027 USA.
EM maja@cs.columbia.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA PPAML
   [FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan
   Foundation; John Simon Guggenheim Foundation; EU [706760]
FX We thank Elliott Ash and Suresh Naidu for the helpful discussions and
   for sharing the Senate speeches. This work is supported by NSF
   IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA
   SIMPLEX N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John
   Simon Guggenheim Foundation. Francisco J. R. Ruiz is supported by the EU
   H2020 programme (Marie Sklodowska-Curie grant agreement 706760).
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Ammar Waleed, 2016, ARXIV160201925
   Arnold BC, 2001, STAT SCI, V16, P249
   Arora Sanjeev, 2015, ARXIV150203520
   Bamler R., 2017, INT C MACH LEARN
   Barkan O., 2016, MACH LEARN SIGN PROC, P1, DOI DOI 10.1109/MLSP.2016
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bishop C. M., 2006, INFORM SCI STAT
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Gelman A., 2003, BAYESIAN DATA ANAL
   Gershman Samuel J, 2014, P 36 ANN C COGN SCI
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Hamilton William L, 2016, ARXIV160509096
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Kim Yoon, 2014, ARXIV14053515
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   KLEMENTIEV A., 2012, INDUCING CROSSLINGUA
   Korattikara A., 2015, ADV NEURAL INFORM PR
   Kulkarni V., 2015, P 24 INT C WORLD WID, P625, DOI DOI 10.1145/2736277.2741627
   Levy O., 2014, ADV NEURAL INFORM PR, V27, P2177, DOI DOI 10.1162/153244303322533223
   Mikolov T., 2013, P NAACL 2013, P746
   Mikolov T, 2013, ICLR WORKSH P
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   MIKOLOV Tomas, 2013, ARXIV13094168
   Mnih Andriy, 2014, INT C MACH LEARN
   Murphy KP, 2012, MACHINE LEARNING PRO
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Rezende D. J., 2014, INT C MACH LEARN
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rudolph Maja, 2017, ARXIV170308052
   Rudolph Maja, 2016, ADV NEURAL INFORM PR, P478
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Yao Zijun, 2017, ARXIV170300607
   Zou W. Y., 2013, P 2013 C EMP METH NA, P1393
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400024
DA 2019-06-15
ER

PT S
AU Ruffini, M
   Rabusseau, G
   Balle, B
AF Ruffini, Matteo
   Rabusseau, Guillaume
   Balle, Borja
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hierarchical Methods of Moments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TENSOR DECOMPOSITIONS
AB Spectral methods of moments provide a powerful tool for learning the parameters of latent variable models. Despite their theoretical appeal, the applicability of these methods to real data is still limited due to a lack of robustness to model misspecification. In this paper we present a hierarchical approach to methods of moments to circumvent such limitations. Our method is based on replacing the tensor decomposition step used in previous algorithms with approximate joint diagonalization. Experiments on topic modeling show that our method outperforms previous tensor decomposition methods in terms of speed and model quality.
C1 [Ruffini, Matteo] Univ Politecn Cataluna, Barcelona, Spain.
   [Rabusseau, Guillaume] McGill Univ, Montreal, PQ, Canada.
   [Balle, Borja] Amazon Res, Cambridge, England.
RP Ruffini, M (reprint author), Univ Politecn Cataluna, Barcelona, Spain.
EM mruffini@cs.upc.edu; guillaume.rabusseau@mail.mcgill.ca;
   pigem@amazon.co.uk
FU IVADO postdoctoral fellowship
FX Guillaume Rabusseau acknowledges support of an IVADO postdoctoral
   fellowship. Borja Balle completed this work while at Lancaster
   University.
CR Anandkumar A., 2015, COLT, P36
   Anandkumar A., 2012, ADV NEURAL INFORM PR, P917
   Anandkumar A, 2017, J MACH LEARN RES, V18, P1
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Anandkumar Animashree, 2012, COLT, V1, P4
   Bailly R., 2011, J MACHINE LEARNING R, V20, P147
   Balle  B., 2014, ICML, P1386
   Balle Borja, 2012, NIPS, P2159
   Balle Borja, 2012, ICML, P1819
   BUNSEGERSTNER A, 1993, SIAM J MATRIX ANAL A, V14, P927, DOI 10.1137/0614062
   Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546
   Chaganty A. T., 2013, ICML, V28, P1040
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Jain P., 2014, J MACH LEARN RES, V35, P824
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kuleshov  V., 2015, J MACH LEARN RES W C, P507
   Kulesza Alex, 2014, AISTATS, P522
   Kulesza Alex, 2015, ARTIF INTELL, P517
   Perrone Valerio, 2016, ARXIV161107460
   Quattoni A., 2014, ICML, P1710
   Robeva Elina Mihaylova, 2016, THESIS
   Ruffini Matteo, 2016, ARXIV161203409
   Savaresi S. M., 2001, P 2001 SIAM INT C DA, P1
   Sievert Carson, 2014, ACL WORKSH INT LANG
   Song L., 2011, P 28 INT C MACH LEAR, P1065
   Steinbach M, 2000, P KDD WORKSH TEXT MI, V400, P525
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Zhang Y., 2014, ADV NEURAL INFORM PR, P1260
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401090
DA 2019-06-15
ER

PT S
AU Ryabko, D
AF Ryabko, Daniil
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Independence clustering (without a matrix)
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The independence clustering problem is considered in the following formulation: given a set S of random variables, it is required to find the finest partitioning {U-1,...,U-k} of S into clusters such that the clusters U-1,...,U-k are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in S is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d. and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of fascinating open directions for further research are outlined.
C1 [Ryabko, Daniil] INRIA Lillle, 40 Ave Halley, Villeneuve Dascq, France.
RP Ryabko, D (reprint author), INRIA Lillle, 40 Ave Halley, Villeneuve Dascq, France.
EM daniil@ryabko.net
RI Jeong, Yongwook/N-7413-2016
CR Bach F.R., 2003, J MACHINE LEARNING R, V4, P1205
   Balcan MF, 2014, J MACH LEARN RES, V15, P3831
   Beirlant J., 1997, INT J MATH STAT SCI, V6, P17
   Benjaminsson S, 2010, FRONT SYST NEUROSCI, V4, DOI 10.3389/fnsys.2010.00034
   Chickering D.M, 1996, LEARNING DATA ARTIFI, P121, DOI DOI 10.1007/978-1-4612-2404-4_12
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Gray R., 1988, PROBABILITY RANDOM P
   Gyorfi Laszlo, 2011, COMMUNICATION
   JIROUSEK R, 1991, KYBERNETIKA, V27, P403
   Kandasamy Kirthevasan, 2014, ARXIV14114342
   Khaleghi A, 2016, J MACH LEARN RES, V17
   Kolchinsky A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00066
   Kraskov A, 2005, EUROPHYS LETT, V70, P278, DOI 10.1209/epl/i2004-10483-y
   Mantegna RN, 1999, EUR PHYS J B, V11, P193, DOI 10.1007/s100510050929
   Marrelec G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137278
   Marti Gautier, 2016, IJCAI 16
   Meek C, 2001, J ARTIF INTELL RES, V15, P383, DOI 10.1613/jair.914
   Pal D, 2010, ADV NEURAL INFORM PR, V23, P1849
   Priness I, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-111
   Ryabko D., 2010, P 27 INT C MACH LEAR, P919
   Ryabko D., 2017, PMLR, V76, P400
   Ryabko D, 2012, TEST-SPAIN, V21, P317, DOI 10.1007/s11749-011-0245-3
   Ryabko D, 2010, J THEOR PROBAB, V23, P565, DOI 10.1007/s10959-009-0263-1
   Ryabko D, 2010, IEEE T INFORM THEORY, V56, P1430, DOI 10.1109/TIT.2009.2039169
   Shields PC, 1998, IEEE T INFORM THEORY, V44, P2079, DOI 10.1109/18.720532
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Zhang K., 2011, P 27 ANN C UNC ART I
   Zhou XB, 2004, J COMPUT BIOL, V11, P147, DOI 10.1089/106652704773416939
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404009
DA 2019-06-15
ER

PT S
AU Saatchi, Y
   Wilson, AG
AF Saatchi, Yunus
   Wilson, Andrew Gordon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bayesian GAN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as label smoothing or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.
C1 [Saatchi, Yunus] Uber AI Labs, San Francisco, CA 94107 USA.
   [Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA.
RP Saatchi, Y (reprint author), Uber AI Labs, San Francisco, CA 94107 USA.
FU NSF [IIS-1563887]
FX We thank Pavel Izmailov and Ben Athiwaratkun for helping to create a
   tutorial for the codebase, helpful comments and validation. We also
   thank Soumith Chintala for helpful advice. We thank NSF IIS-1563887 for
   support.
CR Arjovsky M, 2017, ARXIV170107875
   Borg I., 2005, MODERN MULTIDIMENSIO
   Chen T, 2014, P INT C MACH LEARN
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Karaletsos  Theofanis, 2016, ARXIV161205048
   Kingma D. P., 2013, ARXIV13126114
   Krizhevsky  A., 2010, CIFAR 10
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Radford  A., 2015, ARXIV151106434
   Salimans T., 2016, ABS160603498 CORR
   Tran D., 2017, ADV NEURAL INFORM PR, P5529
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wilson A. G., 2016, ADV NEURAL INFORM PR, P2586
   Wilson Andrew Gordon, 2016, ARTIFICIAL INTELLIGE
NR 14
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403067
DA 2019-06-15
ER

PT S
AU Sadhanala, V
   Wang, YX
   Sharpnack, J
   Tibshirani, RJ
AF Sadhanala, Veeranjaneyulu
   Wang, Yu-Xiang
   Sharpnack, James
   Tibshirani, Ryan J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend
   Filtering Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID TOTAL VARIATION MINIMIZATION; FUSED LASSO; REGRESSION; ALGORITHM; PATH
AB We consider the problem of estimating the values of a function over n nodes of a d-dimensional grid graph (having equal side lengths n(1/d)) from noisy observations. The function is assumed to be smooth, but is allowed to exhibit different amounts of smoothness at different regions in the grid. Such heterogeneity eludes classical measures of smoothness from nonparametric statistics, such as Holder smoothness. Meanwhile, total variation (TV) smoothness classes allow for heterogeneity, but are restrictive in another sense: only constant functions count as perfectly smooth (achieve zero TV). To move past this, we define two new higher-order TV classes, based on two ways of compiling the discrete derivatives of a parameter across the nodes. We relate these two new classes to Holder classes, and derive lower bounds on their minimax errors. We also analyze two naturally associated trend filtering methods; when d = 2, each is seen to be rate optimal over the appropriate class.
C1 [Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Tibshirani, Ryan J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Wang, Yu-Xiang] Amazon AI, Palo Alto, CA 94303 USA.
   [Sharpnack, James] Univ Calif Davis, Davis, CA 95616 USA.
RP Sadhanala, V (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM vsadhana@cs.cmu.edu; yuxiangw@amazon.com; jsharpna@ucdavis.edu;
   ryantibs@stat.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Barbero  A., 2014, ARXIV14110589
   Bogoya JM, 2016, LINEAR ALGEBRA APPL, V493, P606, DOI 10.1016/j.laa.2015.12.017
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9
   Condat Laurent, 2012, HAL00675043
   Donoho DL, 1998, ANN STAT, V26, P879
   Harchaoui Z, 2010, J AM STAT ASSOC, V105, P1480, DOI 10.1198/jasa.2010.tm09181
   Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208
   Hutter  J.-C., 2016, ANN C LEARNING THEOR, P1115
   Johnson NA, 2013, J COMPUT GRAPH STAT, V22, P246, DOI 10.1080/10618600.2012.681238
   Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274
   Korostelev Aleksandr P., 2003, MINIMAX THEORY IMAGE
   Kovac A, 2011, J COMPUT GRAPH STAT, V20, P432, DOI 10.1198/jcgs.2011.09203
   Mammen E, 1997, ANN STAT, V25, P387
   Padilla Oscar Hernan Madrid, 2016, ARXIV160803384
   Poschl C, 2008, CONTEMP MATH, V451, P219
   Rinaldo A, 2009, ANN STAT, V37, P2922, DOI 10.1214/08-AOS665
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sadhanala Veeranjaneyulu, 2016, ADV NEURAL INFORM PR, V29
   Sharpnack James, 2012, INT C ART INT STAT, V15
   Steidl G, 2006, INT J COMPUT VISION, V70, P241, DOI 10.1007/s11263-006-8066-7
   Tansey  W., 2015, ARXIV150506475
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R. J, 2017, ARXIV170205037
   Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189
   Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878
   Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1
   Wang Yu- Xiang, 2014, INT C MACH LEARN, V31
   Wang YJ, 2016, J MACH LEARN RES, V17, P1
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405085
DA 2019-06-15
ER

PT S
AU Salimbeni, H
   Deisenroth, MP
AF Salimbeni, Hugh
   Deisenroth, Marc Peter
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Doubly Stochastic Variational Inference for Deep Gaussian Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to overfitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.
C1 [Salimbeni, Hugh; Deisenroth, Marc Peter] Imperial Coll London, London, England.
   [Salimbeni, Hugh; Deisenroth, Marc Peter] PROWLER Io, Cambridge, England.
RP Salimbeni, H (reprint author), Imperial Coll London, London, England.; Salimbeni, H (reprint author), PROWLER Io, Cambridge, England.
EM hrs13@ic.ac.uk; m.deisenroth@imperial.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Google Faculty Research Award; Microsoft Azure Scholarship
FX We have greatly appreciated valuable discussions with James Hensman and
   Steindor Saemundsson in the preparation of this work. We thank Vincent
   Dutordoir and anonymous reviewers for helpful feedback on the
   manuscript. We are grateful for a Microsoft Azure Scholarship and
   support through a Google Faculty Research Award to Marc Deisenroth.
CR Abadi M., 2015, 160304467 ARXIV
   Baldi P., 2014, NATURE COMMUNICATION
   Bonilla E. V., 2016, 160900577 ARXIV
   Briol F.-X., 2015, 151200933 ARXIV
   Bui T. D., 2016, INT C MACH LEARN
   Calandra Roberto, 2016, IEEE INT JOINT C NEU
   Cutajar K., 2017, INT C MACH LEARN
   Cutler M., 2015, IEEE INT C ROB AUT
   Dai Z., 2016, INT C LEARN REPR
   Damianou A. C., 2011, ADV NEURAL INFORM PR
   Damianou A. C., 2013, INT C ART INT STAT
   Deisenroth M. P., 2011, INT C MACH LEARN
   Diggle PJ, 2007, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-48536-2
   Duvenaud D., 2014, ARTIFICIAL INTELLIGE
   Duvenaud D., 2013, INT C MACH LEARN
   Gal Y., 2015, INT C MACH LEARN
   Garnett R., 2009, INT C MACH LEARN
   Guestrin C., 2005, INT C MACH LEARN
   He Kaiming, 2016, IEEE C COMP VIS PATT
   Hensman J., 2015, ADV NEURAL INFORM PR
   Hensman J., 2014, 14121370 ARXIV
   Hensman James, 2013, UNCERTAINTY ARTIFICI
   Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR
   Hernandez-Lobato J. M., 2015, INT C MACH LEARN
   Kingma D. P., 2015, VARIATIONAL DROPOUT
   Ko J., 2008, IEEE INTELLIGENT ROB
   Krauth K., 2016, 161005392 ARXIV
   Larochelle H., 2007, INT C MACH LEARN
   Lawrence N. D., 2007, INT C MACH LEARN
   Lazaro-Gredilla M., 2012, ADV NEURAL INFORM PR
   Mackay D. J. C., 1999, NEURAL COMPUTATION
   Matthews A. G., 2017, J MACHINE LEARNING R
   Matthews Alexander, 2016, ARTIFICIAL INTELLIGE
   Mattos C. L. C, 2016, INT C LEARN REPR
   Peng H., 2017, 170406735 ARXIV
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rezende D. J., 2014, INT C MACH LEARN
   Snoek J, 2012, ADV NEURAL INFORM PR
   Titsias M. K., 2010, INT C ART INT STAT
   Titsias Michalis, 2013, ADV NEURAL INFORM PR
   Turner R., 2011, BAYESIAN TIME SERIES
   Vafa K., 2016, ADV APPROXIMATE BAYE
   Wang Y., 2016, ARTIFICIAL INTELLIGE
   Wilson Andrew Gordon, 2016, ARTIFICIAL INTELLIGE
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404064
DA 2019-06-15
ER

PT S
AU Savinov, N
   Ladicky, L
   Pollefeys, M
AF Savinov, Nikolay
   Ladicky, Lubor
   Pollefeys, Marc
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Matching neural paths: transfer from recognition to correspondence
   search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences - a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as "matching" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data.
C1 [Savinov, Nikolay; Ladicky, Lubor; Pollefeys, Marc] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Pollefeys, Marc] Microsoft, Redmond, WA USA.
RP Savinov, N (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM nikolay.savinov@nf.ethz.ch; lubor.ladicky@inf.ethz.ch;
   marc.pollefeys@inf.ethz.ch
RI Jeong, Yongwook/N-7413-2016
FU Swiss NSF [163910]
FX We would like to thank Dmitry Laptev, Alina Kuznetsova and Andrea Cohen
   for their comments about the manuscript. We also thank Valery
   Vishnevskiy for running our code while our own cluster was down. This
   work is partially funded by the Swiss NSF project 163910 "Efficient
   Object-Centric Detection".
CR Badrinarayanan Vijay, 2015, ARXIV151100561
   Choy C. B., 2016, ADV NEURAL INFORM PR, P2414
   Donahue J, 2013, ABS13101531 CORR, P2013
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Gatys Leon A., 2015, ARXIV150806576
   Geiger A., 2012, C COMP VIS PATT REC
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378
   Hirschmuller H, 2005, PROC CVPR IEEE, P807
   Kim Seungryong, 2017, ARXIV170200926
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Long J. L., 2014, ADV NEURAL INFORM PR, P1601
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Menze M., 2015, C COMP VIS PATT REC
   Razavian A. S., 2014, ARXIV14036382
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S, 2015, ADV NEURAL INFORM PR, P91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   van den Oord Aaron, 2016, ABS160903499 CORR
   Yosinski J., 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zbontar Jure, 2016, MC CNN GITHUB REPOSI
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401024
DA 2019-06-15
ER

PT S
AU Scarlett, J
   Cevher, V
AF Scarlett, Jonathan
   Cevher, Volkan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Phase Transitions in the Pooled Data Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECOVERY; MODELS
AB In this paper, we study the pooled data problem of identifying the labels associated with a large collection of items, based on a sequence of pooled tests revealing the counts of each label within the pool. In the noiseless setting, we identify an exact asymptotic threshold on the required number of tests with optimal decoding, and prove a phase transition between complete success and complete failure. In addition, we present a novel noisy variation of the problem, and provide an information-theoretic framework for characterizing the required number of tests for general random noise models. Our results reveal that noise can make the problem considerably more difficult, with strict increases in the scaling laws even at low noise levels. Finally, we demonstrate similar behavior in an approximate recovery setting, where a given number of errors is allowed in the decoded labels.
C1 [Scarlett, Jonathan; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
RP Scarlett, J (reprint author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
EM jonathan.scarlett@epfl.ch; volkancevher@epfl.ch
RI Jeong, Yongwook/N-7413-2016
FU European Commission [CRSII2-147633, SNF 200021-146750]; EPFL Fellows
   Horizon2020 [665667]
FX This work was supported in part by the European Commission under Grant
   ERC Future Proof, SNF Sinergia project CRSII2-147633, SNF 200021-146750,
   and EPFL Fellows Horizon2020 grant 665667.
CR Aksoylar C, 2017, IEEE T INFORM THEORY, V63, P749, DOI 10.1109/TIT.2016.2605122
   Alaoui A. E., 2017, DECODING POOLED DATA
   Alaoui A. E., 2016, DECODING POOLED DATA
   Atia GK, 2012, IEEE T INFORM THEORY, V58, P1880, DOI 10.1109/TIT.2011.2178156
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Csiszar I., 2011, INFORM THEORY CODING
   Du D., 1993, SERIES APPL MATH
   Duchi J. C., 2013, DISTANCE BASED CONTI
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Malyutov M., 1998, RANDOM OPER STOCHAST, V6, P339
   MALYUTOV MB, 1978, MATH NOTES+, V23, P84, DOI 10.1007/BF01104893
   Massey J., 1988, INT WORKSH INF THEOR
   Reeves G, 2013, IEEE T INFORM THEORY, V59, P3451, DOI 10.1109/TIT.2013.2253852
   Reeves G, 2012, IEEE T INFORM THEORY, V58, P3065, DOI 10.1109/TIT.2012.2184848
   Scarlett J., 2016, P ACM SIAM S DISC AL
   Scarlett J., 2017, IEEE INT C AC SP SIG
   Scarlett J, 2017, IEEE T INFORM THEORY, V63, P593, DOI 10.1109/TIT.2016.2606605
   Scarlett J, 2016, IEEE T SIGNAL INF PR, V2, P625, DOI 10.1109/TSIPN.2016.2596439
   SEBO A, 1985, J STAT PLAN INFER, V11, P23, DOI 10.1016/0378-3758(85)90022-9
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Wang I. -H., 2016, ALL C COMM CONTR COM
   Wang IH, 2016, IEEE INT SYMP INFO, P1386, DOI 10.1109/ISIT.2016.7541526
   Yeung RW, 2017, IEEE INT SYMP INFO
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400036
DA 2019-06-15
ER

PT S
AU Schroecker, Y
   Isbell, C
AF Schroecker, Yannick
   Isbell, Charles
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI State Aware Imitation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID AVERAGE
AB Imitation learning is the study of learning how to act given a set of demonstrations provided by a human expert. It is intuitively apparent that learning to take optimal actions is a simpler undertaking in situations that are similar to the ones shown by the teacher. However, imitation learning approaches do not tend to use this insight directly. In this paper, we introduce State Aware Imitation Learning (SAIL), an imitation learning algorithm that allows an agent to learn how to remain in states where it can confidently take the correct action and how to recover if it is lead astray. Key to this algorithm is a gradient learned using a temporal difference update rule which leads the agent to prefer states similar to the demonstrated states. We show that estimating a linear approximation of this gradient yields similar theoretical guarantees to online temporal difference learning approaches and empirically show that SAIL can effectively be used for imitation learning in continuous domains with non-linear function approximators used for both the policy representation and the gradient estimate.
C1 [Schroecker, Yannick; Isbell, Charles] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
RP Schroecker, Y (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM yannickschroecker@gatech.edu; isbell@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU Office of Naval Research [N000141410003]
FX This work was supported by the Office of Naval Research under grant
   N000141410003
CR Boularias Abdeslam, 2011, INT C ART INT STAT A, V15, P1
   Brockman G., 2016, OPENAI GYM
   Chernova S., 2014, SYNTHESIS LECT ARTIF, V8, P1, DOI DOI 10.2200/S00568ED1V01Y201402AIM028
   Choi Jaedeug, 2011, NEURAL INFORM PROCES
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Finn C., 2016, INT C MACH LEARN ICM
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Ha David, 2016, ARXIV160909106V4CSLG
   Hester Todd, 2017, 170403732V1CSAI ARXI
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Ho Jonathan, 2016, JMLR WORKSHOP C P, P2760
   Jaderberg Max, 2016, 160805343V1CSLG ARXI
   Klein Edouard, 2013, JOINT EUR C MACH LEA
   Klein Edouard, 2012, NEURAL INFORM PROCES
   Morimura T, 2010, NEURAL COMPUT, V22, P342, DOI 10.1162/neco.2009.12-08-922
   Munos Remi, 2016, NEURAL INFORM PROCES
   Ng Andrew, 2000, INT C MACH LEARN ICM
   Pomerleau Deana, 1989, NEURAL INFORM PROCES
   Ross S., 2011, INT C ART INT STAT A
   Ross Stephane, 2010, INT C ART INT STAT A
   Schaal Stefan, 1997, NEURAL INFORM PROCES
   Schmidhuber Juergen H., 1993, INT C ART NEUR NETW
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton RS, 2016, J MACH LEARN RES, V17
   Tsitsiklis JN, 2002, MACH LEARN, V49, P179, DOI 10.1023/A:1017980312899
   Tsitsiklis JN, 1999, AUTOMATICA, V35, P1799, DOI 10.1016/S0005-1098(99)00099-0
   Ziebart Brian D, 2007, AAAI C ART INT AAAI
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402094
DA 2019-06-15
ER

PT S
AU Schutt, KT
   Kindermans, PJ
   Sauceda, HE
   Chmiela, S
   Tkatchenko, A
   Muller, KR
AF Schuett, K. T.
   Kindermans, P-J
   Sauceda, H. E.
   Chmiela, S.
   Tkatchenko, A.
   Mueller, K-R
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI SchNet: A continuous-filter convolutional neural network for modeling
   quantum interactions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantumchemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.
C1 [Schuett, K. T.; Kindermans, P-J; Chmiela, S.; Mueller, K-R] Tech Univ Berlin, Machine Learning Grp, Berlin, Germany.
   [Sauceda, H. E.] Max Planck Gesell, Fritz Haber Inst, Theory Dept, Berlin, Germany.
   [Tkatchenko, A.] Univ Luxembourg, Phys & Mat Sci Res Unit, Luxembourg, Luxembourg.
   [Mueller, K-R] Max Planck Inst Informat, Saarbrucken, Germany.
   [Mueller, K-R] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
RP Schutt, KT (reprint author), Tech Univ Berlin, Machine Learning Grp, Berlin, Germany.
EM kristof.schuett@tu-berlin.de; klaus-robert.mueller@tu-berlin.de
FU Federal Ministry of Education and Research (BMBF) [01IS14013A]; DFG [MU
   987/20-1]; European Union [657679]; Korean National Research Foundation
   [2012-005741]; Korea government [2017-0-00451]
FX This work was supported by the Federal Ministry of Education and
   Research (BMBF) for the Berlin Big Data Center BBDC (01IS14013A).
   Additional support was provided by the DFG (MU 987/20-1) and from the
   European Union's Horizon 2020 research and innovation program under the
   Marie Sklodowska-Curie grant agreement NO 657679. K.R.M. gratefully
   acknowledges the BK21 program funded by Korean National Research
   Foundation grant (No. 2012-005741) and the Institute for Information &
   Communications Technology Promotion (IITP) grant funded by the Korea
   government (no. 2017-0-00451).
CR Bartok AP, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.184115
   Bartok AP, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.136403
   Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401
   Behler J, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3553717
   Blum LC, 2009, J AM CHEM SOC, V131, P8732, DOI 10.1021/ja902302h
   Boomsma W., 2017, ADV NEURAL INFORM PR, P3436
   Brockherde F, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00839-3
   Bruna J., 2014, ICLR
   Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015
   Chollet F, 2016, ARXIV161002357
   Clevert D.A., 2015, ARXIV151107289
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Eickenberg M., 2017, ADV NEURAL INFORM PR, V30, P6543
   Faber F., 2017, ARXIV170205532
   Gilmer J, 2017, P 34 INT C MACH LEAR, V34, P1263
   Hansen K, 2015, J PHYS CHEM LETT, V6, P2326, DOI 10.1021/acs.jpclett.5b00831
   Hansen K, 2013, J CHEM THEORY COMPUT, V9, P3404, DOI 10.1021/ct400195d
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Henaff Mikael, 2015, ARXIV150605163
   Hirn M, 2017, MULTISCALE MODEL SIM, V15, P827, DOI 10.1137/16M1075454
   Jia Xu, 2016, ADV NEURAL INFORM PR, V29, P667
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kearnes S, 2016, J COMPUT AID MOL DES, V30, P595, DOI 10.1007/s10822-016-9938-8
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Malshe R., 2009, J CHEM PHYS, V130
   Manzhos S, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2336223
   Masci J., 2015, P IEEE INT C COMP VI, P37
   Max-Moerbeck W, 2014, MON NOT R ASTRON SOC, V445, P437, DOI 10.1093/mnras/stu1707
   Montavon G, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/9/095003
   Nieto-Barajas LE, 2015, STOCH ENV RES RISK A, V29, P577, DOI 10.1007/s00477-014-0894-3
   Olafsdottir KB, 2016, COMPUT GEOSCI-UK, V91, P11, DOI 10.1016/j.cageo.2016.03.001
   Pukrittayakamee A, 2009, J CHEM PHYS, V130, DOI 10.1063/1.3095491
   Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22
   Reymond JL, 2015, ACCOUNTS CHEM RES, V48, P722, DOI 10.1021/ar500432k
   Rupp M, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.058301
   Schutt KT, 2014, PHYS REV B, V89, DOI 10.1103/PhysRevB.89.205118
   Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890
   Snyder JC, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.253002
   Van Den Oord A., 2016, 9 ISCA SPEECH SYNTH, P125
NR 41
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401004
DA 2019-06-15
ER

PT S
AU Schulam, P
   Saria, S
AF Schulam, Peter
   Saria, Suchi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Reliable Decision Support using Counterfactual Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CAUSAL; INFERENCE
AB Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and "what if?" reasoning for individualized treatment planning.
C1 [Schulam, Peter; Saria, Suchi] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21211 USA.
RP Schulam, P (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21211 USA.
EM pschulam@cs.jhu.edu; ssaria@cs.jhu.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA YFA [D17AP00014]; NSF SCH [1418590]; NSF Graduate Research
   Fellowship
FX We thank the anonymous reviewers for their insightful feedback. This
   work was supported by generous funding from DARPA YFA #D17AP00014 and
   NSF SCH #1418590. PS was also supported by an NSF Graduate Research
   Fellowship. We thank Katie Henry and Andong Zhan for help with the ICU
   data set. We also thank Miguel Hernan for pointing us to earlier work by
   James Robins on treatment-confounder feedback.
CR Alaa A. M., 2016, ICML WORKSH COMP FRA
   Arjas E, 2004, SCAND J STAT, V31, P171, DOI 10.1111/j.1467-9469.2004.02-134.x
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Brodersen KH, 2015, ANN APPL STAT, V9, P247, DOI 10.1214/14-AOAS788
   Caruana R, 2015, P 21 ACM SIGKDD INT, P1721, DOI [DOI 10.1145/2783258.2788613, 10.1145/2783258.2788613]
   Cheng L. F., 2017, ARXIV170309112
   Cunningham J., 2012, INT C ART INT STAT A, P255
   Daley D. J., 2007, INTRO THEORY POINT P
   Doroudi  Shayan, 2017, UNCERTAINTY ARTIFICI
   Dudik M., 2011, INT C MACH LEARN ICM
   Dyagilev K, 2016, MACH LEARN, V102, P323, DOI 10.1007/s10994-015-5527-7
   Gong M., 2016, INT C MACH LEARN ICM
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Jiang N., 2016, P 33 INT C MACH LEAR, P652
   Johansson F., 2016, INT C MACH LEARN ICM
   Lehman LWH, 2015, IEEE J BIOMED HEALTH, V19, P1068, DOI 10.1109/JBHI.2014.2330827
   Lok JJ, 2008, ANN STAT, V36, P1464, DOI 10.1214/009053607000000820
   Mooij J. M., 2013, ORDINARY DIFFERENTIA
   Morgan SL, 2015, ANAL METHOD SOC RES, P1
   Murphy SA, 2003, J R STAT SOC B, V65, P331, DOI 10.1111/1467-9868.00389
   Nahum-Shani I, 2012, PSYCHOL METHODS, V17, P478, DOI 10.1037/a0029373
   Neyman J., 1923, ROCZNIKI NAUK ROLNIC, V10, P1
   Neyman J. S., 1990, STAT SCI, V5, P465, DOI DOI 10.2307/224538210.1214/SS/1177012031
   Ng AY, 2006, SPRINGER TRAC ADV RO, V21, P363
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Paduraru C., 2012, WORKSH REINF LEARN, P89
   Pearl J, 2009, CAUSALITY MODELS REA
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ribeiro MT, 2016, P 22 ACM SIGKDD INT, P1135, DOI DOI 10.1145/2939672.2939778
   ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6
   ROBINS J, 1992, BIOMETRIKA, V79, P321, DOI 10.2307/2336843
   Robins J., 1997, LECT NOTES STAT, V120, P69, DOI DOI 10.1007/978-1-4612-1842-5
   Robins J M, 2000, STAT MODELS EPIDEMIO, P1, DOI DOI 10.1007/978-1-4612-1284-3_
   Robins JM, 2009, CH CRC HANDB MOD STA, P553
   RUBIN DB, 1978, ANN STAT, V6, P34, DOI 10.1214/aos/1176344064
   Saeed M, 2011, CRIT CARE MED, V39, P952, DOI 10.1097/CCM.0b013e31820a92c6
   Scharfstein D, 2014, STAT BIOPHARM RES, V6, P338, DOI 10.1080/19466315.2014.966920
   Schulam P, 2015, ADV NEURAL INFORM PR, P748
   Sokol A, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-2891
   Soleimani Hossein, 2017, UNCERTAINTY ARTIFICI
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Swaminathan A., 2015, INT C MACH LEARN ICM
   Taubman SL, 2009, INT J EPIDEMIOL, V38, P1599, DOI 10.1093/ije/dyp192
   TAYLOR JMG, 1994, J AM STAT ASSOC, V89, P727
   Wiens J, 2016, J MACH LEARN RES, V17
   Xu Y, 2016, J MACHINE LEARNING R, V56, P282
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401071
DA 2019-06-15
ER

PT S
AU Schwartz, I
   Schwing, AG
   Hazan, T
AF Schwartz, Idan
   Schwing, Alexander G.
   Hazan, Tamir
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI High-Order Attention Models for Visual Question Answering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.
C1 [Schwartz, Idan] Technion, Dept Comp Sci, Haifa, Israel.
   [Schwing, Alexander G.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL USA.
   [Hazan, Tamir] Technion, Dept Ind Engn & Management, Haifa, Israel.
RP Schwartz, I (reprint author), Technion, Dept Comp Sci, Haifa, Israel.
EM idansc@cs.technion.ac.il; aschwing@illinois.edu; tamir.hazan@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU Israel Science Foundation [948/15]; National Science Foundation
   [1718221]
FX This research was supported in part by The Israel Science Foundation
   (grant No. 948/15). This material is based upon work supported in part
   by the National Science Foundation under Grant No. 1718221. We thank
   Nvidia for providing GPUs used in this research.
CR Andreas J., 2016, ARXIV160101705
   Antol S., 2015, ICCV
   Bahdanau D., 2014, ARXIV14090473
   Charikar M., 2002, ICALP
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Das Abhishek, 2016, ARXIV160603556
   Fukui A., 2016, ARXIV160601847
   Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693
   Jabri A., 2016, ECCV
   Jain U., 2017, CVPR
   Kazemi V., 2017, ARXIV170403162
   Kim J. H., 2016, NIPS
   Kim J. H, 2017, ICLR
   Kim Y., 2017, ARXIV170200887
   Lu J, 2016, NIPS
   MA L, 2015, ARXIV150600333
   Malinowski M., 2015, ICCV
   Mostafazadeh N., 2016, ARXIV160306059
   Nam  Hyeonseob, 2016, ARXIV161100471
   Noh H., 2016, ARXIV160603647
   Pham Ninh, 2013, SIGKDD
   Rocktaschel Tim, 2016, ICLR
   Shih K. J, 2016, CVPR
   Xiong C, 2016, ARXIV160301417
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, ICML
   Yang Z, 2016, CVPR
   Yin W, 2015, ARXIV151205193
   Zhu Y., 2016, CVPR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403071
DA 2019-06-15
ER

PT S
AU Scieur, D
   Bach, F
   d'Aspremont, A
AF Scieur, Damien
   Bach, Francis
   d'Aspremont, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Nonlinear Acceleration of Stochastic Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.
C1 [Scieur, Damien; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France.
   [d'Aspremont, Alexandre] PSL Res Univ, CNRS, ENS, Paris, France.
RP Scieur, D (reprint author), PSL Res Univ, ENS, INRIA, Paris, France.
EM damien.scieur@inria.fr; francis.bach@inria.fr; aspremon@ens.fr
RI Jeong, Yongwook/N-7413-2016
FU European Research Council (ERC project SIPA); European Union's Seventh
   Framework Programme (FP7-PEOPLE-2013-ITN) [607290 SpaRTaN]; chaire
   Economie des nouvelles donnees with the data science joint research
   initiative; fonds AXA pour la recherche
FX The authors would like to acknowledge support from a starting grant from
   the European Research Council (ERC project SIPA), from the European
   Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant
   agreement number 607290 SpaRTaN, as well as support from the chaire
   Economie des nouvelles donnees with the data science joint research
   initiative with the fonds AXA pour la recherche and a gift from Societe
   Generale Cross Asset Quantitative Research.
CR Allen-Zhu Z, 2016, ARXIV160305953
   ANDERSON DG, 1965, J ACM, V12, P547, DOI 10.1145/321296.321305
   CABAY S, 1976, SIAM J NUMER ANAL, V13, P734, DOI 10.1137/0713060
   Defazio Aaron, 2014, ADV NEURAL INFORM PR, P1646
   Defossez Alexandre, 2015, P 18 INT C ART INT S, P205
   Fercoq Olivier, 2016, ARXIV160907358
   Flammarion N., 2015, P 28 C LEARN THEOR C, P658
   Jain P., 2016, ARXIV161003774
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lin H., 2015, ADV NEURAL INFORM PR, P3384
   Mesina M., 1977, Computer Methods in Applied Mechanics and Engineering, V10, P165, DOI 10.1016/0045-7825(77)90004-4
   Moulines  E., 2011, ADV NEURAL INFORM PR, P451
   Nedic A, 2001, APPL OPTIMIZAT, V54, P223
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6
   Scieur D., 2016, ADV NEURAL INFORM PR, P712
   Shalev-Shwartz S., 2014, P 31 INT C MACH LEAR, P64
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Varga R. S., 1961, NUMER MATH, V3, P147
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404006
DA 2019-06-15
ER

PT S
AU Scieur, D
   Roulet, V
   Bach, F
   d'Aspremont, A
AF Scieur, Damien
   Roulet, Vincent
   Bach, Francis
   d'Aspremont, Alexandre
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Integration Methods and Optimization Algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. Compared with recent advances in this vein, the differential equation considered here is the basic gradient flow, and we derive a class of multi-step schemes which includes accelerated algorithms, using classical conditions from numerical analysis. Multi-step schemes integrate the differential equation using larger step sizes, which intuitively explains the acceleration phenomenon.
C1 [Scieur, Damien; Roulet, Vincent; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France.
   [d'Aspremont, Alexandre] PSL Res Univ, CNRS, ENS, Paris, France.
RP Scieur, D (reprint author), PSL Res Univ, ENS, INRIA, Paris, France.
EM damien.scieur@inria.fr; vincent.roulet@inria.fr; francis.bach@inria.fr;
   aspremon@ens.fr
FU European Research Council (ERC project SIPA); European Union [607290
   SpaRTaN]; AMX fellowship; chaire Economie des nouvelles donnees; fonds
   AXA pour la recherche
FX The authors would like to acknowledge support from a starting grant from
   the European Research Council (ERC project SIPA), from the European
   Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant
   agreement number 607290 SpaRTaN, an AMX fellowship, as well as support
   from the chaire Economie des nouvelles donnees with the data science
   joint research initiative with the fonds AXA pour la recherche and a
   gift from Societe Generale Cross Asset Quantitative Research.
CR Allen Zhu Z., 2017, P 8 INN THEOR COMP S
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Ben-Tal A., 2001, LECT MODERN CONVEX O
   Bubeck Sebastien, 2015, ARXIV E PRINTS
   Diakonikolas J., 2017, ARXIV170604680
   Duchi J. C., 2010, P 23 ANN C LEARN THE, P14
   Gautschi W., 2011, NUMERICAL ANAL
   Krichene W, 2015, ADV NEURAL INFORM PR, V28, P2845
   Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov  Y., 2007, GRADIENT METHODS MIN
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   Suli E, 2003, INTRO NUMERICAL ANAL
   Taylor A., 2017, THESIS
   Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113
   Wilson Ashia C, 2016, ARXIV161102635
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401015
DA 2019-06-15
ER

PT S
AU Sen, R
   Suresh, AT
   Shanmugam, K
   Dimakis, AG
   Shakkottai, S
AF Sen, Rajat
   Suresh, Ananda Theertha
   Shanmugam, Karthikeyan
   Dimakis, Alexandros G.
   Shakkottai, Sanjay
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Model-Powered Conditional Independence Test
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SEARCH
AB We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution f (x, y, z) of continuous random vectors X,Y and Z, we determine whether X perpendicular to Y vertical bar Z. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution f(CI)(x, y, z) = f(x vertical bar z) f(y vertical bar z) f (z) - the joint distribution if and only if X perpendicular to Y vertical bar Z. - when given access only to i.i.d. samples from the true joint distribution f (x, y, z). To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to f(CI) in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d near-independent samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.
C1 [Sen, Rajat; Dimakis, Alexandros G.; Shakkottai, Sanjay] Univ Texas Austin, Austin, TX 78712 USA.
   [Suresh, Ananda Theertha] Google, New York, NY USA.
   [Shanmugam, Karthikeyan] Thomas J Watson Ctr, IBM Res, Yorktown Hts, NY USA.
RP Sen, R (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
RI Jeong, Yongwook/N-7413-2016; Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU NSF [SaTC 1704778, CNS 1320175]; ARO [W911NF-17-1-0359,
   W911NF-16-1-0377]; US DoT
FX This work is partially supported by NSF grants CNS 1320175, NSF SaTC
   1704778, ARO grants W911NF-17-1-0359, W911NF-16-1-0377 and the US DoT
   supported D-STOP Tier 1 University Transportation Center.
CR Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P604, DOI 10.1007/978-3-540-72927-3_43
   Beygelzimer A., 2009, P 25 C UNC ART INT, P51
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Brenner Eliot, 2013, ARXIV13096820
   Chen T, 2016, P 22 ACM SIGKDD INT, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   de Campos LM, 2000, INT J APPROX REASON, V24, P11, DOI 10.1016/S0888-613X(99)00042-0
   Doran G, 2014, P 30 C UNC ART INT, P132
   Fukumizu K, 2004, J MACH LEARN RES, V5, P73
   Gao W., 2016, ADV NEURAL INFORM PR, P2460
   Gao W., 2016, ARXIV160403006
   Jie Cheng, 1998, LEARNING BAYESIAN NE
   Kalisch M, 2007, J MACH LEARN RES, V8, P613
   Koller D, 1996, TECHNICAL REPORT
   Koller D., 2009, PROBABILISTIC GRAPHI
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kun Zhang, 2012, ARXIV12023775
   Langford John, 2003, P MACH LEARN RED WOR
   Lopez-Paz D, 2016, ARXIV161006545
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Mohri M., 2009, ADV NEURAL INFORM PR, P1097
   Mooij Joris, 2013, ARXIV13096849
   Pearl J., 2009, CAUSALITY
   RAMASUBRAMANIAN V, 1992, IEEE T SIGNAL PROCES, V40, P518, DOI 10.1109/78.120795
   Roos B, 1999, J MULTIVARIATE ANAL, V69, P120, DOI 10.1006/jmva.1998.1789
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Spirtes P., 2000, CAUSATION PREDICTION
   Strobl Eric V, 2017, ARXIV170203877
   Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7
   Vapnik V. N., 2015, MEASURES COMPLEXITY, P11, DOI DOI 10.1007/978-3-319-21852-6_3
   Xing E, 2001, P 18 INT C MACH LEAR, P601
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403002
DA 2019-06-15
ER

PT S
AU Seo, PH
   Lehrmann, A
   Han, B
   Sigal, L
AF Seo, Paul Hongsuck
   Lehrmann, Andreas
   Han, Bohyung
   Sigal, Leonid
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Visual Reference Resolution using Attention Memory for Visual Dialog
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves the previous attention, taking into account recency, which is most relevant for the current question, in order to resolve potentially ambiguous references. The model then merges the retrieved attention with a tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by approximate to 16 % points) in situations, where visual reference resolution plays an important role. Moreover, the proposed model achieves superior performance ( approximate to 2 % points improvement) in the Visual Dialog dataset [1], despite having significantly fewer parameters than the baselines.
C1 [Seo, Paul Hongsuck; Han, Bohyung] POSTECH, Pohang, South Korea.
   [Lehrmann, Andreas; Sigal, Leonid] Disney Res, Los Angeles, CA USA.
RP Seo, PH (reprint author), POSTECH, Pohang, South Korea.
EM hsseo@postech.ac.kr; andreas.lehrmann@disneyresearch.com;
   bhhan@postech.ac.kr; lsigal@disneyresearch.com
RI Jeong, Yongwook/N-7413-2016
FU IITP grant - Korea government (MSIT) [2017-0-01778, 2017-0-01780,
   2016-0-00563]
FX This work was supported in part by the IITP grant funded by the Korea
   government (MSIT) [2017-0-01778, Development of Explainable Human-level
   Deep Machine Learning Inference Framework; 2017-0-01780, The Technology
   Development for Event Recognition/Relational Reasoning and Learning
   Knowledge based System for Video Understanding; 2016-0-00563, Research
   on Adaptive Machine Learning Technology Development for Intelligent
   Autonomous Digital Companion].
CR Andreas J., 2016, CVPR
   Antol S., 2015, ICCV
   Clark K., 2015, ACL
   Clark K., 2016, ACL
   Clark K., 2016, EMNLP
   Das A., 2017, CVPR
   Das Abhishek, 2017, ARXIV170306585
   de Vries Harm, 2017, CVPR
   Deng J., 2009, CVPR
   Fukui A., 2016, EMNLP
   Goyal Yash, 2017, CVPR
   Huang D.-A., 2017, CVPR
   Kim J. H, 2017, ICLR
   Kingma D. P., 2014, ARXIV14126980
   Kumar A., 2016, ICML
   Lin T.-Y., 2014, ECCV
   Lu J, 2016, NIPS
   Malinowski M., 2015, ICCV
   Mansimov E., 2016, ICLR
   Miller A., 2016, EMNLP
   Mun J., 2016, AAAI
   Mun J., 2016, RXIV161201669
   Noh H., 2016, ARXIV160603647
   Noh H., 2016, CVPR
   Reed S., 2016, ICML
   Rohrbach A., 2016, ECCV
   Seo P.H., 2016, ARXIV160602393
   Simonyan Karen, 2015, ICLR
   Strub Florian, 2017, ARXIV170305423
   Sukhbaatar S., 2015, NIPS
   Tapaswi  Makarand, 2016, CVPR
   Vinyals O, 2015, CVPR
   Weston Jason, 2015, ICLR
   Xiong C, 2016, ICML
   Xu H., 2016, ECCV
   Xu K, 2015, ICML
   Yang Z, 2016, CVPR
   Zhang P., 2016, CVPR
   Zhu L., 2015, ARXIV151104670
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403076
DA 2019-06-15
ER

PT S
AU Sharan, V
   Kakade, S
   Liang, P
   Valiant, G
AF Sharan, Vatsal
   Kakade, Sham
   Liang, Percy
   Valiant, Gregory
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Overcomplete HMMs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID IDENTIFIABILITY; PARAMETERS; MODELS
AB We study the problem of learning overcomplete HMMs-those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results-both positive and negative-which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.
C1 [Sharan, Vatsal; Liang, Percy; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA.
   [Kakade, Sham] Univ Washington, Seattle, WA 98195 USA.
RP Sharan, V (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM vsharan@stanford.edu; sham@cs.washington.edu; pliang@cs.stanford.edu;
   valiant@stanford.edu
CR Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689
   Anandkumar A., 2015, COLT, P36
   Anandkumar A., 2013, TENSOR DECOMPOSITION
   Anandkumar Animashree, 2012, COLT, V1, P4
   Bhaskara A., 2013, CORR
   Bhaskara A., 2014, P 46 ANN ACM S THEOR, P594
   BLACKWELL D, 1957, ANN MATH STAT, V28, P1011, DOI 10.1214/aoms/1177706802
   BLISCHKE WR, 1964, J AM STAT ASSOC, V59, P510, DOI 10.2307/2283005
   Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2
   Flaxman A, 2004, ELECTRON J COMB, V11
   Friedman J., 2003, P 35 ANN ACM S THEOR, P720
   Ghahramani Z., 1997, MACH LEARN, V1, P31
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Huang QQ, 2016, IEEE T SIGNAL PROCES, V64, P1896, DOI 10.1109/TSP.2015.2510969
   ITO H, 1992, IEEE T INFORM THEORY, V38, P324, DOI 10.1109/18.119690
   Kakade S., 2016, ARXIV161202526
   Krivelevich M, 2001, RANDOM STRUCT ALGOR, V18, P346, DOI 10.1002/rsa.1013
   Kruskal J. B., 1977, LINEAR ALGEBRA ITS A, V18
   LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071
   MOSSEL E., 2005, P 37 ANN ACM S THEOR, P366
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Shamir E., 1984, N HOLLAND MATH STUDI, V87, P271
   Weiss R., 2015, ICML, P635
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400090
DA 2019-06-15
ER

PT S
AU Shen, J
   Li, P
AF Shen, Jie
   Li, Ping
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Partial Hard Thresholding: Towards A Principled Analysis of Support
   Recovery
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ORTHOGONAL MATCHING PURSUIT; SIGNAL RECOVERY; SPARSE RECOVERY;
   SELECTION; CONSISTENCY; REGRESSION; ALGORITHM
AB In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(s kappa log kappa) iterations where kappa is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.
C1 [Shen, Jie] Rutgers State Univ, Sch Arts & Sci, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, Piscataway, NJ USA.
RP Shen, J (reprint author), Rutgers State Univ, Sch Arts & Sci, Dept Comp Sci, Piscataway, NJ 08854 USA.
EM js2007@rutgers.edu; pingli@stat.rutgers.edu
FU  [NSF-Bigdata-1419210];  [NSF-III-1360971]
FX The work is supported in part by NSF-Bigdata-1419210 and
   NSF-III-1360971. We thank the anonymous reviewers for valuable comments.
CR Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032
   Bahmani S, 2013, J MACH LEARN RES, V14, P807
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Bouchot JL, 2016, APPL COMPUT HARMON A, V41, P412, DOI 10.1016/j.acha.2016.03.002
   Cai TT, 2011, IEEE T INFORM THEORY, V57, P4680, DOI 10.1109/TIT.2011.2146090
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Foucart S., 2013, APPL NUMERICAL HARMO
   Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278
   Jain P., 2011, P ADV NEUR INF PROC, V24, P1215
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jain P, 2017, IEEE T INFORM THEORY, V63, P3029, DOI 10.1109/TIT.2017.2686880
   Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201
   Shen J., 2016, ARXIV160501656
   Shen  Jie, 2017, INT C MACH LEARN, P3115
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Wang J, 2016, IEEE T SIGNAL PROCES, V64, P1076, DOI 10.1109/TSP.2015.2498132
   Yuan M, 2007, J ROY STAT SOC B, V69, P143, DOI 10.1111/j.1467-9868.2007.00581.x
   Yuan X.T., 2014, P 31 INT C MACH LEAR, P127
   Yuan  Xiaotong, 2016, P 30 ANN C NEUR INF, V29, P3558
   Zang T, 2009, ANN STAT, V37, P2109, DOI 10.1214/08-AOS659
   Zhang T, 2011, IEEE T INFORM THEORY, V57, P6215, DOI 10.1109/TIT.2011.2162263
   Zhang T, 2009, J MACH LEARN RES, V10, P555
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403019
DA 2019-06-15
ER

PT S
AU Shen, TX
   Lei, T
   Barzilay, R
   Jaakkola, T
AF Shen, Tianxiao
   Lei, Tao
   Barzilay, Regina
   Jaakkola, Tommi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Style Transfer from Non-Parallel Text by Cross-Alignment
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.
C1 [Shen, Tianxiao; Barzilay, Regina; Jaakkola, Tommi] MIT CSAIL, Cambridge, MA 02139 USA.
   [Lei, Tao] ASAPP Inc, New York, NY USA.
RP Shen, TX (reprint author), MIT CSAIL, Cambridge, MA 02139 USA.
EM tianxiao@csail.mit.edu; tao@asapp.com; regina@csail.mit.edu;
   tommi@csail.mit.edu
RI Jeong, Yongwook/N-7413-2016
FU MIT Lincoln Laboratory
FX We thank Nicholas Matthews for helping to facilitate human evaluations,
   and Zhiting Hu for sharing his code. We also thank Jonas Mueller, Arjun
   Majumdar, Olga Simek, Danelle Shah, MIT NLP group and the reviewers for
   their helpful comments. This work was supported by MIT Lincoln
   Laboratory.
CR Brown P. F., 1990, Computational Linguistics, V16, P79
   Che T., 2017, ARXIV170207983
   Chen X., 2016, ADV NEURAL INFORM PR
   Dou Q., 2012, P 2012 JOINT C EMP M, P266
   GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goyal Kartik, 2017, ARXIV170406970
   Hjelm RD, 2017, ARXIV170208431
   Hu Zhiting, 2017, ARXIV170300955
   Isola  P., 2016, ARXIV161107004
   Jang Eric, 2016, ARXIV161101144
   Kim T, 2017, ARXIV170305192
   Kim Y., 2014, ARXIV14085882
   Kingma D.P., 2013, ARXIV13126114
   Klein G., 2017, ARXIV170102810
   Kusner M. J., 2016, ARXIV161104051
   Lamb A. M., 2016, ADV NEURAL INFORM PR, P4601
   Lantao Yu, 2016, ARXIV160905473
   Liu M.-Y., 2016, ADV NEURAL INFORM PR, P469
   Liu Ming Yu, 2017, ARXIV170300848
   Maddison Chris J, 2016, ARXIV161100712
   Makhzani A., 2015, ARXIV151105644
   Mueller Jonas, 2017, INT C MACH LEARN ICM
   Nuhn Malte, 2013, ANN M ASS COMP LING, P615
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Schmaltz Allen, 2016, P 2016 C EMP METH NA, P2319
   Taigman Yaniv, 2016, ARXIV161102200
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Yi Z., 2017, ARXIV170402510
   Zhu J Y, 2017, ARXIV170310593
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406086
DA 2019-06-15
ER

PT S
AU Shen, W
   Zhao, K
   Guo, YL
   Yuille, A
AF Shen, Wei
   Zhao, Kai
   Guo, Yilu
   Yuille, Alan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Label Distribution Learning Forests
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID FACIAL AGE ESTIMATION
AB Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.
C1 [Shen, Wei; Zhao, Kai; Guo, Yilu] Shanghai Univ, Sch Commun & Informat Engn, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China.
   [Shen, Wei; Yuille, Alan] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
RP Shen, W (reprint author), Shanghai Univ, Sch Commun & Informat Engn, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China.; Shen, W (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM shenwei1231@gmail.com; zhaok1206@gmail.com; gyl.luan0@gmail.com;
   alan.l.yuille@gmail.com
RI Jeong, Yongwook/N-7413-2016
FU National Natural Science Foundation of China [61672336]; Shanghai
   Municipal Education Commission; Shanghai Education Development
   Foundation [15CG43]; ONR [N00014-15-1-2356]
FX This work was supported in part by the National Natural Science
   Foundation of China No. 61672336, in part by "Chen Guang" project
   supported by Shanghai Municipal Education Commission and Shanghai
   Education Development Foundation No. 15CG43 and in part by ONR
   N00014-15-1-2356.
CR Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Criminisi A., 2013, DECISION FORESTCOM
   Gao B.-B., 2017, ARXIV161101731
   Geng X., 2010, P AAAI
   Geng X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3511
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2014, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2014.237
   Geng X, 2014, INT C PATT RECOG, P4465, DOI 10.1109/ICPR.2014.764
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Guo G., 2010, IEEE COMP SOC C COMP, P71
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2014, PROC CVPR IEEE, P4257, DOI 10.1109/CVPR.2014.542
   He Z., 2017, IEEE T IMAGE PROCESS
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Jia Y., 2014, ARXIV14085093
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Parkhi O. M., 2015, P BRIT MACHINE VISIO, V1, P6, DOI DOI 10.5244/C.29.41
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Xing C, 2016, PROC CVPR IEEE, P4489, DOI 10.1109/CVPR.2016.486
   Yang X., 2016, P IJCAI, P2259
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhou Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2733373.2806328
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400080
DA 2019-06-15
ER

PT S
AU Sheth, R
   Khardon, R
AF Sheth, Rishit
   Khardon, Roni
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Excess Risk Bounds for the Bayes Risk using Variational Inference in
   Latent Gaussian Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Bayesian models are established as one of the main successful paradigms for complex problems in machine learning. To handle intractable inference, research in this area has developed new approximation methods that are fast and effective. However, theoretical analysis of the performance of such approximations is not well developed. The paper furthers such analysis by providing bounds on the excess risk of variational inference algorithms and related regularized loss minimization algorithms for a large class of latent variable models with Gaussian latent variables. We strengthen previous results for variational algorithms by showing that they are competitive with any point-estimate predictor. Unlike previous work, we provide bounds on the risk of the Bayesian predictor and not just the risk of the Gibbs predictor for the same approximate posterior. The bounds are applied in complex models including sparse Gaussian processes and correlated topic models. Theoretical results are complemented by identifying novel approximations to the Bayesian objective that attempt to minimize the risk directly. An empirical evaluation compares the variational and new algorithms shedding further light on their performance.
C1 [Sheth, Rishit; Khardon, Roni] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
RP Sheth, R (reprint author), Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
EM rishit.sheth@tufts.edu; roni@cs.tufts.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1714440]
FX This work was partly supported by NSF under grant IIS-1714440.
CR Alquier P, 2016, J MACH LEARN RES, V17
   Banerjee  Arindam, 2006, ICML, P81
   Blei D., 2006, ADV NEURAL INFORM PR, V18, P147
   Boucheron S., 2013, CONCENTRATION INEQUA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Dalalyan A, 2008, MACH LEARN, V72, P39, DOI 10.1007/s10994-008-5051-0
   Germain Pascal, 2016, NIPS, P1876
   Hensman J, 2015, P 18 INT C ART INT S, P351
   Hoffman Matthew D., 2015, P MACH LEARNING RES, P361
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kakade Sham M., 2004, NIPS, P641
   Lacasse A., 2006, NIPS, V19, P769
   Lichman M., 2013, UCI MACHINE LEARNING
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   Meir R., 2003, J MACHINE LEARNING R, V4, P839
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Sheth Rishit, 2015, P 32 INT C MACH LEAR, P1302
   Sheth Rishit, 2016, ARXIV13096835
   Sheth Rishit, 2016, AISTATS, P761
   Snelson E, 2006, ADV NEURAL INF PROCE, P1257
   Teh Y.W., 2006, ADV NEURAL INFORM PR, P1353
   Titsias M, 2009, ARTIF INTELL, P567
   WANG SD, 1986, IEEE T AUTOMAT CONTR, V31, P654
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405023
DA 2019-06-15
ER

PT S
AU Shi, XJ
   Gao, ZH
   Lausen, L
   Wang, H
   Yeung, DY
   Wong, WK
   Woo, WC
AF Shi, Xingjian
   Gao, Zhihan
   Lausen, Leonard
   Wang, Hao
   Yeung, Dit-Yan
   Wong, Wai-kin
   Woo, Wang-chun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.
C1 [Shi, Xingjian; Gao, Zhihan; Lausen, Leonard; Wang, Hao; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Wong, Wai-kin; Woo, Wang-chun] Hong Kong Observ, Hong Kong, Peoples R China.
RP Shi, XJ (reprint author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM xshiab@cse.ust.hk; zgaoag@cse.ust.hk; lelausen@cse.ust.hk;
   hwangaz@cse.ust.hk; dyyeung@cse.ust.hk; wkwong@hko.gov.hk;
   wcwoo@hko.gov.hk
RI Jeong, Yongwook/N-7413-2016
FU General Research Fund from the Research Grants Council [16207316];
   Innovation and Technology Fund from Innovation and Technology Commission
   in Hong Kong [ITS/205/15FP]; Hong Kong PhD Fellowship
FX This research has been supported by General Research Fund 16207316 from
   the Research Grants Council and Innovation and Technology Fund
   ITS/205/15FP from the Innovation and Technology Commission in Hong Kong.
   The first author has also been supported by the Hong Kong PhD
   Fellowship.
CR Alahi  A., 2016, CVPR
   [Anonymous], 2017, CVPR
   Ballas N., 2016, ICLR
   De Brabandere  B., 2016, NIPS
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Finn Chelsea, 2016, NIPS
   Goodfellow I., 2014, NIPS
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hogan RJ, 2010, WEATHER FORECAST, V25, P710, DOI 10.1175/2009WAF2222350.1
   Jaderberg M., 2015, NIPS
   Jain A., 2016, CVPR
   Jeon Yunho, 2017, CVPR
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.1093/biomet/30.1-2.81
   Kingma D. P., 2015, ICLR
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LEE H, 2017, ATMOSPHERE-BASEL, V8, DOI DOI 10.3390/ATMOS8010011
   Liang  X., 2017, CVPR
   Maas A. L., 2013, ICML
   MARSHALL JS, 1948, J METEOROL, V5, P165, DOI 10.1175/1520-0469(1948)005<0165:TDORWS>2.0.CO;2
   Mathieu M., 2016, ICLR
   Perazzi  F., 2016, CVPR
   Ranzato M, 2014, ARXIV14126604
   Shi X., 2015, NIPS
   Srivastava N, 2015, ICML
   Sun JZ, 2014, B AM METEOROL SOC, V95, P409, DOI 10.1175/BAMS-D-11-00263.1
   Villegas R., 2017, ICLR
   Vondrick C., 2016, NIPS
   Woo WC, 2017, ATMOSPHERE-BASEL, V8, DOI 10.3390/atmos8030048
   Wu Y., 2013, CVPR
   Yu F., 2016, ICLR
NR 30
TC 0
Z9 0
U1 4
U2 4
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405068
DA 2019-06-15
ER

PT S
AU Shi, Z
   Zhang, XH
   Yu, YL
AF Shi, Zhan
   Zhang, Xinhua
   Yu, Yaoliang
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and
   Adversarial Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MINIMIZATION
AB Adversarial machines, where a learner competes against an adversary, have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization, often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics, and then we adapt the new stochastic variance-reduced algorithm of Balamurugan & Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.
C1 [Shi, Zhan; Zhang, Xinhua] Univ Illinois, Chicago, IL 60661 USA.
   [Yu, Yaoliang] Univ Waterloo, Waterloo, ON N2L 3G1, Canada.
RP Shi, Z (reprint author), Univ Illinois, Chicago, IL 60661 USA.
EM zshi22@uic.edu; zhangx@uic.edu; yaoliang.yu@uwaterloo.ca
CR Asif K., 2015, UAI
   Babanezhad R., 2015, NIPS
   Balamurugan P., 2016, NIPS
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10
   Defazio A., 2014, NIPS
   Defazio Aaron J., 2014, ICML
   Duchi J. C., 2010, P ANN C COMP LEARN T
   Farnia F., 2016, NIPS
   Goodfellow I., 2014, NIPS
   Johnson R., 2013, NIPS
   Lacoste-Julien S., 2013, ICML
   Lin H., 2015, NIPS
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y, 2005, SIAM J OPTIMIZ, V16, P235, DOI 10.1137/S1052623403422285
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nitanda A., 2014, NIPS
   Rockafellar R. T., 1970, NONLINEAR ANAL 1, V18, P397
   Schmidt  M., 2016, MATH PROGRAMMING
   Shalev-Shwartz S., 2014, ICML
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev- Shwartz Shai, 2016, ICML
   Tseng P., 2009, SIAM J OPTIMIZ UNPUB
   Wang H., 2014, NIPS
   Wang H., 2015, NIPS
   Warmuth MK, 2008, LECT NOTES ARTIF INT, V5254, P256, DOI 10.1007/978-3-540-87987-9_23
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang Yuchen, 2015, ICML
   Zhu ZX, 2015, LECT NOTES ARTIF INT, V9284, P645, DOI 10.1007/978-3-319-23528-8_40
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406011
DA 2019-06-15
ER

PT S
AU Shim, K
   Lee, M
   Choi, I
   Boo, Y
   Sung, W
AF Shim, Kyuhong
   Lee, Minjae
   Choi, Iksoo
   Boo, Yoonho
   Sung, Wonyong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a fast approximation method of a softmax function with a very large vocabulary using singular value decomposition (SVD). SVD-softmax targets fast and accurate probability estimation of the topmost probable words during inference of neural network language models. The proposed method transforms the weight matrix used in the calculation of the output vector by using SVD. The approximate probability of each word can be estimated with only a small part of the weight matrix by using a few large singular values and the corresponding elements for most of the words. We applied the technique to language modeling and neural machine translation and present a guideline for good approximation. The algorithm requires only approximately 20% of arithmetic operations for an 800K vocabulary case and shows more than a three-fold speedup on a GPU.
C1 [Shim, Kyuhong; Lee, Minjae; Choi, Iksoo; Boo, Yoonho; Sung, Wonyong] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
RP Shim, K (reprint author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
EM skhu20@snu.ac.kr; mjlee@dsp.snu.ac.kr; ischoi@dsp.snu.ac.kr;
   yhboo@dsp.snu.ac.kr; wysung@snu.ac.kr
RI Jeong, Yongwook/N-7413-2016
FU Brain Korea 21 Plus Project; National Research Foundation of Korea (NRF)
   - Korea government (MSIP) [2015R1A2A1A10056051]
FX This work was supported in part by the Brain Korea 21 Plus Project and
   the National Research Foundation of Korea (NRF) grant funded by the
   Korea government (MSIP) (No.2015R1A2A1A10056051).
CR Andreas Jacob, 2015, ADV NEURAL INFORM PR, P1783
   Bahdanau D., 2014, ARXIV14090473
   BENGIO Y, 2003, AISTATS
   Bojar O., 2015, P 10 WORKSH STAT MAC, P1
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chelba C., 2013, ARXIV13123005
   Chen Welin, 2015, ARXIV151204906
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Dauphin Y.N, 2016, ARXIV161208083
   Devlin J., 2014, LONG PAPERS ASS COMP, P1370, DOI DOI 10.3115/V1/P14-1129
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Grave E., 2016, ARXIV160904309
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hwang K, 2016, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2016.7472696
   Jean  S., 2014, ARXIV14122007
   Ji Shihao, 2015, ARXIV151106909
   Klein G., 2017, ARXIV170102810
   Koehn P, 2005, MT SUMMIT, V5, P79, DOI DOI 10.3115/1626355.1626380
   Koehn P., 2007, P 45 ANN M ACL INT P, P177, DOI DOI 10.3115/1557769.1557821
   Le HS, 2011, INT CONF ACOUST SPEE, P5524
   Liu X., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4908, DOI 10.1109/ICASSP.2014.6854535
   Luong M.T., 2015, ARXIV150804025
   Merity S., 2016, ARXIV160907843
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mnih A., 2009, ADV NEURAL INFORM PR, P1081
   Mnih A., 2012, ARXIV12066426
   Morin F., 2005, P AISTATS, P246
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tiedemann J, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P2214
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405053
DA 2019-06-15
ER

PT S
AU Shin, H
   Lee, JK
   Kim, J
   Kim, J
AF Shin, Hanul
   Lee, Jung Kwon
   Kim, Jaehong
   Kim, Jiwon
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Continual Learning with Deep Generative Replay
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MEMORY; STABILITY
AB Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.
C1 [Shin, Hanul] MIT, Cambridge, MA 02139 USA.
   [Shin, Hanul; Lee, Jung Kwon; Kim, Jaehong; Kim, Jiwon] SK T Brain, Seoul, South Korea.
RP Shin, H (reprint author), MIT, Cambridge, MA 02139 USA.; Shin, H (reprint author), SK T Brain, Seoul, South Korea.
EM skyshin@mit.edu; jklee@sktbrain.com; xhark@sktbrain.com; jk@sktbrain.com
RI Jeong, Yongwook/N-7413-2016
CR Abraham WC, 2005, TRENDS NEUROSCI, V28, P73, DOI 10.1016/j.tins.2004.12.003
   Ans B, 1997, CR ACAD SCI III-VIE, V320, P989, DOI 10.1016/S0764-4469(97)82472-9
   BALDWIN DA, 1993, CHILD DEV, V64, P711, DOI 10.2307/1131213
   Bornstein MH, 2010, DEV PSYCHOL, V46, P350, DOI 10.1037/a0018411
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1
   Fagott J, 2006, P NATL ACAD SCI USA, V103, P17564, DOI 10.1073/pnas.0605184103
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Gelbard-Sagiv H, 2008, SCIENCE, V322, P96, DOI 10.1126/science.1164685
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Goodfellow I. J., 2017, CORR
   Goodfellow I. J., 2013, ARXIV13126211
   Grutzendler J, 2002, NATURE, V420, P812, DOI 10.1038/nature01276
   Gulrajani Ishaan, 2017, ARXIV170400028
   Hattori M, 2014, NEUROCOMPUTING, V134, P262, DOI 10.1016/j.neucom.2013.08.044
   Hinton GE, 1987, P 9 ANN C COGN SCI S, P177
   Kingma D.P., 2013, ARXIV13126114
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee S. -W., 2017, ARXIV170308475
   Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37
   Mccloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P104, DOI DOI 10.1016/S0079-7421(08)60536-8
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mocanu D. C., 2016, CORR
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5
   O'Neill J, 2010, TRENDS NEUROSCI, V33, P220, DOI 10.1016/j.tins.2010.01.006
   O'Reilly RC, 2002, TRENDS COGN SCI, V6, P505, DOI 10.1016/S1364-6613(02)02005-3
   Ramirez S, 2013, SCIENCE, V341, P387, DOI 10.1126/science.1239073
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318
   Robins A., 1993, Proceedings 1993 The First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems, P65, DOI 10.1109/ANNES.1993.323080
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Stickgold R, 2007, SLEEP MED, V8, P331, DOI 10.1016/j.sleep.2007.03.011
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403006
DA 2019-06-15
ER

PT S
AU Siddharth, N
   Paige, B
   van de Meent, JW
   Desmaison, A
   Goodman, ND
   Kohli, P
   Wood, F
   Torr, PHS
AF Siddharth, N.
   Paige, Brooks
   van de Meent, Jan-Willem
   Desmaison, Alban
   Goodman, Noah D.
   Kohli, Pushmeet
   Wood, Frank
   Torr, Philip H. S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Disentangled Representations with Semi-Supervised Deep
   Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.
C1 [Siddharth, N.; Desmaison, Alban; Wood, Frank; Torr, Philip H. S.] Univ Oxford, Oxford, England.
   [Paige, Brooks] Univ Cambridge, Alan Turing Inst, Cambridge, England.
   [van de Meent, Jan-Willem] Northeastern Univ, Boston, MA 02115 USA.
   [Goodman, Noah D.] Stanford Univ, Stanford, CA 94305 USA.
   [Kohli, Pushmeet] Deepmind, London, England.
RP Siddharth, N (reprint author), Univ Oxford, Oxford, England.
EM nsid@robots.ox.ac.uk; bpaige@turing.ac.uk;
   j.vandemeent@northeastern.edu; alban@robots.ox.ac.uk;
   ngoodman@stanford.edu; pushmeet@google.com; fwood@robots.ox.ac.uk;
   philip.torr@eng.ox.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU EPSRC [EP/M013774/1]; ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC/MURI
   [EP/N019474/1]; Alan Turing Institute under the EPSRC [EP/N510129/1];
   DARPA PPAML through the U.S. AFRL [FA8750-14-2-0006]; Northeastern
   University; Intel; DARPA D3M [FA8750-17-2-0093]
FX This work was supported by the EPSRC, ERC grant ERC-2012-AdG
   321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1, and EPSRC/MURI grant
   EP/N019474/1. BP & FW were supported by The Alan Turing Institute under
   the EPSRC grant EP/N510129/1. JWM, FW & NDG were supported under DARPA
   PPAML through the U.S. AFRL under Cooperative Agreement
   FA8750-14-2-0006. JWM was additionally supported through startup funds
   provided by Northeastern University. FW was additionally supported by
   Intel and DARPA D3M, under Cooperative Agreement FA8750-17-2-0093.
CR Berant  J., 2014, EMNLP
   Burda Y., 2015, ARXIV150900519
   Christopher K. I., 2017, P WORKSH LEARN DIS R
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Eslami SM, 2016, ARXIV160308575
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gershman Samuel J, 2014, COGSCI
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodman N., 2008, P 24 C UNC ART INT, V8, P220
   Gregor Karol, 2015, P 32 INT C MACH LEAR, V37, P1462
   Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017
   Jampani Varun, 2015, INT C ART INT STAT, P425
   Jang Eric, 2016, ARXIV161101144
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Johnson Matthew, 2016, ADV NEURAL INFORM PR
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2530
   Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068
   LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157
   LeAnh Tuan, 2016, ARXIV161009900
   Maaloe Lars, 2016, ARXIV160205473
   Maddison Chris J, 2016, ARXIV161100712
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Ritchie D., 2016, ARXIV161005735
   Schulman John, 2015, ADV NEURAL INFORM PR, P3510
   Siddharth N, 2014, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2014.99
   Sohn K., 2015, ADV NEURAL INFORM PR, P3465
   Sonderby Casper Kaae, 2016, ADV NEURAL INFORM PR
   Stuhlmuller A., 2013, ADV NEURAL INFORM PR, P3048
   Wingate D., 2011, P 43 INT C ART INT S, P770
   Wood F., 2014, P 17 INT C ART INT S, P1024
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406001
DA 2019-06-15
ER

PT S
AU Singh, R
   Lanchantin, J
   Sekhon, A
   Qi, YJ
AF Singh, Ritambhara
   Lanchantin, Jack
   Sekhon, Arshdeep
   Qi, Yanjun
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Attend and Predict: Understanding Gene Regulation by Selective Attention
   on Chromatin
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The past decade has seen a revolution in genomic technologies that enabled a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what the relevant factors are and how they work together. Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach, AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long Short-Term Memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in humans. Not only is the proposed architecture more accurate, but its attention scores provide a better interpretation than state-of-the-art feature visualization methods such as saliency maps.(1)
C1 [Singh, Ritambhara; Lanchantin, Jack; Sekhon, Arshdeep; Qi, Yanjun] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.
RP Qi, YJ (reprint author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.
EM yanjun@virginia.edu
RI Jeong, Yongwook/N-7413-2016
CR Alipanahi Babak, 2015, PREDICTING SEQUENCE
   Ba J., MULTIPLE OBJECT RECO
   Baehrens D, 2010, J MACH LEARN RES, V11, P1803
   Bahdanau D., 2014, ARXIV14090473
   Bannister AJ, 2011, CELL RES, V21, P381, DOI 10.1038/cr.2011.22
   Boros J, 2014, MOL CELL BIOL, V34, P3662, DOI 10.1128/MCB.00205-14
   Cheng C, 2011, GENOME BIOL, V12, DOI 10.1186/gb-2011-12-2-r15
   Chorowski JK, 2015, ADV NEURAL INFORM PR, P577
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Dong XJ, 2013, EPIGENOMICS-UK, V5, P113, DOI [10.2217/EPI.13.13, 10.2217/epi.13.13]
   Dong XJ, 2012, GENOME BIOL, V13, DOI 10.1186/gb-2012-13-9-r53
   Ho B.H, 2015, SOME CURRENT ADV RES, P123
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Karlic R, 2010, P NATL ACAD SCI USA, V107, P2926, DOI 10.1073/pnas.0909344107
   Karpathy A., 2015, VISUALIZING UNDERSTA
   Kelley David R, 2016, BASSET LEARNING REGU
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248
   Lanchantin Jack, 2016, DEEP MOTIF VISUALIZI
   Lanchantin Jack, 2016, ARXIV160803644
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J., 2015, VISUALIZING UNDERSTA
   Li Yao, 2015, COMP VIS ICCV 2015 I
   Lin Z, 2016, 30 AAAI C ART INT
   Luong T., 2015, P 2015 C EMP METH NA, P1412, DOI DOI 10.18653/V1/D15-1166
   McManus S, 2011, EMBO J, V30, P2388, DOI 10.1038/emboj.2011.140
   Mnih Volodymyr, ADV NEURAL INFORM PR, P2204
   Quang D, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw226
   Simonyan K, 2013, DEEP INSIDE CONVOLUT
   Singh R, 2016, BIOINFORMATICS, V32, P639, DOI 10.1093/bioinformatics/btw427
   Sutskever I., 2015, ADV NEURAL INFORM PR, P2692
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Xu H., 2016, ECCV
   Xu K., 2015, ICML, V14, P77
   Yang Zichao, 2016, HIERARCHICAL ATTENTI
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhou J., 2014, ARXIV14031347
   Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/NMETH.3547, 10.1038/nmeth.3547]
NR 38
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406082
DA 2019-06-15
ER

PT S
AU Smith, V
   Chiang, CK
   Sanjabi, M
   Talwalkar, A
AF Smith, Virginia
   Chiang, Chao-Kai
   Sanjabi, Maziar
   Talwalkar, Ameet
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Federated Multi-Task Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.
C1 [Smith, Virginia] Stanford Univ, Stanford, CA 94305 USA.
   [Chiang, Chao-Kai; Sanjabi, Maziar] USC, Los Angeles, CA USA.
   [Talwalkar, Ameet] CMU, Mt Pleasant, MI USA.
RP Smith, V (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM smithv@stanford.edu; chaokaic@usc.edu; maziarsanjabi@gmail.com;
   talwalkar@cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Ahmed A., 2014, C WEB SEARCH DAT MIN
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Anguita D., 2013, EUR S ART NEUR NETW
   Argyriou A., 2007, NEURAL INFORM PROCES
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Aslan O., 2014, ADV NEURAL INFORM PR
   Baytas I. M., 2016, INT C DAT MIN
   Bonomi F., 2012, SIGCOMM WORKSH MOB C
   Carroll Aaron, 2010, USENIX ANN TECHN C M
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen J., 2011, C KNOWL DISC DAT MIN
   Chukwurah KQ, 2016, PROC ASME CONF SMART
   Deshpande A, 2005, VLDB J, V14, P417, DOI 10.1007/s00778-005-0159-3
   Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020
   Evgeniou T., 2004, C KNOWL DISC DAT MIN
   Lopez PG, 2015, ACM SIGCOMM COMP COM, V45, P37, DOI 10.1145/2831347.2831354
   Goncalves AR, 2016, J MACH LEARN RES, V17
   Gorski J, 2007, MATH METHOD OPER RES, V66, P373, DOI 10.1007/s00186-007-0161-1
   Hong K., 2013, SIGCOMM WORKSH MOB C
   Hsieh C.-J., 2014, NEURAL INFORM PROCES, V27
   Huang J., 2013, ACM SIGCOMM C
   Jacob L., 2009, NEURAL INFORM PROCES
   Jaggi M., 2014, NEURAL INFORM PROCES
   Jin X., 2015, C INF KNOWL MAN
   Kim S, 2009, PLOS GENET, V5, DOI 10.1371/journal.pgen.1000587
   Konecny J., 2016, ARXIV161005492
   Konecny Jakub, 2015, ARXIV151103575
   Kuflik T., 2012, UBIQUITOUS DISPLAY E, P7, DOI DOI 10.1007/978-3-642-27663-72
   Kumar A., 2012, INT C MACH LEARN
   Lauritzen S. L., 1996, GRAPHICAL MODELS, V17
   Liu S., 2017, C KNOWL DISC DAT MIN
   Ma C., 2015, INT C MACH LEARN
   Madden S., 2002, S OP SYST DES IMPL
   Madden SR, 2005, ACM T DATABASE SYST, V30, P122, DOI 10.1145/1061318.1061322
   Mairal J., 2014, NEURAL INFORM PROCES
   Mateos-Nunez D., 2015, IFAC WORKSH DISTR ES
   McMahan H. B., 2017, C ART INT STAT
   Miettinen A. P., 2010, USENIX C HOT TOP CLO
   Pantelopoulos A, 2010, IEEE T SYST MAN CY C, V40, P1, DOI 10.1109/TSMCC.2009.2032660
   Qi H., 2017, INT C LEARN REPR
   Rahman S. A., 2015, C PERV COMP TECHN HE
   Rashidi P, 2009, IEEE T SYST MAN CY A, V39, P949, DOI 10.1109/TSMCA.2009.2025137
   Rastegari Mohammad, 2016, EUR C COMP VIS
   Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009
   Shalev-Shwartz S., 2007, INT C MACH LEARN
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Singelee D., 2011, ACM C WIR NETW SEC
   Smith Virginia, 2016, ARXIV161102189
   Takac M., 2013, INT C MACH LEARN
   Tsai C.-Y., 2016, NEURAL INFORM PROCES
   van Berkel CH, 2009, DES AUT TEST EUROPE, P1260
   Wang H., 2013, NEURAL INFORM PROCES
   Wang Jialei, 2016, ARXIV160302185
   Zhang Y., 2017, INT C MACH LEARN
   Zhang Y., 2010, C UNC ART INT
   Zhou J., 2011, NEURAL INFORM PROCES
NR 56
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404048
DA 2019-06-15
ER

PT S
AU Snell, J
   Swersky, K
   Zemel, R
AF Snell, Jake
   Swersky, Kevin
   Zemel, Richard
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Prototypical Networks for Few-shot Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.
C1 [Snell, Jake] Univ Toronto, Vector Inst, Toronto, ON, Canada.
   [Snell, Jake; Swersky, Kevin] Twitter, San Francisco, CA USA.
   [Zemel, Richard] Univ Toronto, Vector Inst, Canadian Inst Adv Res, Toronto, ON, Canada.
RP Snell, J (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
RI Jeong, Yongwook/N-7413-2016
FU Samsung GRP project; Canadian Institute for Advanced Research
FX We would like to thank Marc Law, Sachin Ravi, Hugo Larochelle, Renjie
   Liao, and Oriol Vinyals for helpful discussions. This work was supported
   by the Samsung GRP project and the Canadian Institute for Advanced
   Research.
CR Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Akata Zeynep, 2015, IEEE COMPUTER VISION
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Bellet A., 2013, ARXIV13066709
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Edwards H., 2017, INT C LEARN REPR
   Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321
   Finn Chelsea, 2017, INT C MACH LEARN
   Goldberger J, 2004, P ADV NEUR INF PROC, P513
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ioffe S., 2015, ARXIV150203167
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koch G., 2015, THESIS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Lake BM, 2011, COGSCI
   Liao Renjie, 2016, ADV NEURAL INFORM PR
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856
   Min RQ, 2009, IEEE DATA MINING, P357, DOI 10.1109/ICDM.2009.27
   Ravi Sachin, 2017, INT C LEARN REPR
   Reed Scott, 2016, IEEE COMPUTER VISION
   Rezende D. J, 2014, ARXIV14014082
   Rippel Oren, 2016, INT C LEARN REPR
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2007, P INT C ART INT STAT, P412
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Weinberger K. Q., 2005, ADV NEURAL INFORM PR, P1473
   Welinder P, 2010, CNSTR2010001 CALTECH
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33
NR 36
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404015
DA 2019-06-15
ER

PT S
AU Soltanolkotabi, M
AF Soltanolkotabi, Mandi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning ReLUs via Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form x bar right arrow max(0,< w, x >) with W is an element of R-d denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at 0, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.
C1 [Soltanolkotabi, Mandi] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90007 USA.
RP Soltanolkotabi, M (reprint author), Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90007 USA.
EM soltanol@usc.edu
RI Jeong, Yongwook/N-7413-2016
CR Amelunxen D., 2014, INFORM INFERENCE
   Brutzkus A., 2017, INT C MACH LEARN ICM
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Ganti R., 2015, ARXIV150608910
   Goel S., 2016, ARXIV161110258
   Gordon Y., 1988, MILMANS INEQUALITY R
   HAEFFELE B. D., 2015, ARXIV150607540
   Horowitz JL, 1996, J AM STAT ASSOC, V91, P1632, DOI 10.2307/2291590
   ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K
   Kakade S., 2011, ADV NEURAL INFORM PR, P927
   Kalai Adam Tauman, 2009, COLT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li Yuanzhi, 2017, ARXIV170509886
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nguyen Quynh, 2017, ARXIV170408045
   Oymak S., 2016, ARXIV161007108
   Oymak S., 2015, ARXIV150704793
   Poston T., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P173, DOI 10.1109/IJCNN.1991.155333
   Soltanolkotabi M., 2017, THEORETICAL INSIGHTS, V07
   Soltanolkotabi M, 2017, ARXIV170206175
   Soltanolkotabi Mahdi, 2017, ARXIV170504591
   Tian Y., 2017, INT C MACH LEARN ICM
   Vershynin  Roman, 2010, ARXIV10113027
   Zhong Kai, 2017, ARXIV170603175
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402006
DA 2019-06-15
ER

PT S
AU Song, CB
   Cui, SB
   Jiang, Y
   Xia, ST
AF Song, Chaobing
   Cui, Shaobo
   Jiang, Yong
   Xia, Shu-Tao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding
   Projection onto Simplex
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMIZATION
AB In this paper we study the well-known greedy coordinate descent (GCD) algorithm to solve l(1)-regularized problems and improve GCD by the two popular strategies: Nesterov's acceleration and stochastic optimization. Firstly, based on an l(1)-norm square approximation, we propose a new rule for greedy selection which is non-trivial to solve but convex; then an efficient algorithm called "SOft ThreshOlding PrOjection (SOTOPO)" is proposed to exactly solve an l(1)-regularized l(1)-norm square approximation problem, which is induced by the new rule. Based on the new rule and the SOTOPO algorithm, the Nesterov's acceleration and stochastic optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD) has the optimal convergence rate O(root 1/epsilon); meanwhile, it reduces the iteration complexity of greedy selection up to a factor of sample size. Both theoretically and empirically, we show that ASGCD has better performance for high-dimensional and dense problems with sparse solutions.
C1 [Song, Chaobing; Cui, Shaobo; Jiang, Yong; Xia, Shu-Tao] Tsinghua Univ, Beijing, Peoples R China.
RP Song, CB (reprint author), Tsinghua Univ, Beijing, Peoples R China.
EM songcb16@mails.tsinghua.edu.cn; cuishaobo16@mails.tsinghua.edu.cn;
   jiangy@sz.tsinghua.edu.cn; xiast@sz.tsinghua.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National Natural Science Foundation of China [61771273, 61371078]
FX This work is supported by the National Natural Science Foundation of
   China under grant Nos. 61771273, 61371078.
CR Allen- Zhu Zeyuan, 2014, ARXIV E PRINTS
   Allen- Zhu Zeyuan, 2016, ARXIV E PRINTS
   Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chang C. - C., 2000, LIBSVM INTRO BENCHMA
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Dhillon I. S., 2011, ADV NEURAL INFORM PR, P2160
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Duchi J. C., 2010, P 23 ANN C LEARN THE, P14
   Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lin Q., 2014, ADV NEURAL INFORM PR, V27, P3059
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Shalev-Shwartz S., 2014, P 31 INT C MACH LEAR, P64
   Shalev-Shwartz S, 2006, J MACH LEARN RES, V7, P1567
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang Yuchen, 2015, P 32 INT C MACH LEAR, V951, P2015
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404088
DA 2019-06-15
ER

PT S
AU Song, JM
   Zhao, SJ
   Ermon, S
AF Song, Jiaming
   Zhao, Shengjia
   Ermon, Stefano
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A-NICE-MC: Adversarial Training for MCMC
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MARKOV-CHAIN
AB Existing Markov Chain Monte Carlo (MCMC) methods are either based on general-purpose and domain-agnostic schemes, which can lead to slow convergence, or problem-specific proposals hand-crafted by an expert. In this paper, we propose A-NICE-MC, a novel method to automatically design efficient Markov chain kernels tailored for a specific domain. First, we propose an efficient likelihood-free adversarial training method to train a Markov chain and mimic a given data distribution. Then, we leverage flexible volume preserving flows to obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to train efficient Markov chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples. Empirical results demonstrate that A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of deep neural networks, and is able to significantly outperform competing methods such as Hamiltonian Monte Carlo.
C1 [Song, Jiaming; Zhao, Shengjia; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA.
RP Song, JM (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM tsong@cs.stanford.edu; zhaosj12@cs.stanford.edu; ermon@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Intel Corporation; NSF [1651565, 1522054, 1733686]; TRI; FLI
FX This research was funded by Intel Corporation, TRI, FLI and NSF grants
   1651565, 1522054, 1733686. The authors would like to thank Daniel Levy
   for discussions on the NICE proposal proof, Yingzhen Li for suggestions
   on the training procedure and Aditya Grover for suggestions on the
   implementation.
CR Abadi M., 2016, ARXIV160304467
   Arjovsky M., 2017, ARXIV170107875
   Bengio Y., 2014, DEEP GENERATIVE STOC
   Bordes F., 2017, ICLR
   Boyd S, 2004, SIAM REV, V46, P667, DOI [10.1137/S0036144503423264, 10.1137/s0036144503423264]
   Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675
   de Freitas N., 2001, UNCERTAINTY ARTIFICI, P120
   Dinh L., 2014, ARXIV14108516
   Efron B., 1994, INTRO BOOTSTRAP
   Ermon Stefano, 2014, AAAI, P849
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gorham J., 2015, ADV NEURAL INFORM PR, P226
   Gorham J., 2017, ARXIV170301717
   Gorham J., 2016, ARXIV161106972
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   Grover Aditya, 2017, ARXIV170508868
   Gulrajani Ishaan, 2017, ARXIV170400028
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Jakob W., 2012, ACM T GRAPHIC, V31
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma D. P., 2016, ARXIV160604934
   Landau D. P., 2014, GUIDE MONTE CARLO SI
   Li Yujia, 2015, P 32 INT C MACH LEAR, P1718
   MAHENDRAN N, 2012, J MACHINE LEARNING R, V22, P751
   Mohamed S., 2016, ARXIV161003483
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Radford A., 2015, ARXIV151106434
   Ranganath  R., 2014, AISTATS, P814
   Rezende D. J, 2014, ARXIV14014082
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Srivastava R. K., 2015, ARXIV150500387
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Zhu J Y, 2017, ARXIV170310593
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405022
DA 2019-06-15
ER

PT S
AU Song, L
   Vempala, S
   Wilmes, J
   Xie, B
AF Song, Le
   Vempala, Santosh
   Wilmes, John
   Xie, Bo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI On the Complexity of Learning Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate, that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.
C1 [Song, Le; Vempala, Santosh; Wilmes, John; Xie, Bo] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Song, L (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM lsong@cc.gatech.edu; vempala@gatech.edu; wilmesj@gatech.edu;
   bo.xie@gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1563838, CCF-1717349]
FX The authors are grateful to Vitaly Feldman for discussions about
   statistical query lower bounds, and for suggestions that simplified the
   presentation of our results, and also to Adam Kalai for an inspiring
   discussion. This research was supported in part by NSF grants
   CCF-1563838 and CCF-1717349.
CR Andoni A., 2014, P 31 INT C MACH LEAR, P1908
   BALAZS S, 2009, ALT, V5809, P186
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   Blum A., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P253, DOI 10.1145/195058.195147
   BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3
   Brutzkus Alon, 2017, CORR
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Daniely A., 2016, P 29 C LEARN THEOR C, P815
   Eldan  R., 2016, C LEARN THEOR, P907
   FELDMAN V., 2013, P 45 ANN ACM S THEOR, P655
   Goel Surbhi, 2016, CORR
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Janzamin Majid, 2015, CORR
   Kearns M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P392, DOI 10.1145/167088.167200
   Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351
   Klivans Adam R., 2016, ENCY ALGORITHMS, P475
   Shalev-Shwartz Shai, 2017, CORR
   Shamir Ohad, 2016, CORR
   Song Le, 2017, ARXIV170704615
   Telgarsky M., 2016, ARXIV160204485
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405058
DA 2019-06-15
ER

PT S
AU Song, Z
   Muraoka, Y
   Fujimaki, R
   Carin, L
AF Song, Zhao
   Muraoka, Yusuke
   Fujimaki, Ryohei
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Model Selection for Belief Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS; DEEP
AB We propose a scalable algorithm for model selection in sigmoid belief networks (SBNs), based on the factorized asymptotic Bayesian (FAB) framework. We derive the corresponding generalized factorized information criterion (gFIC) for the SBN, which is proven to be statistically consistent with the marginal log-likelihood. To capture the dependencies within hidden variables in SBNs, a recognition network is employed to model the variational distribution. The resulting algorithm, which we call FABIA, can simultaneously execute both model selection and inference by maximizing the lower bound of gFIC. On both synthetic and real data, our experiments suggest that FABIA, when compared to state-of-the-art algorithms for learning SBNs, (i) produces a more concise model, thus enabling faster testing; (ii) improves predictive performance; (iii) accelerates convergence; and (iv) prevents overfitting.
C1 [Song, Zhao; Carin, Lawrence] Duke Univ, Dept ECE, Durham, NC 27708 USA.
   [Muraoka, Yusuke; Fujimaki, Ryohei] NEC Data Sci Res Labs, Cupertino, CA 95014 USA.
RP Song, Z (reprint author), Duke Univ, Dept ECE, Durham, NC 27708 USA.
EM zhao.song@duke.edu; ymuraoka@nec-labs.com; rfujimaki@nec-labs.com;
   lcarin@duke.edu
FU ARO; DARPA; DOE; NGA; ONR; NSF; NEC Fellowship
FX The authors would like to thank Ricardo Henao for helpful discussions,
   and the anonymous reviewers for their insightful comments and
   suggestions. Part of this work was done during the internship of the
   first author at NEC Laboratories America, Cupertino, CA. This research
   was supported in part by ARO, DARPA, DOE, NGA, ONR, NSF, and the NEC
   Fellowship.
CR Adams Ryan Prescott, 2010, P 26 C UNC ART INT, V9, P1
   Alvarez J. M., 2016, ADV NEURAL INFORM PR, P2270
   Armagan Artin, 2011, Adv Neural Inf Process Syst, V24, P523
   Bornschein J., 2015, INT C LEARN REPR
   Breuleux Olivier, 2010, P 9 PYTH SCI C, P1
   Carlson D, 2016, IEEE J-STSP, V10, P296, DOI 10.1109/JSTSP.2015.2505684
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fujimaki R, 2012, P 15 INT C ART INT S, P400
   Fujimaki R., 2012, P 29 INT C MACH LEAR, P799
   Gan Z., 2015, P 32 INT C MACH LEAR, P1823
   Gan Z., 2015, LEARNING DEEP SIGMOI, P268
   Hayashi K., 2013, ADV NEURAL INFORM PR, V26, P1214
   Hayashi K., 2015, ICML, P1358
   Henao  R., 2015, NIPS, P2800
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu C., 2015, P 32 INT C MACH LEAR, V2, P1227
   Liu CC, 2016, IEEE DATA MINING, P271, DOI [10.1109/ICDM.2016.101, 10.1109/ICDM.2016.0038]
   MacKay D. J, 2003, INFORM THEORY INFERE
   Maddison Chris J., 2017, INT C LEARN REPR
   Mnih A, 2016, INT C MACH LEARN, P2188
   Mnih A., 2014, P 31 INT C MACH LEAR, P1791
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Song Z., 2016, ARTIF INTELL, P1347
   Srivastava N., 2013, UNC ART INT P 29 C, P616
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Zhou  M., 2015, NIPS, P3043
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404066
DA 2019-06-15
ER

PT S
AU Srinivasa, C
   Givoni, I
   Ravanbakhsh, S
   Frey, BJ
AF Srinivasa, Christopher
   Givoni, Inmar
   Ravanbakhsh, Siamak
   Frey, Brendan J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Min-Max Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the application of min-max propagation, a variation of belief propagation, for approximate min-max inference in factor graphs. We show that for "any" high-order function that can be minimized in O (omega), the min-max message update can be obtained using an efficient O (K (omega + log (K)) procedure, where K is the number of variables. We demonstrate how this generic procedure, in combination with efficient updates for a family of high-order constraints, enables the application of min-max propagation to efficiently approximate the NP-hard problem of makespan minimization, which seeks to distribute a set of tasks on machines, such that the worst case load is minimized.
C1 [Srinivasa, Christopher] Univ Toronto, Borealis, Toronto, ON, Canada.
   [Givoni, Inmar] Univ Toronto, Toronto, ON, Canada.
   [Ravanbakhsh, Siamak] Univ British Columbia, Vancouver, BC, Canada.
   [Frey, Brendan J.] Univ Toronto, Vector Inst, Deep Genom, Toronto, ON, Canada.
RP Srinivasa, C (reprint author), Univ Toronto, Borealis, Toronto, ON, Canada.
EM christopher.srinivasa@gmail.com; inmar.givoni@gmail.com;
   siamakx@cs.ubc.ca; frey@psi.toronto.edu
CR Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794
   Behera D., 2012, LECT NOTES MECH ENG, P373
   Behera D. K., 2012, ADV MAT RES, V488, P1708
   Bishop C. M., 2006, PATTERN RECOGNITION
   Edmonds J., 1970, J COMB THEORY, V8, P299, DOI DOI 10.1016/S0021-9800(70)80083-7
   GAIL MH, 1981, BIOMETRIKA, V68, P703, DOI 10.2307/2335457
   Garey M. R., 1979, COMPUTERS INTRACTABI, V174
   GRAHAM RL, 1966, AT&T TECH J, V45, P1563, DOI 10.1002/j.1538-7305.1966.tb01709.x
   Gupta JND, 2001, PROD PLAN CONTROL, V12, P28, DOI 10.1080/09537280150203951
   Gupta R., 2007, P 24 INT C MACH LEAR, P329
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Pinedo ML, 2012, SCHEDULING: THEORY, ALGORITHMS, AND SYSTEMS, FOURTH EDITION, P1, DOI 10.1007/978-1-4614-2361-4
   Potetz B, 2008, COMPUT VIS IMAGE UND, V112, P39, DOI 10.1016/j.cviu.2008.05.007
   Ravanbakhsh S., 2014, P 31 INT C MACH LEAR
   Ravanbakhsh S, 2015, J MACH LEARN RES, V16, P1249
   Tarlow D., 2010, INT C ART INT STAT, P812
   Vinyals M., 2013, COMPUTER J
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405063
DA 2019-06-15
ER

PT S
AU Srivastava, A
   Valkov, L
   Russell, C
   Gutmann, MU
   Sutton, C
AF Srivastava, Akash
   Valkov, Lazar
   Russell, Chris
   Gutmann, Michael U.
   Sutton, Charles
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.
C1 [Srivastava, Akash; Valkov, Lazar; Gutmann, Michael U.; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
   [Russell, Chris] Alan Turing Inst, London, England.
   [Sutton, Charles] Univ Edinburgh, Alan Turing Inst, Edinburgh, Midlothian, Scotland.
RP Srivastava, A (reprint author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
EM akash.srivastava@ed.ac.uk; L.Valkov@sms.ed.ac.uk; crussell@turing.ac.uk;
   Michael.Gutmann@ed.ac.uk; csutton@inf.ed.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Che T., 2017, INT C LEARN REPR ICL
   Cover T. M., 2012, ELEMENTS INFORM THEO
   DIGGLE PJ, 1984, J ROY STAT SOC B MET, V46, P193
   Donahue J., 2017, INT C LEARN REPR ICL
   Dumoulin V., 2017, INT C LEARN REPR ICL
   Dutta Ritabrata, 2016, LIKELIHOOD FREE INFE
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gutmann M., 2011, P C UNC ART INT UAI, P283
   Gutmann MU, 2012, J MACH LEARN RES, V13, P307
   Gutmann Michael U, 2014, ARXIV14074981
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P., 2013, ARXIV13126114
   Larsen A. B. L., 2016, INT C MACH LEARN ICM
   Makhzani A., 2015, 151105644 ARXIV
   Mescheder Lars M., 2017, ABS170104722 ARXIV
   Metz L., 2016, ARXIV161102163
   Radford  A., 2015, ARXIV151106434
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T., 2016, ABS160603498 CORR
   Sonderby C. K., 2016, ARXIV161004490
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Tran D., 2017, ARXIV E PRINTS
   Zhu J Y, 2017, ARXIV170310593
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403037
DA 2019-06-15
ER

PT S
AU Srivastava, N
   Vul, E
AF Srivastava, Nisheeth
   Vul, Edward
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A simple model of recognition and recall memory
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPLEMENTARY LEARNING-SYSTEMS; RETRIEVAL-PROCESSES
AB We show that several striking differences in memory performance between recognition and recall tasks are explained by an ecological bias endemic in classic memory experiments - that such experiments universally involve more stimuli than retrieval cues. We show that while it is sensible to think of recall as simply retrieving items when probed with a cue - typically the item list itself - it is better to think of recognition as retrieving cues when probed with items. To test this theory, by manipulating the number of items and cues in a memory experiment, we show a crossover effect in memory performance within subjects such that recognition performance is superior to recall performance when the number of items is greater than the number of cues and recall performance is better than recognition when the converse holds. We build a simple computational model around this theory, using sampling to approximate an ideal Bayesian observer encoding and retrieving situational co-occurrence frequencies of stimuli and retrieval cues. This model robustly reproduces a number of dissociations in recognition and recall previously used to argue for dual-process accounts of declarative memory.
C1 [Srivastava, Nisheeth] IIT Kanpur, Comp Sci, Kanpur 208016, Uttar Pradesh, India.
   [Vul, Edward] UCSD, Dept Psychol, 9500 Gilman Dr, La Jolla, CA 92093 USA.
RP Srivastava, N (reprint author), IIT Kanpur, Comp Sci, Kanpur 208016, Uttar Pradesh, India.
EM nsrivast@cse.iitk.ac.in; evul@ucsd.edu
RI Jeong, Yongwook/N-7413-2016
CR Bauml KH, 1997, PSYCHON B REV, V4, P260, DOI 10.3758/BF03209403
   Craik FIM, 1996, J EXP PSYCHOL GEN, V125, P159, DOI 10.1037/0096-3445.125.2.159
   Gershman SJ, 2010, PSYCHOL REV, V117, P197, DOI 10.1037/a0017808
   GILLUND G, 1984, PSYCHOL REV, V91, P1, DOI 10.1037/0033-295X.91.1.1
   GLANZER M, 1990, J EXP PSYCHOL LEARN, V16, P5, DOI 10.1037//0278-7393.16.1.5
   Gregg Vernon, 1976, WORD FREQUENCY RECOG
   Kumaran D, 2016, TRENDS COGN SCI, V20, P512, DOI 10.1016/j.tics.2016.05.004
   MANDLER G, 1980, PSYCHOL REV, V87, P252, DOI 10.1037//0033-295X.87.3.252
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   NICKERSON RS, 1984, MEM COGNITION, V12, P531, DOI 10.3758/BF03213342
   RATCLIFF R, 1990, J EXP PSYCHOL LEARN, V16, P163, DOI 10.1037/0278-7393.16.2.163
   Shiffrin RM, 1997, PSYCHON B REV, V4, P145, DOI 10.3758/BF03209391
   Srivastava Nisheeth, 2014, P ANN M COGN SCI SOC
   STERNBERG S, 1969, AM SCI, V57, P421
   Todd PM, 2007, CURR DIR PSYCHOL SCI, V16, P167, DOI 10.1111/j.1467-8721.2007.00497.x
   TULVING E, 1971, J EXP PSYCHOL, V87, P116, DOI 10.1037/h0030186
   Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101
   Wixted JT, 2007, PSYCHOL REV, V114, P152, DOI 10.1037/0033-295X.114.1.152
   Yonelinas AP, 2002, J MEM LANG, V46, P441, DOI 10.1006/jmla.2002.2864
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400028
DA 2019-06-15
ER

PT S
AU Staib, M
   Claici, S
   Solomon, J
   Jegelka, S
AF Staib, Matthew
   Claici, Sebastian
   Solomon, Justin
   Jegelka, Stefanie
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Parallel Streaming Wasserstein Barycenters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMAL TRANSPORT; ALGORITHM
AB Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.
C1 [Staib, Matthew; Claici, Sebastian; Solomon, Justin; Jegelka, Stefanie] MIT CS AIL, Cambridge, MA 02139 USA.
RP Staib, M (reprint author), MIT CS AIL, Cambridge, MA 02139 USA.
EM mstaib@mit.edu; sclaici@mit.edu; jsolomon@mit.edu; stefje@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU DoD; Air Force Office of Scientific Research; National Defense Science
   and Engineering Graduate (NDSEG) Fellowship [32 CFR 168a]; MIT Research
   Support Committee ("Structured Optimization for Geometric Problems");
   Army Research Office grant [W911NF-12-R-0011]; NSF CAREER award
   [1553284]; Defense Advanced Research Projects Agency [N66001-17-1-4039]
FX We thank the anonymous reviewers for their helpful suggestions. We also
   thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center
   for providing computational resources. M. Staib acknowledges Government
   support under and awarded by DoD, Air Force Office of Scientific
   Research, National Defense Science and Engineering Graduate (NDSEG)
   Fellowship, 32 CFR 168a. J. Solomon acknowledges funding from the MIT
   Research Support Committee ("Structured Optimization for Geometric
   Problems"), as well as Army Research Office grant W911NF-12-R-0011
   ("Smooth Modeling of Flows on Graphs"). This research was supported by
   NSF CAREER award 1553284 and The Defense Advanced Research Projects
   Agency (grant number N66001-17-1-4039). The views, opinions, and/or
   findings contained in this article are those of the author and should
   not be interpreted as representing the official views or policies,
   either expressed or implied, of the Defense Advanced Research Projects
   Agency or the Department of Defense.
CR Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741
   Anderes E, 2016, MATH METHOD OPER RES, V84, P389, DOI 10.1007/s00186-016-0549-x
   Angelino E, 2016, FOUND TRENDS MACH LE, V9, pI, DOI 10.1561/2200000052
   Arjovsky M, 2017, WASSERSTEIN GAN
   Baum M, 2015, IEEE SIGNAL PROC LET, V22, P1511, DOI 10.1109/LSP.2015.2410217
   Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439
   Bigot Jeremie, 2016, ARXIV160601025MATHST
   Boissard E, 2015, BERNOULLI, V21, P740, DOI 10.3150/13-BEJ585
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Broderick T., 2013, ADV NEURAL INFORM PR, P1727
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600
   Cuturi Marco, 2014, FAST COMPUTATION WAS, P685
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Frogner C., 2015, ADV NEURAL INFORM PR, P2053
   Genevay Aude, 2016, ADV NEURAL INFORM PR, P3440
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Held M., 1974, Mathematical Programming, V6, P62, DOI 10.1007/BF01580223
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Johnson M. J., 2013, ADV NEURAL INFORM PR, P2715
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kim YH, 2017, ADV MATH, V307, P640, DOI 10.1016/j.aim.2016.11.026
   Kitagawa Jun, 2016, ARXIV160305579CSMATH
   Kloeckner B, 2012, ESAIM CONTR OPTIM CA, V18, P343, DOI 10.1051/cocv/2010100
   Levy B, 2015, ESAIM-MATH MODEL NUM, V49, P1693, DOI 10.1051/m2an/2015055
   MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486
   Minsker S., 2014, P 31 INT C MACH LEAR, P1656
   Montavon G., 2016, ADV NEURAL INFORM PR, V29, P3718
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Neiswanger Willie, P 30 C UNC ART INT U, P623
   Nemirovski Arkadi, 2005, MODELING UNCERTAINTY, P156, DOI [10.1007/0-306-48102-2_8, DOI 10.1007/0-306-48102-2_8]
   Newman D., 2008, ADV NEURAL INFORM PR, P1081
   Peyre G, 2016, P ICML, P2664
   Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37
   Rolet Antoine, 2016, ARTIF INTELL, P630
   Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P1, DOI 10.1007/978-3-319-20828-2
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shamir Ohad, 2016, ADV NEURAL INFORM PR, V29, P46
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Sra Suvrit, 2016, ARXIV160500316STAT
   Srivastava S, 2015, P 18 INT C ART INT S, P912
   Srivastava Sanvesh, 2015, ARXIV150805880STAT
   Villani  C., 2009, GRUNDLEHREN MATH WIS
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647
   Zhang YC, 2015, J MACH LEARN RES, V16, P3299
   Zhang YC, 2013, J MACH LEARN RES, V14, P3321
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402068
DA 2019-06-15
ER

PT S
AU Steinhardt, J
   Koh, PW
   Liang, P
AF Steinhardt, Jacob
   Koh, Pang Wei
   Liang, Percy
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Certified Defenses for Data Poisoning Attacks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SECURITY
AB Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (nonpoisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.
C1 [Steinhardt, Jacob; Koh, Pang Wei; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA.
RP Steinhardt, J (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM jsteinha@stanford.edu; pangwei@cs.stanford.edu; pliang@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Fannie & John Hertz Foundation Fellowship; NSF Graduate Research
   Fellowship; Future of Life Institute grant; Open Philanthropy Project
FX JS was supported by a Fannie & John Hertz Foundation Fellowship and an
   NSF Graduate Research Fellowship. This work was also partially supported
   by a Future of Life Institute grant and a grant from the Open
   Philanthropy Project. We are grateful to Daniel Selsam, Zhenghao Chen,
   and Nike Sun, as well as to the anonymous reviewers, for a great deal of
   helpful feedback.
CR Awasthi P., 2014, STOC, P449
   Bard JF, 1999, PRACTICAL BILEVEL OP
   Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5
   Behzadan V., 2017, VULNERABILITY DEEP R
   Bhatia K., 2015, ADV NEURAL INFORM PR, P721
   Biggio B., 2014, WORKSH ART INT SEC A
   Biggio B, 2012, P 29 INT C INT C MAC, P1467
   Biggio B., 2014, WORKSH STRUCT SYNT S
   Biggio B., 2013, WORKSH ART INT SEC A
   Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57
   Bishop MA, 2002, ART SCI COMPUTER SEC
   Bruckner M., 2011, P 17 ACM SIGKDD INT, P547
   Bruckner M, 2012, J MACH LEARN RES, V13, P2617
   Burkard C., 2017, INT WORKSH SEC PRIV
   Carlini N., 2016, USENIX SECURITY
   Charikar M., 2017, S THEOR COMP STOC
   Chen Y., 2013, ROBUST HIGH DIMENSIO
   Cretu GF, 2008, P IEEE S SECUR PRIV, P81, DOI 10.1109/SP.2008.11
   Diakonikolas I., 2016, FDN COMPUTER SCI FOC
   Diamond S, 2016, J MACH LEARN RES, V17
   Gardiner J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/3003816
   Goodfellow I, 2014, ADV NEURAL INFORM PR
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Gurobi Optimization Inc, 2016, GUROBI OPTIMIZER REF
   Huang S., 2017, ADVERSARIAL ATTACKS
   Kakade Sham M, 2009, ADV NEURAL INFORM PR
   Kerckhoffs A., 1883, J SCI MILITAIRES, V9
   Klivans AR, 2009, J MACH LEARN RES, V10, P2715
   Koh P. W., 2017, INT C MACH LEARN ICM
   Kurakin A., 2016, ADVERSARIAL EXAMPLES
   Lai K. A., 2016, FDN COMPUTER SCI FOC
   Laishram R, 2016, CURIE METHOD PROTECT
   Lakhina A, 2004, ACM SIGCOMM COMP COM, V34, P219, DOI 10.1145/1030194.1015492
   Laskov P., 2014, S SEC PRIV
   Lin Y., 2017, TACTICS ADVERSARIAL
   Liu J, 2016, J MACH LEARN RES, V17
   Lofberg J., 2004, CACSD
   Maas A. L., 2011, ASS COMPUTATIONAL LI
   Mei S., 2015, ASS ADVANCEMENT ARTI
   Mei S., 2015, ARTIFICIAL INTELLIGE
   Metsis V., 2006, P 3 C EM ANT CEAS 06, V17, P28
   Mozaffari-Kermani M, 2015, IEEE J BIOMED HEALTH, V19, P1893, DOI 10.1109/JBHI.2014.2344095
   Nasrabadi N. M., 2011, ADV NEURAL INFORM PR
   Newell A., 2014, WORKSH ART INT SEC A, P83
   Newsome J., 2006, INT WORKSH REC ADV I
   Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2017, DOI 10.1109/TIT.2013.2240435
   Papernot N., 2016, SCI SECURITY PRIVACY
   Papernot N., 2016, TRANSFERABILITY MACH
   Park S, 2017, ACM IEEE INT CONF CY, P155, DOI 10.1145/3055004.3055006
   Rubinstein B., 2009, ACM SIGCOMM C INT ME
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Steinhardt J., 2014, ARXIV14124182
   Steinhardt J., 2016, ADV NEURAL INFORM PR
   Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766
   Szegedy  C., 2014, INT C LEARN REPR ICL
   Tramer Florian, 2016, USENIX SECURITY
   Vuurens J., 2011, ACM SIGIR WORKSH CRO
   Wang G., 2016, THESIS
   Wang H, 2016, NEURAL PLAST, DOI 10.1155/2016/2814056
   Xiao H., 2012, EUR C ART INT
   Xiao H., 2015, INT C MACH LEARN ICM
   Xiao H, 2015, NEUROCOMPUTING, V160, P53, DOI 10.1016/j.neucom.2014.08.081
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Yang  C., 2017, GENERATIVE POISONING
   Zhou Y., 2016, PAC AS C KNOWL DISC
NR 65
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403057
DA 2019-06-15
ER

PT S
AU Stich, SU
   Raj, A
   Jaggi, M
AF Stich, Sebastian U.
   Raj, Anant
   Jaggi, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Safe Adaptive Importance Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DESCENT METHOD; COORDINATE; EFFICIENCY
AB Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants-using importance values defined by the complete gradient information which changes during optimization-enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is (i) provably the best sampling with respect to the given bounds, (ii) always better than uniform sampling and fixed importance sampling and (iii) can efficiently be computed-in many applications at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.
C1 [Stich, Sebastian U.; Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Raj, Anant] Max Planck Inst Intelligent Syst, Stuttgart, Germany.
RP Stich, SU (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM sebastian.stich@epfl.ch; anant.raj@tuebingen.mpg.de;
   martin.jaggi@epfl.ch
RI Jeong, Yongwook/N-7413-2016
CR Alain Guillaume, 2015, VARIANCE REDUCTION S
   Allen-Zhu  Z, 2016, P 33 INT C MACH LEAR, P1110
   Boyd S., 2004, CONVEX OPTIMIZATION
   Csiba Dominik, 2015, ICML 2015
   Csiba Dominik, 2016, IMPORTANCE SAMPLING
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712
   He Xi, 2015, DUAL FREE ADAPTIVE M
   Hsieh C.-J., 2008, P 25 INT C MACH LEAR, V951, P408, DOI DOI 10.1145/1390156.1390208
   Komiya Hidetoshi, 1988, KODAI MATH J, V11, P5
   Lacoste-Julien Simon, 2012, SIMPLER APPROACH OBT
   Liu Jun, 2014, P 31 INT C MACH LEAR, P289, DOI DOI 10.1109/TSP.2015.2447503
   Ndiaye Eugene, 2017, JMLR
   Needell D., 2014, P ADV NEUR INF PROC, P1017
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Osokin A., 2016, P INT C MACH LEARN, P593
   Papa G, 2015, LECT NOTES ARTIF INT, V9355, P317, DOI 10.1007/978-3-319-24486-0_21
   Perekrestenko Dmytro, 2017, AISTATS ARTIFICIAL I, V54, P869
   Qu Zheng, 2014, RANDOMIZED DUAL COOR
   Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1
   Schmidt M., 2015, INT C ART INT STAT J, P819
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Shibagaki Ichiro Takeuchi Atsushi, 2017, STOCHASTIC PRIMAL DU
   Sion M., 1958, PAC J MATH, V8, P171, DOI DOI 10.2140/PJM.1958.8.171
   Stich SU, 2016, MATH PROGRAM, V156, P549, DOI 10.1007/s10107-015-0908-z
   Stich Sebastian U., 2017, ICML 2017, V70, P3251
   Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4
   Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
   Zhu Rong, 2016, NIPS ADV NEURAL INFO, V29, P406
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404044
DA 2019-06-15
ER

PT S
AU Su, QL
   Liao, XJ
   Carin, L
AF Su, Qinliang
   Liao, Xuejun
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Probabilistic Framework for Nonlinearities in Stochastic Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID LEARNING ALGORITHM; BELIEF; SIMULATION
AB We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).
C1 [Su, Qinliang; Liao, Xuejun; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
RP Su, QL (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM qs15@duke.edu; xjliao@duke.edu; lcarin@duke.edu
RI Jeong, Yongwook/N-7413-2016
FU DOE; NGA; NSF; ONR; Accenture
FX The research reported here was supported by the DOE, NGA, NSF, ONR and
   by Accenture.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Agostinelli Forest, 2014, CORR
   Balakrishnan N., 1994, CONTINUOUS UNIVARIAT, V1
   Carlson D.E., 2015, ADV NEURAL INFORM PR, P2971
   Chopin N, 2011, STAT COMPUT, V21, P275, DOI 10.1007/s11222-009-9168-1
   Eisenach Carson, 2017, ICLR
   Frey BJ, 1997, ADV NEUR IN, V9, P452
   Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872
   Gan Zhe, 2015, ADV NEURAL INFORM PR, P2467
   Ghosh Soumya, 2016, P 30 AAAI C ART INT, P1589
   Goodfellow Ian J., 2013, INT C MACH LEARN ICM
   Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34
   Hernandez-Lobato J. M, 2015, P 32 INT C MACH LEAR, P499
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Marlin B.M., 2010, P 13 INT C ART INT S, P509
   Mittelman R, 2014, P 31 INT C MACH LEAR, P1647
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Ravanbakhsh Siamak, 2016, AISTATS, V1050, P14
   ROBERT CP, 1995, STAT COMPUT, V5, P121, DOI 10.1007/BF00143942
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Soudry D., 2014, ADV NEURAL INFORM PR, P963
   Su QL, 2015, IEEE T SIGNAL PROCES, V63, P6258, DOI 10.1109/TSP.2015.2465303
   Su QL, 2015, IEEE T SIGNAL PROCES, V63, P1144, DOI 10.1109/TSP.2015.2389755
   Su QL, 2014, IEEE T SIGNAL PROCES, V62, P5119, DOI 10.1109/TSP.2014.2345635
   Su Qinliang, 2016, P 33 INT C MACH LEAR
   Su Qinliang, 2016, 31 NAT C ART INT AAA
   Sutskever  I., 2007, INT C ART INT STAT, P548
   Sutskever  I., 2009, ADV NEURAL INFORM PR, P1601
   Welling M., 2004, ADV NEURAL INFORM PR, P1481
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404054
DA 2019-06-15
ER

PT S
AU Su, YC
   Grauman, K
AF Su, Yu-Chuan
   Grauman, Kristen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Spherical Convolution for Fast Features from 360 degrees
   Imagery
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB While 360 degrees cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield "flat" filters, yet 360 degrees images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360 degrees imagery directly in its equirec-tangular projection. Our approach learns to reproduce the flat filter outputs on 360 degrees data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360 degrees images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art "flat" object detector to 360 degrees data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.
C1 [Su, Yu-Chuan; Grauman, Kristen] Univ Texas Austin, Austin, TX 78712 USA.
RP Su, YC (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
CR Ba Jimmy, 2014, NIPS
   Barre A., 1987, CURVILINEAR PERSPECT
   Bucilua C., 2006, ACM SIGKDD
   Cohen T., 2017, ARXIV170904893
   Dai J., 2017, ICCV
   Deng J., 2009, CVPR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feichtenhofer C, 2016, CVPR
   Furnari A, 2017, IEEE T IMAGE PROCESS, V26, P696, DOI 10.1109/TIP.2016.2627816
   Girshick R., 2014, CVPR
   Gupta S., 2016, CVPR
   Hansen P., 2007, IROS
   Hansen P., 2007, ICCV
   He K., 2016, CVPR
   He Kaiming, 2017, ICCV
   Hinton G., 2015, ARXIV150302531
   Hu H., 2017, CVPR
   Jaderberg M., 2015, NIPS
   Jain S. D., 2017, CVPR
   Jeon Yunho, 2017, CVPR
   Jia Y., 2014, ACM MM
   Khasanova R., 2017, ARXIV170708301
   Krizhevsky A., 2012, NIPS
   Lai W-S, 2017, IEEE T VISUALIZATION, VPP, P1
   Lecun Y., 1998, P IEEE
   Long  J., 2015, CVPR
   Parisotto E., 2016, ICLR
   Ren S., 2015, NIPS
   Romero A., 2015, ICLR
   Simonyan K., 2014, NIPS
   Simonyan Karen, 2015, ICLR
   Su Y.-C., 2017, CVPR
   Su Y.-C., 2016, ACCV
   Tran D., 2015, ICCV
   Wang Yu- Xiong, 2016, ECCV
   Xiao J., 2012, CVPR
   Yu F., 2016, ICLR
   Zelnik-Manor L., 2005, ICCV
   Zhang Y, 2014, APPL MECH MATER, V518, P3, DOI 10.4028/www.scientific.net/AMM.518.3
   Zhou  B., 2014, NIPS
NR 40
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400051
DA 2019-06-15
ER

PT S
AU Suarez, J
AF Suarez, Joseph
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Character-Level Language Modeling with Recurrent Highway Hypernetworks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved-we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.
C1 [Suarez, Joseph] Stanford Univ, Stanford, CA 94305 USA.
RP Suarez, J (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM joseph15@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Ba J. L., 2016, ARXIV160706450
   Bahdanau D., 2014, ARXIV14090473
   Britz D, 2017, ARXIV170303906
   Chung J., 2014, CORR
   Cooijmans T., 2016, ARXIV160309025
   Dauphin Y.N, 2016, ARXIV161208083
   Greff K., 2016, IEEE T NEURAL NETWOR
   Ha D., 2016, ARXIV160909106
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Ioffe S., 2015, ARXIV150203167
   Jozefowicz R., 2016, ARXIV160202410
   Kalchbrenner N., 2016, ARXIV161010099
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Radford  A., 2017, ARXIV170401444
   Semeniuta S., 2016, ARXIV160305118
   Wu Y., 2016, ARXIV160908144
   Zilly J. G., 2016, ARXIV160703474
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403033
DA 2019-06-15
ER

PT S
AU Suggala, AS
   Kolar, M
   Ravikumar, P
AF Suggala, Arun Sai
   Kolar, Mladen
   Ravikumar, Pradeep
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Expxorcist: Nonparametric Graphical Models Via Conditional
   Exponential Densities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COMPATIBILITY; SELECTION
AB Non-parametric multivariate density estimation faces strong statistical and computational bottlenecks, and the more practical approaches impose near-parametric assumptions on the form of the density functions. In this paper, we leverage recent developments to propose a class of non-parametric models which have very attractive computational and statistical properties. Our approach relies on the simple function space assumption that the conditional distribution of each variable conditioned on the other variables has a non-parametric exponential family form.
C1 [Suggala, Arun Sai; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Kolar, Mladen] Univ Chicago, Chicago, IL 60637 USA.
RP Suggala, AS (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM asuggala@cs.cmu.edu; mkolar@chicagobooth.edu; pradeepr@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1447574, DMS-1264033]; NIH
   as part of the Joint DMS/NIGMS Initiative to Support Research at the
   Interface of the Biological and Mathematical Sciences [R01 GM117594-01];
   IBM Corporation Faculty Research Fund at the University of Chicago Booth
   School of Business
FX A.S. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and
   NSF via IIS-1149803, IIS-1447574, DMS-1264033, and NIH via R01
   GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
   Research at the Interface of the Biological and Mathematical Sciences.
   M. K. acknowledges support by an IBM Corporation Faculty Research Fund
   at the University of Chicago Booth School of Business.
CR Arnold BC, 2001, STAT SCI, V16, P249
   Berti P, 2014, J MULTIVARIATE ANAL, V125, P190, DOI 10.1016/j.jmva.2013.12.009
   BESAG J, 1974, J ROY STAT SOC B MET, V36, P192
   Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009
   Chen HY, 2010, STAT PROBABIL LETT, V80, P670, DOI 10.1016/j.spl.2009.12.025
   Dias R, 1998, J STAT COMPUT SIM, V60, P277, DOI 10.1080/00949659808811893
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   GOOD IJ, 1971, BIOMETRIKA, V58, P255, DOI 10.2307/2334515
   GU C, 1993, ANN STAT, V21, P217, DOI 10.1214/aos/1176349023
   GU C, 1995, STAT SINICA, V5, P709
   Gu C, 2003, STAT SINICA, V13, P811
   Gu C, 2013, STAT SINICA, V23, P1131, DOI 10.5705/ss.2011.319
   Jalali A., 2011, AISTATS, P378
   Jeon YH, 2006, STAT SINICA, V16, P353
   Kolar M, 2015, ADV NEURAL INFORM PR, V28, P2287
   LEONARD T, 1978, J ROY STAT SOC B MET, V40, P113
   Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Masse BR, 1999, CAN J STAT, V27, P819, DOI 10.2307/3316133
   Mei Song, 2016, ARXIV160706534
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   SILVERMAN BW, 1982, ANN STAT, V10, P795, DOI 10.1214/aos/1176345872
   SPEED TP, 1986, ANN STAT, V14, P138, DOI 10.1214/aos/1176349846
   Stone CJ, 1997, ANN STAT, V25, P1371
   Varin C, 2011, STAT SINICA, V21, P5
   Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053
   Wang YJ, 2008, BIOMETRIKA, V95, P735, DOI 10.1093/biomet/asn029
   Yang EH, 2015, J MACH LEARN RES, V16, P3813
   YANG Z, 2014, ARXIV14128697
   Yuan Xiaotong, 2016, ADV NEURAL INFORM PR, P4367
   Zhang HH, 2006, STAT SINICA, V16, P1021
   ZHAO T, 2015, ADV NEURAL INFORM PR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404050
DA 2019-06-15
ER

PT S
AU Sun, T
   Hannah, R
   Yin, WT
AF Sun, Tao
   Hannah, Robert
   Yin, Wotao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Asynchronous Coordinate Descent under More Realistic Assumption
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MODELS
AB Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding of these algorithms is limited because the current convergence theory of asynchronous block coordinate descent algorithms is based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update blocks is assumed to be independent of the block being updated. Additionally, it is assumed that the updates are applied to randomly chosen blocks.
   In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. The convergence theory involves a Lyapunov function that directly incorporates both objective progress and delays. A continuous-time ODE is provided to motivate the construction at a high level.
C1 [Sun, Tao] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
   [Hannah, Robert; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
RP Sun, T (reprint author), Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
EM nudtsuntao@163.com; RobertHannah89@math.ucla.edu; wotaoyin@math.ucla.edu
FU National Key R&D Program of China [2017YFB0202902]; China Scholarship
   Council; NSF [DMS-1720237]; ONR [N000141712162]
FX The work is supported in part by the National Key R&D Program of China
   2017YFB0202902, China Scholarship Council, NSF DMS-1720237, and ONR
   N000141712162
CR Cannelli L., 2016, ARXIV160704818
   Cannelli L., 2017, ARXIV170104900
   Chow Yat Tin, 2017, SIAM J SCI COMPUTING
   Davis D., 2016, ARXIV160400526
   Davis D, 2016, SCI COMPUT, P115, DOI 10.1007/978-3-319-41589-5_4
   De C. M. Sa, 2015, ADV NEURAL INFORM PR, P2674
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Hannah R., 2017, ARXIV170805136
   Hannah R., 2016, ARXIV160904746
   Lai MJ, 2013, SIAM J IMAGING SCI, V6, P1059, DOI 10.1137/120863290
   Leblond R., 2017, P MACHINE LEARNING R, V54, P46
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Mania Horia, 2015, ARXIV150706970
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Peng Z., 2016, ARXIV161204425
   Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950
   Sun Ruoyu, 2017, ARXIV160407130
   Xu Yangyang, 2017, ARXIV170506391
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406025
DA 2019-06-15
ER

PT S
AU Sundaresan, M
   Nabeel, A
   Sridharan, D
AF Sundaresan, Mali
   Nabeel, Arshed
   Sridharan, Devarajan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Mapping distinct timescales of functional interactions among brain
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRANGER CAUSALITY; LINEAR-DEPENDENCE; FEEDBACK; FMRI; DYNAMICS; TOOLBOX
AB Brain processes occur at various timescales, ranging from milliseconds (neurons) to minutes and hours (behavior). Characterizing functional coupling among brain regions at these diverse timescales is key to understanding how the brain produces behavior. Here, we apply instantaneous and lag-based measures of conditional linear dependence, based on Granger-Geweke causality (GC), to infer network connections at distinct timescales from functional magnetic resonance imaging (fMRI) data. Due to the slow sampling rate of fMRI, it is widely held that GC produces spurious and unreliable estimates of functional connectivity when applied to fMRI data. We challenge this claim with simulations and a novel machine learning approach. First, we show, with simulated fMRI data, that instantaneous and lag-based GC identify distinct timescales and complementary patterns of functional connectivity. Next, we analyze fMRI scans from 500 subjects and show that a linear classifier trained on either instantaneous or lag-based GC connectivity reliably distinguishes task versus rest brain states, with similar to 80-85% cross-validation accuracy. Importantly, instantaneous and lag-based GC exploit markedly different spatial and temporal patterns of connectivity to achieve robust classification. Our approach enables identifying functionally connected networks that operate at distinct timescales in the brain.
C1 [Sundaresan, Mali; Sridharan, Devarajan] Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India.
   [Nabeel, Arshed; Sridharan, Devarajan] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India.
RP Sridharan, D (reprint author), Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India.; Sridharan, D (reprint author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India.
EM s.malisundar@gmail.com; arshed@iisc.ac.in; sridhar@iisc.ac.in
RI Jeong, Yongwook/N-7413-2016
FU Wellcome Trust DBT-India Alliance Intermediate Fellowship, a SERB Early
   Career Research award; Wellcome Trust DBT-India Alliance Intermediate
   Fellowship; SERB Early Career Research award; Pratiksha Trust Young
   Investigator award; DBT-IISc Partnership program; Tata Trusts grant
FX This research was supported by a Wellcome Trust DBT-India Alliance
   Intermediate Fellowship, a SERB Early Career Research award, a Pratiksha
   Trust Young Investigator award, a DBT-IISc Partnership program grant,
   and a Tata Trusts grant (all to DS). We would like to thank Hritik Jain
   for help with data analysis.
CR Barnett L, 2017, J NEUROSCI METH, V275, P93, DOI 10.1016/j.jneumeth.2016.10.016
   Barnett L, 2014, J NEUROSCI METH, V223, P50, DOI 10.1016/j.jneumeth.2013.10.018
   Bastos AM, 2015, NEURON, V85, P390, DOI 10.1016/j.neuron.2014.12.018
   Chang C, 2008, NEUROIMAGE, V43, P90, DOI 10.1016/j.neuroimage.2008.06.030
   De Martino F, 2008, NEUROIMAGE, V43, P44, DOI 10.1016/j.neuroimage.2008.06.037
   Dhamala M, 2008, NEUROIMAGE, V41, P354, DOI 10.1016/j.neuroimage.2008.02.020
   Friston KJ, 2000, NEUROIMAGE, V12, P466, DOI 10.1006/nimg.2000.0630
   Ganguli S, 2008, NEURON, V58, P15, DOI 10.1016/j.neuron.2008.01.038
   Gelfand I. M., 1959, AM MATH SOC TRANSL 2, V12, P199
   GEWEKE J, 1982, J AM STAT ASSOC, V77, P304, DOI 10.2307/2287238
   GEWEKE JF, 1984, J AM STAT ASSOC, V79, P907, DOI 10.2307/2288723
   Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127
   Murray JD, 2014, NAT NEUROSCI, V17, P1661, DOI 10.1038/nn.3862
   Ojala M, 2010, J MACH LEARN RES, V11, P1833
   Rajan K, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188104
   Roebroeck A, 2005, NEUROIMAGE, V25, P230, DOI 10.1016/j.neuroimage.2004.11.017
   Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020
   Ryali S, 2011, NEUROIMAGE, V54, P807, DOI 10.1016/j.neuroimage.2010.09.052
   Seth AK, 2013, NEUROIMAGE, V65, P540, DOI 10.1016/j.neuroimage.2012.09.049
   Seth AK, 2010, J NEUROSCI METH, V186, P262, DOI 10.1016/j.jneumeth.2009.11.020
   Shirer WR, 2012, CEREB CORTEX, V22, P158, DOI 10.1093/cercor/bhr099
   Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063
   Sridharan D, 2008, P NATL ACAD SCI USA, V105, P12569, DOI 10.1073/pnas.0800005105
   Vidaurre D, 2017, P NATL ACAD SCI USA, V114, P12827, DOI 10.1073/pnas.1705120114
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404018
DA 2019-06-15
ER

PT S
AU Svenstrup, D
   Hansen, JM
   Winther, O
AF Svenstrup, Dan
   Hansen, Jonas Meinertz
   Winther, Ole
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hash Embeddings for Efficient Word Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by k d-dimensional embeddings vectors and one k dimensional weight vector. The final d dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of B embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.
C1 [Svenstrup, Dan; Winther, Ole] Tech Univ Denmark DTU, Dept Appl Math & Comp Sci, DK-2800 Lyngby, Denmark.
   [Hansen, Jonas Meinertz] FindZebra, Copenhagen, Denmark.
RP Svenstrup, D (reprint author), Tech Univ Denmark DTU, Dept Appl Math & Comp Sci, DK-2800 Lyngby, Denmark.
EM dsve@dtu.dk; jonas@findzebra.com; olwi@dtu.dk
RI Jeong, Yongwook/N-7413-2016
CR Argerich L., 2016, ABS160808940 CORR
   Bai B, 2009, P 18 ACM C INF KNOWL, P187
   Conneau  A., 2016, ABS160601781 CORR
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Huang E.H., 2012, ANN M ASS COMP LING, P873
   Huang P.-S., 2013, P 22 ACM INT C C INF, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   Ioffe Sergey, 2015, ABS150203167 CORR
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johansen A. R., 2016, ABS161006550 CORR
   Johnson R., 2016, ABS160900718 CORR
   Joulin A., 2016, ABS161203651 CORR
   Joulin A., 2016, ABS160701759 CORR
   Kingma D. P., 2014, ABS14126980 CORR
   Kulis B., 2009, ADV NEURAL INFORM PR, P1042
   Manning C. D., 1999, FDN STAT NATURAL LAN, V999
   Mihaltz M., 2016, GOOGLES TRAINED WORD
   Miyato T., 2016, STATISTICS, V1050, P25
   Reisinger J., 2010, HUMAN LANGUAGE TECHN, P109
   Stolcke A., 2000, CSCL0006025 CORR
   Weinberger K. Q., 2009, ABS09022206 CORR
   Xiao Y, 2016, ABS160200367 CORR
   Yogatama D., 2017, ARXIV170301898
   Zhang X., 2015, ABS150901626 CORR
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405001
DA 2019-06-15
ER

PT S
AU Swaminathan, A
   Krishnamurthy, A
   Agarwal, A
   Dudik, M
   Langford, J
   Jose, D
   Zitouni, I
AF Swaminathan, Adith
   Krishnamurthy, Akshay
   Agarwal, Alekh
   Dudik, Miroslav
   Langford, John
   Jose, Damien
   Zitouni, Imed
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Off-policy evaluation for slate recommendation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context-a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learningto-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased-these conditions are weaker than prior heuristics for slate evaluation-and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.
C1 [Swaminathan, Adith] Microsoft Res, Redmond, WA 98052 USA.
   [Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA.
   [Agarwal, Alekh; Dudik, Miroslav; Langford, John] Microsoft Res, New York, NY USA.
   [Jose, Damien; Zitouni, Imed] Microsoft, Redmond, WA USA.
RP Swaminathan, A (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM adswamin@microsoft.com; akshay@cs.umass.edu; alekha@microsoft.com;
   mdudik@microsoft.com; jcl@microsoft.com; dajose@microsoft.com;
   izitouni@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Asadi Nima, 2013, EUR C ADV INF RETR
   Auer P., 2002, J MACHINE LEARNING R
   Auer Peter, 2002, SIAM J COMPUTING
   Bottou  L., 2013, J MACHINE LEARNING R
   Bubeck  S., 2012, FDN TRENDS MACHINE L
   Burges Chris, 2005, INT C MACH LEARN
   Cesa-Bianchi Nicolo, 2012, J COMPUTER SYSTEM SC
   Chapelle Olivier, 2009, C INF KNOWL MAN
   Chapelle Olivier, 2009, INT C WORLD WID WEB
   Chu Wei, 2011, ARTIFICIAL INTELLIGE
   Dani Varsha, 2008, ADV NEURAL INFORM PR
   Dudik  M., 2014, STAT SCI
   Dudik Miroslav, 2011, INT C MACH LEARN
   Dupret Georges E., 2008, SIGIR C RES DEV INF
   Filippi Sarah, 2010, ADV NEURAL INFORM PR
   Guo Fan, 2009, INT C WORLD WID WEB
   Hofmann K., 2016, FDN TRENDS INFORM RE
   Horvitz D. G., 1952, J AM STAT ASS
   Kale Satyen, 2010, ADV NEURAL INFORM PR
   Kohavi Ron, 2009, KNOWLEDGE DISCOVERY
   Krishnamurthy Akshay, 2016, ADV NEURAL INFORM PR
   Kveton Branislav, 2015, ARTIFICIAL INTELLIGE
   Langford John, 2008, INT C MACH LEARN
   Langford  John, 2008, ADV NEURAL INFORM PR
   Li L., 2010, INT C WORLD WID WEB
   Li Lihong, 2011, INT C WEB SEARCH DAT
   Li Lihong, 2015, INT C WEB SEARCH DAT
   Petersen KB, 2008, MATRIX COOKBOOK
   Qin Lijing, 2014, INT C DAT MIN
   Qin T., 2013, ARXIV13062597
   Rusmevichientong Paat, 2010, MATH OPERATIONS RES
   Swaminathan Adith, 2015, INT C MACH LEARN
   Tax Niek, 2015, INFORM PROCESSING MA
   Wang Yu-Xiang, 2017, INT C MACH LEARN
   Wang Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P103, DOI 10.1145/2835776.2835824
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403068
DA 2019-06-15
ER

PT S
AU Syrgkanis, V
AF Syrgkanis, Vasilis
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Sample Complexity Measure with Applications to Learning Optimal
   Auctions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis H and for any sample S of size m, the split-sample growth rate (T) over cap (H) (m) counts how many different hypotheses can empirical risk minimization output on any sub-sample of S of size m/2. We show that the expected generalization error is upper bounded by O (root log((T) over bar (H)(2m))/m). Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.
C1 [Syrgkanis, Vasilis] Microsoft Res, Redmond, WA 98052 USA.
RP Syrgkanis, V (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM vasy@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Balcan M.-F., 2016, P 30 C NEUR INF PROC, P2083
   Balcan MF, 2005, ANN IEEE SYMP FOUND, P605, DOI 10.1109/SFCS.2005.50
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Devanur NR, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P426, DOI 10.1145/2897518.2897553
   Gonczarowski Yannai A., 2016, CORR
   Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   Morgenstern Jamie, 2016, COLT 2016
   Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR, P136
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Pollard D., 2011, SPRINGER SERIES STAT
   Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P1, DOI 10.1145/2940716.2940723
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405042
DA 2019-06-15
ER

PT S
AU Taghvaei, A
   Kim, JW
   Mehta, PG
AF Taghvaei, Amirhossein
   Kim, Jin W.
   Mehta, Prashant G.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI How regularization affects the critical points in linear networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network. In recent years, there is a growing interest in the study of such networks, in part due to the successes of deep learning. The main question of this body of research (and also of our paper) is related to the existence and optimality properties of the critical points of the mean-squared loss function. An additional primary concern of our paper pertains to the robustness of these critical points in the face of (a small amount of) regularization. An optimal control model is introduced for this purpose and a learning algorithm (backprop with weight decay) derived for the same using the Hamilton's formulation of optimal control. The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation. Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation.
C1 [Taghvaei, Amirhossein; Kim, Jin W.; Mehta, Prashant G.] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
RP Taghvaei, A (reprint author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
EM taghvae2@illinois.edu; kim684@illinois.edu; mehtapg@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CMMI grant [1462773]
FX Financial support from the NSF CMMI grant 1462773 is gratefully
   acknowledged.
CR BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248
   BHOJANAPALLI S, 2016, ADV NEURAL INFORM PR, P3873
   Choromanska A., 2015, P 28 C LEARN THEOR C, P1756
   Choromanska Anna, 2015, AISTATS
   Clewley R.H., 2007, PYDSTOOL SOFTWARE EN
   CULVER WJ, 1966, P AM MATH SOC, V17, P1146, DOI 10.2307/2036109
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   FAROTIMI O, 1991, IEEE T NEURAL NETWOR, V2, P378, DOI 10.1109/72.97914
   Ge R., 2015, ARXIV150302101
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gunasekar S., 2017, ARXIV170509280
   Hardt M., 2016, ARXIV161104231
   Higham N. J., 2014, FUNCTIONS MATRICES
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Le Cun Y, 1988, P 1988 CONN MOD SUMM, V1, P21
   Lee J. D., 2016, ARXIV160204915
   Neyshabur B., 2014, ARXIV14126614
   Saxe A.M., 2013, ARXIV13126120
   Soudry D, 2016, ARXIV160508361
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113
   Zhang C, 2016, ARXIV161103530
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402054
DA 2019-06-15
ER

PT S
AU Takeishi, N
   Kawahara, Y
   Yairi, T
AF Takeishi, Naoya
   Kawahara, Yoshinobu
   Yairi, Takehisa
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SPECTRAL PROPERTIES; SYSTEMS; PATTERNS
AB Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.
C1 [Takeishi, Naoya; Yairi, Takehisa] Univ Tokyo, Dept Aeronaut & Astronaut, Tokyo, Japan.
   [Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.
   [Kawahara, Yoshinobu] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.
RP Takeishi, N (reprint author), Univ Tokyo, Dept Aeronaut & Astronaut, Tokyo, Japan.
EM takeishi@ailab.t.u-tokyo.ac.jp; ykawahara@sanken.osaka-u.ac.jp;
   yairi@ailab.t.u-tokyo.ac.jp
RI Jeong, Yongwook/N-7413-2016
FU JSPS KAKENHI [JP15J09172, JP26280086, JP16H01548, JP26289320]
FX This work was supported by JSPS KAKENHI Grant No. JP15J09172,
   JP26280086, JP16H01548, and JP26289320.
CR Arbabi H, 2017, SIAM J APPL DYN SYST, V16, P2096, DOI 10.1137/17M1125236
   Berger E, 2015, ADV ROBOTICS, V29, P331, DOI 10.1080/01691864.2014.981292
   Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010
   Brunton SL, 2016, P NATL ACAD SCI USA, V113, P3932, DOI 10.1073/pnas.1517384113
   Brunton SL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150171
   Budisic M, 2012, CHAOS, V22, DOI 10.1063/1.4772195
   Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Froyland G, 2014, SIAM J APPL DYN SYST, V13, P1816, DOI 10.1137/130943637
   Gao Y., 2016, ADV NEURAL INFORM PR, V29, P163
   Garcia SP, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.027205
   Ghahramani Z, 1999, ADV NEUR IN, V11, P431
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hirata Y, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.026202
   Hirsch MW, 2013, DIFFERENTIAL EQUATIONS, DYNAMICAL SYSTEMS, AND AN INTRODUCTION TO CHAOS, 3RD EDITION, P1
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Johnson M., 2016, ADV NEURAL INFORM PR, V29, P2946
   Kar P, 2014, ADV NEURAL INFORM PR, P694
   Karl M., 2017, P 5 INT C LEARN REPR
   Kawahara Y., 2016, ADV NEURAL INFORM PR, V29, P911
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315
   Krishnan R. G., 2017, AAAI, P2101
   Kutz J. N., 2016, DYNAMIC MODE DECOMPO
   Kutz J.N., 2016, ARXIV160207647
   Kutz JN, 2016, SIAM J APPL DYN SYST, V15, P713, DOI 10.1137/15M1023543
   Lasota A., 1994, CHAOS FRACTALS NOISE
   Li P, 2017, CHAOS, V27, DOI 10.1063/1.4982794
   Liu S, 2013, NEURAL NETWORKS, V43, P72, DOI 10.1016/j.neunet.2013.01.012
   LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
   Lusch  B., 2017, ARXIV171209707
   Mardt A., 2017, ARXIV171006012
   Mauroy A, 2016, IEEE DECIS CONTR P, P6500, DOI 10.1109/CDC.2016.7799269
   Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x
   Mezic I, 2013, ANNU REV FLUID MECH, V45, P357, DOI 10.1146/annurev-fluid-011212-140652
   Otto S. E., 2017, ARXIV171201378
   Proctor JL, 2016, SIAM J APPL DYN SYST, V15, P142, DOI 10.1137/15M1013857
   Proctor JL, 2015, INT HEALTH, V7, P139, DOI 10.1093/inthealth/ihv009
   ROSSLER OE, 1976, PHYS LETT A, V57, P397, DOI 10.1016/0375-9601(76)90101-8
   Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059
   SAUER T, 1991, J STAT PHYS, V65, P579, DOI 10.1007/BF01053745
   Schmid PJ, 2010, J FLUID MECH, V656, P5, DOI 10.1017/S0022112010001217
   Susuki Y, 2015, IEEE DECIS CONTR P, P7022, DOI 10.1109/CDC.2015.7403326
   Takeishi N, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.033310
   Takens F., 1981, LECT NOTES MATH, V1980, P366, DOI DOI 10.1007/BFB0091924
   Tu JH, 2014, J COMPUT DYN, V1, P391, DOI DOI 10.3934/JCD.2014.1.391
   Vlachos I, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.016207
   Vladimir R., 1997, MAT VESNIK, V49, P163
   Watter M., 2015, ADV NEURAL INFORM PR, P2746
   Weigend A. S., 1993, TIME SERIES PREDICTI
   Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5
   YEUNG  E., 2017, ARXIV170806850
NR 52
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401017
DA 2019-06-15
ER

PT S
AU Tanczos, E
   Nowak, R
   Mankoff, B
AF Tanczos, Ervin
   Nowak, Robert
   Mankoff, Bob
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A KL-LUCB Bandit Algorithm for Large-Scale Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB, offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions, which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.
C1 [Tanczos, Ervin; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA.
RP Tanczos, E (reprint author), Univ Wisconsin, Madison, WI 53706 USA.
EM tanczos@wisc.edu; rdnowak@wisc.edu; bmankoff@hearst.com
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1447449]; AFSOR [FA9550-13-1-0138]
FX This work was partially supported by the NSF grant IIS-1447449 and the
   AFSOR grant FA9550-13-1-0138.
CR Audibert J. Y., 2010, COLT 23 C LEARN THEO, P13
   Boucheron S., 2013, CONCENTRATION INEQUA
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Fox Rubin B., 2016, CNET NEWS
   Garivier A., 2011, P 24 ANN C LEARN THE, P359
   Jamieson K. G., 2014, P 27 C LEARN THEOR, V35, P423
   Jamieson Kevin G, 2015, ADV NEURAL INFORM PR, P2656
   Kaufmann E., 2013, C LEARN THEOR, P228
   Kaufmann Emilie, 2016, J MACHINE LEARNING R
   Maillard Odalric- Ambrym, 2011, COLT, P497
   Simchowitz M., 2017, ARXIV170205186
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405094
DA 2019-06-15
ER

PT S
AU Tarvainen, A
   Valpola, H
AF Tarvainen, Antti
   Valpola, Harri
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Mean teachers are better role models: Weight-averaged consistency
   targets improve semi-supervised deep learning results
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NEURAL-NETWORKS
AB The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.
C1 [Tarvainen, Antti; Valpola, Harri] Curious AI Co, Helsinki, Finland.
RP Tarvainen, A (reprint author), Curious AI Co, Helsinki, Finland.
EM tarvaina@cai.fi; harri@cai.fi
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Bachman Philip, 2014, ARXIV14124864CSSTAT
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Gal Y., 2016, INT C MACH LEARN, P1050
   Gastaldi Xavier, 2017, ARXIV170507485CS
   Goodfellow IJ, 2014, ARXIV14126572
   Guo  C., 2017, ARXIV170604599CS
   He K., 2015, ARXIV151203385CS
   Hinton  G., 2015, ARXIV150302531CSSTAT
   Hu J., 2017, ARXIV170901507CS
   Huang Gao, 2016, ARXIV160309382CS
   Kingma D. P., 2014, ARXIV14126980CS
   Laine Samuli, 2016, ARXIV161002242CS
   Maas A. L., 2013, P ICML
   Miyato T, 2017, ARXIV170403976CSSTAT
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Papernot Nicolas, 2015, ARXIV151104508CSSTAT
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Pu Yunchen, 2016, ARXIV160908976CSSTAT
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Russakovsky O., 2014, ARXIV14090575CS
   Sajjadi Mehdi, 2016, ADV NEURAL INFORM PR, P1163
   Salimans T., 2016, ADV NEURAL INFORM PR, P901
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Sarela J, 2005, J MACH LEARN RES, V6, P233
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Singh Saurabh, 2016, ARXIV160506465CS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wager Stefan, 2013, ARXIV13071493CSSTAT
   Wan L, 2013, REGULARIZATION NEURA, P1058
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Xie Saining, 2016, ARXIV161105431CS
   Zhu  X., 2002, LEARNING LABELED UNL
NR 33
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401023
DA 2019-06-15
ER

PT S
AU Teh, YW
   Bapst, V
   Czarnecki, WM
   Quan, J
   Kirkpatrick, J
   Hadsell, R
   Heess, N
   Pascanu, R
AF Teh, Yee Whye
   Bapst, Victor
   Czarnecki, Wojciech Marian
   Quan, John
   Kirkpatrick, James
   Hadsell, Raia
   Heess, Nicolas
   Pascanu, Razvan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Distral: Robust Multitask Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust to hyperparameter settings and more stable-attributes that are critical in deep reinforcement learning.
C1 [Teh, Yee Whye; Bapst, Victor; Czarnecki, Wojciech Marian; Quan, John; Kirkpatrick, James; Hadsell, Raia; Heess, Nicolas; Pascanu, Razvan] DeepMind, London, England.
RP Teh, YW (reprint author), DeepMind, London, England.
RI Jeong, Yongwook/N-7413-2016
CR Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bengio Yoshua, 2012, JMLR WORKSH UNS TRAN
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Bucila Cristian, 2006, P INT C KNOWL DISC D
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Choromanska Anna E., 2015, ADV NEURAL INFORM PR
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fox R., 2016, UNCERTAINTY ARTIFICI
   Fox Roy, 2016, EUR WORKSH REINF LEA
   Gelman A., 2014, BAYESIAN DATA ANAL, V2
   Haarnoja T., 2017, ARXIV170208165
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Jaderberg Max, 2016, INT C LEARN REPR ICL
   Kappen HJ, 2012, MACH LEARN, V87, P159, DOI 10.1007/s10994-012-5278-7
   Lample Guillaume, 2017, PLAYING FPS GAMES DE
   Levine S., 2014, P 31 INT C MACH LEAR, P829
   Levine  S., 2013, ADV NEURAL INFORM PR, V2013, P207
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Levine S, 2016, J MACH LEARN RES, V17
   Mirowski Piotr, 2016, INT C LEARN REPR ICL
   Mnih V., 2016, INT C MACH LEARN ICM
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nachum Ofir, 2017, ARXIV170208892
   Parisotto Emilio, 2016, INT C LEARN REPR ICL
   Pascanu R., 2014, INT C LEARN REPR ICL
   Rawlik Konrad, 2012, ROBOTICS SCI SYSTEMS
   Rusu A. A., 2016, INT C LEARN REPR ICL
   Schaul  Tom, 2015, ABS151105952 CORR
   Schulman J., 2015, INT C MACH LEARN ICM
   Schulman J., 2017, ARXIV170406440
   SUTTON R.S., 1999, NIPS, V99, P1057
   Taylor ME, 2011, AI MAG, V32, P15, DOI 10.1609/aimag.v32i1.2329
   Toussaint Marc, 2006, EDIINFRR0934 U ED SC
   VanHasselt Hado, 2016, DEEP REINFORCEMENT L
   Yosinski J., 2014, ADV NEURAL INFORM PR
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404055
DA 2019-06-15
ER

PT S
AU Thewlis, J
   Bilen, H
   Vedaldi, A
AF Thewlis, James
   Bilen, Hakan
   Vedaldi, Andrea
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Unsupervised learning of object frames by dense equivariant image
   labelling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.
C1 [Thewlis, James; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England.
   [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.
RP Thewlis, J (reprint author), Univ Oxford, Visual Geometry Grp, Oxford, England.
EM jdt@robots.ox.ac.uk; hbilen@ed.ac.uk; vedaldi@robots.ox.ac.uk
FU AIMS CDT (EPSRC) [EP/L015897/1]; ERC [677195-IDIU]
FX This work acknowledges the support of the AIMS CDT (EPSRC EP/L015897/1)
   and ERC 677195-IDIU. Clipart: FreePik.
CR Agrawal Pulkit, 2015, P ICCV
   Bengio Y., 2009, FDN TRENDS MACHINE L
   Bookstein F. L., 1989, PAMI, P3
   Bourlard H., 1988, BIOL CYBERNETICS
   Burgos-Artizzu Xavier P., 2013, P ICCV
   Cootes T F, 1995, CVIU
   Dalal N., 2005, P CVPR
   Doersch C., 2015, P ICCV
   Donahue  J., 2017, P ICLR
   Dumoulin V., 2017, P ICLR
   Felzenszwalb Pedro F., 2010, PAMI
   Fergus Rob, 2003, P CVPR
   Fernando Basura, 2017, P CVPR
   Fischer Philipp, 2015, P ICCV
   GARG R, 2016, P EUR C COMPUT VIS, V9912, P740, DOI DOI 10.1007/978-3-319-46484-8_45
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hinton Geoffrey, 2006, SCIENCE
   Horn B. K., 1981, ARTIFICIAL INTELLIGE
   Ilg Eddy, 2016, ARXIV161201925
   Jaderberg M., 2015, P NIPS
   Kanazawa A., 2016, P CVPR
   Kemelmacher-Shlizerman Ira, 2012, P CVPR
   Koestinger M, 2011, 1 IEEE INT WORKSH BE
   Learned-Miller Erik G, 2006, IEEE T PATTERN ANAL
   Liu C., 2011, PAMI
   Liu Ziwei, 2015, P ICCV
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Misra Ishan, 2016, P ECCV
   Mobahi Hossein, 2014, P CVPR
   Newcombe R. A., 2015, P CVPR
   Noroozi M., 2016, P ECCV
   Novotny D., 2017, P ICCV
   Pathak D., 2017, P CVPR
   Pathak D., 2016, P CVPR
   Rocco I., 2017, P CVPR
   Schmidt T, 2017, IEEE ROBOT AUTOM LET, V2, P420, DOI 10.1109/LRA.2016.2634089
   Sun Y., 2013, P CVPR
   Tassa Yuval, CAPSIM MATLAB PHYS E
   Thewlis J., 2017, P ICCV
   Thewlis James, 2016, P BMVC
   Weber Markus, 2000, P CVPR
   Xiao Shengtao, 2016, P ECCV
   Yu F., 2016, P ICLR
   Yu Xiang, 2016, P ECCV
   Zhang J., 2014, P ECCV
   Zhang R., 2016, P ECCV
   Zhang Weiwei, 2008, P ECCV
   Zhang Z., 2014, P ECCV
   Zhang Zhanpeng, 2016, PAMI
   Zhou T., 2016, P CVPR
   Zhou T., 2015, P CVPR
   Zhou T., 2017, P CVPR
NR 52
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400081
DA 2019-06-15
ER

PT S
AU Tian, K
   Kong, WH
   Valiant, G
AF Tian, Kevin
   Kong, Weihao
   Valiant, Gregory
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Populations of Parameters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Consider the following estimation problem: there are n entities, each with an unknown parameter p(i) is an element of [0, 1], and we observe n independent random variables, X-1, . . . , X-n, with X-i similar to Binomial (t, p(i)). How accurately can one recover the "histogram" (i.e. cumulative density function) of the p(i)'s? While the empirical estimates would recover the histogram to earth mover distance circle minus(1/root t) (equivalently, l(1) distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O (1/t) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, sports analytics, and variation in the gender ratio of offspring.
C1 [Tian, Kevin; Kong, Weihao; Valiant, Gregory] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Tian, K (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM kjtian@stanford.edu; whkong@stanford.edu; valiant@stanford.edu
FU NSF CAREER [CCF-1351108]; ONR [N00014-17-1-2562]; NSF Graduate
   Fellowship [DGE-1656518]; Google Faculty Fellowship
FX We thank Kaja Borge and Ane Nodtvedt for sharing an anonymized dataset
   on sex composition of dog litters, based on data collected by the
   Norwegian Kennel Club. This research was supported by NSF CAREER Award
   CCF-1351108, ONR Award N00014-17-1-2562, NSF Graduate Fellowship
   DGE-1656518, and a Google Faculty Fellowship.
CR Acharya J, 2009, ITW: 2009 IEEE INFORMATION THEORY WORKSHOP ON NETWORKING AND INFORMATION THEORY, P251, DOI 10.1109/ITWNIT.2009.5158581
   Acharya Jayadev, 2016, ARXIV161102960
   Bagby Thomas, 2002, CONSTRUCTIVE APPROXI, V18
   BARLOW P, 1970, NATURE, V226, P961, DOI 10.1038/226961a0
   Daskalakis C, 2015, ALGORITHMICA, V72, P316, DOI 10.1007/s00453-015-9971-3
   Diakonikolas I., 2016, P 29 C LEARN THEOR C, P850
   Kong W., 2016, ARXIV160200061
   KORNEICHUK E N. P, 1991, EXACT CONSTANTS APPR, V38
   Levi R., 2013, THEORY COMPUT, V9, P295
   Levi R, 2014, SIAM J DISCRETE MATH, V28, P1699, DOI 10.1137/120903737
   Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113
   Orlitsky Alon, 2004, P 20 C UNC ART INT B, P426
   Penfold LM, 1998, MOL REPROD DEV, V50, P323, DOI 10.1002/(SICI)1098-2795(199807)50:3<323::AID-MRD8>3.0.CO;2-L
   Stein C., 1956, P 3 BERK S MATH STAT, V1, P197
   Valiant G, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P142, DOI 10.1145/2897518.2897641
   Valiant G, 2011, ACM S THEORY COMPUT, P685
   Valiant P., 2013, P ADV NEUR INF PROC, P2157
   Zou James, 2016, NATURE COMMUNICATION, V7
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405083
DA 2019-06-15
ER

PT S
AU Tian, YD
   Gong, QC
   Shang, WL
   Wu, YX
   Zitnick, CL
AF Tian, Yuandong
   Gong, Qucheng
   Shang, Wenling
   Wu, Yuxin
   Zitnick, C. Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI ELF: An Extensive, Lightweight and Flexible Research Platform for
   Real-time Strategy Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AT more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https : //github.com/facebookresearch/ELF.
C1 [Tian, Yuandong; Gong, Qucheng; Wu, Yuxin; Zitnick, C. Lawrence] Facebook AI Res, Menlo Pk, CA 94025 USA.
   [Shang, Wenling] Oculus, Menlo Pk, CA USA.
RP Tian, YD (reprint author), Facebook AI Res, Menlo Pk, CA 94025 USA.
EM yuandong@fb.com; qucheng@fb.com; wendy.shang@oculus.com; yuxinwu@fb.com;
   zitnick@fb.com
RI Jeong, Yongwook/N-7413-2016
CR Babaeizadeh Mohammad, 2017, INT C LEARN REPR ICL
   BattleCode, 2000, BATTL MITS AI PROGR
   Beattie C., 2016, CORR
   Bellemare M. G., 2012, CORR
   Bhonker Nadav, 2016, CORR
   Brockman G., 2016, CORR
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Buro Michael, 2005, GAMEON NA 2005 C MON, P23
   Chaslot GMJB, 2008, LECT NOTES COMPUT SC, V5131, P60, DOI 10.1007/978-3-540-87608-3_6
   Coumans Erwin, 2010, BULLET PHYS ENGINE
   Ioffe S., 2015, ICML
   Johnson Matthew, 2016, P 25 INT JOINT C ART, P4246
   Kempka M, 2016, ARXIV160502097
   Lample G., 2016, ARXIV160905521
   Maas A. L., 2013, RECTIFIER NONLINEARI
   Maas A. L., 2013, P ICML
   Mirowski  Piotr, 2017, ICLR
   Mnih V., 2016, ARXIV160201783
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair A., 2015, CORR
   Ontanon S., 2013, P 9 AAAI C ART INT I, P58
   Peng Peng, 2017, CORR
   Pumpkin Studios, 1999, WARZ 2100
   SCB Statistics Sweden, 2019, NON TRADITIONAL REF
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sukhbaatar Sainbayar, 2015, CORR
   SUTTON R.S., 1999, NIPS, V99, P1057
   Synnaeve Gabriel, 2016, CORR
   Tian Y., 2015, ARXIV151106410
   Usunier Nicolas, 2017, ICLR
   Wu Yuhuai, 2017, INT C LEARN REPR ICL
NR 32
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402069
DA 2019-06-15
ER

PT S
AU Tibshirani, RJ
AF Tibshirani, Ryan J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections,
   Insights, and Extensions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DUAL ASCENT METHODS; RELAXATION METHODS; CONVERGENCE; DECOMPOSITION;
   PROJECTIONS; RULES; COSTS
AB We study connections between Dykstra's algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the penalty is a separable sum of support functions, is exactly equivalent to Dykstra's algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra's algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.
C1 [Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
   [Tibshirani, Ryan J.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
RP Tibshirani, RJ (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.; Tibshirani, RJ (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM ryantibs@stat.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Auslender A., 1976, OPTIMISATION METHODE
   Bauschke H., 2000, OPTIMIZATION, V48, P409
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Bauschke Heinz H., 2013, ARXIV13014506
   Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Boyle J. P., 1986, LECTURE NOTES STATIS, V37, P28
   Censor Y, 1998, COMMUN APPL ANAL, V2, P407
   Chang TH, 2016, IEEE T SIGNAL PROCES, V64, P3118, DOI 10.1109/TSP.2016.2537271
   Chang TH, 2016, IEEE T SIGNAL PROCES, V64, P3131, DOI 10.1109/TSP.2016.2537261
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   DEUTSCH F, 1994, NUMER FUNC ANAL OPT, V15, P537, DOI 10.1080/01630569408816580
   Deutsch F. R., 2001, BEST APPROXIMATION I
   Douglas J., 1956, T AM MATH SOC, V82, P421, DOI [DOI 10.1090/S0002-9947-1956-0084194-4, 10.1090/S0002-9947-1956-0084194-4]
   DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   El Ghaoui L, 2012, PAC J OPTIM, V8, P667
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712
   Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1
   Gabay D., 1983, STUD MATH APPL, V15, P299, DOI [DOI 10.1016/S0168-2024(08)70034-1.HTTP://WWW.SCIENCEDIRECT.C0M/SCIENCE/ARTICLE/PII/S0168202408700341, DOI 10.1016/S0168-2024(08)70034-1]
   Gaffke N., 1989, METRIKA, V36, P29, DOI DOI 10.1007/BF02614077
   GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41
   Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219
   Halperin I., 1962, ACTA SCI MATH SZEGED, V23, P96
   HAN SP, 1988, MATH PROGRAM, V40, P1, DOI 10.1007/BF01580719
   Hildreth Clifford, 1957, NAV RES LOG, V4, P79, DOI 10.1002/nay.3800040113
   Hong MY, 2017, MATH PROGRAM, V162, P165, DOI 10.1007/s10107-016-1034-2
   IUSEM AN, 1987, SIAM J CONTROL OPTIM, V25, P231, DOI 10.1137/0325014
   IUSEM AN, 1990, MATH PROGRAM, V47, P37, DOI 10.1007/BF01580851
   Jaggi M., 2014, ADV NEURAL INFORM PR, P3068
   Kadkhodaie Mojtaba, 2015, P 21 ACM SIGKDD INT, V21, P497
   Li Xingguo, 2016, INT C ART INT STAT A, V19, P491
   LIONS PL, 1979, SIAM J NUMER ANAL, V16, P964, DOI 10.1137/0716071
   Luenberger D. G., 1973, INTRO LINEAR NONLINE
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   LUO ZQ, 1993, MATH OPER RES, V18, P846, DOI 10.1287/moor.18.4.846
   Nishihara Robert, 2015, P 32 INT C MACH LEAR, V32, P343
   Ortega J. M., 1970, ITERATIVE SOLUTION N
   PIERRA G, 1984, MATH PROGRAM, V28, P96, DOI 10.1007/BF02612715
   Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6
   Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Tibshirani RJ, 2013, ELECTRON J STAT, V7, P1456, DOI 10.1214/13-EJS815
   TSENG P, 1993, MATH PROGRAM, V59, P231, DOI 10.1007/BF01581245
   TSENG P, 1990, SIAM J CONTROL OPTIM, V28, P214, DOI 10.1137/0328011
   TSENG P, 1987, MATH PROGRAM, V38, P303, DOI 10.1007/BF02592017
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   von Neumann J., 1950, FUNCTIONAL OPERATORS, V2
   Wang J, 2015, J MACH LEARN RES, V16, P1063
   WARGA J, 1963, J SOC IND APPL MATH, V11, P588, DOI 10.1137/0111043
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147
NR 56
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400050
DA 2019-06-15
ER

PT S
AU Tran, T
   Pham, T
   Carneiro, G
   Palmer, L
   Reid, I
AF Toan Tran
   Trung Pham
   Carneiro, Gustavo
   Palmer, Lyle
   Reid, Ian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Bayesian Data Augmentation Approach for Learning Deep Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID NETWORKS
AB Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm - generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above - the results also show that our approach produces better classification results than similar GAN models.
C1 [Toan Tran; Trung Pham; Carneiro, Gustavo; Reid, Ian] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.
   [Palmer, Lyle] Univ Adelaide, Sch Publ Hlth, Adelaide, SA, Australia.
RP Tran, T (reprint author), Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.
EM toan.m.tran@adelaide.edu.au; trung.pham@adelaide.edu.au;
   gustavo.carneiro@adelaide.edu.au; lyle.palmer@adelaide.edu.au;
   ian.reid@adelaide.edu.au
RI Jeong, Yongwook/N-7413-2016; Tran, Toan/P-8774-2019
OI Tran, Toan/0000-0001-7182-7548
FU Vietnam International Education Development (VIED); Australian Research
   Council through the Centre of Excellence for Robotic Vision
   [CE140100016]; Australian Research Council through Laureate Fellowship
   [FL130100102]
FX TT gratefully acknowledges the support by Vietnam International
   Education Development (VIED). TP, GC and IR gratefully acknowledge the
   support of the Australian Research Council through the Centre of
   Excellence for Robotic Vision (project number CE140100016) and Laureate
   Fellowship FL130100102 to IR.
CR Bishop C., 2007, PATTERN RECOGNITION
   Carreira-Perpignan M. A., 2005, P 10 INT WORKSH ART, V5, P33
   Chen X., 2016, ADV NEURAL INFORM PR
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Cui XD, 2015, IEEE-ACM T AUDIO SPE, V23, P1469, DOI 10.1109/TASLP.2015.2438544
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Deng J, 2009, IEEE C COMP VIS PATT
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Fawzi A, 2016, IEEE IMAGE PROC, P3688, DOI 10.1109/ICIP.2016.7533048
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hauberg S, 2016, ARTIF INTELL, P342
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li C., 2017, CORR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Odena A., 2016, ARXIV161009585
   Odena A., 2016, ARXIV160601583
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Simard P. Y., 2003, P 7 INT C DOC AN REC, V2
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Tanner M. A., 1991, LECT NOTES STAT, V67
   TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457
   Yaeger L., 1996, NIPS, P807
   Zhang X., 2015, ARXIV150201710
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402082
DA 2019-06-15
ER

PT S
AU Tran, D
   Ranganath, R
   Blei, DM
AF Tran, Dustin
   Ranganath, Rajesh
   Blei, David M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hierarchical Implicit Models and Likelihood-Free Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.
C1 [Tran, Dustin; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA.
RP Tran, D (reprint author), Columbia Univ, New York, NY 10027 USA.
RI Jeong, Yongwook/N-7413-2016
FU Adobe Research Fellowship; NSF [IIS-0745520, IIS-1247664, IIS-1009542];
   ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; John
   Templeton Foundation; Google Ph.D. Fellowship in Machine Learning;
   Facebook; Adobe; Amazon
FX We thank Balaji Lakshminarayanan for discussions which helped motivate
   this work. We also thank Christian Naesseth, Jaan Altosaar, and Adji
   Dieng for their feedback and comments. DT is supported by a Google Ph.D.
   Fellowship in Machine Learning and an Adobe Research Fellowship. This
   work is also supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR
   N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Facebook,
   Adobe, Amazon, and the John Templeton Foundation.
CR Anelli G, 2008, J INSTRUM, V3, DOI 10.1088/1748-0221/3/08/S08007
   Bayer Justin, 2014, ARXIV14117610
   Beaumont MA, 2010, ANNU REV ECOL EVOL S, V41, P379, DOI 10.1146/annurev-ecolsys-102209-144621
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blundell Charles, 2015, INT C MACH LEARN
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chen X., 2016, NEURAL INFORM PROCES
   Cranmer K., 2015, ARXIV150602169
   Diaconis P, 2007, SIAM REV, V49, P211, DOI 10.1137/S0036144504446436
   Dieng A. B., 2017, NEURAL INFORM PROCES
   DIGGLE PJ, 1984, J ROY STAT SOC B MET, V46, P193
   Donahue J., 2017, INT C LEARN REPR
   Dumoulin V., 2017, INT C LEARN REPR
   Fraccaro M., 2016, NEURAL INFORM PROCES
   Gelman A., 2006, DATA ANAL USING REGR
   Gelman A., 2013, TEXTS STAT SCI SERIE
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Goodfellow I., 2014, NEURAL INFORM PROCES
   Gutmann Michael U, 2014, ARXIV14074981
   Hartig F, 2011, ECOL LETT, V14, P816, DOI 10.1111/j.1461-0248.2011.01640.x
   Hernandez-Lobato J. M., 2016, INT C MACH LEARN
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan M. I., 1999, MACHINE LEARNING
   Karaletsos T., 2016, NIPS WORKSH
   KELLER JB, 1986, AM MATH MON, V93, P191, DOI 10.2307/2323340
   Kingma Diederik, 2014, INT C LEARN REPR
   Kusner M. J., 2016, NIPS WORKSH
   Larsen A. B. L., 2016, INT C MACH LEARN
   Li Y., 2016, NEURAL INFORM PROCES
   Liu Q., 2016, ARXIV161200081
   Mackay D. J. C., 1992, THESIS
   Makhzani A., 2015, ARXIV151105644
   Marin JM, 2012, STAT COMPUT, V22, P1167, DOI 10.1007/s11222-011-9288-2
   Mescheder L., 2017, ARXIV170104722
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Mohamed S., 2016, ARXIV161003483
   Murphy KP, 2012, MACHINE LEARNING PRO
   Neal R N, 1994, THESIS
   Papamakarios G., 2016, NEURAL INFORM PROCES
   Pearl J, 2000, CAUSALITY
   Pritchard JK, 1999, MOL BIOL EVOL, V16, P1791, DOI 10.1093/oxfordjournals.molbev.a026091
   Radford A., 2016, INT C LEARN REPR
   Ranganath R., 2016, INT C MACH LEARN
   Ranganath R., 2016, NEURAL INFORM PROCES
   Ranganath Rajesh, 2014, ARTIFICIAL INTELLIGE
   Rezende D. J., 2015, INT C MACH LEARN
   Rezende D. J., 2014, INT C MACH LEARN
   Salimans T, 2015, INT C MACH LEARN
   Sugiyama M., 2012, ANN I STAT MATH
   Teh Y.W., 2010, BAYESIAN NONPARAMETR, V1
   Tran D., 2017, ARXIV171010742
   Tran D., 2015, NEURAL INFORM PROCES
   Tran D., 2016, ARXIV161009787
   Uehara M., 2016, ARXIV161002920
   Wilkinson D. J., 2011, STOCHASTIC MODELLING
NR 55
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405059
DA 2019-06-15
ER

PT S
AU Triantafillou, E
   Zemel, R
   Urtasun, R
AF Triantafillou, Eleni
   Zemel, Richard
   Urtasun, Raquel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Few-Shot Learning Through an Information Retrieval Lens
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular, we view each batch point as a 'query' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.
C1 [Triantafillou, Eleni; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada.
   [Urtasun, Raquel] Univ Toronto, Vector Inst, Uber ATG, Toronto, ON, Canada.
RP Triantafillou, E (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
RI Jeong, Yongwook/N-7413-2016
CR Bellet A., 2013, ARXIV13066709
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Finn C, 2017, ARXIV170303400
   Goldberger J., 2005, ADV NEURAL INFORM PR, P513
   Hazan Tamir, 2010, ADV NEURAL INFORM PR, P1594
   Ioffe S., 2015, ARXIV150203167
   Kaiser L., 2017, ARXIV170303129
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koch G., 2015, THESIS
   Lake B. M., 2011, P 33 ANN C COGN SCI, V172, P2
   Min RQ, 2009, IEEE DATA MINING, P357, DOI 10.1109/ICDM.2009.27
   Mohapatra P., 2014, ADV NEURAL INFORM PR, P2312
   Ravi Sachin, 2017, INT C LEARN REPR, V1, P6
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2007, AISTATS, V11
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shyam Pranav, 2017, ARXIV170300767
   Snell Jake, 2017, ARXIV170305175
   Song H. O., 2016, ARXIV161201213
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song Y., 2016, P 33 INT C MACH LEAR, P2169
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Weinberger K. Q., 2005, ADV NEURAL INFORM PR, P1473
   Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402030
DA 2019-06-15
ER

PT S
AU Tropp, JA
   Yurtsever, A
   Udell, M
   Cevher, V
AF Tropp, Joel A.
   Yurtsever, Alp
   Udell, Madeleine
   Cevher, Volkan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Fixed-Rank Approximation of a Positive-Semidefinite Matrix from
   Streaming Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RANDOMIZED ALGORITHM
AB Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates. Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystrom approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix. Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.
C1 [Tropp, Joel A.] CALTECH, Pasadena, CA 91125 USA.
   [Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Udell, Madeleine] Cornell, Ithaca, NY USA.
RP Tropp, JA (reprint author), CALTECH, Pasadena, CA 91125 USA.
EM jtropp@caltech.edu; alp.yurtsever@epfl.ch; mru8@cornell.edu;
   volkan.cevher@epfl.ch
RI Jeong, Yongwook/N-7413-2016
FU ONR Award [N00014-17-1-2146]; Gordon & Betty Moore Foundation; European
   Commission under Grant ERC Future Proof; SNF [200021-146750,
   CRSII2-147633]; DARPA Award [FA8750-17-2-0101]
FX The authors wish to thank Mark Tygert and Alex Gittens for helpful
   feedback on preliminary versions of this work. JAT gratefully
   acknowledges partial support from ONR Award N00014-17-1-2146 and the
   Gordon & Betty Moore Foundation. VC and AY were supported in part by the
   European Commission under Grant ERC Future Proof, SNF 200021-146750, and
   SNF CRSII2-147633. MU was supported in part by DARPA Award
   FA8750-17-2-0101.
CR Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096
   Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887
   Boutsidis C., 2016, P 48 ACM S THEOR COM
   Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540
   Chiu JW, 2013, SIAM J MATRIX ANAL A, V34, P1361, DOI 10.1137/110852310
   Clarkson K. L., 2009, P 41 ACM S THEOR COM
   Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cohen M. B., 2016, P 43 INT C AUT LANG
   Davis T. A., 2011, ACM T MATH SOFTWARE, V3
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Feldman D., 2016, ADV NEURAL INFORM PR, V29
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718
   Gilbert A. C., 2012, SKETCHED SVD RECOVER
   Gittens A., 2013, THESIS
   Gittens A., 2011, SPECTRAL NORM ERROR
   Gittens A., 2013, REVISITING NYSTROM M
   Gittens A, 2016, J MACH LEARN RES, V17
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Gu M, 2015, SIAM J SCI COMPUT, V37, pA1139, DOI 10.1137/130938700
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Halko N, 2011, SIAM J SCI COMPUT, V33, P2580, DOI 10.1137/100804139
   Higham N. J., 1988, APPL MATRIX THEORY, P1
   Jain P., 2016, 29 ANN C LEARN THEOR, P1147
   Kumar S, 2012, J MACH LEARN RES, V13, P981
   Li H., 2017, ACM T MATH SOFTWARE, V43
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Liberty E., 2009, THESIS
   Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035
   Martinsson PG, 2011, APPL COMPUT HARMON A, V30, P47, DOI 10.1016/j.acha.2010.02.003
   Mitliagkas I., 2013, P ADV NEUR INF PROC, V26, P2886
   Musco C., 2017, SUBLINEAR TIME LOW R
   Platt John C., 2005, P 10 INT WORKSH ART, P261
   Pourkamali- Anaraki F., 2016, RANDOMIZED CLUSTERED
   Tropp J. A., 2017, 201701 ACM CALT
   Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787
   Tygert M., 2014, BETA VERSIONS MATLAB
   Wang S., 2017, SCALABLE KERNEL K ME
   Williams C. K. I., 2000, ADV NEURAL INFORM PR, V13
   Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060
   Woolfe F, 2008, APPL COMPUT HARMON A, V25, P335, DOI 10.1016/j.acha.2007.12.002
   Yang T., 2012, ADV NEURAL INFORM PR, P476
   Yurtsever A., 2017, P 20 INT C ART INT S
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401026
DA 2019-06-15
ER

PT S
AU Nguyen, TD
   Le, T
   Vu, H
   Phung, D
AF Tu Dinh Nguyen
   Trung Le
   Hung Vu
   Dinh Phung
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dual Discriminator Generative Adversarial Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.
C1 [Tu Dinh Nguyen; Trung Le; Hung Vu; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
RP Nguyen, TD (reprint author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM tu.nguyen@deakin.edu.au; trung.1@deakin.edu.au; hungv@deakin.edu.au;
   dinh.phung@deakin.edu.au
RI Jeong, Yongwook/N-7413-2016
FU Australian Research Council (ARC) [DP160109394]
FX This work was partially supported by the Australian Research Council
   (ARC) Discovery Grant Project DP160109394.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Arjovsky M., 2017, ARXIV170107875
   Arora S., 2017, ARXIV170300573
   Berthelot D., 2017, ARXIV170310717
   Che T., 2016, ARXIV161202136
   Coates A, 2011, P 14 INT C ART INT S, P215
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Dumoulin V., 2016, ARXIV160600704
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2017, CORR
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gupta Somesh Das, 2000, MATH STAT
   Hiriart-Urruty J., 2012, FUNDAMENTALS CONVEX
   Hoang Quan, 2017, ARXIV170802556
   Huszar Ferenc, 2015, ARXIV151105101
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Krizhevsky  A., 2009, TECH REP, V1
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Metz L., 2016, ARXIV161102163
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nowozin S, 2016, ADV NEURAL INFORM PR, P271
   Radford  A., 2015, ARXIV151106434
   Reed S., 2016, P 33 INT C MACH LEAR, V3
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Salimans  T., 2016, ADV NEURAL INFORM PR, P2226
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Theis L., 2015, ARXIV151101844
   Wang Ruohan, 2017, ARXIV170403817
   Warde-Farley D, 2017, ICLR SUBMISSIONS, V8
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402070
DA 2019-06-15
ER

PT S
AU Tucker, G
   Mnih, A
   Maddison, CJ
   Lawson, D
   Sohl-Dickstein, J
AF Tucker, George
   Mnih, Andriy
   Maddison, Chris J.
   Lawson, Dieterich
   Sohl-Dickstein, Jascha
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI REBAR: Low-variance, unbiased gradient estimates for discrete latent
   variable models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al., 2016; Maddison et al., 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.
C1 [Tucker, George; Lawson, Dieterich; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA 94040 USA.
   [Mnih, Andriy; Maddison, Chris J.] DeepMind, London, England.
   [Maddison, Chris J.] Univ Oxford, Oxford, England.
RP Tucker, G (reprint author), Google Brain, Mountain View, CA 94040 USA.
EM gjt@google.com; amnih@google.com; cmaddis@stats.ox.ac.uk;
   dieterichl@google.com; jaschasd@google.com
RI Jeong, Yongwook/N-7413-2016
CR AUEB M. T. R., 2015, ADV NEURAL INFORM PR, P2638
   Blei D. M., 2012, P 29 INT C INT C MAC, P1363
   Burda Y., 2015, ARXIV150900519
   Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Gu Shixiang, 2015, ARXIV151105176
   Jang Eric, 2016, ARXIV161101144
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Maddison CJ, 2014, ADV NEUR IN, V27
   Maddison Chris J, 2016, ARXIV161100712
   Mnih A, 2016, INT C MACH LEARN, P2188
   Mnih A., 2014, P 31 INT C MACH LEAR, P1791
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   Owen AB, 2013, MONTE CARLO THEORY M
   Raiko T., 2014, ARXIV14062989
   Ranganath  R., 2014, AISTATS, P814
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Roeder Geoffrey, 2017, ARXIV170309194
   Ruiz F. J. R., 2016, P 32 C UNC ART INT, P647
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wingate D., 2013, ARXIV13011299
   Zaremba Wojciech, 2015, ARXIV150500521, P362
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402066
DA 2019-06-15
ER

PT S
AU Tung, HYF
   Tung, HW
   Yumer, E
   Fragkiadaki, K
AF Tung, Hsiao-Yu Fish
   Tung, Hsiao-Wei
   Yumer, Ersin
   Fragkiadaki, Katerina
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Self-supervised Learning of Motion Capture
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation, optical flow, keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time, manual initialization, or switching to multiple cameras as input resource. In this work, we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly, our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data, and self-supervision from differentiable rendering of (a) skeletal keypoints, (b) dense 3D mesh motion, and (c) human-background segmentation, in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime, ensuring good pose and surface initialization at test time, without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data, and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.
C1 [Tung, Hsiao-Yu Fish; Fragkiadaki, Katerina] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Tung, Hsiao-Wei] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
   [Yumer, Ersin] Adobe Res, San Jose, CA USA.
RP Tung, HYF (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM htung@cs.cmu.edu; hst11@pitt.edu; yumer@adobe.com; katef@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Alldieck T., 2017, CORR
   Andriluka M., 2014, IEEE C COMP VIS PATT
   Balan A.O., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383340
   Ballan L., 2008, 3DPVT
   Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y
   Bogo F., 2016, ECCV 2016
   Brox T., 2006, ECCV
   Cao  Z., 2017, CVPR
   Carreira J., 2015, ARXIV150706550
   Chen C., 2016, CORR
   Chen W., 2016, CORR
   Choo K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P321, DOI 10.1109/ICCV.2001.937643
   Deng J., 2009, CVPR09
   Dosovitskiy A., 2015, ICCV
   Fleet D., 2001, P IEEE C COMP VIS PA
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Handa A, 2016, LECT NOTES COMPUT SC, V9915, P67, DOI 10.1007/978-3-319-49409-8_9
   He K., 2017, CORR
   Ilg E., 2016, CORR
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jaderberg M., 2015, NIPS
   Lin T., 2014, CORR
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Patraucean V., 2015, CORR
   Pavlakos G., 2016, CORR
   Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41
   Rogez G., 2016, NIPS
   Sfmnet F. K., 2017, SFM NET LEARNING STR
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Tome D., 2017, CORR
   Tung H. F., 2017, ICCV
   Urtasun R., 2006, P IEEE COMP SOC C CO
   Varol G., 2017, CVPR
   Vicente S, 2014, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2014.13
   Wei S., 2016, CVPR
   Wu Jiajun, 2016, ECCV
   Yan X., 2016, ADV NEURAL INFORM PR, P1696
   Zhou  T., 2017, UNSUPERVISED LEARNIN
   Zhou X., 2017, CORR
NR 41
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405031
DA 2019-06-15
ER

PT S
AU Uziel, G
   El-Yaniv, R
AF Uziel, Guy
   El-Yaniv, Ran
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-Objective Non-parametric Sequential Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.
C1 [Uziel, Guy; El-Yaniv, Ran] Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.
RP Uziel, G (reprint author), Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.
EM guziel@cs.technion.ac.il; rani@cs.technion.ac.il
RI Jeong, Yongwook/N-7413-2016
FU Israel Science Foundation [1890/14]
FX We would like to thank the anonymous reviewers for providing helpful
   comments. This research was supported by The Israel Science Foundation
   (grant No. 1890/14)
CR ALGOET PH, 1994, IEEE T INFORM THEORY, V40, P609, DOI 10.1109/18.335876
   Biau G, 2011, IEEE T INFORM THEORY, V57, P1664, DOI 10.1109/TIT.2011.2104610
   Biau G, 2010, J NONPARAMETR STAT, V22, P297, DOI 10.1080/10485250802680730
   Borodin A., 2005, ONLINE COMPUTATION C
   BREIMAN L, 1957, ANN MATH STAT, V28, P809, DOI 10.1214/aoms/1177706899
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Gyorfi L, 2008, STATIST RISK MODEL, V26, P145, DOI 10.1524/stnd.2008.0917
   Gyorfi L, 2006, MATH FINANC, V16, P337, DOI 10.1111/j.1467-9965.2006.00274.x
   Gyorfi L., 2003, ADV LEARNING THEORY, V339, P354
   Gyorfi L., 2005, MODELING UNCERTAINTY, P225
   Gyorfi L, 2007, INT J THEOR APPL FIN, V10, P505, DOI 10.1142/S0219024907004251
   Kalnishkan Y, 2005, LECT NOTES COMPUT SC, V3559, P188, DOI 10.1007/11503415_13
   Luenberger D.G., 1997, OPTIMIZATION VECTOR
   Luxburg U. V., 2008, ARXIV08104752
   Mahdavi M., 2013, ADV NEURAL INFORM PR, P1115
   Mannor S, 2009, J MACH LEARN RES, V10, P569
   Rigollet P, 2011, J MACH LEARN RES, V12, P2831
   Stout W., 1974, ALMOST SURE CONVERGE, V24
   Vovk V, 2007, LECT NOTES COMPUT SC, V4539, P439, DOI 10.1007/978-3-540-72927-3_32
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403043
DA 2019-06-15
ER

PT S
AU Van Buskirk, G
   Raichel, B
   Ruozzi, N
AF Van Buskirk, Gregory
   Raichel, Benjamin
   Ruozzi, Nicholas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Sparse Approximate Conic Hulls
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MATRIX; ALGORITHMS
AB We consider the problem of computing a restricted nonnegative matrix factorization (NMF) of an m x n matrix X. Specifically, we seek a factorization X approximate to BC, where the k columns of B are a subset of those from X and C is an element of R(>= 0)(kxn)Equivalently, given the matrix X, consider the problem of finding a small subset, S, of the columns of X such that the conic hull of S epsilon-approximates the conic hull of the columns of X, i.e., the distance of every column of X to the conic hull of the columns of S should be at most an epsilon-fraction of the angular diameter of X. If k is the size of the smallest epsilon-approximation, then we produce an O(k/epsilon(2/3)) sized O(epsilon(1/3))-approximation, yielding the first provable, polynomial time epsilon-approximation for this class of NMF problems, where also desirably the approximation is independent of n and m. Furthermore, we prove an approximate conic Caratheodory theorem, a general sparsity result, that shows that any column of X can be epsilon-approximated with an O(1/epsilon(2)) sparse combination from S. Our results are facilitated by a reduction to the problem of approximating convex hulls, and we prove that both the convex and conic hull variants are d-SUM-hard, resolving an open problem. Finally, we provide experimental results for the convex and conic algorithms on a variety of feature selection tasks.
C1 [Van Buskirk, Gregory; Raichel, Benjamin; Ruozzi, Nicholas] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
RP Van Buskirk, G (reprint author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
EM greg.vanbuskirk@utdallas.edu; benjamin.raichel@utdallas.edu;
   nicholas.ruozzi@utdallas.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CRII [1566137]; DARPA Explainable Artificial Intelligence Program
   [N66001-17-2-4032]; NSF [III-1527312]
FX Greg Van Buskirk and Ben Raichel were partially supported by NSF CRII
   Award-1566137. Nicholas Ruozzi was partially supported by DARPA
   Explainable Artificial Intelligence Program under contract number
   N66001-17-2-4032 and NSF grant III-1527312
CR Arora S, 2016, SIAM J COMPUT, V45, P1582, DOI 10.1137/130913869
   Barman Siddharth, 2015, P 47 ANN ACM S THEOR, P361
   Benson A. R., 2014, P ADV NEUR INF PROC, P945
   Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006
   Bittorf V., 2012, ADV NEURAL INFORM PR, V12, P1214
   Blum A., 2016, P 27 ANN ACM SIAM S, P548
   Civril A, 2012, THEOR COMPUT SCI, V421, P1, DOI 10.1016/j.tcs.2011.11.019
   Clarkson K. L., 2010, CORESETS SPARSE GREE, V6
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Donoho D, 2003, ADV NEURAL INFORM PR
   Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Gillis N, 2014, J MACH LEARN RES, V15, P1249
   Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226
   Greene  D., 2006, P 23 INT C MACH LEAR, P377, DOI DOI 10.1145/1143844.1143892
   Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207
   Kumar A., 2013, INT C MACH LEARN ICM, P231
   Kumar A., 2015, NEAR SEPARABLE NONNE, P343
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li J., 2016, ARXIV160107996
   Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   Patrascu M, 2010, PROC APPL MATH, V135, P1065
   Vavasis SA, 2009, SIAM J OPTIMIZ, V20, P1364, DOI 10.1137/070709967
   Zhou T., 2014, NIPS, P1242
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402057
DA 2019-06-15
ER

PT S
AU van den Oord, A
   Vinyals, O
   Kavukcuoglu, K
AF van den Oord, Aaron
   Vinyals, Oriol
   Kavukcuoglu, Koray
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Neural Discrete Representation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.
C1 [van den Oord, Aaron; Vinyals, Oriol; Kavukcuoglu, Koray] DeepMind, London, England.
RP van den Oord, A (reprint author), DeepMind, London, England.
EM avdnoord@google.com; vinyals@google.com; korayk@google.com
RI Jeong, Yongwook/N-7413-2016
CR Agustsson E., 2017, ARXIV170400648
   Beattie C., 2016, ARXIV161203801
   Bengio Y., 2013, ARXIV13083432
   Bowman S. R., 2015, ARXIV151106349
   Burda Y., 2015, ARXIV150900519
   Chen X., 2016, ARXIV161102731
   Chen X., 2016, ARXIV160603657
   Courville A., 2016, CORR
   Courville A., 2011, INT C ART INT STAT, V15, P233
   Denton E, 2016, ARXIV161106430
   Dieleman S., 2016, CORR
   Dinh L., 2016, ARXIV160508803
   Finn C., 2016, ADV NEURAL INFORM PR, P64
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Gregor K., 2013, ARXIV13108499
   Gregor K., 2016, ADV NEURAL INFORM PR, P3549
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoffman Judy, 2013, JUDY
   Isola  P., 2016, ARXIV161107004
   Jang Eric, 2016, ARXIV161101144
   Kalchbrenner N., 2016, ARXIV161000527
   Kingma D.P., 2013, ARXIV13126114
   Kingma D. P., 2014, ARXIV14126980
   Kingma Diederik P, 2016, IMPROVED VARIATIONAL
   Ledig C, 2016, ARXIV160904802
   Maddison Chris J, 2016, ARXIV161100712
   Mehri S., 2016, ARXIV161207837
   Mnih Andriy, 2016, CORR
   Mnih Andriy, 2014, ARXIV14020030
   Oord  A.v.d., 2016, ARXIV160106759
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Rezende D. J, 2014, ARXIV14014082
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Santoro A., 2016, ARXIV160506065
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Theis L., 2017, ARXIV170300395
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Yang Z., 2017, CORR
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406037
DA 2019-06-15
ER

PT S
AU van der Pas, S
   Roekova, V
AF van der Pas, Stephanie
   Roekova, Veronika
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bayesian Dyadic Trees and Histograms for Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE-RATES; CART
AB Many machine learning tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we shed light on the machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results do not require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to an old problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.
C1 [van der Pas, Stephanie] Leiden Univ, Math Inst, Leiden, Netherlands.
   [Roekova, Veronika] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.
RP van der Pas, S (reprint author), Leiden Univ, Math Inst, Leiden, Netherlands.
EM svdpas@math.leidenuniv.nl; Veronika.Rockova@ChicagoBooth.edu
RI Jeong, Yongwook/N-7413-2016
FU James S. Kemper Foundation Faculty Research Fund at the University of
   Chicago Booth School of Business
FX This work was supported by the James S. Kemper Foundation Faculty
   Research Fund at the University of Chicago Booth School of Business.
CR Abu-Nimeh Saeed, 2007, P ANT WORK GROUPS 2, P60, DOI DOI 10.1145/1299015.1299021
   Anderson T., 1966, MULTIVARIATE ANAL, P5
   Berchuck A, 2005, CLIN CANCER RES, V11, P3686, DOI 10.1158/1078-0432.CCR-04-2398
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, STAT PROBABILITY SER
   BRIOL F. -X., 2015, ADV NEURAL INFORM PR, P1162
   Castillo I., 2016, PREPRINT
   Castillo I, 2014, ANN STAT, V42, P1941, DOI 10.1214/14-AOS1246
   Chen MJ, 2016, BAYESIAN ANAL, V11, P477, DOI 10.1214/15-BA958
   Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832
   Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285
   Coram M, 2006, ANN STAT, V34, P1233, DOI 10.1214/009053606000000236
   Denison DGT, 1998, BIOMETRIKA, V85, P363
   Donoho DL, 1997, ANN STAT, V25, P1870, DOI 10.1214/aos/1069362377
   Feller W., 1968, INTRO PROBABILITY TH, VII
   FLATTO L, 1962, SIAM REV, V4, P211, DOI 10.1137/1004058
   Gao C., 2017, MINIMAX RISK B UNPUN, P1
   Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228
   Ghosal S, 2007, ANN STAT, V35, P192, DOI 10.1214/009053606000001172
   Green DP, 2012, PUBLIC OPIN QUART, V76, P491, DOI 10.1093/poq/nfs036
   Korda N, 2013, ADV NEURAL INFORM PR, P1448
   Liu L., 2015, ARXIV150804812V1
   Nobel A, 1996, ANN STAT, V24, P1084
   Polly E. C., 2010, SUPER LEARNER PREDIC
   Razi MA, 2005, EXPERT SYST APPL, V29, P65, DOI 10.1016/j.eswa.2005.01.006
   Rockova V., 2017, ARXIV170808734
   Rousseau J., 2016, ARXIV E PRINTS
   Roy Daniel M., 2009, ADV NEURAL INF PROCE, P1377
   Scricciolo C, 2007, SCAND J STAT, V34, P626, DOI 10.1111/j.1467-9469.2006.00540.x
   SHEPP LA, 1972, ISRAEL J MATH, V11, P328, DOI 10.1007/BF02789327
   Tang J., 2014, P 31 INT C MACH LEAR, V32, P190
   Zhang T, 2004, ADV NEUR IN, V16, P1149
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402014
DA 2019-06-15
ER

PT S
AU van Seijen, H
   Fatemi, M
   Romoff, J
   Laroche, R
   Barnes, T
   Tsang, J
AF van Seijen, Harm
   Fatemi, Mehdi
   Romoff, Joshua
   Laroche, Romain
   Barnes, Tavian
   Tsang, Jeffrey
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hybrid Reward Architecture for Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.
C1 [van Seijen, Harm; Fatemi, Mehdi; Romoff, Joshua; Laroche, Romain; Barnes, Tavian; Tsang, Jeffrey] Microsoft Maluuba, Montreal, PQ, Canada.
   [Romoff, Joshua] McGill Univ, Montreal, PQ, Canada.
RP van Seijen, H (reprint author), Microsoft Maluuba, Montreal, PQ, Canada.
EM harm.vanseijen@microsoft.com; mehdi.fatemi@microsoft.com;
   joshua.romoff@mail.mcgill.ca; romain.laroche@microsoft.com;
   tavian.barnes@microsoft.com; tsang.jeffrey@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bacon P., 2017, P 31 AAAI C ART INT
   Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Diuk C., 2008, P 25 INT C MACH LEAR
   Fuster J.M., 2003, CORTEX MIND UNIFYING
   Gluck M. A., 2013, LEARNING MEMORY BRAI
   Jaderberg Max, 2017, INT C LEARN REPR
   Kulkarni T. D., 2016, ADV NEURAL INFORM PR, V29
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nair A., 2015, DEEP LEARN WORKSH IC
   Ng A. Y., 1999, P 16 INT C MACHINE L
   Roijers D. M., 2013, J ARTIFICIAL INTELLI
   Russell S., 2003, P 20 INT C MACH LEAR
   Schaul T., 2015, P 32 INT C MACH LEAR
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Sprague N., 2003, INT JOINT C ART INT
   Stout A., 2005, AAAI SPRING S DEV RO
   Sutton R. S., 2011, P 10 INT C AUT AG MU
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Szepesvari C., 2009, ALGORITHMS REINFORCE
   van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177
   Vezhnevets A., 2016, ADV NEURAL INFORM PR, V29
   Wang Z, 2016, PR INT CONGR SOUND V
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405046
DA 2019-06-15
ER

PT S
AU Vandat, A
AF Vandat, Arash
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Toward Robustness against Label Noise in Training Deep Discriminative
   Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Collecting large training datasets, annotated with high-quality labels, is costly and time-consuming. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels, trained in a semi-supervised setting. In our formulation, the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.
C1 [Vandat, Arash] D Wave Syst Inc, Burnaby, BC, Canada.
RP Vandat, A (reprint author), D Wave Syst Inc, Burnaby, BC, Canada.
EM avandat@dwavesys.com
RI Jeong, Yongwook/N-7413-2016
CR Chen L.-C., 2015, ICML, P1785
   Deng J, 2009, COMPUTER VISION PATT
   Deng Z., 2016, CVPR
   Do T. M. T., 2010, INT C ART INT STAT, P177
   Fang Hao, 2015, C COMP VIS PATT REC
   Fergus R., 2009, ADV NEURAL INFORM PR, V22, P522
   Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Ganchev Kuzman, 2010, J MACHINE LEARNING R
   He K, 2016, COMPUTER VISION PATT
   Kingma D. P., 2014, ARXIV14126980
   Kirillov  A., 2015, ARXIV151105067
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, V24, P109
   Lafferty J. D., 2001, INT C MACH LEARN ICM
   Lin Guosheng, 2016, COMPUTER VISION PATT
   Maaten L., 2011, INT C ART INT STAT A, V15, P479
   Misra  Ishan, 2016, CVPR
   Mnih V., 2012, P 29 INT C MACH LEAR, P567
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Neal R. M., 1998, LEARNING GRAPHICAL M
   Patrini G., 2017, COMPUTER VISION PATT
   Peng J., 2009, ADV NEURAL INFORM PR, P1419
   Prabhavalkar R, 2010, INT CONF ACOUST SPEE, P5534, DOI 10.1109/ICASSP.2010.5495222
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Reed S., 2014, ARXIV14126596
   Rohrbach Marcus, 2010, COMPUTER VISION PATT
   Ross Stephane, 2011, COMPUTER VISION PATT
   Schwing A. G., 2015, ARXIV150302351
   Shang C, 2015, IEEE IC COMP COM NET
   Simonyan K, 2014, ARXIV14091556
   Sukhbaatar S., 2014, ARXIV14062080
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Veit Andreas, 2017, ARXIV170101619
   Xiao Tong, 2015, COMPUTER VISION PATT
   Younes Laurent, 1989, PROBABILITY THEORY R
   Zheng S, 2015, INT C COMP VIS ICCV
   Zhu  X., 2003, ICML
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405066
DA 2019-06-15
ER

PT S
AU Varatharajah, Y
   Chong, MJ
   Saboo, K
   Berry, B
   Brinkmann, B
   Worrell, G
   Iyer, R
AF Varatharajah, Yogatheesan
   Chong, Min Jin
   Saboo, Krishnakant
   Berry, Brent
   Brinkmann, Benjamin
   Worrell, Gregory
   Iyer, Ravishankar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal,
   and Observational Relationships in Electroencephalograms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper presents a probabilistic-graphical model that can be used to infer characteristics of instantaneous brain activity by jointly analyzing spatial and temporal dependencies observed in electroencephalograms (EEG). Specifically, we describe a factor-graph-based model with customized factor-functions defined based on domain knowledge, to infer pathologic brain activity with the goal of identifying seizure-generating brain regions in epilepsy patients. We utilize an inference technique based on the graph-cut algorithm to exactly solve graph inference in polynomial time. We validate the model by using clinically collected intracranial EEG data from 29 epilepsy patients to show that the model correctly identifies seizure-generating brain regions. Our results indicate that our model outperforms two conventional approaches used for seizure-onset localization (5-7% better AUC: 0.72, 0.67, 0.65) and that the proposed inference technique provides 3-10% gain in AUC (0.72, 0.62, 0.69) compared to sampling-based alternatives.
C1 [Varatharajah, Yogatheesan; Chong, Min Jin; Saboo, Krishnakant; Iyer, Ravishankar] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Berry, Brent; Brinkmann, Benjamin; Worrell, Gregory] Mayo Clin, Dept Neurol, Rochester, MN 55904 USA.
RP Varatharajah, Y (reprint author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM varatha2@illinois.edu; mchong6@illinois.edu; ksaboo2@illinois.edu;
   Berry.Brent@mayo.edu; Brinkmann.Benjamin@mayo.edu;
   Worrell.Gregory@mayo.edu; rkiyer@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [CNS-1337732, CNS-1624790]; National
   Institute of Health [NINDS-U01-NS073557, NINDS-R01-NS92882,
   NHLBI-HL105355, NINDS-UH2-NS095495-01]; Mayo Clinic and Illinois
   Alliance Fellowships for Technology-based Healthcare Research; IBM
   faculty award
FX This work was partly supported by National Science Foundation grants
   CNS-1337732 and CNS-1624790, National Institute of Health grants
   NINDS-U01-NS073557, NINDS-R01-NS92882, NHLBI-HL105355, and
   NINDS-UH2-NS095495-01, Mayo Clinic and Illinois Alliance Fellowships for
   Technology-based Healthcare Research and an IBM faculty award. We thank
   Subho Banerjee, Phuong Cao, Jenny Applequist, and the reviewers for
   their valuable feedback.
CR Alvarado-Rojas C, 2014, SCI REP-UK, V4, DOI 10.1038/srep04545
   Andersen L. R., 1991, J APPL STAT, V18, P139
   Andrzejak RG, 2009, CLIN NEUROPHYSIOL, V120, P1465, DOI 10.1016/j.clinph.2009.05.019
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Cao P., 2015, P 2015 S BOOTC SCI S
   CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568
   DAGUM P, 1992, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P41
   Delong A, 2009, IEEE I CONF COMP VIS, P285, DOI 10.1109/ICCV.2009.5459263
   Esquenazi Y, 2014, EPILEPSY RES, V108, P547, DOI 10.1016/j.eplepsyres.2014.01.009
   Frey B, 1997, P 35 ALL C COMM CONT, P666
   Gilks WR, 1995, MARKOV CHAIN MONTE C
   Katznelson R., 1981, ELECTRIC FIELDS BRAI, P176
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lee RW, 2014, J CLIN NEUROPHYSIOL, V31, P199, DOI 10.1097/WNP.0000000000000047
   Liu Jie, 2012, JMLR Workshop Conf Proc, V22, P712
   Liu S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/2/026026
   Martinez-Vargas JD, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00156
   Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003
   Stead M, 2010, BRAIN, V133, P2789, DOI 10.1093/brain/awq190
   Varatharajah Y, 2017, I IEEE EMBS C NEUR E, P533, DOI 10.1109/NER.2017.8008407
   Varatharajah Y, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065716500465
   Warren CP, 2010, J NEUROPHYSIOL, V104, P3530, DOI 10.1152/jn.00368.2010
   Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585
   Wetjen NM, 2009, J NEUROSURG, V110, P1147, DOI 10.3171/2008.8.JNS17643
   Wiegerinck W., 2000, P 16 C UNC ART INT, P626
   Worrell GA, 2008, BRAIN, V131, P928, DOI 10.1093/brain/awn006
   Yedidia JS, 2000, NIPS, V13, P689
   Zhang Y, 2010, PROC INT CONF DATA, P1157, DOI 10.1109/ICDE.2010.5447819
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405044
DA 2019-06-15
ER

PT S
AU Varma, P
   He, B
   Bajaj, P
   Khandwala, N
   Banerjee, I
   Rubin, D
   Re, C
AF Varma, Paroma
   He, Bryan
   Bajaj, Payal
   Khandwala, Nishith
   Banerjee, Imon
   Rubin, Daniel
   Re, Christopher
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Inferring Generative Model Structure with Static Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SEGMENTATION; SELECTION
AB Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects the quality of the training labels, but is difficult to learn without any ground truth labels. We instead rely on weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus significantly reducing the amount of data required to learn structure. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations identified, improving over the standard sample complexity, which is exponential in n for learning nth degree relations. Empirically, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.
C1 [Varma, Paroma] Stanford Univ, Elect Engn, Stanford, CA 94305 USA.
   [He, Bryan; Bajaj, Payal; Khandwala, Nishith; Re, Christopher] Stanford Univ, Comp Sci, Stanford, CA 94305 USA.
   [Banerjee, Imon; Rubin, Daniel] Stanford Univ, Biomed Data Sci, Stanford, CA 94305 USA.
   [Rubin, Daniel] Stanford Univ, Radiol, Stanford, CA 94305 USA.
RP Varma, P (reprint author), Stanford Univ, Elect Engn, Stanford, CA 94305 USA.
EM paroma@stanford.edu; bryanhe@stanford.edu; pabajaj@stanford.edu;
   nishith@stanford.edu; imonb@stanford.edu; rubin@stanford.edu;
   chrismre@cs.stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Defense Advanced Research Projects Agency (DARPA) [FA8750-17-2-0095];
   DARPA SIMPLEX program [N66001-15-C-4043]; DARPA [FA8750-12-2-0335,
   FA8750-13-2-0039]; National Science Foundation (NSF) Graduate Research
   Fellowship [DGE-114747]; Joseph W. and Hon Mai Goodman Stanford Graduate
   Fellowship; National Institute of Health (NIH) [U54EB020405]; Office of
   Naval Research (ONR) [N000141210041, N000141310129]; Moore Foundation;
   Okawa Research Grant; American Family Insurance; Accenture; Toshiba;
   Intel; Microsoft; Teradata; VMware; DOE [108845]
FX We thank Shoumik Palkar, Stephen Bach, and Sen Wu for their helpful
   conversations and feedback. We are grateful to Darvin Yi for his
   assistance with the DDSM dataset based experiments and associated deep
   learning models. We acknowledge the use of the bone tumor dataset
   annotated by Drs. Christopher Beaulieu and Bao Do and carefully
   collected over his career by the late Henry H. Jones, M.D. (aka "Bones
   Jones"). This material is based on research sponsored by Defense
   Advanced Research Projects Agency (DARPA) under agreement number
   FA8750-17-2-0095. We gratefully acknowledge the support of the DARPA
   SIMPLEX program under No. N66001-15-C-4043, DARPA FA8750-12-2-0335 and
   FA8750-13-2-0039, DOE 108845, the National Science Foundation (NSF)
   Graduate Research Fellowship under No. DGE-114747, Joseph W. and Hon Mai
   Goodman Stanford Graduate Fellowship, National Institute of Health (NIH)
   U54EB020405, the Office of Naval Research (ONR) under awards No.
   N000141210041 and No. N000141310129, the Moore Foundation, the Okawa
   Research Grant, American Family Insurance, Accenture, Toshiba, and
   Intel. This research was supported in part by affiliate members and
   other supporters of the Stanford DAWN project: Intel, Microsoft,
   Teradata, and VMware. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright notation thereon. The views and conclusions contained herein
   are those of the authors and should not be interpreted as necessarily
   representing the official policies or endorsements, either expressed or
   implied, of DARPA or the U.S. Government. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of DARPA, AFRL,
   NSF, NIH, ONR, or the U.S. government.
CR Alfonseca E, 2012, ACL, P54
   Bach S. H., 2017, ICML
   Balsubramani A., 2015, ADV NEURAL INFORM PR, P1351
   Banerjee I., 2016, ARXIV161200408
   Blaschko M.B., 2010, ADV NEURAL INFORM PR, P235
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Branson S, 2011, IEEE I CONF COMP VIS, P1832, DOI 10.1109/ICCV.2011.6126450
   Bunescu R. C., 2007, ACL
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen X, 2016, J MACH LEARN RES, V17
   Craven M, 1999, Proc Int Conf Intell Syst Mol Biol, P77
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dalvi N. N., 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hoffmann Raphael, 2011, P 49 ANN M ASS COMP, V1, P541
   Joglekar Manas, 2015, 2015 IEEE 31st International Conference on Data Engineering (ICDE), P195, DOI 10.1109/ICDE.2015.7113284
   Kang D., 2017, ABS170302529 CORR
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kaus MR, 2001, RADIOLOGY, V218, P586, DOI 10.1148/radiology.218.2.r01fe44586
   Krishna R., 2016, ARXIV160207332
   Kurtz C, 2014, MED IMAGE ANAL, V18, P1082, DOI 10.1016/j.media.2014.06.009
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Mintz M., 2009, P JOINT C 47 ANN M A, P1003, DOI DOI 10.3115/1690219.1690287
   Oliver A, 2010, MED IMAGE ANAL, V14, P87, DOI 10.1016/j.media.2009.12.005
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Ratner Alexander, 2016, Adv Neural Inf Process Syst, V29, P3567
   Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10
   Roth B., 2013, EMNLP, P24
   Sawyer-Lee R., 2016, CURATED BREAST IMAGI
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Sharma N, 2010, J MED PHYS, V35, P3, DOI 10.4103/0971-6203.58777
   Shin J, 2015, PROC VLDB ENDOW, V8, P1310, DOI 10.14778/2809974.2809991
   Takamatsu  S., 2012, P 50 ANN M ASS COMP, P721
   Varma P., 2017, ARXIV161008123
   Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271
   Yi D., 2016, ARXI161104534
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400023
DA 2019-06-15
ER

PT S
AU Vartak, M
   Thiagarajan, A
   Miranda, C
   Bratman, J
   Larochelle, H
AF Vartak, Manasi
   Thiagarajan, Arvind
   Miranda, Conrado
   Bratman, Jeshua
   Larochelle, Hugo
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Meta-Learning Perspective on Cold-Start Recommendations for Items
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a meta-learning strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adjusted. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline and also outperform production models for Tweet recommendation.
C1 [Vartak, Manasi] MIT, Cambridge, MA 02139 USA.
   [Thiagarajan, Arvind; Miranda, Conrado; Bratman, Jeshua] Twitter Inc, San Francisco, CA USA.
   [Larochelle, Hugo] Google Brain, Mountain View, CA USA.
RP Vartak, M (reprint author), MIT, Cambridge, MA 02139 USA.
EM mvartak@csail.mit.edu; arvindt@twitter.com; cmiranda@twitter.com;
   jbratman@twitter.com; hugolarochelle@google.com
RI Jeong, Yongwook/N-7413-2016
CR Agarwal D, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P19
   Bachman P., 2017, P MACHINE LEARNING R, V70, P301
   Charlin L., 2014, P OF KDD 14, P173
   Covington P, 2016, P 10 ACM C REC SYST, P191, DOI DOI 10.1145/2959100.2959190
   DAS Abhinandan, 2007, P 16 INT C WORLD WID, V16, P271, DOI DOI 10.1145/1242572.1242610
   He X., 2014, P 8 INT WORKSH DAT M, DOI DOI 10.1145/2648584.2648589
   Hidasi B, 2016, P 10 ACM C REC SYST, P241, DOI DOI 10.1145/2959100.2959167
   Hidasi B., 2015, ARXIV151106939
   Hong L., 2013, P 6 ACM INT C WEB SE, P557, DOI DOI 10.1145/2433396.2433467
   Kim D., 2016, P 10 ACM C REC SYST, P233, DOI DOI 10.1145/2959100.2959165
   Koch G., 2015, THESIS
   Koren Y., 2008, P 14 ACM SIGKDD INT, V08, P426, DOI [DOI 10.1145/1401890.1401944, 10.1145/1401890.1401944]
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lam X. N., 2008, P 2 INT C UB INF MAN, P208, DOI DOI 10.1145/1352793.1352837
   Lemke C, 2015, ARTIF INTELL REV, V44, P117, DOI 10.1007/s10462-013-9406-y
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Liu J, 2010, IUI 2010, P31
   Lops P, 2011, RECOMMENDER SYSTEMS HANDBOOK, P73, DOI 10.1007/978-0-387-85820-3_3
   Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35
   Ravi S., 2017, ICLR
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, P1257, DOI DOI 10.1145/1390156.1390267
   Santoro A., 2016, INT C MACH LEARN, V48, P1842
   Snell J., 2017, ABS170305175 CORR
   Stern D. H., 2009, P 18 INT C WORLD WID, P111
   Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069
   Vinyals O., 2016, ADV NEURAL INFORM PR, V30, P3630
   Wang C., 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480
   Zaheer  M., 2017, ABS170306114 CORR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406093
DA 2019-06-15
ER

PT S
AU Vellanki, P
   Rana, S
   Gupta, S
   Rubin, D
   Sutti, A
   Dorin, T
   Height, M
   Sandars, P
   Venkatesh, S
AF Vellanki, Pratibha
   Rana, Santu
   Gupta, Sunil
   Rubin, David
   Sutti, Alessandra
   Dorin, Thomas
   Height, Murray
   Sandars, Paul
   Venkatesh, Svetha
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Process-constrained batch Bayesian Optimisation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PRECIPITATION
AB Prevailing batch Bayesian optimisation methods allow all control variables to be freely altered at each iteration. Real-world experiments, however, often have physical limitations making it time-consuming to alter all settings for each recommendation in a batch. This gives rise to a unique problem in BO: in a recommended batch, a set of variables that are expensive to experimentally change need to be fixed, while the remaining control variables can be varied. We formulate this as a process-constrained batch Bayesian optimisation problem. We propose two algorithms, pc-BO(basic) and pc-BO(nested). pc-BO(basic) is simpler but lacks convergence guarantee. In contrast pc-BO(nested) is slightly more complex, but admits convergence analysis. We show that the regret of pc-BO(nested) is sublinear. We demonstrate the performance of both pc-BO(basic) and pc-BO(nested) by optimising benchmark test functions, tuning hyper-parameters of the SVM classifier, optimising the heat-treatment process for an Al-Sc alloy to achieve target hardness, and optimising the short polymer fibre production process.
C1 [Vellanki, Pratibha; Rana, Santu; Gupta, Sunil; Venkatesh, Svetha] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
   [Rubin, David; Sutti, Alessandra; Dorin, Thomas; Height, Murray] Deakin Univ, Inst Frontier Mat, GTP Res, Geelong, Vic, Australia.
   [Sandars, Paul] Michigan Technol Univ, Mat Sci & Engn, Houghton, MI 49931 USA.
RP Vellanki, P (reprint author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.
EM pratibha.vellanki@deakin.edu.au; santu.rana@deakin.edu.au;
   sunil.gupta@deakin.edu.au; d.rubindecelisleal@deakin.edu.au;
   alessandra.sutti@deakin.edu.au; thomas.dorin@deakin.edu.au;
   murray.height@deakin.edu.au; sanders@mtu.edu;
   svetha.venkatesh@deakin.edu.au
RI Rana, Santu/R-2992-2019; Jeong, Yongwook/N-7413-2016
OI Rana, Santu/0000-0003-2247-850X; 
FU Australian Government through the Australian Research Council (ARC);
   Telstra-Deakin Centre of Excellence in Big Data and Machine Learning;
   ARC Australian Laureate Fellowship [FL170100006]
FX This research was partially funded by the Australian Government through
   the Australian Research Council (ARC) and the Telstra-Deakin Centre of
   Excellence in Big Data and Machine Learning. Prof Venkatesh is the
   recipient of an ARC Australian Laureate Fellowship (FL170100006).
CR Azimi J., 2010, ADV NEURAL INFORM PR, P109
   Azimi J, 2016, J ARTIF INTELL RES, V56, P119, DOI 10.1613/jair.4896
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Brochu E., 2010, ARXIV10122599
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Gardner J.R., 2014, ICML, P937
   Gelbart M. A., 2014, P 30 C UNC ART INT, P250
   Ginsbourger  D., 2008, TECHNICAL REPORT
   Gonzalez J., 2015, ARTIF INTELL, P648
   Hernandez-Lobato D, 2016, J MACH LEARN RES, V17
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robson JD, 2003, ACTA MATER, V51, P1453, DOI 10.1016/S1359-6454(02)00540-2
   Sacks J., 1989, STAT SCI, V4, P409, DOI DOI 10.1214/SS/1177012413
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2012, ADV NEURAL INFORM PR, P2960
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Sutti A, 2011, J NANOSCI NANOTECHNO, V11, P8947, DOI 10.1166/jnn.2011.3489
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Thornton C., 2013, P 19 ACM SIGKDD INT, P847, DOI [10.1145/2487575.2487629, DOI 10.1145/2487575.2487629]
   Wagner R., 1991, HOMOGENEOUS 2 PHASE
   Wang Z, 2013, P 23 INT JOINT C ART, P1778
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403047
DA 2019-06-15
ER

PT S
AU Venkatraman, A
   Rhinehart, N
   Sun, W
   Pinto, L
   Hebert, M
   Boots, B
   Kitani, KM
   Bagnell, JA
AF Venkatraman, Arun
   Rhinehart, Nicholas
   Sun, Wen
   Pinto, Lerrel
   Hebert, Martial
   Boots, Byron
   Kitani, Kris M.
   Bagnell, J. Andrew
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Predictive-State Decoders: Encoding the Future into Recurrent Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with PREDICTIVE-STATE DECODERS (PSDs), which add supervision to the network's internal state representation to target predicting future observations. PSDs are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.
C1 [Venkatraman, Arun; Rhinehart, Nicholas; Sun, Wen; Pinto, Lerrel; Hebert, Martial; Kitani, Kris M.; Bagnell, J. Andrew] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
   [Boots, Byron] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
RP Venkatraman, A; Rhinehart, N (reprint author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
EM arunvenk@cs.cmu.edu; nrhineha@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016; Rhinehart, Nicholas/M-1311-2019
OI Rhinehart, Nicholas/0000-0003-4242-1236
FU Office of Naval Research (ONR) [N000141512365]; National Science
   Foundation NRI [1637758]
FX This material is based upon work supported in part by: Office of Naval
   Research (ONR) contract N000141512365, and National Science Foundation
   NRI award number 1637758.
CR Abadi M., 2016, ARXIV160304467
   Abbeel P., 2005, ROBOTICS SCI SYSTEMS
   Abbeel P., 2005, P 22 INT C MACH LEAR, P1
   Abbeel Pieter, 2005, ADV NEURAL INFORM PR, P1
   Agrawal Pulkit, 2016, ADV NEURAL INFORM PR, P5074
   Astrom KJ., 2010, FEEDBACK SYSTEMS INT
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Boots B., 2012, THESIS
   Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092
   Boots Byron, 2013, UAI 2013
   Bowden RJ, 1990, INSTRUMENTAL VARIABL, V8
   Brockman G, 2016, ARXIV160601540
   Caruana R, 1998, LEARNING TO LEARN, P95
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung  J., 2015, ADV NEURAL INFORM PR, V2015, P2980
   Coates A, 2008, P 25 INT C MACH LEAR, P144, DOI DOI 10.1145/1390156.1390175
   Deisenroth Marc P., 2009, P 26 ANN INT C MACH, P225
   Duan  Y., 2016, P 33 INT C MACH LEAR
   Ghahramani Zoubin, 1999, LEARNING NONLINEAR D, P431
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Greff K., 2016, IEEE T NEURAL NETWOR
   Haarnoja T., 2016, NIPS
   Hausknecht M.J., 2015, ARXIV150706527
   Hefny Ahmed, 2015, NIPS
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hsu Daniel J., 2009, COLT
   Jaderberg M., 2016, CORR
   Kakade S, 2002, ADV NEUR IN, V14, P1531
   Ko J, 2007, GP UKF UNSCENTED KAL, P1901
   Kokkinos Iasonas, 2016, CORR
   Langford J, 2009, P 26 ANN INT C MACH, P593
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levine S, 2016, J MACH LEARN RES, V17
   Ondruska Peter, 2016, 30 AAAI C ART INT
   Oord  A.v.d., 2016, ARXIV160106759
   Pascanu R., 2013, J MACHINE LEARNING R, P1310
   Pinto L., 2017, ARXIV170302702
   Ralaivola Liva, 2004, NIPS
   Ranzato Marc Aurelio, 2016, ICLR
   Ross S., 2011, CVPR
   Ross Stephane, 2011, AISTATS
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Singh Satinder, 2004, UAI
   Song L., 2010, P 27 INT C MACH LEAR, P991
   Sun W, 2016, PR INT CONGR SOUND V
   Sun Wen, 2017, ICML
   Sun Wen, 2016, P 33 INT C MACH LEAR, P1197
   Sutskever I, 2013, THESIS
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Van Overschee P, 2012, SUBSPACE IDENTIFICAT
   Vega-Brown W, 2013, IEEE INT C INT ROBOT, P1907, DOI 10.1109/IROS.2013.6696609
   Venkatraman Arun, 2016, 25 INT JOINT C ART I
   Venkatrarnan A., 2015, AAAI, P3024
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wingate D., 2006, P 23 INT C MACH LEAR, P1017
NR 56
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401021
DA 2019-06-15
ER

PT S
AU Verma, S
   Zhang, ZL
AF Verma, Saurabh
   Zhang, Zhi-Li
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB For the purpose of learning on graphs, we hunt for a graph feature representation that exhibit certain uniqueness, stability and sparsity properties while also being amenable to fast computation. This leads to the discovery of family of graph spectral distances (denoted as FGSD) and their based graph feature representations, which we prove to possess most of these desired properties. To both evaluate the quality of graph features produced by FGSD and demonstrate their utility, we apply them to the graph classification problem. Through extensive experiments, we show that a simple SVM based classification algorithm, driven with our powerful FGSD based graph features, significantly outperforms all the more sophisticated state-of-art algorithms on the unlabeled node datasets in terms of both accuracy and speed; it also yields very competitive results on the labeled datasets - despite the fact it does not utilize any node label information.
C1 [Verma, Saurabh; Zhang, Zhi-Li] Univ Minnesota Twin Cities, Dept Comp Sci, Minneapolis, MN 55455 USA.
RP Verma, S (reprint author), Univ Minnesota Twin Cities, Dept Comp Sci, Minneapolis, MN 55455 USA.
EM verma@cs.umn.edu; zhang@cs.umn.edu
RI Jeong, Yongwook/N-7413-2016
FU ARO MURI Award [W911NF-12-1-0385]; DTRA [HDTRA1-14-1-0040]; NSF
   [CNS-1618339, CNS-1617729]
FX This research was supported in part by ARO MURI Award W911NF-12-1-0385,
   DTRA grant HDTRA1-14-1-0040, and NSF grants CNS-1618339, CNS-1618339 and
   CNS-1617729.
CR Atwood J., 2016, ADV NEURAL INFORM PR, V29, P1993
   Babai L., 2015, CORR
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Borgwardt Karsten M., 2005, DAT MIN 5 IEEE INT C
   Botsch M., 2005, Mathematics of Surfaces XI 11th IMA International Conference. Proceedings (Lecture Notes in Computer Science Vol. 3604), P62
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Bruna J., 2013, ABS13126203 CORR
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Costa F., 2010, P 26 INT C MACH LEAR, P255
   Dai H., 2016, P 33 INT C INT C MAC, V48
   DEFFERRARD M., 2016, ADV NEURAL INFORM PR, P3837
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Grover Aditya, 2016, P 22 ACM SIGKDD INT
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Henaff Mikael, 2015, ARXIV150605163
   Katsikis VN, 2011, APPL MATH COMPUT, V217, P9828, DOI 10.1016/j.amc.2011.04.080
   Kipf T. N., 2016, ARXIV160902907
   Kondor R., 2016, ADV NEURAL INFORM PR, P2982
   Kondor R., 2007, CORR
   Kondor R., 2009, P 26 ANN INT C MACH, P529
   Kondor R., 2008, P 25 INT C MACH LEAR, P496
   Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971
   Montavon G., 2012, ADV NEURAL INFORM PR, V25, P440
   Nadler B., 2005, P NIPS, P955
   Niepert Mathias, 2016, P 33 ANN INT C MACH
   Orsini F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3756
   ROSENBLATT J, 1982, SIAM J ALGEBRA DISCR, V3, P343, DOI 10.1137/0603035
   Shapiro LW, 1987, MATH MAG, V60, P36
   Shervashidze N., 2009, P 12 INT C ART INT S, V2009, P488
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   WHITTAKER E. T., 1996, COURSE MODERN ANAL
   Yanardag P., 2015, P 21 ACM SIGKDD INT, P1365, DOI DOI 10.1145/2783258.2783417
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400009
DA 2019-06-15
ER

PT S
AU Volkovs, M
   Yu, GW
   Poutanen, T
AF Volkovs, Maksims
   Yu, Guangwei
   Poutanen, Tomi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI DropoutNet: Addressing Cold Start in Recommender Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Latent models have become the default choice for recommender systems due to their performance and scalability. However, research in this area has primarily focused on modeling user-item interactions, and few latent models have been developed for cold start. Deep learning has recently achieved remarkable success showing excellent results for diverse input types. Inspired by these results we propose a neural network based latent model called DropoutNet to address the cold start problem in recommender systems. Unlike existing approaches that incorporate additional content-based objective terms, we instead focus on the optimization and show that neural network models can be explicitly trained for cold start through dropout. Our model can be applied on top of any existing latent model effectively providing cold start capabilities, and full power of deep architectures. Empirically we demonstrate state-of-the-art accuracy on publicly available benchmarks.
C1 [Volkovs, Maksims; Yu, Guangwei; Poutanen, Tomi] Layer6 Ai, Toronto, ON, Canada.
RP Volkovs, M (reprint author), Layer6 Ai, Toronto, ON, Canada.
EM maks@layer6.ai; guang@layer6.ai; tomi@layer6.ai
CR Abadi M., 2016, ARXIV160304467
   Abel F., 2017, RECSYS CHALLENGE 201
   Agarwal D., 2009, C KNOWL DISC DAT MIN
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Collobert R., 2011, J MACHINE LEARNING R
   Covington P., 2016, ACM RECOMMENDER SYST
   Gopalan P., 1704, ARXIV13111704
   Gopalan P. K., 2014, NEURAL INFORM PROCES
   Graves A., 2013, C AC SPEECH SIGN PRO
   He  K., 2015, ARXIV151203385
   Hinton  G., 2012, IEEE SIGNAL PROCESSI
   Hu Y., 2008, INT C DAT ENG
   Ioffe S, 2015, INT C MACH LEARN
   Krizhevsky Alex, 2012, NEURAL INFORM PROCES
   Le QV, 2014, INT C MACH LEARN, P1188
   Maaten L. v. d., 2008, J MACHINE LEARNING R
   Srivastava N., 2014, J MACHINE LEARNING R
   Su X., 2009, ADV ARTIFICIAL INTEL
   Van den Oord A., 2013, NEURAL INFORM PROCES
   Vincent P., 2010, J MACHINE LEARNING R
   Wang C., 2011, C KNOWL DISC DAT MIN
   Wang H., 2015, C KNOWL DISC DAT MIN
   Wu C.- Y., 2017, C WEB SEARCH DAT MIN
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405004
DA 2019-06-15
ER

EF